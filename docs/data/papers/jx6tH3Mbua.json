{"id": "jx6tH3Mbua", "number": 19157, "cdate": 1758293950359, "mdate": 1759897056061, "content": {"title": "Consistency Training Helps Stop Sycophancy and Jailbreaks", "abstract": "An LLM's factuality and refusal training can be compromised by simple changes to a prompt. Models often adopt user beliefs (\"sycophancy\") or satisfy inappropriate requests which are wrapped within special text (\"jailbreaking\"). We explore *consistency training*, a self-supervised paradigm that teaches a model to be invariant to certain irrelevant cues in the prompt. Instead of teaching the model what exact response to give on a particular prompt, we aim to teach the model to behave identically across prompt data augmentations (like adding leading questions or jailbreak text). We try enforcing this invariance in two ways: over the model's external outputs (*Bias-augmented Consistency Training*, or BCT, from Chua et al. (2025)) and over its internal activations (*Activation Consistency Training*, or ACT—a method we introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant cues. Because consistency training uses responses from the model itself as training data, it avoids issues that arise from stale training data, such as degrading model capabilities or enforcing outdated response guidelines. While BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak reduction. We think that BCT can simplify training pipelines by removing reliance on static datasets. We argue that some alignment problems are better viewed not in terms of optimal responses, but rather as consistency issues.", "tldr": "Training models to be consistent across prompts can reduce sycophancy and jailbreaks", "keywords": ["LLMs", "activation steering", "representations", "self-supervised learning", "consistency", "safety", "alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/448a0091c8836c7d9392f52a07556239269ffe29.pdf", "supplementary_material": "/attachment/d2480072076cfd968b5e07eaf44959dfd2c01b08.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studies consistency training in the context of LLMs and where consistency training is measured as a self-supervised paradigm that teaches a model to be invariant to certain irrelevant cues in the prompt. This paper studies two such consistency training models: Bias-augmented Consistency Training (BCT) and Activation Consistency Training (ACT)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is addressing a really important problem and is analyzing consistency training using two different approaches (one proposed by them and one different from theirs) and compare the two approaches across popular models. They aim to show that combining these approaches can effectively address sycophancy and jailbreaks.\n\nThe ACT method is a contribution of the authors in this paper and the BCT method is from prior work."}, "weaknesses": {"value": "The takeaways of this paper are a bit weak.\n\nAre they championing BCT over ACT?\n\nDid they come to a conclusion on whether ACT and BCT can be combined?\n\nDid they truly demonstrate sycophancy and jailbreaks can be stopped effectively? The final results are hard to interpret for effectiveness based on the graphs and the writing.\n\nThe writing is too non-committal in several places! I wish the authors take some definitive stances on conclusions."}, "questions": {"value": "I like the problem being addressed this paper and I think it holds promise.\n\nI would like the authors to clearly state some answers to these weaknesses stated above:\n\na. What are the core takeaways of this paper? Did you truly demonstrate that the sycophancy and jailbreaks problems are solved effectively using ACT+BCT? The final results are hard to interpret for effectiveness based on the graphs and the writing.\n\nb. Are you championing BCT over ACT? If BCT is prior work, then the case for ACT is not well articulated. Then what should we takeaway as your contribution? We are the first authors to test BCT over Gemini is not a great way to state a contribution.\n\nc. Did you come to a conclusion on how, when or whether ACT and BCT can be combined as a function of the context?\n\nd. The writing is too non-committal in several places! I wish the authors take some definitive stances on conclusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jyyf2GdpS4", "forum": "jx6tH3Mbua", "replyto": "jx6tH3Mbua", "signatures": ["ICLR.cc/2026/Conference/Submission19157/Reviewer_SCz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19157/Reviewer_SCz6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961966135, "cdate": 1761961966135, "tmdate": 1762931168245, "mdate": 1762931168245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This ICLR submission introduces consistency training methods to reduce two AI safety vulnerabilities in large language models: sycophancy (where models agree with user beliefs even when incorrect) and jailbreaks (where malicious prompts bypass safety guardrails). The authors compare two approaches: Bias-augmented Consistency Training (BCT), which trains models to generate identical outputs for \"clean\" and adversarially \"wrapped\" prompts, and Activation Consistency Training (ACT), a novel method that enforces consistent internal neural activations between prompt variants. Both methods are self-supervised, using the model's own responses to clean prompts as training targets, which avoids issues with outdated training data. The paper evaluates these approaches on multiple Gemma and Gemini models, finding that both reduce sycophancy equally well, though BCT outperforms ACT on jailbreak defense."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Given ICLRs focus, the paper is short on a theoretical justification for why activation consistency should improve robustnes. The mixed results on capability staleness (strong evidence for sycophancy but not jailbreaks) suggest the benefits of fresh data may be context-dependent in ways the authors don't fully explain. ACT's underperformance on jailbreaks relative to BCT, combined with increased helpfulness degradation from both methods. This raises questions about practical deployability. The filterin requirement limits against novel trheats. The evaluation focuses heavily on universal jailbreaks but doesn't address adaptive attacks where adversaries know about the defense. Finally, the paper acknowledges but doesn't adequately address the risk that models might learn to ignore too much information, potentially degrading instruction-following in subtle ways not captured by their benchmarks."}, "weaknesses": {"value": "The paper shows a practical defense against known attacks. It cannot be seen as a solution towards the alignment problem in the sense that it cannot provide strong guarantees. True alignment likely requires the model to internalize values during pretraining, not just behavioral conditioning during fine-tuning."}, "questions": {"value": "Have you thought about evaluaing against encrypted or obfuscated jailbreaks? How do you prevent the model from learning conditional/deceptive behavior? Figure 4 shows BCT increases activation distance, is this because the model is learning \"if wrapped, behave differently\" rather than \"ignore wrapping\"? What is your threat model? Does consistency training defend against this? If not, what does it defend against? How do you verify that the target is safe? You Can consistency training be applied without white-box access? \nWhy does ACT perform worse than BCT on jailbreaks? You frame this as a tradeoff (ACT has better specificity). Could ACT's failure indicate that activation-level training on prompts alone is insufficient? How do you defend against adaptive attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CNEtQ4ymoJ", "forum": "jx6tH3Mbua", "replyto": "jx6tH3Mbua", "signatures": ["ICLR.cc/2026/Conference/Submission19157/Reviewer_XhTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19157/Reviewer_XhTd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985422905, "cdate": 1761985422905, "tmdate": 1762931167797, "mdate": 1762931167797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates BCT, a technique introduced by Chua et al. which trains models to respond consistently to prompts, even when adversarial cues are inserted. The authors compare BCT to other baselines and study the effect of using a static dataset rather than a generated one. They also introduce ACT, which leverages model internals rather than focusing solely on model output. These two techniques result in qualitatively and quantitatively different models, which the paper highlights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1**: Compares activation-aware and blackbox methods using fixed datasets to improve the consistency of models against adversarial/sycophancy-inducing cues.\n- **S2**: Details the scientific process that led to the techniques: presents the motivation for using white box methods with an activation patching experiment.\n- **S3**: Goes beyond performance differences between the methods and investigates the internal/behavioral differences of both models.\n- **S4**: The paper clearly states many limitations in the results, e.g., the staleness effect is much lower when models are recent."}, "weaknesses": {"value": "- **W1** I found the usefulness evaluation quite narrow and not attempting to identify the potential side effects of the technique:\n  - For sycophancy, I would have liked to see a useful cue dataset. For example, the user suggests a way of solving a problem to help the model, or adds \"I'm an ML researcher, summarize this paper for me\" vs \"I'm a high school student with no ML background, can you summarize this paper for me?\" to ensure the model is able to use those cues to articulate its response. While a quantitative analysis would be best, qualitative examples of the BCT/ACT model ignoring/using those useful cues would have been appreciated.\n  - For jailbreaks, testing instructions to roleplay harmless characters on harmless prompts would have been interesting to investigate. Again, qualitative analysis would be sufficient for me.\n  - It should be noted that the authors do acknowledge this limitation in the discussion (L441-444), but having some qualitative analysis seems like a straightforward way to get signal on how big of a concern this is.\n- **W2** It's unclear how the technique improves jailbreak resistance to adversarial pressure from automated systems [4,5] or human jailbreakers [6].\n- **W3** Some technical details remain unclear:\n  - L149-154: unclear what patching \"all token positions t of the two prompts\" does here. Do you patch the activation from the padding tokens? Do you patch only the wrapped answer token and not the corrupted token? Do you patch the template tokens? Please add a \\ref to Figure 1 here if you use the same setup. I find that specifying that you \"left-pad\" at line 151 is more confusing than helpful, as it's just an implementation detail and does not change the activations themselves. I would just remove it.\n  - L196-200: \"In initial experiments, we found that training activation consistency at all token positions led to divergent behavior\": unclear what the setup mentioned here is—all positions (even if not the same token?), all tokens from last suffix + completion?\n  - L242-244: do you sample or use greedy decoding to generate the responses?\n- **W4** In Appendix C.1 and C.2, it would have been nice to also see the Gemini 2.5 Flash pre-ACT/BCT answers to be able to compare them to the ACT/BCT responses.\n- **W5** In Appendix B, the authors only fine-tuned the attention layers. It would be nice to have an ablation on a model that also fine-tunes MLPs/LoRAs, or some references on why the authors thought this was a principled choice."}, "questions": {"value": "- Did you try to do ACT on the biggest suffix + the clean response tokens?\n- L63-64: \"Second, we point out that consistency training is well-suited to the problems of sycophancy and jailbreaks: we want to train the model to be invariant to certain cues in the prompt.\" Wasn't this already pointed out by Chua et al. [2] for sycophancy? This phrasing makes it sound like consistency training for sycophancy is a contribution of this paper, but as far as I understand, it's from Chua et al.\n- The paper evaluates the jailbreak defense on a static set of prompts. However, [1] (**published after this paper and not peer-reviewed**) shows that this kind of evaluation is not representative of real-world defense efficiency. Do you have any thoughts on this?\n- Do you plan to open-source the Gemma models you trained?\n- Curious about your thoughts on using other mechanistic interpretability insights on jailbreak efficiency (e.g., [3], which suggests adversarial suffixes hijack the attention from the template region to the suffix) to improve ACT further.\n\n[1]: Nasr, M., Carlini, N., Sitawarin, C., Schulhoff, S. V., Hayes, J., Ilie, M., ... & Tramèr, F. (2025). The attacker moves second: Stronger adaptive attacks bypass defenses against llm jailbreaks and prompt injections. arXiv preprint arXiv:2510.09023.\n\n[2]: Chua, J., Rees, E., Batra, H., Bowman, S. R., Michael, J., Perez, E., & Turpin, M. (2024). Bias-augmented consistency training reduces biased reasoning in chain-of-thought. arXiv preprint arXiv:2403.05518.\n\n[3] Leong, C. T., Yin, Q., Wang, J., & Li, W. (2025). Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region. arXiv preprint arXiv:2502.13946.\n\n[4] Shi, Chongyang, et al. \"Lessons from Defending Gemini Against Indirect Prompt Injections.\" arXiv preprint arXiv:2505.14534 (2025).\n\n[5] Mehrotra, Anay, et al. \"Tree of attacks: Jailbreaking black-box llms automatically.\" Advances in Neural Information Processing Systems 37 (2024): 61065-61105.\n\n[6] Zou, Andy, et al. \"Security challenges in ai agent deployment: Insights from a large scale public competition.\" arXiv preprint arXiv:2507.20526 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1yaVUOWV5b", "forum": "jx6tH3Mbua", "replyto": "jx6tH3Mbua", "signatures": ["ICLR.cc/2026/Conference/Submission19157/Reviewer_SGcv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19157/Reviewer_SGcv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118147965, "cdate": 1762118147965, "tmdate": 1762931167386, "mdate": 1762931167386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To combat jailbreaks and sycophancy, the authors propose training model consistency to enhance its resistance against such behaviors.\n\nThe authors put forward two methods:\n1. ACT (Activation Consistency Training): When dealing with inputs that have the same essence but different forms, it trains the alignment of activations.\n1. BCT (Bias-Augmented Consistency Training): When dealing with inputs that have the same essence but different forms, it trains by optimizing the loss related to outputs (via minimizing output-related loss).\n\nThe authors found that BCT exhibits a better inhibitory effect on jailbreaks, while both methods can alleviate sycophancy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The suppression of jailbreaks and sycophancy is an important issue.\n\nThe authors have effectively suppressed these two behaviors through the two proposed methods, and conducted experiments on both open-source models and frontier models, which demonstrates the feasibility of the methods."}, "weaknesses": {"value": "The authors conducted all evaluations exclusively on MMLU. As a relatively old test set that focuses on scientific fields, it seems insufficient to validate the new model's reliability.\n\nWhile the authors addressed consistency, whether the model's instruction-following ability has declined is a concern of mine. This is because the authors' data construction approach appears likely to cause the model to ignore some risk-free instruction prompts."}, "questions": {"value": "The paper seems to have not considered the issue of generalizability. Have the authors considered related issues? \n\nAs a type of SFT data, in which steps will it be integrated into model training?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The authors conducted experiments on Gemini Flash, which may violate the anonymity guidelines."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2kq9WoZVO1", "forum": "jx6tH3Mbua", "replyto": "jx6tH3Mbua", "signatures": ["ICLR.cc/2026/Conference/Submission19157/Reviewer_D2My"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19157/Reviewer_D2My"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19157/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762661685637, "cdate": 1762661685637, "tmdate": 1762931167089, "mdate": 1762931167089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}