{"id": "rpblsD3eXG", "number": 16127, "cdate": 1758260320595, "mdate": 1763771465008, "content": {"title": "Communication-Efficient Multi-Device Inference Acceleration for Transformer Models", "abstract": "Transformer models power many AI applications but suffer from high inference latency, limiting their use in real-time settings. Multi-device inference can reduce latency by parallelizing computation. Yet, existing methods require high inter-device bandwidth, making them impractical for bandwidth-constrained environments. We propose ASTRA, a communication-efficient framework that accelerates Transformer inference through a novel integration of sequence parallelism and a Mixed-Precision Attention mechanism designed to minimize inter-device communication. ASTRA compresses non-local token embeddings via vector quantization and preserves task accuracy through two optimizations, Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT and GPT2 across vision and NLP tasks show that Astra achieves up to 2.64$\\times$ speedups over single-device inference and up to 15.25$\\times$ speedups over state-of-the-art multi-device inferences, while operating under bandwidths as low as 10 Mbps.", "tldr": "", "keywords": ["multi-device inference", "communication-efficient transformers", "vector quantization"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e84a666950516676d457d03cecc3b1a8a41fee02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the significant challenge of high inference latency in Transformer models, particularly in multi-device scenarios where inter-device communication becomes a dominant bottleneck in low-bandwidth environments. The authors propose ASTRA, a communication-efficient inference framework that builds on sequence parallelism but introduces a novel Mixed-Precision Attention mechanism to drastically reduce communication overhead. This mechanism computes attention using full-precision embeddings for local tokens while using low-bit vector-quantized (VQ) representations for non-local tokens transmitted between devices. To preserve model accuracy under such aggressive compression, ASTRA introduces two key optimizations: Noise-Augmented Quantization (NAVQ), a training-time regularization strategy that injects noise into quantized embeddings to improve generalization , and Distributed Class Tokens, which replicates the class token to each device and aggregates the outputs to mitigate information bias. Experiments on ViT and GPT-2 models show that ASTRA achieves substantial end-to-end speedups -- up to 2.64x over single-device inference and 15.25x over other multi-device methods -- in bandwidth-constrained settings (as low as 10 Mbps), while incurring only minor accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is pretty interesting wtih 2 proposed optimizations to speedup inference across multiple device. I am not a systems expert, but the math behind the optimizations is correct and makes sense."}, "weaknesses": {"value": "The biggest weakness and drawback from the paper is the lack of large-scale experiments. Espeically the model choices in this paper is so small that none of these optimizations matter. Given the prevelance of open models of much larger sizes even ViTs and Qwen series models, it is empirically needed to make sure the proposed optimizations are quality neutral while providing the benefits. Without these results it hard to make a case for accepting the paper. I hope the authors can scale up the benchmarking further."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SkLiFSRlTs", "forum": "rpblsD3eXG", "replyto": "rpblsD3eXG", "signatures": ["ICLR.cc/2026/Conference/Submission16127/Reviewer_3hGB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16127/Reviewer_3hGB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760734083358, "cdate": 1760734083358, "tmdate": 1762926298281, "mdate": 1762926298281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASTRA, a communication-efficient framework for multi-device Transformer inference under bandwidth-constrained settings. Existing multi-device methods suffer from high inter-device communication overhead, which dominates latency when bandwidth is limited. ASTRA addresses this by combining sequence parallelism with a Mixed-Precision Attention mechanism: local attention is computed at full precision, while remote tokens are compressed via low-bit vector quantization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper observes a real bottleneck in multi-device Transformer inference for low-bandwidth or edge environments, which is increasingly relevant for real-time AI applications."}, "weaknesses": {"value": "1. ASTRA integrates known techniques (sequence parallelism + token quantization + noise augmentation), so the main contribution is in practical integration and bandwidth optimization, not a fundamentally new inference algorithm.\n2. Lacks formal characterization of attention approximation error due to vector quantization and noise injection. Consider adding error bounds or theoretical analysis of how quantization and noise affect attention computation and model accuracy.\n3. Experiments assume stable bandwidth; network variability or heterogeneous device scenarios are not tested.\n4. Latency and energy claims are based on simulation; real-world multi-device deployment is not evaluated. Incorporate hardware-level profiling (multi-GPU, FPGA, or edge devices) to substantiate speedup and efficiency claims."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lzdJwESrE6", "forum": "rpblsD3eXG", "replyto": "rpblsD3eXG", "signatures": ["ICLR.cc/2026/Conference/Submission16127/Reviewer_24hX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16127/Reviewer_24hX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792215120, "cdate": 1761792215120, "tmdate": 1762926297882, "mdate": 1762926297882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (1/2)"}, "comment": {"value": "Thank you for your comments and suggestions. We first provide general responses to all reviewers’ common questions. **The manuscript is also revised to include the additional experiments in Section 4.5**.\n\n**Q1: Can the proposed method, ASTRA, scale to large Transformer Models? (Reviewer 7h8e, mHin, 3hGB)**\n\n**A1:** We additionally evaluate ASTRA on a large transformer model, **Llama-3-8B**, for next-token prediction on English Wikipedia dataset \\[1\\]. 8-bit quantization is enabled for all methods, including the baselines and ASTRA, to execute inference with NVIDIA TitanX GPUs and keep fair comparisons. Results below show that **ASTRA scales to large models while maintaining performance close to the original and delivering substantial speedups under low bandwidth**.\n\n**Accuracy and Compression.** Table 1 reports perplexity (PPL; lower is better) together with communication overhead in bits per token as we vary the number of groups. Compared to the single-model baseline, **ASTRA maintains accuracy within a small margin while achieving a large communication reduction**. For example, at $G=1$, PPL increases from 5.81 (loss \\= 1.76) to 8.06 (loss \\= 2.08) for an over $1600\\\\times$ reduction in communicated datasize. \n\n**Table 1\\.** Task accuracy and communication overhead on Wikipedia with Llama-3-8B. \\#Groups is a hyperparameter in vector quantization. \n\n| Model | \\#Groups | Bits per Token | Compression Ratio | PPL |\n| :---: | :---: | :---: | :---: | :---: |\n| Llama-3-8B | \\- | 1,048,576 | \\- | 5.8118 |\n| ASTRA | 1 | 640 | 1,638.4 | 8.0631 |\n| ASTRA | 16 | 10,240 | 102.4 | 7.9197 |\n| ASTRA | 32 | 20,480 | 51.2 | 7.5656 |\n\n**Latency under Low Bandwidth.** Table 2 summarizes end-to-end latency on 4 devices with 1024 tokens across bandwidths from 10–500 Mbps. ASTRA consistently outperforms state-of-the-art multi-device inference methods. For instance, under low bandwidth (e.g., 10–100 Mbps), **ASTRA attains $1.13-5.13\\\\times$ speedup over the fastest baseline, i.e., BP, Nb=4**. Because 8-bit quantization is applied to all methods, these gains isolate the contribution from ASTRA’s communication-efficient design.\n\n**Table 2\\.** Latency (s) comparison between ASTRA and baselines on 4 devices with 1024 tokens across different bandwidths (Mbps). TP: Tensor Parallelism, SP: Sequence Parallelism, BP: Block Parallelism. \n\n| Bandwidth | 10 | 20 | 50 | 100 | 200 | 500 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Llama-3-8B | 4.578 |  |  |  |  |  |\n| TP | 430.952 | 216.291 | 87.449 | 44.499 | 23.025 | 10.140 |\n| SP | 28.256 | 14.939 | 6.888 | 4.215 | 2.857 | 2.052 |\n| BP, Nb=4 | 4.642 | 3.047 | 2.085 | 1.753 | 1.586 | **1.485** |\n| BP, Nb=8 | 8.011 | 4.780 | 2.773 | 2.101 | 1.762 | 1.561 |\n| ASTRA, G=1 | **1.563** | **1.549** | **1.547** | **1.545** | **1.541** | 1.540 |\n| ASTRA, G=16 | 1.661 | 1.659 | 1.595 | 1.572 | 1.559 | 1.548 |\n| ASTRA, G=32 | 1.940 | 1.796 | 1.661 | 1.630 | 1.603 | 1.583 |\n\n\\[1\\] Wikimedia Foundation, https://dumps.wikimedia.org"}}, "id": "Rc8eNgzWK0", "forum": "rpblsD3eXG", "replyto": "rpblsD3eXG", "signatures": ["ICLR.cc/2026/Conference/Submission16127/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16127/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16127/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763769833209, "cdate": 1763769833209, "tmdate": 1763770051791, "mdate": 1763770051791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASTRA, a framework for accelerating transformer inference across multiple devices in bandwidth-constrained environments. The key part is a Mixed-Precision Attention mechanism that computes local attention with full precision while using vector-quantized embeddings for non-local tokens, reducing communication overhead. To preserve accuracy under aggressive compression, the authors propose Noise-Augmented Vector Quantization (NAVQ) and Distributed Class Tokens. Experiments on ViT and GPT-2 models across vision and NLP tasks demonstrate speedups of up to 2.64x over single-device inference and 15.25x over existing multi-device methods at bandwidths as low as 10 Mbps, while maintaining accuracy within 3.58% of the original models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a real bottleneck: The paper identifies and tackles a genuine problem, that communication dominates latency (58.6-93.5%) in bandwidth-constrained multi-device inference.\n2. Novel compression approach: The Mixed-Precision Attention mechanism is creative, using full-precision for local tokens and VQ for remote tokens.\n3. Good evaluation: Extensive experiments across multiple architectures (ViT, GPT-2), tasks (classification, language modeling), and conditions (bandwidth, device count, heterogeneity) demonstrate broad applicability.\n4. Practical compatibility: The framework integrates with existing quantization methods (8-bit, 4-bit), showing additional speedups of 1.35 to 2.73x when combined."}, "weaknesses": {"value": "1. Limited architectural types: The evaluation focuses only on ViT and GPT-2, which are relatively small and dated models. Modern applications use much larger models (e.g., LLaMA variants). The scalability claims are weakened without evidence on contemporary, production-scale models.\n\n2. Severe zero-shot degradation: Table 3 shows large performance drops in zero-shot settings (e.g., GPT-2M perplexity increases from 43.22 to 62.29, a 44% degradation). This is a critical limitation for practical deployment where generalization is essential. \n\n3. Baselines potentially unfair: The comparison with BP, TP, and SP assumes these methods use full float32 precision. However, these methods could also be combined with standard compression techniques (gradient compression, activation compression). A more fair comparison would evaluate \"BP+8bit quantization\" vs \"ASTRA+8bit quantization\" to isolate ASTRA's contribution. Additionally, recent methods like FlexGen or DistServe are not compared, making it unclear how ASTRA compares to the current sota.\n\n4. Communication model oversimplified: The paper assumes fixed bandwidth and doesn't account for real-world network variability, packet loss, or latency jitter. The latency model appears to assume perfect overlap of computation and communication, which is rarely achievable. Dynamic bandwidth fluctuations common in WiFi environments (cited as the target deployment) could significantly impact the practical speedups. The authors should evaluate under realistic network conditions with variable bandwidth and packet loss, or at minimum discuss how ASTRA degrades under non-ideal conditions."}, "questions": {"value": "Please address the implicitly listed questions in the weakness.\n\nCan you provide a decision tree or heuristic for practitioners to select:\n1. Number of groups G based on task type (vision vs. NLP) and target accuracy?\n2. Commitment loss weight ε based on model architecture and dataset?\n3. When to use distributed vs. single class tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWHe6yvnlR", "forum": "rpblsD3eXG", "replyto": "rpblsD3eXG", "signatures": ["ICLR.cc/2026/Conference/Submission16127/Reviewer_mHin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16127/Reviewer_mHin"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986255678, "cdate": 1761986255678, "tmdate": 1762926297543, "mdate": 1762926297543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a way to improve communication efficiency for multi device Transformer inference using sequence parallelism and mixed-precision. They show significant speedups on ViT and GPT2 scale models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-presented and easy to follow\n- Communication overhead is significant in large Transformer model distributed settings\n- The use of codebooks is interesting"}, "weaknesses": {"value": "- The models used for inference are small and it is not clear to me that these hold at scale."}, "questions": {"value": "- It would be helpful to further motivate the wireless and edge deployment motivation for this framework. Why are we doing inference requests on wifi?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i5ZhHPkGoU", "forum": "rpblsD3eXG", "replyto": "rpblsD3eXG", "signatures": ["ICLR.cc/2026/Conference/Submission16127/Reviewer_7h8e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16127/Reviewer_7h8e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158970672, "cdate": 1762158970672, "tmdate": 1762926297149, "mdate": 1762926297149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}