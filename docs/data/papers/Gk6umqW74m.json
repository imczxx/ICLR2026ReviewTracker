{"id": "Gk6umqW74m", "number": 7266, "cdate": 1758013607400, "mdate": 1759897862931, "content": {"title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "abstract": "Large language models (LLMs) are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science.\nHowever, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce **NewtonBench**, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant.\nMoreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive evaluation of 11 state-of-the-art LLMs reveals a clear but fragile capability for discovery in frontier models: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge for the future of automated science. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.", "tldr": "We introduce NewtonBench, a benchmark for evaluating LLMs’ scientific law discovery via interactive, generalizable experiments.", "keywords": ["large language models", "benchmark", "virtual environment", "generalization", "agent", "scientific law discovery"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29ece7f2faf6baea1131b2ee10b95f411e9b3ee2.pdf", "supplementary_material": "/attachment/40c6e673d08147a4275e4324e3476b79f93a2ae1.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents NewtonBench, which is a new, large-scale benchmark designed to assess and drive progress in LLM-based scientific law discovery. The benchmark introduces the “metaphysical shift” method, which use systematically mutating canonical physical laws to generate a suite of 324 interactive tasks spanning 12 domains. Unlike prior benchmarks focused on static function fitting, NewtonBench requires agents to engage in interactive experimentation within virtualized model systems, aiming to uncover hidden governing equations. Results across 11 leading LLMs reveal both the promise and current fragility of language models in scientific reasoning, especially as complexity and noise increase. The paper analyzes emergent behaviors such as the paradoxical impact of code assistance and provides an in-depth characterization of LLMs’ reasoning limitations in Automating Scientific Discovery."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting task formulation: I think it is a valuable direction to test frontier agents' capability in autonomously make discovery especially in the domain of physics, the authors identified a crucial field worth exploring by the AI4Sci community at large and this direction is critical for future LLM agents for science research.\n2. Evaluation and Discussion: The authors carefully evaluated a wide range of models and made a nice analysis of the exploration-exploitation tradeoff from observation to deeper reasoning.\n3. Nice details: The work has quite a lot of details in appendix and shows considerable effort from the authors to make this a detailed presentation."}, "weaknesses": {"value": "1. \"Re-discover Newton's Laws\": I'm a bit skeptical of how this central RQ is presented, because apparently the LMs have picked up what these laws mean in their pre-training corpora, and this is impossible to undo without systematic machine unlearning technique and access to these proprietary corpora.\n\n2. I understand that the authors' reponse to 1. might be that the ”metaphysical shift\" solves that by essentially permutating the laws into new forms, which appears different but remain physically valid. I'm also skeptical about this claim \"fully prevent memorization\" in 2 points:\n\na. How could we know that the permutated forms are not in training at all? Granted, it might be significantly less exposed than the original law, but is there any actual proof that these variants are unseen or not memorized \"at all\"? I imagine a validation experiment on this may be via elicitation or some other kind of contamination probing experimentation to offer more robust evidence as to whether models have memorized even some parts of these permutated variants.\n\nb. The current explanation used philosophical references and counterfactual reasoning etc. which honestly are not convincing enough for me, maybe the authors would like to strengthen this part, also I don't feel like it's necessary to say it's \"metaphysical shift\" because in essence isn't that just data augmentation under governing physical principles? I feel this wording adds unnecessary complexity to understanding this, making it look more philosophical than it needs to be\n\n3. The presentation is not super clear to me as to the 3 tier systems (vanilla-simple-complex), while the 3 tier permutation is more clear to me, my current understanding is that the systems are defined by the number of \"assisting equations\" towards the f_target which raise a number of questions\n\na. How do you guarantee that there's only one path towards f_target? often times there are many ways to discover/deduce the same laws of physics via different routes\n\nb. Why is it justified to define level of complexity by this? I feel like the number of deduction may not be the best indicator for complexity, especially when these assisting equations aren't really explained or given sample of in a very clear manner in the current manuscript\n\nAs a result I think the permutation 3-level makes more sense to me than the other axis (experimental system etc.) Maybe the authors could improve the presentation of this part as it's very confusing for now."}, "questions": {"value": "See weakness, additionally could the authors clarify and formalize the claim that metaphysical shifts fully prevent memorization? Has any empirical analysis (e.g., recall, overfitting checks) been performed especially with large, closed-source LLMs known for such a method to fully prevent memorization? (mitigate might be a more accurate wording? fully is a really strong wording that requires more evidence to back it up)\n\nAlso I feel like one suggestion I have for the authors is that the manuscript would be significantly easier to comprehend if they can offer visualization of a sample eval run, e.g. with a given model what exactly is the input-output of each step, the current discussion section e.g. have the dichotomous impact part which can be merged with the paradox discussion to save some space for this example. As it's highly non-trivial to illustate the key diff betwee \"active\" and \"passive\", a nice visualization of how any given agent actually do in terms of the concrete input-output would significantly strengthen my understanding of their approach.\n\nIn general, I feel this paper is exploring an interesting+valuable direction but could really benefit a lot from better presentation, therefore I recommend a boardline rating on the positive side with more room for improvement. Overall I appreciate the design of this approach by the authors but this paper could really benefit from better articulation for larger impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fS4P6f3tPN", "forum": "Gk6umqW74m", "replyto": "Gk6umqW74m", "signatures": ["ICLR.cc/2026/Conference/Submission7266/Reviewer_T5rR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7266/Reviewer_T5rR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760516992073, "cdate": 1760516992073, "tmdate": 1762919396911, "mdate": 1762919396911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NewtonBench, a benchmark for evaluating large language models (LLMs) on discovering physical laws through interactive experimentation. It introduces metaphysical shifts, i.e. a transformation method applied to canonical physics equations (for example, modifying exponents or couplings), to generate 108 shifted laws across 12 physics domains. Each law is instantiated in three configurations: 1) Vanilla Equation: the shifted law alone, 2) Simple System: the law embedded in a minimal experimental setup,\n3) Complex System: the law coupled with additional equations. Together these form 324 total tasks. The authors also provide a formal proof that all tasks are finitely solvable.\n\nLLMs interact with each environment by adjusting variables and observing outputs to infer the hidden governing law. Performance is evaluated by symbolic accuracy and robustness to noise. Eleven popular LLMs were tested, showing strong performance on simple cases but substantial drops for complex or noisy systems.\n\nMain contributions include:\n1. A large-scale, interactive benchmark for equation discovery.\n2. The metaphysical shift method for creating diverse, solvable tasks.\n3. A comprehensive evaluation of leading LLMs’ capabilities on the proposed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focus on interactive scientific discovery, which is an important and relatively underexplored area.\n2. It proposes an effective method for generating new equations and provides a formal proof of their finite solvability.\n3. The evaluation is thorough, covering a wide range of models."}, "weaknesses": {"value": "1. The proposed \"metaphysical shifts\" fail to create a \"physically plausible universe\" and ultimately compromise the scientific relevance of the task. The core problem is that the physics descriptions become meaningless once the dimensional structure and proportional dependencies of core quantities are altered. In the laws altered by NewtonBench (such as Newton’s Law of Universal Gravitation, Coulomb’s Law, Heat Transfer, and Hooke’s Law), most are phenomenological in nature and contain at least one parameter or coefficient whose physical identity is defined by its role within that specific law. Examples include the gravitational constant, the permittivity, the thermal conductivity, or the spring constant. These quantities do not possess independent operational definitions outside the context of the laws that introduce them.\nTherefore, when the functional form of such a law is altered (for instance, by changing exponents or couplings), those parameters lose their original physical meaning, since their identity is inseparable from the structure of the law itself. The modified equations thus describe mathematically consistent systems but no longer preserve the physical semantics of the original quantities or interactions. This reframes the task from a genuine physics discovery challenge into interactive equation discovery within an abstract, non-physical framework.\n\n2. The so-called “model system discovery” in NewtonBench does not introduce a fundamentally different epistemic or physical level of reasoning. It merely increases equation compositionality (the number of nested or coupled expressions), which can always be flattened into a single symbolic equation. In real physics, a model system is defined by time-evolving state variables, interactions governed by symmetries or conservation laws, and differential equations that capture causality and dynamics. In contrast, the relations in NewtonBench reduce to static algebraic mappings between scalar quantities, making its “complex systems” only syntactically more complicated rather than fundamentally different, as they remain within the same layer of symbolic algebra."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9c7M648ITm", "forum": "Gk6umqW74m", "replyto": "Gk6umqW74m", "signatures": ["ICLR.cc/2026/Conference/Submission7266/Reviewer_9pai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7266/Reviewer_9pai"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600535650, "cdate": 1761600535650, "tmdate": 1762919396539, "mdate": 1762919396539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **NewtonBench**, a benchmark for evaluating LLMs' scientific law discovery capabilities that addresses fundamental limitations in existing approaches. Current benchmarks suffer from a methodological trilemma (forcing trade-offs between scientific relevance, scalability, and memorization resistance) and oversimplify discovery as static function fitting rather than interactive exploration. NewtonBench resolves these issues through two key innovations: (1) **metaphysical shifts** that systematically mutate canonical physical laws to generate 324 scientifically grounded yet memorization-resistant tasks across 12 physics domains, and (2) **interactive model discovery** requiring agents to actively probe virtual environments to uncover hidden laws embedded in complex systems with confounding variables. Evaluation of 11 state-of-the-art LLMs reveals that while frontier models like GPT-5 and Gemini-2.5-pro demonstrate emerging capability (65-73% accuracy), their performance degrades precipitously with increasing complexity, exhibits extreme sensitivity to observational noise, and paradoxically suffers when given code assistance due to premature exploitation over exploration. The benchmark demonstrates that robust, generalizable scientific discovery in complex, interactive environments remains the core unsolved challenge for AI-driven science."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper exhibits **high originality** through its novel benchmark design principles, **strong quality** in experimental rigor and scope, **good clarity** in presentation and visualization, and **substantial significance** for both AI research and automated science. The work makes meaningful contributions across all four dimensions, with particularly notable strengths in originality (metaphysical shifts resolution of trilemma) and clarity (outstanding visual communication and narrative structure). The significance is enhanced by the timeliness of addressing LLM scientific reasoning at a critical juncture in model capability development."}, "weaknesses": {"value": "### LLM-as-Judge for Primary Metric Introduces Circularity\n\n**Problem**: Symbolic Accuracy, the main evaluation metric, relies on LLM verification of equation equivalence. Using LLMs to judge LLM-generated discoveries creates potential systematic biases.\n\n**Actionable Fixes**:\n\n1. Provide detailed error analysis: On what types of equations does LLM-as-judge fail? Are these random or systematic?\n2. Report inter-rater reliability among human experts\n3. Include traditional symbolic equivalence checking (even if imperfect) as a secondary validation\n4. Test whether different judge models (GPT vs. Gemini vs. Claude) produce consistent verdicts\n\n### Framing Claims \"Scientific Law Discovery\" but Evaluates Only Physics Equations\n\n**Problem**: The paper is titled \"Benchmarking Generalizable **Scientific Law Discovery**\" and makes broad claims about \"AI-driven science,\" \"automated science,\" and \"genuine scientific intelligence\" throughout. However, the evaluation exclusively covers **physics equations**—12 domains, all closed-form algebraic expressions. This creates a fundamental misalignment between claims and evidence. Scientific discovery encompasses far more than physics equations: chemistry involves molecular structures and reaction mechanisms, biology includes qualitative evolutionary principles and statistical patterns, social sciences rarely use closed-form equations, and even within physics the benchmark excludes quantum mechanics, field theories, differential equations, and computational models. The paper's significance claims about measuring \"scientific intelligence\" and guiding development of agents \"capable of genuine scientific discovery\" substantially overreach what physics equation discovery can demonstrate.\n\n**Actionable Fixes**: \n\n(1) **Reframe title and claims** to match actual scope—change to \"Benchmarking Physics Equation Discovery\" or \"Mathematical Law Discovery\"; (2) **Add explicit scope limitations** in Abstract/Introduction acknowledging this represents one facet of scientific discovery; (3) **Discuss generalization boundaries**—which findings likely transfer to non-physics domains (e.g., noise sensitivity) versus which are physics-specific (e.g., dimensional consistency); (4) **Provide expansion roadmap** for chemistry (reaction mechanisms), biology (population dynamics), or qualitative theory discovery to validate claimed generalizability. Honest scoping doesn't diminish the contribution—it makes claims defensible and clarifies what the benchmark actually measures: rigorous evaluation of mathematical law discovery from experimental data, a valuable but bounded capability for AI-assisted science."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Vh62YuMGE", "forum": "Gk6umqW74m", "replyto": "Gk6umqW74m", "signatures": ["ICLR.cc/2026/Conference/Submission7266/Reviewer_cQE7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7266/Reviewer_cQE7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735501895, "cdate": 1761735501895, "tmdate": 1762919396110, "mdate": 1762919396110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces NEWTONBENCH, a benchmark to evaluate whether LLM agents can rediscover scientific laws through interactive experimentation rather than static curve fitting. Tasks are formalized via hidden target equations embedded in model systems, with agents probing a virtual environment and optionally using a code interpreter to hypothesize laws. The benchmark spans 324 tasks from 12 physics domains, created by applying metaphysical shifts to canonical laws to ensure novelty and prevent memorization. Performance is measured by Symbolic Accuracy (structure equivalence) and RMSLE (data fidelity). Experiments on 11 LLMs show frontier models succeed on simple systems but degrade with system and equation complexity and under noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The benchmark is well-motivated and carefully constructed, with clear task formalization, principled law mutations, and dual metrics. The interactive setup and solvability proof are strong points. The experiments are broad (11 models) and include informative analyses (noise robustness, code-assistance effects), with consistent reporting (four runs, CIs).\n* Task specification is clear and formal. The benchmark formalizes Equation and Model with expression-tree semantics and sequential model composition, clarifying what is hidden vs. provided to the agent. Three model settings (Vanilla/Simple/Complex) explicitly encode difficulty via system composition, aligning evaluation with progressive scientific scenarios. This makes difficulty interpretable.\n* The agentic evaluation protocol is principled and interactive. Agents must conduct experiments via a standardized ‘<run_experiment>’ interface rather than passively fit given datasets. This better reflects real discovery workflows.  \n* Experimental suite is comprehensive. 11 diverse LLMs are compared with consistent settings, including “reasoning vs. non-reasoning” categorization and code-use variants. Main results aggregate across difficulty and system complexity with mean and std and additional analyses for noise and code assistance."}, "weaknesses": {"value": "* The paper excludes classical symbolic-regression systems and prior LLM-SR pipelines from evaluation because they do not support the interactive model-system protocol (Ethics), which weakens external validity claims.  Although “Vanilla Equation” tasks exist (Sec. 2), there is no reported attempt to adapt strong SR baselines in this subset to provide a bridge comparison. No direct evidence found in the manuscript. The conclusions about “genuine scientific intelligence” would be stronger if positioned against competitive non-agentic baselines on overlapping subproblems.\n* The method relies on an LLM as a judge for symbolic equivalence. While the authors report 98.3% agreement with humans, details like adjudication model identity and calibration procedures are in the appendix without ablations in the main results. Potential bias arises if the judge shares training or inductive biases with evaluated models; the paper does not present stress tests (adversarial equivalences) for the judge. No direct evidence found in the manuscript.  \n* I am also concerned on the physical plausibility and task realism. The paper acknowledges that some mutated laws, while dimensionally coherent, may be physically implausible in our universe. This could reduce ecological validity for certain claims.There is no user study or expert rating on realism/interpretability of discovered laws beyond dimensional checks. No direct evidence found in the manuscript. The strong claim of resolving the “trilemma” would benefit from quantitative measures of “scientific relevance” beyond coverage of canonical domains."}, "questions": {"value": "* On Vanilla Equation tasks, will you adapt strong symbolic-regression (SR) methods to your submission format so they can output discovered forms and constants, and report SA/RMSLE side‑by‑side with your agent?\n* Where exact protocol parity is impossible, can you include partial evaluations (e.g., passive dataset variants) to triangulate your difficulty claims?\n* In Sec. 4, can you explicitly separate results that require “interactive discovery” from those a non‑agentic baseline could achieve on overlapping subproblems, and provide a table that maps each claim to the relevant baseline category?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xJUJ9q8KDJ", "forum": "Gk6umqW74m", "replyto": "Gk6umqW74m", "signatures": ["ICLR.cc/2026/Conference/Submission7266/Reviewer_KvtF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7266/Reviewer_KvtF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131680331, "cdate": 1762131680331, "tmdate": 1762919395535, "mdate": 1762919395535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}