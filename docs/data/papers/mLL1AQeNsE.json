{"id": "mLL1AQeNsE", "number": 22659, "cdate": 1758334150708, "mdate": 1759896854252, "content": {"title": "DEPTRAI: Detachable External‐memory layer for Parameter-Transformer Injection", "abstract": "Large language models (LLMs) quickly become outdated because the factual knowledge they encode is fixed at training time, and retraining for every new fact is prohibitively expensive. Prior ``internal'' editors apply closed-form perturbations directly to the feed-forward weights, but each new patch is applied in place to the base model, causing edits to accumulate, interfere, and preventing straightforward revocation. We present DEPTRAI—\\textbf{D}etachable \\textbf{E}xternal-memory layer for \\textbf{P}arameter-\\textbf{Tra}nsformer \\textbf{I}njection—that stores each edited fact as a key–value tuple outside the model, leaving all original weights frozen. At inference, the frozen FFN produces a subject key, which is routed to the nearest stored key using a Mahalanobis metric that mirrors the inverse-covariance scaling of closed-form editors. A lightweight gate then either substitutes the edited value or preserves the base projection. This design turns factual patching into a reversible database-style update rather than a permanent modification of parameters. DEPTRAI achieves the highest average performance on sequential editing tasks, outperforming the latest dual-memory method WISE by \\textbf{15–20\\%},", "tldr": "DEPTRAI is a detachable external-memory layer for LLMs that stores edits as key–value facts outside the model", "keywords": ["Large Language Model", "Knowledge Editing", "Model Editing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5829d920b590257850448838a4fd064fb0240334.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DEPTRAI, an editing mechanism for large language models that can be understood as combining two prior lines of work: (1) GRACE-style external key–value memory with routing at inference time, where edits are stored outside the base model and selectively activated, and (2) MEMIT-style closed-form weight update analysis, where desired factual changes are framed as solving a regularized normal equation that trades off edited keys K₁ and preserved keys K₀ via an inverse-covariance term. DEPTRAI keeps the GRACE-like idea of storing edits as key–value pairs and retrieving them at inference, but replaces GRACE’s cosine/dot-product router with a Mahalanobis metric (reduced to dot product form) that is derived directly from the MEMIT/AlphaEdit closed-form mixing coefficients $\\beta=K_1^\\top C^{-1}k$, effectively interpreting $C^{-1}=K_0K_0^T+K_1K_1^T$ as a whitening transform and turning retrieval into *which stored key would MEMIT have blended into this FFN output?*\n\nIn experiments on LLaMA and Qwen models (3/8B, 3B) across sequential editing benchmarks like ZsRE and hallucination correction, DEPTRAI maintains high reliability and locality over long edit sequences (up to 1,000 edits) and reports 15–25% higher average performance than WISE at depth, while noting remaining limitations such as weaker generalization across synonyms and some residual locality interference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- DEPTRAI elegantly combines the strength between two lines of knowledge editing works\n- Good lifelong editing performance on ZsRE and SelfCheckGPT with $\\leq$ 1k timesteps"}, "weaknesses": {"value": "- Missing some more recent lifelong editing baselines such as sLKE [1], LeMOE [2], and ELDER [3].\n- Evaluation is not sufficient regarding\n  - The finetuning baseline should adopt the fair setups as discussed in [4,5]. The FT-L, FT-M are ill-defined baselines which might mislead the community.\n  - Lack layer-wise ablations as the baselines and DEPTRAI choose different layers for editing.\n  - The scaling of timestep is only to 1k. More timesteps can be shown, e.g., up to 5k.\n- The contribution is a bit limited as the novelty is mainly the metric for key similarity.\n- Writing quality can be improved. For example, Figure 1 does not explicitly show the fundamental difference between DEPTRAI and existing approaches such as GRACE and WISE.\n\n\n> [1] Cheng, YuJu, et al. \"Serial lifelong editing via mixture of knowledge experts.\" Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2025.\\\n> [2] Wang, Renzhi, and Piji Li. \"LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\\\n> [3] Li, Jiaang, et al. \"ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 23. 2025.\\\n> [4] Gangadhar, Govind, and Karl Stratos. \"Model editing by standard fine-tuning.\" arXiv preprint arXiv:2402.11078 (2024).\\\n> [5] Yang, Wanli, et al. \"Fine-tuning Done Right in Model Editing.\" arXiv preprint arXiv:2509.22072 (2025)."}, "questions": {"value": "1. Can you provide experiments results with the additional baselines?\n2. Can you perform ablation studies on layer selection and may be other aspects to further analyze the proposed approach?\n3. Can you add a section to discuss the fundamental similarity and difference between MoE adapters/LoRA and codebook-style editing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There's no specific ethic concern."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnNHgvCs8I", "forum": "mLL1AQeNsE", "replyto": "mLL1AQeNsE", "signatures": ["ICLR.cc/2026/Conference/Submission22659/Reviewer_qki6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22659/Reviewer_qki6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426465880, "cdate": 1761426465880, "tmdate": 1762942325272, "mdate": 1762942325272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DEPTRAI, a novel knowledge editing framework for Large Language Models, aiming to address the issues of accumulating interference and irreversibility in existing \"internal editors\"  during sequential editing. The core idea of DEPTRAI is to keep the base LLM weights completely frozen, storing each new fact as a key-value pair in an external memory. Its key innovation lies in a principled router based on Mahalanobis distance, which determines during inference whether to replace the model's internal projection with the external stored value."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strength 1**: Instead of using a trainable router or simple cosine similarity, it ingeniously derives the external routing rule (Mahalanobis distance) from the mathematical principles of internal editing methods. This internal-editing-inspired external routing is a highly novel and principled design."}, "weaknesses": {"value": "**Weakness 1**: The paper claims in the contribution and ablation studies that the Mahalanobis distance router is \"robust to surface form variations\" and outperforms cosine similarity. However, in the conclusion (Section 6), the authors list \"stored keys may not generalize well to synonyms or transliterations\" as a current limitation, which appears inconsistent with the earlier claim of robustness to surface variations.\n\n**Weakness 2**: The theoretical derivation in Section 3.1 (from the $\\beta$ coefficient to the Mahalanobis distance) requires $\\Lambda$ to be a global covariance matrix ($C^{-1}$), in order to compare all keys in the same \"whitened\" space. However, the implementation part in Section 3.2 (\"Memory structure\") implies $\\Lambda$ is local, defining the external storage as $\\mathcal{E}=\\{(\\mu_{j},\\Lambda_{j},v_{j})\\}_{j=1}^{M}$, where each entry $j$ has its own $\\Lambda_j$. This creates a theoretical contradiction.\n\nMore critically, the formula in Section 3.2 (Eq. 14)  used to calculate this local $\\Lambda_j$ appears to be a critical typo. The formula first defines $\\mu_j = k_j$, and then immediately uses the term $(k_j - \\mu_j)$ to calculate $\\Sigma_j$. This necessarily results in the term being a zero vector, causing $\\Sigma_j$ to degenerate into $\\epsilon I$. This would cause the Mahalanobis distance to degenerate into a (scaled) Euclidean distance, thereby completely undermining the paper's core argument about the superiority of Mahalanobis distance (relative to Euclidean or cosine similarity).\n\nIf $\\Lambda$ is global, the authors must clarify how $K_0$ is collected and how $C^{-1}$ is efficiently updated during sequential editing (since $C$ depends on $K_1$)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DA2h2vKgwD", "forum": "mLL1AQeNsE", "replyto": "mLL1AQeNsE", "signatures": ["ICLR.cc/2026/Conference/Submission22659/Reviewer_m911"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22659/Reviewer_m911"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449152861, "cdate": 1761449152861, "tmdate": 1762942325019, "mdate": 1762942325019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DEPTRAI (Detachable External-memory layer for Parameter-Transformer Injection) is a method for integrating new facts in model, essentially a combination of ideas from:\n(a) ROME/MEMIT/AlphaEdit: W_out matrices in FFN layers are associative memories (key/value stores) that can be perturbed/updated (Delta) to account for edits (with keys/values for subject/its edit).\n(b) GRACE: updates are triggered only after a threshold condition is met (deferral mechanism).\n\nHowever: DEPTRAI does not inject Delta updates in model parameters (as in (a)) and does not finetune the values (as in the separately kept codebook in (b)). So the model is not touched and external memory updates do not trigger backpropagation (in that aspect DEPTRAI has strong similarity to the external scope detector idea in [1] or as originally in SERAC). Extensive benchmarking focusing on the sequential editing task (multiple methods/datasets/metrics) identifies the strengths of the proposed approach.\n\n[1] Das, P., Chaudhury, S., Nelson, E., Melnyk, I., Swaminathan, S., Dai, S., Lozano, A., Kollias, G., Chenthamarakshan, V., Navratil, J. and Dan, S., 2024, July. Larimar: Large Language Models with Episodic Memory Control. In International Conference on Machine Learning (pp. 10109-10126). PMLR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive empirical results: multiple methods and datasets are benchmarked; multiple metrics are reported.\n\n- This is an interesting and simple synthesis of core ideas from model editing literature (however editing an external memory instead)."}, "weaknesses": {"value": "- Not touching model parameters (train-free) and still being able to adapt it to new facts is a very appealing idea, but intuitively this should have limitations. There are some hints in the Conclusion (Lines 421-423) but the reader would definitely appreciate more details on this. \n\n- Presentation can be improved: in particular some results could move to the Appendix, key notions could then be developed further and notation or equations could be revisited for corrections. Please see the Questions slot for details/suggestions.\n\n- There is not a clean signal regarding the superiority of DEPTRAI: for example empirical results in Table 4 (Appendix D) are not as encouraging as results in Tables 1 and 2 (main text)."}, "questions": {"value": "- Is Figure 3 a plot of Rel. columns from Table 1?\n\n- Line 234: mu_j = k_j? then Sigma_j's would only be epsilon I?\n\n- Line 262 / Equation (22): Could you clarify the notation/symbol immediately following = sign?\n\n- Lines 278-279: FT-L and FT-M seem to refer respectively to ROME and MEMIT? Is so why using additional alternative names?\n\n- Lines 264-268: Is there an intuition behind the interesting separation in Figure 2 for some of the models?\n\n- Lines 302-306: Since this part is deferred to the the Appendinx, a better use of this space would be to explain/clarify further the key scores in this work: reliability, generalization and locality (and expanding Eq (23)).\n\n\n- For serializable external memory:\n  - what is the cost of computing Sigma_j's, inverting them (Lambda_j's) and storing?\n  - how relatively important can these additional space/time complexities be, assuming typical target factual edit cardinalities M?\n\n\n- How would results from MEMIT compare? Assuming that instead of making sequential updates up to T items (i.e. one-by-one for T=1, 10, 100, 1000) as in the manuscript, we added all T items in one shot (or even updating in item batches of > 1 items), would we expect to see benefits (i.e. sequential vs batch updates)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HWf8arXqOk", "forum": "mLL1AQeNsE", "replyto": "mLL1AQeNsE", "signatures": ["ICLR.cc/2026/Conference/Submission22659/Reviewer_8FBy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22659/Reviewer_8FBy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979473721, "cdate": 1761979473721, "tmdate": 1762942324021, "mdate": 1762942324021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **DEPTRAI**, a detachable external-memory layer for knowledge editing that leaves base LLM weights untouched. Each factual edit is stored as a key–value pair outside the model; at inference, the frozen FFN emits a subject key that is routed to the closest stored key via a Mahalanobis metric derived from the closed-form mixing coefficient used by ROME/MEMIT, and a lightweight gate injects the edited value or preserves the base projection. This reframes editing as reversible, database-style updates rather than cumulative in-place perturbations, mitigating interference and easing revocation. Across LLaMA-3.2-3B, Qwen-2.5-3B, and LLaMA-3.1-8B on ZsRE sequential edits and a hallucination correction setup, DEPTRAI sustains high reliability and near-perfect locality at large edit depths, outperforming recent dual-memory baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea—moving edits out of the base weights into a detachable key–value memory with a principled router—is intuitive yet original. By storing a single subject key and edited value per fact and routing with a Mahalanobis metric derived from the closed-form mixing coefficient used in ROME/MEMIT, the method turns knowledge editing into a reversible, database-style retrieval-and-injection step, avoiding cumulative interference and simplifying revocation/audit.   Methodological quality is strong: the paper motivates the Mahalanobis router from the mixing-coefficient analysis and adds an explicit gate to balance old vs. new information at inference, giving a clear mechanism for locality and reliability. Clarity is good, with an explicit contrast to in-place editors and a step-by-step description of the external layer; the framing “from in-place perturbation to detachable memory” makes the contribution easy to grasp and to implement in existing pipelines."}, "weaknesses": {"value": "The evaluation is concentrated on editing-specific suites and a small “general capability” check that is largely short-form classification/MC (SST, MRPC, RTE, CoLA) plus MMLU. This leaves open whether edits preserve or disrupt performance on harder, generative reasoning and coding tasks. Concretely, the paper does not report effects on contemporary math/coding benchmarks or long-form QA after large edit batches. To strengthen external validity, please (i) add rigorous pre/post-edit results on **AIME’24/’25** and **MATH500** (ii) include **LiveCodeBench** to assess code reliability under heavy edits; (iii) use an instruction-following suite such as **IFEval** to probe instruction adherence; and (iv) include a simple open-domain QA set (e.g., **SimpleQA**) to check whether retrieval-style edits bias factual QA."}, "questions": {"value": "See more details in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "no837qKzva", "forum": "mLL1AQeNsE", "replyto": "mLL1AQeNsE", "signatures": ["ICLR.cc/2026/Conference/Submission22659/Reviewer_QZam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22659/Reviewer_QZam"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152450008, "cdate": 1762152450008, "tmdate": 1762942323665, "mdate": 1762942323665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}