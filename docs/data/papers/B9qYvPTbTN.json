{"id": "B9qYvPTbTN", "number": 21633, "cdate": 1758319886958, "mdate": 1763717863817, "content": {"title": "Matching multiple experts: on the exploitability of multi-agent imitation learning", "abstract": "Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations in multi-agent interactive domains. Despite existing guarantees on the performance of the extracted policy, characterizations of its distance to a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and present challenges associated with the practical case of approximation errors. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium, assuming BC error $\\epsilon_{\\text{BC}}$. Specifically, for the case of dominant strategy expert equilibria, this provides a Nash imitation gap of $\\mathcal{O}\\left(n\\epsilon_{\\text{BC}}/(1-\\gamma)^2\\right)$ for a discount factor $\\gamma$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.", "tldr": "Study of exploitability of multi-agent imitation learning and efficient bounds on the Nash imitation gap.", "keywords": ["imitation learning", "multi-agent systems", "behavioral cloning", "Nash imitation gap"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4b662fb6cc318469f53b6ef697144940fc2252c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies imitation learning in Markov games. It first demonstrates that several previously used metrics, such as BC error or measure matching error, fail to provide theoretical bounds on the performance of learned equilibria. The paper then establishes that obtaining exact lower bounds for the Nash gap is PPAD-hard. Finally, the paper proposes a new notion of best-response continuity and provides theoretical guarantees on the Nash gap under the new condition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Overall the paper is well-written, the question studied in this paper is clearly stated and most proofs are easy to follow.\n- The paper provides constructive examples where BC error and measure matching error fail to capture the performance of learned equilibria.\n- Although the paper is not mathematically heavy, I believe the result carries some significance to the game theory community."}, "weaknesses": {"value": "- One issue with the definition of the tight Nash gap lower bound is that $\\mathcal{M}_{\\epsilon}(\\pi^{E})$ is not a convex set due to the equality constraint. Given this non-convex nature, it is not surprising that computing this bound leads to some hardness results (maybe even NP-hardness). While it is understandable to impose such equality constraint, the authors could consider more natural way to capture this gap.\n\n- To obtain a meaningful result from Lemma 4, one would expect $\\delta(\\epsilon)$ to be a polynomial function of $\\epsilon$. However, this essentially implies that when $\\epsilon = 0$, $\\pi_i^{E}$ is the unique best response policy given $\\pi_{-i}^{E}$. This notion appears quite restrictive as it is well-known that multiple best-response strategies may exist against a fixed policy. \n\n- No numerical experiments are provided."}, "questions": {"value": "- I find it hard to believe that Theorem 3 holds for any $\\epsilon_{\\rho} > 0.$ It is well-known that computing $\\epsilon$-Nash for some constant $\\epsilon$ is easy (see [1] for example). What is the connection between $\\epsilon_{\\rho}$ and the approximation error $\\epsilon$ in your reduction?\n\n----\n[1] Constantinos Daskalakis, Aranyak Mehta, and Christos H. Papadimitriou. A note on approximate nash equilibria. Theor. Comput. Sci., 410(17):1581–1588, 2009."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6pZ4HzaOSJ", "forum": "B9qYvPTbTN", "replyto": "B9qYvPTbTN", "signatures": ["ICLR.cc/2026/Conference/Submission21633/Reviewer_znH1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21633/Reviewer_znH1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874601268, "cdate": 1761874601268, "tmdate": 1762941862634, "mdate": 1762941862634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This theoretical paper studies multi-agent imitation learning with the goal of recovering a Nash equilibrium from demonstrations generated by an expert equilibrium. Exploitability is defined as the distance of the learned policy to a Nash equilibrium, and the authors derive both consistent bounds, which vanish with imitation error, and tractable bounds, which can be efficiently computed. They show that even perfectly matching expert occupancy measures can yield exploitable policies, proving the impossibility of deriving consistent Nash equilibria or tight tractable exploitability lower bounds in general Markov Games. To address this, they introduce the notion of best-response continuity, which allows constructing tractable upper bounds and provides conditions under which consistent and tractable Nash gap bounds can be achieved."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Excellent exposition of the concepts, related work, and background. It is difficult to do based on the sheer quantity of concepts that the work introduces and uses, which makes it all the more impressive and appreciated. Sec. 3 is well balanced with just enough depth to grasp the problem.\n* The proof outline of Th. 3 is appreciated and very useful.\n* The paper presents its results with an extremely high level of polish."}, "weaknesses": {"value": "* The finding that deriving exploitability upper bounds ultimately depnds on characterizing delta (and on the conditions that make delta consistent and tractable) is an interesting and valuable contribution. Still, the paper could better connect this result to more practical considerations. It would help to illustrate how specific techniques or settings might influence/shape delta in applied contexts. The brief mentions of “promoting exploration” and “penalizing risk aversion” (last lines of Sec. 6) move in that direction by linking delta to concrete mechanisms, but expanding on these examples could make the results feel more actionable. In particular, clarifying how delta might be operationalized or controlled through standard control or learning methods (practical “knobs”) would strengthen the paper, since the central result of the paper depends entirely on delta behaving well enough."}, "questions": {"value": "* Having tractable bounds is nice, so why not craft a scenario where the last bound involving delta is tractable and show empirically how tight or simply how informative it is, in a simple toy application?\n* I believe the paper provides important and sound theoretical contributions, and that the exposition is such that even non-seasoned researchers can learn valuable concepts from it. That being said, the ultimate bounds derived in the work provide limited intuitive value. Adding practical “knobs” that give insights as to how to control such bounds could go a long way in that direction.\n\nStyle, typos, suggestions:\n* [minor] L085-086: \"MA-ILR\" -> \"MA-IL\" or rather \"MA-IRL\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hv6bI9Rkck", "forum": "B9qYvPTbTN", "replyto": "B9qYvPTbTN", "signatures": ["ICLR.cc/2026/Conference/Submission21633/Reviewer_itJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21633/Reviewer_itJa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916322322, "cdate": 1761916322322, "tmdate": 1762941862382, "mdate": 1762941862382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General answer to all reviewers about numerical experiments"}, "comment": {"value": "Thank you all for your valuable reviews.\n\nMultiple comments were made about numerical experiments, and we now added numerical validation in **Appendix E** of a new revision of the paper.\n\nSpecifically, for a small toy environment described there, **Figure 5** demonstrates the validity of our bound and **Figure 6** shows the impact of entropy regularization on the tightest $\\delta$ estimate one can compute from the data.\n\nAs outlined in the paper, this has both practical and theoretical implications, especially for entropy regularization which is highly used in practice."}}, "id": "ovnK3r4QFt", "forum": "B9qYvPTbTN", "replyto": "B9qYvPTbTN", "signatures": ["ICLR.cc/2026/Conference/Submission21633/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21633/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21633/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718356653, "cdate": 1763718356653, "tmdate": 1763718356653, "mdate": 1763718356653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the theoretical limits of learning Nash-equilibrium-like policies from expert demonstrations in multi-agent imitation learning (MA-IL), especially in offline settings.  The authors first prove impossibility results, showing that even exact occupancy measure matching may lead to highly exploitable policies in general Markov games.  They further establish the PPAD-hardness of computing tight lower bounds on exploitability, linking the challenge to classical game-theoretic complexity.  Finally, the paper introduces a novel concept of best-response δ-continuity, allowing derivation of tractable and consistent upper bounds on the Nash gap—particularly under dominant strategy equilibria (DSE). The results unify several prior observations and provide theoretical guidance for designing robust MA-IL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a strong theoretical contribution, providing clear impossibility and hardness results that formalize the limits of multi-agent imitation learning (MA-IL).  \n- The proposed δ-continuity framework is a novel and interesting concept that bridges equilibrium stability and policy learning theory.  \n- The PPAD-hardness argument is rigorous and well-motivated.  \n- The proofs are detailed and supported by illustrative examples (though I did not check all the details).  \n- The work clarifies when common imitation learning methods (such as BC and GAIL) can or cannot recover equilibria, which is of both theoretical and practi"}, "weaknesses": {"value": "- My major concern is that, although the paper is theoretically strong, it lacks empirical validation to demonstrate how the theoretical insights (e.g., δ-continuity effects and dominance structures) manifest in practical game settings.  \n- The theory assumes access to known imitation errors (e.g., Equation 2), but it does not discuss how these quantities can be estimated or bounded in real applications.  \n- The assumptions of dominant strategy equilibria (DSE) or δ-continuity, while mathematically elegant, are quite restrictive and may not hold in general-sum or more complex multi-agen"}, "questions": {"value": "Please address the concerns in Weakness, especially that related to empirical validation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "- There are no major ethical concerns associated with this work. The paper is purely theoretical and does not involve human subjects, sensitive data, or potentially harmful application"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gNe1y9YF3I", "forum": "B9qYvPTbTN", "replyto": "B9qYvPTbTN", "signatures": ["ICLR.cc/2026/Conference/Submission21633/Reviewer_ngLP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21633/Reviewer_ngLP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938843712, "cdate": 1761938843712, "tmdate": 1762941862047, "mdate": 1762941862047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}