{"id": "lUdkSdCJYy", "number": 22078, "cdate": 1758325692563, "mdate": 1759896887723, "content": {"title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models", "abstract": "While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).", "tldr": "An efficient and effective algorithm to improve reasoning post-training for diffusion language models via reinforcement learning", "keywords": ["diffusion language models", "discrete diffusion", "reinforcement learning with verifiable rewards", "probabilistic generative models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23a5bdfde05cbe9c8d5d0b75399e5ec41f609d6d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces d2, a new policy optimization framework designed to enhance the reasoning capabilities of masked diffusion language models. The core contribution is a novel GRPO-style policy gradient algorithm that relies on two new estimators for the computationally intractable trajectory likelihood. The first, d2-StepMerge, is a practical, biased estimator that evaluates the likelihood at $N$ discrete steps, providing a theoretically-analyzed trade-off between compute and accuracy that works on existing models like LLaDA. The second, d2-AnyOrder, is a highly efficient, unbiased, single-pass estimator, but it requires a specific \"any-order causal\" property which the paper formally defines and shows is absent in standard DLMs. Empirically, d2 (using the StepMerge estimator) sets a new state-of-the-art for DLMs on several reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The most significant finding is that the proposed d2 framework achieves state-of-the-art results on multiple reasoning benchmarks (e.g., a massive jump to 91.9% on Sudoku) using reinforcement learning without any supervised fine-tuning, demonstrating a more scalable path for imbuing DLMs with reasoning. The authors originally derive a GRPO-style policy gradient for DLMs and introduce two novel likelihood estimators, d2-StepMerge and d2-AnyOrder, to solve the intractable likelihood problem. The paper features a rigorous theoretical derivation (Theorem 3.1), a formal bound on the d2-StepMerge approximation error (Theorem 4.1), and thorough, FLOPs-based empirical validation. Furthermore, the paper has intuitive diagrams (Figure 2) that makes the content more accessible."}, "weaknesses": {"value": "1. The inclusion of the d2-anyorder estimator and its related \"any-order causality\" theory detracts from the paper's core contribution. The primary, SOTA-achieving results are based entirely on d2-stepmerge applied to LLaDA. The d2-anyorder section uses a different model, task, and baseline, which fragments the narrative, confuses the central message, and reduces the paper's overall focus and clarity.\n\n2. The analysis of the d2-stepmerge compute-bias tradeoff is incomplete. While Figure 7 explores various values of $N$, it critically omits the $N=T$ (full trajectory, no-merge) baseline. This data point is essential to represent the \"zero-bias\" anchor. Without it, it is difficult to fully quantify the \"bias\" being traded off or to know if the chosen $N=16$ has fully converged to the best possible performance achievable by the underlying policy gradient.\n\n3. The RL objective is correctly defined over the final sequence likelihood $x_0$ (Eq. 3) (which is intractable for sure), as this is what the reward function $r(x_0)$ evaluates. However, the final policy update (Eq. 5) is expressed in terms of the likelihood of the full diffusion trajectory $x_{0:T}$. The main text fails to adequately explain why optimizing the trajectory likelihood is a valid or equivalent substitute for optimizing the final sequence likelihood, making the theoretical leap from the stated objective to the final algorithm feel abrupt and unmotivated.\n\n4. The motivation for using an off-policy algorithm like GRPO is fundamentally flawed. The key benefit of GRPO/PPO is rollout data reuse, which relies on cheap likelihood calculations when calculating importance ratio. In this DLM setting, calculating the importance ratio requires $N$ expensive forward passes, negating this advantage. This makes the off-policy updates computationally expensive, and it may even be less efficient than a simple fully on-policy algorithm. The paper fails to justify this choice or provide a comparison against an on-policy baseline."}, "questions": {"value": "1. There appears to be a typo in Eq. 6, which defines the d2-StepMerge objective. The time indices for the diffusion states are written as $nL/N$ and $(n+1)L/N$, using the sequence length $L$. However, the text describes partitioning the $T$ diffusion time steps into $N$ segments. Should these indices not be $nT/N$ and $(n+1)T/N$ respectively?\n\n2. I would like to know more detailed explanation of how $D_N$ is computed in Figure 3. Eq. 7 defines $D_N$ as the KL divergence between the full T-step trajectory distribution and the N-step (StepMerge) approximate distribution. Since computing the full KL divergence is computationally expensive—requiring expectations over all possible tokens—it would be helpful to clarify how the KL divergence is actually estimated in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper does not appear to raise any ethical concerns. It does not involve human subjects, sensitive data, or potentially harmful applications."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jsuTVlsEym", "forum": "lUdkSdCJYy", "replyto": "lUdkSdCJYy", "signatures": ["ICLR.cc/2026/Conference/Submission22078/Reviewer_QLxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22078/Reviewer_QLxB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428677985, "cdate": 1761428677985, "tmdate": 1762942056336, "mdate": 1762942056336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "d2 provides an RL method to reinforce the whole denoising trajectory in MDLMs. The authors identify the computational challenge of estimating trajectory likelihoods in non-autoregressive models as a key bottleneck for applying RL. To address this, they propose two distinct policy gradient estimators: d2-StepMerge, which trades off computation for accuracy by merging timesteps, and d2-AnyOrder, which enables an unbiased, one-shot likelihood estimate for models satisfying a specific \"any-order causality\" property. The authors conduct experiments to validate the effectiveness of d2 on LLaDA and Eso-LM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I agree that the whole generation trajectory in MDLMs should be fully taken into consideration in RL algorithms, and StepMerge provides a practical way for ease training overload.\n2. d2-AnyOrder is an ideal way for any-order generation, which can make use of KV-Cache, and need only one pass for probability estimation.\n3. The paper provides extensive experiments across multiple reasoning tasks Sudoku, Countdown, GSM8K, MATH. The ablation studies on the number of segments $N$ are particularly insightful to demonstrate the proper value range for $N$"}, "weaknesses": {"value": "1. StepMerge merges adjacent steps into a segment so that the number of model passes are reduced. However, why don't we just sample the trajectory with fewer steps, and just reinforce such shorter trajectories? I think that is necessary for comparison.\n2. d2-anyorder is ideal. However, it only applies to models that already have such generation ability. It cannot be applied to popular models such as LLaDA, Dream.\n3. There are too few comparisons, such as other MDLM-related RL-methods [1] or more up-to-date MDLMs [2].\n\n[1] Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models\n\n[2] Dream 7B: Diffusion Large Language Models"}, "questions": {"value": "Here are some experiment details I would like to know:\n1. I realize that the computation graph of a whole trajectory could be too heavy to be preserved in GPU. Could you provide more details on LLaDA experiments? Such as: the generation length, the number of steps for results in Tab.2.\n2. d1 and wd1 are actually trained with LoRA. However, the experiment part does not mention the training settings. Are results in Tab. 2 trained with LoRA or full parameters? Are they compared fairly in the same setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FcCKVqPdTr", "forum": "lUdkSdCJYy", "replyto": "lUdkSdCJYy", "signatures": ["ICLR.cc/2026/Conference/Submission22078/Reviewer_M9go"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22078/Reviewer_M9go"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934691441, "cdate": 1761934691441, "tmdate": 1762942056014, "mdate": 1762942056014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a clean theoretical framework for applying RL to diffusion language models. The key contribution is d2-StepMerge, which approximates trajectory log probabilities by evaluating likelihood at N merged denoising steps instead of all T timesteps, offering a  trade-off between computational cost and approximation accuracy. The paper showing that approximation error decreases as N increases. Additionally, the paper introduces d2-AnyOrder, which enables one-shot trajectory likelihood evaluation for any-order causal diffusion models, which is novel. The method achieves significant performance improvements over existing baselines (d1, wd1) across reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Applying RL to any-order causal diffusion LLMs is novel and represents a solid contribution, as prior work has only applied RL to fully-bidirectional diffusion models like LLaDA (d1, wd1) or block-based diffusion approaches like TraceRL. And it shows potential of any-order dLLMs as it can get log prob with one forward pass without approximation with designed attention mask.\n2. The d2-StepMerge method is well-motivated and demonstrates strong empirical performance, outperforming previous sota methods across all four reasoning benchmarks.\n3. The paper provides a clean, theoretically grounded formalization of RL for diffusion LLMs, deriving the objective from first principles. \n4. The thorough analysis of the computation-performance trade-off is valuable, as d2 methods necessarily require more computation per update than baselines like d1 (N=1). The paper transparently shows that this additional cost is justified by superior performance and provides guidance for selecting N in practice."}, "weaknesses": {"value": "My main concern is that the paper mainly concerns P(response | prompt, denoising_order) for a specific, human-selected denoising order, treating this conditional likelihood as the training objective. However, the true quantity we care about is P(response | prompt), which should marginalize over all possible denoising orders. Since this marginalization is intractable, the paper effectively assumes that optimizing for one fixed order generalizes to the overall response quality. \n\nThis might introduces a potential approximation error: when a response is bad, we ideally want the model to avoid generating it under any denoising order, but the current approach only discourages it under the specific order used during training. Different denoising orders could lead to the same final response through different intermediate paths, and the model might learn order-specific artifacts rather than genuinely improving response quality. The paper can be improved if the authors can discuss if this N can be seen as balancing between optimizing for all orders vs the exact denoising order and which one is more beneficial for performance."}, "questions": {"value": "1. Could you elaborate more on the differences between concurrent work. TraceRL and your work? As traceRL also merges timesteps for trajectory likelihood evaluation. \n2. Regarding Algo1's implementation: would it be more computationally efficient to compute and collect likelihood estimates for all N segments first, then perform a single backward pass over the accumulated gradients? Or is computing one segment, immediately backpropagating, then moving to the next—more efficient? The could have memory and optimization implications.\n3. When you compute and collect trajectory likelihoods during training after generation, do you store and reuse the exact denoising order from the generation phase, or do you randomly sample a new denoising order for each gradient update?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nixaeAZmY6", "forum": "lUdkSdCJYy", "replyto": "lUdkSdCJYy", "signatures": ["ICLR.cc/2026/Conference/Submission22078/Reviewer_fryY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22078/Reviewer_fryY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060924922, "cdate": 1762060924922, "tmdate": 1762942055782, "mdate": 1762942055782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on post-training/RLHF on diffusion language models (dllm), and more specifically aimed at improving reasoning performance. The main challenge is the lack of efficient likelihood computation for dllms. To address this the authors propose two likelihood estimators: d2-StepMerge, which trades computational cost for bias by evaluating on a subset of time-steps, and d2-AnyOrder, which allows estimation given an “any-order causal” architecture. The paper compares performance on logical and mathematical reasoning benchmarks for DLMs, using 4 different benchmarks; and show improved results than previous models under equal FLOP budgets (for d2-StepMerge). They also analyze performance for d2-AnyOrder on text-modeling task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper directly tackles a challenging problem: applying PGRL methods to DLMs. The characterization of the \"any-order causality\" property is a good conceptual framing for architecture design. The paper is generally well-structured and generally readable. \nThe d2-stepMerge is a generalization of the d1-method."}, "weaknesses": {"value": "The improved results largely stem from the performance on sudoku. On other tasks the improvement is small. Is there any specific reason for this. Additionally while the accuracy-flop comparison is on sudoku ( primarily because the difference is bigger there), but what about other tasks.\n\nThe paper mentions diffucoder's attempt to reduce vairance in likelihood estimation. They similarly also consider wd1 which also is focused on a similar problem. But they do not present results on these tasks from diffucoder's coupled loss. Since that seems computationally less expensive, that should be considered as a baseline. There are other works which also address the same problem [1,2] which should be discussed (and compared against if feasible)\n\nWhy was d2-anyorder not tried on the reasoning tasks. Additionally, the paper suggests that MDLM losses should be sufficient for any-order but then says current models are not. I do not understand why? Additionally presuming it is true, there needs to be greater and seperate analysis of the 'anyorder' estimate. Finally, a reasonable ablation would be to do fine-tuning with d2-anyorder on llada/d1 to compare performance. Given the title being \"d2\" but using two different estimators on two completely different models/tasks makes it seem that the second estimator was shoe-horned in.\n\nd2-stepmerge is a pretty natural and obvious extension of d1/diffu-GRPO. The theoretical analysis is also pretty straighforward and not novel. The proofs around d2-anyorder are also not precise enough (though i do believe the results are correct).\n\n1 Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo\n2 PADRE: Pseudo-Likelihood based Alignment of Diffusion Language Models"}, "questions": {"value": "I have raised some questions in weaknesses earlier. Some additional questions/suggestions:\n\nCan the authors provide the empirical relation between the number of StepMerge partitions (N) and the bias in likelihood estimation. \nAdditionally more detailed quantitative comparison on non-sudoku tasks would be good\n\nWhat specific changes are required to instill anyorder property in a model like LLaDA? Is it feasible to retrofit this property into a pre-trained model, or does it require training a new model from scratch?\n\nShouldn't the comparison with DDPO (for d2-anyorder) be one which involved the KL penalty. It seems odd to remove KL penalty from your post-training algorithm for comparison than the other way around?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dUZOzVhgGF", "forum": "lUdkSdCJYy", "replyto": "lUdkSdCJYy", "signatures": ["ICLR.cc/2026/Conference/Submission22078/Reviewer_bkFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22078/Reviewer_bkFN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569082924, "cdate": 1762569082924, "tmdate": 1762942055616, "mdate": 1762942055616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}