{"id": "WAaj0BnDP7", "number": 7822, "cdate": 1758037538686, "mdate": 1759897829845, "content": {"title": "MVG-CRPS: A Robust Loss Function for Multivariate Probabilistic Forecasting", "abstract": "Multivariate probabilistic forecasting typically leverages neural network-based distributional regression, often employing Gaussian assumptions to simplify computation. While the standard negative log-likelihood provides analytical convenience, its sensitivity to outliers can severely degrade forecasting accuracy. Conversely, robust alternatives like the Energy Score, although less sensitive to extreme values, rely heavily on computationally expensive sampling approximations, limiting scalability in neural network training. To bridge this gap, we introduce the MVG-CRPS, a novel, strictly proper scoring rule for multivariate Gaussian distributions that maintains robustness to outliers while providing a closed-form expression, enabling efficient training and evaluation. Our approach leverages a whitening transformation, decorrelating multivariate outputs and reducing the multivariate scoring task to tractable univariate CRPS computations. Experiments on real-world datasets for both multivariate autoregressive and univariate sequence-to-sequence (Seq2Seq) forecasting tasks demonstrate that MVG-CRPS enhances robustness and predictive performance.", "tldr": "We introduce a robust loss function for multivariate probabilistic forecasting that enhances the robustness of deep learning models and outperforms traditional scoring rules.", "keywords": ["probabilistic forecasting", "scoring rules", "robust optimization"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f43102afed8179e569db808e39e85c492634dc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a scoring rule called MVG-CRPS. The central assumption of the paper is Gaussianity of the underlying multivariate distribution. Under the Gaussianity assumption, and extending the results of Gneiting et al 2005, this papers shows that MCG-CRPS admits an analytic formula and is strictly proper. Experimental results show that MVG-CRPS has comparable performance with existing los-score and energy score, while significantly reducing training time compared to energy score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow\n- MVG-CRPS targets to address important, practical and well-known problems in probabilistic forecasting: log-score is sensitive to outliers, while energy score has long training time because of Monte Carlo sampling\n- The experiments are extensive in three directions: datasets, baselines and evaluation metrics\n- Extensive experiments on three common baselines show that MVG-CRPS has comparable performance with log-score and energy score\n- Three evaluation metrics, CRPS sum, CRPS mean, and energy score, are used for the experiments. The proposed MVG-CRPS can even outperform models training on energy score when energy score itself is used as an evaluation metric"}, "weaknesses": {"value": "- The main limitation is the assumption of Gaussianity in the target distribution (for the MVG-CRPS to be strictly proper), which may not hold in practice\n- The core idea is to decorrelate the Gaussian distribution so that the CRPS can be computed per dimension, but the contribution seems somewhat limited in novelty"}, "questions": {"value": "- It is claimed in the key contributions that \"MVG-CRPS allows for the analytical computation of derivatives\". In (8), the CRPS involves the CDF of a Gaussian distribution, which generally has no closed form expression. In this case, how can the derivatives be analytically computed?\n- For the training time reported in Table 2, how many samples are drawn to approximate the energy score?\n- Some existing multivariate probabilistic forecasting studies that are related and use the energy score as the training objective could be included in the literature review. E.g., [1-2]\n\n[1] Kan, K., Aubet, F. X., Januschowski, T., Park, Y., Benidis, K., Ruthotto, L., & Gasthaus, J. (2022, May). Multivariate quantile function forecaster. In International Conference on Artificial Intelligence and Statistics (pp. 10603-10621). PMLR.\n\n[2] Olivares, K. G., Négiar, G., Ma, R., Meetei, O. N., Cao, M., & Mahoney, M. W. $\\clubsuit $ CLOVER $\\clubsuit $: Probabilistic Forecasting with Coherent Learning Objective Reparameterization. Transactions on Machine Learning Research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KqylrynMBt", "forum": "WAaj0BnDP7", "replyto": "WAaj0BnDP7", "signatures": ["ICLR.cc/2026/Conference/Submission7822/Reviewer_moW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7822/Reviewer_moW8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761023860814, "cdate": 1761023860814, "tmdate": 1762919867077, "mdate": 1762919867077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is my second review of the MVG-CRPS paper.\nThe authors propose training multivariate Gaussian forecasts using an analytical approximation to the energy score, rather than the negative log-likelihood.\n\nHowever, their claims of robustness are not substantiated. Gneiting’s energy score still contains quadratic terms similar to those in the Gaussian NLL, and no theoretical or empirical validation of robustness is provided.\n\nSimilarly, the claim that Monte Carlo approximations to the energy score are computationally inefficient lacks analysis or evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a reasonable analytical approximation to the energy score under Gaussian assumptions.\n\n2. The paper includes a theorem establishing strict propriety for MVG-CRPS.\n\n3. The benchmark datasets are appropriate, and the authors move beyond overused long-horizon datasets. However, PCA-based approaches may not scale well to longer horizons (see Table A1, max horizon = 60)."}, "weaknesses": {"value": "1. The literature review is narrow. The authors should reference prior work on energy score computation and scaling in forecasting (e.g., CLOVER: Probabilistic Forecasting with Coherent Learning Objective Reparameterization).\n\n2. The claim that Monte Carlo approximations are inefficient is not justified. CLOVER demonstrates sample-efficient Monte Carlo approximations without loss of accuracy (see Appendix H).\n\n3. The energy score is distribution-agnostic, but MVG-CRPS applies only to Gaussian settings—limiting generality.\n\n4. PCA eigen-decomposition scales cubically, making the method impractical for high-dimensional or long-horizon forecasting. An ablation showing runtime vs. horizon length would clarify applicability and limitations.\n\n5. Claims of robustness and efficiency are unsubstantiated both theoretically and empirically. If not validated, such claims should be omitted.\n\n6. No computational timing results are provided to support claims of efficiency."}, "questions": {"value": "1. PCA scales cubically—how can MVG-CRPS be considered efficient?\n\n2. PCA is highly sensitive to outliers; how can MVG-CRPS be described as robust?\n\n3. Gneiting’s energy score includes quadratic terms—why is it treated as robust?\n\n4. On what empirical basis do the authors claim that Monte Carlo approximations to the energy score are inefficient? Please refer to CLOVER Appendix H for counterexamples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KPHiCJlJVb", "forum": "WAaj0BnDP7", "replyto": "WAaj0BnDP7", "signatures": ["ICLR.cc/2026/Conference/Submission7822/Reviewer_TYm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7822/Reviewer_TYm2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764300845, "cdate": 1761764300845, "tmdate": 1762919866776, "mdate": 1762919866776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of probabilistic time series forecasting using parametric models. Specifically, they challenge the common choice of negative log-likelihood as training objective, mainly because of the sensitivity of the latter to outliers (due to the quadratic error terms in Gaussian likelihoods). Alternatives such as the Energy Score are, as argued by the authors, expensive due to Monte Carlo sampling during training. The authors then propose MVG-CRPS, a new loss function that is a weighted sum of the univariate CRPS scores after a whitening transformation. The empirical results show that the proposed loss is equally sensitive to deviations in the Gaussian distribution parameters, and improves over the baselines in terms of probabilistic forecasting metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well-motivated, and as far as I know fill a gap in the literature.\n- The build-up to the proposed loss function is well elaborated and naturally leads to the authors main contribution.\n- As can be seen in the experimental results, the suggested loss function helps mitigating the issues faced by the log-score baseline, all while being computationally more efficient than ES. This aligns well with the initial motivation of the paper."}, "weaknesses": {"value": "- A first concern I have with the approach is the Gaussian assumption on the forecasted time series. Indeed, the multivariate decomposition in the proposed loss function is only true for multivariate gaussian distributions, which is natural given the modelling choice of a multivariate gaussian parameteric model. However, I didn't find any discussion in the paper on the implications of this assumption in scenarios where it doesn't really hold. This can be achieved through simulated non-gaussian toy data or through an analysis of the real world datasets considered in the paper. And just to be clear, I understand that this assumption is also true for the baselines, however given that it's central to your proposed loss, I think more analysis and discussion about it can improve the paper.\n- My second concern is the fact that you only reported probabilistic forecasting metrics (CRPS and ES), which is expected given the nature of the compared algorithms. However, I would expect a comparison also in terms of mean forecasts (MSE or MAE) in order to be more explicit about the tradeoffs involved in comparing the different methods.\n- Related to the previous concern, calibration is a topic that is mentioned a couple of times, however I dont think there is any analysis or comparison in terms of calibration. Correct me if I'm wrong but the reported metrics (CRPS and ES) incorporate both sharpness and calibration information, and it would be interesting to see how do the different losses compare in terms of calibration-only metrics such as ECE or the KS statistic."}, "questions": {"value": "- Can the authors explain how is the whitening transformation different from normalizing the data (target in this case, and since it's time series maybe a joint normalization of the input and the output together). In my understanding this means that the samples $z_t$ will be of unit variance and zero mean so throughout training the model's parametric distribution will tends toward that.\n- Regrading the differentiability of the proposed loss, it's mentioned that the gradients flow through the transformed samples $w_t$ , however the scaling factors $\\sqrt{\\lambda_{i,t}}$ also depend on the model output. What is the reasoning behind that and would it make sense to let the gradients flow also through $\\sqrt{\\lambda_{i,t}}$ (given that torch.linalg.svd is also differentiable with respect to the singular values) ?\n- The proposed loss has a multi-task learning formulation in my opinion, where the different tasks are scaled with a factor depending on the variance of each component. Do you think this can be problematic in cases where some dimensions have large variances so they dominate the others? Maybe looking at the univariate forecasting metrics can reveal this kind of phenomena.\n- Finally, the $\\beta$ parameter of the ES loss is set to $1$ in all the experiments I believe. What is the impact of this hyperparameter and would it improve the performance of the ES-trained models should it by optimized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "41vQvhl7Fj", "forum": "WAaj0BnDP7", "replyto": "WAaj0BnDP7", "signatures": ["ICLR.cc/2026/Conference/Submission7822/Reviewer_4zHP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7822/Reviewer_4zHP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931388744, "cdate": 1761931388744, "tmdate": 1762919866394, "mdate": 1762919866394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}