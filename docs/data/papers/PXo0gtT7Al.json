{"id": "PXo0gtT7Al", "number": 9034, "cdate": 1758108091969, "mdate": 1759897747148, "content": {"title": "Attention, Please! Revisiting Attentive Probing Through the Lens of Efficiency", "abstract": "As fine-tuning becomes increasingly impractical at scale, probing is emerging as the preferred evaluation protocol. Yet, the standard linear probing fails to adequately reflect the potential of models whose pre-training optimizes representations of patch tokens rather than an explicit global representation. This motivates the need for attentive probing, an alternative that uses attention to selectively aggregate patch-level features. Despite its growing adoption, attentive probing remains underexplored, with existing methods suffering from excessive parameterization and poor computational efficiency. In this work, we revisit attentive probing through the lens of the accuracy vs. parameter efficiency trade-off. We present the first comprehensive study of existing methods, analyzing their design choices and benchmarking their performance. Building on this, we propose efficient probing (EP), a simple yet effective multi-query cross-attention mechanism that eliminates redundant projections and reduces the number of trainable parameters. Despite its simplicity, EP outperforms linear probing and prior attentive probing approaches across seven benchmarks, generalizes well to diverse pre-training paradigms, and delivers strong low-shot and layer-wise gains. Beyond evaluation, our analysis uncovers emerging properties of EP, such as complementary attention maps, which open new directions for leveraging probing beyond protocol design.", "tldr": "Linear probing overlooks the potential of models with distributed patch-token representations. We propose efficient probing (EP), a lightweight attention-based method that is faster, more parameter-efficient, and outperforms prior probing approaches.", "keywords": ["Linear Probing", "Attentive Probing", "Vision Transformers", "Vision-Language Models", "Evaluation", "Neural Networks"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7f4038e54545d8cca7ff9e7f600515d231a6902.pdf", "supplementary_material": "/attachment/f0ffed12367166fb6d815db96718ee6958e3dc07.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitations of standard linear probing (LP) for evaluating pre-trained visual representations, particularly for models like those based on Masked Image Modeling (MIM) that distribute information across patch tokens rather than relying on a single global representation (like a [CLS] token). The authors present a systematic study of attentive probing (AP), a protocol that uses attention to selectively aggregate these patch features. They introduce a novel, parameter-efficient AP method called Efficient Probing (EP), which uses a simple multi-query cross-attention mechanism without redundant projections. EP demonstrates state-of-the-art accuracy compared to LP and prior AP methods across various benchmarks and pre-training paradigms while being significantly more efficient in terms of parameters and computational cost. Furthermore, the analysis reveals that EP's high performance correlates with the localization quality of its attention maps, which are shown to be highly diverse and complementary."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper correctly identifies and addresses the misalignment between standard LP and modern pre-training paradigms (MIM, auto-regressive, diffusion) where discriminative information is distributed across patch tokens. Attentive probing is established as the necessary alternative.\n\n2.  This is presented as the \"first comprehensive study\" of attentive probing, offering a unified framework that categorizes existing methods (including those from unrelated tasks) and thoroughly benchmarks their performance against parameter and computational efficiency.\n\n3. The proposed Efficient Probing (EP) method is simple, yet highly effective. By reformulating the attention mechanism as a transformation-free multi-query cross-attention, it significantly improves the accuracy vs. parameter efficiency trade-off, consistently outperforming baselines and competitors.\n\n4. The results show that EP provides consistent and substantial gains across diverse pre-training methods (MIM, JEA, Hybrid, VLM, Generative), with the largest gains seen in models whose pre-training optimizes patch tokens (e.g., DiT, SimMIM)."}, "weaknesses": {"value": "1. The paper positions its contribution against a backdrop where attentive probing is described as \"underexplored\" and existing methods \"suffer from excessive parameterization and poor computational efficiency.\" While EP solves these issues, the baseline comparison set may be inherently weak due to the newness of the protocol, potentially overstating the relative accuracy gain compared to a hypothetical future efficient method.\n\n2. The motivation for removing the key transformation ($W_{Kj}$) in EP is primarily efficiency. While the poor performance of a partially-shared-subspace design (Equation 8) is mentioned, the core reasoning for why learnable queries $u_j \\in \\mathbb{R}^{D_i}$ directly interacting with the full feature space $X$ (Equation 10) works better than projected queries $q_j \\in \\mathbb{R}^{d_a}$ interacting with projected keys $K_j$ (Equation 7) could be elaborated with a more formal argument about representation capacity or gradient flow beyond just empirical verification. The section on the impact of $W_K$ and $W_V$ in MHCA is incomplete in the provided text.\n\n3. While the probing overhead (GFLOPs) is much smaller than the backbone, the overall evaluation still requires training for 90 epochs. The abstract/introduction states that full fine-tuning (FT) is \"unsustainable and prohibitive at scale,\" but the computational cost of an attention-based protocol, even an efficient one, is still much higher than standard LP. This is mitigated but not entirely eliminated, especially when comparing against \"frozen and pre-stored features\" (mentioned in Figure 3 caption) which is a crucial future direction not implemented here.\n\n4. The analysis is incomplete because it omits a comparison with crucial modern, low-cost alternatives that are widely used for large-scale model assessment, such as Zero-Shot classification for Vision-Language Models (VLMs) and PEFT techniques like LoRA. These methods represent highly relevant alternatives to EP's parameter efficiency and performance trade-off in the modern evaluation landscape."}, "questions": {"value": "1. Can EP be adapted for non-classification tasks, and what modifications would be necessary for such applications?\n\n2. How does EP scale when applied to very large backbone models or extremely large datasets, and are there cases where efficiency or accuracy gains diminish?\n\n3. Considering that modern large models often use a vision backbone as a component in multimodal architectures, does the probing result truly reflect the backbone’s capacity, and is probing genuinely necessary in this context?\n\n4. If probing is indeed necessary, would it be possible to supplement the analysis with comparisons to zero-shot and LoRA performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EbSF5IFjp2", "forum": "PXo0gtT7Al", "replyto": "PXo0gtT7Al", "signatures": ["ICLR.cc/2026/Conference/Submission9034/Reviewer_Vb6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9034/Reviewer_Vb6E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557443226, "cdate": 1761557443226, "tmdate": 1762920753923, "mdate": 1762920753923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes a hard look at how we evaluate giant, pre-trained vision models. The core idea is that full fine-tuning is just too expensive , but the standard alternative, linear probing (LP), doesn't do justice to models (like MIM) that spread information across patch tokens instead of a single [CLS] token. The paper champions \"attentive probing\"—using an attention head to smartly aggregate patch features—as the right middle ground."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The biggest strength, in my view, is the systematic benchmark. The field of attentive probing has been a bit all over the place, with different papers (AIM, V-JEPA, etc.) all proposing their own one-off solutions .\n2. EP is a nice piece of engineering. It's not flashy, but it's simple, well-motivated (why have redundant projections?), and it just plain works. It consistently lands on the Pareto frontier for both parameter count and GFLOPs, which is exactly what you want from something called \"efficient.\"\n3. The experimental setup is comprehensive. The authors didn't just test on one model; they covered the gamut of pre-training paradigms (MIM, JEA, VLMs, generative) across a ton of datasets. The extra analyses, like the low-shot probing (Table 3) and layer-wise probing (Fig. 10), really strengthen the case that EP is a robust tool."}, "weaknesses": {"value": "1. While EP is effective, its novelty is a bit thin. The core idea is essentially an ablation study on Multi-Head Cross-Attention (MHCA),\n2. The paper frames this entire problem as \"probing for evaluation.\" But what they're doing—freezing the backbone and training a few extra parameters—is exactly what Parameter-Efficient Fine-Tuning (PEFT) is. The authors even acknowledge EP fits into the PEFT family in Appendix A.2. So, why is there no comparison to mainstream PEFT methods like LoRA, Adapters, or even simpler baselines like BitFit (tuning biases) or just tuning the LayerNorms?\n3. The analysis showing that EP's queries are \"complementary\" is interesting. But the paper never really closes the loop. Is this complementarity the reason EP performs well, or just a neat side effect? Do other methods (like AIM, which Figure 6 shows is also pretty complementary) also benefit? It's presented as a key finding but feels disconnected from the main efficiency argument."}, "questions": {"value": "1. The main question is about the PEFT comparisons. Could you provide data on how EP stacks up against training a LoRA module (with a frozen backbone) using an equivalent number of trainable parameters?\n2. Regarding the complementarity analysis: do you have any experiments to suggest this is a causal driver of performance? For example, what happens if you explicitly add a loss term to force the queries in AIM or V-JEPA to be more complementary? Would their performance increase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B9GZPQNoi8", "forum": "PXo0gtT7Al", "replyto": "PXo0gtT7Al", "signatures": ["ICLR.cc/2026/Conference/Submission9034/Reviewer_Ph7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9034/Reviewer_Ph7P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829515300, "cdate": 1761829515300, "tmdate": 1762920753405, "mdate": 1762920753405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of efficiently evaluating large-scale, pre-trained vision models. The authors argue that as \"fine-tuning becomes increasingly impractical at scale\", probing is emerging as the preferred evaluation protocol. However, standard linear probing (LP) \"fails to adequately reflect the potential of models\" like Masked Image Models (MIM) or autoregressive models, where information is distributed across patch tokens rather than a single global [CLS] token. This motivates \"attentive probing\", which uses attention to aggregate patch-level features. The paper notes that existing attentive probing methods suffer from \"excessive parameterization and poor computational efficiency\". The aothors present the \"first comprehensive study of existing methods\"  for attentive probing, benchmarking their accuracy and efficiency. They also propose \"efficient probing (EP)\" , a \"simple yet effective multi-query cross-attention mechanism\" that \"eliminates redundant projections\"  to reduce parameter count and computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method, EP, is a \"simple multi-query cross-attention mechanism\" that is derived by \"eliminat[ing] redundant projections\"  from standard MHCA. This simplification is well-illustrated in Figure 1.\n\n2. This work provides the \"first comprehensive study\"  of attentive probing methods. The evaluation is extensive, covering \"diverse pre-training paradigms\" (MIM, JEA, VLMs, generative) and multiple datasets.\n\n3. EP consistently achieves a state-of-the-art \"accuracy-parameter trade-off\" , as shown in Figure 2. It achieves \"state-of-the-art top-1 accuracy of 75.6% with less than 1.4M parameters\" on MAE ViT-B, outperforming more complex methods. The gains are largest for the target model classes, such as \"SimMIM +13.6%\" and \"DiT +24.3%\" over LP."}, "weaknesses": {"value": "1. **Placement of Key Ablations**: The justification for EP's specific design is partially buried in Section 4.3. For instance, EP's design is \"transformation-free\", notably lacking the $W_K$ projection. The empirical justification for this comes from an ablation showing that while removing $W_K$ from multi-head AIM hurts performance ($75.1\\% \\rightarrow 72.9\\%$), EP's design performs well without it. This key comparison, which validates EP's specific design choice, should be more central to the method's motivation in Section 3.\n\n2. **Confusing Terminology**: The method is named \"Transformation-free Multi-Query Cross-Attention\" , and the text states \"there are no projection matrices\". However, the value projection $W_V$ is still a critical component of the pooling operation (Equation 2) and the EP method. An ablation in 4.3 confirms that \"removing $W_V$ from $EP_{12}$ degrades performance from $75.1\\% \\rightarrow 72.1\\%$\". The \"transformation-free\" label applies only to the key/query interaction, not the value projection, which could be made clearer.\n\n3. **Matryoshka Probing**: The Matryoshka analysis (Appendix C.3, Table 5) is very compelling. It shows that standard EP, when evaluated at smaller dimensions, \"collapses\" (e.g., 5.5% at $D/8$ ). In contrast, \"Efficient Matryoshka\" resolves this and provides robust \"multi-scale probing without extra parameters\". Given this, should \"Efficient Matryoshka EP\" be the recommended protocol for the community, rather than the standard EP described in the main paper?"}, "questions": {"value": "1. **Method Justification (EP vs. AIM w/o $W_K$)**: The ablation in 4.3 shows that removing $W_K$ from multi-head AIM causes a significant performance drop ($75.1→72.9$). Your proposed EP also lacks a $W_K$ yet achieves high performance (75.6%). Does this imply that EP's design (using full-dimensional $D_i$ queries $u_j$) successfully compensates for the lack of $W_K$, whereas simply dropping $W_K$ from a standard MHCA (like AIM) fails? A direct comparison of (AIM - $W_K$) vs. EP ($EP_{12}$) would clarify if EP's \"transformation-free\" design is truly superior or just more efficient.\n\n2. **Code Implementation**:\n    * **Paper (Table 1)**: The \"QUERY SOURCE\" for V-JEPA is listed as $q_j = W_{Q_j} u$, with $u \\in \\mathbb{R}^{D_i}$. This categorizes it with SimPool, where $u$ is *data-dependent* (derived from the input $X$, e.g., Global Average Pooling). This is explicitly contrasted with methods like AIM or EP, which are listed as having a learnable query $q$.\n    * **Code (`poolings/jepa/attentive_pooler.py`)**: The `AttentivePooler` class (used for \"jepa\" in `main_linprobe.py`) implements its query as a *data-independent, learnable parameter*: `self.query_tokens = nn.Parameter(torch.zeros(1, num_queries, embed_dim))`. The paper's experimental performance for V-JEPA is based on the *code's* learnable-query architecture, but the paper's *analysis* (Table 1) attributes this performance to a *data-dependent* query architecture. Will this invalidates the paper's own framework for comparing V-JEPA against other methods like AIM or EP?\n\n    * **Paper (Table 1 & Sec 3.3)**: The \"ATTENTION\" mechanism for AbMILP is explicitly defined as dot-product attention: $a = \\sigma_m(K^{\\top} q)$, where $K=X$ and $q$ is a single learnable query vector. Section 3.3 further claims this method \"requires only $D_i$ parameters\" (the size of $q$).\n    * **Code (`poolings/abmilp.py`)**: The `ABMILPHead` class seemingly does *not* implement dot-product attention. Instead, it uses a multi-layer MLP (`self.attention_predictor` is `nn.Sequential`) to *predict* attention weights directly from the input features: $a = \\text{softmax}(MLP(X))$. This is an MLP-based attention mechanism, not a dot-product one. The number of parameters in a two-layer MLP (such as `abmilp_depth=2`) is on the order of $D_i \\times D_{hidden} + D_{hidden} \\times 1$, which is far more than $D_i$ parameters (in `main_linprobe.py`, `abmilp_depth` defaults to 2). See details below:\n\n  * Original Code (Supplementary Material `poolings/abmilp.py`)\n```python\n# This is the MLP-based implementation from the supplementary material.\n# Note 'self.attention_predictor' is an nn.Sequential (MLP).\n# This implementation's parameters are O(D^2), not O(D).\nclass ABMILPHead(nn.Module):\n    def __init__(\n            self,\n            dim: int,\n            activation: str= \"tanh\",\n            depth: int = 2, # Default depth=2\n            **kwargs\n        ):\n        super().__init__()\n        self.ATTENTION_BRANCHES = 1\n        attn_pred_layers = []\n        for i in range(depth-1):\n            attn_pred_layers.extend([\n                nn.Linear(dim, dim),\n                (nn.Tanh() if activation == \"tanh\" else nn.ReLU()),\n            ])\n        # The MLP: e.g., Linear(D, D) -> Tanh -> Linear(D, 1)\n        attn_pred_layers.append(nn.Linear(dim, self.ATTENTION_BRANCHES))\n        self.attention_predictor = nn.Sequential(*attn_pred_layers)\n        # ... (other initializations) ...\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # ... (self-attn logic) ...\n        predictor_input = x # (Simplified for clarity)\n        \n        # a = softmax(MLP(X))\n        attn_map = self.attention_predictor(predictor_input) # [B, N, 1]\n        attn_map = F.softmax(attn_map, dim=1)\n\n        # y = V * a (where V=x)\n        out = (x * attn_map).sum(dim=1)\n        return out\n```\n   *  Based on Paper's Description, My Understanding: \n\n```python\n# This implementation strictly follows the paper's description\n# (Table 1 & Sec 3.3): a = softmax(X^T q).\n# It uses dot-product attention with a single learnable query 'q'.\n# This implementation has O(D) parameters.\nclass ABMILPHead(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        \n        # The learnable query vector 'q' (D_i parameters)\n        self.query = nn.Parameter(torch.zeros(1, dim, 1))\n        nn.init.xavier_uniform_(self.query)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x (K and V) shape: [B, N, D]\n        \n        # 1. K^T q (using batch matmul)\n        # (B, N, D) @ (B, D, 1) -> (B, N, 1)\n        attn_logits = x @ self.query\n        \n        # 2. a = softmax(K^T q)\n        attn_map = F.softmax(attn_logits, dim=1) # [B, N, 1]\n\n        # 3. y = V a (where V=x)\n        # (B, N, D) * (B, N, 1) -> sum(dim=1) -> (B, D)\n        out = (x * attn_map).sum(dim=1)\n        return out\n```"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nTKNF1VqnH", "forum": "PXo0gtT7Al", "replyto": "PXo0gtT7Al", "signatures": ["ICLR.cc/2026/Conference/Submission9034/Reviewer_4JLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9034/Reviewer_4JLQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900996666, "cdate": 1761900996666, "tmdate": 1762920753058, "mdate": 1762920753058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}