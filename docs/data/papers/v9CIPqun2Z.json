{"id": "v9CIPqun2Z", "number": 24029, "cdate": 1758351965320, "mdate": 1759896785437, "content": {"title": "DiffKANformer: Diffusion KAN Transformer for General Time Series Analysis", "abstract": "Time series analysis tasks such as forecasting, imputation, anomaly detection, and classification are crucial for applications spanning climate science, financial domain, retail, and cloud infrastructure. We present DiffKANformer, a conditional diffusion model that integrates Kolmogorov-Arnold Networks (KAN) for feature projection and a Diffusion KAN Transformer architecture for denoising, specifically engineered for time series analysis. DiffKANformer introduces two key innovations: (i) a KAN-based projection mechanism in the forward diffusion process that captures complex correlation between features, and (ii) a Diffusion KAN Transformer architecture that effectively models complex long-term dependencies through adaptive univariate functions. Our model achieves superior performance across four fundamental time series analysis tasks, significantly outperforming existing prominent models in forecasting (eight datasets), imputation (six datasets), classification (ten datasets) and anomaly detection (five datasets). Comprehensive ablation studies across all tasks validate the utility of each DiffKANformer component, demonstrating the model's robustness in diverse time series challenges.", "tldr": "", "keywords": ["Diffusion models", "Time Series analysis", "Kolmogorov-Arnold Networks"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5200af2cad78198decc69fc3ddf1b87ab9b7e5ef.pdf", "supplementary_material": "/attachment/ced927898582fc4e01cd7158163b6e591f235a04.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DiffKANformer, a unified framework for time series analysis that combines conditional diffusion models, Kolmogorov-Arnold Networks (KANs) for feature projection in the diffusion process, and a novel Transformer backbone where the MLP blocks are replaced by KANs. DiffKANformer aims to address limitations of prior diffusion-based and Transformer-based models. Namely, it tries to address the limited expressivity in modeling complex non-linear/periodic dependencies and lack of generality across time series tasks by incorporating KANs to enhance both feature projection and denoising.\n\nThe method is positioned as the first to demonstrate state-of-the-art results across forecasting, imputation, classification, and anomaly detection benchmarks. The claim is supported by extensive experiments, ablations, and mathematical derivations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts a unified model that can handle forecasting, imputation, classification, and anomaly detection. This is a unified solution across tasks. \n\nThe model is grounded in a principled, variational framework, with mathematical rigorousness.\n\nIntegrating KANs for both data projection and as a replacement for Transformer MLPs is a creative architectural step, and, according to the ablation tables,  provides clear empirical benefits over standard MLPs in both main and auxiliary tasks."}, "weaknesses": {"value": "The paper is promising and technically interesting, but I have concerns about positioning, clarity, and a few technical presentation issues.\n\nWhile the paper shows superior performance on a broad suite of benchmarks, not all of the most recent diffusion-based transformers and unified models are included as baselines (e.g. TS-Diffusion, DifFormer etc.) The current baseline selection is robust but misses direct one-to-one evaluations with the latest approaches. \n\nThe paper introduces non-Markovian elements in its trainable forward process. However, while the losses are derived, there are insufficient examples or discussions of how the non-Markovian structure concretely manifests in the actual sampling/generation of time series compared to traditional DDPMs. How does training/inference complexity scale, and are there pathological behaviors in terms of stability or convergence as $T$ increases?\n\nThe KL divergence expressions are presented, but it’s not obvious how estimators are computed in practice, especially when the KAN projection is “learned” during the forward process on each mini-batch. Are there subtleties in gradient computation/backpropagation not addressed by standard PyTorch/TF autodiff pipelines?\n\n**The novelty is moderate**. The KAN‑parameterized forward diffusion is a novel and appealing idea; KAN‑DiT is an **incremental** (yet useful) architectural tweak."}, "questions": {"value": "Can the authors clarify how DiffKANformer differs in principle and practice from recently proposed unified diffusion models (e.g., TS-Diffusion)?\n\nAre there technical or empirical distinctions that uniquely favor KAN-based projections and denoisers? \n\nGiven that the forward process is non-Markovian due to KAN projections, what are the implications for sampling complexity, memory efficiency, and convergence/stability in longer time series or higher $T$ values?\n\nAre there any implementation challenges or numerical instabilities when backpropagating through the KAN-parameterized forward process, compared to standard DiT/MLP architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nm21oNffjS", "forum": "v9CIPqun2Z", "replyto": "v9CIPqun2Z", "signatures": ["ICLR.cc/2026/Conference/Submission24029/Reviewer_ZxQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24029/Reviewer_ZxQ2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840185562, "cdate": 1761840185562, "tmdate": 1762942905441, "mdate": 1762942905441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffKANformer, a unified diffusion-based framework that integrates Kolmogorov-Arnold Networks (KAN) into Transformer-based diffusion models for general time series analysis. The model introduces a KAN projection in the forward diffusion process to capture complex nonlinear dependencies among time series variables and replaces the standard MLP in the Transformer block with KAN network as a Diffusion KAN Transformer. The approach is evaluated across multiple tasks; forecasting, imputation, classification, and anomaly detection. It demonstrates consistent improvements over state-of-the-art baselines across 29 benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified framework for diverse tasks : The model provides a single, cohesive architecture that can handle forecasting, imputation, classification and anomaly detection. This unified approach enhances the model's applicability and practical impact.\n\n2. Clear motivation for KAN integration: The motivation to replace ReLU-based MLPs with spline-based KANs is well-articulated. The authors convincingly argue that KANs capture richer nonlinear relationships and better represent high-frequency temporal components.\n\n3. Comprehensive experimental evaluation: The paper conducts large-scale experiments on a broad set of datasets and includes systematic ablations (e.g. with/without KAN projection), which demonstrate robustness and generality of the method."}, "weaknesses": {"value": "1. Although Appendix D provides detailed derivations for the forward posterior, loss and variational objective (Eqs. 14~15) but do not provide the proof that the KAN projection yields a tighter variational bound or improved diffusion stability. The forward KL term and prior distribution are motivated heuristically; no convergence or bound analysis is offered.\n\n2. The architectural modifications substitute the transformer's MLP with a KAN block and incorporate adaLN for conditional scaling. This is a well-executed engineering refinement, but the conceptual advance is incremental rather than groundbreaking. \n\n3. The paper reports runtime and memory overheads but does not analyze asymptotic scaling with respect to sequence length L or diffusion steps T. Large scale deployment implications remain unclear.\n\n4. There is no quantitative interpretation of the learned spline bases or frequency-domain analysis of the KAN functions. Without such analysis, the mechanism of improvement is still opaque.\n\n5. Since some results of extensive baselines are cited from prior works rather than retrained under identical conditions, the comparison fairness remains slightly uneven."}, "questions": {"value": "1. Appendix D defines $q_\\phi(x_t|x,c)$ and $q_\\phi(x_{t-1}|x_t,x,c)$. Are the KAN parameters $\\phi$ shared across t or re-estimated per t? If shared, how does this affect flexibility; if not, how is stability maintained?\n\n2. Section 3.1-3.4 apply the same diffusion backbone to forecasting, imputation and classification with different conditional masks. Are the conditioning strategies implemented within the same noise schedule or adjusted per task?\n\n3. Appendix K measures runtime empirically, but for more formal confirmation, can you provide theoretical computational complexity in T and L?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d3J78nE7DN", "forum": "v9CIPqun2Z", "replyto": "v9CIPqun2Z", "signatures": ["ICLR.cc/2026/Conference/Submission24029/Reviewer_Rj1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24029/Reviewer_Rj1K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997213824, "cdate": 1761997213824, "tmdate": 1762942904939, "mdate": 1762942904939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DiffKANformer, a conditional diffusion model integrating KAN for feature projection and a Diffusion KAN Transformer architecture for denoising in time series analysis. The method introduces a learnable KAN-based forward process and replaces MLP blocks in the denoiser with KAN layers. Extensive experiments across four major tasks (forecasting, imputation, classification, anomaly detection) and 29 datasets show consistent improvements over strong diffusion-based baselines. Theoretical formulation is clear and self-consistent, though mainly architectural rather than conceptual."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.Clear motivation and complete methodological formulation.\n\n2.Comprehensive experiments across multiple time series tasks and datasets.\n\n3.Empirically demonstrates consistent improvements and stability.\n\n4.Writing quality and reproducibility are both high."}, "weaknesses": {"value": "1. The “theory” part (Sections 2.1–2.2) mainly restates the diffusion formulation with a KAN projection. There is no derivation or discussion showing why this design improves optimization or likelihood. The method is clearly written but not theoretically grounded.\n\n2. Runtime results are briefly reported in Appendix Tables 14–15, but no corresponding FLOPs or parameter counts are given, and the discussion is not integrated into the main text.\nAs a result, the computational trade-offs of KAN remain unclear.\n\n3. KAN layers reduce parameters but add spline computations, which likely increase runtime — yet this trade-off is not analyzed.\n\n4. Recent diffusion foundations (e.g., TimeEdit 2024, Latent DiT 2024) are not compared.\n\n5. The “unified framework” is mostly architectural; each task head is still trained separately.\n\nOverall, the work feels more suitable for research exploration than industrial use, since scalability and efficiency remain unverified."}, "questions": {"value": "1.Clarify theory in Sections 2.1–2.2.\nExplain how the KAN projection affects the diffusion loss or variance schedule,\nand show that the formulation recovers DDPM when KAN = Id and c = 0.\nA short appendix derivation would make the theoretical part more convincing.\n\n2.Add computational cost analysis.\nInclude a small table comparing DiffKANformer, DiT, and CnDiff in terms of parameters, FLOPs, and inference time.\nThis would clarify whether KAN’s higher runtime cost is justified by the accuracy gains.\n\n3. Provide one targeted ablation.\nFor example, fix or freeze the KAN projection (or replace it with a linear map) to confirm that the gain truly comes from the learnable forward process.\nThis single ablation would strongly support the main claim.\n\n4. (Optional) Briefly discuss possible runtime optimizations, such as precomputing spline bases or kernel fusion, to make KAN-based diffusion models more practical."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u8i3muNwh2", "forum": "v9CIPqun2Z", "replyto": "v9CIPqun2Z", "signatures": ["ICLR.cc/2026/Conference/Submission24029/Reviewer_ZYvN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24029/Reviewer_ZYvN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997376572, "cdate": 1761997376572, "tmdate": 1762942904040, "mdate": 1762942904040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a diffusion-based time series framework that learns the forward (prediction) process via KAN-projection (combining the KANϕ(x, t) term and a learnable pre-component) and replaces the DiT MLP with a KAN block (DiT-KAN) under adaLN conditions.\nThis approach is evaluated across four tasks (prediction, interpolation, classification, anomaly detection) and multiple datasets, consistently reporting superior performance or SOTA-level results.\nKey contributions include learnable forward diffusion tailored to temporal structure, architectural transition to KAN within DiT, and robustness demonstrated through extensive multi-task evidence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Combining learnable forward diffusion (KAN-projection) with the DiT-KAN backbone for time series.\n- Extensive experiments across multiple datasets and ablation studies (e.g., KAN vs. MLP) highlight architectural advantages, particularly in classification/AD.\n- High-level motivations and design choices are well-explained, and the intuitiveness of KAN-projection is supported by correlation analysis."}, "weaknesses": {"value": "- For classification/AD, the combination method (weighting, schedule) of $L_{rec}$, $L_{diff}$, and class loss is unclear. An explicit formula, $λ$ value, and selection criteria should be provided.\n- It appears only random point masking was used. Since block omissions, channel drops, MNAR, and irregular sampling are common in practice, it would be better to include these or discuss limitations.\n- A comparison of parameters/FLOPs/memory/latency between DiT-KAN and DiT-MLP is needed, and discussing long sequence scalability and alternatives would be beneficial."}, "questions": {"value": "- Were all baselines tuned with the same look-back grid, optimizer schedule, and early-stopping rules? If not, quantify the discrepancy and its impact.\n- Any observed training instabilities from the learnable forward process? What mitigations helped?\n- Sensitivity to spline order/knots and their interaction with diffusion steps T? Are gains robust across ranges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T7VDFjMMtL", "forum": "v9CIPqun2Z", "replyto": "v9CIPqun2Z", "signatures": ["ICLR.cc/2026/Conference/Submission24029/Reviewer_n39M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24029/Reviewer_n39M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007587922, "cdate": 1762007587922, "tmdate": 1762942903682, "mdate": 1762942903682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DiffKANformer is proposed as a conditional diffusion model for general time series analysis, integrating Kolmogorov-Arnold Networks (KANs) into both the forward diffusion process and the denoising architecture. The method introduces a KAN-based projection to capture complex feature correlations and replaces MLPs in the Diffusion Transformer (DiT) with KANs to better model long-term temporal dependencies through adaptive univariate functions. The authors evaluate the model across four core tasks—forecasting (8 datasets), imputation (6 datasets), classification (10 UEA datasets), and anomaly detection (5 benchmarks)—reporting state-of-the-art performance. Ablation studies are provided to justify the utility of KAN projection and the Diffusion KAN Transformer block, and the framework claims to be the first diffusion-based model to unify and excel in all four tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents extensive empirical validation across a remarkably wide range of time series tasks and datasets, which is rare and commendable in diffusion-based time series literature.\n\n2. Replacing MLPs with KANs in the DiT architecture is a conceptually interesting direction, especially given recent theoretical arguments about KANs’ superior function approximation for structured data."}, "weaknesses": {"value": "1. The theoretical justification for the proposed KAN-based forward process is shallow; the derivation in Appendix D assumes a non-Markovian forward process but fails to rigorously analyze how this affects the tightness of the ELBO or the stability of training—critical omissions for a method claiming to “reduce the gap between true NLL and its variational approximation.”\n\n2. The claimed “first unified diffusion model for all four tasks” is misleading; CSDI, TimeGrad, and CnDiff already handle multiple tasks (e.g., forecasting and imputation), and the classification/anomaly detection setups are trivial adaptations using off-the-shelf reconstruction or representation heads, which is not architectural innovations.\n\n3. The ablation in Table 6 conflates the effect of KAN projection with the Diffusion KAN Transformer; no experiment isolates KAN projection alone with a standard DiT backbone, making the contribution attribution ambiguous.\n\n4. Implementation details reveal that condition networks differ per task (dense layer vs. transformer), yet this architectural inconsistency is never discussed nor controlled in ablation, raising concerns that performance gains stem from task-specific design rather than the core DiffKANformer framework.\n\n5. Despite claiming superior efficiency due to fewer parameters (0.5M), Table 14 shows DiffKANformer has significantly higher training time than CnDiff and mr-Diff on ETTh1, contradicting the efficiency narrative and suggesting immature KAN implementation, not architectural advantage.\n\n6. The forward process introduces a learnable condition c and KAN projection, but c is never defined operationally, whether it’s a learned embedding, input statistic, or task-specific encoding remains ambiguous, rendering reproducibility questionable."}, "questions": {"value": "Please address the concerns raised in Weaknesses 1–6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aoPlid8ogo", "forum": "v9CIPqun2Z", "replyto": "v9CIPqun2Z", "signatures": ["ICLR.cc/2026/Conference/Submission24029/Reviewer_L2Qb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24029/Reviewer_L2Qb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176144209, "cdate": 1762176144209, "tmdate": 1762942903292, "mdate": 1762942903292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}