{"id": "GUI3fUOijm", "number": 13499, "cdate": 1758218647051, "mdate": 1759897432765, "content": {"title": "Generalization Aware Minimization", "abstract": "Sharpness-Aware Minimization (SAM) optimizers have improved neural network generalization relative to stochastic gradient descent (SGD). The goal of SAM is to steer model parameters away from sharp regions of the training loss landscape, which are believed to generalize poorly. However, the underlying mechanisms of SAM including whether its bias toward flatter regions is why it improves generalization are not fully understood. In this work, we introduce Generalization-Aware Minimization (GAM), derived by directly applying the goal of guiding model parameters toward regions of the landscape that generalize better. We do so by showing mathematically through a Bayesian derivation that the landscape of expected true (test) loss is a rescaled version of the observed training loss landscape, and that a sequence of perturbative updates in place of SAM's single perturbative update can optimize the expected test loss. We present a practical online algorithm to implement GAM's perturbative steps during training. Finally, we empirically demonstrate that GAM has superior performance over SAM, improving generalization performance on a range of benchmarks. We believe that GAM provides valuable insights into how sharpness-based algorithms improve generalization, is a superior optimizer for generalization, and may inspire the development of still-better optimizers.", "tldr": "We design a generalized version of sharpness aware minimization that directly optimizes the expected test loss landscape, enhancing generalization.", "keywords": ["generalization", "sharpness aware minimization", "loss landscape", "optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e69c30601ca9838be65167f26b974627f0576ade.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper developed a new optimizer, GAM, that directly optimizes the expected test loss. This is   different from SAM, which minimizes the so-called flatness instead. The authors argue that the test loss landscape is a rescaled version of the training loss landscape, and GAM is able to approximate the corresponding gradient after a sequence of perturbative updates. The effectiveness of GAM is finally evaluated on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper is easy to follow and the flow of the proposed algorithm is very clear. The idea of transforming the gradient of training loss to the gradient of test loss is really impressive, which I believe deserves further study."}, "weaknesses": {"value": "While the proposed algorithm is quite impressive, there remains several issues that needs to addressed:\n\n - My biggest concern is about the quadratic assumption. In most cases, as far as I can tell, this assumption does not hold during the training process. I suggest the authors could provide more experimental validations.\n - In Theorem 2, the authors showed that the transformation of one gradient to another. Can the authors provide more intepretation on why requiring $f^\\prime(0)=1$? Moreover, one can notice from Figure 4 that the transformation seems to be not as expected. This could be more severe for high-dimensional distributions, particularly for millions of parameters. What the authors have done to alleviate this issue?\n - In Algorithm 1, how line 13, the gradient with respect to $\\gamma$ is implemented?\n - More experimental results should be included. In Table 1, the authors only reported the results on several simple architectures. And the accuracy on ImageNet-1k is too low (practically, it should be at least 70\\% even for SGD). And the legend of Figure 2 should be better indicated such as including the value of $\\gamma_1$ and using different styles. In brief, more experiments including baselines, computation overhead, hyper-parameter sensitivy should be also be presented."}, "questions": {"value": "please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kl69avqXx8", "forum": "GUI3fUOijm", "replyto": "GUI3fUOijm", "signatures": ["ICLR.cc/2026/Conference/Submission13499/Reviewer_NWzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13499/Reviewer_NWzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760878336484, "cdate": 1760878336484, "tmdate": 1762924115459, "mdate": 1762924115459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the unclear mechanism by which the SAM optimizer improves generalization. It proposes a novel optimizer, GAM, which directly optimizes the expected test loss. The authors prove that the true expected test loss is a rescaled version of the training loss. Building on this, GAM employs a sequence of perturbed updates to directly optimize the expected test loss. Furthermore, the authors design a practical online algorithm that outperforms SAM on several benchmarks. This work demonstrates strong theoretical rigor, innovative methodology, and robust empirical support, with significant heuristic value."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a solid theoretical foundation for understanding the generalization capability of the SAM optimizer.\n- Reveals that a sequence of parameter perturbations can transform the gradient of the training loss into that of the test loss, bridging the gap between optimizing training loss and directly optimizing expected test loss.\n- Well-structured and easy to read."}, "weaknesses": {"value": "Although the proposed method is theoretically sound, it incurs substantial computational overhead, limiting its practical applicability."}, "questions": {"value": "- In equation 11, should the second instance of  $D^t$ be $D^{t-1}$?\n- It is unclear how line 8 in Algorithm 1 is derived from equation 11; further derivation details are needed.\n- The experimental results in Table 1 lack performance data for CRSAM on the ImageNet dataset; an explanation is required.\n- The process for generating Figure 2 is not sufficiently detailed; more methodological description is recommended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KCfF5IAX19", "forum": "GUI3fUOijm", "replyto": "GUI3fUOijm", "signatures": ["ICLR.cc/2026/Conference/Submission13499/Reviewer_3dqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13499/Reviewer_3dqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393607930, "cdate": 1761393607930, "tmdate": 1762924115187, "mdate": 1762924115187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at better generalization of neural network by directly optimizing on the testing loss landscape. This is done by first theoretically showing the testing loss landscape is a rescaled version of the training loss landscape. Then the authors show such difference can be covered by perturbative steps, arriving at GAM algorithms that using multiple online-adjusted perturbative steps. Experiments show GAM surpasses SGD and SAM. It also implies SAM, as single-step GAM, can be seen as implicitly optimizing directly on the testing loss landscape."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper features theoretical analysis that reveals the connection and difference between the testing and training loss landscape.\n- Such connection is used to develop algorithm that optimizes directly on testing loss landscape, which is well motivated.\n- Experiments demonstrate that GAM surpasses SGD and SAM with statistical significance.\n- SAM is connected to GAM, revealing its implicit role of simulating testing loss landscape. This connection provides a new perspective on understanding SAM."}, "weaknesses": {"value": "- Gap between theoretical results and aim / practical use: Theorem 1 lay down the theoretical foundation for this paper. However, its conditions are a bit difficult to comprehend at first glance, especially with quadratic approximations involved. For example, the randomness of quadratic loss parameters ($\\\\tilde{\\\\theta}, \\tilde{M}, \\dots$) is abstracted / comes from which part of the whole training? What does the whole training process as a random process, where the loss parameters are involved, look like. Therefore, I suspect that such result cannot be directly applied to the training process. For example (please correct me if I embed Thm 1 into the training process in a incorrect way),  the training process is modeled by the following random process:\n  \n  $$\n    (\\bar{X}, \\bar{Y}) \\overset{\\text{training}}{\\rightarrow} \\theta \\underset{\\text{training loss}}{\\overset{\\text{approximating}}{\\rightarrow}} (\\tilde{\\theta}^*, \\tilde{M}, \\tilde{c})\n  $$\n\n  and at the same time\n\n  $$\n    \\theta \\underset{\\text{testing loss}}{\\overset{\\text{approximating}}{\\rightarrow}} (\\theta^*, M, c).\n  $$\n\n  Noting that $(\\\\tilde{\\\\theta}^\\*, \\\\tilde{M})$ and $(\\\\theta^\\*, M)$ have a confounder $\\theta$, they are correlated and one cannot have $\\\\theta^\\*, \\\\tilde{\\theta}^\\* \\\\perp  M, \\\\tilde{M}, c, \\\\tilde{c}$, which is an assumption of Theorem 1. Intuitively, it says the training may bias the parameter to empirical-loss flat minima, where the testing-loss flatness may differ a lot, just like overfitting. I believe it is indeed the case especially when the parameter is explicitly or implicitly (eg, SGD is well-known to have flatness bias) optimized toward flat minima.\n- Extra sample use: In Line 5, Algorithm 1, new data is directly sampled from the data distribution to measure how much $\\\\bar{g}\\_{\\theta}$ deviates from $g_{\\theta}$. I read Appendix E and found no indication about how this data is sampled (so I assumed it is some new data. Correct me if I was wrong) and how much of them is used. If this data is indeed newly sampled, I suspect severely unfair comparison in experiments where SAMs do not use extra data. If it is reused training data, then this data is already used in training and is (over)fitted. Can it estimate the testing data without bias? If it comes from training data partition, then will it hurt data efficiency? \n- Such direct access to testing loss landscape also leaves Theorem 1 not fully exploited. Abstractly, Theorem 1 involves by revealing that the difference between the empirical and testing loss landscape has a rescaling-like structure and Theorem 2 says some perturbative can cover this gap, but how it is covered is by directly comparing the two landscapes instead of exploiting the explicit difference revealed by theoretical results.\n- Experiments: The experiments use networks whose architectures may be too old. Also, for deeper networks, GAM seems unable to surpass CRSAM. ImageNet results for CRSAM is also missing."}, "questions": {"value": "1. In the paper, Theorem 2 helps constructing the true testing loss gradients by moving the parameter toward a new place where the empirical gradients have similar gradients with testing gradients, i.e., simulating the gradient modification by moving where the gradient is computed. What is the motivation behind this simulation? Why not just do some calculation and moving the empirical gradients instead? I found \"surprising similarity between the update mechanisms of SAM and GAM\" in the Discussion. Is it related to unifying SAM with GAM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mWzfm4nzjn", "forum": "GUI3fUOijm", "replyto": "GUI3fUOijm", "signatures": ["ICLR.cc/2026/Conference/Submission13499/Reviewer_Zqnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13499/Reviewer_Zqnc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577853290, "cdate": 1761577853290, "tmdate": 1762924114917, "mdate": 1762924114917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper derives a relationship between the expected test loss and the training loss: they share eigenvectors but the eigenvalues differ, so that the Hessian is a rescaling. From this, they come up with their Generalization-Aware Minimization algorithm that estimates this. On CIFAR-10/100, SVHN and ImageNet, they find that at the same iteration, GAM outperforms SAM and"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I'm plausibly unaware of work in the past couple years that could've made this seem more obvious, but the rescaling result seems interesting on its own.\n\nI appreciated adding some intuition and explanation of lines in the algorithm, and the way the assumptions are laid out and justified in Â§C; note I have not checked the proofs in much detail.\n\nThe figures were also good: Figure 1 was very clear and Figure 2 was valuable for further connecting the theory and the outcomes."}, "weaknesses": {"value": "I'm not sure I would call 3x \"manageable\" overhead; at the very least, I expect to see compute-matched results somewhere, for those applications where that's more of a constraint than data (like pretraining foundation models). Saying \"Future work\ncould explore approximations or scalable implementations of higher-order derivatives\" felt weak to me, given all the work out there on approximating second-order derivatives already.\n\nThe settings feel a little unrealistic in 2025, maxing out at a ResNet-50 on ImageNet.\n\nThe gains also seem small for that overhead, e.g., 0.7% on ImageNet (when the results are in the 65% range, not near saturation)"}, "questions": {"value": "I was confused by Equation 11; where's the recursion? Shouldn't there be a t-1 like in line 8 of Algorithm 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYP50c57iX", "forum": "GUI3fUOijm", "replyto": "GUI3fUOijm", "signatures": ["ICLR.cc/2026/Conference/Submission13499/Reviewer_DkRZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13499/Reviewer_DkRZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983939335, "cdate": 1761983939335, "tmdate": 1762924114604, "mdate": 1762924114604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}