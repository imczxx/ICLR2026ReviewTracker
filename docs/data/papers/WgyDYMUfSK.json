{"id": "WgyDYMUfSK", "number": 11340, "cdate": 1758196786014, "mdate": 1759897581775, "content": {"title": "RA-SpaRC: Robust Adaptation with Sparse Plus Low-Rank Compressors", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are widely adopted for their efficiency. However, LoRA assumes model updates are inherently low-rank, which introduces a restrictive bias that results in underperformance compared to full fine-tuning. Hybrid approaches, such as Robust Adaptation (RoSA), improve expressiveness by combining low-rank and sparse components, but they rely on a manually tuned ratio to balance these components, leading to suboptimal parameter allocation across tasks. We introduce RA-SpaRC (Robust Adaptation with Sparse plus Low-Rank Compressors), a new initialization strategy that overcomes this limitation. The key idea is an adaptive allocation mechanism that automatically balances sparse and low-rank components within a given parameter budget. This approach removes the need for manual rank–sparsity tuning and supports arbitrary parameter budgets. This principled and automated design allows RA-SpaRC to consistently outperform LoRA, its variants, and RoSA in extensive experiments across multiple models, delivering more effective and flexible adaptation.", "tldr": "", "keywords": ["Parameter-Efficient Fine-Tuning", "Large Language Models", "Robust Adaptation", "Sparse Plus Low-Rank Decomposition"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37c6b8bf399583fdd7506e824cee310688cf0e51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces RA-SpaRC, a new initialization strategy for Parameter-Efficient Fine-Tuning (PEFT) that addresses the limitation of existing hybrid methods like RoSA requiring manual tuning of sparse and low-rank components. \n\nRA-SpaRC uses an adaptive allocation mechanism based on gradient analysis to automatically balance these components within a fixed parameter budget, outperforming LoRA, its variants, and RoSA across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem of developing an initialization strategy that enables flexible, automatic, and effective budget allocation for Robust Adaptation.\n\nTo this end, they propose RA-SpaRC (Robust Adaptation with Sparse plus Low-Rank Compressors), an initialization strategy for sparse plus low-rank fine-tuning, which could dynamically assign the ratio of low-rank and sparse parts according to the gradient information of different tasks and models.\n\nThe proposed method is very pragmatic and practical.\n\nThe proposed method was examined in comparison with different LoRA variations in several benchmarks."}, "weaknesses": {"value": "Notation should be revised and redundancy in some terms should be fixed.\n\nThe proposed method performs on par in some experiments compared to the state of the art LoRA variations.\n\nThe initialization and running time of the proposed method is pretty large compared to the baseline vanilla LoRA.\n\nTheoretical analyses of several properties of the proposed method, such as the convergence rate, were not provided."}, "questions": {"value": "Have you compared convergence rate of your proposed method and the other LoRA variations theoretically?\n\nThe accuracy boost is less for GSM8K compared to the HumanEval in Table 2. Could you please elaborate this result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GYrcUCHZrF", "forum": "WgyDYMUfSK", "replyto": "WgyDYMUfSK", "signatures": ["ICLR.cc/2026/Conference/Submission11340/Reviewer_Yfsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11340/Reviewer_Yfsr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486823182, "cdate": 1761486823182, "tmdate": 1762922474102, "mdate": 1762922474102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RA-SpaRC (Robust Adaptation with Sparse plus Low-Rank Compressors), a new hybrid PEFT initialization strategy that overcomes the need to manually tune the ratio of parameters between low-rank and sparse components, through an automatic parameter allocation mechanism. The paper shows that RA-SpaRC outperforms classic and hybrid PEFT methods in extensive experiments across multiple models.\n\nThe paper is a pleasure to read and the idea seems interesting and promising, offering new insights into the performance of different PEFT initialisation methods and PEFT methods themselves. Still, the empirical improvements seem limited, and it is not fully clear if the empirical evaluation is completely fair."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe paper is well presented and conveys the message well.\n-\tThe proposed method seems novel in its approach.\n-\tThe claims are supported by theory and experiments.\n-\tThe proposed method advances the PEFT research, introducing a new hybrid approach."}, "weaknesses": {"value": "-\tThe average improvement in Table 1 is marginal, especially when compared to LoRA-One.\n-\tResults in Table 2 are more significant, however comparisons may not be fair when considering classic PEFT methods (i.e. LoRA, LoRA-GA, LoRA-One), as for LLaMA-2 results are not reported for 4.746% parameters, but only for 0.297%, which marginally improves for GSM8K. In addition, Table 2 omits results for PISSA, which seems a relevant competitor.\n-\tIn Section 4.4 the paper acknowledges the initialization and training times as core limitations of the proposed method, where the overhead is respectively 3.75x to 5.33x (depending on the model) and 1.05x to 1.35x. The paper fails to show the trade-offs between performance gains and time overhead, which should be the overarching goal."}, "questions": {"value": "- how does PISSA compare?\n- is there a favourable time-accuracy trade-off for the proposed method?\n- why are the models (Sect. 4.2) fine-tuned only on a sample?\n- what is the accuracy for the experiments in Fig. 2/3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5e5Lzpbm03", "forum": "WgyDYMUfSK", "replyto": "WgyDYMUfSK", "signatures": ["ICLR.cc/2026/Conference/Submission11340/Reviewer_xqnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11340/Reviewer_xqnn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931500512, "cdate": 1761931500512, "tmdate": 1762922473766, "mdate": 1762922473766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RA-SpaRC, a novel initialization strategy for parameter-efficient fine-tuning that automatically balances sparse and low-rank components when adapting large pretrained models. Concretely, RA-SpaRC adopts a compressor-based framework and defines a compressor which optimimally decomposes gradient updates into sparse and low-rank components under a fixed computational budget. It further contributes an efficient alternating projection algorithm to automatically determine the best rank–sparsity trade-off as well as a compressor quality metric that guarantees loss reduction during optimization.\n\nExperiments on LLaMA-2-7B, Qwen2.5-7B, and T5-base models across NLU, NLG, and code reasoning tasks show consistent improvements over LoRA, LoRA-One, and RoSA, without extra memory and with moderate initialization cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method identifies the shortcomings of fixed-ratio sparse + low-rank hybrid PEFT methods and comes up with an elegant formulation to adaptively tune this.\n* The paper reformulates PEFT initialization as applying a compressor on gradients, unifying sparse and low-rank initialization under a unified lens.\n* The authors implement a custom kernel for avoiding the materialization of dense matrices, therefore achieving real speedup compared to naive unoptimized implementations."}, "weaknesses": {"value": "* The evaluation has the potential to be improved.\n    - First off, I would recommend that a more complete experimental setup is provided, which would enhance understanding and reproducibility.\n    - Breadth-wise, results are mainly focused on dense LLMs and text-based tasks. Applying the technique to other networks, e.g., ViTs, would help showcase the generality of the method.\n    - Depth-wise, the paper’s evaluation section mostly focuses on aggregate results (accuracy, efficiency) rather than offering deeper insights into why or how the adaptive rank–sparsity allocation interacts with different layers of large language models (LLMs). Coupling this with different budgets would be even better.\n* The paper focuses on better initialization, but adaptation dynamics during training (e.g., stability or convergence rate) are not deeply analyzed.\n* Though the initialization is efficient, the adaptive search may still be costly for very large models or frequent reinitialization (e.g. adapter soups)."}, "questions": {"value": "* How would the authors propose their solution be applied on non-dense, multi-branch models, like MoE or hybrid-attention structures?\n* How does the method perform under quantized pretrained backbones?\n* How does RA-SpaRC interact with alternative modern optimizers (e.g., Muon)? Is the initialization's benefit amplified or diminished?\n* How does the technique behave against DoRA?\n* Does adaptive allocation generalize across tasks, or must it be recomputed for every fine-tuning run?\n* Could the compressor be integrated dynamically during training rather than only at initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mV85yVH3sE", "forum": "WgyDYMUfSK", "replyto": "WgyDYMUfSK", "signatures": ["ICLR.cc/2026/Conference/Submission11340/Reviewer_YjV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11340/Reviewer_YjV5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955027217, "cdate": 1761955027217, "tmdate": 1762922473450, "mdate": 1762922473450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RA-SpaRC, which first computes the initial gradient on a small data batch and, under a fixed parameter budget, employs a binary search to explore different ranks. It allocates the remaining budget to sparsity, selects the rank $ r^* $ that minimizes reconstruction error, and constructs the sparse matrix ( S ) from the $ s^* $ largest-magnitude elements of the residual."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method can automatically allocate the ratio between low rank and sparsity which reduce the complexity of hyper-parameter fine-tuning.\n\n2. The paper shows many results on various benchmark with different models, which demonstrate it is better than LoRA and its variants."}, "weaknesses": {"value": "1. Since the paper determines the ratio between low-rank and sparse components based on a mini-batch, it should demonstrate whether this ratio remains stable or changes when the samples in the mini-batch vary.\n\n2. From Tables 1 and 2, the performance gap between RA-SpaRC and LoRA-One is minor, while RA-SpaRC incurs higher initialization time. This raises questions about its practical benefit. Moreover, Table 2 should include results for LoRA-One using the same number of parameters as RA-SpaRC when evaluated on LLaMA-2-7B.\n\n3. In Table 4, the performance difference between SpaRC and SVD is minimal. The authors should therefore provide experimental results for the proposed method without the sparse compressor to better isolate its contribution.\n\n4. Since RoSA is the main baseline for comparison, the paper should report results for RoSA under additional configurations to more clearly demonstrate the effectiveness of the proposed adaptive ratio allocation."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A04R94MMrt", "forum": "WgyDYMUfSK", "replyto": "WgyDYMUfSK", "signatures": ["ICLR.cc/2026/Conference/Submission11340/Reviewer_39B7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11340/Reviewer_39B7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979141880, "cdate": 1761979141880, "tmdate": 1762922473031, "mdate": 1762922473031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}