{"id": "68FgdHQvJH", "number": 8832, "cdate": 1758099555779, "mdate": 1759897761280, "content": {"title": "MUIQD: Benchmarking and Facilitating Multimodal LLMs for Underwater Image Quality Perception", "abstract": "Multimodal Large Language Models (MLLMs) have recently demonstrated great potential in cross-modal perception, reasoning and generation. However, their effectiveness in underwater image quality perception, a fundamental requirement for efficient underwater vision tasks, remains largely unexplored. To address this gap, in this paper we propose the first large-scale dataset for benchmarking and facilitating underwater image quality perception of MLLMs, termed MUIQD. MUIQD is composed of two complementary subsets, MUIQD-Description and MUIQD-VQA, addressing the abilities of quality perception and interaction of MLLMs, respectively. Specifically, MUIQD-Description comprises 18634 underwater images covering diverse real-world underwater scenes and typical quality degradations, such as color cast, haze effect, blurring and low contrast, etc. Each image is annotated through rigorous subjective evaluation with detailed descriptions of quality-related attributes and an overall quality level derived from the attributes. To further improve the interactive quality perception capability of MLLMs, we build the visual-question-answering-based dataset MUIQD-VQA, containing more than 93K question-answer pairs derived from the MUIQD-Description dataset generated by DeepSeek. Experimental results demonstrate that the proposed MUIQD dataset promotes the abilities of MLLMs on underwater image quality perception significantly, strongly supporting that MLLMs can be adapted for underwater image quality perception.", "tldr": "", "keywords": ["Underwater image quality perception", "image dataset", "multimodal large language models", "fine-tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/003cc0079f78ebb8462bcd443797a7c6fb605c00.pdf", "supplementary_material": "/attachment/2125c4b21834fac9dfe13f785c6b062743b864c3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical and underexplored gap: the application of Multimodal Large Language Models (MLLMs) to underwater image quality perception, a foundational task for underwater vision applications. The proposed large-scale MUIQD dataset consists of MUIQD-Description and MUIQD-VQA subsets. With the proposed MUIQD, the abilities of existing MLLMs were fully examined. Furthermore, by fine-tuning with MUIQD, the abilities of existing MLLMs on underwater image quality perception were improved significantly, which fully demonstrates the value of the proposed MUIQD dataset. This paper is well-structured and the experimental results are sound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The proposed MUIQD dataset is the first large-scale benchmark tailored for MLLMs in underwater image quality perception tasks, consisting of two complementary subsets (MUIQD-Description and MUIQD-VQA). MUIQD-Description’s 18,634 authentic underwater images and detailed subjective annotations well address the major limitation of existing underwater image quality datasets, which only offer single mean opinion scores (MOS). MUIQD-VQA’s 93K+ question-answer pairs further enables evaluation of MLLMs’ interactive quality perception. The proposed MUIQD provides a high-quality platform for benchmarking MLLMs in underwater image quality perception.\n(2) This work conducted extensive experiments to validate the proposed dataset’s utility, including: (1) fine-tuning seven SOTA MLLMs with both FFT and LoRA; (2) evaluating performance with task-relevant metrics (Precision/Completeness/Relevance for description, accuracy for attribute-based QA); and (3) ablation studies to reveal mutual reinforcement between quality description and attribute perception. Experimental results consistently demonstrate that the proposed MUIQD significantly boosts MLLMs’ performance, confirming the value of MUIQD and MLLMs’ adaptability to underwater image quality perception tasks.\n(3) The paper is well-structured, with clear presentation of the problems, the proposed dataset and the experimental results and analysis."}, "weaknesses": {"value": "(1) The authors mention 30 subjects participated in annotations, but key details about the subjects’ background are missing (e.g., whether they had prior experience in image quality assessment). \n(2) The manuscript states images were collected from existing datasets (Lian et al. 2023; Liu et al. 2024e; etc.), but it does not specify how many images were sourced from each dataset, nor whether any duplicate images were included.\n(3) The authors should explicitly discuss the dataset’s limitations, for example: does MUIQD-VQA cover all critical quality degradations (e.g., motion blur from water current, which is common in real underwater imaging)? are the images in MUIQD biased toward specific underwater scenes (e.g., coastal vs. deep-sea environments)?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2OchbPHCmM", "forum": "68FgdHQvJH", "replyto": "68FgdHQvJH", "signatures": ["ICLR.cc/2026/Conference/Submission8832/Reviewer_62Xc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8832/Reviewer_62Xc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760963313631, "cdate": 1760963313631, "tmdate": 1762920602894, "mdate": 1762920602894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MUIQD, a benchmark for underwater image-quality perception that pairs free-form quality descriptions with attribute-focused VQA. It evaluates seven modern MLLMs using language-based Precision/Completeness/Relevance for descriptions and QA accuracy for attributes, and shows that fine-tuning (LoRA or full-finetune) on MUIQD consistently boosts both abilities, indicating strong transfer between description and attribute QA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Separates quality description and attribute QA to probe complementary aspects of perception (content + essential degradations).\n2. Tests 7 SOTA MLLMs with clear language-based metrics plus attribute-accuracy.\n3. Both LoRA and full fine-tuning on MUIQD yield substantial improvements; cross-task results suggest complementary benefits between description and attribute QA."}, "weaknesses": {"value": "1. Provide a thorough comparison between the proposed dataset and existing underwater image-quality perception datasets, including both quantitative statistics and qualitative examples.\n2. Include stronger baselines by adding advanced IQA methods (e.g., VisualQuality-R1) to the comparisons, and expand the Related Work section to cover representative IQA approaches.\n3. Clearly articulate the advantages of the proposed dataset over prior underwater IQA datasets, e.g., scale, image and scene diversity, acquisition conditions, annotation protocol and reliability, label granularity, and task relevance, supported by concrete metrics.\n4. Explain in detail why fine-tuned models (e.g., Gemma3-12B) underperform their corresponding baselines. Analyze potential causes (data shift, overfitting, optimization choices, instruction/format mismatch, evaluation setup) and discuss diagnostic experiments and remedies."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8ux4hKNXas", "forum": "68FgdHQvJH", "replyto": "68FgdHQvJH", "signatures": ["ICLR.cc/2026/Conference/Submission8832/Reviewer_UWao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8832/Reviewer_UWao"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584603640, "cdate": 1761584603640, "tmdate": 1762920602448, "mdate": 1762920602448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MUIQD, a large-scale dataset designed to benchmark and enhance multimodal large language models for underwater image quality perception. The dataset consists of two subsets: MUIQD-Description, containing 18,634 underwater images annotated with human-written quality descriptions, and MUIQD-VQA, containing over 93K question–answer pairs automatically generated from those descriptions using DeepSeek. The authors fine-tune several SOTA MLLMs with full fine-tuning and LoRA, evaluating them on description similarity and QA accuracy. Results show consistent improvements after fine-tuning, suggesting that MUIQD effectively enhances the quality perception ability of MLLMs for underwater imagery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores a relatively under-examined domain—underwater image quality perception—using MLLMs. Although the methodology is not novel, applying instruction-tuned multimodal learning to this niche, yet practically important, visual domain is uncommon. Establishing a baseline for underwater scenarios broadens the scope of multimodal benchmarking beyond typical terrestrial or aerial imagery domains.\n2. In addition, the experimental section is systematic and reasonably reproducible: multiple models (Qwen, Ovis, LLaVA, Gemma, etc.) and fine-tuning schemes (FFT, LoRA) are compared under consistent settings. Results demonstrate consistent performance gains across all tested MLLMs, lending empirical support to the proposed benchmark’s usefulness."}, "weaknesses": {"value": "1. The work combines existing datasets and tools (DeepSeek, LoRA fine-tuning) without methodological innovation.\n2. Image selection rules are not described, no sampling or balancing strategy is provided, only “random sampling.”\n3. The dual use of DeepSeek for both QA generation and evaluation is problematic and may inflate the observed improvements. No human or cross-model verification is reported.\n4. There is no comparison with traditional underwater IQA metrics (e.g., UCIQE, UIQM) to demonstrate generalization."}, "questions": {"value": "1. How were the 18,634 images selected from existing databases? Were they randomly chosen or curated for quality diversity?\n2. Why is DeepSeek used as both a QA generator and an evaluator? This creates a risk of evaluator–model alignment and may inflate scores for models that echo DeepSeek’s phrasing or bias.\n3. Given the many quality attributes to consider, how do you ensure these sets are representative?\n4. Comparing the fine-tuned MLLMs with traditional computational IQA models to better demonstrate the added value of LLMs in underwater image quality perception."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S29cWKgcS2", "forum": "68FgdHQvJH", "replyto": "68FgdHQvJH", "signatures": ["ICLR.cc/2026/Conference/Submission8832/Reviewer_mHaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8832/Reviewer_mHaP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625737878, "cdate": 1761625737878, "tmdate": 1762920602014, "mdate": 1762920602014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MUIQD, the first large-scale multimodal dataset designed to enhance the underwater image quality perception capabilities of MLLMs. It consists of two components: MUIQD-Description and MUIQD-VQA. The authors used these annotations to fine-tune multiple MLLMs. Experimental results demonstrate that the proposed MUIQD dataset notably improves the ability of MLLMs for underwater image quality perception and supporting that MLLMs can be adapted for underwater image quality perception. This is crucial for marine exploration and image enhancement tasks. However, there are several aspects of the experimental process that lack rigor, and the authors are expected to provide more reasonable explanations and theoretical justifications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe task motivation is clear and has significant application value. Underwater imaging plays a vital role in a variety of underwater tasks, such as resource exploration, marine monitoring, and biological conservation. The application of MLLMs in underwater scenarios is a relatively under-explored yet valuable direction.\n2.\tThe MLLMs involved in the experiments are comprehensive. Fine-tuning and testing were conducted on multiple representative MLLMs (Qwen2.5VL, Ovis, GLM-4V, InternVL, LLaVA, Gemma, etc.). As shown in Table 2 and Table 3, the data comparison is clear."}, "weaknesses": {"value": "1.\tIn Section 3.2, after the subjective experiments, the authors invited only one subject to review all the quality descriptions, which may introduce single-reviewer bias. Several subjects should be recruited to review the descriptions collaboratively; for example, each subject could be randomly assigned a subset of the data, and the post-review results could be aggregated. A description should be considered valid only if it is approved by all reviewers. Such a procedure would more effectively ensure the reliability of the quality annotations.\n2.\tIn Section 3.3, the authors state, “The average length of all the descriptions is 43.4 words, which can describe the underwater image quality comprehensively.” What is the evidence for this claim? If longer descriptions are assumed to be more comprehensive, the word count should correlate positively with completeness. The authors need to explain why 43.4 words are sufficient for a comprehensive description of underwater image quality.\n3.\tIn Section 6.2, during training, the authors froze the visual module of the MLLM to ensure that its general feature-extraction capabilities were well retained. However, the visual module extracts image features for the language module; would unfreezing it (e.g., fine-tuning only higher layers or adding LoRA to vision layers) improve detail-perception ability? A more detailed discussion is required.\n4.\tIn Section 6.4 (Ablations), the authors should include comparative experiments and results obtained by unfreezing the visual module, to substantiate the claim made in Section 6.2 that “freezing the visual module ensures that its general feature-extraction capabilities were well retained.”\n5.\tIn Section 6.1, the authors use DeepSeek to score the experimental results. Since DeepSeek itself is an AI model, is AI-scoring-AI reliable? The ground-truth QA labels and descriptive quality scores partly originate from the same model family used for fine-tuning and evaluation; consequently, the observed improvements may reflect the model’s ability to imitate or align with DeepSeek’s linguistic style and preferences rather than a genuine enhancement in low-level visual perception.\n6.\tIn Table 3, Gemma3-12B performs worse than its baseline after LoRA fine-tuning across all metrics. The authors should provide an explanation and discussion of this anomalous result.\n7.\tExperimentally, the authors should conduct aligned comparisons on the same test set with dedicated underwater image quality assessment (UIQA) models such as EDANet (arXiv:1809.06323) and PIGUIQA (arXiv:2412.15527), both of which have open-source implementations."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0sLnK8Azo", "forum": "68FgdHQvJH", "replyto": "68FgdHQvJH", "signatures": ["ICLR.cc/2026/Conference/Submission8832/Reviewer_Gy26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8832/Reviewer_Gy26"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152325944, "cdate": 1762152325944, "tmdate": 1762920601636, "mdate": 1762920601636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}