{"id": "SrQDYyoxAQ", "number": 13184, "cdate": 1758214843366, "mdate": 1763120045673, "content": {"title": "ReasonGen-R1: Cot for Autoregressive Image Generation Models Through SFT and RL", "abstract": "Although chain-of-thought (CoT) reasoning and reinforcement learning (RL) have driven breakthroughs in large language models(LLMs), their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based \"thinking\" skills via supervised fine-tuning (SFT) on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization (GRPO).\nTo enable the model to reason through text before generating images, We automatically generate and release a corpus of model-crafted rationales paired with input prompts, enabling controlled planning of object layouts, styles, and scene compositions.\nOur GRPO algorithm uses reward signals from a pretrained vision–language model to assess overall visual quality, optimizing the policy in each update. We further design an adaptive entropy loss to prevent model collapse in this relatively complex task.\nEvaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models.", "tldr": "", "keywords": ["Autoregressive Image Generation;RL;GRPO"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fee023dcff5798d455938da6c7763a2931710085.pdf", "supplementary_material": "/attachment/9f8c045a198972c7be026610c0303453d89828e3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ReasonGen-R1, a two-stage training framework to imbue autoregressive text-to-image models with chain-of-thought reasoning abilities. During sft cold start, the authors generate a dataset of textual rationales for image prompts, and fine-tune the image generator to produce these rationales before drawing the image. During GRPO, they are using a single overall visual quality VLM assessment as feedback. Extensive experiments are conducted to demonstrate the method’s effectiveness, along with some in-depth analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The motivation is clear and easy to grasp and the method demonstrates improved performance on tasks that standard image generators struggle with.\n* Good ablation study showing SFT and adaptive entropy loss matters and boost final performance.\n* Well documented and transparent training disclosure and data disclosure."}, "weaknesses": {"value": "* Several highly related work such as GoT-R1, T2I-R1, all uses chain-of-thought plus RL on AR image generation models, is not mentioned or compared in any way at all.\n* The abstract suggests the RL reward is mainly about “overall visual quality” as judged by a VLM and a rather small one . This is a very high-level and coarse signal with potential of hallucinations and hacking.\n* The experiments seem mostly focused on the compositional benchmark, evaluation on broader text-to-image tasks (COCO etc.) are not existent."}, "questions": {"value": "Was there any signs of reward hacking observed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YlP1JeLiGP", "forum": "SrQDYyoxAQ", "replyto": "SrQDYyoxAQ", "signatures": ["ICLR.cc/2026/Conference/Submission13184/Reviewer_4a4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13184/Reviewer_4a4R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761183355014, "cdate": 1761183355014, "tmdate": 1762923882399, "mdate": 1762923882399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "J5Ub2oeMVj", "forum": "SrQDYyoxAQ", "replyto": "SrQDYyoxAQ", "signatures": ["ICLR.cc/2026/Conference/Submission13184/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13184/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120044469, "cdate": 1763120044469, "tmdate": 1763120044469, "mdate": 1763120044469, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReasonGen-R1 proposes a two-stage framework designed to enhance the capabilities of autoregressive image generation models (based on Janus-Pro-7B). Its core contribution lies in integrating the successful Chain-of-Thought (CoT) reasoning mechanism from LLM into a visual generation pipeline, enabling a \"think-and-generate\" process.\n\nThe framework employs a Supervised Fine-Tuning (SFT) stage, utilizing 200k high-quality CoT trajectories annotated by GPT-4.1, to teach the model to explicitly generate reasoning plans. Subsequently, a reinforcement learning (RL) stage based on Group Relative Policy Optimization (GRPO) is used for training, incorporating an adaptive entropy loss to ensure training stability and address entropy explosion in multimodal sequences. \n\nExperimental results demonstrate that this method consistently outperforms baseline models on multiple benchmarks, significantly improving both image quality and instruction alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The key contribution of this work lies in introducing the Chain-of-Thought (CoT) and Reinforcement Learning (RL) paradigms, which have proven effective in the LLM domain, into autoregressive image generation models. By enabling the model to generate a reasoning plan before creating an image, it effectively decomposes complex instruction-following tasks into manageable intermediate steps. \n\n2.  The two-stage training framework ensures that the model learns the correct reasoning structure and format while continuously improving its performance.\n\n3.  The paper provides a comprehensive quantitative evaluation on multiple generation benchmarks. The results demonstrate that ReasonGen-R1 surpasses the strong baseline model, Janus-Pro-7.\n\n4.  Applying RL to autoregressive generative models with interleaved modalities (mixed text/image tokens) is highly prone to training instability. The proposed Adaptive Entropy Loss design effectively mitigates issues of entropy explosion or entropy collapse."}, "weaknesses": {"value": "1.  The reward model (RM) is built upon Qwen-2.5-VL and provides binary scores. The current binary scoring can be quite extreme – minor deviations in text or image quality might result in a reward of 0, which could pose challenges for training. \n\n2.  The autoregressive generative model must generate an entire CoT text sequence during inference, which inevitably increases inference latency. Although performance is improved, the additional computational overhead presents a challenge for real-time or high-throughput application scenarios. The paper should provide a quantitative analysis discussing the trade-off between the extra latency introduced by the \"thinking\" process and the corresponding performance gains."}, "questions": {"value": "1. The reward model (RM) is built upon Qwen-2.5-VL and provides binary scores. Were more fine-grained scoring schemes explored? More analysis regarding the RM would be beneficial.\n\n2. Janus-Pro is inherently a unified model. After this targeted training, how are its original capabilities, such as general understanding, affected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2QPYfgqWUp", "forum": "SrQDYyoxAQ", "replyto": "SrQDYyoxAQ", "signatures": ["ICLR.cc/2026/Conference/Submission13184/Reviewer_cLSw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13184/Reviewer_cLSw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398382566, "cdate": 1761398382566, "tmdate": 1762923882128, "mdate": 1762923882128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It proposes a two-stage pipeline for autoregressive image generation. On GenEval, DPG-Bench, and T2I-Benchmark, REASONGEN-R1 outperforms Janus-Pro-7B and surpasses many diffusion and autoregressive baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear.\n2. The combination of textual reasoning and image tokens is novel.\n3. The ablation study is comprehensive."}, "weaknesses": {"value": "1. RL reward is provided by a single VLM judge (Qwen2.5-VL-7B). Is the policy overfitting that judge?\n2. The evaluation benchmarks (GenEval, DPG-Bench, T2I-Benchmark; Tables 1–3) mostly test object count, color binding, spatial relations, etc. What about the human preference evaluation benchmark? For instance, MM-RewardBench.\n3. Human evaluation is missing.\n4. Figure 4 shows RL is unstable without adaptive entropy loss. The theoretical justification could be proposed.\n5. The work does not achieve optimal performance on T2I-Benchmark. It is nice to give further analysis.\n6. The setting appears oversimplified, as the inference process seems limited to single-step generation. It would be valuable to examine its effectiveness in multi-round iterative refinement or video generation scenarios."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ORSkPxooak", "forum": "SrQDYyoxAQ", "replyto": "SrQDYyoxAQ", "signatures": ["ICLR.cc/2026/Conference/Submission13184/Reviewer_CuiH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13184/Reviewer_CuiH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573917628, "cdate": 1761573917628, "tmdate": 1762923881789, "mdate": 1762923881789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ReasonGen-R1 which a wo-stage training paradigm combining supervised fine-tuning (SFT) with chain-of-thought (CoT) and RL. Although the paper addresses proposes some interesting approaches and questions, please find my detailed comments on weakness and strengths."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well motivated\n\n2. Current results show that out of the methods considered here the proposed method outperforms\n\n3. The paper uses a simple yet straightforward methodology"}, "weaknesses": {"value": "1. Without using some standard large scale benchmarks like Imagenet it is very hard to judge the quality of the model.\n\n2. Although the authors evaluate on DPG bench, genval and compbench. There lies a very inherent bias and noise specific to these benchmarks, they use methods like object detectors which can throw out a lot of false negatives and cannot detect classes beyond a fixed vocabulary. What are steps taken to make sure that the this method does not have these biases\n\n3. The comparisons are outdated, I would have liked to see some better competitor models like GPT-4o, Seedream, Nano Banana, Imagen 4/4-ultra\n\n4. In tab 4 I would have liked to see more models and bigger models\n\n5. Any insights on things like reward hacking or potential biases and issues can be an interesting addition"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gCLq9SCgeE", "forum": "SrQDYyoxAQ", "replyto": "SrQDYyoxAQ", "signatures": ["ICLR.cc/2026/Conference/Submission13184/Reviewer_gQBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13184/Reviewer_gQBK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993439061, "cdate": 1761993439061, "tmdate": 1762923881551, "mdate": 1762923881551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}