{"id": "vIcqXbhU0Y", "number": 17192, "cdate": 1758273281192, "mdate": 1759897191506, "content": {"title": "Coherent Local Explanations for Mathematical Optimization", "abstract": "The surge of explainable artificial intelligence methods seeks to enhance transparency and explainability in machine learning models. At the same time, there is a growing demand for explaining decisions taken through complex algorithms used in mathematical optimization. However, current explanation methods do not take into account the structure of the underlying optimization problem, leading to unreliable outcomes. In response to this need, we introduce Coherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO provides explanations for multiple components of optimization models, the objective value and decision variables, which are coherent with the underlying model structure. Our sampling-based procedure can provide explanations for the behavior of exact and heuristic solution algorithms. The effectiveness of CLEMO is illustrated by experiments for the shortest path problem, the knapsack problem, and the vehicle routing problem.", "tldr": "", "keywords": ["Optimization", "Explainability", "Interpretability", "Sensitivity Analysis", "Regression"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8bf1e62f8f1112670d7333efc16fc81b7cc0c5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study aims to enhance model-agnostic local explanation methods for mathematical optimization problems. In the traditional model-agnostic explanation framework, the goal is to provide an interpretable rule that clarifies the prediction made by a black-box model for a specific data instance. Applying this framework to mathematical optimization presents two significant challenges. First, the result of an optimization algorithm is not just a single value, but rather a solution vector along with its corresponding objective value. Second, the mathematical optimization task is typically constrained within a feasible set, meaning that the explanation must adhere to these constraints. To tackle this explanation challenge, the authors propose an adaptation of the LIME method, originally designed for explaining regression tasks. In this present approach, the explanation is described as a vector of interpretable components (linear functions), with each component corresponding to a specific element of the output. Furthermore, the regularized fidelity function used in LIME is replaced with a regularized loss function that penalizes explanations that violate the constraints of the optimization task. This methodology is experimentally validated across three combinatorial optimization problems: shortest-path, knapsack, and vehicle routing."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**S1.** One of the primary strengths of this paper is its focus on the problem being examined by the authors. Explaining the output of a solving algorithm in relation to the input parameters of a problem instance is more complex than standard post-hoc explanation tasks, as the output is a vector and the explanation must be consistent with the input constraints.\n\n**S2.** This study is well-motivated; the introduction effectively justifies the need for explaining mathematical optimization tasks, particularly for applications in sensitivity analysis and constraint modeling."}, "weaknesses": {"value": "**W1.** The notation used in this paper is quite dense and has not been adequately introduced, which leads to ambiguity in many definitions and results. Additionally, the classes of models employed to explain mathematical optimization problems are not clearly defined.\n\n**W2.** The proposed approach is primarily heuristic and lacks substantial theoretical guarantees. For example, Proposition 3.1 is straightforward, and the last paragraph of Section 3 (Lines 282-292) is quite ambiguous. \n\n**W3.** The sampling method is not explained in detail: virtually nothing is said about the underlying probability distribution. Notably, nothing ensures that the parameters drawn at random will result in a feasible problem.  \n\n**W4.** The current framework does not consider the “succinctness” of explanations, which plays a critical role in their interpretability. The brief mention in Lines 203-204 is unclear, as the penalty parameter does not always ensure the sparsity of explanations. Furthermore, the sizes of explanations are not reported in the experiments."}, "questions": {"value": "Here are some comments and questions related to the above weaknesses:\n\n**C1.** As previously mentioned, the notation is not clearly introduced, which makes the paper quite difficult to understand. For the sake of clarity, the domain and co-domain of the optimization model $ h $ should be defined in Section 2 to help readers grasp what needs to be explained. Additionally, the classes $ \\mathcal{G} $ of explanation models are not formally defined. At the beginning of Section 3, each explanation model $ g$  is described as a $ p+1$-dimensional vector of local explanation models; however, later in Section 3, it is referred to as a vector $\\beta$  of coefficient vectors. This inconsistency is misleading and introduces an excessive amount of notation. I recommend formalizing an explanation as a square matrix of dimension $ (p + 1) \\times (p + 1) $, where each row represents a linear function that explains the $ c$-th component of the output. Furthermore, the class of vectors $ \\beta$  is not properly defined. Are the number and magnitude of the coefficients bounded?\n\n**C2.** The theoretical framework offers almost no theoretical guarantees. As noted earlier, Proposition 3.1 directly follows from equations (7) and (8), but it does not ensure that the computed explanation meets conditions (3) and (4) at convergence. Additionally, the paragraph in Lines 282-292 is quite ambiguous. Line 287 suggests that the objective is to minimize the square loss of a single vector $\\beta_c$, but we should actually be minimizing the square loss of the entire matrix of coefficient vectors. Moreover, in the proof of Theorem A.1 (which is, by the way, better formulated), it is assumed that the matrix in Line 776 is invertible; however, this is generally not the case. I would recommend rewriting the proof using the pseudo-inverse and demonstrating that the resulting solution satisfies conditions (3) and (4).\n\n**C3.** The sampling method should be clearly specified. Currently, there is no information about the probability distribution over the parameter space $\\Theta$. This is a critical aspect of the framework because if a sample $\\theta$ results in an infeasible problem (i.e., the set $X(\\theta)$ is empty), then the explanation task becomes vacuous. The paragraph from Lines 254-259 is too ambiguous; if $\\Theta$ includes infeasible parameter vectors, what is the distribution, and how can we efficiently sample $N$ instances from this distribution in polynomial time?\n\n**C4.** The explanation framework proposed in this study focuses solely on the coherence criterion. While finding coherent explanations is commendable, it is not the only important criterion. Succinctness (or sparsity) is often considered equally vital for interpretability (see, for example, Lage et al. 2019). Unfortunately, this criterion receives little attention in the study. Specifically, the statement in Lines 203-204 is insufficient. Combining the “constraint” regularizer $R_C$ with a “sparsity” regularizer $\\Omega$ does not guarantee that the resulting explanations will be sparse, even in the convex case, because the objective function must balance both regularizers. It would be helpful to include some comments on how we can ensure that explanations are both coherent and sparse. Additionally, the average size (number of nonzero coefficients) of the explanations should be reported in the experiments.\n\n**Reference**\n\nIsaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J. Gershman, Finale Doshi-Velez: Human Evaluation of Models Built for Interpretability. Proceedings of the 7th AAAI Conference on Human Computation and Crowdsourcing (HCOMP), pages 59--67, 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12t01a1Mxm", "forum": "vIcqXbhU0Y", "replyto": "vIcqXbhU0Y", "signatures": ["ICLR.cc/2026/Conference/Submission17192/Reviewer_jA3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17192/Reviewer_jA3G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760894854018, "cdate": 1760894854018, "tmdate": 1762927168178, "mdate": 1762927168178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLEMO, a method for generating local explanations for mathematical optimization models that ensures coherence between the predicted objective value and decision variables with the underlying problem structure. By incorporating coherence regularizers into a LIME-like framework, the approach aims to provide more reliable explanations for both exact and heuristic solvers. The method is evaluated on several classic optimization problems, including shortest path, knapsack, and vehicle routing, with comparisons to baseline explanation methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper identifies a practical issue in post‑hoc explanations for optimization.\n+ The theoretical analysis is comprehensive and well‑developed. \n+ Implementation details and appendices support reproducibility."}, "weaknesses": {"value": "- The core methodological novelty is limited, as CLEMO primarily extends LIME by adding coherence penalties, a conceptually straightforward adaptation. This is particularly true given the proven redundancy of the objective coherence regularizer for problems with fixed linear objectives.\n\n- The method's reliance on an explicit, differentiable problem formulation for the feasibility regularizer is a major practical limitation. It remains unclear how CLEMO could be applied to black-box commercial solvers or complex heuristics where the internal constraint set is inaccessible.\n\n- The experimental evaluation is incomplete. It lacks ablation studies to dissect the contribution of each regularizer, does not normalize metrics for cross-problem comparison, and omits key baselines from the optimization literature, such as methods based on inverse optimization."}, "questions": {"value": "1. Why were the experimental comparisons limited to simple linear models and decision trees, excluding more relevant optimization-specific baselines like SHAP-based or inverse optimization or counterfactual explanation methods?\n\n2. What is the individual contribution of each coherence regularizer? An ablation study showing the performance with only $R_{C_1}$, only \\$R_{C_2}$, and both, would clarify their necessity.\n\n3. The runtime scales poorly with problem size. What specific algorithmic strategies or approximations could be implemented to make CLEMO feasible for large-scale optimization problems?\n\n4. How can the requirement for an explicit problem formulation be relaxed to apply CLEMO to black-box solvers where the internal constraints are not directly available?\n\n5. How sensitive are the explanations to the chosen hyperparameters? Was an ablation study conducted to understand the impact of different $\\lambda$ values on fidelity and coherence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LK3blLopUB", "forum": "vIcqXbhU0Y", "replyto": "vIcqXbhU0Y", "signatures": ["ICLR.cc/2026/Conference/Submission17192/Reviewer_QR8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17192/Reviewer_QR8b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495950117, "cdate": 1761495950117, "tmdate": 1762927167955, "mdate": 1762927167955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on explaining mathematical optimization algorithms such as those used to solve the shortest path problem and the knapsack problem. The authors propose **Coherent Local Explanations for Mathematical Optimization (CLEMO)** to address the limitations of existing methods like LIME, which often generate **incoherent explanations** that violate structural constraints.\nExperiments on SPP, KP, and CVRP demonstrate that CLEMO produces **significantly more coherent explanations** than benchmark methods while maintaining comparable fidelity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simplicity and directness:** To address the incoherence of existing methods, the authors incorporate a coherence constraint directly into the optimization objective, ensuring that the generated explanations better satisfy the structural requirements of the problem.\n2. **Model-agnostic nature:** As a local explanation method, CLEMO can be applied to any black-box optimization model without relying on the internal structure of the model."}, "weaknesses": {"value": "1. **Performance trade-off:** Although coherence improves, fidelity decreases. While the authors claim that the loss in fidelity is acceptable, in real-world applications this reduction may compromise the practical usefulness of the explanations.\n2. **Limited baselines:** The paper only compares CLEMO with LIME and decision tree–based methods. Many more advanced explanation techniques exist, yet no comparison with them is provided.\n3. **Scalability issues:** The computational cost increases sharply as the problem size grows.\n4. **Dependence on convexity:** The reliance on convexity assumptions restricts the applicability of the proposed method."}, "questions": {"value": "1. The example provided in the introduction is confusing. For the original input parameters \\(a_{12}=4.1\\), the model already produces an infeasible solution. To my knowledge, most existing local model-agnostic explanation methods ensure that the surrogate accurately reproduces the prediction for the instance being explained. For example, LIME and SHAP both assign high weight to the original input during surrogate fitting. Could the authors clarify this point?\n2. Building on the previous question, the occurrence of infeasible predictions may indicate that the current setup goes beyond the neighborhood of local explanations. Would it be more appropriate for the user to specify a domain that only produces feasible solutions? Moreover, the proposed regularization introduces another concern.\n   Consider a piecewise linear relationship between (x) and (\\theta):\n   (x = \\theta) when (\\theta < 0.5), and (x = 0.5) when (\\theta \\geq 0.5),\n   with an additional constraint (x \\leq 0.5).\n   A baseline method might yield an explanation such as (x = \\theta) or (x = 0.9\\theta), which would produce an infeasible prediction at (\\theta = 1). In contrast, CLEMO’s regularization would favor an explanation closer to (x = 0.5\\theta), ensuring feasibility. However, note that the original explanation remains reasonably accurate for (\\theta < 0.5), thus effectively describing the model’s local behavior. CLEMO’s regularized explanation, on the other hand, may lose fidelity across the entire domain, making it less accurate everywhere. I suggest the authors discuss this trade-off explicitly.\n3. It would be helpful for the authors to provide a more intuitive example to better illustrate the performance of CLEMO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DbMQxAveE3", "forum": "vIcqXbhU0Y", "replyto": "vIcqXbhU0Y", "signatures": ["ICLR.cc/2026/Conference/Submission17192/Reviewer_LWrw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17192/Reviewer_LWrw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878270859, "cdate": 1761878270859, "tmdate": 1762927167686, "mdate": 1762927167686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}