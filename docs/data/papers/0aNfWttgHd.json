{"id": "0aNfWttgHd", "number": 23126, "cdate": 1758339931082, "mdate": 1759896831213, "content": {"title": "Conformal Prediction Adaptive to Unknown Subpopulation Shifts", "abstract": "Conformal prediction is widely used to equip black-box machine learning models with uncertainty quantification, offering formal coverage guarantees under exchangeable data. However, these guarantees fail when faced with subpopulation shifts, where the test environment contains a different mix of subpopulations than the calibration data.  \nIn this work, we focus on *unknown* subpopulation shifts where we are not given group-information i.e. the subpopulation labels of datapoints have to be inferred. \nWe propose new methods that provably adapt conformal prediction to such shifts, ensuring valid coverage without explicit knowledge of subpopulation structure. \nWhile existing methods in similar setups assume perfect subpopulation labels, our framework explicitly relaxes this requirement and characterizes conditions where formal coverage guarantees remain feasible. \nFurther, our algorithms scale to high-dimensional settings and remain practical in realistic machine learning tasks. Extensive experiments on vision (with vision transformers) and language (with large language models) benchmarks demonstrate that our methods reliably maintain coverage and effectively control risks in scenarios where standard conformal prediction fails.", "tldr": "Conformal prediction fails under distribution shifts. We show how to provably adapt it to handle unknown subpopulation shifts.", "keywords": ["conformal prediction", "distribution shifts", "subpopulation shifts", "uncertainty quantification"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b88a4d02c1f3dba5ef1aa165cb7f419df40dfbe5.pdf", "supplementary_material": "/attachment/789a172a95c28250ed978f2c18072c687c52a0bc.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of applying conformal prediction (CP) under unknown subpopulation shifts, where the proportions of latent subgroups differ between calibration and test data but group labels are unavailable. The authors propose three algorithms that adapt CP to such shifts: Algorithm 1 (Weighted CP with Domain Classifier) uses a domain classifier to estimate subpopulation probabilities and reweights calibration scores; Algorithm 2 (Multiaccuracy-based CP) relaxes the assumption by requiring only multiaccuracy of the classifier, making the method more practical; and Algorithm 3 (Similarity-based CP) handles settings without any domain classifier by using embedding similarity to weigh calibration examples.\n\nEmpirical evaluations on vision benchmarks (ImageNet subpopulation splits) and LLM hallucination detection show that the proposed methods maintain tight coverage across distribution shifts, outperforming standard and group-conditional CP methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the underexplored setting of unknown subpopulation shifts where group labels are unavailable, which is an important step beyond existing CP methods assuming known domains or exchangeability.\n\n2. The authors provide formal coverage guarantees under varying assumptions (Bayes-optimal, multicalibrated, multiaccurate classifiers). \n\n3. Experiments cover both vision (ImageNet-based BREEDS subpopulation shifts) and language (LLM hallucination detection)."}, "weaknesses": {"value": "1. The coverage guarantees hinge on the domain classifier being multicalibrated or multiaccurate—properties that are difficult to ensure in high-dimensional practice. While the authors reference empirical evidence of approximate multicalibration, more rigorous discussion of real-world feasibility is needed.\n\n2. The experiments primarily compare against variants of CP (standard, max, and conditional calibration). It would be informative to compare against general **distribution-shift calibration** methods (e.g., density-ratio-based or covariate shift adaptation techniques).\n\n3. Algorithm 3 introduces parameters $\\beta$ and $\\sigma$, but their sensitivity and impact on performance are *not* analyzed.\n\n4. Although the BREEDS benchmark simulates subpopulation shifts, all domains are derived from ImageNet classes. How about cross-dataset or real-world domain adaptation settings?\n\n5. How robust are the proposed methods to misspecification of the domain classifier? For example, if they systematically bias certain subpopulations rather than random errors?\n\n\n6. Concerning algorithms, \n\n6a. The explanation/clarification of algorithms in main text are missing, making it hard to follow the ideas. \n\n6b. The statements in algorithm are **BROKEN** and **NOT** self-contained. Take Algorithm 1 as an example: \n\n(i) How to \"calculate score $s_i^k$\", based on some equations or score function $S$ or other?\n\n(ii) $\\hat{\\lambda}$ and $\\hat{q}_{\\alpha}$ appear abruptly. \n\n(iii) What is the relationship bewtween $\\hat{\\lambda}$ and $\\hat{\\lambda}_k$, given the latter is the $k$-th entry of $c$?\n\n(iv) What is the definition of $J$?\n\n\n7. Besides algorithms, other writing and presentation issues makes it challenging to follow without constant reference to prior sections.\n\nWhat are the definition of \"A1\", \"A2\", and \"A3\"? If they refer to three algorithms, respectively, should \"oracle\" be \"A1\"?\n\nFurther, I personally do not think Algorithm 1 could be referred as \"orcale\"."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "76HQoRiiky", "forum": "0aNfWttgHd", "replyto": "0aNfWttgHd", "signatures": ["ICLR.cc/2026/Conference/Submission23126/Reviewer_xivC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23126/Reviewer_xivC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541165337, "cdate": 1761541165337, "tmdate": 1762942522715, "mdate": 1762942522715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of applying conformal prediction (CP) under unknown subpopulation shifts, where the proportions of latent subgroups differ between calibration and test data but group labels are unavailable. The authors propose three algorithms that adapt CP to such shifts: Algorithm 1 (Weighted CP with Domain Classifier) uses a domain classifier to estimate subpopulation probabilities and reweights calibration scores; Algorithm 2 (Multiaccuracy-based CP) relaxes the assumption by requiring only multiaccuracy of the classifier, making the method more practical; and Algorithm 3 (Similarity-based CP) handles settings without any domain classifier by using embedding similarity to weigh calibration examples.\n\nEmpirical evaluations on vision benchmarks (ImageNet subpopulation splits) and LLM hallucination detection show that the proposed methods maintain tight coverage across distribution shifts, outperforming standard and group-conditional CP methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the underexplored setting of unknown subpopulation shifts where group labels are unavailable, which is an important step beyond existing CP methods assuming known domains or exchangeability.\n\n2. The authors provide formal coverage guarantees under varying assumptions (Bayes-optimal, multicalibrated, multiaccurate classifiers). \n\n3. Experiments cover both vision (ImageNet-based BREEDS subpopulation shifts) and language (LLM hallucination detection)."}, "weaknesses": {"value": "1. The coverage guarantees hinge on the domain classifier being multicalibrated or multiaccurate—properties that are difficult to ensure in high-dimensional practice. While the authors reference empirical evidence of approximate multicalibration, more rigorous discussion of real-world feasibility is needed.\n\n2. The experiments primarily compare against variants of CP (standard, max, and conditional calibration). It would be informative to compare against general **distribution-shift calibration** methods (e.g., density-ratio-based or covariate shift adaptation techniques).\n\n3. Algorithm 3 introduces parameters $\\beta$ and $\\sigma$, but their sensitivity and impact on performance are *not* analyzed.\n\n4. Although the BREEDS benchmark simulates subpopulation shifts, all domains are derived from ImageNet classes. How about cross-dataset or real-world domain adaptation settings?\n\n5. How robust are the proposed methods to misspecification of the domain classifier? For example, if they systematically bias certain subpopulations rather than random errors?\n\n\n6. Concerning algorithms, \n\n6a. The explanation/clarification of algorithms in main text are missing, making it hard to follow the ideas. \n\n6b. The statements in algorithm are **BROKEN** and **NOT** self-contained. Take Algorithm 1 as an example: \n\n(i) How to \"calculate score $s_i^k$\", based on some equations or score function $S$ or other?\n\n(ii) $\\hat{\\lambda}$ and $\\hat{q}_{\\alpha}$ appear abruptly. \n\n(iii) What is the relationship bewtween $\\hat{\\lambda}$ and $\\hat{\\lambda}_k$, given the latter is the $k$-th entry of $c$?\n\n(iv) What is the definition of $J$?\n\n\n7. Besides algorithms, other writing and presentation issues makes it challenging to follow without constant reference to prior sections.\n\nWhat are the definition of \"A1\", \"A2\", and \"A3\"? If they refer to three algorithms, respectively, should \"oracle\" be \"A1\"?\n\nFurther, I personally do not think Algorithm 1 could be referred as \"orcale\"."}, "questions": {"value": "1. Could authors provide more rigorous discussion of real-world feasibility?\n\n2. Could authors compare against general **distribution-shift calibration** methods (e.g., density-ratio-based or covariate shift adaptation techniques)?\n\n3. Could authors analyze the sensitivity and impact of the introduced hyperparameters, such as $\\beta$ and $\\sigma$?\n\n4. How about cross-dataset or real-world domain adaptation settings?\n\n5.  How robust are the proposed methods to misspecification of the domain classifier? For example, if they systematically bias certain subpopulations rather than random errors?\n\n6. Concerning algorithms, the statements in algorithm are **BROKEN** and **NOT** self-contained. Take Algorithm 1 as an example: \n\n(i) How to \"calculate score $s_i^k$\", based on some equations or score function $S$ or other?\n\n(ii) $\\hat{\\lambda}$ and $\\hat{q}_{\\alpha}$ appear abruptly. \n\n(iii) What is the relationship bewtween $\\hat{\\lambda}$ and $\\hat{\\lambda}_k$, given the latter is the $k$-th entry of $c$?\n\n(iv) What is the definition of $J$?\n\n\n7. What are the definition of \"A1\", \"A2\", and \"A3\"? If they refer to three algorithms, respectively, should \"oracle\" be \"A1\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "76HQoRiiky", "forum": "0aNfWttgHd", "replyto": "0aNfWttgHd", "signatures": ["ICLR.cc/2026/Conference/Submission23126/Reviewer_xivC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23126/Reviewer_xivC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541165337, "cdate": 1761541165337, "tmdate": 1763163739231, "mdate": 1763163739231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes new conformal prediction methods that maintain valid uncertainty coverage under unknown subpopulation shifts, where test data distribution differs from calibration data. The authors develop algorithms that use subpopulation structure through domain classifiers or similarity measures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1- The introduction effectively motivates the problem and provides a thorough overview of previous methods\n\n2- The paper is well written"}, "weaknesses": {"value": "Please check the questions."}, "questions": {"value": "1- how was the parameter $\\beta$ was selected?\n\n2- In the conclusion, the paper mentions experiments with synthetic data. Could the authors clarify where synthetic data was used and for what purpose?\n\n3- If I understand correctly, in Figure 2 the average coverage of the unweighted conformal prediction method appears above the desired $1−\\alpha$. If this interpretation is correct, could the authors explain why over-coverage occurs here?\n\n4- Please include discussion or analysis of the computational complexity of the proposed algorithms, especially in comparison to standard conformal prediction\n\n5- The empirical section need to report the average prediction set size of the proposed methods versus existing baselines\n\n6- Although the algorithms are claimed to scale to high-dimensional tasks, no runtime or memory comparisons are provided to substantiate the claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JseJWpCezJ", "forum": "0aNfWttgHd", "replyto": "0aNfWttgHd", "signatures": ["ICLR.cc/2026/Conference/Submission23126/Reviewer_juAF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23126/Reviewer_juAF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839020233, "cdate": 1761839020233, "tmdate": 1762942522492, "mdate": 1762942522492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is addressing the subpopulation shift for conformal prediction. This problem was addressed prior by Tibshirani et al 2020 under the term covariate shift, however there it is assumed that. the distribution of calibration and test are known. Here the authors first show that the existing approach drastically fail for imperfect estimators, and then provide a series of methods by estimating the subpopulation distribution. Their hierarchy of algorithms are tailored for various levels subgroup prediction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Besides the interesting and applicable problem, the authors break down the problem in different levels of knowledge about the subpopulation. This allows to choose upon the task and the environment.\n\nTheir experimental results cover a wide range of setups from image to language which is a plus. This shows that their method is applicable and not only abstract.\n\nDespite the minor flaws, the writing is good, and the paper is easy to follow."}, "weaknesses": {"value": "**Unclear statement about the subgroups.** I could not understand whether the authors are assuming that the subpopulations are known, and discrete? And if the assignment for such subgroups are given at least over the training data. This is important to be clarified in all algorithms and theorems. This is an important flaw since for instance in Algorithm 1, and 2 the classifier c is assumed to be trained or at least trainable.\n\n**Strong assumptions in Section 3.** Although not made clear, the assumptions noted in Definition 3.2, and 3.4 are very strong. If we were able to train optimal Bayes classifier, or multi-calibrated classifier, then wasn't it easier to achieve subgroup conditional, or even conditional coverage at first place? If so then the subpopulation shift would be meaningless as APS is already capable of providing conditional and hence subpopulation conditional coverage. \n\nHowever the last point is (to the best of my understanding) very strong, I would still think the paper is acceptable due to the results in Section 4."}, "questions": {"value": "1. In algorithm 3, sigma is a function, but you are treating it as a scaler and divide a number by divide it. I can not parse the algorithm. What does setting the score to infinity mean here?\n2. Can you elaborate more on Definition 3.4? What is the expectation over? \n3. Can you make some examples about when an optimal Bayes classifier, and a multicalibrated classifier is even possible to have?\n4. I am not sure but should the guarantee in theorem 3.3, 3.4, and 3.5 be conditional to the X coming from any subpopulation? \n5. Does the theorem 3.1 reduce to Mondrian Conformal Prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VWKRzryCnN", "forum": "0aNfWttgHd", "replyto": "0aNfWttgHd", "signatures": ["ICLR.cc/2026/Conference/Submission23126/Reviewer_Us4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23126/Reviewer_Us4v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900181496, "cdate": 1761900181496, "tmdate": 1762942522044, "mdate": 1762942522044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles conformal prediction under subpopulation shifts when group labels aren't available. The authors propose using domain classifiers to weight calibration data adaptively, with theoretical guarantees under multicalibration and multiaccuracy assumptions. They also introduce a similarity-based approach when no domain information exists. Experiments on ImageNet variants and LLM hallucination detection show reduced coverage variance across test environments compared to standard conformal prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis work addresses a relevant and underexplored problem of adapting conformal prediction to unknown subpopulation shifts.\n2.\tThe theoretical results are clear and intuitive.\n3.\tThis paper provides a clear motivation.\n4.\tThe proposed approach is relevant to a wide range of tasks, from ImageNet to LLM hallucinations."}, "weaknesses": {"value": "1.\tThe theoretical guarantee of Theorem 3.3 relies on a strong assumption of having perfect or multicalibrated domain classifiers.  \n2.\tThis work does not analyze what happens when the domain classifier is not multiaccurate and provides no empirical or theoretical results for this case.\n3.\tAlgorithm 3, which handles the most realistic setting with no domain labels, is purely heuristic and lacks theoretical or empirical motivation.\n4.\tThis paper should compare the proposed approach to other existing methods, such as Robust/max CP and group conditional CP, or the method proposed by Cherian et al. (2024) for LLM validity control."}, "questions": {"value": "1.\tWhat is the “standard LLM uncertainty estimation method” in line 475?\n2.\tIn Algorithm 3, how were the parameters $\\beta$ and $\\sigma$ chosen, and how stable are the results under different values?\n3.\tThe experiments consider 15-26 domains. How does the method scale to 100+ domains? Does the domain classifier turn harder to train, and does the coverage rate have a higher variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FtOZAMl9y0", "forum": "0aNfWttgHd", "replyto": "0aNfWttgHd", "signatures": ["ICLR.cc/2026/Conference/Submission23126/Reviewer_iwrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23126/Reviewer_iwrx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913111321, "cdate": 1761913111321, "tmdate": 1762942521785, "mdate": 1762942521785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}