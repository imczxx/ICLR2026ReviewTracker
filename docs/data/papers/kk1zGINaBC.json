{"id": "kk1zGINaBC", "number": 15550, "cdate": 1758252591955, "mdate": 1759897299949, "content": {"title": "TransES-ETA: A Novel Transformer-based Explainable and Efficient Structure for Predicting Estimated Time of Vessel Arrival", "abstract": "The estimated time of arrival (ETA) prediction is crucial in maritime AI for improving maritime shipping operation efficiency and resilience; however, they currently still face significant challenges. Traditional machine learning-based methods struggle with extracting accurate representations of Automatic Identification System (AIS) data, while neural network-based methods rely heavily on data quality and lack of explainability. In this paper, we propose TransES-ETA, a transformer-based, multi-task framework that jointly handles port call scheduling and ETA prediction in an end-to-end and explainable manner, leveraging the strong correlation between the two tasks.\nWe first design a novel AIS data extractor composed of multiple modules, each responsible for processing different semantic data attribute categories, enabling the transformer to capture accurate representations. Then, the port call schedule module of our proposed pipeline predicts vessel trajectories and generates features that serve as inputs for the subsequent ETA prediction module. Finally, the ETA module aggregates both AIS-derived features and port call schedule features to produce the final ETA estimate. The entire multi-task pipeline is trained in an end-to-end manner, allowing for the simultaneous optimization of both tasks with the shared feature and thereby improving training efficiency. Furthermore, by incorporating port scheduling information, the ETA predictions become more interpretable. Comprehensive evaluations demonstrate that our proposed model achieves mean absolute error (MAE) 2.46h in ETA prediction. Additionally, the pipeline accelerates training and inference by 80.93\\% and  8.84% respectively, making the framework more efficient.", "tldr": "", "keywords": ["maritime AI", "AI application", "temporal model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e393093c4763c9ac3acce8dc2bf10e0f891dcab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TransES-ETA, a Transformer-based multi-task framework for maritime ETA prediction. It jointly learns port call scheduling and ETA estimation from AIS data in an end-to-end, explainable manner. By leveraging the correlation between the two tasks, the model improves prediction accuracy (MAE = 2.46 h) and enhances training and inference efficiency by 80.93% and 8.84%, respectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper presents a framework with strong potential impact in the field of maritime logistics and global trade.\n\n2. The proposed end-to-end framework jointly predicts port call schedules and Estimated Time of Arrival (ETA), effectively leveraging their intrinsic correlation. This design addresses a key limitation of prior two-stage or decoupled models, which treated these two tasks independently.\n\n3. The integration of port scheduling and ETA prediction into a unified pipeline offers practical value for real-world maritime operations and has clear implications for improving efficiency in global shipping management systems."}, "weaknesses": {"value": "1. The paper’s contribution to the broader representation learning and machine learning community appears limited. The methodology is highly domain-specific, focusing primarily on maritime forecasting applications rather than introducing a generally applicable ML framework or advancing theoretical understanding. Consequently, it may be more appropriate for a journal, conference, or workshop specializing in maritime operations or applied geospatial modeling rather than a general ML venue such as ICLR.\n\n2. The experimental scope is restricted to the East Asia region, covering routes from China, Korea, and Japan to Singapore. This limited geographical focus raises concerns regarding the model’s generalizability to other maritime regions, such as Northwest Europe or North America.\n\n3. The number of baseline models used for comparison is insufficient. Only two baselines are reported, which is too few to substantiate claims of superiority. Additional state-of-the-art benchmarks should be included to strengthen the empirical validation.\n\n4. Although the paper reports training and inference time, other efficiency-related metrics are missing. Specifically, there is no analysis of parameter count, computational complexity, or GPU memory consumption, which are essential for evaluating model efficiency.\n\n5. The paper lacks explicit reporting of final hyperparameter configurations. Important details such as the sliding window size, the hidden dimension of the GRU (or GCNwRC), and dropout rates are not specified, which significantly hinders reproducibility.\n\n6. There is no hyperparameter sensitivity analysis. The paper does not explore how performance varies with respect to key hyperparameters such as learning rate, loss weighting factor, or window size, leaving uncertainty about the robustness of the results.\n\n7. The authors do not provide access to source code or implementation details. Neither the main text nor the appendix includes information about code availability, dataset accessibility, or reproducibility guidelines. This omission substantially limits transparency and the ability of other researchers to replicate the work.\n\n8. Minor issue: The reference list contains duplicate entries for the same seminal work. Both refer to \"Attention Is All You Need\" by Vaswani et al., but are presented separately as the NeurIPS 2017 and arXiv 2023 versions. These should be merged into a single citation."}, "questions": {"value": "1. Can the authors provide additional experiments using data from regions beyond East Asia, such as Northwest Europe, North America, or Africa, to demonstrate the generalizability of the proposed model?\n\n2. Could the proposed framework be adapted to other domains beyond maritime transportation, for instance, air traffic scheduling or rail logistics? If so, what modifications would be required?\n\n3. Could the authors provide additional efficiency-related analyses, such as the parameter count, computational complexity (e.g., FLOPs), and GPU memory consumption of the proposed model, to substantiate the claims regarding efficiency and scalability?\n\n4. How sensitive is the model’s performance to variations in hyperparameters such as the learning rate, α in the loss weighting, or the size of the sliding window? Have the authors conducted any preliminary sensitivity analysis or parameter tuning experiments?\n\n5. Are there plans to release the code and dataset used in this study to ensure reproducibility and allow for fair comparison by future researchers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fby9shtNH4", "forum": "kk1zGINaBC", "replyto": "kk1zGINaBC", "signatures": ["ICLR.cc/2026/Conference/Submission15550/Reviewer_GHJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15550/Reviewer_GHJP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971966991, "cdate": 1760971966991, "tmdate": 1762925825228, "mdate": 1762925825228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a model for vessel route forecasting and eta prediction in long ship trips. The topic is relevant and interesting for a growing community."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The topic is relevant and of interest to the relative community."}, "weaknesses": {"value": "W1) seriour concerns about writing quality and readability.\n\n- In the introduction, the authors say that \"Current data-driven approaches, including neural networks or transformer architectures for ETA prediction (Shipley et al. (2022)), struggle with poor data quality and the limited interpretability of end-to-end pipelines\", but their approach is exactly a \"data-driven\" and \"neural network\" method.\n\n- The introduction is poorly written, it is not fluent, and it seems a collage of LLM-written small pieces, without having a logical flow.\n\n- In the related works, among the transformer methods for route prediction, the authors should also acknowledge:\n1) TrAISformer—A Transformer Network with Sparse Augmented Data Representation and Cross Entropy Loss for AIS-based Vessel Trajectory Prediction\n2) Sailing the Seaformer: A Transformer-Based Model for Vessel Route Forecasting\n\n- The sentence \"We encode the above five types of information using formulas and deep learning models.\" doesn't have sense.\n\n- The sentence \"To enhance the robustness and interpretability of the model, a Transformer-based multi-task prediction module is proposed\" doesn't have sense.\n\n- Section 4.2 recalls again the related work being also very general talking about \"recurrent neural network\", LSTM etc without referencing prior works. Also, this should be the method section and not related works should be included here. however, this is the typical writing of LLMs.\n\n- L_port and L_ETA and L_feat recon are not defined.\n\nW2) This is an applied paper presenting barely no novel method. It is just a collection of existing modules (and some of them are outdated and very simple like the GCN and the GRU)\n\nW3) there is no comparison in the experiments.\n\nW4) No ground truth route is reported in figure 2, so it is impossible to understand whether the model correctly predicts route and eta or not.\n\nW5) in table 4 the authors include \"original model\" and \"improved model\", while in table 5 they call about a \"baseline model\". lots of confusion in the results presentation. Also, the box plot in figure 3 is unreadable. \n\nW6) the authors put explainable in the title and throughout the main corpus but never provide explanations.\n\n\nOverall, the paper has no novelty, is poorly written, and it containes barely no comparisons in the experiments."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "REszLSr4fq", "forum": "kk1zGINaBC", "replyto": "kk1zGINaBC", "signatures": ["ICLR.cc/2026/Conference/Submission15550/Reviewer_pFsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15550/Reviewer_pFsG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972219831, "cdate": 1760972219831, "tmdate": 1762925824840, "mdate": 1762925824840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TranSES-ETA, a transformer-based multi-task end-to-end framework for predicting Estimated Time of Arrival (ETA) and port call schedules of maritime vessels using AIS data. The design achieves both efficiency and explainability by jointly modeling port call prediction and ETA regression, utilizing shared feature extraction through a transformer encoder and a novel graph-based encoding strategy. Experiments have been conducted in real-world datasets to show its improvement."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper jointly models port-call schedule and ETA, which leverages their interdependence, and thus improves the interpretability and efficiency of ETA.\n\n2. The proposed GCNwRC encoder can model within-grid AIS pattern in a batch way and capture its local pattern better, which brings large gains in accuracy.\n\n3. The paper is basically well written and easy to follow except for some minor notation ambiguity and typos."}, "weaknesses": {"value": "1. Unconvincing overall performance comparison.  Table 3 mixes cross-paper results (different datasets), so it’s not a fair baseline. The only same-dataset baselines are the authors’ own ablations (Tables 4–5). To claim superiority, you should at least run SOTA baselines on the same split and metrics of the same dataset. And furthermore, Experiments use multiple origins → single destination (Singapore),  there is no cross-region or multi-destination study.\n\n2. Lacking ablations and other experiments. Ablations cover only GRU→GCNwRC and with/without port-call branch, lacking channel-level and feature reconstruction loss. The loss scaling and dynamic weighting in §4.3 lacks necessary sensitivity analyses.\n\n3. Explainability Not Quantified. The paper claims contribution in explainability, but however, there are no quantitative measures or case study show how the ETA prediction is explained.\n\n4. Reproducibility. Several experimental choices remain not detailed enough, including precise architecture hyperparameters (e.g., GCNwRC layer setup), data pre-processing (temporal gaps), and train/test split."}, "questions": {"value": "1. What exactly is M in Table 1, how can a set of variable lengths appear as the tensor dimension N×M×f?\n\n2. In graph construction, why adopt a (bi-)complete graph within each grid instead of a sparse graph? Is there any temporal information encoded at the node/edge level inside the graph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aHTIQzcMGE", "forum": "kk1zGINaBC", "replyto": "kk1zGINaBC", "signatures": ["ICLR.cc/2026/Conference/Submission15550/Reviewer_q9WU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15550/Reviewer_q9WU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490733289, "cdate": 1761490733289, "tmdate": 1762925824331, "mdate": 1762925824331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is not a paper in my field. While the paper presents a well-motivated multi-task framework for vessel ETA prediction that integrates port call scheduling and leverages a novel graph-based feature extractor to improve training efficiency, it falls short of the methodological novelty, technical depth, and scientific rigor expected for publication at ICLR. The work is primarily an engineering integration of existing components (Transformer, GCN, Bi-GRU) applied to a domain-specific problem, without introducing fundamental advances in representation learning, attention mechanisms, or multi-task optimization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear Problem Motivation: Maritime ETA prediction is a high-impact application with real-world operational significance (e.g., fuel efficiency, port scheduling). The paper correctly identifies limitations of traditional two-stage models (error propagation) and black-box end-to-end systems (lack of interpretability).\nMulti-Task Design for Interpretability: By jointly predicting port call sequences and ETA, the model provides interpretable intermediate outputs, which is a valuable step toward transparent AI in safety-critical domains like maritime logistics.\nEfficiency Improvements: Replacing GRU with a graph-based local pattern encoder (GCNwRC) enables batch processing and reduces training time by 80.93%, which is a practical contribution for large-scale AIS data processing."}, "weaknesses": {"value": "Dear Authors, SPC, and AC:\n\nFirst, I want to clarify that this is not a paper in my field. I've skimmed through it and found the following issues:\n\nThe upper part of the structure diagram appears to be a multi-branch feature fusion module. I don't understand the lower part, but the authors don't seem to clearly state the fusion method for these features, nor do they explain the principle behind its effectiveness. It seems more like a mechanical incorporation of existing, even older, networks or models, lacking theoretical basis and design motivation. I hope the authors can provide insights into this design.\n\nThe loss function is also poorly designed, both in its expression and because the authors don't explain the origins of Lfeature reconstruction and Lport. I believe this is a significant flaw.\n\nFurthermore, the authors haven't released the code, making it impossible to reproduce the results.\n\nIn conclusion, I have a negative view of this work, despite my unfamiliarity with the field. If the authors can effectively address my concerns, I will consider raising the grade."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B87pRMSSTu", "forum": "kk1zGINaBC", "replyto": "kk1zGINaBC", "signatures": ["ICLR.cc/2026/Conference/Submission15550/Reviewer_mFJ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15550/Reviewer_mFJ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741073305, "cdate": 1761741073305, "tmdate": 1762925823686, "mdate": 1762925823686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}