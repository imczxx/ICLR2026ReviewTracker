{"id": "2xaNU3lgKC", "number": 5433, "cdate": 1757909122078, "mdate": 1763209436664, "content": {"title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction", "abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. While progress has been made in per-frame predictions of depth, surface normals, and segmentation, achieving stability under motion, occlusion, and illumination changes remains difficult. For this, we design a synthetic data pipeline that produces large-scale photorealistic human images and motion-aligned video sequences with high-fidelity annotations. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level and sequence-level supervision, supporting the learning of spatial accuracy and temporal stability. Building on this, we introduce a model that integrates human-centric priors and temporal modules to jointly estimate temporally consistent segmentation, depth, and surface normals within a single framework. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model to first acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.", "tldr": "", "keywords": ["Dense prediction", "Depth", "Surface normal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a37107f7ca8fe5fbaae0399e7c32c82d7b4c86f3.pdf", "supplementary_material": "/attachment/2fd27c3158111ccd25e288fde58bd335824b2af2.zip"}, "replies": [{"content": {"summary": {"value": "The paper targets temporally consistent human-centric dense prediction (depth, normals, segmentation) across video.\nKey obstacles are the lack of large-scale human video with paired dense labels, and coupling temporal stability with multi-task learning.\nA synthetic data pipeline generates photorealistic human images and motion-aligned sequences with high-fidelity labels, enabling frame- and sequence-level supervision.\nA ViT-based model integrates human-centric priors and temporal modules to jointly predict depth, normals, and segmentation.\nExperiments report state-of-the-art results on THuman2.1 and Hi4D for both depth and surface normal estimation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Build a scalable data synthesis pipeline for human-centric frames and videos with pixel-accurate depth, normals, and segmentation. \n- Going beyond static-image training with video supervision improves temporal stability and generalization in natural scenes."}, "weaknesses": {"value": "1. Points Requiring Clarification\n\n(1) Channel Weight Adaptation (CWA): The manuscript does not explain how the module distinguishes channels dominated by texture and illumination from those that are geometry-related, nor how the reweighting is computed to downweight the former and upweight the latter (thereby weakening the influence of appearance on geometry prediction and maintaining the consistency of the global representation).\n\n(2) Human Geometric Prior fusion: There is no description of how the human geometric prior is fused with the decoder features.\n\n(3) ùêø_{grad}  in Eq. (1): The definition of ùêø_{grad} is missing.\n\n2. Evaluation Completeness\n\n(1) Although segmentation is one of the tasks, quantitative evaluation for segmentation is absent.\n\n(2) The ablation study omits the following:\\\n- (a) a DPT head without the additional CNN branch,\\\n- (b) results for the full model configuration, and\\\n- (c) temporal-layer ablations.\n\n(3) In Table 1, it would be more appropriate to compare against a video-specific depth estimation model (e.g., DepthCrafter) rather than DepthAnything.\n\n3. Minor Suggestion\n\nLine 348 reads awkwardly (‚ÄúLet ùê∏ùëò be a dilated edge map extracted from the current predicted depth M_{edge} = 1 ‚àí Dilate(Ek).\") and likely nees revision."}, "questions": {"value": "1. Regarding Weakness 1: Points Requiring Clarification, would it be possible to provide additional details?\n\n2. Regarding Weakness 2: Evaluation Completeness, would it be possible to share the missing results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vcuJdo1uzR", "forum": "2xaNU3lgKC", "replyto": "2xaNU3lgKC", "signatures": ["ICLR.cc/2026/Conference/Submission5433/Reviewer_4L2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5433/Reviewer_4L2Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760899778226, "cdate": 1760899778226, "tmdate": 1762918059684, "mdate": 1762918059684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets temporally consistent, human-centric dense prediction (segmentation, depth, surface normals) in videos by pairing a large synthetic data pipeline with a ViT-based model that injects human geometric priors. It first proposes a large-scale human-centric synthetic dataset (both images and videos). Then authors propose VIT-based architecture, built from DAViD, with a temporal DPT, human geometric prior, and weight adaptation for local geometry. Experiments are conducted on THuman2.1 and Hi4D datasets, outperforming previous approaches on depth and normal estimation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Synthetic data pipeline: Builds a large human-centric data synthesis pipeline to generate diverse human-centric image and video data.\n* Simple and straightforward design: Uses a ViT backbone for feature extraction with a temporal head (DPT-style) to enforce temporal consistency, while leveraging human priors and local geometry cues.\n* Strong empirical results: competitive or superior performance across multiple benchmarks compared to prior methods (Sapiens, DAViD)"}, "weaknesses": {"value": "* **Insufficient overview of the synthetic data pipeline**. Since the dataset is a key contribution, the paper should include a clear figure of the data generation process and provide additional sample visualizations (e.g., in the supplement) to make the pipeline understandable and auditable.\n* **Missing training-data ablations**. The model is trained on a mixture of SynthHuman and the proposed dataset, but there is no study isolating data effects (e.g., only SynthHuman vs. proposed vs. mixture). Such ablations are needed to quantify the data-driven gains.\n* **Limited technical novelty**. Most components are adapted from prior work‚ÄîViT + local geometry from DAViD and the temporal DPT head from VideoDepthAnything‚Äîmaking the pipeline feel incremental rather than introducing a new technical idea.\n* The paper mentions a foreground/background segmentation component at line 192, but provides no quantitative metrics or qualitative visuals. Please include results to substantiate the claim."}, "questions": {"value": "Will the dataset be open-sourced?\n\n**Conclusion**:\nOverall, this work contributes a large-scale human-centric dataset alongside a simple, effective model for temporally consistent dense geometry. Given that the dataset is the paper‚Äôs most impactful contribution, I strongly encourage the authors to open-source the data (and generation pipeline) to maximize community benefit and reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0zJIfFqkUY", "forum": "2xaNU3lgKC", "replyto": "2xaNU3lgKC", "signatures": ["ICLR.cc/2026/Conference/Submission5433/Reviewer_xgzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5433/Reviewer_xgzd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527122994, "cdate": 1761527122994, "tmdate": 1762918059281, "mdate": 1762918059281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that extends the DAViD framework from static frames to video sequences, introducing two key components, CSE and CWA, to enhance human-centric 4D reconstruction. The approach achieves significant performance improvements, often comparable to large-scale models, while maintaining reasonable efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method demonstrates substantial quantitative gains over existing approaches across multiple benchmarks. The improvements are consistent and, in some cases, even comparable to large-scale models such as Sapiens, which highlights the effectiveness and efficiency of the proposed design.\n2. The paper successfully builds upon the previous observation from DAViD ‚Äî that ‚Äúa single high-fidelity dataset is sufficient to tackle multiple dense prediction tasks and achieve state-of-the-art accuracy.‚Äù Extending this principle from static frames to dynamic, temporal sequences is both meaningful and well-motivated, providing a clear conceptual bridge between frame-level and sequence-level human reconstruction tasks.\n3. The overall architecture and proposed components (CSE and CWA) are logically structured and supported by clear intuitions. The approach balances model complexity and performance, offering a practical solution that integrates geometric priors and adaptive weighting in a coherent way."}, "weaknesses": {"value": "1. The authors highlight a scalable data synthesis pipeline for human-centric frames and videos as one of their main contributions. However, although the pipeline is described in Section 3.1, it is not clear how novel or distinctive this approach is compared to existing dataset synthesis methods. The proposed pipeline appears fairly conventional and lacks clear justification or comparison to prior data generation frameworks.\n2. Table 4 only includes ablation results for models using either CSE or CWA individually. Including a CSE + CWA combination would provide a clearer picture of their complementary effects and help isolate the contributions of each module more effectively.\n3. The ablation studies are conducted only on the Hi4D dataset, where the performance gain is quite substantial. Additional results on the THuman2.1 dataset would verify whether CSE and CWA generalize across different data domains rather than being overfitted to Hi4D.\n4. The paper does not include inference time or computational cost comparisons between the proposed method and the baselines listed in Table 2. Reporting runtime performance, memory consumption, or FLOPs would strengthen the evaluation by demonstrating the method‚Äôs practical applicability.\n5. Although the proposed method is overall reasonable and well-structured, its main technical contributions‚Äîthe channel reweighting module (CWA) and the human geometry prior (CSE)‚Äîare relatively standard. These components resemble existing techniques in feature reweighting and prior-based guidance, which limits the methodological novelty of the paper."}, "questions": {"value": "1. Could the authors explain why the proposed method shows such a significant improvement on the Hi4D dataset‚Äîachieving results even comparable to the large-scale Sapiens model in Table 2?\n2. The paper mentions that ‚Äúthe parameter size of Sapiens-0.3B is equivalent to that of large models of ViT-based methods.‚Äù Should we then assume that DaViD, Sapiens-0.3B, and Ours-L are approximately comparable in model size? A clear statement or table comparing parameter counts would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HAJv4ISU8s", "forum": "2xaNU3lgKC", "replyto": "2xaNU3lgKC", "signatures": ["ICLR.cc/2026/Conference/Submission5433/Reviewer_5n6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5433/Reviewer_5n6R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962744019, "cdate": 1761962744019, "tmdate": 1762918059001, "mdate": 1762918059001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}