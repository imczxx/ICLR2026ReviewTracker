{"id": "jCMm98woUr", "number": 2978, "cdate": 1757310834227, "mdate": 1759898116118, "content": {"title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs", "abstract": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME’25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight–refine–solve framework.", "tldr": "", "keywords": ["Reasoning large language model", "In-context learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5efe97f9f5c78566088173ae2189c76a8f3fdcff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper identifies two causes for the poor performance of few-shot CoT in reasoning LLMs—semantic misguidance and strategy transfer failure—and introduces Insight-to-Solve, a test-time framework that extracts transferable insights from demonstrations to guide reasoning. I2S and its refined variant I2S+ consistently improve performance across open- and closed-source models, including GPT-4.1."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper comprehensively explore the underlying mechanisms behind the bad performance of few-shot CoT on RLMs, revealing the two mechanisms, semantic misguidance and strategy transfer failure.\n- This paper proposes a novel framework Insight-to-Solve, a test-time framework that extracts transferable insights from demonstrations to guide reasoning for RLMs.\n- I2S framework achieves performance improvements on GPQA and AIME across open-source models, and even achieves performance boost on AIME upon GPT4.1."}, "weaknesses": {"value": "- The evaluation benchmarks are relatively narrow, and the GPT series models show substantial improvement on only one benchmark. More comprehensive experiments are needed to demonstrate the framework’s generalizability across diverse reasoning tasks.\n- The study only reports results on Qwen3-1.7B, the smallest model in the Qwen3 series. Additional experiments on larger state-of-the-art reasoning LLMs are necessary to verify the scalability and robustness of the proposed framework.\n- Presentation and typographical issues. In Figure 2, the numerical labels overlap with the histogram bins. Minor typos are also present, such as \"expriment\" (line 359) and \"Closed-soruce\" (line 99)."}, "questions": {"value": "- The authors should clarify why only the smallest model in the Qwen3 series (Qwen3-1.7B) was chosen for evaluation. Given that larger Qwen3 \"thinking\" models are available and well-suited for reasoning tasks, evaluating them would provide a more comprehensive validation of the framework.\n- The reported results are averaged over five runs, but the paper does not include confidence intervals or statistical significance tests. The authors should provide these to assess the robustness and reliability of the observed improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MEH36cloHg", "forum": "jCMm98woUr", "replyto": "jCMm98woUr", "signatures": ["ICLR.cc/2026/Conference/Submission2978/Reviewer_J6ds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2978/Reviewer_J6ds"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945609729, "cdate": 1760945609729, "tmdate": 1762916475952, "mdate": 1762916475952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that few-shot Chain-of-Thought (CoT) demos can hurt LLMs because (i) semantic misleading makes models copy surface patterns from similar examples, and (ii) strategy-transfer failure prevents extracting and applying useful tactics to the target problem. Building on this diagnosis, the authors propose I2S (Insight-to-Solve): a sequential test-time pipeline that (a) compares target vs. demo to extract reusable insights, (b) generates a target-specific reasoning trace from those insights (not the raw demo), and (c) decouples final answering from the demos to avoid copying; I2S+ adds brief self-refinement of the trace. Across AIME’25 and GPQA and multiple models (open and closed), I2S/I2S+ outperforms direct answering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and tells a compelling story, with strong narrative flow from motivation to method. The overall presentation is smooth and coherent, and the method design feels tightly connected to the empirical findings.\n\nThe paper works on a counterintuitive yet important question: why ICL hurts reasoning. The findings are insightful and also drive the proposed method. \n\nThe I2S can also effectively leverage extra compute to further improve the reasoning performance."}, "weaknesses": {"value": "The study focuses primarily on **one-shot ICL**, which overlooks the essence of in-context learning—its strength in the **many-shot** regime where richer exemplar context is available. This limitation makes the analysis and claims feel overstated.\n\nThe section explaining why ICL hurts reasoning is not fully convincing. It relies on two qualitative observations without sufficient quantitative evidence at scale, making it unclear whether these factors truly explain the degradation in performance. The examples are also one-shot, which may not generalize to many-shot cases.\n\nThe proposed method’s scalability to **many-shot settings** remains questionable. It is unclear how the approach would effectively fuse or aggregate information from multiple exemplars, which is crucial for practical deployment in real-world ICL scenarios.\n\nSection 4.2 is a bit hard to follow since it does not explicitly show how each component is done in details. Explicitly referring to the prompts used can help. At present, the writing seems to obscure these details, which makes it harder for the reader to understand and evaluate the core mechanics of the approach."}, "questions": {"value": "Why not use model’s self-generated reasoning paths as in-context exemplars? Reasoning generated by a different teacher model may not be the best exemplar for the student to learn."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y8AqJWRtUl", "forum": "jCMm98woUr", "replyto": "jCMm98woUr", "signatures": ["ICLR.cc/2026/Conference/Submission2978/Reviewer_v1vB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2978/Reviewer_v1vB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830523693, "cdate": 1761830523693, "tmdate": 1762916475423, "mdate": 1762916475423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines why few-shot reasoning demonstrations can harm verifier-trained reasoning LLMs. It identifies two causes, semantic misleading and strategy transfer failure, that lead models to copy rather than reason. To address this, the authors propose Insight-to-Solve (I2S), which turns demonstrations into explicit, reusable insights, and I2S+, which refines reasoning for coherence. Experiments on AIME’25, GPQA, and other benchmarks show consistent accuracy gains and shorter reasoning traces, reframing demonstrations as sources of transferable insights rather than direct exemplars."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposed I2S and I2S+ frameworks, which are conceptually simple yet effective, converting demonstrations into reusable reasoning insights and optionally refining reasoning for coherence.\n- Results show consistent accuracy improvements across multiple reasoning benchmarks while reducing reasoning length by about 20%."}, "weaknesses": {"value": "- The technical novelty of I2S is limited, which is just prompting engineering. The method essentially formalizes a multi-step prompting or reasoning refinement pipeline, similar to prior “deliberation,” “reflect,” or “meta-CoT” approaches, without introducing new learning objectives, model architectures, or theoretical grounding.\n- I2S relies on additional model calls, increasing test-time latency, yet there is no discussion of compute–accuracy trade-offs or efficiency at scale.\n- The paper identifies two failure modes, semantic misleading and strategy-transfer failure,  but never quantifies their frequency or shows examples where I2S still fails.\n- ICLR 2025 -> ICLR 2026"}, "questions": {"value": "- How sensitive are the results to the phrasing of the insight-extraction and refinement prompts? Have the authors evaluated prompt robustness or tried automatic prompt tuning?\n- Since I2S and I2S+ require multiple model calls per query, could the authors report the average inference latency or total token usage to substantiate the claim that compute cost is comparable to few-shot CoT?\n- The paper identifies semantic misleading and strategy-transfer failure as two error modes. Could the authors provide quantitative statistics or examples showing where I2S still fails?\n- The context-length analysis (App. B.2, Table 5) shows reasoning demonstrations reaching up to 13–14 k tokens. How does context truncation affect performance, and could shorter exemplars achieve similar gains?\n- Is there any theoretical or empirical evidence that “insight extraction” leads to more faithful intermediate reasoning (e.g., fewer contradictions or hallucinations) compared to standard CoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7QA3aTtfu", "forum": "jCMm98woUr", "replyto": "jCMm98woUr", "signatures": ["ICLR.cc/2026/Conference/Submission2978/Reviewer_u8De"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2978/Reviewer_u8De"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854679653, "cdate": 1761854679653, "tmdate": 1762916474957, "mdate": 1762916474957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In-context learning with few shot demonstrations was the standard method to improve accuracy of LLM. However, authors found that this method is not applicable to large reasoning models (LRMs), which is already trained to reason with post training using verifier based RL (such as qwen3 and deepseek-r1). The authors identified two failure scenario: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the demonstrations and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model is unable to find reasoning strategy and apply them to target question.\n\nTo solve this, author proposed Insight-to-Solve (I2S) Framework, which turns demonstrations into insights that is reuseable and instance specific. Specifically, they first extracts abstract insights from demonstrations. Then applies them to the target question to generate a reasoning trace. The 3rd step is optional, which performs self-refinement for coherence and correctness (I2S+). In the final step, they decouples final answer generation from the demonstrations. They find that I2S outperform direct answering and I2S+ outperform other baselines (e.g., Self-Refine, Progressive-Hint Prompting) across Closed-ended benchmarks (AIME’25 and GPQA) and Open-ended tasks (GeneralReasoning Engineering and General domains)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and logically organized, making complex ideas accessible without oversimplifying.\n\nThe paper introduces a novel framework Insight-to-Solve that reimagines how reasoning demonstrations are used at inference time."}, "weaknesses": {"value": "How common are the two failure scenario? While the paper identifies two key failure scenario (semantic misguidance and strategy transfer failure), the analysis is mostly qualitative and lacks quantitative evidence. Without quantification, it’s hard to know: Are these dominant failure modes? Are there other important categories?\n\nWhy two failure scenario happen? The framework is empirically motivated, but lacks a formal theoretical explanation of why these failure scenario happens. Could the root cause be: cross-attention shortcut completion? or inductive biases toward surface similarity?\n\nWhat is the computation cost of I2S when compared to baselines? I2S introduces multi-step inference and I2S+ further introduce refinement, which makes it much more costly and lower latency, when compared to baselines such as self-refine and Progressive-Hint Prompting. Could author provide a performance plot where y axis is the accuracy and x axis is the cost (number of input/output tokens)\n\nHow good is the I2S on solving the strategy transfer failure?  Authors show the I2S performance on multiple reasoning datasets to test its reasoning ability, while the paper claims that I2S fixes strategy-transfer failure in section 4. But the motivation on strategy transfer failure is not yet tested. Existing datasets (AIME, GPQA) are single-instance reasoning, not strategy-pair tasks, and not designed to probe transfer. Can authors test their method on reasoning strategy dataset that measures strategy transfer, such as [NaturalThoughts](https://arxiv.org/html/2507.01921v1), Abstraction and Reasoning Corpus, or [ARB](https://arb.duckai.org/). ARC provides gold standard for strategy transfer: latent transformation rules and distractor patterns. ARB splits tasks into multiple subcategory: Law, MCAT Science, Physicis numerical and etc for authors to transfer."}, "questions": {"value": "Can authors share a list of transferable strategies? This can help readers understand what author actually meant"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HsFjmcVDNq", "forum": "jCMm98woUr", "replyto": "jCMm98woUr", "signatures": ["ICLR.cc/2026/Conference/Submission2978/Reviewer_EWz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2978/Reviewer_EWz4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920918561, "cdate": 1761920918561, "tmdate": 1762916474649, "mdate": 1762916474649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}