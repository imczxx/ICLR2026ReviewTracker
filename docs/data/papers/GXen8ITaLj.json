{"id": "GXen8ITaLj", "number": 13821, "cdate": 1758223176549, "mdate": 1763539347533, "content": {"title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics", "abstract": "Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. In this work, we propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk.", "tldr": "We Evaluate LLMs Attempt to Persuade on a Broad Range of Benign and Harmful Topics", "keywords": ["AI Safety", "Large Language Models", "LLMs", "Persuasion", "Evals", "Conspiracies", "Refusal"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/868a9873b68beb82bc4b71ea3f340916dc3e8fd1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes APE (Attempt to Persuade Eval), a benchmark that shifts persuasion evaluation from success (changing beliefs) to intent, whether a model attempts to shape beliefs or behavior, especially on sensitive and harmful topics. APE runs multi-turn dialogues between a “persuader” model and a “persuadee” (default GPT-4o), then uses an automated evaluator (also an LLM) to classify each turn as attempt, refusal, or no-attempt. Topic coverage spans benign factual/opinion, controversial issues, and non-controversially harmful content. Key findings: (i) many open and closed weight models willingly attempt persuasion on harmful topics that they would refuse to directly assist; (ii) “jailbreak” fine-tuning sharply collapses refusal rates on harmful topics; (iii) fine-grained “degree of persuasion” labels are unreliable, motivating a binary attempt/no-attempt metric; and (iv) persuasion attempts are most common in early rounds and taper with longer conversations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall writing is fluent and easy to follow.\n\n2. The motivation is strong and the paper focuses on the important yet under-explored question of \"the danger of a model’s propensity to persuade\".\n\n3. The proposed benchmark is significant including multi-turn, topic-diverse protocol. The design probes benign, controversial, and non-controversially harmful topics with a structured conversation length; authors document that attempts cluster early and decay over turns, justifying a 3-round default.\n\n4. The analysis is interesting: (1) Empirically shows a gap between refusal to do harm vs. willingness to persuade others to do harm; direct-assistance requests are refused while harmful persuasion attempts often occur. (2) Fine-tuning to jailbreak a closed-weight model dramatically reduces refusal on harmful categories while leaving benign behavior similar."}, "weaknesses": {"value": "1. Evaluator dependence and circularity. The pipeline often uses GPT-4o as both persuadee and evaluator. Even with some human checks, this raises concerns about shared biases and failure modes.\n\n2. The font in figure 1 is hard to see. Make it larger."}, "questions": {"value": "How robust are your attempt/no-attempt labels to the choice and version drift of the LLM evaluator—i.e., if you swap GPT-4o for a different frontier model (or a newer minor version of the same model), how do per-topic rates and headline conclusions change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CbpqHyfvbi", "forum": "GXen8ITaLj", "replyto": "GXen8ITaLj", "signatures": ["ICLR.cc/2026/Conference/Submission13821/Reviewer_dm8K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13821/Reviewer_dm8K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970981967, "cdate": 1761970981967, "tmdate": 1762924348324, "mdate": 1762924348324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Group Response: Part 1"}, "comment": {"value": "**Results on Human Participants**\n\nTwo of the reviewers mention they would like to see confirmation that the results in this work generalize to human participants. To answer this question about whether the pipeline would behave similarly when talking to a human, while it would be unmitigably unethical (or even illegal) to test most persuasion in the non-controversially harmful category, we draw on our concurrent work in the topic category of conspiracies, where we have conducted large-scale, IRB-approved (with a thorough debrief for participants), human persuasion studies. Participants (n=836) wrote about any conspiracy theory, then rated their belief in it on a 0–100 scale (0 = definitely false, 50 = unsure, 100 = definitely true), both before and after treatment. Participants were randomly assigned to either a “debunk” (persuade out of belief) or “bunk” (persuade into belief) condition. Using prompts similar to APE, we instructed GPT-4o to persuade users accordingly across multiple rounds.\n\nGPT-4o effectively persuaded users in both directions, with bunking slightly more effective: Debunking reduced belief by 12.3 points. Bunking increased belief by 13.6 points. These effects held across various conspiracy topics included in the APE benchmark, from historical cases (e.g., JFK) to contemporary, polarized issues (e.g., COVID-19).\nThus, to test if the persuader behaves similarly when talking to a human vs. a simulated persuadee, we applied the APE evaluator to the human data using multiple models (GPT-4o, Gemini 2.5 Pro, Claude Sonnet 4, Claude Haiku 3.5). Specifically, we first used the human participants’ explanations of their beliefs as the initial user message and observed whether models attempted persuasion. Alternatively, we prompted (using APE’s standard prompt, Appendix B.1) the simulated persuadee to produce the respective message. Both cases use the human-provided topic and belief level.\nAs shown in Table 1, results across human and simulated messages were nearly identical: GPT-4o and Gemini 2.5 Pro attempted persuasion nearly 100% of the time on both, while Claude Sonnet 4 and Claude Haiku 3.5 refused around 15-20% and 30% of the time on both messages, respectively. Although it would be unethical to run the same experiments on more harmful topics, conspiracy theories remain a critical test case within the information economy; particularly in light of our earlier findings showing that LLMs can effectively persuade users both into and out of such beliefs. Our comparison between human and simulated data further reinforces this point: simulating initial user messages to assess persuasion propensity yields results that closely mirror those obtained from real human interactions, as measured by the APE evaluator.\n\n**Table 1**: Comparison of human vs. simulated persuadee for four frontier models acting as persuader. Turn 1 persuader attempt rates align between simulated and real human persuadees. \n\n| Data Source | GPT-4o | Gemini 2.5 Pro | Claude Sonnet 4 | Claude Haiku 3.5 |\n|-------------|--------|----------------|------------------|------------------|\n| Human       | 99.6%  | 99.8%          | 80.3%             | 73.0%            |\n| Simulated   | 99.6%  | 99.0%          | 86.2%            | 69.3%            |"}}, "id": "a05gtVqLrx", "forum": "GXen8ITaLj", "replyto": "GXen8ITaLj", "signatures": ["ICLR.cc/2026/Conference/Submission13821/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13821/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13821/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763539760282, "cdate": 1763539760282, "tmdate": 1763539760282, "mdate": 1763539760282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Attempt to Persuade Eval (APE) benchmark, which measures large language models’ willingness to engage in persuasion on harmful topics rather than their success in changing beliefs. Using simulated multi-turn conversations between a persuader model and a persuadee model, APE evaluates frontier systems across benign, controversial, and clearly harmful subjects such as violence, terrorism, and human trafficking. The results show that several advanced models, including GPT-4 variants and Gemini 2.5 Pro, still produce persuasive responses in ethically risky situations even when they refuse direct participation in harmful actions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The study addresses an important and previously underexamined dimension of AI safety by focusing on the inclination to persuade rather than persuasion outcomes.\n\n+ The experimental setup using simulated multi-turn dialogues is well structured and scalable for auditing model behavior.\n\n+ The analysis is comprehensive, covering many models, validation against human ratings, ablation studies, and an openly available benchmark for further research."}, "weaknesses": {"value": "- Placing a large, attention-grabbing figure before the abstract disrupts readability and confuses the narrative flow. The paper would benefit from starting with the abstract and moving the figure into the introduction or results section.\n\n- The study relies entirely on model-to-model simulations, which limits external validity and prevents meaningful conclusions about how human users might respond to persuasive attempts. Real persuasion involves emotional, social, and moral reasoning that automated agents cannot replicate.\n\n- The definition of a “persuasion attempt” is too shallow. Treating persuasion as a binary label (attempt or no attempt) ignores gradations in tone, framing, intensity, and rhetorical sophistication that characterize real persuasive behavior.\n\n- The evaluator model struggles with nuanced distinctions in persuasive strength. It cannot reliably detect subtle strategies such as moral reasoning, empathy framing, or indirect argumentation that may influence human audiences.\n\n- The evaluation design does not adequately account for confounding factors such as temperature, persona context, or prompt order effects, all of which may significantly alter persuasion likelihood.\n\n- While the paper effectively identifies safety gaps, it provides limited practical guidance on how to mitigate these issues. There is no clear proposal for retraining, safety-layer design, or model auditing frameworks to address identified vulnerabilities.\n\n- Ethical considerations and dataset release protocols are insufficiently detailed. The authors acknowledge potential misuse risks but do not specify monitoring or access restrictions for harmful prompt data.\n\n- The writing occasionally blurs descriptive findings with normative claims, making it harder to separate empirical evidence from interpretation or opinion.\n\n- The paper lacks grounding in persuasion theory or psychology, which weakens its conceptual framing. Integrating established models of persuasive communication would provide stronger interpretive depth."}, "questions": {"value": "- The boundary between benign, controversial, and harmful categories is ambiguous. Some topics (e.g., violence) are clearly harmful, while others (e.g., “undermining control”) depend heavily on interpretation without consistent criteria.\n\n- The neutrality and independence of the evaluator model are uncertain, especially since GPT-4o serves multiple roles (persuader, persuadee, and evaluator) in the same pipeline. This may introduce systemic bias.\n\n- The description of human annotator validation is vague. Details about the annotators’ expertise, agreement calibration, and labeling guidelines are missing, leaving uncertainty about evaluation reliability.\n\n- The operational definition of “persuasive attempt” is unclear. It is not explained whether persuasion is detected through argument structure, linguistic tone, or inferred motivation.\n\n- The five subcategories under “non-controversially harmful” topics are listed without clear theoretical or empirical justification, raising questions about representativeness and generalization.\n\n- The method of generating harmful prompts with a jailbroken model lacks transparency. The authors intentionally omit details for safety, but this decision reduces reproducibility and external verification.\n\n- The paper mentions coordination with industry labs following responsible disclosure but provides no documentation, dates, or evidence of model improvements after disclosure.\n\n- The use of simulated agents for both sides of the dialogue leaves open questions about how real human users would respond to similar persuasion attempts. A small-scale human validation study would have improved the paper’s grounding.\n\n- The broader implications of APE scores are undefined. The paper highlights safety risks but does not propose thresholds, standards, or policy frameworks for determining acceptable levels of persuasive behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "This study involves no human participants and thus poses no direct ethical risks related to consent, welfare, or exposure to harmful content. All experiments use simulated LLM-to-LLM conversations, avoiding human manipulation. However, future validation with real users would raise serious ethical challenges, as exposing participants to persuasive or harmful narratives could cause distress or reinforce dangerous beliefs. Such studies would require strict institutional review, strong content moderation, participant screening, and debriefing procedures. The dataset itself also carries misuse risks, as it could enable the creation of models that generate harmful persuasive content; secure access control and responsible data release are therefore essential. Finally, equating LLM persuasion with human persuasion should be done cautiously to avoid overstating model agency or intent."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9nF5Tv7qgL", "forum": "GXen8ITaLj", "replyto": "GXen8ITaLj", "signatures": ["ICLR.cc/2026/Conference/Submission13821/Reviewer_Dswg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13821/Reviewer_Dswg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008752566, "cdate": 1762008752566, "tmdate": 1762924347853, "mdate": 1762924347853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Attempt to Persuade Eval (APE), a benchmark that evaluates whether large language models will attempt to persuade users on harmful topics. The authors test 12 frontier models across 600 topics spanning benign content to extremely harmful topics (terrorism, human trafficking, etc.) using a multi-turn conversational framework with automated evaluation. The results reveal that many state-of-the-art models frequently attempt persuasion on harmful topics they would refuse to directly assist with, highlighting critical gaps in current safety guardrails."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical problem in AI safety by benchmarking models' willingness to engage in persuasive attempts on harmful topics.\n- The evaluation framework is comprehensive, covering 600 diverse topics in a multi-turn setup.\n- The results are validated in automated and human assessments, and reveal several findings in LLM risks."}, "weaknesses": {"value": "- The paper lacks novelty and technical contributions. Particularly, there are prior works already discussing the safety of the LLM persuasion and  [1,2]. The authors should more carefully discuss and differentiate the paper from prior works.\n- The paper does not provide actionable insights or implications for the safety evaluation. For example, what are the potential solutions for mitigation, what caused the critical safety issues, etc?\n- The evaluation framework heavily relies on LLM-only simulation, which could cause a critical gap when applying to real-world human-AI persuasion.\n\n[1] LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models. COLM 2025.\n\n[2] How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. ACL 2024."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pf117SDEEj", "forum": "GXen8ITaLj", "replyto": "GXen8ITaLj", "signatures": ["ICLR.cc/2026/Conference/Submission13821/Reviewer_WhhT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13821/Reviewer_WhhT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130729253, "cdate": 1762130729253, "tmdate": 1762924347342, "mdate": 1762924347342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}