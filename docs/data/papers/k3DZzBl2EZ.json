{"id": "k3DZzBl2EZ", "number": 3125, "cdate": 1757338591682, "mdate": 1763694740516, "content": {"title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "abstract": "Recent advances demonstrate that multimodal large language models (MLLMs) exhibit strong multimodal in-context learning (ICL) capabilities, enabling them to adapt to novel vision-language tasks from a few contextual examples. However, existing ICL approaches face challenges in reconciling generalization with robustness across diverse tasks and noisy contextual examples: manually selecting examples produces clean contexts but is labor-intensive and task-specific, while similarity-based retrieval improves scalability but could introduce irrelevant or structurally inconsistent samples that degrade ICL performance. To address these limitations, we propose ContextNav, the first agentic framework that integrates the scalability of automated retrieval with the quality and adaptiveness of human-like curation, enabling noise-robust and dynamically optimized contextualization for multimodal ICL. ContextNav unifies context management and noise-robust contextualization within a closed-loop workflow driven by graph-based tool orchestration. Specifically, it builds a resource-aware multimodal embedding pipeline, maintains a retrievable vector database, and applies agentic retrieval and structural alignment to construct noise-resilient contexts. An Operational Grammar Graph (OGG) further supports adaptive toolchain planning and optimization, enabling the agent to refine its strategies based on downstream feedback. Experimental results demonstrate that ContextNav achieves state-of-the-art performance across various datasets, underscoring the promise of agentic workflows for advancing scalable and robust contextualization in multimodal ICL. The code will be made publicly available on GitHub.", "tldr": "", "keywords": ["agent system", "in-context learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37aab6324705e16aaeccd571b0413e0a39369341.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ContextNav, a novel Chain-of-Thought (CoT)-driven navigation framework designed for multimodal embodied question answering (Embodied QA). The core idea is to augment a large multimodal model (LMM) with an explicit reasoning chain that can be dynamically adapted to changing visual and textual contexts.\n\nThe authors introduce a new benchmark called CEN (Contextual Embodied Navigation), which requires agentic navigation in 3D environments (based on AI2-THOR) conditioned on both linguistic queries and evolving visual observations. ContextNav is shown to outperform baselines such as VLN-BERT, RecCoT, and classical map-based agents, with analysis showing improvements in long-horizon reasoning and policy transferability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on something quite underexplored—how to make vision-language agents actually reason and act over time, rather than just respond to single inputs. The focus on embodied QA with CoT-style reasoning feels fresh and highly relevant.\n\nMost CoT papers just use reasoning chains to get better answers. Here, it’s used to drive navigation—deciding where to go next, based on what’s already seen and reasoned about. That’s a neat twist on the usual setup.\n\nI like that the authors didn’t just propose a model—they also introduced a new benchmark (CEN) that requires both visual and linguistic context to solve. This feels realistic and useful to the broader community.\n\nThe method outperforms several strong baselines (VLN-BERT, RecCoT, etc.), especially on long-horizon tasks. The gains are consistent, and not just on cherry-picked examples.\n\nThe paper includes helpful qualitative visualizations of how CoT steps evolve, how memory is used, and what decisions are made. This makes the agent’s reasoning process transparent and auditable."}, "weaknesses": {"value": "The paper combines CoT + memory, but there’s no solid ablation to tell which part really matters. It would help to show performance with and without the memory module.\n\nSince the system heavily relies on the underlying vision-language model (e.g., for perception, grounding, reasoning), it’d be helpful to test or at least discuss what happens if you switch to a smaller or weaker LMM.\n\nSome examples suggest that the model “imagines” object positions it hasn’t actually seen. Are there ways to ground the reasoning better or prevent hallucinations?\n\nCoT is generated at every step, which might be slow in real-world settings. But there’s no mention of how fast inference is, or whether it can run in real-time. It’s a bit hard to reproduce the method without more info. There’s no pseudocode, no clear prompt templates, and no info on model size, inference cost, or environment config. Some of this could go in the appendix."}, "questions": {"value": "Could you provide an ablation where you disable the memory module? How much does it actually contribute compared to CoT alone?Which specific LMM is used, and did you try smaller or open-source models? How much does performance depend on having a “strong” LMM?\n\nAre there known cases where the CoT causes the agent to make wrong or speculative moves due to hallucinated beliefs? How long does one inference step take? Can this be deployed in a near real-time scenario? Could the benchmark (CEN) be extended to tasks beyond navigation—like multi-room exploration, tool use, or question answering with object manipulation?\n\nWould you consider releasing pseudocode or a Colab to help others reproduce the results? If these questions are addressed in detail, especially around memory vs. CoT contribution and scalability, I’d definitely consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PD2FtR7JZJ", "forum": "k3DZzBl2EZ", "replyto": "k3DZzBl2EZ", "signatures": ["ICLR.cc/2026/Conference/Submission3125/Reviewer_tPU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3125/Reviewer_tPU8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760536492656, "cdate": 1760536492656, "tmdate": 1762916562560, "mdate": 1762916562560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ContextNav, an agent-based framework that helps multimodal large language models learn more effectively from examples in context. Instead of simply retrieving similar samples, ContextNav acts like a curator that selects, filters, and reorganizes examples to reduce noise and improve relevance. It combines automated retrieval with human-like reasoning through three modules: 1. agentic context management to build and update a multimodal database, 2. noise-robust contextualization to remove irrelevant or inconsistent samples, and 3. graph-driven tool orchestration to plan and optimize the workflow based on feedback. Experiments across several vision–language benchmarks show that ContextNav consistently boosts in-context learning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and clearly written, making it easy to follow.\n2. Multimodal in-context learning is a relatively new research area, and the role of agent-based methods in this domain has been largely unexplored.\n3. Experimental results across multiple datasets demonstrate that the proposed approach achieves consistent and notable improvements over baseline methods."}, "weaknesses": {"value": "1. My main concern is that the paper primarily focuses on workflow construction, leveraging LLMs and VLMs to build a pipeline for data collection, embedding generation, dataset management, and demonstration selection. While this represents significant engineering effort, the work places less emphasis on understanding the underlying mechanisms of multimodal in-context learning itself. The research mainly centers on how to select the most useful examples for the MLLM, but multimodal ICL faces a deeper fundamental challenge, such as sensitivity to example choice and order, which I think requires theoretical or empirical analysis beyond pipeline design. Without addressing these core issues, the proposed system may only mitigate rather than resolve the inherent limitations of ICL. I encourage the authors to complement their strong engineering contribution with deeper insights into the nature of multimodal ICL.\n\n2. The agentic pipeline introduces additional inference steps and latency (around 3 seconds per iteration), which may limit scalability or real-time applicability. Moreover, as mentioned earlier, while the use of an agent can mitigate some issues in multimodal in-context learning, it does not fundamentally solve them. To truly address these challenges, future work may need to incorporate a learning-based or adaptive optimization component that allows the system to internalize and improve its contextualization strategy rather than relying solely on procedural orchestration."}, "questions": {"value": "I’m not suggesting additional experiments, but I’m curious: do you have any thoughts on how RL could be integrated into the proposed agent framework to reduce overhead or further improve the agent’s performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DtjNeFAAmG", "forum": "k3DZzBl2EZ", "replyto": "k3DZzBl2EZ", "signatures": ["ICLR.cc/2026/Conference/Submission3125/Reviewer_q6J8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3125/Reviewer_q6J8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681046565, "cdate": 1761681046565, "tmdate": 1762916562330, "mdate": 1762916562330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ContextNav, a framework for multimodal ICL that selecting demonstration examples via an agentic workflow. Key steps include semantical denoising through agentic coherence checking and structural realignment to query format. Empirical results on composite and single-task VL benchmarks across multiple MLLMs show gains over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of agentic AI for multimodal ICL is interesting and timely. Using agentic workflows to improve demonstration examples is reasonable. \n2. The use of benchmark datasets across broad tasks and different families of MLLMs (including both open and close models) is good."}, "weaknesses": {"value": "1. There is one important baseline missing: many-shot ICL (https://arxiv.org/abs/2405.09798v2). Instead of careful curation, it's important to see how much gain we can have compared with including all initial candidates in the context. The decision to limit to 8-shot is not well justified. \n2. Besides VL-ICL, Cache of Thought (https://arxiv.org/abs/2502.20587) should also be included as a baseline.\n3. The introduction of term \"Operational Grammar Graph\" seems unnecessary, making the paper harder to understand. It's not explained until very late (Page 5) although it has been referred 3 times before. I'll recommend not introducing new terms or at least providing a clear explanation at an early stage.\n4. The resampling of datasets for test is not justified. The use of 3:7 difficulty ratio also sounds arbitary, making the comparison with other methods harder.\n5. The memory and toolchain optimization is not explained clearly. How does it help? The results also do not show the benefit of this learning.\n6. The validity of structural alignment (whether semantically equivalent) is not shown experimentally."}, "questions": {"value": "1. Figure 2 seems an important figure for the paper. It might be good to put it in Page 1 or 2 (instead of Page 4).\n2. The term \"Noise-Robust Contextualization\" is not intuitive. Can we use simpler terms like \"Example Refinement/Selection\" or \"Context Denoising\"? Please try not to introduce too many new terms which are not intuitive to understand.\n3. [Line 283]  Also, the sample size of 803 for BlindTest seems arbitary. Is there any reason for this number?\n4. Why random sampling leads to negative ICL gain compared with vanilla MLLM? Adding examples even random sampled examples should help?\n5. Does vanilla MLLM means zero-shot? If yes, \"zero-shot\" seems a more common term.\n6. How are semantic and structural noise proportions computed? Which prompt in Appendix D is used, respectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ojuiTLr1s", "forum": "k3DZzBl2EZ", "replyto": "k3DZzBl2EZ", "signatures": ["ICLR.cc/2026/Conference/Submission3125/Reviewer_zbjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3125/Reviewer_zbjE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878347244, "cdate": 1761878347244, "tmdate": 1762916561039, "mdate": 1762916561039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify limitations in current Multimodal In-Context Learning (ICL) approaches, which either rely on manual selection of support examples or naive similarity-based retrieval that risks including semantically or structurally irrelevant examples. They introduce ContextNav, an agentic framework that treats multimodal contextualization as a closed-loop process. ContextNav constructs and maintains a resource-aware vector database, retrieves candidate examples, and refines them through Agentic Retrieval (semantic filtering) and Structural Alignment (format harmonization) before passing them to the target multimodal model. Its orchestration is governed by an Operational Grammar Graph (OGG) and updated using feedback-driven memory, enabling adaptive tool sequencing. Across a variety of datasets and models, ContextNav achieves roughly 16–17% ICL gains over baselines and provides systematic ablations on each module’s contribution. The work is significant in demonstrating that agentic data curation can enhance multimodal ICL performance, though the framework’s dependence on large models for reasoning introduces practical cost and latency considerations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality:\n- The paper introduces the first agentic framework for multimodal ICL that unifies retrieval, noise filtering, and structural alignment under a closed-loop, graph-driven orchestration scheme.\n- The work also broadens the definition of data-centric ICL by explicitly addressing semantic and structural noise in retrieved support examples.\n2. Quality (technical soundness and empirical rigor):\n- The methodology is well-grounded and reproducible, with explicit mathematical formulations for each module (embeddings, retrieval, agentic filtering, structural alignment, and feedback updates).\n- Experiments are extensive and systematic, spanning diverse datasets (e.g., BlindTest, CLEVR, TextOCR, MathVision) and models (both open and closed weights).\n- The ablation studies convincingly isolate the contribution of each component (Agentic Retrieval, Structural Alignment, and OGG-guided orchestration).\n3. Clarity:\n- The framework is well-presented, with clean notation, functional forms, and cross-referenced equations that make the workflow easy to follow.\n- The appendices (especially the tool library, OGG edges, and prompt templates) greatly enhance transparency and reproducibility.\n4. Significance:\n- The research makes a substantive contribution to the evolution of multimodal ICL, demonstrating that agentic data curation can unlock additional performance from foundation models without architectural changes or retraining.\n- By showing that careful control over context quality can yield consistent gains across a variety of MLLMs, the paper provides both scientific insight and practical implications for ICL advancement."}, "weaknesses": {"value": "1.\tPartial dependence on proprietary MLLMs for agentic reasoning:\nAlthough the evaluation includes several open-source target models (e.g., Qwen2.5-VL-7B, Phi-3.5-V, InternLM-X2.5), the framework’s agentic orchestration relies primarily on proprietary Gemini models. This limits full reproducibility of the agentic pipeline (especially for researchers without access to the relevant APIs) and makes it difficult to assess whether smaller or open-weight policies could effectively substitute as the reasoning backbone.\n\n2.\tLimited inclusion of real-world, noisy multimodal benchmarks:\nThe evaluation focuses primarily on curated or synthetic datasets (e.g., CLEVR, TextOCR, MathVision), which, while controlled, may not capture the heterogeneity and noise found in large-scale real-world multimodal corpora. For a framework explicitly designed to perform noise-robust agentic retrieval, it would be compelling to demonstrate performance on web-scale datasets such as LAION-5B, Conceptual Captions (CC12M), or RedCaps, which contain substantial captioning noise, visual ambiguity, and cross-modal misalignment. Evaluating ContextNav under such conditions would more convincingly validate its claimed advantages in semantic filtering and structural alignment for real-world multimodal reasoning.\n\n3.\tUse of separate unimodal embedding models for multimodal retrieval:\nThe framework employs distinct text and vision embedding models instead of a unified multimodal encoder. Although modular, this design may weaken cross-modal semantic coherence. Using joint multimodal embeddings (e.g., CLIP/BLIP-style) could yield more consistent retrieval and lessen the need for post-hoc structural alignment.\n\n4.\tLack of ablations on orchestration dynamics:\nThe ablations presented effectively isolate each component’s contribution, but they do not analyze how different toolchain paths or OGG planning strategies affect results. A deeper look into orchestration dynamics would clarify how the agent’s reasoning evolves.\n\n5.\tLack of uncertainty quantification or statistical robustness analysis:\nReported results omit error bars, confidence intervals, or variance measures, even though stochasticity exists in retrieval and model inference. Statistical reporting would help confirm that the observed improvements are consistent and reproducible."}, "questions": {"value": "Questions:\t\n1. Could you elaborate on the dataset selection procedure? Specifically, how were the composite-task and single-task datasets chosen, and how do they represent the diversity of multimodal reasoning tasks ContextNav aims to address?\n2.\tFor a framework that emphasizes noise robustness, what was the motivation to use curated datasets rather than noisy corpora? \n3.\tWere there any internal ablations or pilot experiments that compared joint multimodal embeddings against the chosen unimodal embedding pair (Qwen3-Embedding + Qwen2.5-VL encoder)? If so, what performance or resource trade-offs motivated the current design?\n4.\tWhat specifically informed the choice of Gemini models for agentic orchestration? Which other LLMs or MLLMs (e.g., GPT-4o, Qwen2.5-VL-7B) were tested or considered as policy backbones?\n5.\tThe Operational Grammar Graph (OGG) is central to the framework. Could you clarify whether its edge definitions were hand-crafted? How sensitive is ContextNav’s performance to OGG design (e.g., missing or redundant tool dependencies)? \n6.\tWere confidence intervals computed for Table 1 or Figure 3 results? \n7.\tAre there any shared patterns across ContextNav failure modes (e.g., ambiguous queries, highly compositional tasks etc.)?\n\nRebuttal suggestions:\\\n1.\tCould ContextNav’s agentic orchestration be run without proprietary Gemini models, potentially replacing with an open-weight model for the agentic reasoning steps? If performed, please report changes in: (i) semantic/structural noise rates, (ii) ICL gain (%), (iii) token cost and latency.\\\n2.\tSince the framework’s motivation is noise-robust contextualization, please test ContextNav on a noisy, web-scale dataset (some suggestions provided above. A quantification of the semantic/structural noise before and after Agentic Retrieval, and comparison of ICL gains versus VL-ICL and MMICES under the same noisy conditions would be useful.\\\n3.\tCurrent retrieval uses separate text and vision encoders. Please add results using a joint vision-language encoder.\\\n4.\tPlease add error bars or confidence intervals for key metrics (Table 1, Fig. 3, and Fig. 4). \\\n5.\tPlease provide a small figure or table showing ICL gain vs. token cost / latency per model (Phi-3.5V, Qwen2.5-VL, Gemini-2.0-flash, GPT-4o). This will help gauge efficiency and practical deployment feasibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMgBFtb2hd", "forum": "k3DZzBl2EZ", "replyto": "k3DZzBl2EZ", "signatures": ["ICLR.cc/2026/Conference/Submission3125/Reviewer_ppfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3125/Reviewer_ppfZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978808687, "cdate": 1761978808687, "tmdate": 1762916560862, "mdate": 1762916560862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}