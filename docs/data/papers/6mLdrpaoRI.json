{"id": "6mLdrpaoRI", "number": 10780, "cdate": 1758181701669, "mdate": 1759897629249, "content": {"title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "abstract": "While transfer learning is an advantageous strategy, it overlooks the opportunity to leverage knowledge from numerous available models online. Addressing this multi-source transfer learning problem is a promising path to boost adaptability and cut re-training costs. However, existing approaches are inherently coarse-grained, lacking the necessary precision for granular knowledge extraction and the aggregation efficiency required to fuse knowledge from either a large number of source models or those with high parameter counts. We address these limitations by leveraging Singular Value Decomposition (SVD) to first decompose each source model into its elementary, rank-one components. A subsequent aggregation stage then selects only the most salient components from all sources, thereby overcoming the previous efficiency and precision limitations. To best preserve and leverage the synthesized knowledge base, our method adapts to the target task by fine-tuning only the principal singular values of the merged matrix. In essence, this process only recalibrates the importance of top SVD components. The proposed framework allows for efficient transfer learning, is robust to perturbations both at the input level and in the parameter space (e.g., noisy or pruned sources), and scales well computationally.", "tldr": "", "keywords": ["model merging", "transfer learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d73a6f3b1c8f63ec4c272f8b67f14cd09ab7e209.pdf", "supplementary_material": "/attachment/e9bf8883d780871ead346ac4b3be7fe3826e8a1e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AXIS, a model-merging framework designed to efficiently integrate knowledge from multiple fine-tuned models for transfer to new target tasks. AXIS leverages Singular Value Decomposition (SVD) to decompose and aggregate task-specific models trained on different datasets, enabling more effective adaptation to unseen tasks. By fine-tuning only the singular values of the merged model, AXIS efficiently consolidates essential knowledge from multiple source tasks while significantly reducing storage and computational costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The SVD-based rank-1 component selection enables fine-grained and interpretable model merging, substantially improving over traditional fusion methods.\n\n2. The one-time aggregation stage effectively consolidates knowledge, while subsequent fine-tuning operates on a fixed-size matrix, ensuring constant training and memory costs regardless of the number of source models.\n\n3. The method demonstrates robustness under challenging conditions.\n\n4. Comprehensive experiments and comparisons with aTLAS validate the effectiveness and efficiency of the proposed AXIS framework."}, "weaknesses": {"value": "1. The proposed method assumes that all source models share the same architecture and originate from the same pre-trained backbone, which is a restrictive and unrealistic assumption. The approach cannot currently merge models of different architectures (e.g., CNNs) or even different scales of Vision Transformers. This limits its applicability in heterogeneous or cross-architecture settings—arguably the most promising real-world use cases.\n\n2. Although the paper provides extensive experiments, it only compares AXIS with aTLAS. This comparison set is not sufficient to fully establish the method’s advantages. Including other representative baselines—such as knowledge distillation, model stitching, or parameter-efficient fine-tuning (PEFT) approaches—would make the evaluation more convincing.\n\n3. All experiments are conducted on relatively small-scale vision datasets, which share high inter-task similarity. Evaluating on larger-scale datasets or across modalities (e.g., text or multimodal benchmarks) would better test the generalization and scalability of AXIS. Especially in NLP tasks, where task heterogeneity is more pronounced, the benefits of multi-source knowledge integration might be more clearly demonstrated.\n\n4. While SVD provides a solid mathematical foundation, the choice of Top-K singular components is based primarily on empirical intuition. The paper would be strengthened by theoretical or empirical analysis demonstrating why high-magnitude components consistently correspond to transferable knowledge.\n\n5. The overall layout of this article is a bit messy."}, "questions": {"value": "1. Could low-magnitude singular components still contain task-specific or complementary knowledge that may be useful for transfer?\n\n2. Do the selected singular vectors possess interpretable semantics across tasks? Some visualization or qualitative analysis on vision datasets would help illustrate this.\n\n3. How could AXIS be extended to support cross-scale or cross-architecture model merging?\n\n4. Can the method be applied or adapted to textual or multimodal datasets to demonstrate broader generality?\n\n5. Although AXIS achieves parameter efficiency, the FLOPs are not significantly reduced. Could the authors explore further compression to achieve both effective knowledge merging and efficient deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IWGtGYtHI8", "forum": "6mLdrpaoRI", "replyto": "6mLdrpaoRI", "signatures": ["ICLR.cc/2026/Conference/Submission10780/Reviewer_LXNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10780/Reviewer_LXNa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459331858, "cdate": 1761459331858, "tmdate": 1762921991264, "mdate": 1762921991264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for parameter-efficient fine-tuning. It integrates the knowledge from multiple source models which are fine-tuned on different downstream tasks. These fine-tuned models (represented by their parameter difference from the pre-trained model) serve as the knowledge source for the target task. SVD is performed for each parameter difference, and the top-K components with highest singular values are selected to reconstruct the merged difference matrix $\\Delta_m$, which is further used as the starting point of target task fine-tuning. The second stage involves fine-tuning on top singular values after decomposing the sum of the pre-trained parameter and $\\Delta_m$. Experiments on a diverse set of benchmarks are conducted to compare the proposed method AXIS and aTLAS. Results show that AXIS not only achieves overall higher accuracy in terms of test accuracy, but also requires less computation and memory cost, especially when the number of task vectors increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea is interesting and achieves better empirical results compared to the baseline aTLAS on both accuracy and complexity. \n2. The paper is well written. The method is described clearly. \n3. The analytical experiments help readers to understand how the important factors affect the performance of the algorithm."}, "weaknesses": {"value": "1. Despite the good performance, the motivation and rationale of the proposed method seem not fully convincing. Particularly, why is the most transferable useful knowledge for the target task represented by the principal singular components (as stated in line 160-161)? Is there any theoretical insight or empirical evidence for this? I didn’t find explanations for this, but the analytical results seem to be conflict with the motivation stated in line 160-161. Specifically, results in Table 1 show that a simple averaging for Stage 1 already shows good enough results (especially for N=10%/20%, and all results are much better than aTLAS), implying that the singular value fine-tuning in Stage 2 is the dominant factor for the success of the proposed method rather than the low-rank reconstruction. Table 2 also shows that Arbitrary and Bottom components are almost as good as the top components. \n2. While the Stage 2 strategy seems to contribute a lot to the whole framework, the idea of singular value fine-tuning has already been explored [1].\n\n[1] Singular Value Fine-tuning: Few-shot Segmentation requires Few-parameters Fine-tuning. (NeurIPS 2022)"}, "questions": {"value": "1. Why does AXIS achieve higher accuracy than aTLAS with less runtime and memory? \n2. How did you get the source task vectors described in line 238? Are they from the same datasets used in the experiments or external datasets?\n3. In Figure 11, why does the performance of the w/o group drop dramatically when using a moderate number of task vectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iBgHQP5oR9", "forum": "6mLdrpaoRI", "replyto": "6mLdrpaoRI", "signatures": ["ICLR.cc/2026/Conference/Submission10780/Reviewer_R9qd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10780/Reviewer_R9qd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528761368, "cdate": 1761528761368, "tmdate": 1762921990357, "mdate": 1762921990357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AXIS, a two-stage framework for multi-source transfer via SVD-based component extraction and aggregation, followed by parameter-efficient adaptation. AXIS reports higher average accuracy and improved variance, constant memory/runtime during adaptation, and robustness to noisy/pruned task vectors and masked-patch inputs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear component-level view of model merging: extract rank-1 atoms across many sources, globally rank, then merge and adapt, which is a granular alternative to full-vector merging such as aTLAS.\n\n2. If broadly valid, AXIS provides a recipe for fusing many fine-tuned models into a compact, adaptable basis, which is useful for multi-source transfer with predictable compute."}, "weaknesses": {"value": "1. In lines 98-101, the authors claimed that 'While numerous works explore combining models’ weights with diverse architectures..., these often prove less effective than approaches that assume all considered models originate from the same base model'. I would say that these two lines of work target different scenarios. Using the shortcomings of one family of methods as the reason for choosing the other in its own application setting is odd and does not constitute a sound motivation.\n\n2. The definition is confusing. In line 131, task vector is defined as τi, and in line 143, ∆i is used to define task matrix. But in line 152, ∆i is used to referred as task vectors. \n\n3. In Figure 10, the results indicate that AXIS scales consistently with the number of trained parameters. In the case N% = 100%, how does the method perform? Based on the trend, performance appears to surpass standard full-parameter fine-tuning (the baseline), which seems counterintuitive. Where exactly does this performance gain come from?\n\n4. When confronted with multiple unseen tasks, the method appears to require restarting training for each task. Can the authors propose any improvements to training efficiency in this setting? Also, What is the transferability of the learned matrix across tasks? how well does the learned AXIS perform when directly applied to other unseen tasks without retraining?\n\n5. The choice of K in K% appears to rely heavily on post-hoc judgment and manual selection.\n\n6. The manuscript contains substantial unused white space (e.g., the right side of column 96, the area above Fig. 7, the space below Table 1, and below Fig. 10). Please tighten the layout and ensure figures/tables are placed without large gaps. Also, there are several typos, such as line 219 (‘algorithm ?? and figure 2’). \n\n7. Please give a compact, layer-wise definition (matrices vs. biases) and a consistent symbol table, and clarify how non-matrix params are handled (you mention averaging). A small per-layer example would help."}, "questions": {"value": "See weakness. I'm willing to increase the rating once my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6sQRQGfAx3", "forum": "6mLdrpaoRI", "replyto": "6mLdrpaoRI", "signatures": ["ICLR.cc/2026/Conference/Submission10780/Reviewer_eMxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10780/Reviewer_eMxV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888231888, "cdate": 1761888231888, "tmdate": 1762921989483, "mdate": 1762921989483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the AXIS framework to address limitations in multi-source knowledge transfer, such as the coarse granularity, low efficiency, and poor robustness of existing methods like aTLAS.\nAXIS follows a two-stage workflow. In the first stage (knowledge extraction and aggregation), each source task matrix is decomposed using Singular Value Decomposition to obtain elementary components. It then selects the most salient top-K components through global ranking and aggregates them into a single merged matrix. In the second stage (target task adaptation), the merged matrix undergoes Singular Value Decomposition again. Only the top-N principal singular values are fine-tuned for the target task, while other components remain frozen to ensure parameter efficiency.\nExperiments on 21 image classification tasks using ViT-B-32 and ViT-L-14 architectures show that AXIS outperforms aTLAS in accuracy while maintaining constant memory usage and runtime. Moreover, AXIS demonstrates greater robustness against source parameter noise or sparsity and is more resilient to input degradation. It also surpasses parameter-efficient fine-tuning (PEFT) methods such as LoRA—all while relying on the same shared pre-trained model architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "AXIS resolves the efficiency bottleneck of aTLAS where memory and runtime increase linearly with the number of source tasks by splitting multi-source knowledge transfer into two stages: knowledge extraction and aggregation, and target task adaptation. This ensures that memory and runtime costs do not grow with the increase in the number of source models.\nIt can filter out unstructured noise and redundancy by screening the Top-K significant components of the multi-source task matrix through singular value decomposition, and is superior to aTLAS in robustness."}, "weaknesses": {"value": "AXIS fundamentally assumes that all source models share the same architecture as the pre-trained model and are fine-tuned from a common pre-trained base; thus, it is inapplicable when source models are architecturally heterogeneous (e.g., a mix of CNNs and ViTs) or derived from different pre-trained models, severely limiting its use in cross-architecture or cross-pretraining multi-source knowledge transfer scenarios.\nIt performs knowledge aggregation as a one-time offline operation; thus, adding new source models requires recomputing SVD decomposition, top-K selection, and aggregation over all sources (both old and new), making it non-incremental and unsuitable for real-time scenarios with dynamically expanding source models."}, "questions": {"value": "The experiment only focused on the image classification task and was based on the ViT architecture. It did not extend to other computer vision tasks such as object detection and semantic segmentation, nor did it verify the effectiveness in cross-modal scenarios such as NLP and speech. I want to know whether it has generalization ability in non-image classification and non-VIT architecture"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9du5xhrgK2", "forum": "6mLdrpaoRI", "replyto": "6mLdrpaoRI", "signatures": ["ICLR.cc/2026/Conference/Submission10780/Reviewer_uXKj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10780/Reviewer_uXKj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249098412, "cdate": 1762249098412, "tmdate": 1762921989101, "mdate": 1762921989101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AXIS, a two-stage merge-and-tune paradigm for multi-source transfer learning. In the first stage, each source task matrix is decomposed via singular value decomposition (SVD), and rank-one components from all sources are aggregated, selecting the top-K based on their singular values to form a compact merged task matrix. In the second stage, only the singular values of this merged matrix are fine-tuned on the target task, while the singular vectors remain fixed. The authors claim that this design achieves greater performance, efficiency, scalability, and robustness compared to the existing aTLAS framework. Extensive experiments across a wide range of classification benchmarks support these claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper points out a highly relevant problem in modern deep learning, i.e. how to leverage the immense and growing number of specialized models publicly available. The failure of standard transfer learning to utilize this \"wealth of specialized knowledge\"  is a clear and well-motivated gap.\n2. Strong empirical results on scalability (Fig. 5) and robustness (Figs. 6-7) support the paper’s key claims of efficiency and resilience.\n3. Comprehensive ablation studies are conducted, examining sensitivity to K (number of components), N (trainable singular values), selection strategies (top / arbitrary / bottom), and the impact of the final re-SVD orthogonalization. These analyses effectively illustrate why the method works and where its performance gains originate."}, "weaknesses": {"value": "1. The manuscript requires careful proofreading to improve overall presentation quality. There are numerous minor issues that detract from readability. For example, Figure 3 is not referenced in the text, \"Algorithm ??\" appears in line 219 due to a missing reference, and the Related Works section begins with an incomplete line. Addressing these formatting and referencing errors would significantly improve clarity and professionalism.\n2. The use of a radar plot in Figure 3 makes it difficult to clearly assess the claimed state-of-the-art performance. While it provides a compact visualization, it hides exact numerical differences across datasets and even suggests that AXIS underperforms the previous SOTA on several tasks. A detailed table of per-dataset results (e.g., accuracy or relative gain) would provide much clearer evidence and allow fairer comparison with baselines.\n3. The decision to fine-tune only the singular values in the second stage is parameter-efficient, but fixing the singular vectors may limit adaptation flexibility, as vectors from diverse sources might not align well with the target task directions. While the authors partially justify this design through Stage 2 effectiveness experiments, a stronger empirical comparison would provide a more complete validation.\n4. The authors explicitly state AXIS relies on a common architecture and shared pre-trained initialization in conclusion. This restriction weakens applicability in realistic model zoos where many models differ by initialization, minor architectural variants, or LoRA-style adapters.\n5. Limited justification for selecting by raw singular value magnitude as a proxy for transferability is provided. While experiments show it works empirically, the paper lacks theoretical or empirical analysis connecting singular value size to task transferability."}, "questions": {"value": "Same as weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ucK9HpgOkr", "forum": "6mLdrpaoRI", "replyto": "6mLdrpaoRI", "signatures": ["ICLR.cc/2026/Conference/Submission10780/Reviewer_Xmc5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10780/Reviewer_Xmc5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10780/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840170242, "cdate": 1762840170242, "tmdate": 1762921988799, "mdate": 1762921988799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}