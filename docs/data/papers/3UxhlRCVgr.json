{"id": "3UxhlRCVgr", "number": 3535, "cdate": 1757468308096, "mdate": 1759898082487, "content": {"title": "Towards a Unified View of Large Language Model Post-Training", "abstract": "Many approaches with seemingly disparate losses exist for post-training modern language models, such as Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT).\nIn this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process.\nWe derive the Unified Policy Gradient Estimator (UPGE), a framework with four interchangeable parts that unifies a wide spectrum of post-training approaches through their loss gradient form.\nWe further present the calculations of these methods as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs.\nMotivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals.\nHPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns.\nWe provide extensive experiments and ablation studies to verify effectiveness of HPT.\nAcross six mathematical reasoning benchmarks and two out-of-distribution tasks, HPT consistently surpasses strong baselines across models of varying scales and families.", "tldr": "", "keywords": ["Large Language Models", "Post-Training", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/567f18a68653bc10aee70a5998682e740af0ce13.pdf", "supplementary_material": "/attachment/cb7aa6aa267a21948fc65935a51682d9221e0dee.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Unified Policy Gradient Estimator (UPGE) formulation, which unifies post-training practices for large language models, and introduces the Hybrid Post-Training (HPT) algorithm that adaptively selects between SFT and RL based on rollout quality. The UPGE comprises a Stabilization Mask, a Reference Policy Denominator, an Advantage Estimate, and a Likelihood Gradient. The HPT algorithm is designed to balance exploitation and exploration; specifically, it switches to SFT when the current performance on on-policy samples falls below a threshold, and to RL (in particular, Dr. GRPO) otherwise. The paper reports numerical experiments on mathematical reasoning benchmarks using Qwen and LLaMA models, showing that HPT outperforms SFT, GRPO, and mixed-policy baselines (e.g., LUFFY, SRFT). They observe notable gains on AIME-24 compared with LUFFY and SRFT, and demonstrate stronger Pass@k performance"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed UPGE offers a clear framework, making it easier to compare methods and systematically design new loss functions. HPT uses a simple binary, performance-gated switch that improves both accuracy and exploration depth (at least in the reported studies). Across multiple math-reasoning benchmarks and model families, the experimental results show consistent gains over strong baselines."}, "weaknesses": {"value": "1. The empirical scope is narrow: experiments center on math/logic tasks with rule-based correctness verifiers. It’s unclear how HPT performs in domains without reliable automatic verifiers or with noisy, preference-based rewards. \n\n2. The gating threshold is a critical hyperparameter. Although an ablation study is given in appendix and a general idea of ``maintain a dynamic balance between the exploration of RL and the exploitation of SFT'' is mentioned, the paper offers limited guidance on how to set it in practice. \n\n3. It is unclear whether the reported improvement of the HPT truly comes from the gating or the extra information from the evaluation by the verifier."}, "questions": {"value": "1. How should practitioners choose the gate threshold $\\gamma$ in HPT? Furthermore, please explain the rationale behind your selected values of $\\gamma$ for Qwen and LLAMA. \n\n2. Please clarify the definition of $P$. You define  $P=\\frac{1}{k} \\sum_i v\\left(\\tau_i\\right) \\in[0,1]$ (a mean), yet later compare it to integer $\\gamma \\in\\{0,1,2\\}$. Are you actually thresholding the count $\\sum_i v\\left(\\tau_i\\right)$ (range $0 . . k$ )? If the mean is intended, how is an integer $\\gamma$ interpreted? A precise statement will resolve this mismatch.\n\n3. What verifiers are used for each benchmark and what are their known error rates? Please report verifier reliability in terms of false positives and false negatives.\n\n4. Can the observed improvement of HPT be attributed to verifier reliability? It would be informative to conduct a small-scale label-noise stress test by injecting controlled noise into $v(\\tau_i)$. How does HPT degrade as noise increases? \n\n5. In the general mixed-loss form with coefficients $\\alpha, \\beta \\geq 0$, but your main experiments set one to 0 and the other to 1 (binary gating). Have you evaluated fractional mixtures ( $0<\\alpha, \\beta<1$ )? A small ablation could show whether continuous mixing rivals or improves upon binary gating.\n\n6. The numerical results focus on mathematical reasoning benchmarks. How does HPT perform in other tasks? Even a small-scale experiment would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c7SIvozsRM", "forum": "3UxhlRCVgr", "replyto": "3UxhlRCVgr", "signatures": ["ICLR.cc/2026/Conference/Submission3535/Reviewer_t8ra"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3535/Reviewer_t8ra"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549122635, "cdate": 1761549122635, "tmdate": 1762916799230, "mdate": 1762916799230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to unify supervised fine-tuning (SFT) and reinforcement learning (RL) fine-tuning of language models under a single framework. To this end, the paper introduces a unified gradient estimator (UPGE) that is derived from a common objective. Furthermore, the paper proposes a new retraining method named Hybrid pretreating (HPT) that uses a linear combination of SFT and RL. Empirical results are provided to demonstrate the strengths of HPT over SFT and GRPO (as an example of RL fine-tuning)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem studied in the paper is highly relevant to area of post-training language models\n- The motivated to study the problem is clearly stated in the paper. The common objective is sensible and the derivation is clear enough (although the appendix material could be simplified to hand-hold an inexperienced reader through the analytical steps). The gradient estimator appears to be correct.\n- Empirical results are adequate to show the strengths of HPT. I do have a concern with the setup used in experimented noted in weakness\n- Sufficient amount of ablations to show the strengths of HPT."}, "weaknesses": {"value": "- HPT method uses a linear combination of SFT and RL losses in its objective. There is a new hyper parameter, \\gamma, introduced in the paper that appears to require care while setting its value. No guidance is provided on how to set this value\n\n- Empirical analysis uses Qwen2.5 and  LLAMA-3.1 models. The reader (including this reader) may wonder if the observations made with Qwen2.5 extends to Qwen3 and other models. In other words, why were these models chosen for experimentation and not other models? its not clear if the observations carry over to other models."}, "questions": {"value": "(please see weakness for more info)\n\n- How would a user set \\gamma for a new model?\n\n- Are there results available for other models? If so, this might help give the reader confidence about observations being general or applicable to more models than the ones used in the paper.\n\n- The paper uses GRPO as a baseline but HPT itself uses Dr. GRPO. Would it make sense to report performance with Dr. GRPO method (as well)? \n\nOverall, this is a good paper so I would accept given its strength with empirical and analytical analysis. My weakness observed above are shared in th hope that it might be useful for the authors to improve their draft."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n8fPgnB9AS", "forum": "3UxhlRCVgr", "replyto": "3UxhlRCVgr", "signatures": ["ICLR.cc/2026/Conference/Submission3535/Reviewer_dbLE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3535/Reviewer_dbLE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597418731, "cdate": 1761597418731, "tmdate": 1762916798841, "mdate": 1762916798841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Formalizes existing reinforcement learning (RL) and supervised finetuning (SFT) methods as special cases of a more general Unified Policy Gradient Estimator (UPGE) framework. The framework allows accounting for differences in gradient masking (e.g., clipping in PPO), choice of reference policy, and advantage estimate. Then, the paper proposes a Hybrid Post-Training (HTP) procedure that chooses whether to perform SFT or RL updates based on whether the current accuracy of the model on a prompt is below or above a certain threshold. Empirical evaluations demonstrate that HPT surpasses the performance of several alternative post-training methods on mathematical reasoning and general knowledge datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Relatively well-written and easy to follow.\n\n2. The proposed HPT method is conceptually simple and shows promise compared to existing post-training baselines for tasks with verifiable rewards.\n\n3. The empirical analysis of Section 4 is beneficial for understanding the inner-workings of HPT."}, "weaknesses": {"value": "1. The significance of a unified perspective, such as the one proposed in Section 2, greatly depends on whether it allows characterizing similarities and differences between different instances. However, the unified framework is not really used in a meaningful way, which raises the question of why it is necessary or helpful. The proposed HPT method just boils down to either doing RL or SFT depending on an accuracy threshold for a prompt.\n\n2. Related to the above, there is somewhat of a disconnect between the goal of providing a unified perspective for post-training and the proposed HPT, which is specific to tasks where you have verifiable binary rewards. These tasks can be quite narrow compared to the vast domains language models are trained for.\n\n3. It is mentioned several times that different instantiations of UPGE exhibit different bias-variance tradeoffs. Yet, this claim is not well-substantiated in the paper, neither empirically nor theoretically. The main text refers to Appendix C, which lists differences between UPGE instantiations and hand-wavily provides arguments on bias and variance. But what are the biases and variance referred to in these arguments? The different instantiations optimize a different objective. Their optimal policies are different. How do you fairly compare bias and variance in this case? Also, given that the claims are more on the intuitive level and not formalized, it would be best to clearly state this to not mislead readers.\n\n4. The paper does not report standard deviation across random seeds, or any other measure of statistical significance. So it is difficult to assess how consistent and substantial the improvements of HPT compared to baselines are.\n\n\n\nReview Summary and Recommendation\n---\nOverall, the empirical results suggest that HPT is a simple yet competitive post-training method that combines the benefits of SFT and RL. However, the paper does not report any measure of statistical significance and the presentation and significance of the proposed unified framework can be substantially improved. As a result, I believe that the current paper falls on the borderline, where my assessment tends towards acceptance given that the authors can provide experiments verifying that the performance gains of HPT are consistent across random seeds.\n\n\nAdditional (More Minor) Comments\n---\n\n1. The paper refers to different post-training methods as optimizing the same \"Common Objective\". These statements, such as in lines 152-153, are often imprecise. Different methods do not optimize the same objective (e.g., this can be seen from the fact that the optimal policy with respect to them can differ), rather you can frame the different objectives as coming from the same objective family (defined by the \"Common Objective\"). It is important to clarify this point and avoid misleading claims, yet I do not believe this is a major issue since it can easily be fixed.\n\n2. I recommend making the notation in the equation defining $grad_{Uni}$ more explicit so it is clearer what are the components involved (e.g., the prompt and response).\n\n3. Something is off with Equation (4). If $\\pi_{ref} = \\pi_{\\beta}$ then you do not get the correct gradient since there is a superfluous $\\pi_{\\beta} (\\tau | q)$ (there is already one coming from the outer expectation). If $\\pi_{ref} \\neq \\pi_{\\beta}$ then also the gradient is incorrect since there is a missing division by $\\pi_{ref}$.\n\n4. The main text does not specify which data was used for training. This detail is found in the appendix, but I highly recommend putting it in Section 3.1 given its importance."}, "questions": {"value": "--"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fQNcgAKqa4", "forum": "3UxhlRCVgr", "replyto": "3UxhlRCVgr", "signatures": ["ICLR.cc/2026/Conference/Submission3535/Reviewer_HJxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3535/Reviewer_HJxf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666827962, "cdate": 1761666827962, "tmdate": 1762916798484, "mdate": 1762916798484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UPGE - a unified policy gradient estimator for post-training algorithms like RL and sft. Inspired by the analysis in UPGE, the author aimed to combine RL with sft using a dynamic gating mechanism based on the model's performance on the question."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Good paper writing, clear motivation\n* The proposed HPT yielded noticeable improvement against RL or SFT"}, "weaknesses": {"value": "* It’s not clear to me how the unified gradient estimator connects to the proposed HPT. Sections 2.1 and 2.2 do not clearly bridge the concepts to the HPT design described in Section 2.3.\n* Specifically, following the previous point, in order to unify RL and SFT under a common objective, Section 2.2 introduces a non-conventional definition of the post-training optimization target by replacing the original \\pi_{ref} with the behavior policy in the RL context. I think the paper should clarify this setting in more detail.\n* Is there any ablation or analysis on how changes in the gating threshold affect the training dynamics?"}, "questions": {"value": "Just a comment that related to the weaknesses mentioned earlier. The writing of the paper gives me the impression that the authors first built something that works (combining RL and SFT dynamically) and then retroactively proposed a theoretical framework to justify it. The presentation was a bit confusing; I only understood the UPGE part after reading the methodology section and then revisiting it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ySBMbFnjG0", "forum": "3UxhlRCVgr", "replyto": "3UxhlRCVgr", "signatures": ["ICLR.cc/2026/Conference/Submission3535/Reviewer_tTEQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3535/Reviewer_tTEQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954459697, "cdate": 1761954459697, "tmdate": 1762916797185, "mdate": 1762916797185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Unified Policy Gradient Estimator (UPGE), a theoretical framework that unifies various LLM post-training algorithms including SFT and RL methods through their loss gradient form. The authors demonstrate that these seemingly disparate approaches optimize a common objective under different data distribution assumptions and bias-variance tradeoffs. Building on this unified view, the paper introduces Hybrid Post-Training (HPT), an algorithm that dynamically switches between SFT and RL based on real-time performance feedback. Extensive experiments on mathematical reasoning benchmarks show that HPT consistently outperforms strong baselines including SFT, GRPO, LUFFY, and SRFT across models of varying scales (Qwen2.5-Math-1.5B/7B, LLaMA3.1-8B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a strong theoretical contribution by deriving UPGE from a common objective (Eq. 1), showing that SFT and RL are not contradictory but complementary learning signals.\n\n2. The proposed HPT algorithm is well-motivated by the unified theoretical framework. The dynamic switching mechanism based on performance feedback (Eq. 6-8) is practical, avoiding the need for manual tuning of mixing ratios as in prior work like LUFFY.\n\n3. The experimental evaluation is comprehensive, covering multiple model scales (1.5B, 7B, 8B) and families (Qwen, LLaMA), six mathematical reasoning benchmarks and two out-of-distribution tasks.\n\n4. The paper is generally well-written with clear motivation and organization."}, "weaknesses": {"value": "1. The claim that the multi-stage (SFT->RL) process is notoriously resource-intensive seems unreasonable, since SFT requires far fewer computational resources compared to RL.\n\n2. The paper mentions that \"SFT memorizes, RL generalizes\" in related work (Chu et al., 2025) but does not systematically investigate the memorization-generalization tradeoff in HPT. Does HPT's dynamic switching lead to better generalization on out-of-distribution tasks? The limited evaluation on only two OOD tasks (ARC-c, GPQA) is insufficient to draw strong conclusions.\n\n3. The paper does not provide sufficient analysis on computational costs. While HPT avoids the two-stage SFT→GRPO pipeline, it still requires generating multiple rollouts for each question to compute performance feedback. A detailed comparison of training time, GPU memory, and total compute budget against baselines would strengthen the practical applicability claims.\n\n4. The HPT algorithm relies on performance feedback coefficients α and β, as well as the switch threshold γ, which significantly impact model performance. However, the paper provides only limited empirical settings (e.g., γ=0 or 2) without a systematic sensitivity analysis or guidance on adjustment."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IgD8gWnCQr", "forum": "3UxhlRCVgr", "replyto": "3UxhlRCVgr", "signatures": ["ICLR.cc/2026/Conference/Submission3535/Reviewer_PwLU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3535/Reviewer_PwLU"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3535/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993265000, "cdate": 1761993265000, "tmdate": 1762916795937, "mdate": 1762916795937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}