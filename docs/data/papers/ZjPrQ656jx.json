{"id": "ZjPrQ656jx", "number": 7455, "cdate": 1758022744999, "mdate": 1759897852018, "content": {"title": "RuleEdit: Benchmarking Rule-Level Knowledge Editing in Large Language Models", "abstract": "Knowledge editing seeks to update language models without full retraining. While most prior work focuses on isolated factual or instance-level edits, we explore a more structured domain: mathematical rules. We introduce \\textbf{RULE-EDIT}, the first benchmark explicitly designed for editing and evaluating rule-level abstract knowledge in LLMs. Beyond measuring direct edit accuracy, our benchmark is designed to encourage deeper investigation into the interpretability and symbolic reasoning capabilities of LLMs: (1) To what extent do edits to abstract rules propagate to derived instances? and (2) How well do token-level updates align with higher-level symbolic structures across formats? To evaluate this, we propose two new metrics:\\emph{Instance Portability} and \\emph{Rule Understanding} that quantify whether edits correctly generalize to rule-governed examples and maintain consistency across symbolic and natural language representations. Through experiments on best-performing open-source LLMs using representative editing methods, we find that while models can often overwrite formula-level knowledge, they frequently struggle to propagate these edits to rule-derived instances and to maintain consistency across different forms of a rule.  For example, several methods achieve nearly 100\\% reliability on direct rule queries, yet their rule-specific scores remain unsatisfactory (Instance Portability never exceeds 52\\% and Rule Understanding stays below 26\\%).  Our findings highlight the limits of current editing methods and motivate rule editing as a testbed for controllable knowledge in LLMs.", "tldr": "", "keywords": ["Knowledge Editing", "Rule-level Editing"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04e3b099e64202e64aa8844e07e32b8fc0b42d16.pdf", "supplementary_material": "/attachment/207868189fc7aa6a39a8e6ad6022e0c4b213e081.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces RuleEdit which is a new benchmark that evaluates rule-level knowledge editing in LLMs. Unlike majority of prior work that focus on factual or instance level updates, RuleEdit focuses on mathematical rule based edits. Authors also propose two new metrics suitable for rule based edits, namely Instance Portability and Rule Understanding. Lastly, authors use this benchmark and perform experiments using various models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is novel and interesting. It introduces a new perspective on knowledge editing.\n2. This work can open new windows for newer work that can build upon this work.\n3. The paper is clearly written and easy to follow.\n4. Authors considered various model editing approaches in their experiments."}, "weaknesses": {"value": "1. The benchmark lacks realism. It would have been better if authors could have designed the benchmark in a more realistic setup. In mathematical setting, rules are not changed, so this makes the setup not realistic. It would have been better if authors designed the benchmark for more realistic setups.\n2. The benchmark was really small which makes one doubt the statistical significance of the results.\n3. The scope/domain which benchmark covers is really small (e.g., Euclidean geometry and only focusing on fundamental geometric rules).\n4. Since the data is not realistic, it makes me doubt about the practicality, usefulness, and feasibility of the work. Perhaps if the benchmark was more realistic all these doubts could have been cleared.\n5. it would have been nice if authors could also comment about more closed sourced and powerful models.\n6. The evaluations are based on automatic evaluators using DeepSeek. It would have been better if some human verification was done on the results.\n7. The data was generated synthetically and then human verified. This might make the data not diverse and rigorous enough."}, "questions": {"value": "Can you think about a more realistic setup where rule-edits might be found beneficial and applicable to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CJ2cbcRUQ9", "forum": "ZjPrQ656jx", "replyto": "ZjPrQ656jx", "signatures": ["ICLR.cc/2026/Conference/Submission7455/Reviewer_ZFPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7455/Reviewer_ZFPi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674151694, "cdate": 1761674151694, "tmdate": 1762919568736, "mdate": 1762919568736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a new benchmark to explicitly test how good editing methods are in injecting rule-level abstract knowledge. They find that while models can often overwrite formula-level knowledge, they frequently struggle to *propagate* these edits to rule-derived instances and to maintain consistency across different forms of a rule"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This benchmark target an important problem in knowledge editing --- propagation. The perspective of having a common rule is interesting.\n\n* The work finds an important observation that exiting knowledge editing techniques can mostly regurgitate what's injected rather than generalize the knowledge across instances/query, similarly in prior work [1].\n\n* Problem is proposing a f\n\n\n[1] PropMEND: Hypernetworks for Knowledge Propagation in LLMs"}, "weaknesses": {"value": "* Failure at propagation is more of an established observation from prior work [1, 2]. No method is proposed in the work to resolve the problem. Or insights for how to propose a better method.\n\n* The setting is not realistic: 1. what if there's conflicting rules; 2. multi-edit setting\n\n\n[1] Evaluating the Ripple Effects of Knowledge Editing in Language Models\n\n[2] CodeUpdateArena: Benchmarking Knowledge Editing on API Updates"}, "questions": {"value": "* How does the author think of the difference from prior work [1]?\n\n* Choosing math rules feels hard, where the model knows math pretty well. To some extent, the model seems done to fail --- a few gradient descent from editing methods seems hard to overwrite what the model learns from pretraining. \n\n\n[1] CodeUpdateArena: Benchmarking Knowledge Editing on API Updates"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ii6sUENoEj", "forum": "ZjPrQ656jx", "replyto": "ZjPrQ656jx", "signatures": ["ICLR.cc/2026/Conference/Submission7455/Reviewer_aQhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7455/Reviewer_aQhG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862417224, "cdate": 1761862417224, "tmdate": 1762919568125, "mdate": 1762919568125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RULE-EDIT, a benchmark for evaluating rule-level knowledge editing in large language models (LLMs), with a focus on mathematical rules such as geometric formulas. The authors argue that while existing benchmarks like CounterFact or ConceptEdit mostly target instance-level factual edits, rule-level knowledge, more abstract, generalizable, and interpretable, remains underexplored.\n\nThey propose two new metrics: Instance Portability (IP): measures whether edits propagate to rule-derived instances. Rule Understanding (RU): measures cross-view consistency between symbolic and natural language representations.\n\nExperiments across five editing methods (LoRA, ROME, MEMIT, GRACE, PROMPT) and four LLMs (GPT-J, LLaMA-3, Qwen2, Qwen2.5) reveal that existing methods achieve high edit reliability but poor generalization, locality, and symbolic consistency. The paper concludes that current editing techniques mostly perform surface overwriting rather than rule internalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of studying rule-level editing is intuitively appealing and helps connect factual editing with symbolic reasoning.\n\nThe work contributes to a more interpretable, structured view of knowledge editing, a direction of increasing importance for controllable and reliable model updates."}, "weaknesses": {"value": "The key findings that edited knowledge fails to generalize, that locality collapses, and that edits do not propagate coherently are not new. Prior work in knowledge editing (e.g., ROME, MEMIT, GRACE, EasyEdit studies) has repeatedly reported the same issues: distributed representations prevent clean locality, and disjoint parameter subspaces block propagation.\nThis paper re-observes these phenomena in the domain of geometric rules, but does not analyze whether rule-level editing fails for the same or different reasons. As a result, the study feels confirmatory rather than revealing new mechanisms.\n\nThe paper lacks deeper analysis explaining why rule edits fail.\nThere is no representational probing, causal tracing, or neuron-level localization to connect rule propagation failure with parameter entanglement or symbolic abstraction.\nWithout such insight, the work remains descriptive (\"models fail to internalize\") rather than diagnostic (\"models fail because knowledge subspaces are orthogonal / overlapping\").\n\nConnections to known reasoning phenomena such as the \"reverse curse\" where models can answer A→B but not B→A would have strengthened the interpretation, showing that symbolic generalization failures extend consistently across both inference and editing.\n\nThe abstract poses two questions \"(1) how edits propagate\" and \"(2) how token-level updates align with symbolic structures\", which are conceptually redundant. Both address the same propagation-consistency problem, and should be reframed as orthogonal dimensions (e.g., vertical propagation vs. horizontal alignment).\nThe main text also contains substantial repetition. Phrases like “models act as surface-level overwriting mechanisms rather than internalizing rules” appear multiple times. The paper could be significantly condensed for higher information density and clearer logical flow.\n\nOverall, the experiments demonstrate breadth but limited depth or interpretability. The results largely restate known patterns (\"LoRA overfits\", \"GRACE memorizes\", \"PROMPT generalizes weakly\") without advancing understanding."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qjZ2xtFwaf", "forum": "ZjPrQ656jx", "replyto": "ZjPrQ656jx", "signatures": ["ICLR.cc/2026/Conference/Submission7455/Reviewer_4YWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7455/Reviewer_4YWr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040546465, "cdate": 1762040546465, "tmdate": 1762919567765, "mdate": 1762919567765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}