{"id": "G9rclU8orB", "number": 21204, "cdate": 1758314884729, "mdate": 1759896934865, "content": {"title": "Textual Supervision Enhances Geospatial Representations in Vision-Language Models", "abstract": "Geospatial understanding is a critical yet underexplored dimension in the development of machine learning systems for tasks such as image geolocation and spatial reasoning. In this work, we analyze the geospatial representations acquired by three model families: vision-only architectures (e.g., ViT), vision-language models (e.g., CLIP), and large-scale multimodal foundation models (e.g., LLaVA, Qwen, and Gemma). By evaluating across image clusters, including people, landmarks, and everyday objects, grouped based on the degree of localizability, we reveal systematic gaps in spatial accuracy and show that textual supervision enhances fine-grained geospatial representations. Our findings suggest the role of language as an effective complementary modality for encoding spatial context and multimodal learning as a key direction for advancing geospatial AI.", "tldr": "Models pre-trained with text and images show stronger geospatial representations in their latent space.", "keywords": ["Mechanistic Interpretability", "Vision-language models", "Probing", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cefc3f790a2f40714bdfc2ce15c02eddec808431.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores how geospatial representations emerge in vision-only and vision–language interaction models without explicit geographic supervision. It analyzes how textual supervision influences the implicit spatial understanding of visual models by probing layer-wise embeddings and examining the role of prompts and feature manipulation. The results show that incorporating language signals enhances the encoding and controllability of geospatial semantics, suggesting that multimodal learning provides a stronger foundation for spatial reasoning than purely visual training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows that CLIP and VLMs outperform larger vision-only models, confirming that language-based supervision induces finer spatial awareness.\n\nThe paper carried out several interesting experiments to prove the argument. Such as swapping geospatial features between images predictably alters generated locations while preserving semantics.\n\nThe paper is well-structured and easy to follow."}, "weaknesses": {"value": "Without statistical testing, it is unclear whether observed differences are robust across runs.\n\nThe mechanism by which textual prompts re-activate spatial signals remains speculative. It is unclear whether this effect is specific to geospatially oriented queries or would also occur with unrelated textual inputs, making the causal relationship between language and spatial activation uncertain.\n\nSection 4.4 defines “top p dimensions ranked by coefficients”, but it remains unclear how the obtained regression coefficients are ensured to be optimal or stable. Without verifying consistency across different initializations or regularization strengths, the ranking of dimensions may reflect run-specific artifacts rather than intrinsic geospatial factors.\n\nSection 4.5 mostly shows qualitative examples, but there’s no quantitative analysis. Personally, I think it should include some measurable results, like how often the location edit actually works or stays consistent, to show the effect isn’t just anecdotal."}, "questions": {"value": "In Section 4.3, does prompt-based improvement depend on the linguistic specificity of the query (e.g., “Where is this photo?” vs. “Guess lat/long”)?\n\nWhat’s the meaning of “fine-grained”? It is not explicitly defined. Does it refer to coordinate-level precision (e.g., latitude/longitude accuracy) or to semantic granularity (e.g., distinguishing similar landmarks within a region)?\n\nIf two images have similar content (e.g., buildings sharing architectural styles) but are located in different countries or even continents, can the probe still recover their coordinates? And in such cases, which would perform better, vision-only or vision-language models? Another interesting observation is that clusters such as Food or People Closeups achieve positive R², even though their visual content seems sort of unrelated to geography from the examples. What cues do models rely on to localize these images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zvtERtiOYw", "forum": "G9rclU8orB", "replyto": "G9rclU8orB", "signatures": ["ICLR.cc/2026/Conference/Submission21204/Reviewer_uh9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21204/Reviewer_uh9U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582079658, "cdate": 1761582079658, "tmdate": 1762941613185, "mdate": 1762941613185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how different model families — vision-only architectures (e.g., ViT), vision-language models (e.g., CLIP), and large-scale multimodal foundation models (e.g., LLaVA, Qwen, Gemma) — learn and encode geospatial representations.\n\nThrough clustering-based analyses of images (people, landmarks, objects) grouped by localizability, the authors demonstrate that textual supervision significantly enhances fine-grained geospatial understanding. The findings suggest that language provides complementary cues for encoding spatial context and highlight multimodal learning as a promising direction for advancing geospatial AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Comprehensive evaluation across multiple model families, from vision-only encoders to advanced multimodal foundation models.\n* Large-scale dataset construction and systematic analysis covering diverse image categories, enabling meaningful cross-model comparison.\n* Empirical evidence supporting the hypothesis that textual supervision improves geospatial representation quality.\n* Strong visualization and interpretability results, revealing how current models implicitly capture spatial cues.\n* The topic lies at the intersection of interpretability and multimodal representation learning, which is timely and relevant to the ICLR community."}, "weaknesses": {"value": "1. Insufficient discussion on recent visual self-supervised models (e.g., Web-SSL, language-free visual encoders [3]) that may already achieve strong spatial representations without textual supervision.\n\n2. Data imbalance and representational bias — the paper acknowledges that landmark data are unevenly distributed. However, it does not examine whether this imbalance propagates into model performance, particularly for underrepresented regions or low-resource geographies.\n\n3. Lack of temporal analysis — the study focuses on static, long-standing landmarks. It would be valuable to assess model understanding of newer or dynamic landmarks to evaluate the temporal robustness of geospatial representations.\n\n4. Prompt-based enhancement is somewhat obvious — while textual supervision improves performance, the paper does not explore whether finetuning (e.g., SFT) introduces catastrophic forgetting or enhances model specialization."}, "questions": {"value": "1. How does this work conceptually align with or differ from the Platonic Representation Hypothesis [1]? Does geospatial representation follow similar convergence trends across modalities?\n\n2. Could you provide more discussion on how large-scale, language-free visual encoders (e.g., [3]) compare to multimodal ones in terms of spatial representation quality?\n\n3. To what extent does data imbalance affect model geospatial reliability for low-visibility or low-resource regions?\n\n4. Have you analyzed temporal sensitivity — i.e., whether newer landmarks absent from training data are recognized differently by various model families?\n\n5. What would happen if the models were finetuned with explicit geospatial supervision? Would this improve or degrade their general multimodal reasoning ability?\n\n**References**\n\n[1] Huh, M., Cheung, B., Wang, T., & Isola, P. The Platonic Representation Hypothesis. ICML 2025.\n\n[2] He, J., Nie, T., & Ma, W. Geolocation Representation from Large Language Models as Generic Enhancers for Spatio-Temporal Learning. AAAI 2025.\n\n[3] Fan, D., Tong, S., Zhu, J., et al. Scaling Language-Free Visual Representation Learning. ICCV 2025.\n\n[4] Menon, S., & Vondrick, C. Visual Classification via Description from Large Language Models. ICLR 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6f2IoKf4bC", "forum": "G9rclU8orB", "replyto": "G9rclU8orB", "signatures": ["ICLR.cc/2026/Conference/Submission21204/Reviewer_APe2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21204/Reviewer_APe2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742556173, "cdate": 1761742556173, "tmdate": 1762941612488, "mdate": 1762941612488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether vision-only encoders, contrastive VLMs, and instruction-tuned MLLMs implicitly encode geospatial information without explicit geo-supervision. Using layer-wise ridge probes to predict latitude/longitude and reporting R^2, the authors find that models trained with textual supervision show stronger geolocation structure that vision-only models. They also report prompt-conditioning that preserves/boosts geospatial signal in later LLM layers, etc.. Data come from clustered YFCC100M and a sampled Google Landmarks subset with geocell balancing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The claim is clear, that is textual supervision systematically strengthens geospatial representations across image types.\n2. The authors attempt at geographic balancing via geocells; clusters in YFCC are described and visualized."}, "weaknesses": {"value": "1. The conclusions of this paper are of no value. Its most important takeaway is that multimodal models outperform vision-only models in geospatial representation. This is easy to understand: multimodal models are trained on human knowledge—specifically, text—whereas vision-only models are trained merely to extract visual features without drawing on human knowledge. Under these circumstances, it is only natural that multimodal models would better capture geospatial representation.\n2. The authors refer to the ability to identify where a photo was taken as ”geospatial representation,“ which is quite odd. Generally, geospatial representation should refer to the geospatial nature of the image’s own features.\n3. Representation-swapping is a single-model, qualitative case study; interesting but not systematically evaluated (stability issues are acknowledged).\n4. The prompt that asks for coordinates (“Guess the latitude and longitude…”) may teach the model to surface memorized textual priors rather than reflect purely visual geospatial encoding."}, "questions": {"value": "If you size-match models and hold the image corpus constant while selectively corrupting or removing text supervision during pretraining, do the observed R² and downstream gains persist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E4GyzLGl1S", "forum": "G9rclU8orB", "replyto": "G9rclU8orB", "signatures": ["ICLR.cc/2026/Conference/Submission21204/Reviewer_wXaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21204/Reviewer_wXaT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803387895, "cdate": 1761803387895, "tmdate": 1762941611822, "mdate": 1762941611822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors examine the geospatial representation capabilities of pre-trained vision-only foundation models, vision-language models and multimodal LLMs. The authors perform linear probing regression on various layers of the transformers in those pretrained models and analyze their performance in prediction of the longitude and latitude. Using this method, they are able to identify the top features associated with geospatial information and conduct an experiment where they swap that part of the features in text prompts to obtain manipulated model outputs from VLMs. They conclude that textual information is beneficial for geospatial representations in foundation models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents a very interesting and creative research question, i.e. what kind of foundation models are good “geo-guessers”. This is a relatively new and previously unexplored area in understanding the emergent properties of large foundation models.\n2. The authors are able to systematically explore and identify layers and dimensions where the models produce latent geospatial information representations. This method makes good contributions to the interpretability of these large models.\n3. The experiments shown in Section 4.5 are genuinely surprising and interesting."}, "weaknesses": {"value": "Although I do really like this paper, unfortunately there are a few aspects of this paper that make it not ready for publication right away.\n1. The overall story of the paper is very messy and confusing:\n\n    (i) In the introduction section, the authors summarize their research question as: “To what extent do these models internalize global location knowledge as an emergent property of their training and fine-tuning pipelines?” However, the remainder of the paper does not discuss or contain experiments that study this phenomenon as an emergent property. According to [1], “an ability to be emergent if it is not present in smaller models but is present in larger models”. However, this paper does not study model scaling as a factor of this ability.\n\n    (ii) The experiments in Section 4.5, though the most interesting, seem very disconnected to the rest of the paper, since the rest of the paper is more about comparing vision-only encoders to textual-visual models, and Section 4.5 is mainly about steering VLMs. However, Section 4.5 is used as the featured example in Figure 1. This makes the overall flow of the paper extremely confusing.\n2. The experimental results are not sufficient to support the main claim: as the title suggests, the authors claim that textual supervision can enhance geospatial representation in those models. However, the comparison conducted in this paper is among various pre-trained models that are trained in completely different settings. The setting differences include model size, training data, and training duration. Given that there are so many confounding factors, I don't think it is reasonable to directly draw the conclusion that textual-visual models are better because they incorporate text data – for example, it is possible that they are better simply because they have a better training data mixture, their model has better architecture, or their model is simply larger. It would be better if the authors compare vision-only models with textual-visual models in a more rigorous setting, i.e. keeping the data source, model size and model architecture the same and only excluding textual information in the vision-only setting.\n3. Figure 3 is very confusing – what do these pictures represent in this plot?\n\nReference:\n\n[1] Wei et al. Emergent Abilities of Large Language Models. 2022."}, "questions": {"value": "It would be great if the authors can answer my questions in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VswEE2FX5s", "forum": "G9rclU8orB", "replyto": "G9rclU8orB", "signatures": ["ICLR.cc/2026/Conference/Submission21204/Reviewer_QEj4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21204/Reviewer_QEj4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040654448, "cdate": 1762040654448, "tmdate": 1762941611009, "mdate": 1762941611009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}