{"id": "JAalsmy7bZ", "number": 18058, "cdate": 1758283327811, "mdate": 1759897135854, "content": {"title": "LEMON - a foundation model for single-cell nuclear morphologies for digital pathology", "abstract": "Representation learning is a central challenge in Computational Pathology (CP), with direct implications for cancer research and precision medicine. While Self Supervised Learning (SSL) has advanced patch and slide-level analysis of Whole-Slide Images (WSIs), single-cell representation learning has remained underexplored, despite its importance for characterizing cell types and phenotypes. We introduce LEMON (Learning Embeddings from Morphology Of Nuclei), a self-supervised foundation model for scalable single-cell image representation. Trained on millions of cell images spanning diverse tissues and cancer types, LEMON provides versatile and robust morphology representations that enable large-scale single-cell studies in pathology. We demonstrate its effectiveness across diverse prediction tasks on five benchmark datasets, establishing LEMON as a new paradigm for cell-level computational pathology.", "tldr": "We introduce LEMON (Learning Embeddings from Morphology Of Nuclei), a self-supervised foundation model for scalable single-cell image representation.", "keywords": ["Self-supervised representation learning", "Computational Pathology"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27f03fba04f68fb48bde11fa67e2d52a333438ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes LEMON, a self-supervised foundation model designed for single-cell nuclear image representation learning in computational pathology. Unlike existing self-supervised learning (SSL) models trained on slide- or patch-level data, LEMON focuses on cell-level representations. The authors curate a massive dataset (several millions of segmented nuclei from TCGA WSIs) and benchmark LEMON on multiple downstream tasks (cell classification, mitosis detection, and gene expression regression). They compare LEMON with both natural-image and histopathology foundation models (e.g., DINOv2, UNI, Virchow2, Volta) and report superior performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper addresses foundation models at the single-cell level, which is a real gap in computational pathology. Previous works mostly focus on specific domains or limited data. LEMON generalizes this to a broad, cross-tissue, multi-organ setting and defines a reproducible recipe for training such models.\n\n+ The authors benchmark across seven downstream tasks, spanning classification and regression across several datasets (NuCLS, MIDOG++, HEST). The results consistently demonstrate superior performance, particularly against tile-level foundation models and Volta.\n\n+ Adaptation on MoCov3 and DINOv2 to 40x40 images (e.g., removing local-to-global loss, replacing KoLeo regularizer with KDE) has been carried out, and the authors analyze augmentation policies, dataset scale, and diversity effects. The experiments are systematic, including FLOPs comparison and ablation studies, which lend strong empirical validity and evidence."}, "weaknesses": {"value": "- The methodological contributions are mostly engineering adaptations of existing SSL paradigms (MoCov3 and DINOv2) rather than introducing new theoretical insights or architectures. The novelty lies in scale and application domain rather than new algorithmic concepts.\n\n- While the dataset scale is impressive, details on accessibility, cleaning, and balancing are missing. It is unclear whether the dataset or pre-trained weights will be released (or under what license), which may limit reproducibility and community impact.\n\n- The representation interpretability analysis (e.g., t-SNE in Fig. 4) is qualitative. Biological validation or correlations with known phenotypes or molecular subtypes are absent, which would strengthen claims about biological utility and \"foundation model\" status."}, "questions": {"value": "1. How well does LEMON generalize to slides from different institutions, staining protocols, or scanners? Have you quantified batch effects beyond the discussion section?\n\n2. Will the pre-trained models or the large-scale cell dataset (or at least sampling scripts) be publicly released? Without this, how can the community benchmark against LEMON?\n\n3. In Fig. 3, performance saturates near 1 M cells. Is this due to model capacity, training strategy, or dataset redundancy? Could larger ViT architectures (ViT-B/8, ViT-L/8) yield further gains with appropriate scaling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUComHOSEP", "forum": "JAalsmy7bZ", "replyto": "JAalsmy7bZ", "signatures": ["ICLR.cc/2026/Conference/Submission18058/Reviewer_PwFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18058/Reviewer_PwFY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695217379, "cdate": 1761695217379, "tmdate": 1762927846696, "mdate": 1762927846696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LEMON, a self-supervised foundation model for single-cell nuclear morphology, addressing a gap left by patch- and slide-level models in computational pathology. By curating a large-scale dataset of millions of 40x40 nucleus images from 10 cancer types, the authors adapt and train SSL frameworks for this new domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The \"Performance vs. FLOPs\" analysis (Figure 2) is a nice result, showing that LEMON achieves state-of-the-art performance while requiring orders of magnitude fewer FLOPs than the large tile-level foundation models."}, "weaknesses": {"value": "- The finding that MoCov3 consistently outperforms the modified DINOv2 (Table 5) is a key result. The paper hypothesizes this is due to the 40x40 image size favoring a global contrastive objective. However, the DINOv2/iBOT framework includes a masked-image modeling objective specifically designed to learn fine-grained local features. Why do the authors believe this objective, which seems ideal for capturing chromatin-level detail, fails in this low-resolution setting?\n- The dataset composition study (Figure 3, top row) shows performance saturating at just 1M cell images, yet the final models are trained on 10M cells. Why was 10M used if saturation occurred at 1M? Furthermore, this 1M saturation point seems surprisingly low for an SSL model. Does this suggest that the morphological diversity of nuclei is relatively constrained, and the model learns the full distribution of appearances after only 1M examples?\n- The comparison to tile-level foundation models (UNI, Virchow2) in Table 1 involved resizing the 40x40 cell images up to 224x224 to match the models' expected input size. This resizing likely introduces significant artifacts and is far from the models' training distribution. How confident are the authors that the poor performance of these models isn't just an artifact of this \"up-resizing\" protocol, rather than a true failure of tile-level features to capture cell morphology?"}, "questions": {"value": "- The augmentation study (Table 2) is excellent. It's surprising that a \"realistic\" stain augmentation like RandstainNA (a1+gmm1) hurt classification performance compared to the a1+gray policy. Why do the authors think a more realistic simulation of stain variation was detrimental for classification tasks, while a simple grayscale augmentation was beneficial?\n- The t-SNE analysis in Figure 4 provides strong qualitative evidence that the embedding space captures morphological phenotypes, which are annotated by a pathologist . The model is also benchmarked on gene expression prediction (HEST). Is there a way to bridge these two results? For example, if the cells in the t-SNE are colored by their ground-truth (or predicted) expression of a known marker gene, does this align with the expert-annotated morphological clusters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qLyIVXJCP", "forum": "JAalsmy7bZ", "replyto": "JAalsmy7bZ", "signatures": ["ICLR.cc/2026/Conference/Submission18058/Reviewer_Dpnm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18058/Reviewer_Dpnm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034814612, "cdate": 1762034814612, "tmdate": 1762927846019, "mdate": 1762927846019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a novel family of vision foundation models called LEMON for digital pathology operating at the cell level. They propose to first extract images of 40*40 pixels of each cell within an image, before learning cell representations using well-known self-supervised learning frameworks such as MoCov3 and DinoV2. To this end, they investigate various ViT architectures, SSL losses and data augmentation strategies, while pre-training models on several million of cells extracted from around 10k WSI of the TCGA database. The authors show that their best model (relying on MoCov3) outperforms a range of SSL models, learned on either natural or pathology images, on 5 cell-level tasks. Then they provide a range of ablation studies on the aforementioned different SSL settings and study the scaling laws of their LEMON-MoCov3 model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Overall, the paper is well-written.\n- Propose a novel family of vision foundation models for cell-level tasks pre-trained on 10M cell images extracted from the large-scale TCGA database.\n- Study various SSL frameworks including MoCoV3, DinoV2 and variants of the latter, where they observe that its local-to-global objective is detrimental to learning cell representations.\n- Study various augmentation strategies with a concern for batch effect mitigation.\n- Show that Lemon-mocov3 outperforms many baselines over 4 cell-level tasks with a significant margin, while being competitive with the best baselines on a 5th task.\n- Study scaling laws for Lemon-mocov3, showing that performance tends to saturate over 1M cell images."}, "weaknesses": {"value": "- **W1: removing spatial information between cells**. Models proposed by the authors inherently remove spatial/environmental information between cells, which is an important factor in digital pathology, e.g within tertiary lymphoid structures to discriminate different immune cells [A, B]. I remark that Volta includes environmental information and achieves quite competitive performances while using a likely weaker encoder and learned from less data. I believe that it would be interesting to further test the importance of that information. For instance, as we can observe fairly weak performances of tile-level pathology FM on the cell-level task, which I believe is because zooming in like authors did to get cell embeddings lead to a too big domain drift for these models, some spatially-informed baselines could be derived from these model I suggest a rather simple baseline which assumes that these models embed patch tokens (14*14 pixels) and image CLS token into an Euclidean space. One could compute a tile embedding and corresponding cell-segmentation mask. Then, for each cell, a cell embedding could be computed as a convex combination of the patch token embeddings it belongs to, whose weights coincide with the proportions of pixels associated with the cell contained in the patch token. \n\n\n[A] Pitzalis, C., Jones, G. W., Bombardieri, M., & Jones, S. A. (2014). Ectopic lymphoid-like structures in infection, cancer and autoimmunity. Nature Reviews Immunology, 14(7), 447-462.\n\n[B] Schaadt, N. S., Schönmeyer, R., Forestier, G., Brieu, N., Braubach, P., Nekolla, K., ... & Feuerhake, F. (2020). Graph-based description of tertiary lymphoid organs at single-cell level. PLoS Computational Biology, 16(2), e1007385.\n\n- **W2: data curation**. It seems to be an overstatement in Section 2.1 that a data curation technique was applied, while the authors simply randomly sample cells within WSI if I understood correctly. Methods such as the one in Vo & al (2024), also studied recently for histopathology FM in [C] with tailored batch sampling strategies, could have been relevant to use in the paper.\n\n[C]  Chen, B., Vincent-Cuaz, C., Schoenpflug, L. A., Madeira, M., Fournier, L., Subramanian, V., ... & Rätsch, G. (2025, September). Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 554-564). Cham: Springer Nature Switzerland.\n\n- **W3: clarity**. It can be beneficial for the paper to clarify certain points, including the following\n     - a) The choice of SSL methods included in the study, namely MoCoV3 and DinoV2, is not clearly justified. Other methods, such as MAE, BeiT etc, could have been considered, knowing that the authors observed that DinoV2 was not performing that well within the LEMON framework. This matter should be discussed in the paper and therefore the distinction between SSL methods should be more detailed than the current one, being contrastive methods vs non-contrastive ones.\n     - b) In section 2.1 can you mention in the paper at which magnification the TCGA images have been processed, knowing that it includes both 20x and 40x images?  \n     - c) In Figure 3, could the authors had a last column with global performance computed as the averaged performance across datasets to ease comparison between configurations? \n\n\n- **W4: benchmark**. I believe that some points should be added/clarified in the current benchmark\n      - a) It is not clear in this type of applications why authors used a balanced accuracy to compare methods instead of e.g a macro f1 score or AUPRC. Could you provide these complementary metrics ? \n      - b) Moreover it seems to be important to underline that NuCLS comes from TCGA WSIs hence is an in-domain dataset. Finally, I believe that it would be relevant to include the PanNuke dataset which is a large dataset often used in the literature (see CellVIT and so on).\n\n- **W5. batch effect**. As suggested by the authors in their augmentation schemes and conclusion, batch effects are important to consider. It could be relevant to add to the paper an evaluation on that matter and also to consider benchmarks taking transformed images (e.g using Macenko or variants) as inputs instead of raw images."}, "questions": {"value": "I invite the authors to address/discuss the weaknesses mentioned above which are already mixed with questions for conciseness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tCr5Rte6pZ", "forum": "JAalsmy7bZ", "replyto": "JAalsmy7bZ", "signatures": ["ICLR.cc/2026/Conference/Submission18058/Reviewer_RDwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18058/Reviewer_RDwH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762457396676, "cdate": 1762457396676, "tmdate": 1762927845461, "mdate": 1762927845461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}