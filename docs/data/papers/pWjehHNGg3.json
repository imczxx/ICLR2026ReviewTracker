{"id": "pWjehHNGg3", "number": 24951, "cdate": 1758362325445, "mdate": 1759896740742, "content": {"title": "Machine Text Detectors are Membership Inference Attacks", "abstract": "Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model’s probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (ρ > 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce Mint, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks.", "tldr": "", "keywords": ["Membership Inference Attack", "Machine-generated Text Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6864c7b78666b0e6091d5f0ad378efe7a16116d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that membership inference attacks (MIAs) and machine-generated text detection (MGTD) are two faces of the same statistical problem: under standard asymptotic conditions, both are optimally solved by the same likelihood-ratio test comparing a target language model’s distribution to the true human-text distribution, which the authors prove is uniformly most powerful and whose maximum advantage is bounded via Pinsker’s inequality; they then recast many popular metrics—e.g., DetectGPT/Neighborhood (perturbation curvature), Binoculars (cross-model entropy), Min-K% variants, Reference/Zlib/DC-PDD—explicitly as approximations to this optimal statistic. Building on this unified view, the paper conducts large-scale experiments covering 7 state-of-the-art MIAs and 5 MGTDs across 13 domains and 10 generators, finding strong cross-task transferability (Spearman’s ρ≈0.66 overall, ρ≈0.78 for stronger methods) and, strikingly, that the MGTD method Binoculars attains state-of-the-art performance on MIAs as well; they also analyze outliers such as Zlib, whose poor transfer is attributed to differing priors (machine text being more compressible), and release MINT, a unified evaluation suite to facilitate fair, cross-task comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is the first to connect machine-generated text (MGT) detection with membership inference attacks (MIA), constituting a powerful theoretical contribution.\n2. Extensive empirical studies substantiate the conclusions.\n3. The figures and tables are clear and provide strong support for the claims."}, "weaknesses": {"value": "1. Can your theory be extended to multilingual settings?\n2. In **Theorem 2.3**, you state that all machine-generated text detection algorithms are effectively optimizing the single score $\\frac{L(x; M)}{L(x; Q)}$, and since the true human distribution $L(x; Q)$ is not directly accessible, perturbation-based methods (e.g., DetectGPT) approximate $L(x; Q)$ via multiple random perturbations. Therefore, the more perturbations that are performed—or the more perturbed segments that are generated—the higher the detection accuracy, correct? In the original DetectGPT paper, it seems only the claim that “generating more perturbed segments yields higher accuracy” is established; I am curious whether the other claim (i.e., increasing the number of perturbation rounds also improves accuracy) holds as well.\n3. I am very curious how certain attacks on MGT detectors affect your proposed optimal statistic—for example, the attack in paper [a]. When discussing attacks, we often assume that making LLM-generated text more human-like will better fool detectors; however, some attacks do not behave exactly this way. This paper [a] finds an interesting fact that is, if you use **I**n-**C**ontext **L**earning (ICL) to generate more human-like texts, the detectors tend to be more accurate. It would be great if the paper could include some discussion along these lines.\n\n[a] Wang, Yichen, et al. \"Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks.\" Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2024."}, "questions": {"value": "See weaknesses. I'm not **very sure** about my evaluation (especially the central part of MIA), and I will update (most likely raise) my ratings and confidence after rebuttal if the authors can address my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lUgMwtCWkz", "forum": "pWjehHNGg3", "replyto": "pWjehHNGg3", "signatures": ["ICLR.cc/2026/Conference/Submission24951/Reviewer_qgWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24951/Reviewer_qgWS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760606875253, "cdate": 1760606875253, "tmdate": 1762943258700, "mdate": 1762943258700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is an interesting paper which seeks to tie together the ideas of detection of machine generated text and of membership inference attacks. I think more of this type of paper should exist - as our field expands exponentially it is really useful to have analysis which tries to tie things together. This isn't necessarily a new idea, the authors cite two articles noting the similarity between 'Membership attack' and 'DetectGPT', each a fundamental idea in its own domain. The authors of this paper seek to build upon this by quantifying the transferability of techniques from one problem (detecting machine generated text) to the other (membership inference) using a range of strategies. \n\nGiven that the fundamental observation that membership attack and machine generated text detection may rely on similar methods isn't new, I think there's a fairly high bar to pass in terms of the quality of the analysis. I raise some concerns below about the robustness of the comparison to different methods of generating machine generated text and about the breadth of the analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There is a useful (and I think new) conclusion that Binoculars (developed for machine generated text detection) appears to outperform all of the specialised techniques for membership inference attacks on the problem of membership inference.\n\nThe writing is very clear, I am not very familiar with the literature on membership inference attacks but had no problem understanding the points that the authors were making."}, "weaknesses": {"value": "Essentially all of the complaints below boil down to not feeling that the analysis is sufficiently in depth. \n\n1) The choice of baselines for machine generated text detection appears incomplete, and nearly all are focused on the same essential strategy (comparing how likely a machine finds the text, either through log-likelihood or log-rank). I believe all of the techniques apart from Lastde take log-likelihood as their essential measure. What about other strategies taking a fundamentally different approach? e.g. those seeking to gain information from fluctuations in the log-likelihood, e.g. (Yang Xu, Yu Wang, Hao An, Zhichen Liu, and Yongyuan Li. 2024. Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 10108–10121, Miami, Florida, USA. Association for Computational Linguistics.) Or the intrinsic dimension approach Tulchinskii, E., Kuznetsov, K., Kushnareva, L., Cher-niavskii, D., Nikolenko, S., Burnaev, E., Baran-nikov, S., and Piontkovskaya, I. (2024). Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. Advances in Neural Information Processing Systems, 36.\n\n2) I'm not convinced by the way that figure 2 is plotted. Why does it make sense to plot performance rank against performance rank, rather than performance against performance?\n\n3) I find it hard to draw strong conclusions from Figure 3, when the numbers are so close. I don't think it's good that the experiments are run in a setting where all detectors perform so well, do we have enough data to conclude that AUROCs of 99.4 and 98.4 are meaningfully different?\n\n4) Beyond this, I don't think the experiment run with the three charts plotted in Figure 3 is particularly useful to evaluate the relative performances on different tasks. Should I think that the top and middle parts of Figure 3 are very similar or not? Logrank and Loss seem to have wildly different performances in the two settings. I think I would agree with the papers conclusion, 'These results provide\npromising evidence of the transferability of MIAs to machine text detection in real-world scenarios.', but I had hoped for more than promising evidence and instead a thorough and comprehensive analysis.\n\n5) There is a general trend within papers looking at detection of machine generated text (and ML more widely) for mutually contradictory rankings to appear in different papers. I think it's incumbent upon all of us writing in this domain to carefully guard against this, and I think more should be done in the present paper (since model ranking is the core experiment that it runs).\nLastde++ is shown as the worst performing text detector in this paper, whereas in the Lastde paper, Lastde++ strongly outperforms Fast-DetectGPT. (Lastde++ and Fast-DetectGPT is the only pair of detection strategies covered both in the Lastde paper and the paper under review). My suspicion is that this is due to generation strategy. The RAID benchmark considers texts generated both by pure sampling from a language model probability distribution and greedy decoding. It wasn't clear to me which texts were used in this evaluation (maybe both). It seems quite plausible that greedy decoding boosts likelihood based detectors such as FastDetect-GPT more than it does detectors such as Lastde. Additionally, texts in RAID appear to have been accidentally generated using top-k sampling, since this used to be a default in huggingface (see TempTest Appendix 8.2: Local Normalization Distortion and the Detection of Machine-generated Text: Tom Kempton, Stuart Burrell, Connor J Cheverall Proceedings of The 28th International Conference on Artificial Intelligence and Statistics, PMLR 258:1972-1980, 2025. Appendix 8.2). \n\nLet me reiterate that I think the authors have picked an interesting question, and I learned something by reading this paper, so thank you for that. But let me also stress that I think this kind of paper requires really excellent analysis of the different techniques, I don't think that patching up the criticisms above would necessarily be enough for me to recommend acceptance."}, "questions": {"value": "Please could you detail exactly the lengths of passages of text under review."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YZA5lFSqzC", "forum": "pWjehHNGg3", "replyto": "pWjehHNGg3", "signatures": ["ICLR.cc/2026/Conference/Submission24951/Reviewer_EPu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24951/Reviewer_EPu5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761136265575, "cdate": 1761136265575, "tmdate": 1762943258489, "mdate": 1762943258489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically builds a connection between two tasks: membership inference attacks (MIAs) and machine-generated text detection (MGT detection).\nIt both theoretically and empirically validates that the performance of 12 methods on the two tasks is correlated across many setups.\nThe paper also contributes a unified evaluation suite."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical section presents evidence for the target similarity of the two tasks.\n\n2. The experiments are comprehensive.\n\n3. It is surprising to find MGT detection methods perform pretty well on MIA tasks."}, "weaknesses": {"value": "1. The experimental setting is not perfectly aligned for the two tasks. For example, the MIA generators are all Pythia models, but Pythia is not within the MGT generators. If there is any reason for this, it should be mentioned in the paper.\n\n2. The theoretical section mainly suggests that the absolute performance is correlated, but the presented major results are on rank correlation, which weakens the findings. One reason for this might be the unaligned experiment setting.\n\n3. Lack of explanation of Binoculars' good performance on MIA. Is that because of the introduction of the second model (M_ref)? If so, why does DC-PDD, which also has an M_ref, not perform that well? Is that because Binoculars has an M_ref which is stronger than DC-PDD's and also the target model (Pythia)?"}, "questions": {"value": "1. Sec 2.4: It is unclear to me why \"the likelihood under the true distribution is approximated by the expected likelihood of multiple perturbations.\" Maybe some middle steps are missed here. For example, it is assumed here assuming the model's distribution is a narrower distribution compared with the true distribution, so that perturbation, though not under the model's distribution, is still within the true distribution. (I might be wrong) More explanation is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LIGWlcsbMX", "forum": "pWjehHNGg3", "replyto": "pWjehHNGg3", "signatures": ["ICLR.cc/2026/Conference/Submission24951/Reviewer_EQbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24951/Reviewer_EQbi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942313573, "cdate": 1761942313573, "tmdate": 1762943258235, "mdate": 1762943258235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors argue that membership inference attacks (MIAs) on large language models are equivalent, at least in the limit, to LLM text detection, and that the In order to support their argument they focus on two statistical tests and decision functions, one used by MIA method, the Neighbourhood Attack, and one - by a common LLM text detector, Binoculars, and prove their equivalence. Additionally, they prove that in the limit, the functions used by these decision functions are asymptotically optimal, at least with regard to a fixed false positive rate. The authors then validate their hypothesis by experimentally comparing the performance on a selection of MIAs and LLM detectors on both MIA and LLM tasks, and claim additional contribution by publishing a comprehensive mixed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Equivalence and optimality proofs are always valuable in the context of algorithm development, given their ability to provide definitive and incontestable insights into the performance of different method classes. The authors' focus on Type I error, most concerning in the context of LLM detectors, in their proofs is a strong point, making this paper more relevant to the LLM detectors community."}, "weaknesses": {"value": "I am not convinced of the interest or contribution of this paper, at least not to a level that warrants acceptance to a conference such as ICLR.\n\nSpecifically: \n- The author's theory hinges on the assumption that texts from its training dataset are generated by a model with a lower perplexity. While generally assumed, more recent results suggest that this might not be the case, with recent findings [1] indicating that even low-perplexity LLM-generated sequences do not map directly to the training data. \n- For LLM detectors, no evasion attacks are considered, despite being a critical problem in that field, as highlighted by the RAID benchmark paper authors cite themselves. In that setting, the models are known to be generating texts that are not present in their training dataset, and the problem of detection cannot be equivalent to the MIA problem, at least to the best of my understanding. \n- For MIA attacks, the authors are using a dataset known to induce post-hoc datasets with a positive bias, which has been criticized in the past as a poor proxy for the detection tasks, namely by [2].\n- I am not convinced of the value of the proofs. As mentioned by the authors, MIA and LLM text detection have been understood to be related. However, I am not convinced that a proof of equivalence for a specific MIA and a specific LLM detector with convenient detection functions adds value to the field.\n- Additionally, validation of the claims regarding the benchmark publication was impossible to validate, given that the linked repository returned file-not-found errors for all Python files in the directory structure except the top-level one.\n\n[1] Low-Perplexity LLM-Generated Sequences and Where To Find Them (https://aclanthology.org/2025.acl-srw.51/) Wuhrmann et al., ACL 2025\n[2] Meeus, M., Shilov, I., Jain, S., Faysse, M., Rei, M., & Montjoye, Y.D. (2024). SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It). 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), 385-401."}, "questions": {"value": "- What is the difference between black-box and white-box models for detectors? To the best of my understanding, none of them rely on text perplexities during the generation by design and treat both settings in the same fashion. \n\n- Why did you choose to use AUC as a metric, despite citing the landmark (Carlini et al. 2022) paper specifically arguing in the abstract for the use of TPR at fixed low FPR? \n\n- Could you please provide common information regarding experimental set-up (hardware, OS & drivers version, requirements.txt, and versions of unfrozen libraries), CO2 emissions estimation, GenML tools usage, ..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y7dROVYdAE", "forum": "pWjehHNGg3", "replyto": "pWjehHNGg3", "signatures": ["ICLR.cc/2026/Conference/Submission24951/Reviewer_U6kd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24951/Reviewer_U6kd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982754823, "cdate": 1761982754823, "tmdate": 1762943257991, "mdate": 1762943257991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}