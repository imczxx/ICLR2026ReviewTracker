{"id": "PLZx2hpauY", "number": 14637, "cdate": 1758240670921, "mdate": 1759897357871, "content": {"title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness", "abstract": "Models are susceptible to adversarially out-of-distribution (OOD) data despite large training-compute and research investments into their robustification. Zaremba et al. (2025) make progress on this problem at test time, showing that LLM reasoning aids achievement of top-level specifications designed to thwart attacks, resulting in a correlation between reasoning effort and robustness to jailbreaks. However, this benefit of inference compute fades when attackers are given access to gradients or multimodal inputs. We address this gap, clarifying that  inference-compute scaling can offer benefits even in such cases. Our approach argues that compositional generalization, through which OOD data is understandable via its in-distribution (ID) components, fuels successful application of defensive specifications to adversarially OOD inputs. Namely, we posit the Robustness from Inference Compute Hypothesis (RICH): inference-compute defenses profit as attacked data's contents become more in-distribution. We empirically support this hypothesis across various vision language models and attack types, finding robustness gains from test-time compute are present as long as specification following on OOD data is enabled by compositional generalization, while RL finetuning and long reasoning traces are not critical. For example, we show that adding test-time defensive specifications to a VLM robustified via adversarial pretraining causes the success rate of gradient-based multimodal attacks to fall, but this same intervention provides no such benefit to non-robustified models. This correlation of inference-compute's robustness benefit with base model robustness is the rich-get-richer dynamic of the RICH: attacked data components are more ID for robustified models, aiding the compositional generalization needed for OOD data. Accordingly, we argue for layering of train-time and test-time defenses to obtain their synergistic benefit.", "tldr": "Adversarial pretraining makes test-time compute scaling a better defense.", "keywords": ["adversarial robustness", "inference-compute scaling", "VLMs", "efficiency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d887a03381d7ef9a2902f06859ba9734dd586951.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a hypothesis, the \"Robustness from Inference Compute Hypothesis\" (RICH), which posits that spending compute at inference time as an adversarial defense technique is more effective the more in-distribution the contents of the attack are. They then perform a robustness exploration on three versions of the LLaVA VLM trained with different robustness \"strengths\", and show that the experiments support this hypothesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## originality\nThe \"RICH\" hypothesis is novel as far as I know. That something being in-distribution improves performance is not at all surprising, but formulating robustness in this way is not something I had heard about before\n\n## quality\nThe work appears to be of high quality. While the authors primarily study \"only\" three models from the LLaVA model family, they do a large number of experiments (plus, academic compute levels should not be punished imo).\n\n## clarity\nThe paper is above-average in terms of clarity. The clear narrative around RICH, and the use of boxes to showcase results, both significantly add to the presentation of the paper.\n\n## significance\nWhile I don't consider this result groundbreaking (as mentioned before, that the model is more robust in-distribution is not surprising), it is still a compelling and interesting piece of work which I believe adds to the (currently limited) literature on VLM adversarial robustness scaling."}, "weaknesses": {"value": "Not too many complaints.\n\n1) It would be nice to have a study where the authors themselves perform adversarial training, and then talk about increasing adversarial robustness as a result of increasing amount of adversarial training, akin to the approach in [1], in order to talk about models of different robustness levels. Using three different off-the-shelf models and simply stating they have low/medium/high levels of robustness feels like a strong assumption, and it's hard to rule out other confounders from the fact that the different variants were trained with different techniques. Is there a reason the authors were not able to perform adversarial training themselves (was it simply a scope/compute limitation, or is there another reason)?\n\n2) softening the language a bit around the RICH hypothesis \"definitely being the correct explanation\" and being supported by all the experiments (I'm particularly uncertain about whether Figure 3 supports it). Basically being a bit more humble with the language here, especially given a lack of experiments on frontier models.\n\n3) doing just a little bit more work in cleaning up the presentation and situating the work in the literature (I'll put a list of what to do in the questions).\n\n---\n\n[1] https://arxiv.org/abs/2407.18213"}, "questions": {"value": "* To what extent is it reasonable to say that the three models studied represent low/medium/high amounts of robustness? Is there a way to make this more convincing?\n* In Figure 3 (right), you show that increasing K improves robustness across models. But in line 359-361 (in the box), you seem to say that it only helps the already-more-robust model (Delta2). This seems wrong based on Figure 3 (right). It might seem true based on Figure 3 (left), but that's only because the attack saturates, so I don't think it's fair to say even based on that. Would you agree? Am I missing something?\n* Figure 1: what happens if you repeat the whole brown text (not just the bit in the curly braces)? Why did you not choose to repeat the whole instruction?\n* in Figure 2, why do you think that adding only a subset of the defenses (middle two columns) leads to *worse* performance (I would have expected it to be at least the same, if not better)?\n\n## Suggestions for improved clarity\n* Figure 1 caption: the bold text is not so clear what it's trying to say\n* line 160: why is there no box around the hypothesis as there is around the questions? seems it should be a fancy box even.\n* line 214: \"If a clear security...\" I'm not sure this sentence is true? Like I don't see how it is implied.\n* line 215: what does \"generalizing to a specification and adversarially OOD data\" mean?\n* all tables: remove vertical bars. See https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf\n* line 297: it's not clear where this question and where its answer come from\n* table 1: what is the \"up arrow\" next too the \"attacker loss\" text?\n* table 1: why is there (0.0) next to the Delta2 numbers?\n* table 1: what happens if you don't pre-fill the model response? Please mention in the main text why you needed to do this in order to get it to work.\n* 304: \"provides the same robustness benefits from applying the security specification\" I'm not sure what this means\n* 305: model -> model's\n* 415: traditional -> realistic"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eIdKOq1cVC", "forum": "PLZx2hpauY", "replyto": "PLZx2hpauY", "signatures": ["ICLR.cc/2026/Conference/Submission14637/Reviewer_Xw6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14637/Reviewer_Xw6j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761430954431, "cdate": 1761430954431, "tmdate": 1762925013453, "mdate": 1762925013453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether increasing inference time compute scaling (e.g., security specifications) can systematically enhance model robustness to white-box adversarial attacks in large vision language models. Specifically, the paper hypothesizes that models already robustified via adversarial training can leverage additional inference-time computation (e.g., reasoning, repetition, or Chain-of-Thought prompting) to further resist attacks, while non-robust models cannot. To validate this, the authors conduct a series of white-box and black-box adversarial attack experiments using multiple vision-language models (VLMs) with different robustness levels — including LLaVA-v1.5 (non-robust), FARE-LLaVA (moderately robust), and Delta2LLaVA (strongly robust) — and compare their performance under varying inference-compute settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper expands on Zaremba et al. (2025) by extending inference-time compute robustness analysis to white-box settings, where attackers have gradient access.\n- The results reaffirm the importance of adversarial training during pretraining, showing that such robustness is a prerequisite for inference-time compute to yield benefits under white-box or multimodal attacks."}, "weaknesses": {"value": "- While the paper aims to extend prior work (Zaremba et al. (2025)) by testing inference-time scaling under white-box attacks, this scenario may not be practical in real attack scenarios, as it assumes the attacker has full access to model parameters and gradients — a condition rarely met for attacking state-of-the-art closed-source models. Moreover, in the more practical black-box transfer attack setting, the paper’s results (e.g., Table 3) show that inference-time scaling provides little to no measurable robustness improvement, which weakens the real-world applicability of the proposed hypothesis and its claimed benefits.\n- The suggested inference-time scaling methods—such as security specifications or pre-filled responses—may require image-specific ground-truth prior information from the defender, which is impractical and non-scalable in real-world defense settings. Moreover, the paper does not examine how such interventions affect utility on clean images or whether a robustness–utility trade-off exists, leaving the practical impact of these defenses unclear.\n- The paper tests its hypothesis on only three models (LLaVA, FARE-LLaVA, and Delta2-LLaVA), which raises concerns about generalizability. With such a narrow set of architectures and training setups, it’s unclear whether the claimed relationship between base robustness and inference-time scaling effects truly holds in broader contexts (e.g., closed-source VLMs, or reasoning-tuned VLMs).\n- One of the paper’s central claims —that inference-time compute improves robustness primarily when adversarial inputs resemble the model’s training distribution—is conceptually expected rather than novel. It largely reiterates the established understanding that robustness diminishes as inputs deviate from the training manifold, rather than introducing a fundamentally new mechanism or theoretical perspective.\n- Also, the paper claims that even non-robust models can gain robustness from inference-time scaling when facing weaker (i.e., near in-distribution via reduced $\\epsilon$) attacks. However, this scenario appears impractical and weakly motivated: in realistic threat scenario, an attacker with full gradient access (as assumed in the white-box setup) would not choose a weak attack. Moreover, as shown in Figure 3 (right), once the reduced $\\epsilon$ PGD attack is applied for sufficient steps (≈50), the effect of inference-time scaling nearly vanishes."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ad9hudbsYI", "forum": "PLZx2hpauY", "replyto": "PLZx2hpauY", "signatures": ["ICLR.cc/2026/Conference/Submission14637/Reviewer_5kEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14637/Reviewer_5kEV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706007924, "cdate": 1761706007924, "tmdate": 1762925012792, "mdate": 1762925012792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Robustness from Inference Compute Hypothesis (RICH): inference-time compute defenses are most effective when the attacked data's components are closer to the model's in-distribution data. The authors provide empirical evidence across vision-language models, showing that inference compute (e.g., Chain-of-Thought, response pre-filling) significantly boosts robustness primarily for models that are already robust via adversarial training, creating a \"rich-get-richer\" dynamic."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Conceptual Framework: The RICH hypothesis offers a clear, falsifiable explanation for when inference-compute scaling succeeds or fails for robustness.\n- Systematic Evaluation: Rigorous experiments across a matrix of models, attacks, and compute levels.\n- Actionable Insight: The synergy between adversarial training and inference compute is a valuable finding."}, "weaknesses": {"value": "- Limited Scope of \"Compute\": The methods used (prompt repetition, pre-filling) are weak proxies for the complex, multi-step reasoning often implied by \"inference compute,\" limiting the generalizability of the findings.\n- Model Scale: Experiments are conducted on smaller VLMs. Validation on frontier-scale reasoning models (e.g., o1-preview) is needed to confirm the hypothesis holds for the most advanced systems.\n- Incremental Novelty: The core idea—that generalization benefits from in-distribution data—is foundational. The paper's contribution is applying this to inference-time defense, but the conceptual leap is moderate."}, "questions": {"value": "- Given the reliance on compositional generalization, how do you expect the RICH hypothesis to apply in purely textual domains against sophisticated jailbreaks?\n- Is there a point of diminishing returns for inference compute (K) on robust models, or do the benefits continue to scale linearly in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wh3IDNCbm3", "forum": "PLZx2hpauY", "replyto": "PLZx2hpauY", "signatures": ["ICLR.cc/2026/Conference/Submission14637/Reviewer_18GW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14637/Reviewer_18GW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855577708, "cdate": 1761855577708, "tmdate": 1762925012263, "mdate": 1762925012263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Robustness from Inference Compute Hypothesis: that the closer an attack's contents are to being in-distribution for the model, the more inference-time compute scaling improves robustness. It then suggests that robustified models can defend against white-box or multi-modal attacks, since the adversarial attacks are more within distribution for the model. It studies this in the vision language model setting with empirical experiments. The paper uses various LLaVA-based models with increasing amount of adversarial training, and finds that using more inference scaling in the form of adding security specifications or pre-filling improves robustness, as well as CoT-prompted VLM models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper proposes the RICH theory for explaining when adversarial robustness benefits from inference compute scaling.\nQuality: The main claims of the paper are supported by the experiments\nClarity: The paper is generally clear and well-written, with separately broken out hypotheses and research questions.\nSignificance:\n* Addresses adversarial robustness in VLMs, which has unique challenges compared to other settings\n* The paper addresses an open question in from Zaremba et al. (2025)"}, "weaknesses": {"value": "Clarity: \n\nW1) I needed to read over things a few times to shift gears between thinking about whether or not the data is in-distribution for the model, to whether or not the data is in-distribution for _following the security specification_. I think I now understand that the paper has a large focus on confirming that adversarially trained models are more able to generalize their instruction following training to enforcing the security specification on adversarial inputs.\n\nW2) The story around prefilling is less fleshed out than the story around the security specification."}, "questions": {"value": "My main questions are around supporting the specific details of the RICH hypothesis more thoroughly.\n\nQ1) How are the experiments different from the null hypothesis that models which have undergone more adversarial training are more adversarially robust?\n\nQ2) Data can be more or less in-distribution even without adding adversarial attacks, for instance by choosing a particular domain not well represented in the training data, or by adding non-adversarial noise or transformations to an image. Are there results which suggest that modifying _in-distribution-ness_ itself helps to modulate adversarial robustness for robustified and non-robust models, besides varying the number of PGD steps in Table 2?\n\nQ3) Inference time scaling in the form of using more CoT tokens rather than more repetitions of externally created text seems to be fairly different. Is there a relationship between CoT length and adversarial robustness? The main experiment that investigates inference time scaling appears to be in Table 2.\n\nQ4) What's the relationship between the prefill and inference compute scaling? It's unclear to me if the paper explored repeating the prefill or not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RnjIenF64Y", "forum": "PLZx2hpauY", "replyto": "PLZx2hpauY", "signatures": ["ICLR.cc/2026/Conference/Submission14637/Reviewer_rpuG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14637/Reviewer_rpuG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762321213654, "cdate": 1762321213654, "tmdate": 1762925011843, "mdate": 1762925011843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}