{"id": "BXwMPYDsT5", "number": 8687, "cdate": 1758095007380, "mdate": 1759897769621, "content": {"title": "Merge-of-Thought Distillation", "abstract": "Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite the practical\navailability of multiple candidate teachers and growing CoT corpora. We revisit\nteacher selection and observe that different students have different “best teachers,”\nand even for the same student, the best teacher can vary across datasets. Therefore, to unify multiple teachers’ reasoning abilities into a student to overcome conflicts among various teachers’ supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific\nsupervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including\nDeepseek-R1, Qwen3-32B, and OpenAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation, improves\ngeneral reasoning beyond mathematics while reducing catastrophic forgetting, and\nshows robustness to distribution-shifted and peer-level teachers. Finally, we have\ndemonstrated MoT possesses consensus CoT by eliminating teacher-specific inductive biases and inter-teacher conflicts while repeatedly reinforcing the learning\nof consensus reasoning features. These results position MoT as a simple, effective route to efficiently distilling long CoT capabilities from diverse teachers into\ncompact students.", "tldr": "", "keywords": ["Large language model", "Long chain-of-thought", "Distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86661dd9a4a10ab7ee0f4110ed00d936b6dd8856.pdf", "supplementary_material": "/attachment/aa3859996ea674d3ab94917c5005da7b46619cdf.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes \"Merge-of-Thought Distillation (MoT)\", a framework that alternates between teacher-specific branch fine-tuning and weight-space merging to consolidate the chain-of-thought reasoning capabilities of multiple large language models into a single student model. The authors claim that with only about 200 samples, their method can surpass some larger models on mathematical reasoning benchmarks and mitigate catastrophic forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1) It provides a systematic study of multi-teacher long chain-of-thought distillation, addressing a gap in the existing literature which often focuses on single-teacher settings.\n2) The experimental design is comprehensive, covering various model scales and datasets, and is supported by thorough ablation studies and theoretical analysis."}, "weaknesses": {"value": "1) The core method of \"branch training and weight averaging\" is essentially a straightforward application of existing model merging techniques, lacking substantial algorithmic or structural innovation. The overarching idea of multi-teacher fusion is not a breakthrough in itself.\n2) The paper identifies the critical problem of \"teacher selection mattering\" but does not address it; instead, the method circumvents it by merging all teachers. It lacks dynamic assessment of teacher quality or a weighted merging mechanism.\n3) The core methodology is overly simplistic and does not incorporate more advanced fusion strategies. The experiments fail to adequately demonstrate its generalization capability to non-mathematical tasks, and the trade-off between computational cost and performance gain is not sufficiently discussed.\n4) Some baseline results are taken from original papers without reproduction under a unified setup, potentially compromising fairness. The absence of comparisons with more state-of-the-art multi-teacher distillation or fusion methods weakens the persuasiveness of the claims."}, "questions": {"value": "1) The paper compares MoT against naive multi-teacher data mixing (MTD) and one-shot weight merging. However, a more fair baseline would be an ensemble of the multiple independent teacher models, i.e., averaging their output probabilities at inference time. What are the significant advantages of MoT over such an output-level ensemble in terms of performance, efficiency, or stability?\n2) Weight averaging is a very basic and sensitive merging technique. Did the authors experiment with more advanced merging algorithms, such as TIES-Merging or Task Arithmetic? If so, what were the results? If not, why was such a simple method chosen as the core innovative component?\n3) The paper lacks comparisons with state-of-the-art methods specifically designed for multi-teacher or data fusion. For example, what are the distinct advantages and disadvantages of MoT compared to Gradient Blending (weighting teachers at the gradient level) or DARE (a pruning method before weight merging)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UpCsf30woo", "forum": "BXwMPYDsT5", "replyto": "BXwMPYDsT5", "signatures": ["ICLR.cc/2026/Conference/Submission8687/Reviewer_HWmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8687/Reviewer_HWmF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932612119, "cdate": 1761932612119, "tmdate": 1762920498572, "mdate": 1762920498572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of distilling long chain-of-thought (CoT) reasoning from multiple teacher models. The authors start with the key observation that there is no single \"best teacher\" for all students or datasets. To address this, they propose Merge-of-Thought Distillation (MoT), a lightweight, iterative framework. In each round of MoT, the student model is branched out and finetuned separately on the CoT data from each individual teacher. These branches are then merged back into a single model via weight averaging. This merged model becomes the initialization for the next round. The authors show that this method is highly data-efficient, enabling a Qwen3-14B student trained on only ~200 CoT samples to surpass strong baselines like Deepseek-R1 and Qwen3-32B. The paper also provides analysis suggesting MoT mitigates catastrophic forgetting, is robust to distribution-shifted teachers, and creates a \"consensus CoT\" in a flatter loss landscape."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strong Motivation & Problem: The paper is motivated by a clear, practical, and important problem. The initial analysis (Fig. 1, Table 1) showing that the optimal teacher varies by student and dataset is a strong justification for moving beyond single-teacher distillation.\n\nImpressive Empirical Results: The performance gains are the primary strength of this paper. The claim that MoT with only 200 samples can elevate a 14B model to outperform a 32B model (Table 3) is a very strong and compelling result.\n\nData Efficiency: The ability to achieve these results with such a small, 200-sample dataset is highly significant and makes the method practical and accessible.\n\nThorough Analysis: The authors provide a comprehensive set of experiments, comparing MoT not only to the best single-teacher distillation (STD) but also to naive multi-teacher data mixing (MTD) and one-shot post-hoc merging (Table 2, Table 4). The additional analyses on catastrophic forgetting (Table 5) and the loss landscape (Appendix D) are valuable additions."}, "weaknesses": {"value": "Despite the strong results, the paper's contribution seems to lie more in a successful application of existing techniques than in a novel methodological breakthrough, which makes it borderline.\n\n- Limited Novelty: The core mechanism of MoT (iteratively training separate models on sharded data and averaging their weights) is conceptually almost identical to well-established algorithms like Federated Averaging (FedAvg) in federated learning. While the application (multi-teacher CoT distillation) is new, the method itself (iterative SFT + weight averaging) is not. The paper does not sufficiently differentiate MoT from this and other related model-merging literature.\n\n- Underdeveloped Merging Strategy: The \"Merge\" in \"Merge-of-Thought\" is implemented as simple parameter averaging. Given the extensive and recent literature on more sophisticated model merging techniques (e.g., TIES, DARE, task arithmetic), restricting the method to simple averaging feels like a missed opportunity. It is unclear if the gains come from the iterative process alone, or if the merging step itself could be further optimized.\n\n- Unclear Computational Cost: The paper emphasizes data efficiency (200 samples) and total \"steps\" (250). However, the MoT process involves training K (here, K=4) branches for 50 steps in each of 5 rounds. This totals (4 branches * 50 steps/branch * 5 rounds) = 1000 total branch-steps, which is 4x the computational cost of the 250-step STD/MTD baselines. This significant compute-for-data trade-off is not clearly discussed.\n\n- Indirect Evidence for \"Consensus CoT\": The claim that MoT \"eliminates teacher-specific inductive biases\" is very strong. The evidence provided (Fig. 5) shows that MoT has lower confidence on stylistic tokens from one teacher. This is interesting but indirect; it could be interpreted as the averaging process simply \"washing out\" or \"diluting\" a specific teacher's signal, rather than intelligently \"eliminating\" its bias while preserving a core \"consensus.\""}, "questions": {"value": "1. Could the authors explicitly discuss the relationship between MoT and Federated Averaging (FedAvg)? How do they see the conceptual novelty of MoT in light of FedAvg, which also performs iterative, sharded training followed by weight averaging?\n\n2. Why was simple parameter averaging chosen as the only merging strategy? Were more advanced merging techniques explored? Would they be compatible with the iterative MoT framework, and could they potentially offer further gains?\n\n3. Could the authors provide a clear comparison of the total computational cost (e.g., total branch-steps or GPU-hours) for MoT versus the STD and MTD baselines? This would clarify the compute/data/performance trade-off."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CeotRd0Xa", "forum": "BXwMPYDsT5", "replyto": "BXwMPYDsT5", "signatures": ["ICLR.cc/2026/Conference/Submission8687/Reviewer_Came"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8687/Reviewer_Came"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947333695, "cdate": 1761947333695, "tmdate": 1762920498234, "mdate": 1762920498234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Merge-of-Thought Distillation (MoT), a novel framework for distilling long chain-of-thought (CoT) reasoning capabilities from multiple, potentially heterogeneous, teacher LLMs into a single, smaller student model. The core motivation is that there is no single \"best teacher\" for a given student and dataset, and naive methods like mixing teacher data or one-shot model merging fail to reconcile conflicting supervision signals. MoT addresses this by iteratively alternating between two steps: (1) Teacher-Specific SFT (Branch Training), where the student is fine-tuned on each teacher's reasoning paths, and (2) Weight-Space Merging, where the parameters of these specialized branches are averaged. This process is designed to reinforce consensus reasoning features while suppressing teacher-specific noise and inductive"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "While model merging and CoT distillation are active areas of research, their combination in an iterative, multi-teacher co-distillation framework is novel. \nThe paper is generally well-written and structured."}, "weaknesses": {"value": "The premise of the paper, \"different students have different best teachers,\" is only derived from two math evaluation datasets. An insufficient number of evaluation datasets may lead to biased conclusions.\nThe core merging operation is a simple uniform average of parameters. While the results show this is effective, the field of model merging has advanced with more sophisticated techniques. The paper would be strengthened by a direct comparison against a more advanced merging baseline to confirm that the gains are from the iterative process and not just a more clever one-shot merge."}, "questions": {"value": "In evaluation sets outside of mathematical reasoning, does the phenomenon of \"Teacher choices are not universal\" still exist?\nIs the number of teacher models the more, the better?\nHow does the model perform on the HMMT benchmark?\nHow were BOBA-200 and S1k-200 sampled? Are there any guiding principles? For reasoning tasks [1], the quality of prompts obtained through random sampling varies significantly.\nHow does this method perform on other series of models, such as LLaMA?\n\n[1] Reinforcement Learning for Reasoning in Large Language Models with One Training Example"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJUDFjMhKx", "forum": "BXwMPYDsT5", "replyto": "BXwMPYDsT5", "signatures": ["ICLR.cc/2026/Conference/Submission8687/Reviewer_6CXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8687/Reviewer_6CXT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988696566, "cdate": 1761988696566, "tmdate": 1762920497785, "mdate": 1762920497785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Merge-of-Thought Distillation (MoT), an iterative multi-teacher CoT \ndistillation framework that alternates between (a) teacher-specific supervised fine-tuning \nbranches and (b) weight-space averaging of the resulting student variants. The aim is to fuse \ncomplementary reasoning signals from heterogeneous teachers while suppressing teacher-\nspecific noise and conflicts. Empirically, the authors show that the “best” single teacher depends \non both the student and the dataset, and that naïve multi-teacher data union or one-shot post-hoc \nmerging does not reliably resolve cross-teacher conflicts. MoT is reported to outperform single-\nteacher and naïve multi-teacher baselines on AIME24/25 using only ~200 CoT exemplars, with \nadditional evidence of smaller catastrophic-forgetting drops and gains on non-math reasoning \nbenchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The branch-train + weight-merge loop is easy to implement; it sidesteps brittle manual teacher selection.\n2. Demonstrates competitive AIME improvements using only ~200 CoT samples, even versus larger models.\n3. MoT not only strengthens general reasoning but also helps mitigate catastrophic forgetting compared to best STD"}, "weaknesses": {"value": "1. Experiments focus primarily on the Qwen family and math-centric CoT, with limited  coverage of other backbones or domains. This narrows external validity. The paper itself  notes comparisons primarily around Qwen bases.\n2. Several baseline results are taken from papers with different setups, as the original  code/models are unavailable. This weakens the claim of direct superiority.\n3. MoT requires training multiple branches across several rounds, making it more compute-intensive than single-teacher distillation, while often underperforming STD in  the first round before eventual gains appear."}, "questions": {"value": "1. How sensitive are MoT gains to random seeds across both branch training and merging rounds, when using a fixed early-stopping rule rather than “best of” selection? Please report mean±std over multiple independent runs.\n2. Have you tested LLaMA, Mixtral, or Mistral-based models? Are MoT gains specific to Qwen?\n3. Have you tested MoT on other domains (e.g., code reasoning or scientific QA)? If not, what are the expected limitations?\n4. How does MoT behave when one teacher is adversarial or noisy? Is the method robust to such imbalance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyfZzizote", "forum": "BXwMPYDsT5", "replyto": "BXwMPYDsT5", "signatures": ["ICLR.cc/2026/Conference/Submission8687/Reviewer_9K2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8687/Reviewer_9K2Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041444090, "cdate": 1762041444090, "tmdate": 1762920497373, "mdate": 1762920497373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}