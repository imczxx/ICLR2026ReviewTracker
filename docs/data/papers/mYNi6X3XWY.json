{"id": "mYNi6X3XWY", "number": 13377, "cdate": 1758217140720, "mdate": 1759897441641, "content": {"title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.", "tldr": "", "keywords": ["Vision-Language Models", "Spatial Reasoning", "Cross-Viewpoint Localization"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a40e91149ca2a7f27726698014e94f757025343d.pdf", "supplementary_material": "/attachment/72a23771920b5b8010efe8aa4ec86d7acda0ba92.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduce ViewSpatial-Bench, a comprehensive new benchmark with an automated 3D annotation pipeline, to systematically evaluate the capability of multi-perspective spatial localization with five different tasks. The paper also shows by training on the curated large-scale dataset, the model performance on this benchmark improves by 40%, with generalizability into embodied interaction scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive benchmark ViewSpatial-Bench covering five different tasks from both camera and human perspectives. \n2. Extensive experiments revealing the common failure of current models in multi-perspective spatial localization and performance improvement with fine-tuning. \n3. The paper is well written."}, "weaknesses": {"value": "1. Concerns regarding overfitting and generalization. The performance leap of MVSM is suspicious and raises the question of whether the model has learned a generalizable skill of perspective-taking or has simply memorized the patterns in the ViewSpatial-Bench training set. The results on VSI-Bench show a much more modest improvement (+2.37% average), suggesting that the learned skill may not transfer as effectively to an out-of-distribution benchmark. \n2. Comparison with previous work. As shown in Table 2, the difference between ViewSpatial-Bench and SPHERE only lies in 3D-Coord, which seemingly is not used in the experiments. Considering other similar benchmarks such as MindCube, this raises concerns regarding the novelty of the work. In addition, the benchmark only covers in-door scenes (from ScanNet), which is acknowledged by the author at L459. This raises concerns when it comes to outdoor environments. \n3. The paper introduces the \"Multi-View Spatial Model (MVSM)\", which is not a novel model design, but a VLM fine-tuned on the proposed dataset. This is a bit overclaiming and should be made clearer."}, "questions": {"value": "See as above in weaknesses. I'm happy to adjust the scores if the author can address the three concerns in the weaknesses.\n\nFormat: table title should always appear before the table."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAVuGdr3ZY", "forum": "mYNi6X3XWY", "replyto": "mYNi6X3XWY", "signatures": ["ICLR.cc/2026/Conference/Submission13377/Reviewer_CiyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13377/Reviewer_CiyB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390628392, "cdate": 1761390628392, "tmdate": 1762924019026, "mdate": 1762924019026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work leverages two well known datasets (ScanNet and CoCo) to build a new resource focused on perspective taking.  Questions are phrased as from the perspective of individuals in the image (their right vs right side of image)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clearly we want visual models to have the same abilities as humans -- building representations of the implied 3D scene so that they can reason about relations and perspectives. The work is evaluated on a suite of appropriate models and a model is trained on the task directly."}, "weaknesses": {"value": "1. See below, I'm unclear on what we learn from the FT-ing experiments \n2. I have a slight concern about the diversity of the data and the domains chosen, both of which are very canonical.  Given that what might be a small amount of training nearly solves this dataset, the longevity of the work is in question and makes the reader wonder what could be done to build a more robust evaluation."}, "questions": {"value": "- Data generation describes the presence of distractors (also shown in Fig 4), should I interpret evaluation as multiple choice? (random baseline shifts by category)\n\n*Evaluation*\nI'm having trouble understanding the training and corresponding evaluation claims.  \n- \"Applicability of our training methodology\" just means that you did SFT in domain, correct? Why is it surprising that this improved performance? \n- You then only perform ZST comparisons for the pretrained models.  Is there a reason that few shot cannot be run to give the model the basic task structure?  \n- Related concern is how often the models generated answers that weren't in the set of options? For example \"back\" or \"left\" when \"back-left\" was required\n- Can training curves be provided for the fine-tuned model, or additional details on how many samples were required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51bnPPYDpd", "forum": "mYNi6X3XWY", "replyto": "mYNi6X3XWY", "signatures": ["ICLR.cc/2026/Conference/Submission13377/Reviewer_FvxK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13377/Reviewer_FvxK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961447457, "cdate": 1761961447457, "tmdate": 1762924018634, "mdate": 1762924018634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ViewSpatial-Bench, a new benchmark for evaluating multi-perspective spatial localization in VLMs across five tasks covering camera-centric and human-centric viewpoints. The authors develop an automated 3D spatial annotation pipeline to construct QA pairs from images in ScanNet and MS-CoCo.  Experiments show that competitive VLM struggle with cross-viewpoint spatial reasoning. To address this, they train a Multi-View Spatial Model on spatially annotated samples, achieving ~46% absolute improvement over the base model. In addition, to validate MVSM's generalization abilities in practical applications, the authors evaluated on VSI-Bench and VSI-App benchmarks and showed improved performance over baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is logically coherent overall.\n- The paper is easy to understand. \n- The research problem is important in the VLM community.\n- The proposed MVSM method is effective."}, "weaknesses": {"value": "As a dataset and benchmark paper, the overall quality control and evaluation rigor fall short of expectations for publication at a top venue like ICLR.\n\n**Insufficient Human Annotation & Lack of Quality Control**\n- Only 864 out of 5,712 samples received human annotation, and no inter-annotator agreement or reliability metrics are reported. This is highly concerning, particularly since the authors acknowledge that automated annotation is unreliable for human-perspective tasks. Stating that annotation is “complex” is not an excuse for minimal human involvement—if the majority of samples are not manually verified, the benchmark’s label quality is questionable, and reported model performance numbers may not be trusted. A rigorous benchmark should involve multi-annotator labeling (potentially combined with the automated pipeline), iterative refinement to eliminate systematic annotation errors, and reporting of agreement scores alongside clear annotation protocols. I would be happy to reconsider once the majority of the test samples are carefully annotated. \n\n**Answer Validity from Images Alone**\n- Since questions are generated from metadata rather than from human interpretation of the images, it is unclear whether the image alone always contains sufficient visual information to yield a unique and unambiguous answer. It remains questionable whether a model—or even a human—can reliably answer some questions solely based on the provided image without access to metadata. \n\n**Problematic or Ambiguous Questions**\n- Several questions appear under-specified, or ambiguous. For example, in Figure 9, the prompt “Standing at table, gazing at chair, where should books be?” has the provided answer “front”. However, the desk is pretty large, and depending on where one stands around the table, “right” could be an alternative answer. Such cases indicate inconsistencies in spatial frame-of-reference grounding and suggest the benchmark may introduce artificial or unclear phrasing not aligned with natural human spatial reasoning.\n\n**Lack of Statistical Rigor in Reporting Results**\n- Table 2 and Table 3 present accuracy values without confidence intervals, variance, or statistical testing. As this is a benchmark paper, stronger evidence of robustness and significance is needed. Reporting standard deviations or significance tests is essential to support claims of superiority and to ensure results are reliable.\n\n**The Dataset Is Not Realistic**\n- The dataset only covers elementary elements of spatial direction (front, back, left, right, etc.). A useful dataset should cover more realistic tasks such as asking the distances and navigation."}, "questions": {"value": "- The human performance baseline is missing. What is the performance of humans in these tasks? \n- How to ensure that visual information is sufficient and the answer is uniquely determined (without relying on metadata)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "es6HB4GAYZ", "forum": "mYNi6X3XWY", "replyto": "mYNi6X3XWY", "signatures": ["ICLR.cc/2026/Conference/Submission13377/Reviewer_eAeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13377/Reviewer_eAeM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970026551, "cdate": 1761970026551, "tmdate": 1762924018158, "mdate": 1762924018158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the limitations of current vision-language models in reasoning about spatial relationships from different viewpoints. While these models handle spatial reasoning from their own (camera-centered) perspective, they often fail to interpret scenes from another entity’s point of view. To study this issue, the authors introduce ViewSpatial-Bench, a benchmark designed to evaluate multi-viewpoint spatial localization through five types of tasks that involve both camera and human perspectives. The benchmark is constructed using an automated 3D annotation pipeline that produces directional labels for a diverse set of images. Experiments on various vision-language models show that performance declines when models are required to reason from non-egocentric perspectives. The authors further fine-tune a model on their dataset, referred to as the Multi-View Spatial Model, which achieves higher accuracy, suggesting that incorporating explicit 3D spatial information can improve perspective-based reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and well-motivated problem formulation, identifying the lack of perspective-taking ability in current multimodal systems and linking it convincingly to challenges in embodied AI and human–robot interaction.\n\n2. The proposed ViewSpatial-Bench is a systematically designed benchmark encompassing five tasks that jointly evaluate egocentric and allocentric reasoning. Its integration of automated 3D annotation with human verification provides a scalable and reliable framework for assessing spatial understanding.\n\n3. The experimental evaluation is thorough, covering a range of major VLMs (e.g., GPT-4o, Gemini-2.0) and revealing consistent performance asymmetries between egocentric and allocentric settings. The accompanying analysis offers useful diagnostic insights into model limitations.\n\n4. The fine-tuned Multi-View Spatial Model (MVSM) demonstrates consistent and interpretable improvements across tasks and generalizes to external benchmarks (VSI-Bench, VSI-App). Additional analyses on backbone variation and answer formats support the robustness of the findings.\n\n5. Overall, the paper is well-written and clearly presented, with informative figures, tables, and methodological descriptions that facilitate reproducibility."}, "weaknesses": {"value": "1. The methodological novelty is limited. While the benchmark is comprehensive, its conceptual basis—evaluating and fine-tuning for 3D spatial reasoning—builds directly on existing work. The MVSM primarily extends a Qwen-VL baseline through additional spatially annotated data rather than introducing new model architectures or explicit 3D reasoning mechanisms.\n\n2. The comparative analysis omits several relevant baselines, including specialized models such as SpatialVLM, SpatialPin, SpatialReasoner, Space-Qwen, and the Gemini 2.5 series, which would strengthen the empirical claims.\n\n3. The paper’s assertion of being the “first comprehensive benchmark” is somewhat overstated, as prior works (e.g., 3DSRBench, SPHERE, All-Angles Bench) already address similar multi-view evaluation objectives. The contribution lies more in the dataset scale and integration than in conceptual originality."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vcCFFx5zQ0", "forum": "mYNi6X3XWY", "replyto": "mYNi6X3XWY", "signatures": ["ICLR.cc/2026/Conference/Submission13377/Reviewer_qX8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13377/Reviewer_qX8B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977441339, "cdate": 1761977441339, "tmdate": 1762924017732, "mdate": 1762924017732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}