{"id": "WHVk2qoCIY", "number": 25240, "cdate": 1758365596772, "mdate": 1759896728177, "content": {"title": "Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting", "abstract": "LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS.  We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting.  We introduce DHARMA, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is here https://anonymous.4open.science/r/SafeAgents/ .", "tldr": "We introduce SafeAgents, a framework for evaluating security vulnerabilities in multi-agent LLM systems, revealing that popular architectures contain significant security flaws stemming from design choices like autonomy levels and context sharing.", "keywords": ["Multi-agent systems", "Vulnerability Attacks", "Security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d6aaacd05f8e26949a2377c2ded7905fe48cbd5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SAFEAGENTS, a unified framework for evaluating the safety of multi-agent systems (MAS) under adversarial prompting. It proposes a new diagnostic metric called DHARMA that classifies failures across planner and sub-agent levels, aiming to identify “weak links” in different architectural designs. The authors evaluate different representative MAS frameworks across four safety benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The topic is timely and relevant as multi-agent systems are increasingly being deployed in real-world applications, yet their safety remains underexplored.\nThe experiment is also comprehensive, covering multiple architectures and benchmarks."}, "weaknesses": {"value": "The writing requires significant improvement. \n\nFor instance, in lines 301–303, the authors write: “Planner-Failed identifies a critical failure mode where the planner does not refuse but fails to generate a valid executable plan, yet the system continues execution despite a valid plan.” This sentence is self-contradictory — the planner “fails to generate a valid plan,” yet the text later says “despite a valid plan.” Presumably, the authors meant “an invalid plan.” Such inconsistencies seriously affect readability.\n\n\nLine 356: “for example” should not be capitalized mid-sentence.\n\n\nLine 351: “ReAct” is mentioned without citation or definition; it appears abruptly without prior explanation or connection to Table 1.\n\n\nLine 459: the abbreviation “SLM” is unclear — if it refers to Small Language Model, it should first be written in full when it appears for the first time. Furthermore, it is unclear why SLMs are introduced at all. Most experiments in the paper involve large, frontier models, so the inclusion of SLMs requires explicit motivation. If the authors believe that smaller models offer unique practical advantages (e.g., efficiency or on-device deployment, as discussed in [1]), this rationale should be articulated and justified.\n\n\nThe writing can also be improved by reducing redundancy. Integration Complexity (line 221) and Lack of Systematic Comparison (line 235) essentially describe the same problem — fragmented evaluation pipelines. These can be consolidated into one coherent discussion to improve focus and reduce redundancy.\n\nThe paper does not clearly describe how the DHARMA Classification Result is computed. Since DHARMA is presented as a “classification” metric, it is unclear why the reported results are continuous values exceeding 1. A clear mathematical formulation or example trajectory would help readers understand how these scores are derived.\n\nThe section “Implementation choices within the same architectural family create substantial security variations” is one of the most promising parts of the paper, but remains underdeveloped. It would be valuable to provide concrete examples of how specific design decisions (e.g., sub-agent autonomy, context organization, or planning strategy) increase or mitigate vulnerability. Doing so would offer actionable insights for practitioners seeking to design more secure MAS architectures. In this regard, more detailed descriptions of the implementation differences across the included frameworks (e.g., Magentic-One, LangGraph, OpenAI Agents) would make the analysis substantially more informative than the current high-level division between “centralized” and “decentralized” systems. Since the high-level ideas of them are similar, but those subtle nuances make them perform differently.\n\nThe four benchmarks covered in the experiments represent distinct threat models — such as direct injection versus indirect injection — which correspond to fundamentally different vulnerability types. The paper would benefit from an explicit comparison across these threat models, discussing which architectural designs are more robust under which attack categories. Such analysis would make the conclusions more actionable for the broader research community."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RBw4y5kldi", "forum": "WHVk2qoCIY", "replyto": "WHVk2qoCIY", "signatures": ["ICLR.cc/2026/Conference/Submission25240/Reviewer_v7hB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25240/Reviewer_v7hB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537171831, "cdate": 1761537171831, "tmdate": 1762943377470, "mdate": 1762943377470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SAFEAGENTS, a unified evaluation layer for multi-agent systems across popular frameworks, and DHARMA, a taxonomy for labeling where safety breaks down (planner vs sub-agent vs unmitigated execution vs error). Experiments span several benchmarks (jailbreaks, prompt-injection, code exec, web tasks) and compare single-agent vs centralized/decentralized MAS."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual move from “did it refuse?” to “where did it fail?” is overdue and useful in practice.\n\n2. DHARMA’s top-level split (planner vs sub-agent) is intuitive and maps to actionable design knobs.\n\n3. SAFEAGENTS abstraction could enable apples-to-apples comparisons across frameworks instead of the usual benchmark bingo.\n\n4. It shows centralized planners can spread bad plans efficiently when guardrails wobble."}, "weaknesses": {"value": "1. The DHARMA taxonomy contains internal inconsistencies that make categories non-exclusive. In particular, the leaf describing planner failure simultaneously references the absence of a valid plan and the continuation “despite a valid plan,” which is self-contradictory.\n\n2. The paper relies on a single LLM-as-judge to assign DHARMA labels, yet it provides no human adjudication study, no second judge model, and no error bars. Because the core claims depend on these fine-grained labels, the absence of inter-rater reliability and sensitivity analyses leaves the results vulnerable to judge-specific artifacts.\n\n3. Mechanistic claims are supported by anecdotes rather than prevalence analysis. The paper highlights illustrative traces such as sub-agent refusal being ignored or execution continuing after planning failure, but it does not quantify how frequently these patterns occur within or across datasets."}, "questions": {"value": "1. How do you impose a deterministic, mutually exclusive labeling scheme for DHARMA on multi-turn trajectories, especially when the planner first refuses and later proceeds, and will you correct the contradictory \"planner failed yet valid plan\" leaf by publishing explicit state-transition rules, tie-break priorities, and worked examples that map concrete log events to exactly one label with reported human/LLM agreement?\n\n2. What is the empirical reliability of your LLM-as-judge pipeline, and how sensitive are your conclusions to the choice of judge model? You assert manual classification is impractical and default to an LLM judge, but you never report human-human agreement, human-LLM agreement, or a second-judge ablation, even though the entire contribution hinges on these fine-grained labels.\n\n3. Which design primitive actually causes the reported safety deltas once you remove framework confounds, and do those effects persist across different threat models? Right now you attribute differences to stratified versus combined planning and to “context organization,” but those vary alongside prompt scaffolds, retry logic, tool wrappers, and message ordering across frameworks; so far that’s correlation with a cape on.\n\n4. Additional question on paper type: this paper reads more like a positional paper, and what concrete experimental depth elevates it beyond framework evangelism? Much of the writing emphasizes the SAFEAGENTS abstraction and the taxonomy rhetoric, with generous prose on classes, methods, and configurability, but the decisive evidence is thin where it matters most: validated labels, single-primitive causal ablations, stratified threat-model results, and uncertainty reporting. If the aim is a main-track paper, you need to move beyond “we propose a framework and a lens” into “we prove which knobs matter, how much, and when,” backed by reproducible logs and reliability metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kGA9cK61xc", "forum": "WHVk2qoCIY", "replyto": "WHVk2qoCIY", "signatures": ["ICLR.cc/2026/Conference/Submission25240/Reviewer_Z4bc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25240/Reviewer_Z4bc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980016003, "cdate": 1761980016003, "tmdate": 1762943377180, "mdate": 1762943377180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SafeAgents, a unified framework for evaluating the safety of multi-agent LLM systems, and proposes DHARMA, a metric that localizes vulnerabilities (planner, sub-agent, or orchestration-level). Evaluations across five MAS architectures and four benchmarks show that common design choices can create “weak links” leading to unsafe behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a timely and underexplored problem of security in multi-agent LLMs.\n2. Broad empirical coverage across multiple architectures, datasets, and models."}, "weaknesses": {"value": "1. The paper has limited technical contribution. The “weak links” analysis and DHARMA classes are mostly heuristic categorizations without strong theoretical grounding.\n2. The paper has limited novelty. The contributions are primarily in system design and taxonomy rather than algorithmic innovation."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wx93nT3nne", "forum": "WHVk2qoCIY", "replyto": "WHVk2qoCIY", "signatures": ["ICLR.cc/2026/Conference/Submission25240/Reviewer_2hNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25240/Reviewer_2hNM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076165800, "cdate": 1762076165800, "tmdate": 1762943376967, "mdate": 1762943376967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical and under-explored security vulnerabilities of LLM-based Multi-Agent Systems (MAS). The authors posit that safety guarantees developed for single-agent (SA) systems do not transfer to MAS, as the decomposition of tasks and fragmentation of context introduce new attack vectors. Existing evaluation metrics (like ASR or ARIA) are insufficient because they only report final outcomes and cannot pinpoint why or where in the agentic pipeline a failure occurred.\n\nUsing SAFEAGENTS and DHARMA, the authors conduct a comprehensive study on five MAS architectures across four safety benchmarks. Their key findings reveal that centralized systems are not inherently safer and can be more vulnerable than single agents. They identify critical weak links tied to design choices: for instance, centralized systems that delegate only atomic instructions (low sub-agent autonomy) are less secure because this \"obscures harmful objectives\" from the sub-agents, leading to unmitigated execution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a vital and forward-looking research gap. As the community shifts from single-agent systems to more complex multi-agent collaborations, understanding their unique security profile is paramount. The authors correctly argue that single-agent safety alignment does not guarantee MAS safety, providing a strong motivation for their work.\n\nThe paper delivers non-obvious and important insights. The central finding—that low sub-agent autonomy (atomic delegation) is a critical weak link—is a key, actionable takeaway for MAS designers."}, "weaknesses": {"value": "1. The DHARMA metric itself is well-designed, but its implementation relies on an LLM-as-judge to classify trajectories. This introduces a significant potential source of error and non-determinism. The paper provides the (very long) prompts used for classification  but offers no validation of the judge's accuracy. A misclassification by the judge (e.g., labeling a Sub-agent-Ignored as Unmitigated Execution) could materially skew the core results in Table 2. The reliability of the paper's central metric is therefore based on an unverified assumption of the judge's correctness.\n\n2. The analysis attributes security differences to specific \"design primitives,\" such as sub-agent autonomy (atomic vs. high-level tasks). However, the frameworks being compared (Magentic vs. LangGraph) also differ in other primitives simultaneously, such as planning strategy (stratified vs. combined) and context organization (the literal prompt structure). The effects of these primitives are entangled. It's difficult to be certain that the vulnerability comes from \"atomic delegation\" (autonomy) and not, for example, simply a less safe prompt template (context organization) in Magentic. The study does not fully disentangle these variables."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rAfGSCq06f", "forum": "WHVk2qoCIY", "replyto": "WHVk2qoCIY", "signatures": ["ICLR.cc/2026/Conference/Submission25240/Reviewer_imRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25240/Reviewer_imRK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762526291932, "cdate": 1762526291932, "tmdate": 1762943376798, "mdate": 1762943376798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}