{"id": "cS6xizdYD5", "number": 7027, "cdate": 1758005463264, "mdate": 1759897877487, "content": {"title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations", "abstract": "In Vision–Language–Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) $\\pi_0$ demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6\\% on the $\\pi_0$ backbone and 10.4\\% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4\\% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6\\% under perturbations of four modalities. Code and demo videos available at https://anonymous.4open.science/r/RobustVLA-283D.", "tldr": "We evaluate and enhance the robustness of VLAs under 17 uncertainties in 4 modalities.", "keywords": ["Robustness", "Vision-Language-Action Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24afaa89d3b5edf893bafacccbf39c79e5ed2973.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work evaluates the robustness of VLAs for robotic manipulation and identify the action modality as particularly fragile, in contrast to other modalities (vision, language, etc.) They propose RobustVLA, a framework designed to improve the robustness of VLAs to numerous axes of variation. The core of their method relies on adversarial training and a multi-armed banded (UCB) algorithm to select the most effective input perturbations. Results mostly include evaluations in the LIBERO benchmark, with a modest experimentation in the real-world. Their method achieves improvements relative to baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper's mathematical formulation of RobustVLA using the UCB algorithm appears novel and it is a creative application of the multi-armed bandit problem to VLAs. Moreover, the work considers an extensive amount of perturbations (17) and offers improvements in simulation (12% in LIBERO) and real. Moreover, the inference speed is low, which is valuable in robotic applications. \n\nAdditionally, the use of real-world experiments greatly strengthens the paper's claims because LIBERO and other simulation-based approaches often differ substantially from the real-world, limiting generalization of results. By having hardware experiments, there is great confidence in the results presented herein."}, "weaknesses": {"value": "A minor concern is with the claim that $\\pi_0$ is more robust with diffusion than an action-head; I don't think its tenable to make this claim with only a 5% increase in performance in LIBERO. The margin is too small and the sim-to-real gap is often large enough to make such claims unreliable. I suggest the author's revise this claim and discuss it with more nuance. \n\nAdditionally, while the real-world deployment of your method is commendable, the reliance on a small number of fine-tuning steps makes it difficult to assess the generalization of the results. When fine-tuning VLAs for a specific embodiment, often more than 25 demonstrations are required for good performance. While your approach may confer benefit in the low-data regime, it would be great to see how your method compares to the baselines as the number of demos increases, e.g., N=25, 50, 75, 100. \n\nFinally, another point of contention is the use of the BYOVLA baseline. Upon reading that paper, the authors only use BYOVLA for visual robustness, but your experiments consider it for non-visual modalities as well? In this case, the claim that \"existing visual-robust VLAs do not show improvements in other modalities\" needs revises. I also would suggest framing prior work as a motivation, rather than a result, since BYOVLA never proposed evaluating other modalities such as language or actions. \n\nI also would recommend clarifying that your approach fine-tunes a VLA, and is not a pre-training strategy when turning a VLM into a VLA. \n\nLastly, I would attempt to qualify some statements in Section 3, which only focus on simulation. There is a sim-to-real gap, and without extensive real-world experimentation, it is hard to confidently transfer results from one domain to the other."}, "questions": {"value": "Clarification of the BYOVLA baseline: the manuscript states that it applies BYOVLA, a visual robustness method, to non-visual perturbations. However, little detail about the implementation is described. My primary concern stems from 1) possible misuse of BYOVLA for other modalities and 2) misuse of BYOVLA as a method. This work states they use a GradCAM (gradient-based attribution method) for BYOVLA but my reading of that paper indicates that the authors therein do not use GradCAM at all in their method. It would be great to provide a detailed description of how BYOVLA was applied to each modality since it was used as a baseline throughout the entire paper. \n\nRationale for small number of fine-tuning demonstrations: related to my previous point, 25 demos is often not enough for VLAs to adapt to a task. It would be great to see how performance of RobustVLA, and the other methods, scale as N increases from 25 up to 100-200. Its very surprising that a model like $\\pi_0$ can't adapt to a FR5 setup.\n\nIs omega time-invariant in Equation 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HDIbbkOYKJ", "forum": "cS6xizdYD5", "replyto": "cS6xizdYD5", "signatures": ["ICLR.cc/2026/Conference/Submission7027/Reviewer_jsEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7027/Reviewer_jsEV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760979776137, "cdate": 1760979776137, "tmdate": 1762919230400, "mdate": 1762919230400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically studies the robustness of Vision–Language–Action (VLA) models under multimodal perturbations and proposes RobustVLA, a unified training framework to improve VLA robustness across four modalities: action, observation (vision), environment, and instruction. The authors first design a benchmark of 17 perturbations on the LIBERO suite and find that (1) the action modality is the most fragile, (2) existing vision-centric robustness methods do not generalize to other modalities, and (3) diffusion-based action heads (π₀) are inherently more robust than autoregressive ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. 17 perturbations across four modalities, validated on two major VLA backbones and in real-world deployment.\n\n2. The approach is ~50× faster in inference than some vision-robust baselines (e.g., BYOVLA), which is important for real-time robotics."}, "weaknesses": {"value": "•\tThe use of flow-matching loss as a proxy for action quality is heuristic; correlation with actual task success is assumed but not analyzed quantitatively.\n\n•\tPerformance dependence on ε, PGD steps, λ_in/out, and UCB α is not reported, raising reproducibility concerns.\n\n•\tIt remains unclear how UCB avoids over-fitting to frequently selected perturbations; no ablation against uniform or curriculum sampling is shown.\n\n•\tThe lack of domain randomization method: This method is the most common and effective method in the field of robot learning, but it is missing in this study."}, "questions": {"value": "1.\tCan you provide quantitative evidence that the flow-matching loss correlates with task success rate (e.g., correlation plots between loss change and success drop)?\n\n2.\tHow is UCB initialized and tuned (α, exploration schedule)? Would uniform or curriculum sampling achieve similar results?\n\n3.\tHow sensitive are the results to the adversarial ε and PGD steps?\n\n4.\tWhile some simple real-world experiments have been conducted, the 17 proposed perturbations have been extensively analyzed primarily in simulations. In real-world robotic manipulation tasks, which perturbations are the primary challenges that urgently need to be addressed, and which can be ignored? For example, the baseline algorithm pi0 can already handle long-horizon, two-arm clothes-folding tasks. Do these 17 perturbations actually exist in such real-world problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8OPd9vMseW", "forum": "cS6xizdYD5", "replyto": "cS6xizdYD5", "signatures": ["ICLR.cc/2026/Conference/Submission7027/Reviewer_MSSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7027/Reviewer_MSSU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306941905, "cdate": 1761306941905, "tmdate": 1762919229781, "mdate": 1762919229781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates and enhances the robustness of Vision–Language–Action (VLA) models under multi-modal perturbations spanning actions, language instructions, environments, and visual observations. The study finds that action perturbations are the most fragile modality, existing visual-robust VLAs fail to generalize beyond visual noise, and the π₀ model, equipped with a diffusion-based action head, exhibits the strongest inherent robustness.\n\nTo address these weaknesses, the authors introduce RobustVLA, a unified framework that strengthens robustness through two key components:\nOutput robustness: adversarially optimzing against worst-case action noise via a flow-matching objective;\nInput robustness: enforcing action consistency under semantically equivalent input variations, with a UCB-based adaptive mechanism to prioritize the most harmful perturbations during training.\n\nExtensive experiments on the LIBERO benchmark demonstrate that RobustVLA achieves +12.6% robustness on π₀ and +10.4% on OpenVLA, while maintaining over 50× faster inference compared to prior visual-robust models. In real-world robotic evaluations, RobustVLA further improves success rates by 65.6% under combined multi-modal disturbances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper presents a comprehensive evaluation of Vision–Language–Action (VLA) models under 17 perturbations across four modalities, offering a detailed analysis of their robustness weaknesses.\n- The proposed RobustVLA framework effectively improves resilience to both input and output perturbations through offline optimization against worst-case action noise and a multi-armed bandit–based adaptive training strategy.\n- RobustVLA achieves significant robustness gains while maintaining strong performance under clean, unperturbed conditions.\n- The framework demonstrates strong real-world generalization on the FR5 robot, delivering a 65.6% performance improvement under multi-modal disturbances with only limited demonstrations.\n- RobustVLA also offers high computational efficiency, achieving faster inference than prior visual-robust VLAs and eliminating reliance on external large models, making it well-suited for practical robotic applications."}, "weaknesses": {"value": "- The real-world evaluation is limited to a small set of tasks conducted on a single robotic platform (FR5). Broader validation across multiple robots, environments, and task domains would strengthen the claims of generalizability.\n- The RobustVLA framework integrates several advanced components. Although each element is clearly described, the overall methodological complexity may hinder ease of adoption and reproducibility for researchers less familiar with these techniques.\n- The paper touches on the robustness–accuracy trade-off but lacks a detailed quantitative or theoretical analysis of this balance. A deeper exploration of how robustness affects clean performance across varying tasks and noise levels would provide valuable insight."}, "questions": {"value": "The real-world experiments were conducted on a single robotic platform (FR5 robot). ​ Can the authors provide insights into how RobustVLA would perform on other robotic platforms or in different environments? Are there any specific limitations or challenges in adapting the framework to other systems?\n\nThe paper states that hyperparameters for UCB exploration and adversarial training were not extensively optimized. ​ Could the authors provide more details on how sensitive the framework is to these hyperparameters? Are there plans to explore optimization techniques for these parameters?\n\nThe failure analysis in the real-world experiments is insightful but could benefit from more detailed explanations of why the baselines failed under specific perturbations. This would help readers better understand the unique advantages of RobustVLA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NMSr8PhHPy", "forum": "cS6xizdYD5", "replyto": "cS6xizdYD5", "signatures": ["ICLR.cc/2026/Conference/Submission7027/Reviewer_w8cu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7027/Reviewer_w8cu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849058788, "cdate": 1761849058788, "tmdate": 1762919229461, "mdate": 1762919229461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of multi-modal robustness in Vision-Language-Action (VLA) models, which often fail under real-world noise beyond visual corruption. Through systematic evaluation of leading VLAs (OpenVLA, π₀, BYOVLA) across 17 perturbations in four modalities (action, observation, environment, and instruction), the authors find that actions are the most fragile, visual-robust methods don’t generalize to other modalities, and diffusion-based π₀ is inherently more robust than autoregressive models.\n\nTo overcome these issues, they propose RobustVLA, a framework that enhances robustness for both outputs (actions) and inputs (visual, language, and environmental signals). It adversarially trains against worst-case action noise in the flow-matching objective and enforces consistent actions across perturbed inputs using an adaptive UCB-based noise selection strategy.\n\nOn the LIBERO benchmark, RobustVLA improves robustness by 12.6% on π₀ and 10.4% on OpenVLA, while being 50× faster than BYOVLA. In real-world robot tests, it achieves 65.6% higher success under diverse perturbations. Overall, RobustVLA provides an efficient, unified solution for building robust and reliable VLA models across all modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Comprehensive multi-modal robustness evaluation is very important for the VLA research. The paper goes beyond the common focus on visual perturbations and systematically evaluates 17 types of noise across four modalities (action, observation, environment, and instruction). This provides one of the most complete analyses of VLA robustness to date and clearly identifies action perturbations as the dominant failure mode\n\n2. RobustVLA demonstrates consistent and significant gains in robustness across all tested modalities, with +12–13% improvement in simulation and +65% in real-world robot experiments, while maintaining clean performance and 50× faster inference compared to visual-robust baseline models. \n\n3. The framework generalizes to both diffusion-based and autoregressive VLAs (π₀ and OpenVLA) and does not rely on external large models or expensive perception modules. \n\n4. The paper is well-structured, clearly presents its findings."}, "weaknesses": {"value": "1. Most experiments only cover a small set of tabletop manipulation tasks with relatively simple perturbations. The results may not fully demonstrate robustness under more complex, long-horizon, or dynamic real-world scenarios. It is unclear whether different levels of perturbations may have various performance observations. \n\n2. Discussion on trade-offs and generalization: The paper claims robustness without loss of clean accuracy, but does not explore possible trade-offs in data efficiency, stability during training, or transferability to new robot morphologies or unseen tasks. These aspects are important for assessing the general utility of RobustVLA. \n\n3. Table 1 provides results on 17 noise types, yet the analysis mostly reports average success rates without detailed per-modality insight. For example, under observation noise, RobustVLA’s improvement on “Dead Pixel” is dramatic (20.8 → 93.8%), but for “Color Jitter,” the gain is moderate (61.7 → 69.5%). The paper does not explain why certain corruptions benefit more or how robustness scales with noise level (beyond the limited Figure 3a). A deeper breakdown, e.g., visualizing robustness curves per modality or ablation across perturbation strength, would better reveal the method’s internal behavior."}, "questions": {"value": "While the paper reports similar clean performance to π₀ (95.5% vs. 96.0%), it remains uncertain whether the robustness-oriented training objective might negatively affect performance on other standard or real-world benchmarks not included in LIBERO. Since real-world policies often require precise control in unperturbed environments, it would be valuable to evaluate whether RobustVLA maintains its performance on such original tasks without introducing unintended degradation. Could authors provide more explanations on it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g3uEEzCqYH", "forum": "cS6xizdYD5", "replyto": "cS6xizdYD5", "signatures": ["ICLR.cc/2026/Conference/Submission7027/Reviewer_Q9r5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7027/Reviewer_Q9r5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058171097, "cdate": 1762058171097, "tmdate": 1762919229092, "mdate": 1762919229092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper simply run a study and find analyze the robustness of multiple policy models agains 17 different perturbations and finds that the action perturbations have the most impact while Pi-0 model is the most robust one.\n\nThey propose a model, based on Pi-0, for a robust policy against all modalities perturbations. Authors show significantly better robustness for the proposed method while the success rate is also best or close to the best. The model contains 3 loss terms, one is based on pi-0, second adds robustness agains semantically preserving input perturbation (KL divergence) and the third one adds robustness against output perturbations (adversarial worst case perturbation)."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Superior results on both task accuracy and also perturbation resilience.\n\nStudy before formulation. I enjoyed the first study and findings on different methods and types of perturbations. It made the intuition behind the proposed model design clear."}, "weaknesses": {"value": "Please read my questions section."}, "questions": {"value": "1- The worst perturbation, computed by PGD, is a theoretical worst case and not necessarily represents the robot/environment real world noise/issues distribution. How is this justified in your proposed formulation?\n\n2- How come the noise accumulation in series of actions is addressed in this formulation? I assume smallest noise in each step will be propagated and accumulated with previous and next time-steps noise.\n\n3- Rectified flow makes the modeling and compute easier, but I am not sure how it is a valid assumption for this problem. Why can we assume a constant velocity for all the steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5ybDivciYA", "forum": "cS6xizdYD5", "replyto": "cS6xizdYD5", "signatures": ["ICLR.cc/2026/Conference/Submission7027/Reviewer_hH7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7027/Reviewer_hH7B"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070128469, "cdate": 1762070128469, "tmdate": 1762919228481, "mdate": 1762919228481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}