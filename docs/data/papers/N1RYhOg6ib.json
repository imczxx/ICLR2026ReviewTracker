{"id": "N1RYhOg6ib", "number": 8434, "cdate": 1758083401683, "mdate": 1759897784110, "content": {"title": "Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching", "abstract": "Guidance provides a simple and effective framework for posterior sampling by steering the generation process towards the desired distribution. When modeling discrete data, existing approaches mostly focus on guidance with the first-order Taylor approximation to improve the sampling efficiency. However, such an approximation is inappropriate in discrete state spaces since the approximation error could be large. A novel guidance framework for discrete data is proposed to address this problem: We derive the exact transition rate for the desired distribution given a learned discrete flow matching model, leading to guidance that only requires a single forward pass in each sampling step, significantly improving efficiency. This unified novel framework is general enough, encompassing existing guidance methods as special cases, and it can also be seamlessly applied to the masked diffusion model. We demonstrate the effectiveness of our proposed guidance on energy-guided simulations and preference alignment on text-to-image generation and multimodal understanding tasks.", "tldr": "", "keywords": ["Discrete Flow Matching", "Guidance", "Posterior Sampling"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03ed2c928706c399ece215f78ff48ae93af0f3c6.pdf", "supplementary_material": "/attachment/0851269e4e5a33cb7d86e1da376070e749261361.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Discrete Guidance Matching (DGM), a posterior-based guidance rule intended for discrete flow and diffusion models. The central theoretical statement asserts that, provided the density ratio \\(r(x)=q_1(x)/p_1(x)\\) is known and the source and target conditional paths match, one can recover the target posterior by reweighting the source posterior at each step. The manuscript positions this identity as a unifying lens on class-conditional guidance, energy-guided sampling, and RLHF-like objectives, and argues that the resulting sampler uses a single forward pass per step rather than the multiple passes required by rate-based or first-order schemes. Empirically, the paper demonstrates a two-dimensional toy “energy-guidance” setup and applies the approach to a multimodal discrete flow model finetuned with rewards, reporting modest improvements on GenEval and a composite multimodal score. Despite the tidy narrative, the practical validity of the assumptions and the evidentiary weight of the experiments remain limited; taken together, the contribution feels incremental, and I recommend rejection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Conceptually, the work is clean and easy to follow: the authors articulate a clear posterior-reweighting view that helps connect several strands of discrete guidance under a single umbrella. The derivations are presented with serviceable notation, and the core loss used to fit the guidance term—cast via a Bregman-style objective with an optional target-data regularizer—is straightforward to implement in existing codebases. The paper also does a decent job motivating why rate-based or gradient-first-order methods can be computationally heavy in discrete settings, and it frames the proposed posterior-based rule as a way to reduce the number of model calls at sampling time. The organization is generally strong, with definitions and assumptions stated upfront, and the training/evaluation pipeline, while narrow, is at least described with sufficient detail to enable reproduction by a motivated practitioner."}, "weaknesses": {"value": "The primary weakness is that the advertised “exactness” depends on an oracle-like requirement—knowing or accurately estimating the terminal density ratio—and on heavy modeling constraints—absolute continuity \\(q_1\\!\\ll\\!p_1\\) and equality of the conditional paths—that are rarely guaranteed in real discrete generative pipelines. There is no robustness theory that translates estimation error in \\(\\widehat r\\) into bounds on divergence between the induced terminal law and the desired target, nor is there sensitivity analysis for violations of absolute continuity or path mismatch; as a result, the central theorem’s practical relevance is unclear. The empirical section compounds this concern: the compute narrative around “one forward pass per step” is not cleanly audited, with timing comparisons confounded by different initial distributions and no end-to-end latency or throughput reported on the main RLHF tasks, and the additional cost of training a ratio model plus guidance head is not measured. Methodologically, the work criticizes first-order approximations for discrete data yet employs a first-order update in its own sampler without analyzing the induced bias, which undercuts the rhetorical stance. The evaluation design risks reward leakage by incorporating evaluation metrics into the reward itself, while the reported gains are small, lack statistical characterization (no confidence intervals, multi-seed variance, or significance tests), and are restricted to a narrow set of settings; broader discrete domains (language-only, program synthesis, tabular) and standard discrete diffusions are not explored. Finally, the “target-data regularizer” raises circularity concerns in scenarios where access to \\(x_1\\sim q_1\\) is precisely what guidance is meant to provide, and several strong baselines typical in modern RLHF or discrete-guidance literature are missing, making it difficult to assess how much the method truly advances the state of the art."}, "questions": {"value": "The core theoretical promise of “exactness” rests on conditions that are rarely met in realistic discrete pipelines and on an oracle-like requirement that the terminal density ratio be known or accurately estimated; absent a robustness analysis, the practical force of the theorem is limited. In particular, the absolute-continuity requirement \\(q_1\\!\\ll\\!p_1\\) can be violated in RLHF or reward-driven regimes where the target allocates mass to sequences/events that the base model assigns essentially zero probability, leading to degeneracy of importance weights and ill-posed guidance; the manuscript neither diagnoses these failure modes nor proposes remedies (support repair, smoothing, clipping) or theoretical guarantees under misspecification. \n\nThe conditional-path matching assumption is likewise heavy: equating \\(p_{t|1}\\) and \\(q_{t|1}\\) (often with matched corruption schedules) is nontrivial outside the authors’ favored flow parameterization, and there is no sensitivity or stability bound quantifying how a small path mismatch per step compounds into terminal bias. Even within the proposed posterior scheme, the sampler relies on a first-order update while simultaneously critiquing first-order approximations for discrete data; the resulting approximation error is unanalyzed and unablated, which undercuts the “exact” branding. On the empirical side, the efficiency narrative is not audited end-to-end: the 2-D timing study confounds initialization (masked vs. uniform) and does not report system-level latency/throughput for the main tasks under fixed hardware, batch, and backend, while the additional training cost for the ratio estimator and guidance head is omitted—function-call bookkeeping is not a substitute for wall-clock and GPU-hour accounting. The evaluation design further risks reward leakage by optimizing with a metric (e.g., GenEval) used again at test time, blurring the line between genuine generalization and metric overfitting; moreover, reported gains are small, lack multi-seed variance or confidence intervals, and do not survive scrutiny without statistical characterization. Breadth is limited: beyond a toy 2-D setup and a single multimodal flow, there is no coverage of standard discrete diffusion baselines (e.g., multinomial/discrete diffusion, argmax flows) or of diverse discrete domains such as language-only generation, program synthesis, or tabular/graph settings; critical ablations are missing (quality of the learned \\(r\\), robustness to support violations, influence of the step-size/first-order update, and sensitivity to the initial distribution). Finally, the “target-data regularizer” presumes access to \\(x_1\\sim q_1\\), which is conceptually circular when the whole point of guidance is to approximate \\(q_1\\) in the first place. \n\nThese issues are compounded by substantial missing baselines across discrete diffusion/guidance and RLHF preference optimization: beyond a couple of handpicked methods, the paper omits widely used, directly comparable approaches in both families. Representative missing or under-engaged baselines that should be reported under matched hardware, batch size, and privacy/backend settings include Dhariwal & Nichol (2021, classifier guidance for diffusion), Ho & Salimans (2022, classifier-free guidance), Austin et al. (2021, D3PM for discrete state spaces), Hoogeboom et al. (2021, autoregressive/discrete diffusion), Chang et al. (2022, MaskGIT as a discrete generative baseline), Ouyang et al. (2022, RLHF for instruction following as a canonical PPO/SFT+RM baseline), Rafailov et al. (2023, DPO as a strong non-RL preference optimizer), Yuan et al. (2023, RRHF), Xu et al. (2023, ImageReward as a modern reward model for T2I), Kirstain et al. (2023, PickScore as a learned preference scorer), Wu et al. (2024, HPS v2 as a unified human-preference score), Dathathri et al. (2020, PPLM for posterior control of discrete text, closely related in spirit to guidance), and Bai et al. (2022, Constitutional AI / RLAIF variants showing strong preference optimization without explicit ratio models). Including these methods would stress-test the claimed unification and computational advantage, reveal where DGM sits relative to best-practice discrete diffusion/guidance and modern preference optimization, and help separate genuine algorithmic gains from metric leakage or implementation choices.\n\n**missing references:**  \nDhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS.*  \nHo, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598.*  \nAustin, J., et al. (2021). Structured Denoising Diffusion Models in Discrete State-Spaces (D3PM). *NeurIPS.*  \nHoogeboom, E., et al. (2021). Autoregressive Discrete Diffusion. *arXiv:2103.12027.*  \nChang, W., et al. (2022). MaskGIT: Masked Generative Image Transformer. *CVPR.*  \nOuyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. *NeurIPS.*  \nRafailov, R., et al. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *NeurIPS.*  \nYuan, W., et al. (2023). RRHF: Rank Responses to Align Language Models with Human Feedback. *NeurIPS.*  \nXu, K., et al. (2023). ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. *NeurIPS.*  \nKirstain, Y., et al. (2023). PickScore: A Large-Scale Human Preference Dataset and Scorer for Text-to-Image. *arXiv.*  \nWu, J., et al. (2024). HPS v2: A Unified Human Preference Score for Text-to-Image Evaluation. *arXiv.*  \nDathathri, S., et al. (2020). Plug-and-Play Language Models: A Simple Approach to Controlled Text Generation. *ICLR.*  \nBai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. *arXiv.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Can you provide a robustness result that quantitatively links error in the learned density ratio \\(\\widehat r\\) (e.g., in sup norm or an f-divergence) and any violations of absolute continuity or path matching to a provable bound on the gap between the sampler’s terminal distribution and \\(q_1\\), thereby clarifying when “exactness” meaningfully approximates reality? What precisely constitutes the claimed “single forward pass” at sampling time—do you count the base posterior model and the guidance head—and can you report end-to-end latency and throughput on your RLHF tasks under a fixed hardware, batch size, and backend, alongside a corrected two-dimensional timing study that uses identical initializations across methods? Given that your reward includes evaluation-aligned metrics (e.g., GenEval), can you ablate them from the reward or add held-out metrics not used for training to demonstrate genuine generalization and avoid metric leakage, and can you expand the baseline set to include stronger contemporary RLHF/discrete-guidance approaches? Finally, since your sampler still applies a first-order update despite arguing such approximations are problematic in discrete spaces, can you analyze the resulting approximation error and show that your empirical gains persist under alternative update schemes or step-size regimes so that conclusions do not hinge on this choice?"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MN9hnT7ASs", "forum": "N1RYhOg6ib", "replyto": "N1RYhOg6ib", "signatures": ["ICLR.cc/2026/Conference/Submission8434/Reviewer_nrvw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8434/Reviewer_nrvw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635709420, "cdate": 1761635709420, "tmdate": 1762920326799, "mdate": 1762920326799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Discrete Guidance Matching, a CTMC-based framework for exact, efficient guidance in discrete generative models like diffusion and flow matching. It derives precise transition rates via density ratio correction, requiring one model pass per step. Training minimizes Bregman divergence with regularization. It generalizes prior methods and outperforms baselines in tasks like energy-guided sampling, text-to-image preference alignment, and multimodal understanding, with faster sampling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike existing methods relying on first-order Taylor approximation, the paper derives the exact transition rate for the target distribution using density ratios, eliminating non-negligible approximation errors in discrete state spaces.  \n2. The framework encompasses existing guidance methods, such as class-conditional and energy-weighted guidance, as special cases and seamlessly applies to both discrete flow models and discrete diffusion models, including masked diffusion. \n3. Each sampling step requires only 1 forward pass vs. multiple function calls in rate-based or approximated methods, leading to faster sampling than rate-based baselines."}, "weaknesses": {"value": "1. Theoretical Soundness Gaps in CTMC: The paper fails to verify that the derived target transition rate satisfies CTMC’s core conservativeness condition $ u_t^q(x,x) = -\\sum_{z \\neq x} u_t^q(z,x) $. This omission means the transition rate may not form a valid CTMC, breaking the probabilistic consistency of the generation process.  \n2. Unsubstantiated High-Dimensional Assumption: The coordinate-wise independent transition rate assumption, like for high-dimensional data, contradicts the inherent dimensional coupling in real-world discrete tasks such as long text and high-res images. The paper only validates its performance in 2D low-dimensional spaces and provides no analysis of high-dimensional performance, thereby limiting its practical utility. \n3. Missing Non-Negativity Constraints for Density Ratios: The Bregman divergence training with $ F(x)=<x, log x> $ does not guarantee non-negative density ratios $ r(x)=q_1(x)/p_1(x) $. Negative density ratios would lead to \"negative probability transitions,\" which violate CTMC physics; however, the paper lacks regularization or pre-sampling checks to prevent this.  \n4. Incomplete Experimental Scope: High-dimensional tasks such as 32×32 discrete images and low-target-sample regimes, which are critical for real-world few-shot alignment, are not tested, leaving key practical questions unanswered."}, "questions": {"value": "1. Did you verify that the derived target transition rate satisfies CTMC’s conservativeness condition $ u_t^q(x,x) = -\\sum_{z \\neq x} u_t^q(z,x) $? If yes, please include the proof in the revised manuscript; if not, how do you address the resulting probabilistic inconsistency?  \n2. The coordinate-wise independence assumption for high-dimensional data ignores dimensional coupling. Do you have a strategy to model this coupling, for example, low-rank decomposition, or pairwise terms? Can you provide experimental results on high-dimensional tasks like $ D=512 $ text sequences?  \n3. How do you ensure the estimated density ratio $ r(x) $ remains non-negative during training and sampling? Have you observed negative density ratios in experiments, and if so, how did you resolve them?  \n4. For low-target-sample regimes like 10-shot or 1-shot preference alignment, does your regularization term still work? If not, do you have a weak-supervision or prior-based alternative to improve data efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2DFjgSeNV0", "forum": "N1RYhOg6ib", "replyto": "N1RYhOg6ib", "signatures": ["ICLR.cc/2026/Conference/Submission8434/Reviewer_N6g7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8434/Reviewer_N6g7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809397965, "cdate": 1761809397965, "tmdate": 1762920326336, "mdate": 1762920326336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for general guidance in discrete flow models. While these models support unified understanding and generation for images and text, the framework works to improve the preference of the results according to specific reward signals."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1, The theory is solid. I am not a theory expert, but I validated the correctness of proof for theorem 1 and theorem 2. \n\n2, The overall advantage in both generation and understanding proves the effectiveness of the proposed framework. From both quantitative results on T2I and case studies on 2D data, I can see considerable improvements compared to existing methods."}, "weaknesses": {"value": "1, The presentation and delivery is problematic. The word Guidance should be used to describe inference-time guidance. When using an external reward to post-train the model, the method should be defined as RL (as stated in the Experiment Section). Also, the most referred reference in the paper [A] is a training-free guidance method for discrete flow matching model. If the authors want to categorize the method as guidance, at least they should explicitly notify the reason and the difference compared to existing guidance method (training is needed here). \n\nAnother critical problem in presentation is the missing of reasonable transition. For example, there is no clear problem definition on what your guidance wants to do in empirical tasks, and for theorem 1, no explanation is available on why we need to consider D-1 vectors in the discrete flow matching guidance (When I refer to your reference literature, they do not mention this D-1 term). \n\n2, Due to the above misconception on RL and guidance, I am not sure whether it is suitable not to include other RL methods based on direct preference as baselines.\n\nReference:\n\n[A] Unlocking Guidance for Discrete State-Space Diffusion and Flow Models"}, "questions": {"value": "1, Why do you consider your method with RL training as guidance?\n\n2, Could you supplement the intuitive explanation for all your theorems and tell how they help to approach your method as a conclusion?\n\nI would actively engage the rebuttal and would love to raise my score if my concerns are well solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QPwNPdnjq5", "forum": "N1RYhOg6ib", "replyto": "N1RYhOg6ib", "signatures": ["ICLR.cc/2026/Conference/Submission8434/Reviewer_gFdm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8434/Reviewer_gFdm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877066284, "cdate": 1761877066284, "tmdate": 1762920325922, "mdate": 1762920325922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Given a pre-trained discrete flow matching (DFM) model parametrized by a posterior $p_{1|t}$, and trained to generate samples from a distribution $p_1$ (somewhat confusing, it is denoted as the _source distribution_), the paper goal is to adapt the model to sample the target distribution $q_1=p_1(\\cdot | y)$ where $y$ is a conditioning variable. \n\nTo achieve this, the authors propose to add ans \"exact guidance\" which learn an additional model $h_t (x_1^d, x_t)$ such that $$q_{1|t}(x_1^d|x_t) = \\frac{h_t (x_1^d, x_t)}{\\sum_s h_t (s, x_t)} p_{1|t}(x_1^d|x_t)$$ is the posterior of DFM model trained to sample the target distribution $q_1$. \n\nFinally, the exact guidance method is evaluated mainly on the task of text-to-image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper properly introduces its goal, the notation with a the required preliminaries work, and presents it method in a clear mathematical writing.\n\n2. The proposed method attempts to advance fine tuning for conditional sampling which is a very important problem in generative models.\n\n3. To the best of my knowledge the proposed method is innovative and constitutes a fair contribution."}, "weaknesses": {"value": "While paper is interesting I still have a number of concerns:\n\n1. The \"RLHF ON MULTIMODAL BENCHMARK\" subsection is lacking:\n\n    a. The application of exact guidance to RLHF seems to be an important application of the method and should probably be introduced more clearly before the experiment section. In particular, explicitly state what object does $h_t^{\\theta}$ approximate in the this settings. Additionally, even though the actual loss for training in this setting appears in the Appendix G, would good if there was a direct reference from the paper.\n\n    b. In the main paper a single choice of probability path is introduced in equation 4. However going over the appendix it seems a different choice of probability path was used in the experiment, the metric induced path as in equation 21. Further, this probability path seems to use an additional metric which is not specified. While it is reasonable to leave these equations in the Appendix, it is expected to mention it in the experimental setup.\n\n   c. Almost no implementation detail is provided for experiment: which pre-trained model is used is not clear, architecture and model size are not specified, and number of training iteration as well as effective batch size.\n\n   d. It might be standard approach but I still find it disturbing that the GenEval reward model was used both in training and evaluation.\n\n2. The sampling section is lacking, going over the appendix it seems there is a strong connection between previous suggested sampling algorithm and the exact guided sampling algorithm. Stressing the difference in the main paper could help the reader.\n\n3. I find at least two mathematical claims that where introduced in the main paper but were not actually used by the proposed methods (negative claims): \n\n    a. Theorem 2 is introduced and in the next three lines it is already stated that the authors do not make any use of it.\n\n    b. Remark 1, this remark seems to claim again that the direct approximation of the rate would not yield the desired flow.\n\nThe paper is already heavy with alot of math to parse, and I would consider moving these to the appendix. Additionally, there is currently alot of important information in the appendix and it seems that rearranging the paper can possible make it alot easier to understand the main contribution and experiments of the paper."}, "questions": {"value": "In Algorithm 2 Sampling with Posterior-Based Guidance it is state that the guided posterior needs to be calculated at each step, Since the sequence length and vocabulary size can potentially be large, how does this affect the sampling cost compared to the vanilla sampling in Algorithm 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NulXtH3YZh", "forum": "N1RYhOg6ib", "replyto": "N1RYhOg6ib", "signatures": ["ICLR.cc/2026/Conference/Submission8434/Reviewer_375z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8434/Reviewer_375z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901457594, "cdate": 1761901457594, "tmdate": 1762920325557, "mdate": 1762920325557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}