{"id": "GoK1PfVgXU", "number": 14520, "cdate": 1758237892857, "mdate": 1759897365174, "content": {"title": "On the Limitation and Redundancy of Transformers: A Rank Perspective", "abstract": "Transformers have showcased superior performance across a variety of real-world applications, particularly leading to unparalleled successes of large foundation models. However, the overall computation and memory loads of these large models trained on web-scale datasets are considerably increasing, calling for more *efficient* learning methods. In this work, we step towards this direction by exploring the architectural limitation and *redundancy* of Transformers via investigating the ranks of attention score matrices. On one hand, extensive experiments are conducted on various model configurations (model dimensions, heads, layers, etc) and data distributions (both synthetic and real-world datasets with varied sequence lengths), uncovering two key properties: The attention rank is eventually upper bounded (limitation) and gets saturated (redundancy), as the head dimension $d_h$ increases. We call them the *low-rank barrier* and *model-reduction effect*, respectively. Most importantly, the redundancy appears that *both the attention rank and learning performance simultaneously get marginal enhancements when increasing modeling parameters*. On the other hand, we provide rigorous demonstrations for these observations under idealized settings through a fine-grained mathematical analysis, highlighting (i) a consistent theoretical upper bound ($\\approx 0.63n$, $n$: the sequence length) on the attention rank (regardless of $d_h$) given random weights; (ii) a critical position of the rank saturation ($d_h=\\Omega(\\log n)$). These results contribute to the principled understanding and assessment of Transformers' model capacity and efficiency, and are also successfully verified in practical applications such as multi-head *latent* attention (MLA) applied in DeepSeek-V3.", "tldr": "This paper reveals an inherent low-rank limitation in attention matrices of Transformers, showing that increasing head dimensions eventually yields diminishing returns, with theoretical and empirical support for both rank and performance saturation.", "keywords": ["Transformer efficiency", "attention mechanism", "model capacity analysis", "matrix rank"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1aaafac129a6e87fe66eb40a551c4a682d8a02a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the rank properties of the attention score matrix in Transformers and claims that the attention mechanism exhibits an inherent low-rank limitation, which leads to parameter redundancy when increasing the hidden dimension d_h. The authors theoretically derive a rank upper bound (≈ 0.63 n) under Gaussian initialization and validate it empirically by measuring numerical rank and performance saturation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper targets an important question about the expressive limits of the attention mechanism, which is of theoretical and practical relevance.\n2.The analysis of the rank distribution and its connection to model width is presented clearly, and the paper includes both theoretical derivations and empirical support.\n3.The writing is fluent, and the motivation (understanding redundancy in attention) is well presented."}, "weaknesses": {"value": "1. The main theorem relies on several restrictive assumptions: Inputs are approximately orthogonal vectors, Query/key projection matrices are random Gaussian without training, The softmax is treated in the hardmax (zero-temperature) limit. These assumptions do not capture the structure of trained attention layers with non-isotropic embeddings, finite temperature, or residual/MLP interactions. The theoretical results therefore describe initialization behavior, not trained dynamics\n2. Redundancy would require showing that after training, increasing dimension d_h yields little additional expressive power or performance improvement—something not established here.The current logic (“low rank at initialization → redundancy after training”) is weakly supported and risks conflating initialization degeneracy with functional redundancy."}, "questions": {"value": "1.Can you release some of your assumptions to make it more realistic? For example, can the rank upper bound be extended or bounded under finite temperature (softmax) rather than hardmax assumptions?\n2.How does the observed rank behave in trained attention layers compared to random initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GdC53jo1Td", "forum": "GoK1PfVgXU", "replyto": "GoK1PfVgXU", "signatures": ["ICLR.cc/2026/Conference/Submission14520/Reviewer_YBFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14520/Reviewer_YBFV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806046205, "cdate": 1761806046205, "tmdate": 1762924914748, "mdate": 1762924914748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the architectural limitations and redundancies of Transformer models from the perspective of matrix rank in the attention mechanism. Through extensive empirical studies and rigorous mathematical analysis, the authors demonstrate that the rank of the attention score matrix - after Softmax activation - plateaus at approximately $0.63N$ (where $N$ is sequence length), regardless of head dimension. This “low-rank barrier” persists across model sizes, data distributions, and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel theoretical insight:\nThe proposed strict upper bound offers a clear understanding of the inherent expressive limitations of the attention mechanism.\n\nEmpirical validation:\nThe claims are systematically validated across both text and vision domains, spanning multiple model depths, data distributions, and comprehensive ablation studies. The accompanying mathematical analysis complements the empirical findings, providing a cohesive and robust theoretical foundation.\n\nPractical relevance:\nThe results have direct implications for the design of efficient, high-capacity Transformer architectures, particularly under resource constraints.\n\nClarity:\nThe paper is written with clarity and supported by well-designed figures and tables that effectively illustrate the key phenomena such as saturation, accuracy plateaus, and the impact of head dimensionality."}, "weaknesses": {"value": "I'm missing a clear/detailed discussion why does the Softmax operation lowers the rank. \nFor random matrix without softmax the rank is bounded by $d$, i.e., rank($AB$) $\\le$ min (rank($A$),rank($B$)). \n\nSo hypothetically, by extending $d$ we can reach a rank of $N$. I also verified this with random $W_Q$ and $W_K$, however, the additional Softmax operation on the score matrix creates this threshold $0.63N$.\n\nThe $T\\rightarrow 0$ limit is fairly understood. \nbut for $T>0$ it is not clear to me, conceptually, why the rank is limited \n\nI took the liberty to do some short calculations with randomly initialized weights.\nInterestingly, with $d=1$ under random initialization the matrix rank is 1, but after softmax (at $T>0$) the rank goes to $N$. \nAs we increase $d$ the Softmax rank lowers until some minimum and then increases back in a similar way as your experiments up to $0.63N$.\n\nI don't see this behavior in your figures. can you elaborate on this?"}, "questions": {"value": "I have expressed my concerns and main question above in the weakness section.\nHere are other small comments\n\n154 Typo \"examine their\"\n\nFigure 3: The caption says: \"rank saturation across varied embedding dimensions at different Transformer layers\". I don't see any relation to transformer layers in the figures"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0yzEIqfuGf", "forum": "GoK1PfVgXU", "replyto": "GoK1PfVgXU", "signatures": ["ICLR.cc/2026/Conference/Submission14520/Reviewer_TzCx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14520/Reviewer_TzCx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813025571, "cdate": 1761813025571, "tmdate": 1762924914266, "mdate": 1762924914266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the evolution of the attention matrix rank as a function of the number of heads in a transformer neural network. The main contributions of the paper are that (i) the rank first increases as the number of heads grows and then attains a saturation value and stops increasing after a certain threshold. Such a saturation value is measured through several empirical ablation experiments and theoretically characterised under some restricting assumptions on the input sequence of tokens and on the query and key matrices (Theorem 1); (ii) further increasing the head dimension does not result in any rank increase and performance improvements. Several experiment on controlled toy setups and real-world datasets are provided to support the claims and theoretical results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of studying the attention rank is interesting and the discussed saturation effect represents a potential limit to the expressivity of this component in transformers."}, "weaknesses": {"value": "- The motivations of the paper are not entirely clear. In particular, a priori, it is not obvious why a rank saturation phenomenon occurring in the attention matrix should translate into worse performance. Even if this were true, two points are not entirely addressed by the paper: 1) how this phenomenon would result in detrimental effects during training; 2) most of the analysis focuses on single attention layers and it is not obvious how the results of the paper translates to modern deep multi-layer transformer architectures.\n- The assumptions at the core of Theorem 1 are quite stringent and it is not clear to what extent they would apply to more realistic scenarios. For example, the low temperature case and orthonormality assumptions on the input sequence can be easily violated in real-world transformers. That being said, the findings of the theorem are interesting, but I believe the paper would greatly benefit from an analysis showing their robustness to the relaxation of some assumptions. For example, how does the rank evolve when the temperature is bigger than 1? \n- Presentation clarity could be improved. Generally I believe the results could be introduced and explained in more details. Also, in Section 2.1 (Setup), $n$ is said to be fixed at 100 and $d_h \\leq 192$. However, Figure 2 does not seem to show any result for $d_h = 192$ and the sequence length is changed in panel (a)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3XMsYiapca", "forum": "GoK1PfVgXU", "replyto": "GoK1PfVgXU", "signatures": ["ICLR.cc/2026/Conference/Submission14520/Reviewer_bQiq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14520/Reviewer_bQiq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910549658, "cdate": 1761910549658, "tmdate": 1762924913822, "mdate": 1762924913822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Authors in this paper investigate the fundamental limitations of Transformer architectures using attention matrix ranks as a proxy for goodness of representation. The authors have established a theoretical upper bound of **0.63n** on attention ranks (where *n* is sequence length) and demonstrate a **model-reduction effect** where both attention rank and performance saturate as head dimension increases beyond $\\Omega(\\log n)$.  \n\nThe authors have performed rigorous theoretical analysis under the idealized condition such as *hardmax attention* and *orthonormal inputs*, and demonstrated the empirical validation on NLP and vision tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors have performed a rigorous mathematical analysis, using probabilistic analysis with matrix perturbation theory, and provided a concrete upper bound ($\\approx$ $0.63n$) on attention ranks.  This upper bound *$d_h$* = $\\Omega(\\log n)$ can provide actionable guidance for future LLM design.\n\n2. The experimental evaluation is comprehensive, ranging from NLP (IMDB) tasks to vision tasks (CIFAR-10/100, SVHN) across various model configurations and architectures. Moreover, the link to recent architectural varints DeepSeek-V3’s MLA further highlights the  practical utility of this work.\n\n\n3. The **model-reduction effect** directly addresses a key challenge in modern LLM development---the diminishing returns of scale---and provides a theoretical foundation for guiding more efficient architectural design."}, "weaknesses": {"value": "1. The theoretical analysis assume hardmax activation and orthonormal inputs , precisely $X X^{\\top} = I + E$ where $|E_{ij}| = o(1/n^{3/2})$. However, the real Transformer-based models  use softmax and learned embeddings that likely violate these assumptions.  While Lemma 1 bounds the hardmax--softmax gap, this bound may not be tight for practical temperature values.  Also, the orthonormality assumption is only empirically verified on initialized models, **not trained ones**.\n\n2. The paper heavily relied on random initialization but provides limited analysis of how training affects attention ranks. The rank properties at initialization may not persist after training, potentially limiting the practical applicability of the theoretical bounds. Apparently, the connection between rank saturation and performance is correlational rather than causal.\n\n3. The analysis does not address how the prevalent architectural techniques such as positional encodings, layer normalization,  \ndifferent attention patterns (sparse, local, etc.),  affect these bounds. The single-head analysis may not fully capture multi-head attention dynamics beyond simple concatenation arguments."}, "questions": {"value": "1. Can authors extend the analysis beyond hardmax to practical softmax temperatures (e.g., $T = 1/\\sqrt{d_h}$) and discuss how these affect the 0.63*n* bound?  It would also be useful to understand how training and fine-tuning influence rank behavior under realistic temperature scaling.\n\n2. The current findings suggest correlation between rank saturation and performance.  Could authors design controlled or synthetic experiments to test whether rank limitations directly cause performance saturation?\n\n3. How would factors like positional encodings (RoPE vs NoPE), layer normalization, and multi-head concatenation affect your theoretical bounds?  Does the model-reduction effect persist under these architectural variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JwRcDpR8zs", "forum": "GoK1PfVgXU", "replyto": "GoK1PfVgXU", "signatures": ["ICLR.cc/2026/Conference/Submission14520/Reviewer_MFk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14520/Reviewer_MFk7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14520/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762830488665, "cdate": 1762830488665, "tmdate": 1762924913459, "mdate": 1762924913459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}