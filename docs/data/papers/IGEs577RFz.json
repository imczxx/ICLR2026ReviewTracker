{"id": "IGEs577RFz", "number": 17543, "cdate": 1758277373402, "mdate": 1759897168333, "content": {"title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems", "abstract": "Automating the formalization of mathematical statements for theorem proving remains a major challenge for Large Language Models (LLMs). LLMs struggle to identify and utilize the prerequisite mathematical knowledge and its corresponding formal representation in languages like Lean. Current retrieval-augmented autoformalization methods query external libraries using the informal statement directly, but overlook a fundamental limitation: informal mathematical statements are often complex and offer limited \ncontext on the underlying math concepts.\nTo address this, we introduce DRIFT, a novel framework that enables LLMs to decompose informal mathematical statements into smaller, more tractable ``sub-components''. This facilitates targeted retrieval of premises from mathematical libraries such as Mathlib. Additionally, DRIFT retrieves illustrative theorems to help models use premises more effectively in formalization tasks. \nWe evaluate DRIFT across diverse benchmarks (ProofNet, ConNF, and MiniF2F-test) and find that it consistently improves premise retrieval, nearly doubling the F1 score compared to the DPR baseline on ProofNet. Notably, DRIFT demonstrates strong performance on the out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14\\% and 42.25\\% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that retrieval effectiveness in mathematical autoformalization depends heavily on model-specific knowledge boundaries, highlighting the need for adaptive retrieval strategies aligned with each model’s capabilities.", "tldr": "", "keywords": ["autoformalization", "retrieval augmented generation", "decomposition"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8744a689d37772f7ade9c2e0b352800ad629e550.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an end-to-end autoformalization pipeline that converts informal mathematical statements into Lean 4 declarations. The system couples a retriever (trained by the authors) with a generator (commercial or open LLMs) and evaluates outputs using automated Lean checks and a symbolic-equivalence metric. The main claims are (i) training a task-specific retriever improves grounding for generation, and (ii) the pipeline can robustly produce syntactically valid, type-checked Lean statements across a curated benchmark. The paper reports head-to-head comparisons against several general-purpose LLMs and ablations around retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear end-to-end systemization of a practical autoformalization pipeline for Lean 4, with sensible stages (retrieve → generate → check). The paper’s “illustrate” section is genuinely helpful: the qualitative visual walkthroughs of retrieved contexts, intermediate rewrites, and final Lean artifacts improve interpretability and reproducibility.\n- Ablations on retrieval are valuable; retrieval quality is a major bottleneck in Lean autoformalization, so any careful analysis there is welcome.\n- Timely topic & potential impact. There is fast-moving SOTA on Lean autoformalization and proof generation (Goedel/ Kimina/ process-supervised approaches); a robust, reproducible pipeline can help the community measure progress on statements specifically."}, "weaknesses": {"value": "W1. Retriever novelty & baselines are weak.\n\nThe only trained component is a DPR-style dual encoder, which is basically the same as one in RAutouformalizer. Moreover, Lean-specific retrieval methods already exist (LeanSearch’s semantic search[3]; LeanExplore’s hybrid multi-signal retrieval[4]), and the paper neither compares against nor leverages them as baselines or components. This makes it hard to justify training a new DPR when stronger plug-ins are available. \n\nW2. Missing SOTA autoformalizer baselines.\n\n The paper compares primarily to generic LLMs (GPT-4.1, DeepSeek-V3.1) but omits direct comparisons to autoformalizer-specialized systems, notably Goedel-Formalizer[1] and Kimina-Autoformalizer[2], which are expressly designed to translate informal math to Lean 4 statements and are publicly available. Given their focus and reported quality, they are the most relevant baselines for this task. Including them (or explaining why they cannot be included) is essential for positioning. \n\nW3. Frontier model coverage is incomplete/out-of-date.\n\nFor the generator, the paper focuses on GPT-4.1 and DeepSeek V3.1. The current frontier for mathematical/logic tasks prominently features DeepSeek R1 (0528), OpenAI’s o-series (o3), Gemini 2.5 Pro, and Claude 4.1; these models publicly advertise stronger reasoning/coding capabilities and should be part of the comparison, at least in a retrieval-on vs retrieval-off ablation to substantiate the retriever’s benefit. Having only a single frontier model (Claude 4) is not persuasive in 2025. \n\nW4. Stale toolchain / dataset snapshot raises representativeness concerns.\n\n Experiments are run on Lean 4.7.0 / an older mathlib snapshot. Lean and mathlib have evolved significantly (Lean 4.25.0-rc2 exists; mathlib’s scale has expanded beyond 200k theorems), and style/namespace changes accumulate. Results confined to an older snapshot may under- or over-estimate real-world robustness. Authors should either (i) re-run on a contemporary toolchain and a recent mathlib commit or (ii) justify the choice and discuss compatibility gaps. \n\nW5. Evaluation metric (BEq+) may under-state performance without human adjudication.\n\n BEq+ is a reasonable automated proxy, but even its authors note a relatively high false-negative rate; strict symbol-level equivalence can mark semantically correct paraphrases as wrong. The paper reports low success rates (sub-25% in places); without a human-adjudicated subset or complementary metrics, it is hard to interpret practical significance. A small-scale human study or relaxed-equivalence cross-check (e.g., type-equivalence under definitional unfolding) would strengthen claims. \n\n[1] Yong Lin, et al. \"Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction\" arXiv preprint arXiv:2508.03613 (2025)\n\n[2] Wang, Haiming, et al. \"Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning\" arXiv preprint arXiv:2504.11354 (2025).\n\n[3] Gao, Guoxiong, et al. \"A semantic search engine for Mathlib4.\" arXiv preprint arXiv:2403.13310 (2024).\n\n[4] Asher, Justin. \"LeanExplore: A search engine for Lean 4 declarations.\" arXiv preprint arXiv:2506.11085 (2025)."}, "questions": {"value": "Toolchain & dataset. What constraints led you to Lean v4.7.0/that mathlib commit? Please discuss how brittle your pipeline is to syntax/tactic drift across versions. Additionally, could you explain in detail how you conduct data extraction from mathlib and prepare them for embedding training in detail?\n\nGround truth. How do you construct ground truth (oracles) for decomposition and retrieval tasks? Detail the pipeline used to obtain ground truth (oracles) from Lean.\n\nInterpreting BEq+. Given BEq+’s known false negatives, do you have a human-adjudicated subset to calibrate precision/recall? How often do your “failures” reflect symbol-level mismatches vs true semantic errors? Consider reporting: (a) case study on type-checks but BEq+-fails; (b) human-judged correctness. \n\nAblations on retrieval → generation sensitivity. Please report end-to-end success vs top-k retrieval quality (e.g., R@k buckets) to quantify how much the generator depends on retrieval depth and filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P2NLasXEZ7", "forum": "IGEs577RFz", "replyto": "IGEs577RFz", "signatures": ["ICLR.cc/2026/Conference/Submission17543/Reviewer_AXr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17543/Reviewer_AXr5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713330688, "cdate": 1761713330688, "tmdate": 1762927416201, "mdate": 1762927416201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the DRIFT framework , which aims to enhance the process of mathematical statement autoformalization. DRIFT decomposes complex informal mathematical statements into sub-queries, then accurately retrieves the corresponding formal dependencies. Based on these retrieved dependencies, it provides illustrative contextual examples to guide the model in applying them correctly. Through this structured process, DRIFT achieves improved performance in automatic formalization of mathematical statements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.DRIFT introduces an innovative approach by decomposing complex informal statements into sub-queries, which allows for precise retrieval of the required formal dependencies. In addition to retrieving relevant premises, the framework also provides illustrative examples of their usage, effectively guiding the model to apply the dependencies correctly during formalization.\n\n2.The experiments demonstrate both the generality and effectiveness of the proposed framework. Moreover, the ablation studies clearly reveal the individual contributions and roles of different modules within DRIFT, strengthening the empirical support for the proposed design."}, "weaknesses": {"value": "1.The ablation study shows that the retrieval module plays a crucial role in the overall performance of DRIFT. However, the paper does not compare this module with existing Lean premise retrieval methods, such as Lean Search or other established retrieval baselines. Including such comparisons would provide a clearer understanding of the advantages and limitations of the proposed retrieval component.\n\n2.The experiments primarily focus on general-purpose reasoning models such as GPT and DeepSeek. However, there are now several large models that have been specifically trained or fine-tuned on Lean. It remains unclear whether DRIFT has been tested on these Lean-specialized models, and such evaluation could further demonstrate the framework’s adaptability and robustness."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RWhvSsNir3", "forum": "IGEs577RFz", "replyto": "IGEs577RFz", "signatures": ["ICLR.cc/2026/Conference/Submission17543/Reviewer_3Kvp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17543/Reviewer_3Kvp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744851860, "cdate": 1761744851860, "tmdate": 1762927415797, "mdate": 1762927415797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DRIFT: a four-stage framework for autoformalizing informal math statements into Lean. DRIFT (i) Decomposes an informal statement into atomic, concept-focused sub-queries (with predicted formal “anchors”), (ii) Retrieves dependent premises from a formal library using a finetuned dense retriever, (iii) Illustrates usage via a small set of demonstrative theorems chosen by a greedy coverage algorithm, and (iv) Formalizes the statement conditioned on the retrieved context. Across ProofNet (in-distribution), MiniF2F-test (largely self-contained), and ConNF (out-of-distribution), DRIFT improves dependency retrieval F1 and downstream formalization, with especially large gains on ConNF where it even surpasses an oracle* retrieval setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework adds an Illustrate step that selects a minimal set of theorems to demonstrate how retrieved premises are used, addressing the gap between definition and usage, an underexplored angle in prior work.\n\n2. Clear end-to-end design validated on three complementary benchmarks; the method substantially boosts BEq+ and type-check rates over strong retrieval baselines and zero-shot, with striking OOD gains on ConNF.\n\n3. Ablations show the Illustrate step is crucial (removing it sharply reduces BEq+ on ProofNet/ConNF), and quantify contributions of Decompose vs. Retrieval.\n\n4. The pipeline is easy to follow, and can inform future RAG design for formal methods."}, "weaknesses": {"value": "1. The decomposer appends predicted formal representations; the paper argues this helps anchoring, but does not quantify robustness when anchors are wrong/noisy.\n\n2. The ablation discussion suggests decomposed retrieval may introduce diverse noise that requires illustrative scaffolding; more error taxonomy and qualitative failure analyses (on both ProofNet and MiniF2F) would bolster the explanation and guide adaptive retrieval strategies.\n\n3. There is a chance that I missed this somewhere, but the paper does not seem to report compute/latency costs for decomposition + retrieval + illustration vs. baselines? Given practical adoption, cost-quality tradeoffs matter.\n\n4. The paper observes retrieval can distract in low-dependency regimes (MiniF2F). It would help to show an adaptive gate (e.g., predicting when to skip retrieval or to down-weight illustration) and quantify decision quality."}, "questions": {"value": "1. How does performance vary with the number of sub-queries, top-k per sub-query, and the illustration budget m? Any evidence of diminishing returns or overfitting with larger m?\n\n2. You surpass Oracle* on ConNF; can you provide diagnostics (e.g., premise coverage of selected theorems, overlap with ground truth, qualitative examples) explaining where illustrative theorems help beyond oracle dependencies?\n\n3. Some slight inconsistencies in terminologies: e.g., \"DeepSeek-3.1\" and \"DeepSeek-V3.1\" both appeared."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uyQXyA3pio", "forum": "IGEs577RFz", "replyto": "IGEs577RFz", "signatures": ["ICLR.cc/2026/Conference/Submission17543/Reviewer_Wmuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17543/Reviewer_Wmuf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996999595, "cdate": 1761996999595, "tmdate": 1762927414713, "mdate": 1762927414713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DRIFT, a new technique for autoformalization. Their pipeline first decomposes a statement into sub-problems, retrieves relevant premises and their usage in sample theorems, and finally uses all the included retrievals to perform formalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The pipeline for autoformalization is novel and has not been explored before in the literature. The results are also strong, and the authors also ablate removing several modules (Sec 5.3), providing interesting insights into the value add of each module.\n- The techniques used for the modules are interesting, and the method outperforms zero-shot and retrieval-augmented baselines."}, "weaknesses": {"value": "- As discussed in the paper, I suspect that the benchmarks could be contaminated, especially miniF2F and ProofNet. There are many instances of these two popular benchmarks on GitHub, and it would be surprising if models had not seen them before, even if the results are low. It is possible that retrieval could remind or steer the model to a certain distribution that can elicit its recall ability of seeing these benchmarks.\n- The pipeline consists of a set of modules, but none of them seem to be particularly optimized for performance. For example, for the decompose module, only one decomposition prompt seems to have been tested. \n- The previous issue could lead to misleading interpretations of the ablation study: the authors noted that in one of the experiments, removing the \"decompose\" module does not degrade performance, but I wonder if this would be different if the retrieval and formalizer models were replaced with stronger models.\n- The paper does not compare with other autoformalization techniques in the literature, making it hard to assess its significance and effectiveness"}, "questions": {"value": "- Can the authors demonstrate gains on proof autoformalization using this method as well?\n- Why was m=3 selected for the illustration stage? Is it possible that scaling this up to much more examples will improve more (e.g. in https://arxiv.org/abs/2404.11018)\n- How does DRIFT compare to other autoformalization techniques in performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DY1BMB0GZX", "forum": "IGEs577RFz", "replyto": "IGEs577RFz", "signatures": ["ICLR.cc/2026/Conference/Submission17543/Reviewer_nbLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17543/Reviewer_nbLa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17543/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762474322615, "cdate": 1762474322615, "tmdate": 1762927414025, "mdate": 1762927414025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}