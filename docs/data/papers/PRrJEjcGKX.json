{"id": "PRrJEjcGKX", "number": 9678, "cdate": 1758134250116, "mdate": 1759897704848, "content": {"title": "ALIGNEDEDIT: PROMPT-ALIGNED WEAK GUIDANCE FOR TEXT-GUIDED IMAGE EDITING", "abstract": "Text-guided image editing has advanced rapidly, yet most approaches still rely on classifier-free guidance. We introduce ALIGNEDEDIT for semantic image editing, employing semantically weak guidance to produce natural edits that align with the instruction prompt. CFG is a de facto standard guidance technique that use an uncondition model to steer sampling toward the positive condition and amplify its signal. However, this mechanism that use condition and uncondition model that misalend in semantic space induce over-editing, artifacts, and unintended changes. ALIGNEDEDIT employs aligned yet semantically weak guidance, preventing error accumulation and producing faithful edits without unintended modifications, resulting in a more natural appearance. To obtain an aligned yet semantically weak model, ALIGNEDEDIT identifies semantically strong tokens in each attention block and attenuates their embeddings to reduce semantic strength. Because the semantically weak model is derived directly from the model itself, no explicit negative prompt is required, making the method substantially less sensitive to prompt choice. We apply our guidance to two diffusion-based editing models, CosXL and Kontext. Across diverse benchmarks of Emu-Edit for real-image editing, HQ-Edit for synthetic editing, and ImgEdit-Bench for multi-turn editing, our method yields edits that are more natural and more faithfully aligned with the prompt.", "tldr": "Substitute for CFG guidance fo text based image editing, using the semantic weak model.", "keywords": ["diffusion models", "image editing", "semanticediting", "CFG guidance", "sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a5a1cb7f361009473a2daae26e1ec441c54f7815.pdf", "supplementary_material": "/attachment/70dc5faa24a97775fa781517d20058e0a93f4db6.pdf"}, "replies": [{"content": {"summary": {"value": "AlignedEdit addresses over-editing/artifacts in text-guided image editing (TIE) caused by Classifier-Free Guidance (CFG)’s semantic misalignment. It proposes Semantic Weak Guidance (SWG): identifying semantically strong tokens via cross-attention saliency in each transformer layer/timestep, then attenuating them to build a model-internal weak model (no external negative prompts). It provides a training-free, prompt-insensitive alternative to CFG, enabling natural, faithful TIE for applications like creative editing, advancing practical diffusion-based image manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It innovatively solves CFG’s semantic misalignment by constructing a model-internal semantic weak model, via cross-attention saliency to identify/attenuate strong tokens, instead of relying on external negative prompts. This avoids prior weak-model flaws (e.g., PostEdit’s imprecision) and is training-free, a creative departure from CFG substitutes like AutoGuidance (needs auxiliary training).\n\n2. It fills the gap of natural, faithful TIE, enabling applications like creative editing (e.g., precise color/object edits). As a prompt-insensitive, general CFG replacement for CosXL/Kontext, it advances diffusion-based editing’s practicality, setting a new standard for balancing fidelity and structure preservation."}, "weaknesses": {"value": "1. Maybe my question is quite desperate and disappointed. The most concern for me is that text-guided image editing has been varied from inversion-based techniques to large-scale training techniques (similar to InstructPix2Pix, but with more data and large networks, e.g. Qwen-Image-Edit, Hunyuan-Image-3.0, InstructX, etc.) It would be hard for inversion-based techniques to compete with them, so as to this method ALIGNEDIT. How do you compare with the SOTA editing methods and can your method be applied to other domains/downstream tasks?\n\n2. Experiments focus on single-task edits (e.g., color change, adding small objects) but omit multi-target (e.g., \"smile + background swap\") or extreme occlusion cases. Add such prompts to HQ-Edit, evaluating if SWG avoids semantic confusion and preserves structure, ensuring robustness in real-world complex edits.\n\n3. From Table 1, it shows that StableFlow is still the best among many evaluation metrics with CFG. The SWG is not that outperforming as claimed in the paper. Can your SWG applied to StableFlow to outperform CFG?"}, "questions": {"value": "It seems the format is not totally correct. This paper is with a bit different font style from what I observed from the official template."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pobtz2Nzbr", "forum": "PRrJEjcGKX", "replyto": "PRrJEjcGKX", "signatures": ["ICLR.cc/2026/Conference/Submission9678/Reviewer_S3ot"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9678/Reviewer_S3ot"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860987129, "cdate": 1760860987129, "tmdate": 1762921196218, "mdate": 1762921196218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper proposes a new text-guided image editing method using semantically weak guidance (SWG) to produce natural and prompt-aligned edits while avoiding over-editing and artifacts caused by traditional CFG. ​\n\nthe key idea is that: for the unconditional part (which will be used in cfg), it still feeds the original text, instead of null text, and dynamically identifies and attenuates high-saliency tokens during editing, such that it can create a weak, (in my understanding, still better than null feature) anchor point for cfg\n\nALIGNEDEDIT outperforms existing methods like CFG, PostEdit, and IceEdit across benchmarks in several metrics"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "After reviewing several papers that read more like technical reports, I finally found one that shows some academic depth — the authors are willing to dig deeper and explore something fundamental and interesting.\n\nFrom the results, their method appears to be effective to some extent. I also appreciate their ablation study, which provides valuable insights into how their approach works."}, "weaknesses": {"value": "I have two main concerns:\n\nit seems that this appraoch should be generalizable to T2I as well? I am wondering what's the motivation try editing tasks? because in T2I several works have similar ideas (which in general people find that your can use a weal/bad model for your unconditional), for example \"Guiding a Diffusion Model with a Bad Version of Itself\", can you compare with these works?\n\ntheir main table should be present in a better way: because you care about relative comparision, thus i think you should put cosXL and cosXL w your guidance together. Same for Kontext and Kontext w your guidance together. But the main question i have is for HQ-Edit, why your approach have such a bad FID? 107 and even 269? To me if FID is above 100 or even 200 it basically too bad or even meaningless for images. I suspect they hack other metrics"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zciouy70AE", "forum": "PRrJEjcGKX", "replyto": "PRrJEjcGKX", "signatures": ["ICLR.cc/2026/Conference/Submission9678/Reviewer_SZ2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9678/Reviewer_SZ2d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854525844, "cdate": 1761854525844, "tmdate": 1762921195526, "mdate": 1762921195526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AlignedEdit, a sampling strategy that replaces the unconditional branch in classifier‑free guidance (CFG) with a model‑internal, prompt‑aligned “semantically weak” pathway. Concretely, at each layer and timestep the method identifies the most salient text tokens via cross‑attention, attenuates only those embeddings, and re‑computes the block; the resulting weakened model is then contrasted with the standard (strong) branch to form guidance (Eq. 3). This avoids negative prompts, aims to curb error accumulation and over‑editing, and promises more faithful yet natural edits."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Replacing the unconditional branch in image editing is an interesting way for improving image editing.\n\n2. Results on SDXL‑ and FLUX‑based editors, with single‑turn and multi‑turn datasets; multi‑turn gains on ImgEdit‑Bench are especially encouraging."}, "weaknesses": {"value": "1. The paper does not clearly articulate why replacing the unconditional branch in CFG with semantic weak guidance is theoretically necessary, nor does it rigorously validate how this substitution improves the underlying mechanism of guidance. While Section 1–2 briefly mention CFG’s issues such as semantic misalignment and over-editing, these points remain qualitative and generic—claims like “misaligned in semantic space induces over-editing” are stated but never mathematically or empirically analyzed in the context of text-guided editing. There is no formal examination of how the semantic gradients or score-estimation behavior differ between CFG and SWG, nor any quantitative study showing that CFG’s unconditional branch causes semantic drift. As a result, the motivation for introducing the semantic weak model and its mechanistic superiority over CFG are conceptually plausible but insufficiently substantiated.\n\n2. The experimental section primarily presents visual comparisons and benchmark metrics but lacks deeper analysis that connects the results to the paper’s central claims. The authors assert that semantic weak guidance mitigates over-editing, reduces error accumulation, and preserves structural fidelity, yet these statements are not quantitatively or qualitatively validated. For example, there is no ablation isolating the claimed effects (e.g., degree of over-editing or misalignment under CFG vs. SWG), no analysis of error propagation across diffusion steps, and no perceptual or statistical evaluation that directly demonstrates “semantic alignment.” As a result, while the side-by-side examples illustrate plausible improvements, they remain anecdotal and do not constitute evidence supporting the claimed mechanisms or motivations.\n\n3. The weak branch requires computing attention saliency and re‑computing blocks with attenuated tokens (the text states “recomputed” within each block), which implies added latency/VRAM; no runtime or memory analysis is provided.\n\n4. Numerous typos and terminology slips—e.g., “CFT” vs. CFG, “misalend” in abstract,  “applued,” and use of “logit” to refer to ε‑predictions—impair clarity."}, "questions": {"value": "See weaknesses 1 & 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "34vHHCrbAV", "forum": "PRrJEjcGKX", "replyto": "PRrJEjcGKX", "signatures": ["ICLR.cc/2026/Conference/Submission9678/Reviewer_vXms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9678/Reviewer_vXms"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967869940, "cdate": 1761967869940, "tmdate": 1762921195098, "mdate": 1762921195098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AlignedEdit, an inference-time guidance strategy for text-guided image editing that replaces the standard Classifier-Free Guidance. Instead of contrasting a conditional model with an unconditional model as in CFG, the authors construct a semantically weak but prompt-aligned version of the conditional model by selectively attenuating high-saliency text tokens identified via cross-attention maps at each denoising step and transformer block. This internal weak model avoids the semantic misalignment inherent in CFG’s unconditional branch, thereby reducing over-editing, artifacts, and unintended structural changes. The results shows that the proposed method achieve better qualitative and quantitative results as compared to the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates the limitations of CFG in text-guided image editing i.e., semantic misalignment between conditional and unconditional branches leading to over-editing and artifacts. This is a known but under-addressed issue in the community.\n2. The proposed method requires no retraining, auxiliary models, or negative prompts.\n3. The experiments span multiple models, datasets (real, synthetic, multi-turn), and metrics (CLIP-based, SSIM, FID, ImageReward, user studies).\n4. The observation that semantic influence is distributed and time/layer-dependent (Fig. 2) is valuable and informs the adaptive token attenuation strategy.\n5.The trade-offs between attenuation strength and number of tokens are well explored (Fig. 8 and 9), justifying the selective suppression approach."}, "weaknesses": {"value": "1. To the best of my knowledge, the concept of using a “weakened” version of the conditional model for guidance is reminiscent of prior works: Spatiotemporal Skip Guidance [1] also constructs an internal weak pathway. PAG [2] and SAG [3] use internal perturbations for guidance. Prompt-to-Prompt [4] manipulates attention to control semantics. While AlignedEdit differs in how the weak model is constructed (saliency-based token attenuation), the high-level paradigm i.e., replacing the unconditional model with a controlled variant of the conditional one is not entirely new.\n\n2. Computing attention maps and identifying top-K salient tokens at every timestep and block adds non-negligible latency. The paper does not report inference speed or FLOPs, which is critical for real-world deployment.\n\n3. I observed some ambiguities in the implementation details: (1) How is K (number of tokens to attenuate) chosen? Is it fixed or adaptive? (2) What is the attenuation scalar (e.g., 0.2 in Fig. 9)? Is it tuned per prompt or fixed? (3) How are saliency scores aggregated across attention heads? The formula (Eq. 5) is provided, but robustness to head variability is not discussed.\n\n4. Notably missing is comparison to CFG++ (Chung et al., 2024), which also addresses CFG’s trajectory curvature and error accumulation. Also, Adaptive Scaling (Malarz et al., 2025) is cited but not compared.\n\n[1] Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling\n[2] Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance\n[3] Improving Sample Quality of Diffusion Models Using Self-Attention Guidance\n[4] Prompt-to-Prompt Image Editing with Cross Attention Control"}, "questions": {"value": "1. How does AlignedEdit compare to CFG++ or Adaptive Scaling in terms of both fidelity and efficiency? Were these omitted due to implementation constraints?\n\n2. What is the computational cost of your method relative to standard CFG? Can the saliency computation be approximated or cached to reduce overhead?\n\n3. Is the attenuation strategy robust to prompt phrasing variations? Since you claim reduced sensitivity to prompt choice, have you tested paraphrased prompts or adversarial rewordings?\n\n4. In Eq. 3, you use zweak_c in both image-guided and null-image terms. Is this necessary? Have you tried using the original zc in the image-conditioned term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W70oBF8olH", "forum": "PRrJEjcGKX", "replyto": "PRrJEjcGKX", "signatures": ["ICLR.cc/2026/Conference/Submission9678/Reviewer_BXrQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9678/Reviewer_BXrQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9678/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999565657, "cdate": 1761999565657, "tmdate": 1762921194154, "mdate": 1762921194154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}