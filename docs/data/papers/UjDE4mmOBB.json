{"id": "UjDE4mmOBB", "number": 10830, "cdate": 1758182768553, "mdate": 1759897625961, "content": {"title": "Empirically Testing Expressivity Bounds for Neural Network Architectures", "abstract": "Understanding neural networks' algorithmic capabilities is key to predicting their performance on real-world tasks, and formal language recognition offers a rigorous framework for evaluating what algorithms these models can employ. To this end, a growing body of theoretical work has sought to characterize the classes of formal language that various neural network architectures---particularly transformers---can recognize.\nThese mathematical results generally rely on simplified versions of the transformer network that differ from those used in practice,\nso empirically testing the hypothesized expressivity bounds remains crucial. Some work has already evaluated neural networks as recognizers of hand-picked languages, but in order to test broad claims about language recognition bounds, one should test on an appropriately broad distribution of languages and see how well the hypotheses and experiments align. This paper is an attempt to do just that, using efficient algorithms for randomly sampling formal languages from certain language classes and generating labeled datasets to be used for training and evaluating neural networks on those languages. We do so for the classes of regular languages, context-free languages, partially-ordered star-free languages, and star-free languages of varying dot-depth. Along the way, we develop a novel algorithm for efficiently sampling a string of a desired length from a probabilistic context-free grammar. We experimentally test the language recognition performance of transformers, simple RNNs, and LSTMs on these classes and find that transformers can recognize only a subset of star-free languages, underperforming theoretical predictions.\nSurprisingly, all architectures surpass theoretical expectations on context-free languages. Our code is publicly available.", "tldr": "We test transformers, RNNs, and LSTMs on a large number of languages at different levels of the Chomsky hierarchy to verify theoretical expressivity bounds.", "keywords": ["transformers", "rnns", "lstms", "chomsky hierarchy", "star-free languages", "language recognition"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa92a7104a3af8a46616005e95fab9d39a905113.pdf", "supplementary_material": "/attachment/60c36ae32e475ad1cbc3fc505ccf86ca1811bae8.zip"}, "replies": [{"content": {"summary": {"value": "This paper samples random languages from a set of language families; specifically context-free languages, regular languages, star-free languages with differing dot counts, and regular languages corresponding to partially ordered dfas. A model from one of three classes (LSTM, RNN, Transformer) is trained on each of these languages and average results are reported for both average performance and the percentage of fully solved languages. Additionally, an algorithm is presented to sample strings of a particular length from a context-free grammar. The overall results are that on these distributions, all model classes fail at fully solving most languages, except in the PODFA family, but are nonetheless able to make fairly accurate predictions on all language families, in particular the context-free languages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The technical presentation is generally pretty clear and elementary (See minor weaknesses and questions for some caveats to this), and in general the paper’s theoretical component is very rigorous and sound."}, "weaknesses": {"value": "A significant weakness with this work is the lack of a detailed discussion in the main text of the fact that the sampling procedures are uncalibrated, that is, there is no attempt made to ensure that the distributional choices do not affect the apparent difficulty of each language family. (To take an extreme example, if the regular languages sampled all corresponded to 30 state DFAs, while the CFG sampler always returned the nested parentheses language, then the CFGs would be “simpler” without this saying much about the inherent difficulty of CFGs in general vs regular languages in general). I acknowledge that to some degree, such calibration is impossible, as there are qualitative differences between these spaces so there is no way to ensure that the sampling procedures are identical; however, I do believe that some attempt could be made to at least calibrate via some non-neural method. E.g., the use of some non-neural method such as an n-gram could be used to provide a general estimate of how difficult the distribution sampled from each language family is. I ask this primarily because I find it difficult to interpret the result showing that CFGs are surprisingly easy to learn heuristics for, due to the uneliminated possibility that the sampling procedure you have devised biases towards CFGs that can be approximated with easy-to-learn shallow heuristics. I believe this problem is fundamental to any comparison between language classes (though it does not affect comparisons between model classes).\n\n\nMinor:\n\nAn unfamiliar reader might not realize that star-free languages contain languages that must be represented with stars in regular expressions; this should be clarified in a footnote. I personally had to refresh my memory by looking it up, despite working with automata regularly."}, "questions": {"value": "Definition 13 is the polynomial ring, right? If I am correct, I think you should mention this for clarity’s sake, even if only in a footnote. It might also be clearer to just introduce this as polynomials rather than abstract vectors. I could be wrong and have missed something, in which case, ignore this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0JNKGDSJ3", "forum": "UjDE4mmOBB", "replyto": "UjDE4mmOBB", "signatures": ["ICLR.cc/2026/Conference/Submission10830/Reviewer_rgAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10830/Reviewer_rgAY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846572830, "cdate": 1761846572830, "tmdate": 1762922034436, "mdate": 1762922034436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides sampling algorithms for sampling from the set of all languages in various subcategories of regular languages (e.g., star-free languages of varying dot-dept) and for sampling from those resulting languages with varying string length constraints, and tests LSTMs, RNNs, and Transformers on their ability to learn to recognize those languages."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I found the presentation of this work accessible, and it gave a nice clear description of background in theoretical expressivity results in neural networks for language recognition. This is a technical subfield, and so, nicely presented exposition is a big plus!\n\nI want to acknowledge that there’s a ton of work that goes into the results provided – sampling from the set of languages, sampling from the length-constrained set of strings for each language – and characterizing the computability or hardness of each. I found the appendix really nice, characterizing a lot of extra formal details; efficiently sampling from a constrained set of lengths under a PCFG isn’t obvious! It seems like a ton of solid work went into getting the formal properties right and I value that highly."}, "weaknesses": {"value": "I think “empirically testing expressivity bounds” is just not what this paper does. It does characterize learnability for a set of hyperparameters and a specific learning algorithm and a specific loss etc. I think the expressivity bounds are what they are – if the proofs are right (and the premises hold for the architectures being used; I realize this is not always the case, e.g., with nonstandard position encodings) – then maybe this is just semantics but we’re really exploring the relationship between expressivity and learnability for these architectures. I’m not arguing that the authors disagree with the proofs in the literature – read on for my point:\n\nThis leads to the core weakness here, which is that I’m just not convinced in the empirical results; I see no details as to the size/depth/other hyperparmaeters of each of the networks, nor the learning algorithm, nor the number of samples used for training, nor the number of training steps. Certainly we can never test expressivity by evaluating learnability, but if we want to try to measure how they diverge in practice, we need to work very hard at hyperparameter optimization so that we don’t underestimate the learnability of a language under a given model architecture. In this, relative to the details we find for the definitions and sampling algorithms for the formal languages provided, this paper provides very few details about the effort that went into the learning process. The paper notes that 5 hyperparameter settings were used – which? I looked over the appendix as well and could find no details. And regardless, this seems really low. I really struggle with the claim of “testing expressivity bounds” here – it seems like a ton of work should go into getting the layer count, optimizer, hidden dimensionality right, and we should see learning curves as a function of the number of samples and training steps. This is especially true given the odd optimization behavior of deep networks (e.g., “grokking.”)\n\nThis is tough for me because the formal presentation of this work is really strong, and I feel like I learn from that and the discussions around how to sample from families of languages. But the core claims of the work are inherently empirical—can these architectures recognize members of these language families–, and I don’t think these claims are properly supported.\n\nNext, and this is minor – I’m generally in agreement that there’s utility of understanding the connections between classes of formal languages and network expressivity. However, while the authors are right that we shouldn’t “just” focus on specific exemplars of languages of a class like the Dyck-k languages, these exemplars at least are well-motivated through their connections we care about, e.g., natural languages and programming languages. I’d like the authors to do more to motivate why their particular sampling strategies over, e.g., the family of dot-depth 3 languages, lead to languages that we should care about."}, "questions": {"value": "You note that your algorithms for sampling from the set of languages of a given class have every language in the class as their support. Can you characterize though how the likelihood decays, e.g., as a function of complexity? I’m just not sure which star-free languages your sampling methods end up putting, e.g., exponentially small probability on (as a function of number of DFA states for example.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "El58fFHfO8", "forum": "UjDE4mmOBB", "replyto": "UjDE4mmOBB", "signatures": ["ICLR.cc/2026/Conference/Submission10830/Reviewer_F4v3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10830/Reviewer_F4v3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951608999, "cdate": 1761951608999, "tmdate": 1762922033695, "mdate": 1762922033695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical study evaluating the ability of Transformers, RNNs, and LSTMs to recognize formal languages. \nThis work introduces novel methods for randomly sampling languages from different classes in the Chomsky hierarchy (Regular, Star-Free, and Context-Free) and for generating labeled datasets with controlled string lengths. The experiment results reveal that, all architectures perform well on simpler Partially-Ordered DFA (PODFA) languages, and all models achieve high average accuracy on Context-Free languages, though they master very few of them perfectly. The work has its primary contribution in helping mitigate the gap between theoretical representational capacity and empirical learnability, which can use formal language recognition as a mean for probing the representational capabilities of neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's primary strength lies in testing on a broad distribution of randomly sampled languages, it provides a more comprehensive and reliable assessment of what these models can learn in practice. This addresses a significant limitation in prior work.\n\n2. Another contribution is the development of novel algorithms for sampling random languages, particularly for Star-Free languages of bounded dot-depth and Context-Free languages. These methods are valuable and will facilitate more robust empirical studies in this research area.\n\n3. The paper provides comprehensive preliminaries that effectively ground the study for a broad audience. The clear exposition of formal language theory, automata, and the specific language classes under investigation sets a solid foundation for understanding the experimental goals and results."}, "weaknesses": {"value": "1. The discussion of the experimental results remains somewhat surface-level. For example, for Context-Free languages, where the paper notes the high average accuracy but low perfect learning rate, suggesting models only learn approximations. However, it does not delve deeper into why this might be the case or how the nature of the randomly sampled CFGs influences learnability. A more in-depth analysis connecting model failures to specific structural properties of the languages would significantly strengthen the paper's conclusions.\n\n2. As an empirical study, the paper would benefit from providing more concrete details about the experimental setup. Key information, such as the size of the training/validation/test datasets for each language and the parameter counts of the different model architectures used, is missing from the main text. Including these details is crucial for assessing the scale of the experiments and the generalizability of the results.\n\n3. The work is built upon, and frequently cites, a series of prior studies by Butoi et al. While grounding the work in established literature is good practice, the specific novel contributions of this paper could be more clearly introduced."}, "questions": {"value": "My questions have been stated in detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vDDh2Q5Mmm", "forum": "UjDE4mmOBB", "replyto": "UjDE4mmOBB", "signatures": ["ICLR.cc/2026/Conference/Submission10830/Reviewer_rq1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10830/Reviewer_rq1a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962917217, "cdate": 1761962917217, "tmdate": 1762922033352, "mdate": 1762922033352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically studies the expressiveness of several neural networks—Transformers, RNNs, and LSTMs. The paper tested their ability to learn randomly generated formal languages across the Chomsky hierarchy. The paper developed a probabilistic context-free grammar sampling algorithm to generate unbiased datasets, avoiding common hand-crafted benchmarks. The experiments reveal discrepancies between theoretical expressivity results and empirical behavior, particularly showing that Transformers underperform on some theoretically learnable languages, while RNNs and LSTMs sometimes exceed predicted expressivity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The random language generation framework is novel and could become a useful tool for empirical expressivity research.\n2. The evaluation spans multiple architectures and formal language classes with consistent methods."}, "weaknesses": {"value": "1. The work identifies discrepancies but does not deeply investigate the reason behind it\n2. The theoretical insight is borrowed from prior expressivity literature. There is no new formal result on understanding the expressiveness are introduced.\n3. The sampled datasets and model sizes are relatively small, which limits generalizability."}, "questions": {"value": "For transformer (TF) architecture, existing research works have pointed out that indeed TF or other neural nets cannot learn formal languages directly. However, under the concept or technique of chain-of-thought, TF models can solve difficult tasks to some extend. Can authors discuss how this work is related to the above observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "iCR3L6OklV", "forum": "UjDE4mmOBB", "replyto": "UjDE4mmOBB", "signatures": ["ICLR.cc/2026/Conference/Submission10830/Reviewer_uGxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10830/Reviewer_uGxv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10830/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980032397, "cdate": 1761980032397, "tmdate": 1762922032875, "mdate": 1762922032875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}