{"id": "piylyBPSau", "number": 5994, "cdate": 1757949949257, "mdate": 1759897940647, "content": {"title": "GenCoGS: Generative Completion-based 3D Gaussian Splatting for High-Fidelity Few-Shot Novel View Synthesis", "abstract": "Conventional few-shot novel view synthesis (NVS) methods using 3D Gaussian Splatting (3DGS) have demonstrated significance, but remain constrained by their overdependence on the limited information from training views. Their unsatisfactory scene completion capability would underrepresent those scene regions either unobserved in training views or with local details and thus cause floating artifacts against high fidelity.\nTo address these challenges, we propose GenCoGS, a unified 3DGS-based few-shot NVS method focusing on initializing and optimizing 3DGS representation using generative completion-based strategies to enhance scene completion.\nSpecifically, our generative point cloud completion-based strategy produces and filters complementary points toward a complete point cloud with refined structural and appearance information for Gaussian initialization;\nThe generative pseudo view completion-based strategy leverages an image-to-video diffusion model to synthesize complete pseudo views, which benefits Gaussian optimization especially within unobserved scene regions and mitigates hallucination for less appearance distortion. \nIntegrating both strategies enables accurate and coherent scene completion for high-fidelity few-shot NVS.\nExtensive experiments on three benchmark datasets demonstrate the superiority of our GenCoGS for few-shot NVS evaluated in common metrics compared to baseline methods. Compared to those 3DGS-based few-shot NVS methods, our GenCoGS achieves improvements of up to $2.40$dB, $0.08$ and $0.125$ in PSNR, SSIM and LPIPS.", "tldr": "", "keywords": ["3D Gaussians Splatting", "Novel View Synthesis", "few-shot", "Generative Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a50c34837de36ab1252f796711d98220bcaab17f.pdf", "supplementary_material": "/attachment/97ef9042275dff9bd74549e98c12aa083e9a1308.zip"}, "replies": [{"content": {"summary": {"value": "This paper novelly leverage a point cloud completion network to improve the completeness of the initial point cloud used in 3D Gaussian Splatting (3DGS). It also proposes a new filtering strategy to mitigate the hallucination artifacts introduced by point cloud completion. In addition, the paper introduces GCGO (Generative point cloud Completion-based Gaussian Optimization) to further suppress hallucinations arising from the diffusion model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first paper that leverages a point cloud completion network to enhance 3D Gaussian Splatting (3DGS). To address the hallucination problem introduced by incorporating such additional priors, the paper further proposes a CPF (Completion Prior Filtering) module to effectively suppress these artifacts and improve reconstruction reliability.\n\n2. The modules proposed by the authors are all effective and consistently improve performance."}, "weaknesses": {"value": "1. One concern is the choice of using a point cloud completion network instead of employing geometry foundation models such as MVSNet based method [2], VGGT, or MapAnything [1] to obtain denser and more accurate 3D points. In general, these geometry-oriented models are capable of producing high-quality 3D structures, and recent works such as DepthSplat [3] and FlowR [4] have already explored this direction. Therefore, I find it difficult to be fully convinced that the capability of geometry foundation models would be inferior to that of a point cloud completion network in this context.\n\n2. The ablation study also raises some concerns. We do not clearly observe a step-by-step improvement in 3DGS performance as each proposed component is introduced. In addition, it is unclear what the baseline in Table 4 represents. If the baseline corresponds to the original 3DGS, the reported performance is inconsistent with that shown in Table 1. If it does not correspond to 3DGS, then it is difficult to effectively evaluate the actual improvement brought by the proposed method.\n\n3.The naming of “Completion-based Gaussian Optimization” (GCGO) is somewhat confusing, since the section mainly describes how diffusion is used for novel-view images, rather than directly optimizing the Gaussian parameter. The proposed (GCGO) formulation does not clearly differ from using diffusion priors for novel-view enhancement.\n\n4.  The paper assumes that CLIP features “hold multi-view consistency” and that an I2V diffusion model can maintain such consistency during pseudo-view completion. However, there is no evidence demonstrated CLIP can hold multi-view consistenc. And temporal coherence in video diffusion models does not imply geometric or cross-view consistency, which usually requires to be fine-tuned by novel view synthesis dataset. \n\n5. This paper did not make comparison with really SOTA such as Difix3D+ [5] and Genfusion [6].\n\n[1] Keetha, N., Müller, N., Schönberger, J., Porzi, L., Zhang, Y., Fischer, T., ... & Kontschieder, P. (2025). MapAnything: Universal feed-forward metric 3D reconstruction. arXiv preprint arXiv:2509.13414.\n\n[2] Izquierdo, S., Sayed, M., Firman, M., Garcia-Hernando, G., Turmukhambetov, D., Civera, J., ... & Watson, J. (2025). MVSAnywhere: Zero-Shot Multi-View Stereo. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 11493-11504).\n\n[3] Xu, H., Peng, S., Wang, F., Blum, H., Barath, D., Geiger, A., & Pollefeys, M. (2025). Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 16453-16463).\n\n[4] Fischer, T., Bulò, S. R., Yang, Y. H., Keetha, N., Porzi, L., Müller, N., ... & Kontschieder, P. (2025). Flowr: Flowing from sparse to dense 3d reconstructions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 27702-27712).\n\n[5] Wu, J. Z., Zhang, Y., Turki, H., Ren, X., Gao, J., Shou, M. Z., ... & Ling, H. (2025). Difix3d+: Improving 3d reconstructions with single-step diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 26024-26035).\n\n[6]Wu, S., Xu, C., Huang, B., Geiger, A., & Chen, A. (2025). Genfusion: Closing the loop between reconstruction and generation via videos. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 6078-6088)."}, "questions": {"value": "1. Could you do an ablation study based on 3DGS?\n\n2.May you give the detailed explanation of Completion-based Gaussian Optimization? I do not understand why it is an Optimization.\n\n3. The formula [10] given by GCGO means the diffusion conditioned by CLIP features. what is the difference with ReconX ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "z6dkB404Py", "forum": "piylyBPSau", "replyto": "piylyBPSau", "signatures": ["ICLR.cc/2026/Conference/Submission5994/Reviewer_XGYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5994/Reviewer_XGYu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541501549, "cdate": 1761541501549, "tmdate": 1762918399283, "mdate": 1762918399283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GenCoGS proposes to improve few-shot novel-view synthesis with 3D Gaussian Splatting with a two-step procedure. First, generative point-cloud completion: a generate-and-filter complementary-point cloud to densify the SfM point cloud. Second, generative pseudo-view generation: sampling pseudo camera poses via a perturbed camera trajectory, producing conditional completions with an image-to-video diffusion model. The authors report consistent gains across LLFF/DTU/Shiny and present ablation results showing that each component helps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation. The paper tackles a real failure mode for few-shot 3DGS methods - poor scene completion and floating artifacts - and proposes a strategy to address it.\n2. Two-stage design. Separating initialization (complementary points) from optimization (pseudo-view completions) is sensible and interpretable.\n3. Practical ablations. The paper includes a comprehensive ablation suite. Supplementary material is particularly strong.\n4. Quantitative gains. The method shows consistent rendering quality improvement over all the datasets"}, "weaknesses": {"value": "1. Novelty. The paper currently presents GenCoGS as a \"generative completion\" approach, but most subcomponents (point completion, filtering, pseudo-view generation, consistency loss) resemble adaptations of existing ideas. Without clearer theoretical or algorithmic justification, the method may seem as an engineering combination of known blocks.\n\n2. Complementary Point Generator (CPG). The paper doesn’t clearly specify whether the CPG network is trained offline (on a dataset of partial -> complete point clouds) or per scene, nor the loss functions used. This directly affects reproducibility and interpretability of the paper.\n\n3. Complimentary Point Filtering (CPF). CPF uses a global distance threshold to remove outliers in generated points. This is brittle and scene-dependent; the ablation shows sensitivity. It does not seem to be robust enough to be generalizable.\n\n4. Evaluation. Metrics are reported from single runs without standard deviation. The paper also claims hallucination attenuation but provides no quantitative measure of it.\n\n5. Practicality. Training takes 3x longer and uses 2.5x more memory than vanilla 3DGS, yet the paper underplays this limitation. It is important to know whether GenCoGS is practical.\n\nFor every point, see the Questions section for suggestions."}, "questions": {"value": "For the points mentioned in the Weaknesses:\n\n1. Novelty. Explicitly state the unique contribution in one paragraph\n\n2. CPG. Elaborate on CPG training details such as:\n2.a. Training details of CPG\n2.b  Clarify whether CPG is pretrained once and then frozen, or jointly optimized per scene\n2.c If trained per-scene, provide the time overhead and discuss how this scales\n2.d Explain the intuition behind all the methods used to generate the complimentary points: DGCNN, PE, kNN, Dynamic Query Mechanism, FoldingNet\n\n3. CPF. CPF seems like a fine-tuning trick with a sensitive threshold:\n3a. Provide ablations on sensitivity of the delta parameter over other datasets and different amount of views\n3b. Provide the statistics on what % of the newly added points are filtered out\n\n4. Evaluation. The whole setup with relatively small gains seems \"seed dendent\". Therefore\n4a. Provide mean and std for all the metrics over 5 runs at least on LLFF dataset\n4b. Can the authors somehow quantify the hallucination and its attenuation that was achieved using your approach?\n\n5. Practicality. Can you provide compute-runtime/quality tradeoff? For example, on the X axis number of iterations/number of generated views and on the Y PSNR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VG80f75ydT", "forum": "piylyBPSau", "replyto": "piylyBPSau", "signatures": ["ICLR.cc/2026/Conference/Submission5994/Reviewer_YRYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5994/Reviewer_YRYJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730036306, "cdate": 1761730036306, "tmdate": 1762918398963, "mdate": 1762918398963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GenCoGS, a generative completion-based framework that enhances few-shot novel view synthesis (NVS) using 3D Gaussian Splatting (3DGS). \n\n(1) Generative Point Cloud Completion-based Gaussian Initialization (GCGI) that refines Gaussian initialization through complementary point generation and filtering\n\n(2)Generative Pseudo View Completion-based Gaussian Optimization (GCGO), employing diffusion-based pseudo-view synthesis and a generative consistency loss.\n\nExperiments on LLFF, DTU, and Shiny datasets show consistent improvements in PSNR, SSIM, and LPIPS over baselines like FSGS and BinoGS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tCombines complementary strategies (GCGI + GCGO) into a coherent and unified 3DGS framework, improving both initialization and optimization phases.\n\n2.\tConducts comprehensive experiments across multiple datasets and shot settings, showing clear and consistent gains over strong baselines.\n\n3.\tDemonstrates reduced artifacts and more complete scene reconstruction in unobserved regions, indicating effective integration of generative priors for scene completion."}, "weaknesses": {"value": "1.\tLimited conceptual novelty: The method primarily extends prior diffusion-augmented NVS ideas (e.g., ViewCrafter, ReconFusion) to point cloud based 3DGS, without introducing a substantially new theoretical concept.\n\n2.\tWeak theoretical grounding: The paper lacks formal analysis or intuition on how generative completion improves geometric consistency or the optimization dynamics of Gaussian Splatting.\n\n3.\tExternal priors: The diffusion-based pseudo-view completion uses a pre-trained I2V model as a black box, but the paper does not analyze its domain sensitivity, generalization, or contribution through ablation.\n\n4.\tPresentation clarity: The paper is technically dense, and mathematical sections could be simplified or supplemented with a clearer pipeline overview or pseudocode for better readability."}, "questions": {"value": "1.\tHow does GenCoGS compare computationally (training time and GPU memory) with FSGS and BinoGS?\n\n2.\tCould the Complementary Point Filtering (CPF) module inadvertently remove valid geometry in textured or noisy regions?\n\n3.\tCould the authors test on more challenging or real-world datasets, and include failure case analysis to assess generalization limits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zg1TdvU9yp", "forum": "piylyBPSau", "replyto": "piylyBPSau", "signatures": ["ICLR.cc/2026/Conference/Submission5994/Reviewer_5er1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5994/Reviewer_5er1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881529015, "cdate": 1761881529015, "tmdate": 1762918398219, "mdate": 1762918398219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a generative-model-based initialization and optimization framework for few-shot novel view synthesis. Specifically, a set-to-set generation module is used to complete the SfM-reconstructed point cloud, and an image-to-video diffusion model is employed to produce pseudo-views for supervision. Extensive experiments demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly organized, making it easy to follow.\n- The proposed initialization strategy is interesting, as it leverages generative models to complete point clouds.\n- Both qualitative and quantitative results demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "- Several important implementation details of GCGI are missing: (1) the computation of the positional embedding in Eq. 1. (2) the initialization strategy and the number of dynamic queries Q. (3) whether DGCNN, $M_E$, $M_D$, and Foldingnet are pretrained. (4) the structure details of $M_E$ and $M_D$. (5) in the top-left of Fig. 2, whether “encoder-decoder” refers to $M_E$/$M_D$ or FolderNet.\n- The same mathematical symbol is overloaded to denote multiple concepts, which may cause confusion. For example, $m$ in Line 180 and Line 321, $C$ in Line 164 and Line 284."}, "questions": {"value": "- This work states that GCGI 'produces and filters complementary points toward a complete point cloud with refined structural and appearance information for Gaussian initialization'. However, Sec. 3.1 primarily describes structural point cloud completion for initialization. Could the authors clarify how the appearance information is refined within this module?\n- GIGC aims to achieve set-to-set generation. Have the authors considered an alternative pipeline in which image-to-video (I2V) generation is first applied, followed by structure-from-motion (SfM) on both the real and generated views to obtain a more complete point cloud? Besides, my concern is that the set-to-set generation models is a bit out-of-date, does it perform good than the recent I2V generation models?\n- Could the authors clarify the differences between the proposed GCGO and the similar techniques used in 3DGS-Enhancer [A] and DIFIX3D+ [B]?\n- In Eq. 18, $L_{reg}$ applies an L1 loss on pseudo views, which also filters the noises. Why does the $L_{img}$, used at the same stage, also adopt an L1​ loss that does not filter noise? These two losses may be redundant—could the authors clarify the motivation behind them?\n\n[A] Liu, Xi, Chaoyi Zhou, and Siyu Huang. \"3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors.\" NeurIPS 2024.\n\n[B] Wu, Jay Zhangjie, et al. \"Difix3d+: Improving 3d reconstructions with single-step diffusion models.\"CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cqv3ylmTIN", "forum": "piylyBPSau", "replyto": "piylyBPSau", "signatures": ["ICLR.cc/2026/Conference/Submission5994/Reviewer_tyFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5994/Reviewer_tyFj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936337624, "cdate": 1761936337624, "tmdate": 1762918397871, "mdate": 1762918397871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}