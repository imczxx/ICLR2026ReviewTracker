{"id": "NEFldJX4zb", "number": 3053, "cdate": 1757323240356, "mdate": 1763724610212, "content": {"title": "Fine-Grained Class-Conditional Distribution Balancing for Debiased Learning", "abstract": "Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable.\nRecent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. \nHowever, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. \nTo address this limitation, we propose a novel Multi-stage data-Selective reTraining strategy (MST), which describes each distribution in greater detail using the hard confusion matrix.\nBuilding on these finer descriptions, we propose a fine-grained variant of CCDB, termed FG-CCDB, which enhances distribution matching through more precise confusion-cell-wise reweighting. FG-CCDB learns sample weights from a global perspective, effectively mitigating spurious correlations without incurring substantial storage or computational overhead.\nExtensive experiments demonstrate that MST serves as a reliable proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods.\nMoreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.", "tldr": "", "keywords": ["group robust classification", "spurious correlations", "short-cut mitigation", "distribution balancing"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6f0c22c2f34b8a1c4cd8200b44bdc6e555565e2.pdf", "supplementary_material": "/attachment/5a14c45e19ccd408fd2904e15b65966c05ffd2e5.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method named Fine-Grained Class-Conditional Distribution Balancing (FG-CCDB) to mitigate spurious correlations in deep learning models under the condition of no bias annotations. The authors point out that existing methods (e.g., CCDB) model class-conditional and marginal distributions as single Gaussian distributions, which are too coarse-grained to capture the complex multi-modal structures in real-world data. To address this, they design a **Multi-stage data-Selective reTraining (MST)** strategy that leverages model overfitting behavior to identify \"modes\" (i.e., bias patterns) through multi-stage data filtering. Based on this, FG-CCDB performs distribution alignment at the mode level, achieving more precise sample reweighting. Experimental results show that the method can rival bias-supervised approaches in binary classification tasks and even outperform them in multi-class tasks, with low computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "（1）Proposed Multi-stage data-Selective reTraining (MST), which utilizes model overfitting to construct highly biased training sets through multi-stage data filtering. This generates reliable pseudo-bias labels and forms a hard confusion matrix to characterize fine-grained \"mode\" structures, offering a new perspective for unsupervised bias exploration.  \n（2）Proposed FG-CCDB, which performs class-conditional and marginal distribution alignment at the mode level based on MST. This achieves finer and more effective distribution matching than the original CCDB, effectively mitigating spurious correlations.  \n（3）The method requires no ground-truth bias annotations, is computationally efficient, and allows sample weights to be computed via closed-form solutions. It achieves superior or comparable performance to existing unsupervised and partially supervised methods on multiple binary and multi-class benchmarks, particularly in strong-bias multi-class scenarios."}, "weaknesses": {"value": "（1）The definition of \"mode\" is dependent on specific bias model architectures and training processes, lacking semantic clarity and theoretical guarantees.  \nThe authors define a mode as (s, y), where s is a \"pseudo-bias label\" predicted by an auxiliary bias model. However, the physical meaning of s is ambiguous—it may correspond to a single shortcut, a combination of shortcuts, or even entangled uninterpretable patterns. This makes \"mode\" a black-box concept, with unclear mapping to latent bias factors in the data generation process. Moreover, the quality of s heavily relies on the initial ERM model's overfitting behavior. If the model fails to capture major biases, the entire MST process may collapse.  \n\n（2）Hyperparameter choices in MST (e.g., γ=10%, top 50% high-confidence samples) lack systematic analysis and generalization guarantees.  \nWhile the authors claim γ=10% is a \"sweet spot,\" this conclusion seems empirical, with no sufficient ablation studies to validate its robustness across datasets or bias strengths. Similarly, selecting the top 50% high-confidence samples appears arbitrary, with no discussion of alternative proportions (e.g., 30% or 70%). These critical hyperparameters lack theoretical justification or adaptive mechanisms, reducing the method's universality and reproducibility.  \n\n（3）The method's validity critically depends on the strong assumption that \"overfitted models accurately reflect bias structures,\" which may not hold in complex real-world scenarios. \nThe core idea is to exploit ERM models' overfitting to reveal biases. However, in real-world data, biases may be subtle or diverse, and models might overfit to noise or irrelevant features instead of true bias cues. Additionally, when multiple competing biases exist, the model may capture only one, leaving MST unable to reveal the full bias structure. While experiments show strong performance, the robustness of this assumption has not been validated in more challenging scenarios with hidden or complex biases.  \n\n（4）Insufficient comparison with existing methods (e.g., XRM, DebiAN) to highlight FG-CCDB's core innovations. \nThe paper includes XRM and DebiAN as baselines but provides brief descriptions of their mechanisms. XRM uses auxiliary models to generate pseudo-group labels, while DebiAN iteratively trains a \"discoverer\" model to identify biases. MST is essentially another pseudo-labeling strategy. Reviewers expect a more detailed comparison: e.g., does MST's pseudo-label quality significantly outperform XRM or DebiAN? On which bias types does MST excel? Otherwise, FG-CCDB may appear as another variant of existing pseudo-labeling + reweighting paradigms rather than a fundamental breakthrough.  \n\n（5）The experimental evaluation lacks direct quantitative analysis of mode partitioning quality. \nWhile Figure 2(b) shows a heatmap of the joint distribution J and claims it reflects true biases, there are no quantitative metrics (e.g., mutual information, F1-score with ground-truth biases) to evaluate the accuracy of MST-generated mode partitions. If the mode partitioning itself contains significant errors, subsequent FG-CCDB distribution matching may amplify these errors, leading to performance degradation. The authors claim MST \"serves as a strong proxy for ground-truth bias annotations,\" but this assertion requires stronger empirical or theoretical evidence."}, "questions": {"value": "（1）FG-CCDB assumes samples within a mode are homogeneous and assigns them equal weights. However, a mode (e.g., a specific background for a class) may still have substructures or diversity. Does ignoring intra-mode heterogeneity affect reweighting effectiveness? Have you considered further subdivision within modes or introducing continuous weights?  \n\n（2）In multi-stage MST, repeating \"bias enhancement learning\" up to three times improves performance. Is there a saturation point? Would further iterations cause model collapse or overfitting to noise? Could you provide performance curves across iteration counts to demonstrate convergence?  \n\n（3）Although FG-CCDB is computationally efficient, MST requires training multiple auxiliary models, involving multiple training stages. Compared to single-training methods like ERM or uLA, how much additional training time does it incur? Is this computational cost acceptable in practical applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kvV3v6sy2L", "forum": "NEFldJX4zb", "replyto": "NEFldJX4zb", "signatures": ["ICLR.cc/2026/Conference/Submission3053/Reviewer_FipK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3053/Reviewer_FipK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744113636, "cdate": 1761744113636, "tmdate": 1762916529849, "mdate": 1762916529849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all the reviewers for their constructive feedback. For ease of reading, we use the following abbreviations: for example, R1W1 denotes the first weakness raised by Reviewer 1, and R1Q1 denotes the first question raised by Reviewer 1. The main manuscript and the supplementary material have been revised accordingly."}}, "id": "L8bfZPFtD0", "forum": "NEFldJX4zb", "replyto": "NEFldJX4zb", "signatures": ["ICLR.cc/2026/Conference/Submission3053/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3053/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3053/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763709198409, "cdate": 1763709198409, "tmdate": 1763709198409, "mdate": 1763709198409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon the existing Class-Conditional Distribution Balancing (CCDB) framework and proposes two key components: \n- MST (Multi-stage data-Selective reTraining): a strategy to characterize the bias structure through the hard confusion matrix, serving as a proxy for bias annotations.\n- FG-CCDB (Fine-Grained Class-Conditional Distribution Balancing) - a fine-grained extension of CCDB that performs distribution alignment at the mode level, enabling a more detailed representation of multi-modal data distributions. \n\nThe method aims to achieve annotation-free debiasing and robust generalization by modeling complex intra-class variations caused by spurious correlations. Experimental results show that MST can substitute for bias annotations in supervised baselines (e.g., GroupDRO, DFR), and FG-CCDB outperforms prior approaches in multi-class, bias-heavy scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured and clearly written:\nThe paper is logically organized, and the presentation of ideas—from motivation to methodological formulation—is easy to follow.\n\n- Conceptual improvement over CCDB:\nBy replacing the single-Gaussian assumption with a multi-modal, mode-based distribution matching framework, the paper effectively extends CCDB to more realistic data distributions.\n\n- Novel use of confusion matrix:\nEmploying the confusion matrix to infer bias-aligned and bias-conflicting “modes” is elegant and intuitively appealing.\nIt offers a discrete, bias-agnostic way to describe intra-class spurious correlations.\n\n- Annotation-free contribution:\nThe approach is promising in scenarios where human bias annotations are unavailable or infeasible, showing comparable results to bias-supervised baselines."}, "weaknesses": {"value": "- Indirect validation of MST as bias substitute:\nThe core assumption that MST can replace human-provided bias annotations is only indirectly validated through final task performance. The paper does not report any direct quantitative measure (e.g., F1, ARI, or NMI) of how well MST’s predicted bias partitions align with human-labeled bias groups. Without this, it is unclear whether MST truly captures bias structure or simply produces partitions that happen to improve performance.\n\n- Limited comparison with recent label-free debiasing methods:\nAlthough the paper positions itself within the “annotation-free” literature, it lacks comparisons with the latest label-free or label-free-from-features (LFF) methods, such as those that employ generative modeling or causal data augmentation for debiasing. Including these would strengthen the empirical validity and demonstrate broader applicability.\n\n- Experimental generality:\nMost experiments are conducted on benchmark datasets (e.g., Waterbirds, CMNIST) with relatively simple, well-defined bias sources.\nThe performance of MST and FG-CCDB under multi-bias or entangled bias scenarios remains uncertain.\n\n- Ablation analysis depth:\nWhile ablations for MST and FG-CCDB are presented, the interaction between the two modules is not deeply analyzed. It is unclear how errors from MST propagate to FG-CCDB, or whether FG-CCDB can compensate for imperfect bias predictions."}, "questions": {"value": "As the paper claims, can the authors experimentally demonstrate how well the MST matches human labels?\nCan the authors of the paper present performance comparison results with the latest methods using 'other label free + generative model methods'?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RxLJncUeYf", "forum": "NEFldJX4zb", "replyto": "NEFldJX4zb", "signatures": ["ICLR.cc/2026/Conference/Submission3053/Reviewer_exPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3053/Reviewer_exPH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808570657, "cdate": 1761808570657, "tmdate": 1762916529425, "mdate": 1762916529425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses debiased learning under spurious correlations without relying on bias annotations. It targets the problem that existing methods mitigate spurious correlations by matching class-conditional to marginal distributions but rely on overly coarse single-Gaussian approximations that fail in multi-modal, multi-class settings. The authors propose MST to exploit ERM overfitting, training on a small split, and amplifying bias via per-class top-confidence selection to derive discrete modes from the final confusion matrix. Then they propose FG-CCDB to transform this matrix into mode-wise weights that align class-conditional to marginal distributions, reducing spurious reliance with lightweight, scalable computation. Experiments that MST can effectively substitute for human bias annotations in supervised methods, and FG-CCDB consistently outperforms or matches state-of-the-art bias-agnostic baselines while maintaining low computational and memory overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-The paper offers a thorough and compelling analysis of the limitations in prior work, clearly diagnosing CCDB’s single-Gaussian assumption and proposing an elegant, scalable remedy via mode-wise matching derived from confusion-matrix–based distributions. \n-The approach demonstrates strong practical value, showing robustness to multi-shortcut scenarios (e.g., UrbanCars) with competitive or superior performance relative to both bias-agnostic and supervised baselines.\n-The experimental evaluation is comprehensive. Ablations cleanly disentangle the contributions of MST and FG-CCDB, and the correlation-shift analyses substantiate the mechanism that the method reduces reliance on bias-related features."}, "weaknesses": {"value": "-The proposed mechanism lacks theoretical grounding. There is no formal analysis of how iterative bias amplification improves minority-mode recall. While the empirical evidence is compelling, a theoretical treatment would substantially strengthen the contribution.\n-Several methodological details require further clarification. The selection of top-confidence samples hinges on the biased model’s calibration. Miscalibration could distort mode discovery, yet no temperature scaling or calibration baseline is reported."}, "questions": {"value": "Why fix the top-50% per-class in bias enhancement? Did you explore adaptive thresholds (e.g., based on class-wise confidence distributions, entropy) or rank-based schedules across iterations? Any results on temperature scaling to improve confidence reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EpXvBHrJX1", "forum": "NEFldJX4zb", "replyto": "NEFldJX4zb", "signatures": ["ICLR.cc/2026/Conference/Submission3053/Reviewer_2Y79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3053/Reviewer_2Y79"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879691683, "cdate": 1761879691683, "tmdate": 1762916529254, "mdate": 1762916529254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}