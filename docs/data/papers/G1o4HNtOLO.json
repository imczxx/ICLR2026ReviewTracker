{"id": "G1o4HNtOLO", "number": 5066, "cdate": 1757841690203, "mdate": 1763652884121, "content": {"title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization", "abstract": "Tokenizers are a key component of state-of-the-art generative image models, extracting the most important features from the signal while reducing data dimension and redundancy. Most current tokenizers are based on KL-regularized variational autoencoders (KL-VAE), trained with reconstruction, perceptual and adversarial losses. Diffusion decoders have been proposed as a more principled alternative to model the distribution over images conditioned on the latent. However, matching the performance of KL-VAE still requires adversarial losses, as well as a higher decoding time due to iterative sampling. To address these limitations, we introduce a new pixel diffusion decoder architecture for improved scaling and training stability, benefiting from transformer components and GAN-free training. We use distillation to replicate the performance of the diffusion decoder in an efficient single-step decoder. This makes SSDD the first diffusion decoder optimized for single-step reconstruction trained without adversarial losses, reaching higher reconstruction quality and faster sampling than KL-VAE. In particular, SSDD improves reconstruction FID from 0.87 to 0.50 with 1.4⨉ higher throughput and preserve generation quality of DiTs with 3.8⨉ faster sampling. As such, SSDD can be used as a drop-in replacement for KL-VAE, and for building higher-quality and faster generative models.", "tldr": "We introduce a single-step diffusion decoder for image reconstruction from latent embeddings, and show improvements both on image quality and decoding speed.", "keywords": ["diffusion", "flow matching", "autoencoder", "decoder", "image", "image generation", "generative"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5187b1f27bf366bf12b3558f3353f967252dab86.pdf", "supplementary_material": "/attachment/810ae18c5b84c9e25ef8ed81b7f04a0c0c7528ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SSDD, an efficient single-step diffusion decoder, which achieves improvements in both reconstruction quality and sampling speed without relying on GAN loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces SSDD, a single-step decoder, and provides extensive experiments demonstrating its effectiveness in improving reconstruction fidelity and training efficiency. \n\n\n2. The exploration of removing GAN losses to achieve more stable training is valuable and practically relevant for large-scale diffusion modeling."}, "weaknesses": {"value": "1. This is primarily an engineering-focused work. The proposed SSDD architecture is largely based on U-ViT, with modifications to the resolution hierarchy and the inclusion of REPA loss, LPIPS loss, and a distillation technique. While these design choices lead to strong empirical performance, the methodological novelty and conceptual insights remain limited. Therefore, although this work demonstrates improvements in training efficiency, the insights are relatively limited, which leads me to keep my overall rating at the marginal level.\n\n2. The experiments are conducted mainly on ImageNet. It would strengthen the paper to include additional evaluations on out-of-distribution datasets such as COCO or other domains, to verify the generalization of both reconstruction and generation performance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NhcHAjivgE", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_S7Tn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_S7Tn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813746106, "cdate": 1761813746106, "tmdate": 1762917852690, "mdate": 1762917852690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General answer to all reviewers"}, "comment": {"value": "We would like to thank the reviewers for highlighting important strengths of our paper, including that our work “targets a practical and relevant limitation in generative pipelines”, that our “model design is computationally efficient” and “can potentially serve as a drop-in replacement for VAEs”, that “SSDD achieves impressive performance improvements”, that our paper “provides extensive experiments demonstrating its effectiveness”, and that the “the exploration of removing GAN losses to achieve more stable training is valuable and practically relevant for large-scale diffusion modeling”.\n\nSome concerns have been raised about both specific technical points of the paper, and about the contributions of this work. While we answer specific questions to each reviewer separately, we would like to provide a general answer to highlight the contributions our work provides to the community.\nMain takeaway from our paper: similarly as to how diffusion models have been shown to beat GANs for pixel-space generation [1], we show that they can also replace completely GAN-optimized decoders for the decoding stage of the current latent diffusion models [2]. Existing works using diffusion-based decoders [3,4,5] suffered from diverse drawbacks both in image quality and generation speed. Our results show for the first time that diffusion-trained decoders can provide improvements on both fronts in pixel-space, and contribute to shifting the perspective on optimal decoding methods.\nMethodological contribution: far from behind a straightforward task, pixel-space decoding of latents has been an ongoing research topic ([3,4,5]) facing issues about difficulties of pixel-space diffusion as well as sampling speed on higher resolution images. We provide a clear method to overcome these issues, analyzing the impact of each component (Tables 4, S2, S3, S4, Figures S1, S3, S4) and impact on speed, quality (Table 1) and scalability to higher resolutions (Table 2, Figure 3). All together, they allow training a new class of efficient pixel-space decoders that was not possible before.\nTechnical contribution: we are releasing the training code of our models (already in the supplementary material of the submission), and will release the weights of trained decoders at different compression scales. We believe it provides a valuable contribution to the community, as there are no open-source or open-weight diffusion decoders providing improved results over KL-VAE decoders.\nAnalytical insights: our paper provides theoretical and analytical insights about the behavior of diffusion decoders for sampling and distillation. This behavior, as already mentioned by [3], differs from usual diffusion decoders. We provide extensive experiments with additional theoretical insights, highlighting the cause of the behavior that affects all LPIPS-regularized diffusion-based decoders beyond SSDD.\nSome reviews question the novelty of the contribution, as several components have been applied to others tasks, each individually. Our contribution is not about introducing such a new component on top of an existing method (as some, like REPA [6], were), but about a methodology and design choice leading to “strong empirical performance”, as it was highlighted in the reviews. Existing work on diffusion decoders [3,4,5] has shown that the training of diffusion-based decoders is a non-trivial task, and existing methods suffer from multiple shortcomings. We hope our contributions can provide significant advances to this research direction.\nSome reviews also refer to SSDD as an “engineering-focused work”. While we consider design choices, our work is focused on methodological aspects. We provide extensive comparison with baselines on multiple aspects, and experiments highlighting the impact of each component. We do not optimize or engineer aspects such as the dataset or the latent diffusion model, using standard dataset (ImageNet) and diffusion model (DiT).\n\nWe thank the reviewers for taking the same to provide helpful comments on our work. We updated the paper to include additional proposed evaluations, and are open to further suggestions if some experiments seem unconvincing.\n\nAdditionally, we fixed a technical issue in the training code (already fixed in the code submitted with the supplementary material) hindering training, and updated SSDD results accordingly, providing stronger results on reconstruction tasks. We further removed the ‘H’ model size, which was not providing significantly stronger results than the new L and XL models.\n\n[1] Diffusion Models Beat GANs on Image Synthesis, Dhariwal et al.\n[2] High-Resolution Image Synthesis with Latent Diffusion Models, Rombach et al.\n[3] Epsilon-VAE: Denoising as Visual Decoding, Zhao et al.\n[4] Diffusion Autoencoders are Scalable Image Tokenizers, Chen et al.\n[5] Improving Image Generation with Better Captions, Betker et al.\n[6] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think"}}, "id": "kuZV3rmO3o", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653026148, "cdate": 1763653026148, "tmdate": 1763653026148, "mdate": 1763653026148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SSDD (Single-Step Diffusion Decoder), a new image tokenizer designed to replace traditional KL-regularized VAEs. The method's key contributions include: 1) A new pixel diffusion decoder architecture that combines a convolutional U-Net with a central transformer block, designed for improved scalability and stability. 2) A GAN-free training scheme that relies on a combination of a flow-matching objective, a perceptual LPIPS loss, and a REPA feature regularization loss. 3) A distillation process that transfers the high-quality output of a multi-step diffusion decoder into a fast, single-step model. The authors demonstrate that SSDD achieves state-of-the-art reconstruction quality on perceptual metrics like rFID and LPIPS, surpassing existing VAEs and diffusion decoders while offering significantly higher throughput. They also show that using SSDD as a decoder for a DiT model preserves generation quality while drastically speeding up inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper performs good improvements against baseline.\n- Extensive experiments are conducted."}, "weaknesses": {"value": "- The baseline is just $\\epsilon$-VAE which is limited.\n- What is the difference between train regular decoder and then train a refiner against diffusion decoder?\n- Diffusion decoder is more like a generation task instead of reconstruction. The usage of it is questionable.\n- There is no novelty in the method side."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0O0HvJ1AVz", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_QJRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_QJRp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883866268, "cdate": 1761883866268, "tmdate": 1762917852410, "mdate": 1762917852410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SSDD (Single-Step Diffusion Decoder), a novel diffusion-based autoencoder for image tokenization that addresses key limitations of existing approaches. The authors propose a hybrid U-Net-transformer architecture that leverages flow matching with perceptual alignment (LPIPS) and REPA regularization to achieve state-of-the-art reconstruction quality without adversarial training. Through a distillation strategy, they compress multi-step diffusion behavior into a single-step decoder, achieving 3.8× faster sampling while maintaining generation quality. SSDD improves reconstruction FID from 0.87 to 0.50 compared to KL-VAE with 1.4× higher throughput, making it suitable as a drop-in replacement for existing tokenizers in generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  SSDD achieves impressive performance improvements across multiple metrics, particularly in reconstruction FID (0.87→0.50) and generation speed (3.8× faster) compared to baselines.\n-  Successfully eliminates the need for adversarial training in both encoder training and decoder distillation, while achieving competitive or superior results.\n- The paper includes extensive experiments with thorough ablations, multiple baselines, and evaluation across various resolutions (128×128 to 512×512) and model sizes (13.4M to 345.9M parameters)."}, "weaknesses": {"value": "- The core architecture builds heavily on existing components (U-ViT, REPA loss, flow matching). While the combination is effective, the individual components are not novel. \n- Although SSDD is the first work to demonstrate that a single-step diffusion decoder can match the performance of multi-step diffusion decoders, similar work has already been thoroughly explored in the context of conditional diffusion models (text-to-image or image-to-image). Replacing the condition from text with latents seems not particularly special. This significantly diminishes the contribution of this work."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CDKRSXXG7j", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_TKd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_TKd1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994664371, "cdate": 1761994664371, "tmdate": 1762917852085, "mdate": 1762917852085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SSDD, a single-step diffusion decoder designed for efficient image tokenization. The goal is to overcome the inefficiency of KL-regularized VAEs and multi-step diffusion decoders, which typically require adversarial losses and iterative sampling. SSDD introduces a U-ViT–based pixel diffusion decoder trained under a GAN-free flow-matching objective and perceptual regularization, followed by a distillation stage that compresses a multi-step diffusion process into a single-step model. The method claims to improve reconstruction FID from 0.87 to 0.50 and increase decoding throughput by 1.4×, while maintaining the generation quality of DiT models with 3.8× faster sampling. Experiments on ImageNet demonstrate quantitative advantages over KL-VAE, SD-VAE, LiteVAE, ε-VAE, and VA-VAE, across multiple compression setups."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The work targets a practical and relevant limitation in generative pipelines — the trade-off between reconstruction quality and decoding speed. The single-step distillation from a multi-step diffusion decoder is an intuitive and useful engineering contribution that could simplify downstream diffusion training.\n\n- The model design is computationally efficient (U-ViT backbone, GAN-free objective) and can potentially serve as a drop-in replacement for VAEs in large-scale text-to-image systems."}, "weaknesses": {"value": "**Soundness**\n\nMy main concern is with the quantitative evaluation. Unless I’m missing something, several reported numbers—especially in Table 3—look inconsistent with established baselines. For instance, the paper claims substantial gaps for the KL-VAE f8c4 tokenizer on ImageNet 256×256; the no-CFG FIDs in the teens and with-CFG FIDs around 6.x are noticeably worse than what is typically reported for comparable setups. This discrepancy makes it difficult to interpret the claimed SSDD gains. Please clarify:\n\n- the exact evaluation pipeline (data preprocessing, Inception network/version, number of samples, seeds),\n- the CFG scale search protocol and whether scores are reported with best-searched CFG,\n- whether your KL-VAE/SD-VAE checkpoints and training strictly reproduce the original implementations.*\n\n**Technical contribution**\n\nThe “new” elements are largely combinations of existing ideas. The authors are essentially applying standard distillation methods to diffusion models for diffusion decoders, without providing significant insights. \n\n**Presentation**\n\nThe presentation of this paper has significant room for improvement. I highly recommend the authors to polish their paper to a high standard of English. Sentences in this paper are often incomplete or ungrammatical. Examples:\n\n- 'Common tokenizers such as the KL-VAE from Rombach et al. (2022) are optimized with L1 reconstruction loss, LPIPS (Zhang et al., 2018), and a GAN discriminator (Goodfellow et al., 2014), to which a KL-regularization of the latent space is added.' What does the 'which' refer to here is unclear. It will not be a major obstacle for an experienced reader to comprehend, but it is not grammatically correct. \n\n- 'Pixel-space diffusion decoders mainly leverage the same convolutional U-Net architectures (Zhao et al., 2025a) that were found successful in early pixel-space diffusion models (Dhariwal & Nichol, 2021).' I believe there is a typo."}, "questions": {"value": "- Baseline Consistency:\nThe reported rFID improvement over KL-VAE is substantial, but the baseline result (rFID = 0.87) appears considerably weaker than the commonly cited performance of the official Stable Diffusion VAE. Could the authors clarify their training setup—including optimizer, loss weights, and data preprocessing—and verify whether their reproduction matches the official KL-VAE checkpoint performance on ImageNet 256×256? Without this confirmation, the strength of the reported improvement is difficult to assess.\n\n- Perceptual Quality After Distillation:\nDoes the single-step distillation process lead to any loss of perceptual fidelity or fine-detail degradation (e.g., texture aliasing, local blurring, or over-smoothing)? The qualitative examples shown in Figure 3 (right) are not convincing, since the reference image is already very blurry and lacks high-frequency details. Including sharper examples or zoomed-in patches from higher-detail regions would make the comparison more informative.\n\n- Generalization and Scalability:\nCould the authors discuss whether the proposed decoder generalizes to higher-resolution images (e.g., 512×512 or 1024×1024) or to cross-domain datasets (e.g., faces, artworks, medical images) without retraining? If retraining is required, how sensitive is the single-step distillation process to scale and domain shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HFvzeM1vBG", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_8oNA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_8oNA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762588661261, "cdate": 1762588661261, "tmdate": 1762917851810, "mdate": 1762917851810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}