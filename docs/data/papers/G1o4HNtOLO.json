{"id": "G1o4HNtOLO", "number": 5066, "cdate": 1757841690203, "mdate": 1759897996923, "content": {"title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization", "abstract": "Tokenizers are a key component of state-of-the-art generative image models, extracting the most important features from the signal while reducing data dimension and redundancy. Most current tokenizers are based on KL-regularized variational autoencoders (KL-VAE), trained with reconstruction, perceptual and adversarial losses. Diffusion decoders have been proposed as a more principled alternative to model the distribution over images conditioned on the latent. However, matching the performance of KL-VAE still requires adversarial losses, as well as a higher decoding time due toiterative sampling. To address these limitations, we introduce a newpixel diffusion decoder architecture for improved scaling and training stability, benefiting from transformer components and GAN-free training. Weuse distillation to replicate the performance of the diffusion decoder in an efficient single-step decoder. This makes SSDD the first diffusion decoder optimized for single-step reconstruction trained without adversarial losses,reaching higher reconstruction quality and faster samplingthan KL-VAE. In particular, SSDD improves reconstruction FID from 0.87 to 0.50 with 1.4⨉ higher throughput and preserve generation quality of DiTs with 3.8⨉ faster sampling. As such, SSDD can be used as a drop-in replacement for KL-VAE, and for building higher-quality and faster generative models.", "tldr": "We introduce a single-step diffusion decoder for image reconstruction from latent embeddings, and show improvements both on image quality and decoding speed.", "keywords": ["diffusion", "flow matching", "autoencoder", "decoder", "image", "image generation", "generative"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8120168804331fa40d40c0b8c1c3bb656df9b10f.pdf", "supplementary_material": "/attachment/810ae18c5b84c9e25ef8ed81b7f04a0c0c7528ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SSDD, an efficient single-step diffusion decoder, which achieves improvements in both reconstruction quality and sampling speed without relying on GAN loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces SSDD, a single-step decoder, and provides extensive experiments demonstrating its effectiveness in improving reconstruction fidelity and training efficiency. \n\n\n2. The exploration of removing GAN losses to achieve more stable training is valuable and practically relevant for large-scale diffusion modeling."}, "weaknesses": {"value": "1. This is primarily an engineering-focused work. The proposed SSDD architecture is largely based on U-ViT, with modifications to the resolution hierarchy and the inclusion of REPA loss, LPIPS loss, and a distillation technique. While these design choices lead to strong empirical performance, the methodological novelty and conceptual insights remain limited. Therefore, although this work demonstrates improvements in training efficiency, the insights are relatively limited, which leads me to keep my overall rating at the marginal level.\n\n2. The experiments are conducted mainly on ImageNet. It would strengthen the paper to include additional evaluations on out-of-distribution datasets such as COCO or other domains, to verify the generalization of both reconstruction and generation performance."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NhcHAjivgE", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_S7Tn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_S7Tn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813746106, "cdate": 1761813746106, "tmdate": 1762917852690, "mdate": 1762917852690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SSDD (Single-Step Diffusion Decoder), a new image tokenizer designed to replace traditional KL-regularized VAEs. The method's key contributions include: 1) A new pixel diffusion decoder architecture that combines a convolutional U-Net with a central transformer block, designed for improved scalability and stability. 2) A GAN-free training scheme that relies on a combination of a flow-matching objective, a perceptual LPIPS loss, and a REPA feature regularization loss. 3) A distillation process that transfers the high-quality output of a multi-step diffusion decoder into a fast, single-step model. The authors demonstrate that SSDD achieves state-of-the-art reconstruction quality on perceptual metrics like rFID and LPIPS, surpassing existing VAEs and diffusion decoders while offering significantly higher throughput. They also show that using SSDD as a decoder for a DiT model preserves generation quality while drastically speeding up inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper performs good improvements against baseline.\n- Extensive experiments are conducted."}, "weaknesses": {"value": "- The baseline is just $\\epsilon$-VAE which is limited.\n- What is the difference between train regular decoder and then train a refiner against diffusion decoder?\n- Diffusion decoder is more like a generation task instead of reconstruction. The usage of it is questionable.\n- There is no novelty in the method side."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0O0HvJ1AVz", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_QJRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_QJRp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883866268, "cdate": 1761883866268, "tmdate": 1762917852410, "mdate": 1762917852410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SSDD (Single-Step Diffusion Decoder), a novel diffusion-based autoencoder for image tokenization that addresses key limitations of existing approaches. The authors propose a hybrid U-Net-transformer architecture that leverages flow matching with perceptual alignment (LPIPS) and REPA regularization to achieve state-of-the-art reconstruction quality without adversarial training. Through a distillation strategy, they compress multi-step diffusion behavior into a single-step decoder, achieving 3.8× faster sampling while maintaining generation quality. SSDD improves reconstruction FID from 0.87 to 0.50 compared to KL-VAE with 1.4× higher throughput, making it suitable as a drop-in replacement for existing tokenizers in generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-  SSDD achieves impressive performance improvements across multiple metrics, particularly in reconstruction FID (0.87→0.50) and generation speed (3.8× faster) compared to baselines.\n-  Successfully eliminates the need for adversarial training in both encoder training and decoder distillation, while achieving competitive or superior results.\n- The paper includes extensive experiments with thorough ablations, multiple baselines, and evaluation across various resolutions (128×128 to 512×512) and model sizes (13.4M to 345.9M parameters)."}, "weaknesses": {"value": "- The core architecture builds heavily on existing components (U-ViT, REPA loss, flow matching). While the combination is effective, the individual components are not novel. \n- Although SSDD is the first work to demonstrate that a single-step diffusion decoder can match the performance of multi-step diffusion decoders, similar work has already been thoroughly explored in the context of conditional diffusion models (text-to-image or image-to-image). Replacing the condition from text with latents seems not particularly special. This significantly diminishes the contribution of this work."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CDKRSXXG7j", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_TKd1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_TKd1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994664371, "cdate": 1761994664371, "tmdate": 1762917852085, "mdate": 1762917852085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SSDD, a single-step diffusion decoder designed for efficient image tokenization. The goal is to overcome the inefficiency of KL-regularized VAEs and multi-step diffusion decoders, which typically require adversarial losses and iterative sampling. SSDD introduces a U-ViT–based pixel diffusion decoder trained under a GAN-free flow-matching objective and perceptual regularization, followed by a distillation stage that compresses a multi-step diffusion process into a single-step model. The method claims to improve reconstruction FID from 0.87 to 0.50 and increase decoding throughput by 1.4×, while maintaining the generation quality of DiT models with 3.8× faster sampling. Experiments on ImageNet demonstrate quantitative advantages over KL-VAE, SD-VAE, LiteVAE, ε-VAE, and VA-VAE, across multiple compression setups."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The work targets a practical and relevant limitation in generative pipelines — the trade-off between reconstruction quality and decoding speed. The single-step distillation from a multi-step diffusion decoder is an intuitive and useful engineering contribution that could simplify downstream diffusion training.\n\n- The model design is computationally efficient (U-ViT backbone, GAN-free objective) and can potentially serve as a drop-in replacement for VAEs in large-scale text-to-image systems."}, "weaknesses": {"value": "**Soundness**\n\nMy main concern is with the quantitative evaluation. Unless I’m missing something, several reported numbers—especially in Table 3—look inconsistent with established baselines. For instance, the paper claims substantial gaps for the KL-VAE f8c4 tokenizer on ImageNet 256×256; the no-CFG FIDs in the teens and with-CFG FIDs around 6.x are noticeably worse than what is typically reported for comparable setups. This discrepancy makes it difficult to interpret the claimed SSDD gains. Please clarify:\n\n- the exact evaluation pipeline (data preprocessing, Inception network/version, number of samples, seeds),\n- the CFG scale search protocol and whether scores are reported with best-searched CFG,\n- whether your KL-VAE/SD-VAE checkpoints and training strictly reproduce the original implementations.*\n\n**Technical contribution**\n\nThe “new” elements are largely combinations of existing ideas. The authors are essentially applying standard distillation methods to diffusion models for diffusion decoders, without providing significant insights. \n\n**Presentation**\n\nThe presentation of this paper has significant room for improvement. I highly recommend the authors to polish their paper to a high standard of English. Sentences in this paper are often incomplete or ungrammatical. Examples:\n\n- 'Common tokenizers such as the KL-VAE from Rombach et al. (2022) are optimized with L1 reconstruction loss, LPIPS (Zhang et al., 2018), and a GAN discriminator (Goodfellow et al., 2014), to which a KL-regularization of the latent space is added.' What does the 'which' refer to here is unclear. It will not be a major obstacle for an experienced reader to comprehend, but it is not grammatically correct. \n\n- 'Pixel-space diffusion decoders mainly leverage the same convolutional U-Net architectures (Zhao et al., 2025a) that were found successful in early pixel-space diffusion models (Dhariwal & Nichol, 2021).' I believe there is a typo."}, "questions": {"value": "- Baseline Consistency:\nThe reported rFID improvement over KL-VAE is substantial, but the baseline result (rFID = 0.87) appears considerably weaker than the commonly cited performance of the official Stable Diffusion VAE. Could the authors clarify their training setup—including optimizer, loss weights, and data preprocessing—and verify whether their reproduction matches the official KL-VAE checkpoint performance on ImageNet 256×256? Without this confirmation, the strength of the reported improvement is difficult to assess.\n\n- Perceptual Quality After Distillation:\nDoes the single-step distillation process lead to any loss of perceptual fidelity or fine-detail degradation (e.g., texture aliasing, local blurring, or over-smoothing)? The qualitative examples shown in Figure 3 (right) are not convincing, since the reference image is already very blurry and lacks high-frequency details. Including sharper examples or zoomed-in patches from higher-detail regions would make the comparison more informative.\n\n- Generalization and Scalability:\nCould the authors discuss whether the proposed decoder generalizes to higher-resolution images (e.g., 512×512 or 1024×1024) or to cross-domain datasets (e.g., faces, artworks, medical images) without retraining? If retraining is required, how sensitive is the single-step distillation process to scale and domain shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HFvzeM1vBG", "forum": "G1o4HNtOLO", "replyto": "G1o4HNtOLO", "signatures": ["ICLR.cc/2026/Conference/Submission5066/Reviewer_8oNA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5066/Reviewer_8oNA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762588661261, "cdate": 1762588661261, "tmdate": 1762917851810, "mdate": 1762917851810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}