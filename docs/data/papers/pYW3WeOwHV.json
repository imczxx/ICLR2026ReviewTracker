{"id": "pYW3WeOwHV", "number": 21333, "cdate": 1758316345259, "mdate": 1763673426654, "content": {"title": "Optimal Formats for Weight Quantisation", "abstract": "Weight quantisation is an essential technique for enabling efficient training and deployment of modern deep learning models. However, the recipe book of quantisation formats is large, and formats are often chosen empirically. In this paper, we propose a framework for systematic design and analysis of quantisation formats. By connecting the question of format design with the classical quantisation theory, we show that the strong practical performance of popular formats comes from their ability to represent values using variable-length codes. We frame the problem as minimising the KL divergence between original and quantised model outputs under a model size constraint, which can be approximated by minimising the squared quantisation error, a well-studied problem where entropy-constrained quantisers with variable-length codes are optimal. We develop nonlinear quantisation curves for block-scaled data across multiple distribution families and observe that these formats, along with sparse outlier formats, consistently outperform fixed-length formats, indicating that they also exploit variable-length encoding. Finally, by using the relationship between the Fisher information and KL divergence, we derive the optimal allocation of bit-widths to individual parameter tensors across the model’s layers, saving up to 0.25 bits per parameter when applied to large language models.", "tldr": "", "keywords": ["Quantization", "Numerical Formats", "Low Precision", "Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d4c5306d0c147eeae6dc437a6fbe860bb935943.pdf", "supplementary_material": "/attachment/237a2347b1d3c4426e4a5b39f09c2184775c9316.zip"}, "replies": [{"content": {"summary": {"value": "The paper explores various tweaks to scalar quantization formats and proposes minor improvements over existing practices.\nHowever, I am not sure what is the main takeaway here."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper does a thorough evaluation of various settings of scalar quantization."}, "weaknesses": {"value": "- The overall message of the paper is unclear.\n- Concepts are sometimes not introduced or introduced in inappropriate places (Lloyd-max, Huffman coding).\n- Some proposed schemes (i.e. huffman compression) are impractical and cannot be used in efficient GPU kernels.\n- Saying that block schemes perform variable-length encoding seems far fetched.\n- Variable bit allocation could be explored more and compared to previous works like Evopress."}, "questions": {"value": "What is the main takeaway for people who work with scalar quantization here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EZ0pHdUxUD", "forum": "pYW3WeOwHV", "replyto": "pYW3WeOwHV", "signatures": ["ICLR.cc/2026/Conference/Submission21333/Reviewer_VQ1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21333/Reviewer_VQ1o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413156149, "cdate": 1761413156149, "tmdate": 1762941702949, "mdate": 1762941702949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. We are encouraged that several reviewers (ZtPY, HQnU, 9Q1w) found our formalisation and foundations solid and principled, and that our empirical analysis was viewed as thorough and rigorous (HQnU, VQ1o). We identified two primary concerns that we address below, with further points covered in the individual responses. \n\nWe have updated the manuscript in response to the reviewers’ questions and suggestions, which we greatly appreciate. \n\n.\n\n---\n### Approximations (ZtPY, HQnU, 9Q1w)\n\n_Can we confirm the validity of the approximations required to link KL divergence to minimising squared error?_\n\nThe three key approximations are: 1) a second-order Taylor expansion of KL divergence, 2) diagonal Fisher and 3) constant-per-tensor Fisher. \n\nBefore discussing them individually, we note that these approximations are **only used to inform format design** (in our notation, choosing the set $\\tilde{\\Theta}$), but not to search for optimal quantised parameters within these formats. Applying them to optimisation would imply that direct-cast (round-to-nearest) is optimal, however this is known to perform poorly compared with post-training quantisation and quantisation-aware training approaches. This is a subtle, but an important distinction. \n\nWe also compare these approximations to the local-objective assumption, commonly underlying PTQ techniques such as GPTQ (Frantar et al., 2023). These methods replace the global optimisation objective (such as output KL divergence) with a local layer-wise squared reconstruction error, allowing the use of full (per-layer) Hessian, but relying on the strong assumption that improving the local objective is aligned with improving the global objective. As with SqueezeLLM (Kim et al, 2024), our method maintains a global objective; thus, relative strength of assumptions is not strictly comparable. \n\nWe have added additional discussion sections for each approximation into Appendix A to further guide the reader, which we summarise here: \n\n - **2nd order expansion**: Assumes smoothness and a small perturbations. Smoothness is typically expected, as the models were trained with this assumption. Perturbation magnitude is hard to evaluate, but this effect will make the approximation worse for low bit-width formats. \n\n - **Diagonal Fisher**: A strong assumption which is not robust to a change of basis (e.g. if optimising rotated parameters), but still commonly used due to tractability (e.g. Adam optimiser). \n\n - **Scaled-identity Fisher**: Empirically, diagonal Fisher values vary both within and across tensors (Figure 12). We use this assumption only in specific cases (allocating variable bit-width per tensor), but not throughout the work; formats such as weighted Lloyd-Max do not rely on it.\n\n---\n\n### Hardware efficiency: (ZtPY, HQnU, VQ1o)\n\n_Not all formats considered in the work permit efficient implementation._\n\nWe agree that not all formats considered permit efficient hardware implementation (as acknowledged in Section 6). Our aim, however, was to characterise the **space of possible size vs. KL divergence tradeoffs**, which includes formats that may be impractical today. The scope is intentional for several reasons: \n\n - Entropy-coded formats establish an upper bound on achievable compression performance; they showcase a gap between current hardware-efficient formats and the theoretical frontier. \n\n - There already exist demonstrations of fast nonlinear formats and entropy codes. For example, QLoRA (Dettmers et al, 2023) reports speed claims while using the nonlinear NF4 format, and DFloat11 (Zhang et al, 2025) employs efficient Huffman coding to compress the exponents of bfloat16 tensors. \n\n - A comprehensive study of runtime performance across all linear/nonlinear and grouped, sparse, or compression schemes would undoubtedly be valuable, but is beyond the scope of current work, which focuses on the compression limits of quantisation formats. \n\n.\n\n.\n\n--- \n\n### References \n\n[Frantar et al., 2023] \"GPTQ: Accurate post-training quantization for generative pre-trained transformers\", Frantar, Elias, et al., 2022. \n\n[Kim et al, 2024] \"SqueezeLLM: Dense-and-sparse quantization”, Kim, Sehoon, et al. 2024. \n\n[Dettmers et al, 2023] \"QLoRA: Efficient finetuning of quantized LLMs\", Dettmers, Tim, et al., 2023. \n\n[Zhang et al, 2025] \"70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DFloat11)\", Zhang, Tianyi, et al. 2025."}}, "id": "xWWdkIr6wH", "forum": "pYW3WeOwHV", "replyto": "pYW3WeOwHV", "signatures": ["ICLR.cc/2026/Conference/Submission21333/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21333/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21333/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763674523146, "cdate": 1763674523146, "tmdate": 1763674523146, "mdate": 1763674523146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a theoretical framework for choosing the optimal data format for quantization. It first demonstrates that it is sufficient to minimize the squared error between the quantized and non-quantized tensors to find the optimal data format, then compute the optimal format for some known distribution. For unknown distributions, it proposes to fit the experimental distribution with some known ones, using scaling or k-means.\nUnder this framework, the paper show that variable-length code format consistently outperform fixed-length ones. It is also shown that optimal data format choice can also save up to 0.25 bit per parameter for LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper advances the formalization of quantization data format selection, which is often tackled only empirically;\n- Mathematical background seems solid, the SoTA seems adequately cited;\n- Supplementary materials is rich and support some asumptions and approximations made in the paper."}, "weaknesses": {"value": "- I found the paper hard to read and follow. Overall structure could be improved. The figures are way out of place with the text. Some figures mentioned in the main text are missing (figure 8, 29, 33...) only to be found in the supplementary material.\n- Overall, it seems to me that the main paper is not entirely self-supporting without the help of supplementary material.\n- Actual quantization results on LLMs models are completely absent from the paper, and again, can be found only in supplementary material.\n- The reduction of the optimal data format problem finding to the minimization of the squared error between quantized and unquantized tensors relies on a lot of approximations. These approximations are further accumuled with the need to fit unknown distributions. Overall, I find it hard to properly appreciate the validity of these, even though some of them are addressed in supplementary material."}, "questions": {"value": "I would appreciate if more insight could be provided regarding the various hypothesis made in the paper. I think that there is both too much information and too little information: the list of all known distribution's optimal format could be shortened. The related work comes very late in the paper with questionable impact. What do the authors think about perhaps simplifying these sections and bringing more insights on the main propositions and demonstrations notably from the supplementary materials?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "785eJlh5yp", "forum": "pYW3WeOwHV", "replyto": "pYW3WeOwHV", "signatures": ["ICLR.cc/2026/Conference/Submission21333/Reviewer_9Q1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21333/Reviewer_9Q1w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645522484, "cdate": 1761645522484, "tmdate": 1762941702423, "mdate": 1762941702423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the design of optimal quantization formats for neural network weight compression. The authors frame the problem as minimizing KL divergence between original and quantized model outputs under a memory constraint, which they approximate via Fisher-information-weighted squared quantization error. This theoretical reduction enables the application of classical quantization theory techniques to neural network weight compression. They derive optimal element-wise quantizers based on the cube root density rule and extend these to block-scaled formats. A key insight is that effective quantization formats exploit variable-length encoding through either block absmax scaling, sparse outlier storage, or explicit lossless compression. The authors also propose a Fisher-information-based scheme for optimal bit-width allocation across layers. Experiments on multiple LLM families (Llama 3, Qwen 2.5, Gemma 3, Phi 4) validate these insights."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper establishes a principled theoretical framework for analyzing quantization formats by reducing the problem to Fisher-information-weighted squared quantization error, enabling the application of classical quantization theory to neural network weight compression. This is a significant, principled contribution that enables systematic format design rather than ad-hoc heuristics.\n- The power of this theoretical framework is demonstrated by the authors' ability to directly leverage the rich domain of classical quantization theory to derive concrete technical results: cube root density quantizers for block-scaled Normal, Laplace, and Student-t distributions; the signmax scaling scheme; and a variable bit allocation scheme based on Fisher information. This connection to established theory provides both rigor and a pathway for future advances.\n- The variable-length encoding insight provides a unifying explanation for why seemingly disparate techniques (block scaling, sparse outliers, compression) succeed. This is valuable both conceptually---explaining _why_ current methods work---and practically---suggesting that future format designs should explicitly consider variable-length encoding mechanisms.\n- The experimental evaluation is comprehensive and rigorous, covering 11 models across 4 families, multiple formats, both direct-cast and QAT settings, and extensive ablations (block size, scale format, symmetric/asymmetric variants). Validation on synthetic data before real models strengthens confidence in the approach.\n- The paper is exceptionally well-written with clear progression from problem formulation to theory to empirical validation. Figures effectively communicate key insights and extensive appendices provide implementation details without cluttering the main narrative.\n- Quantization is critical for sustainable model training and inference. This paper makes important contributions to our theoretical understanding of quantization---a key step toward developing better methods and providing principled guidance for practitioners."}, "weaknesses": {"value": "- The authors only empirically investigate transformer LLMs, and even among these, Gemma models exhibit behavior that deviates from the theoretical predictions. This raises concerns about the generality of the framework. If discrepancies arise within transformer LLMs alone, it is unclear how well the insights would extend to other architectures such as CNNs, GNNs, or state-space models.\n- The theoretical framework relies on three approximations: second-order Taylor expansion of KL divergence, diagonal Fisher approximation, and constant-per-tensor Fisher. The authors provide extensive empirical validations of their assumptions (Figures 10-12) across different models, but it remains unclear whether these approximations hold more generally outside the specific experimental conditions tested.\n- As the authors note, even if the cube root density quantizers are theoretically optimal, the practical utility is limited by optimized implementations and hardware support. As the paper's focus is on theoretical insights rather than hardware efficiency, this is not a critical flaw, but it does limit immediate applicability."}, "questions": {"value": "The following questions are intended to help better understand the scope of the work and to think about future directions. These are challenging topics, and a lack of definitive answers is perfectly fine and will not be held against the work.\n\n1. Can you provide more insight into why Gemma models show different behavior compared to Llama, Qwen, and Phi families? What additional analysis or probing have you conducted to understand the source of this discrepancy?\n    - Gemma 3 is architecturally distinct from the other models evaluated---it uses a 5:1 local-to-global attention layer split (with different RoPE base frequencies for local and global attention layers) whereas the other models use standard global attention throughout. Do you believe these architectural differences contribute to the observed discrepancies?\n    - What do you think the implications of this discrepancy are for the generality of your theoretical framework?\n2. The work focuses exclusively on transformer LLMs. How do you anticipate the framework and its underlying assumptions would hold up for fundamentally different architectures, such as CNNs, state-space models, or GNNs? Are there specific architectural features (e.g., different weight distributions, inductive biases) that you believe would make the framework more or less applicable?\n3. Can you characterize the regime where your approximations are valid? For instance, under what conditions (model architectures, weight distributions, quantization bit-widths) can practitioners expect any of the three key approximations to break down?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iYeWWL6A8D", "forum": "pYW3WeOwHV", "replyto": "pYW3WeOwHV", "signatures": ["ICLR.cc/2026/Conference/Submission21333/Reviewer_HQnU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21333/Reviewer_HQnU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986938111, "cdate": 1761986938111, "tmdate": 1762941701909, "mdate": 1762941701909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a theoretical framework for designing quantization formats by minimizing the KL divergence between quantized and reference models under a memory constraint. By connecting modern quantization with classical rate–distortion theory, the authors show that efficient schemes perform well because they implicitly use variable length encoding. They derive optimal elementwise quantizers for common distributions, introduce new scaling methods such as RMS, absmax, and signmax, and present a Fisher information based rule for allocating bits across tensors. Experiments on large language models including LLaMA 3, Qwen 2.5, Gemma 3, and Phi 4 show that formats exploiting variable length encoding through block scaling, sparse outlier storage, or compression consistently outperform fixed length ones. The study provides a principled explanation for the effectiveness of formats such as NF4 and SF4 and identifies uniform quantization with lossless compression as the theoretical optimum."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a solid theoretical perspective by linking neural network quantization with classical information theory, offering useful insights into why certain formats perform well. It introduces new scaling schemes and a Fisher information based bit allocation rule that appear to improve efficiency across model tensors. Experiments on several large language models support the proposed framework and suggest its potential practical value."}, "weaknesses": {"value": "I have the following concerns about the paper:\n\n1. Equations (1) and (2) require stronger justification: Minimizing the KL divergence does not necessarily guarantee that the model’s accuracy will be preserved, and the validity of Equation (2) needs a clearer theoretical explanation.\n\n2. Additional background is needed: to help readers follow the technical development. For instance, the sections around lines 143–146 and 153–157 would benefit from more context and introductory material.\n\n3. Experimental evaluation is insufficient: Given the extensive prior work in this area, it is important to include comparisons with existing quantization methods such as SmoothQuant and QuaRot to better demonstrate the advantages of the proposed approach.\n\n4. This work has limited focus on practical hardware efficiency, as the proposed non-linear quantization formats may be difficult to implement or accelerate on existing hardware."}, "questions": {"value": "How does minimizing KL divergence ensure preservation of task-level accuracy, and could the authors provide theoretical or empirical evidence linking KL divergence to model performance?\n\nWhat assumptions are required for Equation (2) to hold, and how sensitive are the results to violations of these assumptions?\n\nCan the authors expand the background discussion to better contextualize the derivations around lines 143–157?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bfemHF8dDh", "forum": "pYW3WeOwHV", "replyto": "pYW3WeOwHV", "signatures": ["ICLR.cc/2026/Conference/Submission21333/Reviewer_ZtPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21333/Reviewer_ZtPY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318432408, "cdate": 1762318432408, "tmdate": 1762941701584, "mdate": 1762941701584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}