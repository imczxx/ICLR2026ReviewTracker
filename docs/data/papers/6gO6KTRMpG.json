{"id": "6gO6KTRMpG", "number": 14916, "cdate": 1758245490171, "mdate": 1759897341617, "content": {"title": "Language Models Use Lookbacks to Track Beliefs", "abstract": "How do language models (LMs) represent characters’ beliefs, especially when those beliefs may differ from reality? This question lies at the heart of understanding the Theory of Mind (ToM) capabilities of LMs. We analyze LMs' ability to reason about characters’ beliefs using causal mediation and abstraction. We construct a dataset, CausalToM, consisting of simple stories where two characters independently change the state of two objects, potentially unaware of each other's actions. Our investigation uncovered a pervasive algorithmic pattern that we call a lookback mechanism, which enables the LM to recall important information when it becomes necessary. The LM binds each character-object-state triple together by co-locating their reference information, represented as Ordering IDs (OIs), in low-rank subspaces of the state token's residual stream. When asked about a character's beliefs regarding the state of an object, the binding lookback retrieves the correct state OI and then the answer lookback retrieves the corresponding state token. When we introduce text specifying that one character is (not) visible to the other, we find that the LM first generates a visibility ID encoding the relation between the observing and the observed character OIs. In a visibility lookback, this ID is used to retrieve information about the observed character and update the observing character's beliefs. Our work provides insights into belief tracking mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.", "tldr": "We study how language models represent and track characters’ beliefs in a story.", "keywords": ["Mechanistic Interpretability", "Belief Tracking", "Theory of Mind"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c149c4776b449fed8f298106dee29cb3cee31420.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "n/a - see ethics review"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n/a - see ethics review"}, "weaknesses": {"value": "n/a - see ethics review"}, "questions": {"value": "n/a - see ethics review"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I no longer think it is ethical to work on Theory of Mind in academia, and I think we should be actively discouraging development of these methods. I know that's a strong statement; please bear with me for a minute. I wrote one of the papers referenced heavily works like this, and I have since found out that my work has since been used for unethical applications including persuasion, marketing, and political advertising, in academia and in industry. Yes there are upsides to Theory of Mind – that's why I and many others started working on it in the first place. Better affective technologies, agents that can anticipate your needs without you asking, therapeutic agents, education agents...etc. But I think while these are indeed upsides, we can achieve these goals in other ways that leave the user's agency and mind uninvaded by AI's – e.g., by creating tools that users can have an easier time adapting to and learning, rather than tools that anticipate and adapt to the user. Essentially, I think the upsides are limited and clever solutions could get to the same ends in less invasive ways. And the downsides are basically infinite. If we're successful in building agents that really can understand what we're thinking and feeling without saying it, those agents will be used by big companies and people in power to successfully simulate our thinking and manipulate us into doing what they want. This isn't conjecture or conspiracy theory; each major tech company is actively investigating AI persuasion and marketing for exactly this reason, as are political campaigns and governments throughout the world. I would know, because many of them have reached out to me to help them implement their methods. As people, we should be scared by this.\n\nThere's a counter argument of course, which is that this is a cat and mouse game like security, and that academia shouldn't attempt to restrict research into these methods: it's best to have this all out in the open so that people can research defenses. While it's possible that defenses could be created against this type of technology, I think that is a dangerous and unproven hypothesis to rely on. It may be the case that this is a setting in which offense is much easier than defense, especially because unlike in computer security, the defender is ultimately a human, not an algorithm. The best and most secure defense would be to train humans to think critically about these types of attacks, and that is a very hard thing to do, especially as the attacks get more and more sophisticated and human education systems become increasingly politicized and unequal. I think it is much better to keep this pandora's box closed for as long as possible, and to work on functional uses of AI that have fewer ethical downsides. \n\nI would encourage the ethics committee to think carefully about this, discuss among themselves (and maybe reach out to me if you want to chat more), and do as much as possible to prevent works like this from being published in top conferences (which increases the visibility and incentive to work on this subject).  I would also encourage the authors to think carefully about whether they want to contribute to this academic legacy. I worked on this area because I believed in the upsides. I have since regretted my participation in this field and hope it does need lead to bad outcomes; if some day superhuman ToM capabilities come to pass and our society is worse off for it, it will be a burden on all that worked on building it."}, "rating": {"value": 0}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "a41iwax6n0", "forum": "6gO6KTRMpG", "replyto": "6gO6KTRMpG", "signatures": ["ICLR.cc/2026/Conference/Submission14916/Reviewer_Jrau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14916/Reviewer_Jrau"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761324790355, "cdate": 1761324790355, "tmdate": 1762925260495, "mdate": 1762925260495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims evidence for a specific mechanism, the \"lookback\", that allows a transformer architecture to dereference information about an entity in a sequence of tokens that presents a Theory of Mind problem, so as to correctly answer queries about what the entity knows. The paper's central claim is that the encoding of such information in the residual stream is based on the relative ordering of tokens corresponding to entities in a set of relational statements (first character vs. second character, first object vs. second object), not on structures that encode information as attributes of the entity's identity. The authors present a benchmark dataset (CausalToM) to analyze a model's ability to simulate ToM, and show through causal interventions in the layer-by-layer evolution of the residual stream that ordinal position (encoded as \"Ordering IDs\"), as opposed to identity, is how the LM manages information to correctly answer ToM queries."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The paper addresses an extremely challenging and important problem in understanding the internal mechanisms by which language models perform Theory of Mind reasoning. \n\nThe methodology used to address this question is very clearly laid out, and to this reviewer's mind well motivated. The paper provides clear and useful graphical presentations of the mechanism proposed and the results of the interchange interventions in both the no-visibility and visibility cases. The contribution of a structured dataset of simple stories to provide a way to effectively elicit interpretable responses to confirm or disconfirm the Ordering IDs hypothesis is also an important contribution. \n\nThe findings demonstrate consistency across models in a given model family (Meta Llama) at multiple sizes (70B and 405B), and preliminary evidence suggests the mechanism generalizes to more naturalistic scenarios (i.e., the BigToM benchmark), strengthening confidence in the robustness of the identified patterns."}, "weaknesses": {"value": "The paper would be significantly strengthened by addressing the following issues related to soundness and presentation. This reviewer sincerely hopes that these can be satisfactorily addressed in the rebuttal phase. \n\n1) The paper's central claim that models use Ordering IDs rather than identity-based or semantic representations is not adequately distinguished from plausible alternatives. There is only the briefest mention of prior work, and it assumes a great deal of familiarity from the reader with what appears to be a very specific body of work. The paper does not explain how the Ordering ID hypothesis was developed, what alternatives were considered, or whether there was exploratory analysis not reported. This makes it very difficult to assess the proposed mechanism in the context of work in mechanistic interpretability to date. \n\n2) The paper lacks crucial information about computational requirements, experimental iteration (how many analyses were attempted before arriving at reported results), and robustness checks (sensitivity to hyperparameters, sample selection, random seeds). Code availability is not mentioned. \n\n3) The paper uses specialized mechanistic interpretability terminology (\"residual stream,\" \"QK-circuit,\" \"OV-circuit\") without adequate definition, assuming familiarity that may not be universal even among the ICLR community. While core concepts like interchange interventions are explained well, a brief background section defining key architectural concepts would significantly improve accessibility and aid reproducibility."}, "questions": {"value": "Does reducing ToM to positional bookkeeping (lookup mechanism + Ordering IDs) suggest sophisticated behavioral mimicry (à la Searle's Chinese Room thought experiment) rather than understanding or intentionality? What additional evidence would demonstrate conceptual understanding beyond structural pattern extraction? Are the Ordering IDs grounded in meaningful semantic relations about information access and belief formation, or are they arbitrary indices that correlate with correct answers in this constrained task structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qa4Wn0ac3h", "forum": "6gO6KTRMpG", "replyto": "6gO6KTRMpG", "signatures": ["ICLR.cc/2026/Conference/Submission14916/Reviewer_zFrv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14916/Reviewer_zFrv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785877159, "cdate": 1761785877159, "tmdate": 1762925259959, "mdate": 1762925259959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the internal mechanisms by which LLMs track characters' beliefs in ToM tasks.  The authors construct CausalToM, employ causal mediation analysis and causal abstraction to identify systematic computational patterns. Three specific lookback mechanisms are identified: (1) binding lookback that links character-object-state triples via ordering IDs, (2) answer lookback that retrieves state token values, and (3) visibility lookback that updates beliefs based on character observability. The mechanisms are validated through interchange intervention experiments on Llama-3-70B-Instruct and Llama-3.1-405B-Instruct models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Unlike previous works in the Theory of Mind (ToM) domain, such as prompt-based (Think twice, TimeToM), tool-based (Social world model), or model-based approaches (Bayesian framework), this paper analyzes the model’s belief reasoning ability from a novel and interpretable perspective. In ToM research, there has long been debate over whether models’ ToM abilities are truly robust, and whether a correct answer to a ToM question genuinely reflects capabiltiy level. Analyzing this issue from the viewpoint of interpretability offers a promising path toward resolving this controversy. The paper presents excellent visualizations and provides a clear description of the research background."}, "weaknesses": {"value": "The data pattern of CausalToM mentioned in the paper is quite simple.\n\nTheory of Mind (ToM) is a broad framework encompassing various dimensions of mental states, and its scenarios are often diverse and complex. The interpretability analysis in this paper is applied only to a narrow data scope (simple story settings and the belief dimension). When the data scenarios become more complex (e.g., longer narratives or richer social contexts), can this method still maintain good scalability and generalization?\n\nMoreover, the interpretability analysis is conducted only on the LLaMA series models. Will these observed phenomena also appear in other model families?"}, "questions": {"value": "See Weakness.\n\nThe models used by the authors are 70B and 405B. Would the same phenomena described in the paper also appear if the model size were around 7–32B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TeKTrJjzyF", "forum": "6gO6KTRMpG", "replyto": "6gO6KTRMpG", "signatures": ["ICLR.cc/2026/Conference/Submission14916/Reviewer_AD4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14916/Reviewer_AD4K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912524390, "cdate": 1761912524390, "tmdate": 1762925259368, "mdate": 1762925259368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks a concrete mechanistic question pertaining to ToM: how transformers store, update, and retrieve characters and their states. The dataset is CausalToM, a toy two‑sentence story set with two characters, two objects (containers), and two object states (contents). Each example concludes with a question, such as “What does Bob believe the bottle contains?”, accompanied by optional visibility statements that specify who can observe whom. The authors analyze Llama‑3‑70B‑Instruct using interchange interventions on residual activations (i.e., patching counterfactual activations) to observe how the model’s behavior changes.\n\nThe central finding is a pervasive lookback mechanism. The model “writes” tags (my terminology; referred to as OIs in the paper) for the “first/second” character, object, and state into the residual stream. Further, state tags are bound to the appropriate character/object tags at the state token, and the model then “looks back” from the answer position to retrieve (i) the right state tag and then (ii) the actual state token via attention. The lookback is localized layer-wise (e.g., tags form approximately layers 20–34; binding occurs at the state token, approximately layers 33–38). Figures 1, 3, 4, and 7 illustrate the pointer/address/payload flow and the three lookbacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the results are surprising and insightful. The analysis is not hand-wavy.\n\n* Strong causal methodology. The authors use interchange interventions (activation patching) with carefully matched counterfactual stories to manipulate specific internal variables and measure IIA layer by layer. For example, patching the final “Answer:” token at mid layers redirects the answer pointer (layers 34–52), whereas patching late layers swaps the answer payload (at layers> 56).\n\n\n\n* Careful dataset design for causal analysis. CausalToM is deliberately simple (two characters/objects/states), so counterfactuals differ in only one factor at a time."}, "weaknesses": {"value": "1. Analysis restricted to successful cases. All mechanistic experiments are run on 80 correctly answered examples. This risks selection bias: we only study the circuit when it worked. What happens for incorrect cases?\n\n\n2. Scaling beyond “first/second” is unclear. tags/OIs encode first vs. second character/object/state, which is perfect for this dataset, but what happens as you scale up?\n\nA small toy study (e.g., 3+ entities per type) would help address both weaknesses by revealing failure modes and testing whether the mechanism extends beyond binary order."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9xF0G4jYUG", "forum": "6gO6KTRMpG", "replyto": "6gO6KTRMpG", "signatures": ["ICLR.cc/2026/Conference/Submission14916/Reviewer_dpdh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14916/Reviewer_dpdh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143559846, "cdate": 1762143559846, "tmdate": 1762925258802, "mdate": 1762925258802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}