{"id": "NHP2Y8IVMU", "number": 24082, "cdate": 1758352479786, "mdate": 1759896782550, "content": {"title": "Exploring Interpretability for Visual Prompt Tuning with Cross-layer Concepts", "abstract": "Visual prompt tuning offers significant advantages for adapting pre-trained visual foundation models to specific tasks. However, current research provides limited insight into the interpretability of this approach, which is essential for enhancing AI reliability and enabling AI-driven knowledge discovery. In this paper, rather than learning abstract prompt embeddings, we propose the first framework, named Interpretable Visual Prompt Tuning (IVPT), to explore interpretability for visual prompts by introducing cross-layer concept prototypes. Specifically, visual prompts are linked to human-understandable semantic concepts, represented as a set of category-agnostic prototypes, each corresponding to a specific region of the image. IVPT then aggregates features from these regions to generate interpretable prompts for multiple network layers, allowing the explanation of visual prompts at different network depths and semantic granularities. Comprehensive qualitative and quantitative evaluations on fine-grained classification benchmarks show its superior interpretability and performance over visual prompt tuning methods and existing interpretable methods.", "tldr": "", "keywords": ["prompt tuning", "explainable AI", "knowledge discovery", "prototype learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91530b7bf2a9a38b7f1c30c88c6b37b0c9127dd2.pdf", "supplementary_material": "/attachment/1068ce8f5229a05f35c590ffbaf6e878d9a4aba2.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a novel, interpretable approach to visual prompt tuning. Here, interpretable tokens are appended to each ViT layer's computation, thereby extracting relevant information while keeping the image encoder frozen. The interpretability comes from the fact that the learnable tokens are not just embeddings, but they can be interpreted as localized prototypical parts, which give insights into which parts of the image are being focused on for each prompt token."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work manages to nicely integrate interpretability methodology into visual prompt tuning(VPT). Although related to prototype-based literature, it is non-trivial how to integrate it into VPT. The method is predominantly presented in a clear manner. Figures support the textual understanding.\nResults look promising in the aspects accuracy as well as interpretability, showcasing that the interpretability framework does not bottleneck the predictive performance on CUB. Also, the results cover various aspects of interpretability, ranging from quantitative metrics to visualizations and human user studies. I appreciate that clean code is provided."}, "weaknesses": {"value": "* (i) My main concern is that each across datasets, the axes of evaluations are not the same. That is, Accuracy is only shown for CUB, and visualizations are missing for PartImageNet and PASCAL-Part. This gives the impression that for the left-out evaluations, the results were omitted due to being poor. This is concerning for the performance aspect, as CUB is a quite simple dataset and if this method is unable to generalize to other more complex datasets, it is problematic. Note that I would expect a performance degradation due to the interpretability adjustments, so the goal for IVPT should just be to not lose out too much. In the current state of the manuscript, I have to assume that IVPT is unable to reach a performance similar to baselines in all datasets apart from CUB.\n\n* (ii) An additional major concern is the faithful interpretability for more complex datasets. For CUB, Gleason-2019, Stanford Cars, FGVCAircraft datasets, the subparts are similar across samples, thereby making them interpretable. Additionally, the parts are also what is important for classification. However, I could envision that for more complex datasets (e.g. ImageNet and Pascal), the prototypes are becoming less interpretable, as they might be used to process information in ways that are not directly understandable when looking at the activation map. That is, without explicitly enforcing it, the visualization in image-space might not be anymore what is going on in the computations. \n\n* In line with the previous point, there might be considerable leakage, as the image regions are processed via weighted average and MLP such that the actual attention computations could deviate greatly from what is visualized. \n\n* I think the separation of $n$ learnable prompts and $m$ concept prototypes is sometimes unclear. E.g. line 128-129 and formula 3 use similar notation but different lengths, which was confusing at first."}, "questions": {"value": "* Can the authors provide predictive performance compared to baselines on other datasets to counter concern (i)?\n\n* Can the authors provide qualitative results for PartImageNet and PASCAL-Part to counter concern (ii)?\n\n* If I understand correctly, previously in VPT, the prompt tokens were learnable embeddings that were the same for different samples. Now, the prompt tokens are weighted average of the input image, thereby different per-image. How does that differ conceptually from the previous global prompt tokens? What are the benefits and potential downsides?\n\n* The concept regions are computed patch-wise. How did the authors obtain non-rectangular prompt visualizations, such as in Figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hxXxLgNwWU", "forum": "NHP2Y8IVMU", "replyto": "NHP2Y8IVMU", "signatures": ["ICLR.cc/2026/Conference/Submission24082/Reviewer_RFsS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24082/Reviewer_RFsS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760962001632, "cdate": 1760962001632, "tmdate": 1762942928156, "mdate": 1762942928156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to propose a fine-tuning paradigm that learns interpretable visual prompt during fine-tuning via cross-layer concept prototypes. A concept region discovery module is proposed to learn prototypes with semantic meaning, and an intro-region feature aggregation module is proposed to group the features belonging to certain regions. Experimental results in CUB demonstrate improved performance compared to visual prompt tuning and show superior interpretability scores for the learned visual prompts compared to part-prototype networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work investigates the interpretability of visual prompts during fine-tuning, which is a less explored area.\n- The method design in making the visual prompts more interpretable is reasonable.\n- Experimental results show improved performance and interpretability compared to 2 types of baselines."}, "weaknesses": {"value": "*Motivation*\n\nWhy at all should the “visual prompt” be interpretable is not well justified. Since they are parameters learned to adapt the model from one domain to another domain, if they are possible to be interpretable, I would expect them to explain the domain shift instead of part-prototypes. Part-prototypes could explain a decision making process, but are not that meaningful in a fine-tuning process? Especially when the sum of the contribution of part-prototypes do not fully explain the decision making process in the proposed framework (see next concern).\n\n\n*Interpretability*\n\nThe final classification process in part-prototype networks can be fully interpreted by the contribution of each individual part-prototype. However, the prompts in this work only contribute to part of the classification logits, most tokens from frozen part of the network remain uninterpretable. Do the framework adopt a classification token in the final prediction or the average of all token representations? If using a classification token, the information are aggregated from both interpretable visual prompt tokens and rest tokens via self-attention, making the contribution of the visual prompt unclear. Such a mechanism also makes it less meaningful to make visual prompts interpretable.\n\n*Evaluation*\n\nHow are the areas that the evaluated visual prompts correspond to calculated and evaluated against the annotations in CUB?\n\n*Missing details/analysis*\n\nWhat’s the difference between the interpretability scores of prompts in different layers? How many prompts are used in each layer? What’s the influence of number of prompts on their interpretability?"}, "questions": {"value": "1.\tHow do you obtain the performances of ProtopNet and its following works based on ViT architectures in Table 1? How are they implemented? These original works are not designed for ViT and do not report relevant results.\n\n2.\tCan the learned visual prompts explain anything related to visual prompt **tuning**?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ehmxXZGRvK", "forum": "NHP2Y8IVMU", "replyto": "NHP2Y8IVMU", "signatures": ["ICLR.cc/2026/Conference/Submission24082/Reviewer_PdEH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24082/Reviewer_PdEH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915697715, "cdate": 1761915697715, "tmdate": 1762942927690, "mdate": 1762942927690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IVPT, an interpretable visual prompt tuning framework that aligns prompts with category-agnostic concept prototypes across network layers. It introduces concept-region discovery, intra-region feature aggregation, and cross-layer concept fusion to make prompts human-understandable while preserving accuracy. Experiments show IVPT achieves better interpretability and comparable performance to standard VPT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It introduces a clear, concept-grounded approach that links visual prompts to human-understandable concepts across layers.\n2. It demonstrates improved interpretability metrics and visualization quality without sacrificing classification accuracy.\n3. The method is model-agnostic and works across different ViT backbones and domains, showing good generalization."}, "weaknesses": {"value": "1. Experiments focus mainly on fine-grained classification. Broader tasks (e.g., detection, segmentation) are not explored.\n2. The method relies on well-learned concept prototypes, which may be sensitive to initialization or domain shift.\n3. The multi-layer prototype alignment and multiple loss terms increase implementation and computational complexity."}, "questions": {"value": "How stable are the learned concept prototypes when transferring to new domains or unseen categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DpwkJ8f024", "forum": "NHP2Y8IVMU", "replyto": "NHP2Y8IVMU", "signatures": ["ICLR.cc/2026/Conference/Submission24082/Reviewer_ANRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24082/Reviewer_ANRv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152429330, "cdate": 1762152429330, "tmdate": 1762942927337, "mdate": 1762942927337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the interpretability of vision–language models (VLMs) through a task-specific concept alignment framework. The authors aim to understand how visual and linguistic concepts align across different tasks, providing both qualitative and quantitative analyses to uncover internal reasoning processes of VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic is highly relevant and well-motivated, addressing the growing need to make large-scale vision–language models more interpretable and transparent.\n\nThe paper provides comprehensive experimental analysis, including multiple datasets and evaluation settings, with clear presentation of alignment patterns."}, "weaknesses": {"value": "While the analysis is useful, the methodological contribution is limited compared to prior interpretability frameworks. The paper primarily extends known alignment techniques rather than introducing a fundamentally new interpretability paradigm. The related work section lists many recent studies but lacks an in-depth synthesis or a critical comparison. A more detailed discussion of existing SOTA interpretability methods and their limitations would help clarify what specific gap this work fills. Because the related work discussion is broad but not deep, the method section does not clearly delineate the paper’s unique conceptual or technical contribution relative to existing approaches.\n\nWhile the experiments are well executed, the paper would benefit from showing a potential use case, for example, how this concept alignment framework could assist in model debugging, bias detection, or downstream task understanding."}, "questions": {"value": "Can you expand the related work section to more deeply analyze existing SOTA interpretability methods and highlight the specific gap your work addresses?\n\nHave you considered evaluating your approach in a concrete application setting (e.g., identifying bias, failure analysis, or improving model transparency for users)?\n\nHow generalizable is your approach across different VLM architectures? Are there differences in alignment quality depending on the model type?\n\nCould you include an example of how concept alignment results might be used in a downstream interpretability workflow or decision-support scenario to make the method’s impact more tangible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c8lVS6WckX", "forum": "NHP2Y8IVMU", "replyto": "NHP2Y8IVMU", "signatures": ["ICLR.cc/2026/Conference/Submission24082/Reviewer_9y8U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24082/Reviewer_9y8U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183512462, "cdate": 1762183512462, "tmdate": 1762942927111, "mdate": 1762942927111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}