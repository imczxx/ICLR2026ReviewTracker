{"id": "S9zfLNEEFN", "number": 10436, "cdate": 1758171394884, "mdate": 1759897650782, "content": {"title": "MedSCAMA: Medical SCale-Aware Multi-Agent Framework for Medical Image Retrieval and Retrieval-Augmented Generation", "abstract": "Medical image retrieval and retrieval-augmented generation require representations that capture both visual similarity and clinically meaningful semantics across multiple levels of granularity. However, existing image encoders may miss pixel-to-organ-to-context cues, and report encoders may flatten hierarchical findings with global summaries, weakening vision-language alignment and alignment between query and targets in retrieval tasks. To address these gaps, we propose MedSCAMA (Medical SCale-Aware Multi-Agent Framework), a collaborative multi-agent framework designed to enable multi-scale feature modeling and retrieval-augmented reasoning. Specifically, ScaFormer encodes multi-scale visual features, the RU and PC Agents provide hierarchical text and similarity cues, and the QA Agent performs adaptive retrieval for evidence-grounded reasoning. Experiments across multiple medical imaging benchmarks demonstrate that MedSCAMA substantially enhances both retrieval quality and diagnostic reasoning, offering more accurate, interpretable, and clinically relevant results than existing approaches. This multi-scale, multi-agent design provides a principled foundation for integrating vision, language, and reasoning in medical AI systems.", "tldr": "", "keywords": ["Medical Image Retrieval", "Multi-Scale Representation Learning", "Scale-Aware Mixture-of-Experts", "Soft Contrastive Learning", "Preference Optimization", "Adaptive Retrieval", "Question Answering", "Vision-Language Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6de0fea49dbeac49544ea330b36ec5510aa45444.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MedSCAMA, a scale-aware multi-agent framework for medical image retrieval and retrieval-augmented generation. MedSCAMA introduces a ScaFormer for multi-scale visual encoding with a sliding-window Mixture-of-Experts, a Report Understanding (RU) Agent for hierarchical report decomposition, a Pairwise Comparison (PC) Agent trained with soft contrastive loss and Direct Preference Optimization, and a Question Answering (QA) Agent for adaptive evidence retrieval and reasoning. Experiments on medical retrieval and QA benchmarks demonstrate improved performance over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1.  The idea of ScaFormer is sound. Introducing a scale-aware Mixture-of-Experts design would enable multi-resolution feature extraction, a crucial aspect in medical imaging.\n\n2. Experimental results on medical retrieval and QA tasks show consistent improvements over strong baseline methods."}, "weaknesses": {"value": "1. Over-complex designs. Given the complexity of model designs, it is suggested to improve the writing for better readability.\n \n2. Figure quality. It is suggested to enhance the quality of Figure 2 for better clarity.\n\nOverall, the proposed framework seems effective. However, it is strongly suggested to improve the writing for better readability."}, "questions": {"value": "please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FQlMnfYC0i", "forum": "S9zfLNEEFN", "replyto": "S9zfLNEEFN", "signatures": ["ICLR.cc/2026/Conference/Submission10436/Reviewer_fL5n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10436/Reviewer_fL5n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510415901, "cdate": 1761510415901, "tmdate": 1762921738937, "mdate": 1762921738937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MedSCAMA (Medical SCale-Aware Multi-Agent Framework), a large and intricate architecture for medical image retrieval and retrieval-augmented generation. The framework integrates multiple modules: a ScaFormer visual encoder based on a sliding-window Mixture-of-Experts (ScaMoE) to model multi-scale features; a Report Understanding (RU) Agent that decomposes radiology reports into hierarchical textual segments; a Pairwise Comparison (PC) Agent trained via soft contrastive learning and Direct Preference Optimization (DPO) to produce continuous similarity signals; and a Question Answering (QA) Agent for adaptive evidence retrieval and diagnostic reasoning. Experiments on MIMIC-IR, CTRATE-IR, IU-Xray, and MIMIC-CXR datasets show performance improvements over baselines such as MedCLIP, RADIR, and MMed-RAG in both retrieval and generation tasks. The framework achieves consistent but modest gains across metrics like Recall@5 and AUROC, with additional analysis on expert activation and adaptive retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is comprehensive in scope and technically ambitious. It recognizes a real challenge in medical vision-language systems—the need to reconcile multi-scale image structures and hierarchical text semantics—and attempts to address it through a unified multi-agent design. The sliding-window ScaMoE mechanism is an interesting adaptation of Mixture-of-Experts for hierarchical vision modeling, and the alternating optimization between contrastive learning and preference-based tuning is well formulated. The experiments are extensive, covering multiple modalities and benchmarks, and the results are consistently positive, indicating that the proposed system is functional and well-engineered. The visual analysis of expert activations adds interpretability, and the ablation studies confirm that each module contributes measurable improvements. The writing is generally clear and the system design is coherent."}, "weaknesses": {"value": "Despite its scale and technical sophistication, the paper’s novelty and scientific depth are limited. The so-called multi-agent framework is more a modular pipeline than a true collaborative or interactive system. The agents operate sequentially—text decomposition, similarity scoring, and retrieval—without any emergent cooperative behavior or feedback mechanism. As a result, the “multi-agent” terminology feels overstated. The work primarily combines existing methods (Q-Former, Mixture-of-Experts, contrastive learning, and DPO fine-tuning) into a complex engineering stack rather than introducing new principles in representation learning.\n\nThe dependence on GPT-5 for generating supervision signals is a major concern. Both the RU and PC agents rely heavily on synthetic labels—hierarchical report segmentation and preference triplets—without human validation or error analysis. This dependence introduces potential bias, noise, and lack of reproducibility, especially since GPT outputs can vary with prompt phrasing or randomness. The paper does not provide any quantitative assessment of annotation quality, nor does it discuss robustness to annotation errors.\n\nThe “scale-aware” concept also lacks depth. The number of scales (S=4) and window size (w=3) are chosen arbitrarily without theoretical justification or empirical sensitivity analysis. It is unclear how these scales correspond to anatomical granularity or clinical semantics. The experiments show numerical gains, but there is little evidence that these gains stem from scale reasoning rather than simply adding more parameters. The claim that ScaMoE learns scale-specific semantics is supported only by one plot of expert activations, which is insufficient to substantiate the claim.\n\nThe experimental results, though positive, are incremental. Improvements such as Recall@5 from 5.18% to 8.45% or AUROC from 87.13% to 91.33% are not large enough to justify the system’s complexity, given that it combines several heavy backbones (Swin-Base, Vicuna-7B, LLaVA-Med). There is no discussion of runtime, memory consumption, or inference latency, which are crucial for assessing real-world feasibility in clinical environments. The framework appears computationally expensive and over-engineered.\n\nConceptually, the paper blurs the line between system integration and research contribution. It provides no new learning objective, no theoretical analysis, and no clear insight into why multi-scale multi-agent interaction improves representation alignment. The writing is polished but leans toward descriptive exposition rather than analytical reasoning. In summary, the work is a strong system paper but lacks the depth and originality expected for ICLR."}, "questions": {"value": "How are the different agents trained and coordinated? Are they optimized jointly or separately, and how do their gradients interact if trained end-to-end?\n\nHow is “collaboration” between agents concretely defined or measured? Could the same pipeline be replicated with a single transformer model using scale tokens rather than distinct agents?\n\nHow were GPT-5 annotations evaluated for consistency and correctness? Were any human experts involved in validating hierarchical decompositions or preference labels?\n\nWhat is the computational cost in terms of training time, GPU hours, and model parameters compared to simpler baselines such as RADIR or MedDr?\n\nHow robust is the model to datasets with different reporting styles or imaging modalities? Does performance hold on datasets like RSNA, NIH-CXR, or pathology scans?\n\nCan the authors justify the number of scales and window size with empirical or theoretical reasoning? Is the performance sensitive to these hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iwQ7n7D1Gy", "forum": "S9zfLNEEFN", "replyto": "S9zfLNEEFN", "signatures": ["ICLR.cc/2026/Conference/Submission10436/Reviewer_J5rp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10436/Reviewer_J5rp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893112707, "cdate": 1761893112707, "tmdate": 1762921738569, "mdate": 1762921738569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors create a model that extracts and leverages hierarchical structure from medical images and their accompanying radiology reports. The ScaFormer creates hierarchical features from images using a mixture of experts weighted using a sliding window. A report understanding agent generates hierarchical sections of a radiology report. These respective hierarchies for image and text are joined together using multi-scale feature alignment, which involves a pairwise comparison agent that is trained using alternating contrastive and DPO losses. Finally, a VLLM QA agent is attached to ScaFormer to summon evidence over available databases. The authors conduct ablation studies and multiple task evaluations and find that their model outperforms other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The central idea–the merging of hierarchical features between images and reports–is a good one and is well thought out.\n- The range of strategies implemented by the authors is impressive and coherent, and is well backed up by ablation studies in both the main text and appendix.\n- The usage of a clinically grounded weight for contrastive learning (RU Agent) appears novel.\n- While most of the individual techniques employed are not groundbreaking, I feel this paper is better than the sum of its parts due to the careful construction of a complex model with respect to a well-motivated problem.\n- The authors show great results over all comparable benchmarks, and perform ablation studies (some of which I would recommend including in the main text due to their relevance to readers).\n- Components of the composite model are differentiable in a way that enables end-to-end training."}, "weaknesses": {"value": "- The authors use question/answer pairs for VQA which are not human or clinically validated.\n- While the tasks are standard (Image→Image, etc.), it is not clear how the authors rank relevant items for recall comparison. \n- This approach seems extremely intensive to train/run.\n- The authors do not provide an ablation study on the number of scales used, as well as the number of experts and the sliding window size, making it hard to determine how best to utilize hierarchical scales.\n- The writing is generally rushed and the related works does a so-so job of establishing how the innovations posed by the authors connect to what has been done in the literature. \n- The appendix is lengthy and detailed (which is good), but more should be included in the paper, especially in the analysis section which is quite short.\n- The authors claim that Med-SCAMA enhances diagnostic reasoning. While they show that it can do yes/no VQA, I would not call that reasoning.\n- There is no discussion of potential failure modes for this system, of which there must be.\n- The order of events in Figure 2 Part 4 is confusing; more elaboration needed in caption. Same goes for the arrows in Figure 1 Part 1.\n- Typo: optinal → optimal (Line 172)\n- NDCG@k is never directly defined (Line 357)"}, "questions": {"value": "- What datasets were used for training? This is not directly stated, making it hard to compare performance to baseline models.\n- This work is built on the premise that single-scale encoders conflate features incorrectly. Is there any evidence in prior work that this is the case, or only anecdotal?\n- Related to the idea of expressing the problem, in the Figure 1 caption the authors state that “the image encoder focused..”; what image encoder; is it a hypothetical one?\n- Does this approach require especially large LLMs like GPT-5, or can it work well on smaller open-source models?\n- How was the LLM-based pairwise similarity validated? How can we be sure that this is an effective method? \n- What effect does applying independent cross-attention modules have going from the Q-Former to the ScaFormer?\n- How effective is this for pathologies that exist at multiple scales? Is there any added benefit?\n- How are experts combined using the sliding window approach? This is not sufficiently explained.\n- What is the “ScaFormer agent” (Line 318)?\n- What is MIMIC-IR? The derivation of this dataset is not well-detailed in the manuscript.\n- If the authors address the validation of their LLM components, I would be inclined to improve my score. The same goes for other parts that need clarification."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors state that they use LLMs for handling radiology reports (e.g. GPT 5) and in their ethics statement detail that they adhered to all access and usage restrictions. However, PhysioNet (source of MIMIC-CXR images and reports) has a policy that the reports cannot be shared with any third-party LLM that retains the data–which GPT-5 usually does. Can the authors confirm that they adhered to the policy set forth by PhysioNet? Link to PhysioNet policy: https://physionet.org/news/post/llm-responsible-use."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tvh8hFDWP6", "forum": "S9zfLNEEFN", "replyto": "S9zfLNEEFN", "signatures": ["ICLR.cc/2026/Conference/Submission10436/Reviewer_6n5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10436/Reviewer_6n5C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955523789, "cdate": 1761955523789, "tmdate": 1762921738090, "mdate": 1762921738090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}