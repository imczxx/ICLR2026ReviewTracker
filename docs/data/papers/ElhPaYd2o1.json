{"id": "ElhPaYd2o1", "number": 24220, "cdate": 1758354249110, "mdate": 1759896776112, "content": {"title": "Joint Pixel-Token Compression for Efficient Video-Language Models", "abstract": "Recent Video Multi-Model Language Models(VLLMs) have achieved significant progress in multimodal understanding. However, they have been challenged by high computational cost due to the massive video frames and huge video tokens generated from the video encoders. Conventional video processing often relies on uniform sampling, which together with the large number of tokens generated by visual encoders, leads to substantial redundancy in visual information. To address this issue, we propose a joint pixel-token compression (P-T) strategy to minimize computational burden. Specifically, firstly, in the terms of pixel-level compression, the similarity is evaluated by calculating the pixel-wise differences between consecutive frames. This strategy enables the selection of more semantically informative frames for better video understanding. Secondly, during token-level compression, redundancy in visual information is reduced by measuring the cosine similarity of tokens at corresponding positions between frames. By adopting this strategy, we can eliminate redundant tokens. Our model is a plug-and-play module that can be easily integrated into different baselines. We conduct extensive experiments under both training-free and training settings and achieve significant improvements (Notably, even after discarding 50\\% of the visual tokens, our method yields a 0.9\\% performance gain on the MVBench benchmark with the Qwen2.5-VL model), demonstrating the effectiveness of our approach.", "tldr": "", "keywords": ["video compression", "Video Multi-Model Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9501d1111bc75703fe452b589c856a7a20363f3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the problem of redundant visual information in large video-language models. To achieve this goal, the authors propose two compression methods: pixel-level compression and visual token-level compression. Both methods employ a similar technique in which a similarity metric is used to evaluate the information redundancy between adjacent frames or tokens. Frames or tokens with high similarity scores are pruned. Experiments conducted under both training-free and training-based settings demonstrate the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is straightforward and easy to understand.\n2. The algorithm is clearly explained and well-documented.\n3. The experiments are comprehensive, covering both training-free and training-based settings."}, "weaknesses": {"value": "The main concern lies in the novelty of the proposed approach. The core idea—pruning frames or visual tokens based on similarity scores—has been extensively explored in prior studies on video compression and visual token reduction. While the paper presents a clear and well-executed implementation, it is not immediately evident how the proposed method substantially advances beyond existing techniques in terms of algorithmic innovation or conceptual contribution. A deeper analysis or clearer differentiation from related work would strengthen the paper’s claim of novelty."}, "questions": {"value": "1. What specific types of information are pruned at the pixel level, and what types are removed at the token level? How are these two forms of compression complementary to each other?\n2. In Table 5, the authors examine different similarity metrics. Could other information-theoretic metrics, such as mutual information, also be applied in this context?\n3. Similarity-based pruning methods have been extensively explored in prior research. What are the key differences between the proposed algorithm and existing approaches in the literature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B8dLmeGrit", "forum": "ElhPaYd2o1", "replyto": "ElhPaYd2o1", "signatures": ["ICLR.cc/2026/Conference/Submission24220/Reviewer_BDB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24220/Reviewer_BDB3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710519225, "cdate": 1761710519225, "tmdate": 1762943003660, "mdate": 1762943003660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a joint pixel–token compression framework to reduce the computational cost of Video LLMs. The key idea is to perform pixel-level compression to remove redundant video frames based on inter-frame similarity, followed by token-level compression that prunes redundant visual tokens across frames using cosine similarity. This dual-stage approach targets both temporal and spatial redundancies. The method is plug-and-play and compatible with popular VLLMs like LLaVA-Video and Qwen2.5-VL, requiring minimal or no retraining. Experiments show that the method can discard over 50% of visual tokens while maintaining or even improving accuracy"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Plug-and-play design. The framework can be easily integrated into different VLLMs without architecture modification or retraining.\n- Strong empirical validation. Experiments on multiple benchmarks show consistent performance improvements.\n- Thorough ablation studies. The authors systematically evaluate the effects of compression ratio, similarity measures, and architectural choices, providing insights into robustness and design trade-offs."}, "weaknesses": {"value": "- Limited technical contribution. The proposed pixel-level and token-level token reduction are well studies in previous works [1-6], making this paper limited contribution.\n- Dependence on hyperparameters. Compression thresholds (τ, ρmin, ρmax) may need careful tuning for different datasets, but the paper provides limited discussion on sensitivity or generalization.\n- Limited theoretical insight. The method is primarily heuristic, lacking a theoretical explanation or analysis of why the joint compression preserves semantic fidelity.\n\n\n[1] TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos.\n[2] Video Compression Commander:Plug-and-Play Inference Acceleration for Video Large Language Models. \n[3] HoliTom: Holistic Token Merging for Fast Video Large Language Models.\n[4] LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs.\n[5] Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs.\n[6] AdaTP: Attention-Debiased Token Pruning for Video Large Language Models."}, "questions": {"value": "no."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4m9UnfYwfh", "forum": "ElhPaYd2o1", "replyto": "ElhPaYd2o1", "signatures": ["ICLR.cc/2026/Conference/Submission24220/Reviewer_fGcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24220/Reviewer_fGcp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905552469, "cdate": 1761905552469, "tmdate": 1762943003314, "mdate": 1762943003314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the substantial computational demands of video understanding with vision-language large models (VLLMs), this paper proposes a joint pixel-token compression strategy. It combines pixel-level compression, achieved by selecting keyframes based on inter-frame differences, with token-level compression, implemented by pruning redundant visual tokens according to their semantic similarities. The approach is evaluated across three VLLMs, demonstrating its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and easy to follow.\n* The proposed approach is intuitive and readily adaptable across different types of VLLMs.\n* Exploring joint frame-level and token-level compression represents a promising direction for efficient video understanding in VLLMs."}, "weaknesses": {"value": "* While the idea is intuitive, it is also relatively straightforward, as both frame-level keyframe selection and token-level visual token pruning have been explored in prior work. Given this, a more comprehensive ablation study would be essential to provide deeper insights. Unfortunately, the current experimental setup offers limited analytical value in this regard.\n* The performance gains reported are not consistently substantial. In certain configurations, such as LLaVA-Video on VideoMME with subtitles, the improvement is marginal and may not constitute a statistically or practically meaningful gain.\n* Lack of comparison with the training-free methods developed for image LLMs, such as IG-VLM [1], SF-LLaVA [2], TS-LLaVA [3]. This type of methods usually only uses token compressions with uniformly sampled frames. And experiments on more datasets, e.g. MLVU [4], LongVideoBench [5], TempCompass [6], EgoSchema [7] etc., are also expected to draw a clear conclusion.\n\n[1] Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee. (2024) An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM.\n\n[2] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, Afshin Dehghan (2024) SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models.\n\n[3] Tingyu Qu, Mingxiao Li, Tinne Tuytelaars, Marie-Francine Moens (2024) TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models.\n\n[4] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, Zheng Liu (2024) MLVU: Benchmarking Multi-task Long Video Understanding\n\n[5] Haoning Wu, Dongxu Li, Bei Chen, Junnan Li (2024) LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding\n\n[6] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, Lu Hou (2024) TempCompass: Do Video LLMs Really Understand Videos?\n\n[7] Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik (2023) EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QrzH6zwbse", "forum": "ElhPaYd2o1", "replyto": "ElhPaYd2o1", "signatures": ["ICLR.cc/2026/Conference/Submission24220/Reviewer_eJST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24220/Reviewer_eJST"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973506531, "cdate": 1761973506531, "tmdate": 1762943002886, "mdate": 1762943002886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a compression scheme for the visual tokens in a VLM, to reduce computational complexity while maintaining (or even slightly increasing) performance. The proposed scheme consists of 2 steps: first frames are selected ('pixel-level'), using a simple L1-norm based comparison of frames. Then tokens with high mean similarity to other tokens are pruned ('token-level'), with a dynamic pruning ratio. \nResults are reported for both a training-free as well as a finetuning setup, using the MVBench benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is relatively simple and easy to implement. This also makes it easy to integrate with various VLM ('plug and play').\n2. Both training-free and finetuning settings are tested."}, "weaknesses": {"value": "1. The authors are not the first to show the number of tokens for a VLM can be reduced significantly without significant impact on the accuracy. \nFocusing on the training-free setting, compared to e.g. DyCoke, they improve somewhat, but it's hard to tell whether this is significant. It could also be due to a somewhat better choice of hyperparameters (by luck, or by trying out a few things as shown in the ablation study and picking the best). Even if it is, the take-home message of the paper is, at best, 'with some tweaking of the token selection, we can improve the process a bit\".\n\n2. While the proposed method is relatively simple, the description of the token-level compression seems incomplete and therefore hard to reproduce. For instance, \n- it's unclear whether the similarity between tokens is computed before or after positional embeddings are added. \n- if the token compression works with a sliding window to partition frames into groups (l. 235), every frame serves as anchor frame at some point ? Or do you work with non-overlapping windows ? Then the term 'sliding window' may be confusing.\n- it's unclear how the min/max pruning ratio is enforced, when the threshold tau is predefined (and fixed?).\n- I've no idea how to interpret the notation in eq. 4\n- an algorithm for the token compression, as provided for the pixel compression, might help clarifying the process.\n\n3. Hyperparameters such as group size, min. and max. pruning ratio, similarity threshold, etc. are not specified. It's also not explained how their value has been determined (especially relevant for the training-free setting) . \n\n4. The different experiments are somewhat redundant. It would be more interesting \n- to see a comparison against naive baselines such as randomly dropping frames or tokens;\n- compare against other similar methods such as Deco (https://arxiv.org/abs/2405.20985) or LlaMa-VID (https://llama-vid.github.io/)\n- including other benchmarks with longer videos such as VideoMME, MLVU, LongVideoBench\n\n5. The text is somewhat repetitive, re-iterating on the same points over and over. It could be shortened significantly."}, "questions": {"value": "1. Are positional embeddings added before the similarity between tokens is computed ? \n2. How are the values of the hyperparameters of the method determined ? \n3. Can you include a comparison against state-of-the-art methods that are not training-free ? \n3. Instead of showing on-par accuracies with lower compute/memory budget, it would be more interesting to see that, on longer videos, you can actually improve accuracy by capturing more of the video content than the default setting that samples a too coarse set of frames."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LGpbPp9fZa", "forum": "ElhPaYd2o1", "replyto": "ElhPaYd2o1", "signatures": ["ICLR.cc/2026/Conference/Submission24220/Reviewer_YEaF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24220/Reviewer_YEaF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090950612, "cdate": 1762090950612, "tmdate": 1762943002616, "mdate": 1762943002616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}