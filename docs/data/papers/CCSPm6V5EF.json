{"id": "CCSPm6V5EF", "number": 24064, "cdate": 1758352315287, "mdate": 1759896783547, "content": {"title": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality", "abstract": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient alternative to human evaluation, demonstrating strong performance on well-defined tasks. However, its reliability in open-ended tasks with dynamic environments and complex interactions remains unexplored. To bridge the gap, we introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge performance in web development, with support for both non-interactive evaluation based on static observations and continuous interactive evaluation with a dynamic web environment. WebDevJudge comprises human preference labels over paired web implementations, annotated with structured and query-grounded rubrics to ensure high-quality ground truth. Using this benchmark, we comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic workflows. We systematically investigate the impact of different paradigms and guidance mechanisms. Our experiments reveal a significant gap between LLM judges and human experts. In-depth analysis indicates this gap stems from fundamental model limitations, including failures in recognizing functional equivalence, verifying task feasibility, and mitigating bias. Overall, WebDevJudge presents a significant challenge to LLM-as-a-judge, offering insights to guide future research toward developing more reliable and capable automated evaluators for complicated scenarios.", "tldr": "A meta-evaluation benchmark for assessing LLM-as-a-judge in the context of web development.", "keywords": ["large language models", "evaluation", "LLM-as-a-judge", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ac66163301701a08543df7101502cfe6c776d85.pdf", "supplementary_material": "/attachment/a9f0a841a05f3d4b275ca954d48d24de2794ceab.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces WebDevJudge, a benchmark to evaluate LLM judges in the web development domain. The benchmark consists of paired web implementations with human preference labels. The paper uses this benchmark to evaluate LLMs, MLLMs, and agentic workflows. The findings show a significant performance gap between the best LLM judges and human experts. The paper also identifies failure modes in LLM judges, such as a positional bias and inability to recognize \"functional equivalence\" between implementations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper focuses on an important and understudied problem, that of establishing the reliability of LLM judges in web dev scenarios.\n- The idea of using a query-grounded rubric tree for annotation is a novel and strong methodological contribution for collecting complex annotations. The high inter-annotator agreement (89.7%) validates this rubric-based approach and is especially motivating compared to w/o rubric.\n- The evaluation of existing evaluators is comprehensive, testing LLMs, MLLMs, and agentic workflows across static and interactive paradigms.\n- The error analysis is well done. It identifies precise failures like the inability to recognize \"functional equivalence\" and provides a clear -diagnostic analysis using the WebDevJudge-Unit dataset."}, "weaknesses": {"value": "- The benchmark is fairly small (654 examples), which may limit the generalizability of the strong claims.\n- Some of the claims about the benefits of agentic evaluators might need caveats since the main result is based on a single agent (UI-TARS-1.5). \n- The paper identifies a precision/recall trade-off between static LLMs and interactive agents and suggests an \"ideal evaluator\" would combine them, but doesn’t actually run this experiment."}, "questions": {"value": "1. Do you think the rubric tree used during human evals introduces any undesirable biases that could cause agreement to artificially go up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OWVl3O18ha", "forum": "CCSPm6V5EF", "replyto": "CCSPm6V5EF", "signatures": ["ICLR.cc/2026/Conference/Submission24064/Reviewer_ayxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24064/Reviewer_ayxS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804790141, "cdate": 1761804790141, "tmdate": 1762942920364, "mdate": 1762942920364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces WebDevJudge. A new meta benchmark to evaluate LLM-as-a-judge performance in web development. They show existing LLM judges underperform versus human experts in their setup. The benchmark is not solved. The benchmark supports both static text and real-time interaction setups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A good analysis on the new benchmark that is introduced, in a space where benchmarks (and especially meta benchmarks) are much needed.\n2. Draws conclusions that seem sound, and are useful in both designing solutions as designing other benchmarks."}, "weaknesses": {"value": "1. Details about the agentic setup are lacking. They have some details in the appendix, but no analysis of where the agentic setup fails. We know agentic setups with interaction with GUIs are still not great (and we know that they often are better with e.g. selecting using elements vs coordinates), but more details on how useful the agentic side is for the final conclusion would be good for the paper.\n2. In the appendix I see examples, but none have images. If this is a known limitation, can you highlight it? E.g. some websites are designed around images and will look worse if no images are used.\n3. Could propose (even in a weak-form) potential solutions to the current short-comings."}, "questions": {"value": "1. Can you share the few-shot LLM generation setup? What prompts were used (just example in appendix).\n2. How did you go from 1713 high-quality instances to 654 instances after rubric?\n3. “With explicit instructions to ignore position bias and remain objective”. How is this done? Why don’t we randomly swap position to avoid position bias? And “remain objective”, how does that work? What is the impact? You analyze this later, but what happens if you would do the evaluation with both positions (we can’t do this with humans since they will remember, but we can do this with LLMs).\n4. “Imposing rigid structured metrics might constrain the models’ inherent reasoning processes” — Did you use constrained decoding here? Or is it still free-form output (but with the mentioning of the rubric)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0csRXFGnis", "forum": "CCSPm6V5EF", "replyto": "CCSPm6V5EF", "signatures": ["ICLR.cc/2026/Conference/Submission24064/Reviewer_qgbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24064/Reviewer_qgbg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827974632, "cdate": 1761827974632, "tmdate": 1762942920089, "mdate": 1762942920089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces WebDevJudge, a meta-evaluation benchmark meant to test how (M)LLMs can evaluate web development (both using static evaluators or interactive evaluators such as agents).  The authors create WebDevJudge by collecting and filtering example queries from webdev-arena-preference-10k and creating a hierarchical rubric for evaluating each instance. The rubric is meant to represent an effective evaluation strategy and help the evaluator focus on binary features for making its judgement, the authors show that this method yields stronger annotator agreement between humans. Although these rubric trees aid human annotation, the authors find that it has marginal effects on evaluators. The key results in the paper find that LLM evaluators (even very large ones such as R1) often have rather low agreement with human annotators when evaluating web development (in many settings including agentic evaluators, pair-wise evaluation, etc.)\n\nThe authors continue to analyze the failure cases for these evaluators by looking at how important multiple modalities are for evaluation (finding code to be the most important). The authors look at position biases for pair-wise evaluation, finding it to be prevalent. Additional errors within these evaluators include functional equivalence (knowing when something is close/good enough) and that agentic systems often fail due to their own operational reliability (meaning static evaluators and agentic verifiers both have shortcomings). Overall this is an excellent benchmark and analysis paper highlighting the core limitations within LLM-as-a-judge evaluators is not in how the task is set up nor the methodology for performing the evalation (pairwise or direct, single model or agentic) but lies within the core capabilities of the model. This work highlights future directions for improving LLM evaluators and offers a dataset for researchers to experiment on."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **A challenging benchmark that evaluators can be tested on**, the highest performing evaluator achieves only 66% agreement with humans, indicating a large gap for improvement.\n- **Clear insight into challenges in current evaluators**, the analysis that follows the main results from the benchmark highlights several key factors causing models to fail to be more effective evaluators. Other researchers can easily identify and work on improving these common failure modes.\n- **Extremely well written, clear experimental design, good reproducibility**"}, "weaknesses": {"value": "- **Low number of human annotators**: Only two annotators were used; their agreement is high, but I do wonder about some of the examples where the models are failing to agree with humans. If you gave those examples to more annotators, maybe we would find that they are actually somewhat ambigious.\n- **Lack of concrete examples**: Some of these failure modes are quite high-level, like \"operational reliability.\" I didn't see any example outputs from the models, but placing a few in the appendix may go a long way in helping other researchers understand more concretely what the errors are. Another suggestion would be to release the evaluation traces from your models. Personally, I've found that to be incredibly helpful, as it avoids having other researchers rerun sometimes costly experiments.\n- I would like to see an actual rubric tree somewhere. Maybe I missed it (or it is in the appendix somewhere), but it would help me fully understand what the leaf nodes are and how they work.\n\n\nThe rest of my \"weaknesses\" are really questions for the authors. I overall think this is a strong submission."}, "questions": {"value": "1. Most of webdev-arena-preference-10k is filtered out. Which of the three criteria filtered out the most of them (or was it the environment filter)? I would think we'd want to capture as many of these examples as possible including the ambigious ones because that's how we expect users to query the model. I am wondering if this filtering biased the findings in someway where the dataset is primiraly focused on a subset of \"webdev\" where evaluators are exhibiting interesting failure cases.\n2. I know this is not standard, but given your hierarchical rubric, I am wondering if there was any thought in trying more than two examples for pair-wise evaluation?  I believe there's work that shows more than two examples does not improve performance for direct comparisons, but with your hierarchical rubric and for failure cases like functional equivalence, I wonder if having more examples could help (this may be a costly experiment, though, so totally feel free to ignore this).\n3. How often is the lack of agreement due to functional equivalence? Is there a way to bin the lack of agreement by the failure modes presented in the paper?  I am wondering if 1, there's a way to establish what seems the most important to improve to enhance these models but also 2 to see if these models are just suffering from \"ambiguity\". The example in Figure 4 feels potentially ambigious and the LLM evaluator could be correct, for example \"though the exact text of symbol differs\" where the expected text is \"demonstration\" and the seen text is \"presentation\", it doesn't feel as clear-cut to me that these two words are functionally equivalent in all contexts and maybe if we asked more humans we'd find that there is actually some disagreement here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "761OKxpUZq", "forum": "CCSPm6V5EF", "replyto": "CCSPm6V5EF", "signatures": ["ICLR.cc/2026/Conference/Submission24064/Reviewer_L3iM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24064/Reviewer_L3iM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891271881, "cdate": 1761891271881, "tmdate": 1762942919149, "mdate": 1762942919149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WebDevJudge, a meta-evaluation benchmark for assessing the reliability of the LLM-as-a-judge paradigm in web dev tasks. The benchmark builds upon webdev-arena-preference and covers both static evaluation (code + screenshots) and dynamic evaluation (live interactive environment). It introduces the notion of a rubric and shows how the rubric leads to higher inter-annotator agreement for both humans and LLMs"}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Given the widespread adoption of LLM as a judge, the need for better \"judgements\" of llm-as-a-judge has grown as well. This paper adeptly addresses that. The paper text is for the most part clear and easy to follow. The benchmark is also original as to my knowledge there is not a specific benchmark for web judgements."}, "weaknesses": {"value": "- The main issue I see is that the results for all models seem relatively similar (~50s to 60s). There's not a lot of variation in terms of performance and there's no statistical tests to indicate that these values are actually meaningful. I have a strong suspicion that the reason these numbers are so similar is in fact because of the rubric. As pointed out in the paper, it increases inter-annotator agreement, but my guess is that it likely increases agreement *overall* as well and doesn't accurately test a model's judgement capability (especially since this rubric will not be used in-the-wild). As a result, I don't quite buy that this benchmark is meaningfully capturing llm-as-a-judge performance.\n- A smaller point is that the tables and figures are difficult to understand in isolation. For example, figure 1 is not parseable as is and requires significant context outside of the figure. Same with Table 1. Perhaps more information in the captions could help here."}, "questions": {"value": "- Can you please answer my main issue? Could you also highlight inter-model agreement for all the models in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mC1Kijoe0G", "forum": "CCSPm6V5EF", "replyto": "CCSPm6V5EF", "signatures": ["ICLR.cc/2026/Conference/Submission24064/Reviewer_FhLS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24064/Reviewer_FhLS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951789889, "cdate": 1761951789889, "tmdate": 1762942918812, "mdate": 1762942918812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}