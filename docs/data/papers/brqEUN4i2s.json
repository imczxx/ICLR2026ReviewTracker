{"id": "brqEUN4i2s", "number": 16115, "cdate": 1758260205585, "mdate": 1763055441617, "content": {"title": "Geometric Constraints as General Interfaces for Robot Manipulation", "abstract": "We present GeoManip, a framework to enable generalist robots to leverage essential geometric constraints derived from object-part relations for robot manipulation. For example, cutting a carrot typically requires the knife’s blade to be perpendicular to the carrot’s medial axis. By capturing geometric constraints through symbolic language representations and translating them into low-level actions, GeoManip bridges the gap between natural language and robotic execution, boosting the generalizability across diverse, even unseen tasks, objects, and scenarios. Beyond vision-language-action models that require extensive training, GeoManip operates training-free by leveraging large foundational models: a constraint generator to predict stage-specific geometric constraints and a geometry parser to locate the involved object parts. A solver then optimizes trajectories for the inferred constraints from the task descriptions and scenes. Further, GeoManip learns in-context and provides five appealing human-robot interaction features: on-the-fly policy adaptation, learning from human demonstrations, learning from failure cases, long-horizon action planning, and efficient data collection for imitation learning. Extensive evaluations on both simulations and real-world scenarios demonstrate GeoManip’s state-of-the-art performance, with superior out-of-distribution generalization while avoiding costly model training.", "tldr": "", "keywords": ["Embodied AI", "Robotics", "LLM"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8f280164384cf03073f516cf4d9523a6121329de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using VLM-specified geometric constraints as an intermediate representation for robot manipulation tasks. The pipeline consists of the following steps: inferring relevant object geometries using a combination of foundation models (object detectors, segmenters, VLMs), using VLMs to generate constraints as Python funtions that maps the set of identified geometries to a cost for the given task, and an optimization-based motion planner that generates motion trajectories of the robots by solving the constraints. The work builds on top of previous work ReKep, which considers only keypoint-based constraints. The main contribution appears to be the support for a broader set of geometries, including those that are difficult to be specified with keypoints, such as the rim of a cup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is written with clarity in general and easy to follow.\n- The empirical results are extensive across two simulated domains and in the real world and are shown to be competitive to a range of baselines which require task-specific training data.\n- The paper also demonstrates a set of additional benefits, including learning from human demonstrations, and adaptation."}, "weaknesses": {"value": "- The provided URL points to an empty google sites. There is no video results.\n- While the proposed “select-process” module supports a broader set of geometries compared to keypoints used in ReKep, an apparent issue appears to be that it’s difficult to be integrated in the downstream optimization process. For example, in Figure 4, when a cup rim is identified, how may the VLM write the constraint function to leverage such geometry? This appears to be unclear from the paper."}, "questions": {"value": "See weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F6wlGlouCw", "forum": "brqEUN4i2s", "replyto": "brqEUN4i2s", "signatures": ["ICLR.cc/2026/Conference/Submission16115/Reviewer_wsXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16115/Reviewer_wsXV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952505026, "cdate": 1761952505026, "tmdate": 1762926289135, "mdate": 1762926289135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "jt2MnWaylT", "forum": "brqEUN4i2s", "replyto": "brqEUN4i2s", "signatures": ["ICLR.cc/2026/Conference/Submission16115/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16115/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763055440958, "cdate": 1763055440958, "tmdate": 1763055440958, "mdate": 1763055440958, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the GeoManip framework, which leverages essential geometric constraints derived from object-part relations for robot manipulation. This framework captures geometric constraints and translates them into low-level actions. This paper first proposes a constraint generator to predict stage-specific geometric constraints and a geometry parser to locate the involved object parts. Further, a solver is utilized to optimize trajectories. The results prove the effectiveness of this method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.  This paper proposes a VLM-driven method for fine-grained robotic manipulation with geometric constraints.\n2. This paper proposes a cost function for the trajectory generation."}, "weaknesses": {"value": "1. This paper presents limited novelty compared to ReKep. The method for deriving geometric information still relies on the inherent capabilities of existing models such as SAM and VLMs, and this level of incremental work is insufficient to meet the ICLR acceptance criteria.\n\n2. This method relies on a carefully human-designed prompt and motion planning rule for target task manipulation, which makes it difficult to apply in fine-grained robotic manipulation tasks such as folding.\n\n3. Relying on the static vision perception and motion planning, it seems that this method cannot correct its behavior in a dynamic environment when failure cases occur. It needs to re-prompt with human intervention, re-generate the motion trajectory, which makes it not suitable for dynamic robotic manipulation tasks."}, "questions": {"value": "1. This work utilizes SAM and Grounding-DINO for object detection and segmentation to extract geometric-aware information; however, these perception model always fails when the objects are not obvious, so how does this kind of framework improve the robustness instead of utilizing a more powerful vision foundation model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XYAXARYndt", "forum": "brqEUN4i2s", "replyto": "brqEUN4i2s", "signatures": ["ICLR.cc/2026/Conference/Submission16115/Reviewer_KGDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16115/Reviewer_KGDf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988993420, "cdate": 1761988993420, "tmdate": 1762926288605, "mdate": 1762926288605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GeoManip tackles the problem of generalizable robotic manipulation by representing tasks through object–part geometric constraints. The authors note two main challenges in prior work: weak transfer of manipulation skills across object categories and the absence of interpretable interfaces linking language to low-level control. To address this, GeoManip introduces a training-free framework consisting of three modules. A Geometry Parser extracts functional object parts via SAM and vision–language reasoning, a Constraint Generator converts task descriptions into symbolic geometric relations, and a Solver optimizes these constraints into executable trajectories. Experiments in Meta-World, OmniGibson, and real-world settings show that GeoManip surpasses existing baselines and supports interactive task editing, failure correction, and demonstration-based learning, demonstrating strong generalization to new objects and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\n\nGeoManip introduces the idea of using object–part geometric constraints as a general interface for robotic manipulation, differing from prior approaches that map vision or language directly to actions. Its main contribution lies in integrating part parsing, constraint generation, and trajectory optimization into a unified, training-free framework. However, the paper does not discuss or compare with closely related methods such as CoPa, which share a similar underlying concept, leaving some ambiguity in how its originality stands relative to existing work.\n\n**Quality**\n\nThe technical quality of the work is strong, with a well-structured methodology and thorough implementation across both simulation and real-world experiments. The modular design is well justified, and the results clearly demonstrate the feasibility and generalization ability of the approach. Although some quantitative evaluations are relatively limited, the experiments as a whole adequately support the paper’s main claims.\n\n**Clarity**\n\nThe paper is well organized and easy to follow, with clear figures and explanations that effectively convey the system workflow and constraint mechanism. The writing is concise, and the main technical ideas are communicated clearly. Some implementation details such as prompt construction and parameter settings are described briefly, but this does not significantly affect the overall readability.\n\n**Significance**\n\nGeoManip provides an interpretable and general framework for connecting language, geometry, and robot action through spatial constraints. Its training-free design and demonstrated transfer across diverse tasks highlight both practical potential and conceptual impact. The approach contributes meaningfully to advancing geometry-based reasoning in robotic manipulation and may inspire further work on modular, explainable robot learning."}, "weaknesses": {"value": "**Limited Real-Time Efficiency and Computational Cost**\n\nThe GeoManip framework relies on multiple sequential vision–language model calls for segmentation, constraint generation, and cost function formulation, followed by trajectory optimization. This design introduces significant computational overhead, yet the paper does not report inference time or system latency. Without runtime analysis, it remains unclear whether the framework can operate effectively in time-sensitive or dynamic environments, where high responsiveness and low-latency control are essential.\n\n**Limited Expressiveness of Geometric Constraints**\n\nGeoManip primarily models static spatial relations between rigid parts, such as position and orientation. However, many real-world tasks involve non-rigid objects like cloth, rope, or deformable materials, as well as dynamic interactions with friction, elasticity, or moving targets. The current constraint formulation lacks the expressiveness to handle these phenomena, limiting the framework’s applicability to more complex or physics-rich manipulation tasks.\n\n**Insufficient Baseline Coverage and Fairness**\n\nThe experimental evaluation omits several relevant comparisons that would strengthen the analysis. In OmniGibson, GeoManip is only compared with ReKep, and in Meta-World it is constrained to simple translational motions. The real-world tests include only OpenVLA as a baseline. The absence of comparisons with classical or constraint-based planners such as LGP, CHOMP, or CuRobo makes it difficult to assess relative advantages in constraint feasibility, trajectory optimality, and computational efficiency.\n\n**Lack of Ablation and Model Dependency Analysis**\n\nThe method’s performance heavily depends on large vision-language models for reasoning and part parsing, yet the paper does not include ablation experiments to quantify this dependence. No analysis is provided on how results change when using smaller or weaker models (e.g., replacing GPT-4o with a lightweight LLM), making it hard to judge the robustness and scalability of the proposed approach.\n\n**Unclear Relation to Prior Constraint-Based Manipulation Work**\n\nThe paper positions “geometric constraints as a general interface” as its key contribution but does not discuss or cite conceptually similar research, such as \"CoPa: General Robotic Manipulation through Spatial Constraints of Parts\". Both works share the core idea of using part-level spatial constraints to generalize manipulation behavior. The lack of acknowledgment or empirical comparison with CoPa leaves the contribution boundary ambiguous and reduces clarity in positioning within the existing literature."}, "questions": {"value": "**Questions**\n\n**Q1: Generalization and Robustness Boundary** – The paper emphasizes strong out-of-distribution generalization, yet it remains unclear how GeoManip performs when facing geometrically novel objects that are linguistically describable but structurally unseen. Can the vision–language model accurately infer appropriate manipulation constraints for such unfamiliar geometries? In addition, how well does the system maintain performance under environmental variations such as changes in lighting, occlusion, or clutter?\n\n**Q2: Inference Speed and Computational Efficiency** – What is the total runtime required for a full manipulation task in both simulation and real-world experiments? Please provide detailed timing statistics for each pipeline component—part parsing, constraint generation, cost function formulation, and trajectory optimization—to assess whether the framework can meet real-time or on-robot control requirements.\n\n**Suggestions**\n\n**S1: Broader Generalization and Robustness Evaluation** – Evaluate GeoManip on linguistically describable but geometrically novel objects, as well as under diverse environmental variations such as lighting, occlusion, clutter, and background changes. Provide quantitative success rates or degradation curves to clearly define the system’s generalization boundary and real-world robustness.\n\n**S2: Comprehensive Runtime Profiling** – Report detailed timing statistics for each pipeline stage, including part parsing, constraint generation, cost function computation, and trajectory optimization, in both simulation and real-world settings. This would clarify computational feasibility and identify potential bottlenecks for real-time or on-robot deployment.\n\n**S3: Fine-Grained Module Diagnostics** – Introduce explicit quantitative metrics for each system component, such as the usable part detection rate in the select–process stage, constraint correctness or consistency score, and average error of cost function optimization. These metrics would help isolate dominant error sources and guide targeted system improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KhXbikviav", "forum": "brqEUN4i2s", "replyto": "brqEUN4i2s", "signatures": ["ICLR.cc/2026/Conference/Submission16115/Reviewer_G14J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16115/Reviewer_G14J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998224011, "cdate": 1761998224011, "tmdate": 1762926288206, "mdate": 1762926288206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}