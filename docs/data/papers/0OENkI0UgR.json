{"id": "0OENkI0UgR", "number": 6447, "cdate": 1757984587977, "mdate": 1759897913981, "content": {"title": "PDUNet: Proximal-Guided Deep Unrolling Network for Time Series Forecasting", "abstract": "Recent time series forecasting methods have increasingly incorporated model-driven formulations and data-driven inference to leverage their complementary strengths in interpretability and pattern learning, achieving remarkable results in many practical tasks. However, in most existing methods, the forecasting target is only used as a training supervision signal rather than being explicitly incorporated into the modeling process, making it difficult for models to leverage future modality information for structural constraint and path guidance. As a result, the modeling structure and inference path remain isolated, lacking a clear optimization objective, and ultimately degenerate into an uncontrollable and uninterpretable static mapping process. To address these challenges, we propose Proximal Deep Unrolling Network (PDUNet), a coupled and closed-loop forecasting framework that starts from modeling, leverages data-driven mechanisms for solution, and progressively optimizes the inference path. Specifically, we formulate an optimizable forecasting equation based on the coupling between future variables and historical inputs, and adopt a proximal optimization algorithm to perform step-wise decoupling and update. This optimization process is further unfolded into a dual-branch state space structure, where the Temporal-SSD captures temporal evolution through causal modeling, while the Channel-SSD employs a non-causal mechanism to model global interactions among variables, jointly enabling progressive inference and dynamic prediction. Experiments on eight public benchmark datasets show that PDUNet outperforms existing state-of-the-art models in long-term forecasting tasks.", "tldr": "", "keywords": ["Time series forecasting", "Unmixing", "Mamba network", "Shared mechanism."], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/230954702f9f9c63191b48cc384aebe14f3ae3f2.pdf", "supplementary_material": "/attachment/4558e02cf429b1b8dd397cc6e069744d97d0234a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PDUNet, a novel framework for time series forecasting that integrates model-driven and data-driven paradigms. The core idea is to formulate forecasting as an iterative optimization problem where the future sequence is treated as a latent variable and progressively refined. This is achieved by unrolling a proximal gradient descent algorithm into a deep network, comprising a Gradient Descent Module (GDM) and a Proximal Mapping Module (PMM). The PMM leverages a Dual-SSD structure, with separate branches (Temporal-SSD and Channel-SSD) to capture temporal and inter-channel dependencies. Extensive experiments on eight benchmark datasets demonstrate that PDUNet achieves state-of-the-art performance, particularly in long-term forecasting horizons."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Moving beyond the standard \"static mapping\" paradigm and explicitly modeling the future as an optimizable variable within an unrolled optimization loop is a compelling and novel direction. This provides a clear, interpretable, and controllable inference path, which is often missing in deep learning-based forecasters.\n\n2. This paper provides a rigorous formulation, from the proximal-based optimization objective (Eq. 4) to its gradient derivation and unrolling into a learnable architecture (GDM, PMM). \n\n3. This paper is generally well-written. Figures 1 and 2 effectively illustrate the core conceptual shift and the model's architecture, making the proposed methodology easy to grasp."}, "weaknesses": {"value": "1. This paper empirically shows that iterative updates improve performance, but it lacks any theoretical grounding. There is no discussion on the convergence guarantees of the unrolled proximal gradient method under the learned parameters.\n\n2. This paper repeatedly emphasizes the \"interpretability\" and \"controllability\" of the proposed framework. However, no qualitative analysis or visualization is provided to demonstrate this. For instance, visualizing how the prediction $\\hat{\\mathbf{Y}}^{(k)}$ evolves across iterations $K$ or analyzing the learned prior $\\mathcal{J}(\\cdot)$ would be necessary to substantiate these claims and provide tangible insights into the model's decision-making process.\n\n3. The selection of baselines is good but could be strengthened by including recent models like TimeBridge or TQNet that have shown strong performance in time series."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8n7crBrfLv", "forum": "0OENkI0UgR", "replyto": "0OENkI0UgR", "signatures": ["ICLR.cc/2026/Conference/Submission6447/Reviewer_gt7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6447/Reviewer_gt7t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760881597426, "cdate": 1760881597426, "tmdate": 1762918841256, "mdate": 1762918841256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PDUNet, which formulates long-horizon multivariate time series forecasting as a structured optimization problem over future variables and solves it via proximal gradient methods combined with deep unrolling. Each iteration consists of a Gradient Descent Module (GDM) followed by a Proximal Mapping Module (PMM). The PMM contains a parallel Dual-SSD design—Temporal-SSD and Channel-SSD—that performs state-space modeling along the time and channel axes, yielding a progressively refined prediction trajectory. Training uses a combination of the primary forecasting loss and a stage-consistency regularizer. Across eight standard datasets and multiple strong baselines, PDUNet shows stable advantages for medium/long horizons, and ablations verify the necessity of the unrolling mechanism and dual-path state-space modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The PMM employs Dual-SSD (time/channel-parallel state-space units), shifting expensive nonlinear computation into a compressed state space, balancing expressiveness and efficiency.\n\nS2. The algorithmic pipeline, module boundaries, and data flow are clear.\n\nS3. Comparisons are fairly comprehensive across datasets and horizons; performance is robust for long-horizon forecasting."}, "weaknesses": {"value": "W1. Early deep unrolling works have already networked “gradient steps + proximal mappings” (e.g., LISTA [1]); subsequent Plug-and-Play / proximal-unrolling lines further integrated learnable priors into iterative frameworks [2]. In the sequence domain, optimization- or recursion-based prediction/filtering (e.g., KalmanNet [3]) and continuous-dynamics modeling (Neural ODE [4]) are also representative. PDUNet’s contribution, explicitly optimizing future targets and realizing a structured “proximal” map via Dual-SSD, is interesting, but quantitative and mechanistic contrasts against these main lines (to clarify fundamental differences and irreplaceability) remain limited.\n\nW2. The Dual-SSD design is methodologically close to the SSM/S4 family (Gu et al., 2021) and Mamba (Gu & Dao, 2024) for efficient long-sequence modeling; the paper also compares to iTransformer (Liu et al., 2023), PatchTST (Nie et al., 2023), DLinear (Zeng et al., 2022), and TimeMixer (Chen et al., 2024). A more systematic contrast on complexity, spectral/phase stability, and long-horizon degradation would better position PDUNet’s strengths and limitations relative to these lines.\n\nW3. While the paper emphasizes progressive convergence, it lacks quantitative evidence such as iteration-wise error/step convergence curves and consistency of selected key lags across random seeds.\nW4. The PMM effectively learns a data-driven proximal operator, yet the corresponding prior/regularizer \nJ(⋅) is not sufficiently interpreted or visualized; instability in J could lead to pattern leakage.\n\nReferences\n[1] Gregor, K., & LeCun, Y. “Learning Fast Approximations of Sparse Coding.” ICML, 2010, pp. 399–406.\n[2] Venkatakrishnan, S. V., Bouman, C. A., & Wohlberg, B. “Plug-and-Play Priors for Model Based Reconstruction.” IEEE GlobalSIP, 2013, pp. 945–948.\n[3] Revach, G., Shlezinger, N., Ni, X., et al. “KalmanNet: Neural Network Aided Kalman Filtering for Partially Known Dynamics.” IEEE Transactions on Signal Processing, 70:1532–1547, 2022.\n[4] Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. “Neural Ordinary Differential Equations.” NeurIPS, 2018."}, "questions": {"value": "Q1. Under channel permutation and channel-subset missing perturbations, do the factors/states of Dual-SSD remain similar?\n\nQ2. Under the same complexity budget (aligned parameters/memory/latency), how does PDUNet compare with S4/Mamba for long-horizon forecasting performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0V0ZegygZd", "forum": "0OENkI0UgR", "replyto": "0OENkI0UgR", "signatures": ["ICLR.cc/2026/Conference/Submission6447/Reviewer_nPrH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6447/Reviewer_nPrH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634282796, "cdate": 1761634282796, "tmdate": 1762918840296, "mdate": 1762918840296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Proximal Deep Unfolding Network (PDUNet), a coupled closed-loop prediction framework that begins with modeling, leverages data-driven mechanisms for resolution, and progressively optimizes the inference path. Unlike prior work that treats prediction targets solely as training supervision signals, the paper establishes an optimizable prediction equation based on the coupling between future variables and historical inputs, employing approximate optimization algorithms for gradual decoupling and updating. Extensive experiments demonstrate the effectiveness of PDUNet."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is relatively novel. The paper establishes an optimizable prediction equation based on the coupling between future variables and historical inputs, employing an approximate optimization algorithm for gradual decoupling and updating. Furthermore, the paper unfolds this optimization process into a dual-branch state space structure. Temporal-SSD and Channel-SSD jointly achieve progressive inference and dynamic prediction.\n\n2. State-of-the-art performance. As shown in Table 1, the paper achieves state-of-the-art performance on most datasets."}, "weaknesses": {"value": "1. Lack of visual experiments. While the paper performs well in quantitative experiments, it lacks complementary qualitative experiments. For example: Do the prediction curves fit the true values? Are the forecasts for cycles and trends accurate? Can extreme values be predicted? The paper needs to provide some visual experiments.\n\n2. Lack of comparison with recent methods. I note that the paper employs SSM, so highly relevant prior work should be compared, including both methodology and experiments. This would help demonstrate the paper's innovation and effectiveness.\n\n[1] Chimera: Effectively modeling multivariate time series with 2-dimensional state space models (NeurIPS24)\n[2] TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable-and Time-Aware Hyper-state [ICML'25]\n\n3. I have some confusion regarding the ablation experiments in Table 3. When exploring a hyperparameter such as K, does the author keep all other parameters constant? Based on my previous experience, optimal hyperparameters often vary across different datasets. However, as shown in Table 3, the three optimal parameters are identical across all three datasets. I think this seems counterintuitive.\n\n4. Efficiency metrics must be provided to facilitate the assessment of the method's practical efficiency. Examples include the number of parameters, FLOPs, and throughput.\n\n5. How does this method perform in short-term forecasting? The authors did not restrict the scope to long-term time series forecasting in the title. Therefore, some short-term benchmarks such as PEMs can be provided."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9uaRR1qtAJ", "forum": "0OENkI0UgR", "replyto": "0OENkI0UgR", "signatures": ["ICLR.cc/2026/Conference/Submission6447/Reviewer_MwUo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6447/Reviewer_MwUo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966831368, "cdate": 1761966831368, "tmdate": 1762918839718, "mdate": 1762918839718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In most existing time series forecasting methods, the forecasting target is only used as a training supervision signal and is not explicitly incorporated into the modeling process. This makes it difficult for models to utilize future modality information for structural constraints and path guidance. This paper proposes a new forecasting framework called PDUNet, which tightly integrates the model-driven and data-driven paradigms through a learnable proximal unfolding process. Specifically, PDUNet formulates the forecasting task as an iterative optimization problem and introduces a Dual-Structural State Decoder (Dual-SSD) to capture structural dependencies in both the temporal and channel dimensions. The entire process is modeled as an interpretable and controllable evolutionary path guided by gradient descent and proximal mapping. This design enables PDUNet to jointly optimize prediction accuracy and structural consistency during the iterative process. The paper demonstrates the effectiveness of the proposed method on multiple public datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a new forecasting framework called PDUNet, which tightly integrates the model-driven and data-driven paradigms through a learnable proximal unfolding process. Specifically, PDUNet formulates the forecasting task as an iterative optimization problem and introduces a Dual-Structural State Decoder (Dual-SSD) to capture structural dependencies in both the temporal and channel dimensions. The entire process is modeled as an interpretable and controllable evolutionary path guided by gradient descent and proximal mapping. This design enables PDUNet to jointly optimize prediction accuracy and structural consistency during the iterative process. The paper demonstrates the effectiveness of the proposed method on multiple public datasets."}, "weaknesses": {"value": "1. The paper formulates an optimizable prediction equation based on the coupling between future variables and historical inputs and employs a proximal optimization algorithm for step-by-step decoupling and updating. However, the code implementation is confusing and seems to differ from the theoretical description in the paper. For example, many learnable parameters are initialized in the code with annotations of their physical meanings, without implementing the corresponding functions, such as not calculating the gradients for g'. In reality, it is merely the output of neural networks. The future variable Y_PROX is also initialized to all zeros, which seems to have no explicit connection with the actual future variables.\n\n2. The paper lacks comparative experiments on time efficiency with different baselines.\n\n3. The experiments in the paper are not solid and convincing. The classic dataset traffic is missing. Moreover, the ablation experiments were only conducted on three small-scale datasets.\n\n4. The experimental evaluation in the paper is not very reliable. It was only assessed on a 96-window, while different methods may be suitable for different window lengths. It would be better to evaluate on multiple window lengths, similar to DUET. For example, when the window length is 512, the method in the paper may not be better than DUET.\n\n5. Compared to most datasets, the variance differences in some results compared to the baselines are excessively large, such as in the solar dataset. It is unclear what causes the differences."}, "questions": {"value": "The paper proposes to use an optimizable prediction equation based on the coupling between future variables and historical inputs and employs a proximal optimization algorithm for step-by-step decoupling and updating. However, in the actual code implementation, many parts use initialized learnable parameters and neural network outputs as substitutes, and it seems that there are no additional functional constraints under the theory of proximal optimization. How can we ensure that these initialized parameters conform to the theory of proximal optimization? How can we ensure that their optimization direction is towards the ideal direction described in the paper? For example, they could be optimized to match the physical meanings of the corresponding variables annotated in the code."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bTOYeBRSX", "forum": "0OENkI0UgR", "replyto": "0OENkI0UgR", "signatures": ["ICLR.cc/2026/Conference/Submission6447/Reviewer_KkV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6447/Reviewer_KkV7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993630620, "cdate": 1761993630620, "tmdate": 1762918838702, "mdate": 1762918838702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}