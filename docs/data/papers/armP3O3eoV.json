{"id": "armP3O3eoV", "number": 11812, "cdate": 1758204005466, "mdate": 1763560369354, "content": {"title": "Random Projections for Spectral Algorithms in Mis-specified Setting: Sobolev Norm Learning Rates and Minimax Optimality", "abstract": "Random projections (RP) offer an effective approach to reducing computational and storage costs while preserving the geometric structure of the data. However, existing studies primarily focus on the optimal generalization performance of specific kernel-regularized algorithms with RP in the well-specified setting under restrictive conditions. In this paper, we provide a comprehensive and improved analysis of the generalization performance of RP-based spectral algorithms under general conditions, without increasing computational complexity. By leveraging the embedding property of the RKHS and a refined analysis of the operator similarity, we establish optimal learning rates in Sobolev norms that match the minimax lower bounds up to logarithmic factors. For both randomized sketches and Nystr\\\"{o}m sub-sampling (uniform or leverage-based), we show that the projection dimension needed for optimality is proportional to the average or maximal effective dimension, yielding a significant reduction in computational cost while maintaining the statistical efficiency.  Our results do not rely on the uniform boundedness assumption on the target function and hold for a broad range of source conditions, i.e., $s\\geq \\alpha-1/\\beta$, where $s,\\beta$, and $\\alpha$ denote the smoothness index, capacity index, and the embedding index, respectively. In the benign case when $\\alpha=1/\\beta$, the optimality holds for all $s\\in (0,2\\tau]$ with $\\tau$ denoting the quantification index. Experimental results confirm our theoretical findings and demonstrate the practical effectiveness of RP.", "tldr": "", "keywords": ["Random projections; Kernel methods; Mis-specified learning; Minimax optimality; Interpolation space"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f840ff013d20f5e203d25643faf3d29b6432acf0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the generalization error of scalar-valued regression estimators derived from spectral algorithms within RKHS, specifically when combined with Random Projection techniques (including Nystrom methods). The authors aim to provide a unified analysis that incorporates several refinements previously studied in isolation: handling the misspecified case (by taking into account the embedding index $\\alpha$), relaxing assumptions on the target function's boundedness, using general source conditions, applying general spectral algorithms, incorporating random projections, and analyzing convergence in Sobolev norms. The main results presented are convergence rates (upper and lower bounds) under these combined conditions, with a claim that these rates avoid the saturation effect typically seen in kernel methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an important and relevant problem for the ICLR community: understanding the theoretical properties of scalable kernel methods (like those using Nyström or other random projections) under realistic assumptions (mis-specification, general smoothness).\n* The paper attempts to cover a wide range of modern theoretical aspects relevant to kernel regression, potentially offering a more complete picture."}, "weaknesses": {"value": "Despite the interesting goal, the paper suffers from significant weaknesses, including potentially erroneous central claims and numerous presentation errors. These issues undermine the reliability of the results and require major revisions.\n\n**Major Issues**\n\n* **Erroneous Claim on Saturation Effect**: The paper claims that the derived rates ``do not exhibit saturation effect.'' This contradicts established theoretical results for kernel methods, which shows that saturation is unavoidable for KRR (Zhang, 2023). The error appears to stem from Lemma 8, specifically the application of Eq. (13), which seems to ignore the condition $\\nu \\leq \\tau$. A correct application likely leads back to the standard saturation condition $s \\leq \\tau + \\gamma$ (see [1]).\n* **Issues with General Source Condition (GSC)**: The paper emphasizes the GSC as a significant generalization, but its presentation is flawed ($\\tau$ used before definition, parameter $s$ missing from $f_{\\rho} \\in \\Omega_{\\phi,R}$). More critically, the analysis later appears to rely on the property $P\\phi(A)P=\\phi(PAP)$ (P projection, A operator). It's questionable if any commonly used non-linear $\\phi$ (like the Hölder case) satisfies this under the paper's assumptions. This potentially invalidates the applicability of the results.\n\n**Other Issues**\n\n* **Uniformly Bounded Eigenfunctions (UBE)**: Example 1 (line 1041) suggests that the UBE assumption is a common or mild condition, citing prior work that makes this claim. This is mathematically incorrect and spreading this misconception is problematic. See [2] for a counter-example of the claim.\n* **Lack of Precision Regarding Measures**: Examples 2 \\& 3 claim certain RKHSs are \"benign\" without specifying the crucial dependence on the underlying probability measure. These properties often hold only for specific measures (like the uniform measure). Similarly, Section C.3.2 discussing Sobolev embeddings needs to specify the assumed measure. Without this, the claims are ill-defined or potentially false.\n* **Lower bound**: The theorem statement claims only a bound on the effective dimension is needed, but the proof appears to use lower and upper bounds on eigenvalues. The theorem statement should precisely list all necessary assumptions. Assumption 3 and the mis-specification parameter $\\alpha$ are mentioned in the theorem and discussion but seem irrelevant to the lower bound analysis. As for previous lower bound (e.g. Zhang 2024) the bound is valid for any smoothness level irrespective of $\\alpha$.\n* **Typos** (064-083)  stiuded, projectins (twice), (097) ,our, (143) the reproducing property holds that, (263) that leverage operator spectral, (326) plain Nyström, (350) an projection, (772-1487-1546-1563-1698-1713-) various typos with norms, (808) wwe, (1111) determined by $r$, (1131) by allowing the index function by allowing the index function, (1442) Assumption 2 (should it be 4?)...\n* **Redundant Proofs & Lack of Attribution**: Section C.6 and the proof of Lemma 4 appear to reproduce technical results already established in the literature (Zhang 2024) without clear attribution or justification for their inclusion. This makes the paper appear more technically dense than necessary and obscures the novel contributions. Similarly, how different are the proofs of Lemma 16 and 17 compared to Theorem 13 and 15 in Zhang 2024? Lemma 23 can be found in Fischer & Steinwart. \n\n[1] Blanchard and Mucke. Optimal rates for regularization of statistical inverse learning problems, 2018.\n[2] Minh, Niyogi, and Yao. Mercer’s theorem, feature maps, and smoothing, 2006."}, "questions": {"value": "* **Mercer Assumptions:** The analysis assumes the domain $\\mathcal{X}$ is compact and the kernel $K$ is Mercer. However, much of the modern analysis of kernel methods relies on weaker conditions, often only requiring the kernel to be square-integrable w.r.t. the measure to ensure the integral operator is Hilbert-Schmidt \\citep{fischer2020sobolev}. Could you clarify why the more restrictive classical Mercer conditions are invoked or needed?\n\n* Could the authors detail how they go from $||O_{K,n,\\lambda}^{1/2} g_{\\lambda}(O_{K,n})P C_{K,n,\\lambda}^{1/2}||$ to $||O_{K,n,\\lambda} g_{\\lambda}(O_{K,n})||$ (1682-1684), I could not convinced myself of this step. \n\n* Have the authors considered extending their analysis to the vector-valued setting (relevant for conditional mean embeddings [1]) or incorporating recently proposed relaxations of noise assumptions [2]?\n\n[1] Li, Meunier, Mollenhauer, Gretton. Optimal rates for regularized conditional mean embedding learning. 2022\n[2] Mollenhauer, Mucke, Meunier, retton. Regularized least squares learning with heavy-tailed noise is minimax optimal. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AGqeKwTiGl", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Reviewer_i6nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Reviewer_i6nj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760983865890, "cdate": 1760983865890, "tmdate": 1762922833168, "mdate": 1762922833168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the generalization error of scalar-valued regression estimators derived from spectral algorithms within RKHS, specifically when combined with Random Projection techniques (including Nystrom methods). The authors aim to provide a unified analysis that incorporates several refinements previously studied in isolation: handling the misspecified case (by taking into account the embedding index $\\alpha$), relaxing assumptions on the target function's boundedness, using general source conditions, applying general spectral algorithms, incorporating random projections, and analyzing convergence in Sobolev norms. The main results presented are convergence rates (upper and lower bounds) under these combined conditions, with a claim that these rates avoid the saturation effect typically seen in kernel methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an important and relevant problem for the ICLR community: understanding the theoretical properties of scalable kernel methods (like those using Nyström or other random projections) under realistic assumptions (mis-specification, general smoothness).\n* The paper attempts to cover a wide range of modern theoretical aspects relevant to kernel regression, potentially offering a more complete picture."}, "weaknesses": {"value": "Despite the interesting goal, the paper suffers from significant weaknesses, including potentially erroneous central claims and numerous presentation errors. These issues undermine the reliability of the results and require major revisions.\n\n**Major Issues**\n\n* **Erroneous Claim on Saturation Effect**: The paper claims that the derived rates ``do not exhibit saturation effect.'' This contradicts established theoretical results for kernel methods, which shows that saturation is unavoidable for KRR (Zhang, 2023). The error appears to stem from Lemma 8, specifically the application of Eq. (13), which seems to ignore the condition $\\nu \\leq \\tau$. A correct application likely leads back to the standard saturation condition $s \\leq \\tau + \\gamma$ (see [1]).\n* **Issues with General Source Condition (GSC)**: The paper emphasizes the GSC as a significant generalization, but its presentation is flawed ($\\tau$ used before definition, parameter $s$ missing from $f_{\\rho} \\in \\Omega_{\\phi,R}$). More critically, the analysis later appears to rely on the property $P\\phi(A)P=\\phi(PAP)$ (P projection, A operator). It's questionable if any commonly used non-linear $\\phi$ (like the Hölder case) satisfies this under the paper's assumptions. This potentially invalidates the applicability of the results.\n\n**Other Issues**\n\n* **Uniformly Bounded Eigenfunctions (UBE)**: Example 1 (line 1041) suggests that the UBE assumption is a common or mild condition, citing prior work that makes this claim. This is mathematically incorrect and spreading this misconception is problematic. See [2] for a counter-example of the claim.\n* **Lack of Precision Regarding Measures**: Examples 2 \\& 3 claim certain RKHSs are \"benign\" without specifying the crucial dependence on the underlying probability measure. These properties often hold only for specific measures (like the uniform measure). Similarly, Section C.3.2 discussing Sobolev embeddings needs to specify the assumed measure. Without this, the claims are ill-defined or potentially false.\n* **Lower bound**: The theorem statement claims only a bound on the effective dimension is needed, but the proof appears to use lower and upper bounds on eigenvalues. The theorem statement should precisely list all necessary assumptions. Assumption 3 and the mis-specification parameter $\\alpha$ are mentioned in the theorem and discussion but seem irrelevant to the lower bound analysis. As for previous lower bound (e.g. Zhang 2024) the bound is valid for any smoothness level irrespective of $\\alpha$.\n* **Typos** (064-083)  stiuded, projectins (twice), (097) ,our, (143) the reproducing property holds that, (263) that leverage operator spectral, (326) plain Nyström, (350) an projection, (772-1487-1546-1563-1698-1713-) various typos with norms, (808) wwe, (1111) determined by $r$, (1131) by allowing the index function by allowing the index function, (1442) Assumption 2 (should it be 4?)...\n* **Redundant Proofs & Lack of Attribution**: Section C.6 and the proof of Lemma 4 appear to reproduce technical results already established in the literature (Zhang 2024) without clear attribution or justification for their inclusion. This makes the paper appear more technically dense than necessary and obscures the novel contributions. Similarly, how different are the proofs of Lemma 16 and 17 compared to Theorem 13 and 15 in Zhang 2024? Lemma 23 can be found in Fischer & Steinwart. \n\n[1] Blanchard and Mucke. Optimal rates for regularization of statistical inverse learning problems, 2018.\n[2] Minh, Niyogi, and Yao. Mercer’s theorem, feature maps, and smoothing, 2006."}, "questions": {"value": "* **Mercer Assumptions:** The analysis assumes the domain $\\mathcal{X}$ is compact and the kernel $K$ is Mercer. However, much of the modern analysis of kernel methods relies on weaker conditions, often only requiring the kernel to be square-integrable w.r.t. the measure to ensure the integral operator is Hilbert-Schmidt \\citep{fischer2020sobolev}. Could you clarify why the more restrictive classical Mercer conditions are invoked or needed?\n\n* Could the authors detail how they go from $||O_{K,n,\\lambda}^{1/2} g_{\\lambda}(O_{K,n})P C_{K,n,\\lambda}^{1/2}||$ to $||O_{K,n,\\lambda} g_{\\lambda}(O_{K,n})||$ (1682-1684), I could not convinced myself of this step. \n\n* Have the authors considered extending their analysis to the vector-valued setting (relevant for conditional mean embeddings [1]) or incorporating recently proposed relaxations of noise assumptions [2]?\n\n[1] Li, Meunier, Mollenhauer, Gretton. Optimal rates for regularized conditional mean embedding learning. 2022\n[2] Mollenhauer, Mucke, Meunier, retton. Regularized least squares learning with heavy-tailed noise is minimax optimal. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AGqeKwTiGl", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Reviewer_i6nj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Reviewer_i6nj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760983865890, "cdate": 1760983865890, "tmdate": 1763549378148, "mdate": 1763549378148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "N2LloT22Or", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763560368585, "cdate": 1763560368585, "tmdate": 1763560368585, "mdate": 1763560368585, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive and improved analysis of the generalization performance of RP-based spectral algorithms under general\nconditions, without increasing computational complexity. The focus is on generalization performance in mis-specified settings (where the target function may not lie in the RKHS) under general conditions, without assuming uniform boundedness on the regression function. Matching upper and minimax lower bounds in Sobolev norms are established to show the optimality of the algorithm. Numerical experiments confirm theoretical rates and practical benefits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is well-written and the structure is clear, despite the technical complexities. A table summarizing the related results is provided, allowing for a easy comparision.\n\n* This paper presents a comprehensive analysis of the spectral algorithms with random projection, unifying Radomized Sketches, plain Nystrom and ALS Nystrom. The settings are general regarding the source condition, embedding properties and algorithms. Moreover, the assumptions are discussed in detail.\n\n* The theory in this paper is strong, proving the minimax optimal rates of SARP. Moreover, the required projection dimension is less than or equal to that required in the literature, providing theoretical guarantees for reducing computational costs.\n\n* Empirical experiments are provided to validate the theory."}, "weaknesses": {"value": "* Assumption 4 cannot hold for $\\phi(u) = u^{s/2}$ when $s >2\\tau$. Consequently, the saturation effect still holds for KRR, in contrast to the claim in the paper. See the proof of Lemma 8 and Proposition 2.\n\n* The technical contribution in this paper seems to be marginal. The proof idea and steps seem to be standard. A overview of the technical novelty can be provided and emphasized.\n\n* It would improve the contribution of this paper to propose practical criteria for choosing the projection dimension."}, "questions": {"value": "1. While the proof in this paper is erroneous, is it possible that indeed SARP does not suffer from the saturation effect? Do we have supporting empirical evidence?\n\n\n1. Is the current requirement of the projection dimension $m$ minimal for the optimal rates? Is it possible to establish a lower bound for it, or what are the difficulties here?\n\n2. Can you weaken requirement that $\\mathcal{X}$ is compact? What will be the impact of non-compactness to the effectiveness of random projection methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UYiuhDYrZr", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Reviewer_3MZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Reviewer_3MZ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682771697, "cdate": 1761682771697, "tmdate": 1762922832723, "mdate": 1762922832723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a comprehensive study about the generalization performance of RP-based spectral algorithms under more general conditions. \n\nThe main modification of assumptions in this paper includes two parts. Firstly, it proposes to impose classical Bernstein condition on the noise $\\epsilon$ for the purpose of replacing uniform boundedness of $f_p,$ which may be difficult to satisfy in practice. Then, the paper expands traditional H\\\"older source condition to assumption 4 that covers more general cases. \n\nUnder the new and general assumptions, this paper first derives a minimax lower bound for the learning rate in Sobolev norms, which is the first to generalize current conclusions to both well-specified and mis-specified cases. Afterwards, it presents the sharp learning rates for general spectral algorithms with random projections (SARP), which recovers previous result for spectral algorithms without RP. Furthermore, it finds the optimal learning rates that up to a logarithmic factor of the minimax lower bounds. Finally, paper applies above theorem to three algorithms, including randomized sketches, plain and ALS Nystr\\\"om sub-sampling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is a solid work with in-depth theoretical proofs.\n\n2. This paper is the first to establish bounds under general conditions, which have not been fully solved by previous works. \n\n3. This work includes a large number of appendices to explain their findings, including Table 2, which provides a detailed comparison of various conditions."}, "weaknesses": {"value": "1. Although this paper is built on more general conditions, all bounds in this paper recover previous results without getting a tighter bound. This may weaken the contribution of this work.\n\n2. The lack of thorough discussion about specific cases of generalization of conditions (e.g. mis-specified setting and uniformly boundedness). Since one of the most important contribution of this paper is the generalization of conditions, a more thorough discussion about it can greatly demonstrate the contribution of this work. It's not very clear to me what new situations are covered in practice because of mis-specified setting and no uniformly boundedness on $f_p$.\n\n3. Lack of comparison about conditions with previous works. There are some other works also built on more general conditions according to Table 2. Moreover, this work introduces embedding as a condition, which is not required in most other works. Thus, the general condition of this work is in doubt and needs more discussions to prove."}, "questions": {"value": "1. Why projection dimension needed for optimality is proportional to the empirical effective dimension? The conclusion is not so straightforward to me based on current presentation.\n\n2. How to select appropriate projection dimension and regularization parameter in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C7zCRb46P4", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Reviewer_TgQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Reviewer_TgQw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984393749, "cdate": 1761984393749, "tmdate": 1762922832265, "mdate": 1762922832265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive theoretical analysis of Spectral Algorithms with Random Projections (SARP), establishing their minimax optimality under very general conditions, particularly in the common scenario where the true function may not belong to the model's hypothesis space (the \"mis-specified setting\")."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "By applying recent developments in spectral algorithms, the authors analyzed random projection methods and derived convergent results.\n\nThis is an interesting research direction that merits further investigation in future work."}, "weaknesses": {"value": "I did not check the entire proof. The paper would benefit from a more careful revision of the writing.\n\n1. Can you provide more examples such that assumption 2 holds?\n\n2. Could you be more careful on the assumption 3? It is ambiguous for non-expert.  \n\nThis is an interesting topic, however, it needs to be more careful on presentation."}, "questions": {"value": "Same to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEq04r6M4I", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Reviewer_DTFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Reviewer_DTFG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986805133, "cdate": 1761986805133, "tmdate": 1762922831881, "mdate": 1762922831881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To all Reviewers"}, "comment": {"value": "We sincerely thank all the reviewers for their insightful feedback and diligent evaluation of our paper. Their valuable input has significantly enriched the quality and clarity of our work. We have carefully considered all questions, concerns, and comments raised by the reviewers and provided detailed responses to each review separately. Our responses have been meticulously integrated into the revised manuscript, with particular attention given to the following key aspects:\n\n(1) We have added a more detailed description and intuitive example to Assumption 2 and 3 (*Reviewer DTFG*), have modified the form of Assumption 4 and added an additional Assumption 5 (*Reviewer i6nj*), have relaxed the compactness and Mercer assumptions (*Reviewers 3MZ4 i6nj*).\n\n(2) We have highlighted the contributions of this paper from the aspects of generalization of conditions (*Reviewer TgQw*), proof techniques (*Reviewers 3MZ4 i6nj*) and comparisons with the related work (*Reviewers TgQw 3MZ4 i6nj*); \n\n(3) We have removed the incorrect claim on the saturation effect and clarified that this does not affect our main results and contributions (*Reviewers 3MZ4 i6nj*), have corrected the description on the lower bound and examples of benign RKHSs (*Reviewer i6nj*);\n\n(4) We have expanded the theoretical and empirical discussion of the projection dimension \n$m$ and the choice of the regularization parameter (Reviewers TgQw, 3MZ4);\n\n(5) We have added a detailed proof of the general source condition and the key operator inequality, and improved the paper by correcting typos, clarifying notation, and outlining several promising directions for future work (Reviewer i6nj).\n\n**Revised passages are marked in red, and the important parts we wish to emphasize are highlighted in blue in the updated manuscript.** \n\nOnce again, we extend our heartfelt gratitude for your time, expertise, and contribution to our work."}}, "id": "V46k9lt1cy", "forum": "armP3O3eoV", "replyto": "armP3O3eoV", "signatures": ["ICLR.cc/2026/Conference/Submission11812/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11812/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission11812/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763445864938, "cdate": 1763445864938, "tmdate": 1763445864938, "mdate": 1763445864938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}