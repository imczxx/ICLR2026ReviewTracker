{"id": "upUl6hMYwy", "number": 1286, "cdate": 1756869033708, "mdate": 1759898217288, "content": {"title": "UniHand: A Unified Model for Diverse Controlled 4D Hand Motion Modeling", "abstract": "Hand motion plays a central role in human interaction, yet modeling realistic 4D hand motion (*i.e.*, 3D hand pose sequences over time) remains challenging. \nResearch in this area is typically divided into two tasks: \n(1) Estimation approaches reconstruct precise motion from visual observations, but often fail under hand occlusion or absence; \n(2) Generation approaches focus on infilling motion from incomplete sequences and synthesizing diverse motions from inputs such as object priors. \nHowever, this separation not only limits the effective use of heterogeneous condition signals that frequently arise in practice, but also prevents knowledge transfer between the two tasks.\nWe present **UniHand**, a unified diffusion-based framework that formulates both estimation and generation as conditional motion synthesis. \nUniHand integrates heterogeneous inputs by embedding structured signals into a shared latent space through a joint variational autoencoder, which aligns conditions such as MANO parameters and 2D skeletons.\nVisual observations are encoded with a frozen vision backbone, while a dedicated hand perceptron extracts hand-specific cues directly from image features, removing the need for complex detection and cropping pipelines.\nA latent diffusion model then synthesizes consistent motion sequences from these diverse conditions.\nExtensive experiments across multiple benchmarks demonstrate that UniHand delivers robust and accurate hand motion modeling, maintaining performance under severe occlusions and temporally incomplete inputs.", "tldr": "", "keywords": ["Hand Motion Modeling", "Diffusion Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/947fa6cfa748004fe6f6cfcafbc327a5871d6b61.pdf", "supplementary_material": "/attachment/27e18ba50f3f9eca511939f68e4f4f989bcf963e.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents UniHand, a unified diffusion-based framework for 4D hand motion modeling that jointly addresses motion reconstruction and generation. It reformulates both tasks as a conditional motion synthesis problem, bridging the gap between estimation and generation. A Joint Variational Autoencoder (Joint VAE) aligns heterogeneous conditional inputs such as 2D and 3D keypoints and MANO parameters into a shared latent space, enabling robust modeling under incomplete or inconsistent data. The Hand Perceptron module extracts hand-specific features directly from full visual inputs without requiring explicit detection or cropping, while a canonical coordinate system ensures spatial and temporal consistency under dynamic camera settings. Experiments on DexYCB, HO3D, and HOT3D datasets demonstrate that UniHand achieves state-of-the-art results, especially under severe occlusion or partial input, validating its effectiveness and generalization for unified hand motion estimation and generation. However, despite its strong performance, the method suffers from algorithmic complexity and high computational cost, and the exploration of the unique challenges in multi-modal alignment for 4D hand pose estimation remains insufficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a unified diffusion-based framework that integrates both estimation and generation for 4D hand motion modeling, offering a fresh formulation of conditional motion synthesis that extends beyond task-specific designs.\n- The technical design, including the Joint VAE and Hand Perceptron modules, is well-motivated and validated through comprehensive experiments on multiple datasets, showing robustness under occlusion and dynamic camera motion.\n- The paper is clearly written and systematically structured."}, "weaknesses": {"value": "- Real-world deployment may be limited without efficient preprocessing of input modalities.\n- Heavy computational and data requirements for training."}, "questions": {"value": "- While UniHand demonstrates strong performance, the diffusion-based generation pipeline and multimodal latent alignment introduce high computational cost. Can the authors quantify the training and inference time compared to existing methods, and discuss possible simplifications or acceleration strategies?\n- The paper acknowledges that UniHand struggles to maintain globally consistent trajectories under large camera movements due to the lack of explicit camera extrinsics. Could the authors elaborate on potential ways to address thisâ€”e.g., by incorporating implicit camera modeling or scene-aware constraints?\n- Although UniHand integrates visual, 2D, and 3D conditions through a joint VAE, the paper could further investigate how multimodal alignment contributes to 4D hand pose estimation quality. Would ablation or visualization of the latent space help clarify how modalities interact and which provide the most benefit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3E6d4GWGDC", "forum": "upUl6hMYwy", "replyto": "upUl6hMYwy", "signatures": ["ICLR.cc/2026/Conference/Submission1286/Reviewer_4cS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1286/Reviewer_4cS2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760523167300, "cdate": 1760523167300, "tmdate": 1762915727031, "mdate": 1762915727031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to unify both hand pose estimation and generation using a single diffusion-based framework.\nThe proposed model, UniHand, handles both structured MANO and key points (as conditions for generation), as well as images (for estimation). \nSeparate encoders are used to bring different hand conditions into the same latent space, then the method runs denoising diffusion process in the latent space, aiming to to produce smooth 4D motion output."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well motivated: it aims to bring the estimation and the generation model together. \nThe proposed model indeed can do these two task in a unified way. \nThis paper does not propose a new problem formulation, but the effort to introduce an unified solution for two problems are interesting and valuable.\n\nThe writing is clear in general: we can understand how the proposed framework achieve the proposed goal at the high level."}, "weaknesses": {"value": "The main issue of this paper is it does not address the proposed goal: unifying both estimation *and generation*. The generation ability of the framework is not tested/reported. \nThe paper is motivated by unifying the estimation and generation into the same framework. The motivation is sound, and the de-noiser in the framework is indeed a generative model. However, all results are reported **only on the hand pose estimation task**. \nThe sole focus of estimation deviates from what is described in the title, abstract and intro, as well as in the related works where the authors talk about unifying generation under their framework in Line 134-136.\nThe authors could have shown their results on any generation problems, e.g. [Zuo et al., 2023] in Line 128 or [Zuo et al., 2024] in Line 115-116 of the grasp generation problem. I understand that extending the system for these generation tasks can involves much more work, but since the authors claims the unified method, one would expect the generation task results.\n\nIn Line 347-348, the authors proposes to evaluate the generation via estimation:\n> Hand pose estimation in the camera coordinate space provides the most direct way to evaluate the quality of motion generation conditioned on visual observations.\n\nbut above is saying that the estimation and the generation are the same task, which contracts what the authors say in Line 013 and throughout the whole introduction!\n\nThe author should really test their framework on a few tasks listed in their Sec 2.2, without the results on the generation task, otherwise this paper gives a wrong impression to the audience. \n\nRegarding the estimation evaluation setup, it is unclear to me when $c_{2D}$ and $c_{3D}$ are provided to the model? In my understanding, the standard estimation task setup does not provide $c_{2D}$ and $c_{3D}$ to the model.\n\nSince providing experiment results on the real generation task will lead to substantial changes to the paper and is beyond what can be done during rebuttal, I suggest rejection."}, "questions": {"value": "Apart from the mismatch between what is claimed and what is experimented, the writing of this paper is good in general.\n\nThe points that is unclear:\n\n1. Figure 1, bottom left, what are two red boxes represent at the bottom left? are they $g$?\n\n2. Shouldn't motion encoder  be called hand pose encoder? \n\n3. Line 207-208, better to say \"At each _autoregression_ step\" instead of simply \"At each step\", which can mean diffusion step.\n\n4. Line 883, DeepSpeeds need reference.\n\n5. Line 118, EASY-HOI is not relevant to Hand Motion Generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8UehyGZH4d", "forum": "upUl6hMYwy", "replyto": "upUl6hMYwy", "signatures": ["ICLR.cc/2026/Conference/Submission1286/Reviewer_LJ9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1286/Reviewer_LJ9z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679888749, "cdate": 1761679888749, "tmdate": 1762915726919, "mdate": 1762915726919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work seeks to unify the hitherto distinct domains of hand motion generation and hand motion reconstruction. This is achieved by a variational autoencoder embedding various non-visual modalities (hand trajectories, human body keypoints) into a shared latent space. In parallel, a hand perceptron extracts frame-wise features from visual inputs. The extracted non-visual latent representations are then used by a latent diffusion model to generate the hand pose sequence, with visually informative features being provided at every layer of the motion denoiser. The work compares the proposed method with multiple hand motion generation and reconstruction baselines on three datasets, consistently outperforming them."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method shines when evaluated against numerous baselines, even in the presence of significant occlusion (Table 1). It is able to handle multimodal conditioning input, making it flexible and able to benefit from various types of known information at inference time.\nThe proposed method is thoroughly ablated with respect to its components and possible input modalities.\nThe work includes an honest discussion of its limitations."}, "weaknesses": {"value": "The submission could benefit from more qualitative examples in the supplementary material. This is especially relevant for generative models."}, "questions": {"value": "How does the method differentiate between the left and the right hand in its output?\nHow does the method perform in the presence of feet, which are often misdetected as hands in egocentric videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HXiwurDvGX", "forum": "upUl6hMYwy", "replyto": "upUl6hMYwy", "signatures": ["ICLR.cc/2026/Conference/Submission1286/Reviewer_76ft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1286/Reviewer_76ft"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985977446, "cdate": 1761985977446, "tmdate": 1762915726816, "mdate": 1762915726816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}