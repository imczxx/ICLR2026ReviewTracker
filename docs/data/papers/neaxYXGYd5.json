{"id": "neaxYXGYd5", "number": 7881, "cdate": 1758040719997, "mdate": 1759897824533, "content": {"title": "DPQuant: Efficient and Private Model Training via Dynamic Quantization Scheduling", "abstract": "Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present DPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to $2.21\\times$ theoretical throughput improvements on low‑precision hardware, with less than 2% drop in validation accuracy.", "tldr": "", "keywords": ["differential privacy", "quantization"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20b94aa8f108606e1f83e0858b68e13ac074084e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates dynamic quantization for differentially private SGD. The authors propose a specific method that converts specific layers into a low-precision format and design a selection criterion based on the difference between the quantized and non-quantized layers. They empirically validated the strength of their dynamic quantization approach across various settings, including different configurations and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "•  The paper investigates the quantization of differentially private deep learning.\n\n•  The authors provide some basic analysis of why DP training interacts poorly with quantization.\n\n•  Based on their observations, the authors’ new training method enables faster training compared to FP32 training."}, "weaknesses": {"value": "Please refer to the Questions section."}, "questions": {"value": "•\tIn the related work section, the authors mention that all existing methods are orthogonal to this work. However, for readers who are not familiar with quantization, it is hard to understand the relationship between these methods and the authors' work. The authors need to improve their related work section. Maybe including more on DP network pruning or layer selection, such as [1]. \n\n[1] Differential Privacy Meets Neural Network Pruning, arXiv:2303.04612\n\n•\tThe reviewer is not convinced why the authors shift their perspective from the L2 norm (for noise addition) to the L-inf norm (for analysis). Can the authors clarify this reason or explain why the L2 norm is not sufficient for their analysis? As far as I understand, Figure 1 does not support the reason for the L-inf analysis, and the empirical part uses the L2 norm.\n\n•\tDoes the quantization method meet the properties in Proposition 1, such as the unbiased and scale-invariant properties?\n\n•\tIf a low-precision format induces the DP accuracy drop, can using higher precision mitigate the accuracy drop in DP training?\n\n•\tWhen quantizing, is there any difference between floating-point and fixed-point quantization in terms of differential privacy?\n\n•\tThe authors should carefully check the paper for readability. For example, the GTSRB dataset is not explained or cited when it is first used. And the indicator function in Eq. 1 should be clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Byht92WIi7", "forum": "neaxYXGYd5", "replyto": "neaxYXGYd5", "signatures": ["ICLR.cc/2026/Conference/Submission7881/Reviewer_ieCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7881/Reviewer_ieCQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641874110, "cdate": 1761641874110, "tmdate": 1762919916915, "mdate": 1762919916915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on differential privacy (DP) and the impact of quantization variance. The authors hypothesize that quantization variance leads to disproportionately large accuracy degradation in DP settings. To address this issue, they propose DPQUANT, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. The main procedure consists of (i) probabilistic sampling of layers and (ii) loss-aware layer prioritization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The mathematical analysis is clearly presented, and the theoretical treatment of differential privacy in the proposed algorithm, such as the post-processing of the update exponential moving average (EMA), appears sound. The empirical settings are well constructed, and the figures and tables are clear and easy to interpret. Compared with vanilla DP-SGD, the proposed method demonstrates meaningful performance improvements."}, "weaknesses": {"value": "However, the related work and comparison baselines raise significant concerns. Quantization is a widely used technique across various applications, and it is natural to consider its potential benefits within differential privacy. Indeed, several prior works have explored the interplay between quantization and DP, including:\n- Kang, Tianqu, et al. \"The effect of quantization in federated learning: a rényi differential privacy perspective.\" 2024 IEEE International Mediterranean Conference on Communications and Networking (MeditCom). IEEE, 2024.\n- Youn, Yeojoon, et al. \"Randomized quantization is all you need for differential privacy in federated learning.\" arXiv preprint arXiv:2306.11913 (2023).\n- Kim, Muah, Onur Günlü, and Rafael F. Schaefer. \"Effects of quantization on federated learning with local differential privacy.\" GLOBECOM 2022-2022 IEEE Global Communications Conference. IEEE, 2022.\n- Wang, Yongqiang, and Tamer Başar. \"Quantization enabled privacy protection in decentralized stochastic optimization.\" IEEE Transactions on Automatic Control 68.7 (2022): 4038-4052.\n-Xiong, Sijie, Anand D. Sarwate, and Narayan B. Mandayam. \"Randomized requantization with local differential privacy.\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.\nWhile some of these works focus on federated learning, the authors should still acknowledge them and discuss how their method relates to or differs from these approaches.\n\nBeyond these, it is also important to consider recent advances in DP optimization methods beyond DP-SGD. Since DP-SGD is known to suffer from excessive noise and slow convergence, relying solely on it as a baseline is insufficient to demonstrate the full benefit of the proposed approach. Several improved variants could be relevant comparisons:\n- Wang, Zihao, et al. \"{DPAdapter}: Improving Differentially Private Deep Learning through Noise Tolerance Pre-training.\" 33rd USENIX Security Symposium (USENIX Security 24). 2024.\n- Wei, Jianxin, et al. \"Dpis: An enhanced mechanism for differentially private sgd with importance sampling.\" Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. 2022.\n- Park, Jinseong, et al. \"Differentially private sharpness-aware training.\" International Conference on Machine Learning. PMLR, 2023.\n- Denisov, Sergey, et al. \"Improved differential privacy for sgd via optimal private linear operators on adaptive streams.\" Advances in Neural Information Processing Systems 35 (2022): 5910-5924.\n\nIn summary, while the proposed method is technically sound and the presentation is clear, the related work section and choice of baselines need substantial improvement to strengthen the novelty and empirical validity of this paper.\n\nIf these concerns are properly addressed, I would be pleased to raise my evaluation score."}, "questions": {"value": "It is unclear whether the Exponential Moving Average (EMA) update plays an essential role in the proposed framework. While the paper mentions EMA as part of the post-processing step for maintaining differential privacy, there is no ablation study isolating its effect. It would strengthen the work to report performance both with and without EMA, or to justify why EMA is necessary for stability or privacy preservation in DPQUANT.\nFurthermore, it would be helpful to discuss whether other optimization or averaging techniques (e.g., momentum-based methods, adaptive optimizers such as Adam or Adagrad, or other moving-average variants) could serve a similar role. Clarifying whether the proposed approach is tightly coupled with EMA or can generalize to other optimizers would provide deeper insight into the method’s robustness and applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oZo7IEwGRg", "forum": "neaxYXGYd5", "replyto": "neaxYXGYd5", "signatures": ["ICLR.cc/2026/Conference/Submission7881/Reviewer_ixjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7881/Reviewer_ixjt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983263889, "cdate": 1761983263889, "tmdate": 1762919914932, "mdate": 1762919914932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies methods to reduce quantization variance in DP-SGD, which slows down the convergence a lot. Technical efforts are dedicated to achieve good performance, maintain DP guarantee, and avoid extra computation. A number of experiments on computer vision and one language modeling are presented on relatively old architectures, showing a near Pareto-optimal utility-compute tradeoffs and about 2X theoretical speed improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall the paper is clearly written and shows the originality in combining quantization and DP. The problem of DP training efficiency is an important one, highlighting the significance of this work. The experiments are careful and convincing, especially the ablation study. I enjoy reading the details in this paper, such as Pareto optimality and error bars."}, "weaknesses": {"value": "1. Wrong positioning of mixed-precision training\n\nThe authors state \"While effective for standard SGD, mixed-precision training degrades significantly under DP-SGD. To our knowledge, no prior work has explored mixed-precision training when differential privacy mechanisms are employed.\" Both sentences are wrong. Mixed-precision training with DP is comparable to DP-SGD with full-precision. See Appendix T of \"https://arxiv.org/pdf/2110.05679\" (no code though) and Section 3.4/Appendix C of \"https://arxiv.org/pdf/2311.11822\" (code is open-sourced). The caveat is the loss scaling when using fp16, and DP training with bf16 can just run without loss scaling.\n\n2. Limitation to DP-SGD\n\nI recommend the authors to analyze DP-Adam or DP-SignSGD, where the gradient norm is always \\sqrt{d} and line 230 won't hold. How would you extend your analysis in this case? Also, DP-Adam and other adaptive optimizers are the main-stream for large models. Can your method work directly? What will be the performance of quantization empirically?\n\n3. Limitation to training from scratch\n\nDP pre-training is hard. I think this is why the authors stick to small and toy datasets. However, the methodology should be applicable to small-scale finetuning as well. Did the authors try that?\n\n4. What about non-DP?\n\nThe methods in Section 5.1 and 5.2 should also improve non-DP quantization, even though additional care was taken to make it DP. Are these methods novel? Are there evidence these methods also work in non-DP scenario?\n\n5. Not practical for throughput\n\nAnother minor weakness (and my review score does not depend on this point) is that the improvement of throughput is theoretical, not applicable in practice."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AEPYaaT4w2", "forum": "neaxYXGYd5", "replyto": "neaxYXGYd5", "signatures": ["ICLR.cc/2026/Conference/Submission7881/Reviewer_Bdkn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7881/Reviewer_Bdkn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054432967, "cdate": 1762054432967, "tmdate": 1762919914407, "mdate": 1762919914407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper observes that quantization variance is detrimental in DP-SGD. The authors propose two methods to reduce the variance, which achieve good Pareto-optimal performance when used in combination. Specifically, the loss-aware method takes care of DP guarantee. Some experiments on small datasets and models show a good utility-compute tradeoffs while leveraging the quantization speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the identification of the effects of quantization variance is novel, and the solutions to reduce the variance is novel. Hence this work has fair significance in improving DP efficiency. The motivation of the solutions is reasonable, as variance in DP noise is already known as the key blocker in DP convergence like \"Pre-training Differentially Private Models with Limited Public Data\". Hence it is not surprising another type of variance from quantization also harms the convergence."}, "weaknesses": {"value": "In practice, especially in NLP, models are optimized by adaptive optimizers like AdamW or Muon, whereas this work has a narrow focus on DP-SGD (maybe not even with momentum?) I would encourage the authors to put DP-AdamW in empirical experiments which should not require extra efforts.\n\nIn \"Pre-training Differentially Private Models with Limited Public Data\", it is claimed that variance only harms the early stage of training, e.g. in pretraining regime, to which this paper's experiments are limited. In fact, many existing works including \"Large Language Models Can Be Strong Differentially Private Learners\" have shown that DP finetuning is comparable to non-DP finetuning, suggesting variance is less detrimental in the finetuning regime. I wonder would DPQuant be less effective in this regime?\n\nAlso please use a different notation for policy. Right now both policy and probability is p."}, "questions": {"value": "Can the authors add DP-AdamW at least for NLP experiments?\n\nDoes non-DP quantization works better in finetuning? like comparing fp4 with quantization to fp32 without quantization, will the gap in pretraining be larger than in finetuning?\n\nHow necessary is DP quantization in finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PGghHWPQPs", "forum": "neaxYXGYd5", "replyto": "neaxYXGYd5", "signatures": ["ICLR.cc/2026/Conference/Submission7881/Reviewer_oPEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7881/Reviewer_oPEM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112212717, "cdate": 1762112212717, "tmdate": 1762919913553, "mdate": 1762919913553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}