{"id": "DKvfjlXDJt", "number": 13437, "cdate": 1758217849073, "mdate": 1759897437657, "content": {"title": "BenchName: a Set of Benchmarks for Long-Context Code Models", "abstract": "The fields of code and natural language processing are evolving rapidly, with models becoming better at processing long context windows --- supported context sizes have increased by orders of magnitude over the last few years. However, there is a shortage of comprehensive benchmarks for code processing that go beyond a single file of context, while the most popular ones are limited to a single method. With this work, we aim to close this gap by introducing BenchName, a suite of six benchmarks for code processing tasks that require project-wide context. These tasks cover different aspects of code processing: library-based code generation, CI builds repair, project-level code completion, commit message generation, bug localization, and module summarization. For each task, we provide a manually verified dataset for testing, an evaluation suite, and open-source baseline solutions based on popular LLMs to showcase the usage of the dataset and to simplify adoption by other researchers. We publish the benchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace Hub for all the datasets, and link to the GitHub repository with baselines are available in the manuscript.", "tldr": "Collection of benchmarks for code processing models working with project-level context", "keywords": ["code generation", "code completion", "code summarization", "bug localization", "diff summarization", "evaluation", "datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1bca2cd10b53815367a7579030335a501978d89.pdf", "supplementary_material": "/attachment/db3b62e724af0a842bb406f83e2814767707b1aa.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents BenchName, a suite of six benchmarks designed to evaluate long-context code models on realistic software-engineering tasks that require repository-level reasoning rather than single-file snippets. The tasks span library-based code generation, CI build repair, project-level code completion, commit message generation, bug localization, and module summarization, each derived from high-quality open-source repositories. The authors provide detailed metrics, evaluation pipelines, and strong baselines from both open and proprietary LLMs, showing that the tasks are not saturated and expose meaningful gaps in model capabilities. Results highlight that many large models under-utilize long contexts. The benchmark correlates across tasks but retains complementary structure, supporting its use as a composite evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The benchmark set covers a wide range of six tasks in code domain, representing common applications of code LLMs.\n* The paper presents sufficient details about data sourcing, problem consturction, and evaluation metrics for each of the tasks.\n* Experiments are conducted over a comprehensive collection of both open-source and proprietary LLMs."}, "weaknesses": {"value": "* I'm not fully convinced of the motivation part as stated in L048-057. Evaluating with SWE-Bench tasks in the agentless mode [1], i.e., gathering relevant context offline and prompting the model once to generate the code patch, seems a good alternative that simultaneously avoids multi-turn conversations and preserves the order of software development.\n\n* Regarding the lib-based code generation task, first, the task by itself does not necessarily require additional cross-file context, because the libraries involved are likely seen during model training, and thus, the relevant context can be implicitly retrieved from models' parametric knowledge. One cannot disentangle memorization from utilization of long context when interpreting the evaluation results. Second, traditional BM25 over file chunks might be a better way than the proposed API lists for constructing additional context. Third, the proposed metric only checks API names but overlooks the arguments or the functional correctness of the entire script. \n\n* Regarding the project-level code completion task, while it is claimed that the proposed benchmark respects the order of software development by leveraging commits, the subsampling of lines for completion still exposes part of the new code in the target commit. Besides, feeding full file context to the prompt likely performs worse than the traditional chunk-based BM25 retrieval. \n\n\n\nReferences:\n\n[1] Xia, Chunqiu Steven, et al. \"Agentless: Demystifying llm-based software engineering agents.\" arXiv preprint arXiv:2407.01489 (2024)."}, "questions": {"value": "* In the evaluation of project-level code completion, if subsampled lines for completion are consecutive, how do you match each generated line to groundtruth? If the model generates an extra line at the beginning, will that impact all subsequent lines due to mismatches of line numbers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1DLm4qNwa0", "forum": "DKvfjlXDJt", "replyto": "DKvfjlXDJt", "signatures": ["ICLR.cc/2026/Conference/Submission13437/Reviewer_XwV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13437/Reviewer_XwV3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761076474169, "cdate": 1761076474169, "tmdate": 1762924063235, "mdate": 1762924063235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BenchName, a six-task benchmark suite for ML4SE that explicitly requires module- to repository-level context while avoiding multi-step agent orchestration. Tasks span library-based code generation, CI build repair, project-level code completion, commit-message generation, bug localization, and module summarization. Datasets are rigorously filtered and manually verified, accompanied by baseline implementations and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ This paper addresses a timely and relevant problem, i.e., code generation in long-context scenarios, and presents a carefully curated benchmark suite for its evaluation. The problem is important, and as no standard benchmark currently exists, the paper has strong practical implications.\n+ The benchmark construction applies rigorous corpus filtering and human verification to ensure high-quality data across the six investigated tasks.\n+ Extensive experiments are conducted on 19 LLMs, yielding several insightful findings and practical recommendations for future research."}, "weaknesses": {"value": "- Although challenging, it would strengthen the paper to incorporate functionally correct metrics (e.g., pass@1) rather than relying solely on lexical similarity measures in the evaluated tasks.\n- The authors are encouraged to expand Section 3 to include more in-depth analytical studies.\n- Most tasks are Python-centric; including additional programming languages such as C and Java would improve the benchmark’s generality and appeal."}, "questions": {"value": "- The reviewer is curious whether most of the content in the provided dataset is actually relevant to the completion point. If not, could these long contexts simply be noise that the model should ignore during reasoning? This phenomenon seems analogous to retrieval-augmented generation (RAG)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GePhSwAxzv", "forum": "DKvfjlXDJt", "replyto": "DKvfjlXDJt", "signatures": ["ICLR.cc/2026/Conference/Submission13437/Reviewer_fL9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13437/Reviewer_fL9H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372146340, "cdate": 1761372146340, "tmdate": 1762924062870, "mdate": 1762924062870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces “BenchName”, a set of six benchmarks designed to evaluate the long-context processing capabilities of code LLMs. The evaluation on a wide range of SOTA models reveals that the tasks are still challenging for current models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely topic. The paper addresses a timely problem, i.e., long-context code processing evaluation. Tasks like HumanEval and MBPP cannot evaluate the long-context capability, while benchmarks like SWEBench are too complex and not fine-grained enough. The community does need benchmarks to evaluate specialized, fine-grained long-context code comprehension capabilities.\n2. Diversity and practical usefulness of tasks. The benchmark includes six distinct tasks: library-based code generation, project-level code completion, CI build repair, commit message generation, bug localization, and module summarization. The tasks are diverse and can evaluate long-context code processing capabilities from various aspects. Plus, the tasks are prevalent in software development practice.\n3. Non-trivial efforts in benchmark construction. The manual filtering and verification of datasets are crucial for the quality of the benchmark."}, "weaknesses": {"value": "1. Limited language scope. The benchmarks are heavily focused on Python. While bug localization includes Java and Kotlin, the two most detailed tasks in the main paper (library based generation and project-level completion) are Python-only.\n2. Effectiveness of library-based code generation evaluation metrics. The recall of appearance of API calls in the ground truth solution that also appear in the generated program is not a convincing metric for evaluating library-based code generation. \n3. Effectiveness of repo-level code completion capability. The exact match and perplexity metrics are not convincing enough to represent repo-level code completion capability.\n4. The description of bug localization evaluation is not clear.\n5. Using a small model as the LLM-as-judge assessor for module summarization evaluation."}, "questions": {"value": "1. Why only Python projects are included in the benchmark (except bug localization)? It’s known that for tasks like CI-build error repair, repositories often use languages like TypeScript, Java, Go, JavaScript etc. in addition to Python.\n2. What’s the correlation between the appearance of API calls in the ground truth solution that also appear in the generated program and the quality of library-based code generation? Why the correctness of code generation is not evaluated? Although the program is not always executable, at least some efforts should be invested to evaluate correctness[1].\n3. Why use exact match and perplexity metrics for repo-level code completion evaluation, instead of using correctness metrics?\n4. What’s the reason of using a smaller model (Mistral 7B) as the assessor for LLM-as-judge? Is the perplexity score w.r.t. to a small model a meaningful evaluation metric?\n\nminor:\n5. Figure 1: 0,89=>0.89\n6. Is the name of the benchmark a placeholder or it's the final name? If it's a placeholder please fix it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FeDuUTVAPu", "forum": "DKvfjlXDJt", "replyto": "DKvfjlXDJt", "signatures": ["ICLR.cc/2026/Conference/Submission13437/Reviewer_LFit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13437/Reviewer_LFit"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952876998, "cdate": 1761952876998, "tmdate": 1762924061811, "mdate": 1762924061811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BenchName packs six tasks testing LLMs on code that needs whole-project context. Old suites stick to short snippets; this one targets real, long-context hurdles. Every task ships with a checked dataset, eval harness, and open baselines. Two tasks get full write-ups in the paper, the rest in the appendix. OpenAI o1-style reasoners lead the pack. Data and code are open."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a clear and significant gap in the ML4SE landscape. As LLMs' context windows expand, there is a pressing need for benchmarks that can rigorously evaluate their ability to process and reason over entire codebases. BenchName is a direct and substantial contribution to this area.\n\nThe authors demonstrate a strong commitment to data quality. The emphasis on manual verification, rigorous filtering pipelines, and thoughtful data collection strategies is commendable. A key example is the project-level code completion task, where using Git history to create repository snapshots prevents a common and critical data leakage issue found in other benchmarks.\n\nThe paper evaluates a wide and very current range of models, including proprietary leaders , open-source models and specialized \"reasoning\" models. The experiments are well-designed, exploring the impact of context size and different context composition strategies, which provides valuable insights into current model capabilities."}, "weaknesses": {"value": "The metrics are only a first cut, and their limits show. API Recall in the library-generation task counts correct calls yet never docks hallucinated ones nor runs the code for pass@k. CompScore leans on Mistral-7B as judge; authors curb positional bias, but verbosity and style preferences remain unmeasured, and human agreement is unchecked.\n\nSpace constraints leave four of the six benchmarks detailed solely in the appendix. While understandable, this fragmentation makes the main paper feel incomplete and forces readers to shuttle back and forth between the main text and supplements to grasp the full contribution."}, "questions": {"value": "1. Why was API Recall chosen as the sole metric? Has API Precision or F1 been included to capture both correct and hallucinated calls? More importantly, can the generated code be executed? An execution-based measure such as pass@k would assess correctness far more reliably than mere API usage.\n2. The benchmark uses an oracle to provide the model with the exact files and code blocks that need to be changed. This simplifies the task significantly. What is the performance drop when this oracle is removed? \n3. Could you please clarify precisely how the path distance is calculated? Have you considered comparing it against a simple semantic retrieval baseline for context composition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KEyXK0T3pd", "forum": "DKvfjlXDJt", "replyto": "DKvfjlXDJt", "signatures": ["ICLR.cc/2026/Conference/Submission13437/Reviewer_kuz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13437/Reviewer_kuz4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973089095, "cdate": 1761973089095, "tmdate": 1762924061204, "mdate": 1762924061204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}