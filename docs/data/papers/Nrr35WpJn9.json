{"id": "Nrr35WpJn9", "number": 15386, "cdate": 1758250853090, "mdate": 1759897310207, "content": {"title": "AutoRAN: Automated Hijacking of Safety Reasoning in Large Reasoning Models", "abstract": "This paper presents AutoRAN, the first framework to automate the hijacking of internal safety reasoning in large reasoning models (LRMs). At its core, AutoRAN pioneers an execution simulation paradigm that leverages a weaker but less-aligned model to simulate execution reasoning for initial hijacking attempts and iteratively refine attacks by exploiting reasoning patterns leaked through the target LRM's refusals. This approach steers the target model to bypass its own safety guardrails and elaborate on harmful instructions.\nWe evaluate AutoRAN against state-of-the-art LRMs, including GPT-o3/o4-mini and Gemini-2.5-Flash, across multiple benchmarks (AdvBench, HarmBench, and StrongReject). Results show that AutoRAN achieves approaching 100% success rate within one or few turns, effectively neutralizing reasoning-based defenses even when evaluated by robustly aligned external models.\nThis work reveals that the transparency of the reasoning process itself creates a critical and exploitable attack surface, highlighting the urgent need for new defenses that protect models' reasoning traces rather than merely their final outputs.", "tldr": "This paper presents AutoRAN, an automated framework that hijacks the safety reasoning of large reasoning models by simulating execution traces and exploiting the reasoning exposed in their refusals.", "keywords": ["Large Reasoning Models", "Automated Red Teaming", "Adversarial Attacks", "Jailbreaking", "LLM Security", "Reasoning Hijacking", "Safety Alignment", "Execution Simulation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/385baf2ba5fa28dcb9546a081ed87c65513b1a66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Their paper proposes an novel framework AutoRAN. AutoRAN introduces an execution simulation paradigm that utilizes a weaker yet less-aligned model to simulate reasoning during initial hijacking attempts. It then iteratively refines the attacks by exploiting the reasoning patterns revealed through the target LRM’s refusal responses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a new framework AutoRAN, which is the first framework for automated reasoning hijacking in LRMs.  \nThis framework systematically probes two complementary attack surfaces in LRMs. Execution Hijacking, where initial\nprompts trigger task-execution mode bypassing safety checks, and Targeted Refinement, where the attack adaptively improves by analyzing reasoning exposed in refusals."}, "weaknesses": {"value": "The AutoRAN framework involves three main steps: (1) prompt initialization, (2) prompt refinement, which iteratively adjusts candidate prompts to exploit weaknesses in the target model’s safety reasoning, and (3) response evaluation. \nWhile the pipeline is well-engineered, the overall approach appears highly implementation-driven. As a research paper, it would benefit from stronger theoretical grounding or analysis to support and generalize its findings."}, "questions": {"value": "1. For the evaluation part, the evaluation model is using Qwen3-8B-abliterated, will the selection of evaluation model affect the results? Is it reasonable to try multiple models or calculate the average of several models? \n2. In the prompt refinement part, how to decide the number of iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JhdwDoClEW", "forum": "Nrr35WpJn9", "replyto": "Nrr35WpJn9", "signatures": ["ICLR.cc/2026/Conference/Submission15386/Reviewer_qLTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15386/Reviewer_qLTF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760548307953, "cdate": 1760548307953, "tmdate": 1762925669716, "mdate": 1762925669716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AutoRAN is an automated jailbreak attack for black-box reasoning models that leverages the target model’s own exposed reasoning structure to elicit harmful behavior. It borrows the framework of iterated attacks for instruction-tuned models such as PAIR and adapts it directly to hijacking reasoning traces by incorporating information extracted from the reasoning trace into the attacker model. The results of AutoRAN show that it can break state-of-the-art black-box models such as gpt-o3-mini to 100% Attack Success Rate. Furthermore, ablations suggest that leveraging exposed reasoning can break most models in a single-turn, be more effective than H-CoT and MouseTrap, and is more cost-effective."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "AutoRAN is timely because there are only few published works on attacks that directly target reasoning traces. The authors show through ablations that AutoRAN is more cost efficient compared to other attacks that target LRMs (H-CoT, MouseTrap) as shown in Tables 8 and 9. Furthermore, it can elicit harmful behavior from most prompts within a single iteration."}, "weaknesses": {"value": "1. The threat model is not sufficiently explained. After reading the methodology, it seems that AutoRAN is built on the assumption that the attacker only has access to $\\textit{safe}$ reasoning traces from the target model. This setting only applies to production models such as GPT and Claude which hide the harmful content of the reasoning. I think this should be explained more clearly in the abstract and threat model. Mentioning this as having “black-box access” (line 147) does not seem sufficient because the definition of black-box vs. white-box for reasoning models is not clear.\n2. There is no explanation of how $\\texttt{SimulateReasoning}$ works even though it seems like a crucial aspect of AutoRAN. In Algorithm 1, it only shows that it takes as input the query $q$. How does it simulate or imitate the high-level reasoning structure of the target model with information on only the query?\n3. The threat model assumes that the attacker has access to intermediate reasoning traces ($p_i$) from the target model. However, the judge only classifies a jailbreak based on the answer ($y_i$). I believe it is more reasonable for the judge to also take as input $p_i$ because if the reasoning trace contains harmful information, it means the attacker has also obtained the information. If this is not the case because the threat model assumes a setting where $p_i$ is always safe, it would be helpful to clarify this point.\n4. Appendix D shows that the CoT generated by the attacker model already contains harmful information. This suggests that (1) the attacker already obtained the harmful behavior/information they were targeting even before attacking the target model and (2) the performance of AutoRAN could come from extracting information out of an abliterated attack model rather than the target model. The harmfulness of the CoT generated by the attacker model should be evaluated and quantitatively shown that it is safe. Otherwise, an attacker would have no need to attack the target model, making the threat model and attack pipeline setting paradoxical.\n5. Comparing o4-mini in Table 1 to other models would make the claim in 299-303 stronger. Case analysis on just o4-mini does not reveal why o4-mini requires more iterations than other models."}, "questions": {"value": "Additional Comment:\n\n1. I think tables 8 and 9 should be moved up into the main body instead of Table 3 as it sufficiently shows that AutoRAN is more cost effective than other methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2cuUAja7ud", "forum": "Nrr35WpJn9", "replyto": "Nrr35WpJn9", "signatures": ["ICLR.cc/2026/Conference/Submission15386/Reviewer_fu94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15386/Reviewer_fu94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760916420695, "cdate": 1760916420695, "tmdate": 1762925668982, "mdate": 1762925668982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large language models, through reinforcement learning on chain-of-thoughts to perform complex tasks, have demonstrated remarkable improvement in reasoning like math or coding problems. However, this performance improvement also comes with safety concerns, since these more capable models are in general also more capable of causing harm. Moreover, these models often use reasoning as a mechanism for safety – hence studying the newer safety risks that come with reasoning is a useful problem.\n\nThe paper introduces an automated jailbreaking method targeted against reasoning models that they call AutoRAN. The core idea involves using predefined narrative templates with a less aligned reasoning model to represent the chain-of-thought process of the target model in order to create a prompt that can elicit unsafe behavior, and then automated refining of this attack prompt based on generations received from the target model. The automatic refinement process shows a strong attack success rate against state-of-the-art commercial reasoning models. The proposed method can also be used to collect data for training reasoning models to be safer — the resulting models show strong improvements in safety, lowering attack success rate from 100% to 8% while only marginally increasing over-refusal rate (from ~9% to  ~11%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea is simple and intuitive and shows very strong attack success rate (100%) against commercial reasoning models like o3-mini or Gemini-2.5-Flash.\n\n2. The idea is presented in a convincing manner, with detailed experiments on 3 different benchmarks to make the claims convincing.\n\n3. Appendix of the paper provides all prompt templates. The paper also includes an anonymized version of their code (**Note: I have not tested it out yet**) which increases my confidence in the claimed results.\n\n4. I particularly like how the proposed attack method can also be used to generate data to train the defender, achieving remarkable improvement (lowering attack success rate from 100% to 8%) while only minimally increasing over-refusal rates on benign prompts."}, "weaknesses": {"value": "(**Dependence on predefined narrative templates**)\n\nThe proposed attack depends on pre-defined narrative templates. This makes this type of attack easy to defend with — one needs to only collect the templates and some attack prompts using this template, and further finetune the target model to recognize this type of templates as having harmful intent.\n\n(**Dependence on revealed thinking process**) \n\nThe dependence of the attack system on the chain-of-thought/thinking process $p$ in addition to the final answer $y$. Could the authors run an ablation without using the thinking process (modifying corresponding steps in the prompt refinement step which uses $p$) and report the attack success rate? This should be a baseline to show how important the revealed thinking process is for generating a successful attack. Also this provides an easy way to prevent these attacks against reasoning models — API providers can simply choose not to reveal the thinking process and provide only the final answer. \n\n(**Adding a system prompt that cautions the model against this type of attack**)\n\nCould one add a system prompt to the target model specifically providing guidelines to recognize this sort of attacks? How would that affect the attack success rate and over-refusal rates?\n\n(**Evaluation on Claude thinking models**)\n\nFinally, could the authors add evaluation of their attack against Claude extended thinking? Either Claude 4 Opus or Claude 4 Sonnet with extended thinking modes enabled should be sufficient. I am curious because these models advertise explicit safety mechanisms in their announcement, and hence showcasing large success rates against these models would convince me to increase my score.\n\n\n(Minor)\n\n1. Figure 4 is very hard to read. Instead of stacked bar plots, maybe use separate bar plots. Only reporting a few models is okay, move the rest of the plots to the appendix.\n\n2. For reasoning used as an adaptive mechanism for safety, the authors could cite [2] as well."}, "questions": {"value": "1. Could the assumption that the attacker model is less aligned compared to the target model be formalized?\n\n2. After the prompt refinement step, does the target model receive the new prompt with the previous conversation history in context, or is the new prompt sent to the target model in a new conversation history? In other words, is the nature of the interaction truly multi-turn or not?\n\n3. How does it compare with multi-round jailbreaks where prompts are also designed using an attacker LLM via in-context prompting [1]? I think [1] should be cited and properly discussed as a prior work.\n\n4. I like how the authors used the proposed attack method as a safety mechanism by generating additional data to train the target model with. Could one extend this framework to the attacker LLM as well? In other words, can you train the attacker to improve its attack quality? Could you train the attacker/defender jointly in a self-play RL manner, and improve both of them simultaneously?\n\n5. If Section 4.6, how well does the training of the defender model generalize across different prompt templates? I.e., if during training time, it only sees a few narrative templates, and then at test-time it sees similar attacks but with different narrative templates, how well would the model generalize?\n\n# References\n\n[1] Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks, https://arxiv.org/abs/2402.09177\n\n[2] Reasoning as an Adaptive Defense for Safety, https://arxiv.org/abs/2507.00971"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3cosbfth2i", "forum": "Nrr35WpJn9", "replyto": "Nrr35WpJn9", "signatures": ["ICLR.cc/2026/Conference/Submission15386/Reviewer_GmJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15386/Reviewer_GmJN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760991097877, "cdate": 1760991097877, "tmdate": 1762925668438, "mdate": 1762925668438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a recipe called AutoRAN for bypassing the safety reasoning of large reasoning models (or LRMs). The method asks the target LRM to perform a task with several fixed templates. If the model responds with hints about why it refuses to carry out a task, the AutoRAN attack uses a weaker attacker model to refine its queries to be more specifically aligned with the harmful objective to achieve. The process repeats until success, as gauged by a 'helpfulness' score. The final evaluation targets 3 well known LRMs on 3 prior benchmarks. Several other models serve as judges, the evaluation is cross-judge evaluated. \n\nThe evaluation reports that AutoRAN is often 100% successful in about 10 iterations on the quantitative metrics chosen.  Using the examples generated by AutoRAN, LRMs can be fine-tuned to align to requisite safety goals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow.\n\nThe paper's main methods are to generate a class of bad inputs, which if not trained on, are problematic for safety alignment. As the paper shows that they can be used to make models more robust. \n\nThe evaluation is with diverse LRMs and cross-validated."}, "weaknesses": {"value": "* The approach might be new in the context of LRMs, but lacks novelty at a highest level of comparison. It is essentially a GAN-style design between the target model and attacker model. The main novelty is in the way of generating attack examples.\n\n* The attack, as such, appears to have short-term value and is not necessarily difficult to detect. The paper itself reports that if we train models with the generated examples, the attack efficacy drops. So the attack is not targeting a fundamental weakness in CoT reasoning.\n\n* If we turn to Table 3, is it fair to conclude that there the attacker pays more in terms of tokens used compared to victims? That is, there is an asymmetric cost disadvantage at least in that sense to the attack?\n\n* The paper touches upon the issue of variation based on benchmarks when it explains why there are discrepancies in HarmBench. Different models are judging quantitatively on subjective criteria. This is perhaps unavoidable though the way safety alignment is defined presently. It is not a limitation specific to this work."}, "questions": {"value": "Do you agree that AutoRAN appears to target a short-term limitation of CoT reasoning, since you can retrain to mitigate the issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tb21eRaqwB", "forum": "Nrr35WpJn9", "replyto": "Nrr35WpJn9", "signatures": ["ICLR.cc/2026/Conference/Submission15386/Reviewer_LsSs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15386/Reviewer_LsSs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988577553, "cdate": 1761988577553, "tmdate": 1762925667887, "mdate": 1762925667887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}