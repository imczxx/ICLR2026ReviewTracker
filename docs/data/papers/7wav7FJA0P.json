{"id": "7wav7FJA0P", "number": 25183, "cdate": 1758365075534, "mdate": 1759896731184, "content": {"title": "PathHD: Efficient Large Language Model Reasoning over Knowledge Graphs via Hyperdimensional Retrieval", "abstract": "Recent advances in large language models (LLMs) have enabled strong reasoning\nover structured and unstructured knowledge. When grounded on knowledge graphs\n(KGs), however, prevailing pipelines rely on neural encoders to embed and score\nsymbolic paths, incurring heavy computation, high latency, and opaque decisions,\nwhich are limitations that hinder faithful, scalable deployment. We propose a\nlightweight, economical, and transparent KG reasoning framework, PathHD, that\nreplaces neural path scoring with hyperdimensional computing (HDC). PathHD\nencodes relation paths into block-diagonal GHRR hypervectors, retrieves candidates\nvia fast cosine similarity with Top-K pruning, and performs a single LLM call to\nproduce the final answer with cited supporting paths. Technically, PathHD provides\nan order-aware, invertible binding operator for path composition, a calibrated\nsimilarity for robust retrieval, and a one-shot adjudication step that preserves\ninterpretability while eliminating per-path LLM scoring. Extensive experiments on\nWebQSP, CWQ, and the GrailQA split show that PathHD (i) achieves comparable\nor better Hits@1 than strong neural baselines while using one LLM call per query;\n(ii) reduces end-to-end latency by 40–60% and GPU memory by 3–5× thanks\nto encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that\nimprove error diagnosis and controllability. These results demonstrate that HDC\nis a practical substrate for efficient KG–LLM reasoning, offering a favorable\naccuracy–efficiency–interpretability trade-off.", "tldr": "We present PathHD, a lightweight hyperdimensional computing framework for efficient and interpretable large language model reasoning over knowledge graphs.", "keywords": ["Large Language Models", "Efficient Reasoning", "Knowledge Graphs", "Hyperdimensional Computing"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d567b32afa8d543c4a7ae426d8c2796b87b2116.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PathHD, a lightweight and interpretable framework for LLM reasoning over knowledge graphs. The key innovation is the replacement of neural path-scoring modules with hyperdimensional computing (HDC)-based non-commutative path encodings. Each KG relation is represented as a block-diagonal unitary matrix, and relation sequences are composed using a Generalized Holographic Reduced Representation (GHRR) binding operator. Candidate paths are retrieved by cosine similarity in the hypervector space, and only the top-K are fed into a single LLM call for adjudication and answer generation. Experiments on WebQSP, CWQ, and GrailQA show that PathHD achieves comparable or superior accuracy to strong LLM+KG baselines while reducing latency by 40–60% and GPU memory by 3–5×, achieving a strong accuracy–efficiency–interpretability trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of introducing HDC into KG-LLM reasoning is novel and elegant. The order-aware, invertible GHRR binding addresses a longstanding limitation of commutative path encodings, ensuring directionality and compositional faithfulness.\n\n2. The method is encoder-free, relying purely on vector algebra instead of transformer-based scoring, which leads to substantial latency and cost reductions.\n\n3. PathHD produces path-grounded rationales, allowing the model to cite supporting relations explicitly, which facilitates error diagnosis and aligns with the growing focus on interpretable and faithful LLM reasoning."}, "weaknesses": {"value": "1. Limited experimental scope. All evaluations are based on Freebase knowledge graph, it would strengthen the cross-domain generalization of the proposed method to test on domain-specific KGs and QAs. For example, UMLS and biomedical QA datasets.\n\n2. The system’s accuracy ultimately depends on whether the correct path is enumerated before HDC scoring. This step is briefly described, but its cost, errors, and coverage are not deeply analyzed.\n\n3. Although latency and interpretability are measured, qualitative analyses of failure cases or sensitivity to prompt phrasing are missing."}, "questions": {"value": "1. How sensitive is performance to the dimension (d) and block size (m) of the GHRR hypervectors? Could lower-dimensional configurations preserve efficiency without losing accuracy?\n\n2. How does candidate enumeration interact with the hyperdimensional retrieval — could the retrieval guide enumeration adaptively instead of relying on BFS?\n\n3. Is the projection from SBERT to HDC space trained or fixed? How critical is this step to generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzVrW2KhAc", "forum": "7wav7FJA0P", "replyto": "7wav7FJA0P", "signatures": ["ICLR.cc/2026/Conference/Submission25183/Reviewer_eUmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25183/Reviewer_eUmU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958898590, "cdate": 1761958898590, "tmdate": 1762943354991, "mdate": 1762943354991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research presents PathHD as a system which enhances Large Language Model (LLM) reasoning capabilities on Knowledge Graphs (KGs) through improved efficiency. The current methods experience long processing times and high resource consumption because they use neural encoders at a slow pace and perform multiple LLM evaluations to assess reasoning paths. The system uses Hyperdimensional Computing (HDC) to transform symbolic paths into order-sensitive vectors which enables quick retrieval before the LLM generates the final answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts thorough ablation tests which validate its fundamental design decisions. The research demonstrates that the non-commutative GHRR binding operator maintains path directionality and the single-LLM adjudication step enhances accuracy while Top-K pruning with $K=3$ achieves optimal performance and latency balance.\n2. The system achieved excellent performance in WebQSP/GrailQA and CWQ tasks when using GPT-4 and small open models including Llama-3-8B which demonstrated significant improvements in single-call evaluations."}, "weaknesses": {"value": "1.  When handling complex compositional reasoning tasks, can the authors explain the performance since the lack of the computational power of multi-call LLM agent methods?\n2. The framework requires an initial \"Plan\" stage to create candidate reasoning paths which determine its overall success. The success of the system thus seems to critically depend on the correctness of this plan. If the plan fails to produce the correct reasoning paths, the system will fail, regardless of how efficient the subsequent steps are. However, it seems that the paper lacks details about this plan generation process and lacks discussion on how robust it is.\n3. The system depends on one LLM execution to function as a protective mechanism which handles unclear situations and fixes mistakes that vector-based retrieval produces. The system faces two major risks because it depends on the LLM to resolve ambiguities and correct errors from vector-based retrieval. The system fails when the LLM does not receive the correct path as one of its Top-K candidates. The system depends on the LLM to produce accurate results which makes it a critical failure point.\n4. The model's performance is highly sensitive to the choice of the hypervector dimension $d$. Different datasets require different optimal dimensions to achieve peak performance, and accuracy can even decrease if the dimension is too large. This creates a significant tuning challenge and means that efficiency gains might be reduced on more complex datasets that require a larger dimension."}, "questions": {"value": "Please explain the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pLSHgcO1jW", "forum": "7wav7FJA0P", "replyto": "7wav7FJA0P", "signatures": ["ICLR.cc/2026/Conference/Submission25183/Reviewer_Rc8u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25183/Reviewer_Rc8u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969210699, "cdate": 1761969210699, "tmdate": 1762943354098, "mdate": 1762943354098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PathHD: a method for question answering over knowledge graphs. At the core of the method is the idea of Generalized Holographic Reduced Representations (GHHR), which are randomized high dimensional representations of relations in the KG. Composing GHHRs with non-commutative operators like matrix multiplication allows computing representations of relation paths $r_1\\rightarrow r_2$ different from paths that involve the same relations but in different directions, like $r_2\\rightarrow r_1$. The authors apply these representations by mapping a query to a GHRR and a cosine-similarity-like operator for retrieving relevant paths from the KG, which are then fed to an LLM as context for answering the question. Experiments show that the method is competitive while requiring only 1 LLM call, with additional ablations that help understand the impact of different elements in PathHD on performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is essentially training-free: GHHRs are randomized representations that are computed once and fixed to later compute similarity scores for an arbitrary query. This results in a method that is less computationall expensive to deploy. (Though one variant seems to require training, more below.)\n2. The method is backed by theoretical bounds on the probability of a false match (Proposition 1) for a given query, which decays exponentially with the dimension of the representations.\n3. PathHD performs competitively with respect to baselines that rely on multiple calls to an LLM. Further experiments show that this performance does not degrade significantly when PathHD uses different LLMs as a backbone.\n4. The ablation experiments are comprehensive, covering the effect of composition operators, top-k pruning, and hypervector dimension."}, "weaknesses": {"value": "1. The paper borrows heavily from known results in vector symbolic architectures (VSA) and high-dimensional probability, which limits the novelty of the proposed method. The method applies such results to the problem of question answering, with the key contribution lying in the mapping of a question to a hypervector. Unfortunately, this mechanism is the one that receives less attention in the paper (more below).\n2. The paper's major issues are in relation to clarity of exposition:\n   - P2-L098: the method is stated to enable scoring with $\\mathcal{O}(Nd)$, but at this point it is not known what $N$ and $d$ are.\n   - The definition of a KG is not complete. A KG is assigned a symbol $\\mathcal{G}$, with no further details: is there a set of entities and relations? How are triples represented? Additionally, there is a set of \"relation schemas\" $\\mathcal{Z}$ that again is also just a symbol. It is not clear what a relation schema really is.\n   - The key contribution on mapping a question to a hypervector is not clear. Two short paragraphs seem to explain this: L154-L161 and L192-L195. Several concerns arise here. A question needs to be mapped to a query plan \"via schema-based enumeration (depth $\\leq L_{\\text{max}}$) and, when helpful, refine or rank these plans by a lightweight prompt.\" i) What do you mean by schema-based enumeration? Is this related to the set of relation schemas $\\mathcal{Z}$? How do you choose $L_{\\text{max}}$? What do you mean by a lightweight prompt? Is this a prompt to an LLM, and if so, does that mean that the method does in fact require more than 1 call to an LLM? Later candidate paths are instantiated by matching templates or BFS: depending on how this interpreted, it can result in a costly method to run, but there are no further details. \n   - There is a text-projection approach for computing a query hypervector that relies on a **fixed or learned** map, but more details (e.g. learned how?) are missing, and unless I missed it, the ablation comparing plan-based with text-projection claimed in L161 is actually missing from the paper.\n   - The calibrated score (L206) relies on an IDF function (inverse document frequency after having to refer to the appendix) whose details are not clear. It also relies on hyperparameters $\\alpha,\\beta,\\lambda$ and how these are tuned is also not clear.\n3. The results claim state-of-the-art on WebQSP and GrailQA, when i) on WebQSP this is the case when considering H@1 only, and ii) on GrailQA only 5 baselines, none from the Embedding or Retrieval family, are considered. Out of the LLMs+KG family, only two baselines are considered and in this case PathHD overlaps with KG-Agent. A test of significance would be warranted here to know whether the difference between PathHD (86.7 H@1)  and KG-Agent (86.1) is indeed significant or if it is due to chance.\n4. No supplementary material with code or link to an anonymous repository is available that could be used to replicate the experiments or clarify many of the missing details in the paper.\n5. Overall, the paper presents an interesting contribution with competitive results, based on results from the VSA literature. While the results are promising, the exposition in the paper and the lack of an auditable implementation limit the potential impact of the paper."}, "questions": {"value": "1. Can you please clarify the issues highlighted in W2?\n2. Can you please elaborate on the significance of the small differences noted in GrailQA with respect to KG-Agent?\n3. Equations (1) and (2) and their preceding sentences seem almost duplicate, or is there a difference between them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1izKbY9Pv", "forum": "7wav7FJA0P", "replyto": "7wav7FJA0P", "signatures": ["ICLR.cc/2026/Conference/Submission25183/Reviewer_qif8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25183/Reviewer_qif8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995242314, "cdate": 1761995242314, "tmdate": 1762943353739, "mdate": 1762943353739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight reasoning framework that replaces neural path scorers in KG–LLM reasoning with Hyperdimensional Computing (HDC). The new methodology, PathHD, encodes relation paths into Generalized Holographic Reduced Representation (GHRR) hypervectors and therefore uses cosine similarity + Top-K pruning to sort candidates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n1. The idea of using cosine similarity and embedding for ranking instead of neural scorer is intuitive and easy to follow. The paper explains its motivations and methods(Generalized Holographic Reduced Representation embedding) clearly. It’s noted that this binding is non-communicative.\n\n2.\tThe paper eliminates per-path neural encoding and uses only a single LLM call, the method achieves huge reductions in latency and computational cost. The results in Figure 3 (accuracy vs. latency) show PathHD succeeds to improve the efficiency compared to previous method.\n\n3.\tThe experiment part is extensive, the paper has conducted lots of ablation study, including Binding Operator (Table 3), LLM Adjudicator (Table 4), the impact of the choice of K(Table 5)."}, "weaknesses": {"value": "1. I think the novelty of this paper is slightly restricted, as there are many related RAG work, introducing embedding designs is ok but not novel enough as this is only for planning, but not for like deciding length of path, control budget of number of candidate paths and so on,.\n\n2.The performance is not satisfactory regarding the accuracy.In Table 2, consider WebQSP and CWQ which has a lot of baselines, this paper(PathHD) only gets one SOTA in 2*2=4 settings."}, "questions": {"value": "1. Can you conduct experiments on your own to help with the omitted results in Table2? \n\n2. Why the related work is located after the experiment part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TcDMYiGyST", "forum": "7wav7FJA0P", "replyto": "7wav7FJA0P", "signatures": ["ICLR.cc/2026/Conference/Submission25183/Reviewer_XCRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25183/Reviewer_XCRa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25183/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762273000704, "cdate": 1762273000704, "tmdate": 1762943353307, "mdate": 1762943353307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}