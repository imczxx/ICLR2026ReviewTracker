{"id": "OEgWVhdom2", "number": 2754, "cdate": 1757239592049, "mdate": 1759898129218, "content": {"title": "Diffeomorphic Optimization", "abstract": "Optimization is a challenging task due to the rugged nature of the optimization landscape and the concentration of data on a low-dimensional manifold. Our approach starts from the observation that flow and diffusion models map the data manifold to a smooth and simple base space. We thus propose to reparameterize the optimization problem in terms of these simple base-space variables. Using concepts from differential geometry, we demonstrate that this reparameterization naturally constrains optimization to the data manifold and results in a smoother optimization surface. We extend diffeomorphic optimization to matrix groups, such as $SO(3)$ and $SE(3)$, which allows us to empirically demonstrate the effectiveness of our approach in the highly relevant task of protein design.", "tldr": "Optimization of samples with respect to a target space loss function while staying on the learned manifold.", "keywords": ["Diffusion Models", "Geometric Machine Learning", "Differentiable Geometry"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a0789d6857e5cd5f9d3fea5426ebee7a880b2c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Diffeomorphic Optimization, a novel framework that performs optimization constrained to the data manifold learned by a generative model. The core idea is to reparameterize the optimization problem in the latent base space of a diffeomorphic generative model (e.g., a flow or diffusion model). By leveraging differential geometry, the authors prove that gradient descent in the latent space is first-order equivalent to Riemannian gradient descent on the data manifold. The method is further extended to matrix Lie groups, such as SO(3) and SE(3), enabling applications to 3D geometry problems like protein structure refinement and protein–ligand docking. Empirical results demonstrate faster convergence, smoother optimization trajectories, and improved geometric consistency compared to Euclidean and guidance-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work makes a significant contribution at the intersection of geometric machine learning and generative model optimization.\n- Introduces a new and conceptually elegant framework linking differential geometry and generative model optimization.\nThe diffeomorphic parameterization idea is both theoretically deep and practically impactful.\n- The proofs (Section 3) are mathematically solid, with correct use of Riemannian and Lie group formalism.\n- Writing is precise and pedagogical; the paper explains nontrivial geometry intuitively without oversimplifying.\n- Provides a general foundation for manifold-aware optimization, relevant to numerous scientific and AI domains.\n-  Demonstrates real impact on SE(3) applications (e.g., protein docking) where geometric consistency is crucial."}, "weaknesses": {"value": "- While results on protein-related tasks are convincing, evaluations on non-biological manifolds (e.g., moulecules, crystal materials) would help establish broader generality.\n- The theoretical guarantees rely on the diffeomorphism being smooth and bijective. In practice, diffusion models may only approximate this property. A discussion or metric quantifying deviation from diffeomorphism would strengthen the work."}, "questions": {"value": "- How sensitive is the optimization quality to the accuracy of the learned diffeomorphism?\nDoes the framework degrade gracefully if the generative mapping g slightly violates invertibility or smoothness?\n- Have the authors explored second-order extensions (e.g., Riemannian Newton or natural gradient methods) within the same diffeomorphic framework?\n- Is the adjoint-state method in Section 4 computationally scalable for large molecule systems (many SE(3) transformations)?\n- For reproducibility, will the authors release the code and pretrained diffeomorphic models used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UTsDiAUXxm", "forum": "OEgWVhdom2", "replyto": "OEgWVhdom2", "signatures": ["ICLR.cc/2026/Conference/Submission2754/Reviewer_AExf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2754/Reviewer_AExf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446020667, "cdate": 1761446020667, "tmdate": 1762916362180, "mdate": 1762916362180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is based on D-flow and extends gradient-based guidance in SO(3) space to guide protein backbone generation. The demonstrated applications are interesting. However, the contributions are limited in methodology, experiments, and theory (see weakness)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear presentation: The paper provides illustrative toy data experiments, algorithm pseudocode, and detailed implementations of geometric operators. The writing is clear and free of obvious errors.\n- Interesting domain application: protein backbone secondary structure modification, pocket-ligand docking, and energy optimization are important applications in the field of protein design."}, "weaknesses": {"value": "- From a method perspective, this method extends D-flow to SO(3) space, which has already been explored in previous literature, e.g., [1].\n- From an application perspective, the paper lacks quantitative evaluation against existing guided generation methods.\n- From a theoretical perspective, the main results (Theorems 1 and 2) are restatements of well-established results in Riemannian optimization and matrix Lie groups (e.g., [2,3]), with limited novel theoretical contribution.\n\n[1] Wang, Luran, et al. \"Training free guided flow matching with optimal control.\" arXiv preprint arXiv:2410.18070 (2024).\n\n[2] Absil, P-A., Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2008.\n\n[3] Do Carmo, Manfredo Perdigao, and J. Flaherty Francis. Riemannian geometry. Vol. 2. Boston: Birkhäuser, 1992."}, "questions": {"value": "- Complexity and efficiency are not clearly discussed.\n- It is unclear how much the proposed method differs from D-flow in practice. If we ignore the rotations (i.e., frame orientations) and only consider Cα atoms in protein backbone generation, which is in SE(3), the method appears to reduce to a standard D-flow formulation. The paper does not explicitly clarify whether any additional benefits arise beyond applying D-flow to Euclidean coordinates embedded in SE(3)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "weCxJlMlIC", "forum": "OEgWVhdom2", "replyto": "OEgWVhdom2", "signatures": ["ICLR.cc/2026/Conference/Submission2754/Reviewer_39ZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2754/Reviewer_39ZQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872555865, "cdate": 1761872555865, "tmdate": 1762916361864, "mdate": 1762916361864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Diffeomorphic Optimization, a novel method for optimizing arbitrary differentiable cost functions on data manifolds by leveraging flow-based and diffusion generative models. The key insight is that these models learn diffeomorphic (smooth and invertible) maps from simple base distributions to complex data distributions, allowing optimization to be performed in the simpler base space rather than directly on the data manifold.  A significant technical contribution is extending this framework to matrix Lie groups (SO(3) and SE(3)), which are crucial for protein structure representation. The authors develop two methods for backpropagation through ODE solvers on these groups: (1) repurposing existing autograd engines to compute Riemannian gradients, and (2) deriving an adjoint state method for matrix Lie groups. The method is demonstrated on protein design tasks using state-of-the-art generative models (FrameFlow, DiffDock, AlphaFlow), showing successful optimization of secondary structure, protein-ligand docking scores, and Rosetta energy functions while maintaining physically plausible structures throughout the optimization trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a mathematically rigorous foundation connecting differential geometry, generative models, and optimization. Theorem 1 establishes that gradient descent in base space is equivalent to gradient descent on the data manifold up to quadratic corrections, giving the core theoretical motivation. The two proposed methods for handling backpropagation (autograd repurposing and adjoint state method) are both theoretically sound and practically implementable.\n\nThe method demonstrates clear improvements across diverse protein tasks."}, "weaknesses": {"value": "The method requires backpropagation over the entire ODE integration trajectory, which is computationally expensive. While the authors argue this is acceptable in protein design due to wet-lab bottlenecks, it limits broader applicability.\n\nWhile the authors' optimization scheme allows us to enforce manifold constraints, it is not clear whether it biases the optimization towards certain minima. As the optimization objective is highly non-convex, it is essential to explore and obtain a reasonable estimate of the global minima rather than focusing on gradient descent to find a local minima. Sampling allows us to see a broader range of solutions rather than finding a single optimal solution. Under this light, it is unclear if the proposed methodology provides a practical advantage over guided sampling methods.\n\nThe theory and experiments do not provide any details on the accumulation of estimation errors during backpropagation over the entire ODE trajectory. Some analysis of the estimation error would be highly relevant for practical adoption."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8xOrLc5Eyw", "forum": "OEgWVhdom2", "replyto": "OEgWVhdom2", "signatures": ["ICLR.cc/2026/Conference/Submission2754/Reviewer_aRBD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2754/Reviewer_aRBD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997704525, "cdate": 1761997704525, "tmdate": 1762916360676, "mdate": 1762916360676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}