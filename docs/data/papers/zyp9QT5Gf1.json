{"id": "zyp9QT5Gf1", "number": 17783, "cdate": 1758280465212, "mdate": 1759897154136, "content": {"title": "Learning with Interaction: Agentic Distillation for Large Language Model Reasoning", "abstract": "Recent advancements in large language models (LLMs) have demonstrated remarkable reasoning abilities to solve complex tasks, which has propelled the progress toward artificial general intelligence (AGI). However, these gains come with significant computational costs, limiting their practical deployment. A promising direction is to distill reasoning skills from larger teacher models into smaller, more efficient student models, yet existing data-centric distillation approaches suffer from passive learning, over-learning on simple tasks, and persistent knowledge gaps. To overcome these limitations, we introduce Agentic Distillation, a novel framework for adaptive and active distillation. In Agentic Distillation, student LLMs interact with teacher LLMs modeled as environments, receiving feedback tokens to guide their reasoning process and selectively updating their capabilities when necessary. To address the off-policy and gradient vanishing challenges introduced by feedback tokens, we devise a tailored importance sampling and clipping strategy within a unified objective that both incentivizes reasoning and injects knowledge into student LLMs. Extensive experiments show that Agentic Distillation significantly enhances reasoning performance while improving efficiency, offering a scalable path for equipping compact LLMs with advanced reasoning abilities.", "tldr": "We present a novel distillation method based on agentic interaction for LLM reasoning.", "keywords": ["large language models", "distillation", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46f572c1911b630bcb8bb22450bde60f657116f8.pdf", "supplementary_material": "/attachment/a4a125c61b3f2461b6667c96804d13f4495d04e4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Agent Distillation to address existing data-centric distillation, which over learn on easy samples. They propose letting a student model interact with a stronger teacher model, and ask for the teacher's feedback when the student is not able to solve the problem on their own. This feedback will be used to guide the student, and it shows that learning from teacher-generated feedback effectively improves distillation performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The detailed discussion of several issues when trying to inject teacher-generated tokens into the student LM is insightful (e.g., being off-policy and gradient vanishing), and the author provides solutions to address these issues.\n2. The evaluation is conducted on extensive tasks, and the benchmarks are well chosen.\n3. The improvement is consistent across tasks, showing strong performance from learning from the teacher’s feedback."}, "weaknesses": {"value": "1. Some qualitative analysis will help, for example, show what the student is actually generating after training, and how it improves the performance. Does the student also generate feedback-style reasoning during test time?\n2. Adding a baseline on SFT from the teacher’s full trajectory (including the feedback) and then doing RL for correctness would further strengthen the claim. How important is the interaction? Can we collect feedback in an offline manner?\n3. The method figure is not clear; for example, it's hard to see that the teacher is generating multi-turn feedback. Also, how to decide when the student model needs help is not clear either.\n4. \"When to use external feedback\" is controlled solely by prompting the student model. However, models can be over-confident or ill-calibrated, necessitating the need for an analysis on how often the student is over-confident (does not call the teacher model but cannot solve the problem on its own).\n5. Some variants on when to use external feedback are also necessary to justify this design choice."}, "questions": {"value": "1. Typo in Fig 1’s caption: gap instead of grap.\n2. Text in Fig 3 is too small, especially for the equations.\n3. There is an additional a’ in line 119.\n4. The teacher feedback is used when any single rollout fail, or when all rollouts fail for a question? If it’s the former, how does this method avoid overlearning, since the student model is actually able to solve this question (but not always correct).\n5. Using the objective from equation 16, does the student model also generate self-correct or feedback-style content during test time?\n6. What if the SFT baseline is also distilling teacher’s feedback (and followed by RL training for correctness)? What would the performance be like\n7. In abstract, the statement \"...reasoning abilities to solve complex tasks, which has propelled the progress toward artificial general intelligence\" is not necessary or should be backed by citations.\n8. In the Knowledge Boundary Expansion analysis, which dataset is this, and what is the sample size? In my opinion, the more direct way to test this is to see how many previously unsolvable problems become solvable after training.\n\nMissing reference on distilling from interaction: https://arxiv.org/abs/2402.01620"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GBwzyKXich", "forum": "zyp9QT5Gf1", "replyto": "zyp9QT5Gf1", "signatures": ["ICLR.cc/2026/Conference/Submission17783/Reviewer_qKrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17783/Reviewer_qKrD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761339459947, "cdate": 1761339459947, "tmdate": 1762927625632, "mdate": 1762927625632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AgenticDistillation, a knowledge distillation (KD) approach. The authors motivate the idea with two problems in standard KD: overlearning (overfitting to simple questions) and knowledge gaps between the teacher and student. Their method prompts the student to actively seek teacher feedback on particular steps and allows models to learn from the feedback the teacher provides. The paper then introduces a gradient clipping method for performing RL on and SFT on different parts of the response (question and response). The method is tested on datasets from math, code, and science domains, with improvements over the base models and ablations that they compare against"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Framing distillation as a method where the agent asks for information from an oracle is an interesting idea\n- RL approach for learning from feedback appears novel\n- The authors tested a variety of models, including reasoning and non-reasoning models."}, "weaknesses": {"value": "- Under-reported baselines: In Table 1, the baselines the authors report seem lower than what has been reported in published work. For example, the Qwen 2.5 tech report (https://arxiv.org/pdf/2409.12122v1, Table 5) has AIME 24 performance at 5/30 (16%) while this paper reports 9%. Past work (e.g. https://arxiv.org/abs/2506.11902, Table 1) has also reported higher baseline numbers for MATH-500 (76.5%, rather than 73.00 reported here). In several cases, the gain reported from distillation largely disappears when considering the stronger baseline numbers. Can the authors explain why their baselines are consistently lower than prior work?\n\n- No external baselines: All baselines compared against are internal models, but no other competing distillation methods were evaluated (e.g. https://arxiv.org/abs/2503.07067, https://aclanthology.org/2025.acl-industry.4/, https://arxiv.org/abs/2509.25837) although several are cited in related work. \n\n- Potential data leakage: how sure are the authors that none of the datasets tested on are included in the training data sourced from DAPO, OpenScienceReasoning, and Reasoning Gym?\n\n- It's not clear to me what happens at test time. During training, the model asks for information from the oracle -- is this prompt also followed at test time? If not, why is the model improving from training?"}, "questions": {"value": "# Comments\n- Small quibble: I find the motivation w.r.t. AGI unnecessary, and a bit of a leap (i.e. the connection between distillation and AGI is a bit tenuous)\n- many of the sentences are incomplete and the paper could use more polishing (e.g. L061)\n- L324 typo in Instruct"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bJPTyqC7zS", "forum": "zyp9QT5Gf1", "replyto": "zyp9QT5Gf1", "signatures": ["ICLR.cc/2026/Conference/Submission17783/Reviewer_3uKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17783/Reviewer_3uKu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791213991, "cdate": 1761791213991, "tmdate": 1762927624452, "mdate": 1762927624452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Agentic Distillation as a novel paradigm for knowledge transfer, aiming to distill complex reasoning capabilities from large, computationally expensive teacher models into smaller student models. Unlike traditional static knowledge distillation, this approach introduces an interactive and agent-based learning environment. The authors motivate this work by pointing out that current data-centric distillation methods suffer from passive learning, over-fitting on simple examples, and persistent knowledge gaps. While conceptually innovative, the method relies on dynamic interaction, which introduces non-trivial overhead and stability risks that must be comprehensively addressed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of shifting from passive data-centric distillation (e.g., logit-matching) to an active, interactive, agent-based learning framework is a major intellectual contribution. It offers a genuine new direction for solving the knowledge gap problem.\n\n2. The agentic distillation framework provides a flexible structure that could potentially integrate advanced components, such as tool-use or external memory, making the overall distillation process more comprehensive and future-proof."}, "weaknesses": {"value": "1. The method contradicts its primary goal of efficiency by introducing significant training-phase complexity. The paper explicitly notes that training time \"may grow considerably\" with teacher complexity. This dramatically limits the ability of the deep learning community to reproduce, scale, or even test this method without substantial, often inaccessible, compute resources.\n\n2. The success of the distillation is likely critically dependent on the specific design of the \"interaction\" protocol, the reward signals, and the complexity of the agent architecture. If the results are highly sensitive to these hyper-parameters, the methodology is not broadly applicable or robust.\n\n3. Traditional distillation offers a clear, convex optimization target (e.g., KL divergence). Introducing complex, nested optimization loops and dynamic feedback makes the objective function non-trivial, harder to analyze, and obscures which components (the distillation loss, the agentic feedback, or the interaction environment) are providing the primary performance gains."}, "questions": {"value": "The paper notes a risk of \"unstable improvements across tasks\" due to the dynamic, interactive nature of the training. Given that a key strength is the potential for generalized reasoning, how do the authors explicitly decouple the emergent reasoning skills from the specifics of the training interaction protocol? Specifically, how is the learned policy guaranteed to be a generalized reasoning model, rather than an agent that has merely overfit to the teacher's interactive prompt-response and self-correction style within the training environment, leading to a catastrophic collapse in performance on static, zero-shot benchmarks where the interactive scaffold is absent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UqbC6li3wu", "forum": "zyp9QT5Gf1", "replyto": "zyp9QT5Gf1", "signatures": ["ICLR.cc/2026/Conference/Submission17783/Reviewer_fuv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17783/Reviewer_fuv7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036714267, "cdate": 1762036714267, "tmdate": 1762927623717, "mdate": 1762927623717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Agentic Distillation, a distillation method wherein a student LLM optionally queries a teacher LLM, in the process obtaining feedback which is then jointly optimized with its own generated tokens using GRPO. To stably learn from the teacher's feedback tokens (sampled from the teacher's policy), the authors introduce an importance sampling coefficient and a clipping strategy. Experiments are conducted on different reasoning and coding benchmarks with multiple student+teacher combinations to show that Agentic Distillation outperforms SFT on teacher trajectories and RL with its own trajectories (w/o any teacher interaction)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* In off-policy distillation, a student passively learns from teacher trajectories instead of learning from the feedback obtained on its own (student's) trajectories. Agentic Distillation presents a way of mitigating this issue.\n\n* Experiments are pretty thorough with the main result being that actively learning from teacher's feedback tokens can be more beneficial than imitating teacher's trajectories."}, "weaknesses": {"value": "* An important missing baseline is on-policy distillation. It is the most common and effective way of distillation which has been shown to outperform off-policy distillation. It is also compute-efficient because querying the teacher’s log probabilities requires just a single forward pass from the larger model, while the trajectories are generated by the smaller and cheaper student.\n\n* The paper lacks examples and analysis of the kind of queries that the student generates for the teacher and the teacher's subsequent feedback. Without such analysis, it's hard to understand if the student only asks for hints or full answers? Additionally, what is stopping the teacher from giving out complete answers in which case, agentic distillation will turn into vanilla off-policy distillation. In summary, I'm unsure how the student learns to balance between always asking the teacher for complete solutions versus never interacting. Even though the authors write \"This trend suggests that early in training, the student LLM queries the teacher LLM frequently to learn new knowledge.\", this requires more analysis, examples, and explanation.\n\n[1] On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. Agarwal et al., 2023"}, "questions": {"value": "* How does your method compare to on-policy distillation? \n\n* \"This method employs a temperature coefficient to sharpen the teacher LLM’s distribution\" -- Could you explain this more? How do you use the temperature?\n\n* Did you try experimenting with a weaker teacher and a stronger student? \n\n* Since your teacher LLMs are thinking LLMs, does the student also learn from the think tokens or only the response tokens? \n\n* Can you share some examples and statistics of the queries generated by the student? Similarly, the teacher's feedback would also be interesting to look at."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l2aFSlX7MY", "forum": "zyp9QT5Gf1", "replyto": "zyp9QT5Gf1", "signatures": ["ICLR.cc/2026/Conference/Submission17783/Reviewer_8BPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17783/Reviewer_8BPw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762414426569, "cdate": 1762414426569, "tmdate": 1762927623344, "mdate": 1762927623344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}