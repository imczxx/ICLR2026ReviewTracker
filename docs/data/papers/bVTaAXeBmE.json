{"id": "bVTaAXeBmE", "number": 16252, "cdate": 1758262368043, "mdate": 1759897252095, "content": {"title": "DEAS: DEtached value learning with Action Sequence for Scalable Offline RL", "abstract": "Offline reinforcement learning (RL) presents an attractive paradigm for training intelligent agents without expensive online interactions. However, current approaches still struggle with complex, long-horizon sequential decision making. In this work, we introduce DEtached value learning with Action Sequence (DEAS), a simple yet effective offline RL framework that leverages action sequences for value learning. These temporally extended actions provide richer information than single-step actions and can be interpreted through the options framework via semi-Markov decision process Q-learning, enabling reduction of the effective planning horizon by considering longer sequences at once. However, directly adopting such sequences in actor-critic algorithms introduces excessive value overestimation, which we address through detached value learning that steers value estimates toward in-distribution actions that achieve high return in the offline dataset. We demonstrate that \\metabbr consistently outperforms baselines on complex, long-horizon tasks from OGBench and can be applied to improve the performance of large-scale Vision-Language-Action models that predict action sequences, significantly improving performance in both RoboCasa Kitchen simulation tasks and real-world manipulation tasks.", "tldr": "", "keywords": ["Reinforcement Learning", "Offline Reinforcement Learning", "Vision Language Action Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a901127f61b72a88838643cad338d92123e0742c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a value learning framework that leverages action sequences for value learning to facilitate complex, long-horizon sequential decision making. Experimental results on OGBench, RoboCasa Kitchen, and real-world tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of the proposed method is well explained, and this paper is easy to follow.\n- The proposed DEtached value learning with Action Sequence (DEAS) framework that decouples critic training from the actor, encouraging the value function to prioritize high-return actions observed in the offline dataset.\n- The proposed approach outperforms baselines in various benchmarks and real-world tasks."}, "weaknesses": {"value": "- No quantitative results are provided to demonstrate the prevention of value overestimation.\n- The VLA experiments are conducted with only a single baseline. It would be more convincing to include comparisons with other VLA methods, such as OpenVLA [1].\n- No sensitivity analysis is conducted on the hyperparameter $H$.\n\nReference:\n\n[1] Kim et al. \"OpenVLA: An Open-Source Vision-Language-Action Model\", arXiv preprint arXiv:2406.09246, 2024."}, "questions": {"value": "Have you compared your method with other competitive baselines such as CQL [1]?\n\nReference:\n\n[1] Kumar et al. \"Conservative Q-Learning for Ofﬂine Reinforcement Learning\", NeurIPS, 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ueKWOgOT6S", "forum": "bVTaAXeBmE", "replyto": "bVTaAXeBmE", "signatures": ["ICLR.cc/2026/Conference/Submission16252/Reviewer_dtkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16252/Reviewer_dtkT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400795334, "cdate": 1761400795334, "tmdate": 1762926406196, "mdate": 1762926406196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DEAS, a method for offline reinforcement learning that models each decision as an action sequence rather than a single action. Building on the IQL framework, DEAS learns a critic consisting of $Q$ and $V$ networks while allowing an arbitrary policy architecture for action generation. During critic training, both the predicted and target values are treated as distributions and optimized via a cross-entropy loss. Experiments on continuous-control tasks, including both simulated and real-world robotic environments, demonstrate the effectiveness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Effectively leverages action sequences to address sparse rewards and long-horizon credit assignment challenges.\n2. Appropriately builds on the IQL framework for stable critic learning under the offline setting.\n3. Demonstrates strong empirical performance on complex, long-horizon continuous-control tasks in both simulation and real-world environments."}, "weaknesses": {"value": "1. The novelty of the paper is somewhat limited. It mainly combines existing ideas from action-sequence RL, IQL, and HLG.  \n2. The cross-entropy loss formulations in Equation (1) and Equation (2) are confusing. Typically, the predicted distribution appears inside the logarithm term and the target distribution outside. In contrast, both equations place the target inside the log term, which is unconventional. Moreover, in Equation (2), the role of parameter $\\theta$ on the right-hand side is unclear, as only $\\psi$ and $\\hat{\\theta}$ are explicitly shown.\n3. The paper provides limited discussion on how to learn the policy based on the trained critic. Since the policy outputs an entire sequence of actions per decision, the resulting product action space becomes extremely large, making policy learning challenging. However, the authors do not explain how this issue is addressed."}, "questions": {"value": "1. How can the policy be effectively learned when each decision involves generating a high-dimensional action sequence? Does the paper propose any mechanism to mitigate the exponential growth of the action space?  \n2. Why do Equations (1) and (2) place the target distribution inside the logarithm term in the cross-entropy loss, and how should we interpret the missing parameter $\\theta$ in Equation (2)?\n3. How are the support bounds $v_{min}$ and $v_{max}$determined? These values appear to constrain the expressiveness of the critic network, while setting a larger range may make the learning process more difficult.\n\nMinor comments:\n1. The description of IQL in Section 3 is inaccurate. The paper states that “By using $\\tau > 0.5$, Equation 3 penalizes the overestimated value in out-of-distribution actions.” (By the way, there is no Equation 3.) However, since $u = Q - V$, where $Q$ is obtained from the target network and $V$ is the learnable network, the equation should instead penalize the **underestimated** values of the value network.  \n2. There is also no Equation (4.2), as referenced in Algorithm 1. Please verify and correct the numbering.  \n3. It is recommended to mention HLG again after Equation (2)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AbfZIqett6", "forum": "bVTaAXeBmE", "replyto": "bVTaAXeBmE", "signatures": ["ICLR.cc/2026/Conference/Submission16252/Reviewer_uCyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16252/Reviewer_uCyh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640567488, "cdate": 1761640567488, "tmdate": 1762926405811, "mdate": 1762926405811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of scaling offline reinforcement learning to complex, long-horizon tasks. The authors identify that while using action sequences (instead of single actions) can reduce the effective planning horizon, it introduces severe value overestimation when combined with standard actor-critic frameworks.The core contribution is DEAS (DEtached value learning with Action Sequence), a method that learns a value function over $H$-step action sequences. To prevent value overestimation, DEAS \"detaches\" the critic and value function training from the actor. It does this by adopting an IQL-style expectile regression loss, which learns the value function by regressing only on the in-sample action sequences found in the offline dataset. This avoids querying the actor for out-of-distribution actions.The framework is stabilized by incorporating distributional RL and dual discount factors ($\\gamma_1$ for intra-option rewards, $\\gamma_2$ for inter-option values). The authors demonstrate that DEAS significantly outperforms existing single-step and action-sequence baselines on complex OGBench tasks and can be used to improve the performance of large-scale Vision-Language-Action (VLA) models in simulation and on a real robot."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Clear Problem Identification:** The paper clearly articulates a significant and practical problem: coupled actor-critic methods are unstable when the action space is $\\mathcal{A}^H$.\n2.  **Strong Empirical Results:** This is the paper's greatest strength. DEAS demonstrates state-of-the-art performance, with significant gains over baselines on the challenging OGBench tasks.\n3.  **Real-World & VLA Validation:** The authors successfully scale the method to large VLA models and demonstrate its effectiveness and stability on a real robot. The fact that the QC baseline *collapses* on the real-world task while DEAS improves performance is a very strong supporting data point.\n4.  **Effective Ablations:** The ablation studies in Table 4 are well-executed and convincingly demonstrate the necessity of each component of the method (e.g., $H > 2$, the combination of IQL and distributional RL)."}, "weaknesses": {"value": "1.  **Limited Conceptual Novelty:** As stated in the *Contribution* section, the method is a synthesis of IQL, fixed-duration options, and distributional RL. The contribution is an empirical and engineering one, not a fundamental algorithmic advance.\n2.  **Imprecise Technical Terminology:** The paper repeatedly invokes \"SMDP Q-learning\" and the \"options framework\", but the method itself is not a true SMDP. The use of a *fixed, deterministic* duration $H$ does not create an option induced Semi-Markov process. The system is, more accurately, a standard 1st-order MDP where the agent selects from a temporally-extended, high-dimensional action space $\\mathcal{O} = \\mathcal{A}^H$. This imprecise language is confusing and oversells the connection to the general options framework. From this point of view, it's only another variant of ordinary chunked action sequences and the performance boost whether come from temporal abstraction is skeptical.\n3. **Unjustified Optimality**: Detaching value functions and fixed horizon clearly alters the underlying decision process compares original SMDP. The convergence and optimality of DEAS is unjustified and not bounded.\n4.  **Poor Scalability with Horizon $H$:** The paper's own ablation study reveals a significant scalability limitation. The authors state that \"when the sequence length becomes longer than 8, it requires proportionally larger actor networks to handle the increased action dimensions\". This creates a direct and problematic coupling between the task horizon ($H$) and the required network capacity, which severely limits the method's applicability to *truly* long-horizon problems.\n5.  **Increased Hyperparameter Sensitivity:** The method adds several critical new hyperparameters that require tuning, including the sequence length $H$ (which Table 4a shows is task-dependent), and the dual discount factors $\\gamma_1$ and $\\gamma_2$, which the authors note are \"critical for stable training\"."}, "questions": {"value": "1.  Can the authors clarify the novelty of the method beyond the combination of IQL and action sequences? Is the primary contribution the empirical demonstration that this combination is effective, or is there a more fundamental insight that I have missed?\n2.  Regarding the scalability weakness: The paper notes the actor network must scale with $H$. This seems to be a major bottleneck. Have the authors investigated methods to decouple this than the entire $H$-step block at once?\n3.  Given that the duration $H$ is fixed and deterministic, why was the method presented as an SMDP rather than, more precisely, as a standard MDP with a temporally-extended action space $\\mathcal{O} = \\mathcal{A}^H$? I suggest authors give this paper a major re-write and detach concepts such as SMDPs and options out of scope.\n4.  Given the engineering success and strong empirical performance, I am happy to raise score if authros position their contribution from a more rigorous view."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXW1PFyYVo", "forum": "bVTaAXeBmE", "replyto": "bVTaAXeBmE", "signatures": ["ICLR.cc/2026/Conference/Submission16252/Reviewer_EcDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16252/Reviewer_EcDt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665560554, "cdate": 1761665560554, "tmdate": 1762926405217, "mdate": 1762926405217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an offline RL framework that tackles the value-overestimation problem through detached value learning. To improve stability, it also integrates distributional RL and dual discount factors for intra- and inter-option transitions. Empirically, DEAS is shown to outperform baselines on 30 long-horizon OGBench tasks and to improve large Vision-Language-Action (VLA) systems on both RoboCasa simulations and real Franka robot manipulation tasks. The main contributions are: (1) framing sequence-level offline RL under SMDP Q-learning with detached critics, (2) a recipe (distributional RL + dual discounts) for stable learning with action sequences, and (3) demonstrating practical gains on long-horizon benchmarks and VLA applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments applying DEAS to large VLA models are a strong selling point: DEAS improves VLA performance using mixed-quality offline rollouts plus a small set of expert data, which is important for real-world robotics.\n2. The manuscript is structured logically and presents a well-targeted technical design."}, "weaknesses": {"value": "1. My main concern is that the paper’s core components are largely integrations or adaptations of prior work, such as IQL-style detached learning, distributional RL methods, and option/sequence mechanisms similar to Q-Chunking.\n2. Detached value learning updates the critic using in-distribution high-return actions, but when the offline dataset contains predominantly suboptimal trajectories, this strategy may still yield biased or miscalibrated value estimates (under- or over-pessimism). The paper does not adequately verify whether the learned value function aligns with true returns across different data quality levels. I recommend that the authors analyze Q-error across bins of trajectory quality and conduct sensitivity studies using datasets with controlled expert-to-suboptimal ratios (e.g., 10%, 30%, 50% expert data).\n3. Several key equations are unnumbered and inconsistently referenced, which increases the difficulty of understanding Algorithm 1.\n4. Although the central motivation of DEAS is to alleviate value overestimation induced by action sequences, the experimental results only report task-level success metrics without providing direct analyses of overestimation (e.g., predicted Q-values vs. realized returns). Including such quantitative evidence would substantially strengthen the paper’s claims."}, "questions": {"value": "1. Do you have experiments integrating DEAS (action sequences + detached critic) with representative methods such as CQL? For example, does adding detached sequence-level critics to CQL further reduce overestimation or improve long-horizon performance? \n2. To address potential value miscalibration, have you evaluated DEAS across datasets with varying proportions of expert trajectories (e.g., 10%/30%/50% expert)? How does data quality influence (a) final performance, and (b) the calibration of predicted Q vs realized returns? If absent, please add such sensitivity analysis or discuss limitations clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dl9L4EB5QT", "forum": "bVTaAXeBmE", "replyto": "bVTaAXeBmE", "signatures": ["ICLR.cc/2026/Conference/Submission16252/Reviewer_FBjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16252/Reviewer_FBjU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16252/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245675182, "cdate": 1762245675182, "tmdate": 1762926404573, "mdate": 1762926404573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}