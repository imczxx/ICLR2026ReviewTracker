{"id": "KQYnfeBNjl", "number": 15832, "cdate": 1758255894110, "mdate": 1759897279036, "content": {"title": "SAE as a Crystal Ball: Interpretable Features Predict Cross-domain Transferability of LLMs without Training", "abstract": "In recent years, pre-trained large language models have achieved remarkable success across diverse tasks. Besides the pivotal role of self-supervised pre-training, their effectiveness in downstream applications also depends critically on the post-training process, which adapts models to task-specific data and objectives. However, this process inevitably introduces model shifts that can influence performance in different domains, and how such shifts transfer remains poorly understood. To open up the black box, we propose the SAE-based Transferability Score (STS), a new metric that leverages sparse autoencoders (SAEs) to forecast post-training transferability. Taking supervised fine-tuning as an example, STS identifies shifted dimensions in SAE representations and calculates their correlations with downstream domains, enabling reliable estimation of transferability \\textit{before} fine-tuning. Extensive experiments across multiple models and domains show that STS accurately predicts the transferability of supervised fine-tuning, achieving Pearson correlation coefficients above 0.75 with actual performance changes. Beyond this, we take an initial step toward extending STS to reinforcement learning. We believe that STS can serve as a principled tool for guiding post-training strategies in LLMs.", "tldr": "", "keywords": ["Post-training", "Transferability", "Sparse Autoencoder", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60324a493662dc48224641f9c9cc2313c4af8337.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper shows that SAE activations on specific domains (in ICL) can predict gains from SFT on the same domain, with high correlation.\nIt studies what portion of features that are activated during ICL also get bolstered by SFF, and it then does ablation studies to check their impact on model accuracy. It checks the portion of overlapping features between ICL and SFT, and it makes sure that results hold in RL (with a new formulation). The paper finally proposes an application to designing the data mixture used for SFT based on SAE feature activations)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of this papers lie in the novel insight that SAE activations in ICL can predict improvements from SFT. The link between ICL and SFT was present in the literature, and the authors locate themselves in the literature well.\nThe paper is comprehensive, and it covers different modalities (RL/SFT) and applications (data mix for SFT). It correlates features with SFT gains well (0.7/0.8 correlation), and it shows the (very strong) effects of ablating features.\nThe paper also studies overlaps and uncovers significant portion of feature shifts (increase in activation under ICL) are the same as what SFT causes."}, "weaknesses": {"value": "The main weaknesses are:\n- no sample standard deviations for statistics you calculate\n- no R2 for Figure 4's linear regressions, it would be interesting to know the fraction of the improvement from SFT you predict\n- the SFT you do is can be ineffective (Figure 5), so predicting it is less interesting\n- SAEs are lossy, and so is your predictive method (compounded lossyness)\n\nI am open to increasing my score if the (first 2 or 3) weaknesses above are addressed\n\nMinor:\n- the contributions in the intro are too verbose"}, "questions": {"value": "Can you unpack the most important features in terms of neurons (that you could later fine tune selectively doing something like a selective LoRA)? Have you tried a sparsity penalty on SAE weights (to prune weights to save compute)?\n\nHave you tried computing sample standard deviations and R2s?\n\nHow do you perform compared to trying to predict improvements form SFT based on model activations directly (using a probe)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFAi62qUMU", "forum": "KQYnfeBNjl", "replyto": "KQYnfeBNjl", "signatures": ["ICLR.cc/2026/Conference/Submission15832/Reviewer_MqTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15832/Reviewer_MqTi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760702399249, "cdate": 1760702399249, "tmdate": 1762926060914, "mdate": 1762926060914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of the unpredictable transferability of post-training to downstream tasks. Post-training on data can introduce model shifts that improve the performance of some tasks while degrading others. To address this issue, the paper proposes a Sparse Autoencoder (SAE)-based Transferability Score (STS) to forecast post-training transferability. The method leverages supervised answers as demonstrations for in-context learning and identifies the SAE dimensions that exhibit the largest changes, which correlate with downstream task performance. Experiments across multiple models and domains demonstrate that the proposed transferability score accurately predicts the effects of both supervised fine-tuning and reinforcement learning (RL) tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper focuses on an interesting problem of forecasting changes in downstream task performance without additional training.\n\nS2: The proposed fine-tuning-free approach for estimating transferability is both novel and useful.\n\nS3: The investigation of SAE dimension shifts under both fine-tuning and in-context learning (ICL) is insightful.\n\nS4: The paper evaluates the proposed STS method across multiple major open models (Qwen2.5-7B, Llama3-8B, Gemma2-9B) and demonstrates consistently high correlations."}, "weaknesses": {"value": "W1: The study is limited to a single training dataset (LIMO) and a single evaluation benchmark (MMLU-Pro). Broader domains (e.g., dialogue, code generation) and larger model scales remain underexplored, even though these are areas where STS would likely be most valuable.\n\nW2: There are potential reproducibility issues, as details on the SAE architectures, training procedures, hyperparameters, and prompt templates are either missing or insufficiently explained.\n\nW3: Methodologically, STS relies heavily on the monosemanticity assumption of SAEs that each latent corresponds to a distinct human-interpretable concept. It also depends on access to high-quality demonstrations to estimate feature shifts (based on the weaker correlations observed in the RL experiments)."}, "questions": {"value": "Q1: Figure 2: What exactly do “raw model dimensions” refer to?\n\nQ2: What does the ICL prompt template look like?\n\nQ3: Line 243: The statement “the shifted features before SAE are more uniformly distributed” could be clarified. Does this mean less sparsity, or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lHOR7uxY93", "forum": "KQYnfeBNjl", "replyto": "KQYnfeBNjl", "signatures": ["ICLR.cc/2026/Conference/Submission15832/Reviewer_efyn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15832/Reviewer_efyn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761523815754, "cdate": 1761523815754, "tmdate": 1762926060524, "mdate": 1762926060524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to leverage (the change in) SAE activations to predict fine-tuning performance.\n\nThe premises are (well-studied either empirically/theoretically in prior work):\n\n1. The sparse activations in SAE are domain/task-specific.\n2. ICL can approximate taking a few gradient steps during the SFT process.\n\nThe setup is that, given a fine-tuning/source dataset (e.g., Math) and eval datasets (e.g., Engineering/Law), we want to predict the change in performance on the eval datasets from fine-tuning on the source dataset. The authors propose to use SAE to perform this prediction as follows:\n\n- Identify the top-changed SAE activation dimensions before/after adding source-domain ICL examples in zero-shot prompting (ICL simulates fine-tuning).\n- Identify how often those dimensions activate when feeding in the target domain examples, and use this as a measure of transferability (since SAE activation is associated with task relevance)\n\nThey found that the proposed measure\n\n1. Correlates with the absolute change in performance (not the improvement) after fine-tuning.\n2. Has the potential to be used in applications such as dataset mixture setting, e.g., to mitigate catastrophic forgetting due to fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The reviewer finds the topic interesting and timely, and the proposed technique could be useful in LLM training workflows.\n2. The experiment design is generally sound, and the application demonstrated in section 5 is well-motivated."}, "weaknesses": {"value": "The reviewer feels that this paper has the potential to have greater impact, but is limited by its current presentation and the scope of the experiments.\n\n1. Details on the experiment setup/results are lacking.\n- It appears that all experiments are performed once; ideally, it should be repeated over different random seeds (e.g., for initialization and train/test split) and report the mean + std.\n- Since SAE should be interpretable, a qualitative analysis of the identified SAE activations would be nice as a sanity check to see whether the selected dimension semantically aligns with the task.\n- Following the comment above, another direction is to annotate all SAE features and identify those related to the target task, and then see how much of the relevant features are \"recalled\" in the top-changed SAE activations.\n- The paper would greatly benefit if more source training domains can be evaluated.\n\n2. The fact only absolute change in performance is reported, rather than the signed improvement/decrease in performance, is rather disappointing. Would it be possible to also predict the sign of the change using the proposed approach?\n- Furthermore, in the paper, it was unclear to the reviewer that \"accuracy shift\" meant absolute change rather than the signed improvement/decrease."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5Qshy7TNaE", "forum": "KQYnfeBNjl", "replyto": "KQYnfeBNjl", "signatures": ["ICLR.cc/2026/Conference/Submission15832/Reviewer_Fh4V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15832/Reviewer_Fh4V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809635712, "cdate": 1761809635712, "tmdate": 1762926059996, "mdate": 1762926059996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a metric for predicting how supervised fine tuning will improve performance without performing the fine-tuning. The metric called STS trains a sparse autoencoder on hidden activations in the LLM to obtain sparse \"monosemantic\" latent features. The central assumption is that SAE dimensions that shift during in-context leraning also shift during fine-tuning. A correlation between these shifted features is the core of STS. Results on presented on a math dataset for several public models with ablations on SAE size, layer, etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Addresses an interesting and timely problem - forecasting post-training transfer effects for LLMS\n* Builds on recent interpretability work with SAEs\n* Correlations in the experiments are consistent suggesting the metric captures a genuine phenomenon"}, "weaknesses": {"value": "* Limited conceptual novelty. Reframes existing ideas on representation drift and feature correlation under transferability and SAEs. Lacks a theoretical link between SAE features and fine tuning.\n* Limited empirical evidence. While the experiments presented are indictive of some trend, the central hypothesis is only tested on one dataset and adaptation direction. The scope is too narrow to lend credible evidence to a correlation between ICL feature drift and SFT\n* Lack of baselines. No comparison is made against more simple correlations such as cosine distance or linear probe similarity.\n* Overstatement wrt principled prediction"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e15UKRFhJQ", "forum": "KQYnfeBNjl", "replyto": "KQYnfeBNjl", "signatures": ["ICLR.cc/2026/Conference/Submission15832/Reviewer_czDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15832/Reviewer_czDh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169141420, "cdate": 1762169141420, "tmdate": 1762926059060, "mdate": 1762926059060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}