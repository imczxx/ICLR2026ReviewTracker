{"id": "vsxOOB9PTZ", "number": 21606, "cdate": 1758319570145, "mdate": 1759896912701, "content": {"title": "Non-Euclidean Harmonic Losses", "abstract": "Cross-entropy loss has long been the standard choice for training deep neural networks, yet it suffers from interpretability limitations, unbounded weight growth, and inefficiencies that can contribute to costly training dynamics. Recent work introduced harmonic loss, a distance-based alternative grounded in Euclidean geometry, which improves interpretability and mitigates phenomena such as grokking, also known as delayed generalization on the test set. However, the study of harmonic loss remains narrow: only Euclidean distance is explored, and no systematic evaluation of computational efficiency or sustainability was conducted. In this paper, we extend harmonic loss by systematically investigating a broad spectrum of distance metrics as replacements for the Euclidean distance. We comprehensively evaluate distance-tailored harmonic losses on both vision backbones and large language models. Our analysis is framed around a three-way evaluation of model performance, interpretability, and sustainability. On vision tasks, cosine distances provide the most favorable trade-off, consistently improving accuracy while lowering carbon emissions, whereas Bray-Curtis and Mahalanobis further enhance interpretability at varying efficiency costs. On language models, cosine-based harmonic losses markedly improve gradient and learning stability, strengthen representation structure, and reduce emissions relative to cross-entropy and Euclidean heads. Our code is available at:  https://anonymous.4open.science/r/rethinking-harmonic-loss-5BAB/", "tldr": "We extend harmonic loss beyond Euclidean distance by testing diverse metrics on vision and language models, showing that tailored variants can outperform cross-entropy and Euclidean, improving accuracy, interpretability, and sustainability.", "keywords": ["Harmonic loss", "Explainable AI", "Green AI", "Deep Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0484dd11a70a8dc5b24b38d62fa3bf1bda57751d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends harmonic loss beyond its original Euclidean form to explore how different distance metrics influence deep-learning model performance, interpretability, and sustainability. The authors replace the Euclidean distance in harmonic loss with a broad family of metrics (cosine, Manhattan, Chebyshev, Minkowski, Bray–Curtis, Mahalanobis, etc.) and systematically benchmark them on both vision (MNIST, CIFAR-10/100, ResNet, PVT) and language (BERT, GPT, Qwen) tasks. Results show that cosine-based harmonic losses consistently yield smoother optimization, improved gradient stability, clearer feature structures, and lower carbon emissions compared to cross-entropy and Euclidean baselines. Bray–Curtis distances enhance interpretability by concentrating feature space geometry, while Mahalanobis offers strong representation clarity at a computational cost. Theoretical analysis establishes finite minimizers and PAC-Bayes generalization bounds for 1-homogeneous distances, ensuring well-posed learning. Overall, the study demonstrates that choice of geometric distance crucially affects performance–interpretability–sustainability trade-offs, positioning non-Euclidean harmonic losses as simple, interpretable, and energy-efficient replacements for cross-entropy in both vision and language models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tComprehensive extension of harmonic loss — The paper systematically generalizes harmonic loss beyond Euclidean distance, offering the first large-scale comparison of diverse distance metrics across both vision and language domains.\n\t2.\tStrong empirical validation — Extensive experiments on multiple architectures (CNNs, ResNet, PVT, GPT, BERT, Qwen) demonstrate consistent advantages of cosine-based harmonic losses in accuracy, stability, and sustainability.\n\t3.\tImproved interpretability — The proposed framework yields more structured, low-dimensional representations where class prototypes have clear geometric meaning, aligning with interpretability-by-design principles.\n\t4.\tEnergy efficiency and Green AI contribution — By including CodeCarbon-based measurements, the paper quantifies training emissions and shows that certain non-Euclidean distances reduce carbon footprint compared to cross-entropy.\n\t5.\tSolid theoretical grounding — The authors provide proofs for scale invariance, finite minimizers, and PAC–Bayes generalization bounds, giving mathematical support to the empirical results.\n\t6.\tPractical simplicity and reproducibility — The non-Euclidean harmonic losses can be implemented as a “drop-in replacement” for standard classifier heads with minimal code changes, and the open-source repository ensures reproducibility and accessibility for future research."}, "weaknesses": {"value": "1.\tLimited novelty in concept — The work mainly extends an existing loss (harmonic loss) rather than introducing a fundamentally new framework.\n\t2.\tEmpirical results lack statistical depth — The paper reports averages but provides limited statistical significance analysis or error bars.\n\t3.\tHigh computational cost for some metrics — Distances like Mahalanobis add complexity and may reduce scalability despite interpretability gains.\n\t4.\tInsufficient real-world validation — Experiments are limited to standard benchmarks (MNIST, CIFAR, OpenWebText) without testing on large-scale or practical applications."}, "questions": {"value": "Please check weaknesses, and try to argue them, I will definitely read your response, good luck!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JqEzb5D90C", "forum": "vsxOOB9PTZ", "replyto": "vsxOOB9PTZ", "signatures": ["ICLR.cc/2026/Conference/Submission21606/Reviewer_jU1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21606/Reviewer_jU1M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934050449, "cdate": 1761934050449, "tmdate": 1762941852491, "mdate": 1762941852491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends harmonic loss from Euclidean distance to a family of non-Euclidean metrics (cosine, Minkowski/Lp, Chebyshev, Manhattan, Canberra, Bray–Curtis, Mahalanobis, etc.) and evaluates them across vision backbones (MLP, CNN, ResNet-50, PVT) on MNIST/CIFAR-10/100 and LLM pretraining setups (GPT/BERT/Qwen) on OpenWebText. The study targets a three-way evaluation: performance, interpretability (via PCA structure/prototype semantics), and sustainability (CodeCarbon emissions). The headline claim is that cosine-based harmonic heads offer the best all-round trade-off (accuracy/stability/structure/emissions), with Bray–Curtis and Mahalanobis trading off efficiency for added structure. The paper also provides theoretical conditions for scale invariance and finite minimizers under 1-homogeneous distances and a margin-style PAC-Bayes generalization view."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear, well-motivated extension**: Replacing Euclidean distance in the harmonic link with a registry of distances is simple yet impactful, and the DistLayer abstraction underscores plug-and-play practicality.\n2. **Comprehensive empirical sweep**: The study spans multiple architectures and two domains (vision/LLMs), with consistent protocol and multi-criteria reporting (accuracy/F1, PCA structure, emissions). Radar summaries are helpful to see trade-offs.\n3. **Takeaway with practical value**: Cosine emerges as a robust default across settings; Bray–Curtis and Mahalanobis are positioned as “interpretability-forward” choices with known costs—useful guidance for practitioners.\n4. **Theory that matches the objective’s spirit**: The scale-invariance/finite-minimizer result for 1-homogeneous distances (harmonic link) and a margin-style PAC-Bayes perspective provide conceptual support for the geometry choices."}, "weaknesses": {"value": "1. **Interpretability probes are narrow**: PCA concentration and prototype semantics are informative, but additional probes (e.g., class-conditioned separability margins, cluster purity/ARI, linear probe transfer, or prototype visualization fidelity for vision) would broaden interpretability evidence beyond PCA variance ratios.\n2. **LLM metrics need anchoring**: The paper introduces “Gradient Stability,” “Model Health,” “Clipping Quality,” and “Learning Quality,” but clearer operational definitions, units, and relationships to widely accepted metrics (loss perplexity curves, training variance, overflow rates) would help reproducibility/interpretability.\n3. **Novelty & Relation to Prior Work**: The paper does not re-introduce harmonic loss but broadens its geometry. This is incremental conceptually yet substantial empirically/thematically (interpretability+sustainability)."}, "questions": {"value": "1. Is cosine temperature sensitivity? and is it a simple heuristic for setting it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "z9U0K8LOZA", "forum": "vsxOOB9PTZ", "replyto": "vsxOOB9PTZ", "signatures": ["ICLR.cc/2026/Conference/Submission21606/Reviewer_mGc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21606/Reviewer_mGc1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976231436, "cdate": 1761976231436, "tmdate": 1762941852189, "mdate": 1762941852189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors extend Euclidean harmonic losses (explained to be a formulation that replaces softmax logit normalization with a normalization based on distances to class prototypes) to “non-Euclidean” harmonic losses that are essentially replace 2-norm with different distance metrics such as 1-norm, inf-norm, p-norm, etc. The authors evaluate the effect of these loss functions on multiple models trained on image classification and on LLM prediction in terms of (1) accuracy, (2) interpretability, and (3) sustainability or carbon footprint. Overall, the paper is very well written and thoroughly evaluated, but several elements of the approach are not clear from the writing and should be better clarified/explained. \nThe authors extend Euclidean harmonic losses (explained to be a formulation that replaces softmax logit normalization with a normalization based on distances to class prototypes) to “non-Euclidean” harmonic losses that are essentially replace 2-norm with different distance metrics such as 1-norm, inf-norm, p-norm, etc. The authors evaluate the effect of these loss functions on multiple models trained on image classification and on LLM prediction in terms of (1) accuracy, (2) interpretability, and (3) sustainability or carbon footprint. Overall, the paper is very well written and thoroughly evaluated, but several elements of the approach are not clear from the writing and should be better clarified/explained."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is extremely well written and generally clear in terms of concept.\n- The three axes of evaluation are interesting and serve as an excellent example for the community. Many portions of the analysis are interesting.\n- This paper is addressing an interesting and growing line of research relevant to the ICLR and broader machine learning community."}, "weaknesses": {"value": "- Some details, such as where exactly the harmonic loss is implemented is unclear in the main text. Does this normalization occur at every layer, wherever there is a softmax, or only at the final classification layer?\n- The work appears incremental, in that modifying the distance function is a small change, even if thoroughly evaluated. The start of the paper suggests greater insights into the geometry of the learned representations, but this is not really provided pictorally or via metrics in the evaluation section.\n- I am not sure “non-Euclidean” is an appropriate framing for the paper. Certainly the distances investigated are not the 2-norm, but the vector spaces they (1-norm, p-norm, etc.) are operating on are still Euclidean."}, "questions": {"value": "- How are the class prototypes determined or computed? This seems like a serious computational challenge if they are learned.\n- Is there only 1 prototype per class? Why not 10 ? Is this not a hyperparameter?\n- Are harmonic losses computed at every layer or only at the final classification layer?\n- The paper refers to the “learned representation” at various places. Does this refer to intermediate layers or the last logit layer before the output?\n- At the final layer, could the class prototypes just be 1-hot encoded vectors for each class? If so, are these losses that conceptually different from cross-entropy? Are they that different than standard MSE or MAE losses?\n- In Line 270, what is the difference between interpretability and representation clarity?\n- Can the authors clarify whether the interpretability measurement via explained-variance using PCA is done on the last logit layer or in the interior of the network?\n- Why exactly is EV, which speaks to the geometry of the representation, a good measure of interpretability? This is not clear or well-justified.\n- The main arguments rely significantly on the work of Baek et al. (cited at least 8 times, 4 times in the introduction). It would help to understand why the authors feel this work is meaningfully different, beyond comparison in multiple models.\n- I really liked the presentation of this paper, but the current presentation is not clear enough in terms of implementation in the main text. Please convince me!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mQDU6DNAD6", "forum": "vsxOOB9PTZ", "replyto": "vsxOOB9PTZ", "signatures": ["ICLR.cc/2026/Conference/Submission21606/Reviewer_f5hr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21606/Reviewer_f5hr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982376877, "cdate": 1761982376877, "tmdate": 1762941851929, "mdate": 1762941851929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the harmonic loss paradigm into a flexible, geometry-aware framework for classification. It demonstrates that choosing the right distance metric can improve not only predictive performance but also interpretability and energy efficiency. The authors also experimentally show that cosine harmonic loss provides the most consistent benefits across both vision and language domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a novel generalization of harmonic loss by extending it beyond the Euclidean setting to a wide family.\n2.  The study benchmarks nine distance metrics across multiple backbones and downstream tasks."}, "weaknesses": {"value": "1. While the paper proves useful properties, the results mostly extend previous harmonic loss proofs under 1-homogeneous assumptions without offering new geometric insights or closed-form derivations for how specific distance families affect optimization landscapes or gradients\n2. The experiments only compare harmonic losses across metrics, but the work omits stronger baselines that directly compete on interpretability or robustness, such as angular-margin losses (e.g., ArcFace) \n3. interpretability is measured only via PCA variance metric\n4. The connection between distance choice and grokking mitigation is not experimentally verified"}, "questions": {"value": "1.  Cosine and Mahalanobis distances introduce nontrivial curvature in feature space, then how these geometries alter convergence behavior or class separation boundaries?\n2. How does the proposed distance-based harmonic loss relate to contrastive or triplet loss frameworks in terms of margin constraints and gradient structure?\n3. All vision results are on small-scale dataset, so I am curious about how the findings  to generalize to larger datasets (e.g., ImageNet) or deeper architectures?\n4. Were emissions normalized by total FLOPs or wall-clock time across runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F18PsgMsBs", "forum": "vsxOOB9PTZ", "replyto": "vsxOOB9PTZ", "signatures": ["ICLR.cc/2026/Conference/Submission21606/Reviewer_kBGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21606/Reviewer_kBGd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051401720, "cdate": 1762051401720, "tmdate": 1762941851693, "mdate": 1762941851693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}