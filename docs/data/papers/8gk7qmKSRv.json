{"id": "8gk7qmKSRv", "number": 14704, "cdate": 1758242091778, "mdate": 1759897353870, "content": {"title": "Towards demystifying the optimization landscape of RLVR methods", "abstract": "GRPO has achieved impressive success in the landscape of reasoning models. However, the motivation behind its origins along with the reasons for its effectiveness remain elusive. In this work, we fill some of the gaps and demonstrate that in on-policy setting, GRPO's optimization can be viewed as a weighted combination of maximization of likelihood for correct rollouts and minimization for the incorrect ones. This finding gives a different perspective on the optimization landscape of GRPO. Motivated by this, we analyze the positive and negative part of GRPO's objective function independently, and find that their global minima correspond to undesired solutions. While optimization of the positive term leads to entropy minimization and length collapse, optimizing for the negative term leads to entropy maximization and length explosion. Using this lens, we show the presence of instability in on-policy training of some recent algorithmic advances trying to simplify GRPO's objective. Surprisingly, we find that PPO is also susceptible to such training instabilities. However, despite the presence of bad global minima in GRPO's objective function, it doesn't converge to either of them. We identify design choices in GRPO's advantages that aid convergence of GRPO to good minima. We also demonstrate the effectiveness of using clipping in stabilizing the optimization process, thereby preventing training instabilities even when training only for minimizing the likelihood of incorrect rollouts. This highlights the surprising stability of off-policy methods as compared to using their on-policy versions.", "tldr": "", "keywords": ["Reasoning", "Language Models", "GRPO"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d8bce23c00d4a6ccc3fa0acb5cd6f629bf2f323.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper demystifies the GRPO algorithm, widely used for LLM reasoning, by re-framing its on-policy optimization as a weighted combination of maximizing the likelihood of correct rollouts and minimizing that of incorrect ones. The authors demonstrate that optimizing either of these objectives independently leads to unstable bad minima, characterized by either entropy collapse or entropy explosion. The study reveals that GRPO's success stems from its specific advantage calculation and the use of clipping, which work together to stabilize the training process and prevent convergence to these undesirable solutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n\n2. The paper offers an intuitive way to understand the optimization of RLVR methods. It reframes the objective as a balance between two competing forces: maximizing the likelihood of correct responses and minimizing the likelihood of incorrect ones. This model clearly explains why simpler methods are unstable, showing they can lead to \"length collapse\" or \"length explosion\" when one force overpowers the other.\n\n3.  The paper presents a practical discovery: off-policy training, when combined with clipping, can be significantly more stable than its on-policy counterpart. This finding challenges common assumptions about on-policy methods and provides a valuable, counter-intuitive insight for practitioners."}, "weaknesses": {"value": "1. While the paper's analysis of the problem is insightful, the solution it proposes—using token-level normalization—is not new. As the authors acknowledge, this technique is already a key component in several other recent and successful methods. Therefore, the paper's contribution feels more like a strong explanation for why an existing method works, rather than a new solution derived from its analysis indicating that, while the analysis is valuable, it doesn’t lead to any substantively new insight or proposal derived from the authors’ interpretation and findings.\n\n2. The paper's core claims are about training instability and collapse, which are phenomena often highly sensitive to random seeds and initialization. The authors state that all experiments were run only once. This could be regarded as a significant limitation considering the instability of GRPO algorithm. The claims would be much stronger if they were supported by results averaged over multiple runs(seeds) to show variance and confirm that the observed collapses are consistent.\n\n3. While the paper identifies that clipping is the key to off-policy stability, it admits a \"do not have a complete understanding\"  of the underlying mechanism why it works. Understanding how clipping \"induces stability\" is left as an \"interesting future direction\", making the paper's \"demystification\" partially incomplete.\n\n4. I believe that \"RLVR method\" term in the title is too broad considering the algorithm handled in the paper. I would recommend that the authors consider changing the title. (for instance, changing \"RLVR method\" into GRPO)"}, "questions": {"value": "1. The paper attributes the observed instability primarily to the 'on-policy setting' itself. However, the GSPO[1] posit that the instability in GRPO does not stem from the on-policy setting, but rather from the high-variance noise introduced by its fundamentally flawed 'token-level importance sampling' design. GSPO algorithm is also an off-policy method, yet it achieves stable training where GRPO fails. Do the authors believe their perspective—that clipping is the critical component for stability in combined with off-policy methods—generalizes to sequence-level algorithms like GSPO as well? In other words, under a sequence-level optimization framework, do you still consider the clipping mechanism to be equally critical for maintaining stability than on-policy setting??\n\n[1] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, & Junyang Lin. (2025). Group Sequence Policy Optimization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h7BXT0Sazg", "forum": "8gk7qmKSRv", "replyto": "8gk7qmKSRv", "signatures": ["ICLR.cc/2026/Conference/Submission14704/Reviewer_HVFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14704/Reviewer_HVFW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727593046, "cdate": 1761727593046, "tmdate": 1762925069305, "mdate": 1762925069305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the GRPO algorithm for optimizing LLMs in a reinforcement learning with verifiable rewards (RLVR) setting. The findings highlight differences between on-policy and off-policy training, the importance of training both on positive and negative samples, and the importance of likelihood ratio clipping. The authors show that removing certain components can lead to training instabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides an in-depth analysis of different components of the RLVR optimization of LLMs. Evaluation is done on different datasets, and the trends seem to be similar across the different datasets."}, "weaknesses": {"value": "Certain parts of the paper are not very clear (see Questions).\n\nFurthermore, the paper claims that \"PPO collapses in on-policy setting\". This claim seems misleading. On-policy PPO here means that $\\pi_{\\theta_\\mathrm{old}} = \\pi_\\theta$, which means that the ratio $\\pi_\\theta(a|s) / \\pi_{\\theta_\\mathrm{old}}(a|s)$ is always 1. This, in turn, means that the clipping is never active, and what is left is essentially a vanilla policy gradient algorithm (similar to on-policy GRPO in eq. 6). Since the clipped loss is the central component of PPO, this notion of \"on-policy PPO\", therefore, does not bear a lot of resemblance to PPO anymore."}, "questions": {"value": "1. Figure 1 is not clear. What exactly is 1(a) showing? Is this just an illustration, or is this some visualization of a loss landscape? Which loss is shown here? I assume the positive + negative likelihood loss? What is C_DL? The caption only says that it \"leads to improved performance\". (b) and (c) are also not clear. What do the black dots, arrows, and orange / brown curves represent? What are the little lines on the top right of the curves? \n\n2. The caption of Figure 1 states that \"importance sampling reduces the norm of the gradients, resulting in slower convergence\". However, I did not find any data in the paper backing this up.\n\n3. C_NL and C_PL are the minima of L_NL and L_PL, respectively. This should not directly mean that the they are also minima of L_CL, which is the (weighted) sum of L_PL and L_NL, but the paper often treats these points as minima of L_CL or even GRPO's loss (e.g., in section 4.3 \"Clearly, the two critical solutions C_PL and C_NL [...], are critical solutions of L_CL as well\" or in the Figure 1(a)). I would appreciate it if the authors could clarify why these points are also minima of L_CL / GRPO's loss. \n\n4. Section 4.3.: At some point, the critical solutions are referred to as S_PL and S_NL, instead of C_PL and C_NL. Do both refer to the same thing?\n\n5. The paper repeatedly claims that it is surprising that off-policy training is more stable than on-policy training, e.g., in section 7. To me, this does not seem surprising since in on-policy training, problematic updates have a very immediate and potentially catastrophic effect on the training data for the next updates, which makes it hard to recover from the suboptimal update. In the off-policy case, the data-collecting policy and the optimized policy are somewhat decoupled, which can help with this problem. Increasing the stability of training was also the reason why, e.g., DQN uses a replay buffer. I would appreciate it if the authors could elaborate on why they expect on-policy training to be more stable than off-policy training.\n\nTypos:\n\n1. Section 4.1: \"decease\" --> \"decrease\"\n\n2. Section 4.1: \"in length of model's entropy\" \n\n3. Section 4.3.: \"the gradients becomes zero\" --> \"the gradients become zero\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nii39WiZPp", "forum": "8gk7qmKSRv", "replyto": "8gk7qmKSRv", "signatures": ["ICLR.cc/2026/Conference/Submission14704/Reviewer_f2dX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14704/Reviewer_f2dX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818603068, "cdate": 1761818603068, "tmdate": 1762925068743, "mdate": 1762925068743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper starts by analyzing GRPO. The authors \"demystify\" it by breaking its objective function into two parts: a positive term that maximizes the likelihood of correct answers and a negative term that minimizesthe likelihood of incorrect answers.\n\nThe paper's key findings are:\n* GRPO is stable because its advantage calculation acts as a built-in stabilizer, preventing it from converging to these bad solutions.\n* PPO's value estimators have \"large errors from the true estimates\".\n* Off-policy training with clipping is more stable than standard on-policy training, as clipping also prevents these collapses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to read and easy to follow.\n2. The hypotheses raised by the authors are sound and accompanied by experiment observations.\n3. The authors carry out experiments on multiple datasets, which cross validate their ideas."}, "weaknesses": {"value": "1. The instabilities and collapses the authors identify are well-known failure modes that arise from the combination of function approximation, bootstrapping and off-policy learning, namely deadly triad [1]. \n2. PPO's value estimators having \"large errors from the true estimates\" is also a well known issue as critic models tend to overestimate values [2].\n3. Becuase of 1 and 2, I question the novelty of this paper. i.e., I don't think this paper has enough new insights, nor does this paper offers novel solutions (clipping is not novel) to the above findings that achieve SOTA results.\n4. I also think Figure 1 is really confusing. Why on the loss surface, C_DL is the most optimal trajectory but C_NL leads to the minimum entropy? Also I don't understand the illustration of Figure 1(c) completely.\n\n[1] Van Hasselt H, Doron Y, Strub F, Hessel M, Sonnerat N, Modayil J. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648. 2018 Dec 6.\n\n[2] Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 30, No. 1)."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LD2ioQpOJI", "forum": "8gk7qmKSRv", "replyto": "8gk7qmKSRv", "signatures": ["ICLR.cc/2026/Conference/Submission14704/Reviewer_LhWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14704/Reviewer_LhWV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982095035, "cdate": 1761982095035, "tmdate": 1762925068415, "mdate": 1762925068415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to deliver concrete insights into the specific reasons that GRPO has proven to be an effective RLVR optimization technique for reasoning. The work presents a series of ablative experiments, focused primarily on elements of policy gradient algorithms deriving from PPO (e.g. clipping, reweighting with advantage functions, etc) to demonstrate the differences in training stability as well as investigate the differences between on- and off-policy training."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I am grateful for works such as this paper that attempt to explore the specific contributions of known methods. These papers are critically important for improved scientific understanding and help to develop improved algorithmic approaches. This papers sets an ambitious goal to study the effects of various elements of GRPO across datasets, model family as well as training paradigm. \n\nI felt that the most interesting section of the analysis came in Section 4.3 where the paper dug into the reweighting mechanism of GRPO which appears to balance between negative and positive generations. This led into a potentially deeper insight into the approximate advantage function used in GRPO. I was left wishing that the majority of the paper focused on this analysis rather than trying to cover every aspect of what the authors identified as contributions of GRPO for RLVR."}, "weaknesses": {"value": "Overall, I'm not entirely sure that this paper introduced any novel insights beyond those identified in the literature they cited that have deeply investigated specific aspects of GRPO. While these unified surveys can be really useful if done rigorously and thoroughly, I do not feel that this paper meets those criteria. In many ways, it feels that the paper is trying to do too much at once and comes across as unfocused. This led to an overly distracted presentation in the paper where proposed insights are not deeply motivated or justified.\n\nPerhaps the major flaw of this work is that it comes across as unaware of the RL theory underlying policy gradient methods, where many of the proposed insights about stability, variance and the trade-offs between on- and off-policy training have been well understood for decades. The discussion in the final paragraph largely restates the principled motivations that led to the development of trust region policy gradient approaches (of which TRPO and, later, PPO derive from). Policy gradient methods have been known to be reweighted MLE objectives since their introduction. Reformulations of policy gradient methods as weighted regression have further established this relationship (see, Peters and Schaal, \"Using Reward-weighted Regression for Reinforcement Learning of Task Space Control\" (2007)). \n\nWithin the perspectives of LLM Reasoning, where I feel the authors are largely situated within, I think that there are some errors in the proposed development of the component losses for positive and negative generations individually. This is especially true within the lens that these are ablations of the GRPO objective since there is no controlling of simplified advantage function being a relative estimate of the group. Without this, it's not clear whether or not $L_{PL}$ and $L_{NL}$ are valid comparisons. There was not sufficient justification or grounding of these re-derivations to ensure that the policy gradient objective was not affected through the use of a biased baseline (for more about the use of baselines in policy gradient approaches, I'd highly recommend this recent blog post: https://fatemi.github.io/posts/pg-baseline/).\n\nThe GRPO objective provided in Equation 3 is incomplete as there is no aggregation over the group, including length normalization. Please revisit Shao, et al (2024). This omission leads me to have less confidence in the remaining development of the various objectives and the resulting analyses. This concern extends to the imprecise manner in which the advantage approximations are made throughout Section 4. \n\nThe insights from Takeaway 2 are well established in the community, particularly those surrounding the effect of negative gradients. Please see the following two papers for detailed analysis.\n- Setlur and Yang, et al (2025), \"e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs\"\n- Fatemi, et al (2025), \"Concise Reasoning via Reinforcement Learning\""}, "questions": {"value": "I do not have any further questions for the authors beyond the concerns raised in the \"Weaknesses\" section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t6vA3zdXM0", "forum": "8gk7qmKSRv", "replyto": "8gk7qmKSRv", "signatures": ["ICLR.cc/2026/Conference/Submission14704/Reviewer_meCw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14704/Reviewer_meCw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14704/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130019375, "cdate": 1762130019375, "tmdate": 1762925068104, "mdate": 1762925068104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}