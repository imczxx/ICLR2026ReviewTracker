{"id": "Exk9KSgMVl", "number": 17693, "cdate": 1758279379756, "mdate": 1759897159698, "content": {"title": "Leaner Transformers: More Heads, Less Depth", "abstract": "Transformers have reshaped machine learning by leveraging attention to capture complex dependencies, driving major advances across domains. Their success has fueled the belief that ever-larger models are required for strong performance. In this paper, we challenge this assumption by showing that many transformers are unnecessarily oversized. We present a theoretical principle that redefines the role of multi-head attention, demonstrating that multiple heads improve the conditioning of the Jacobian of the attention block. Guided by this insight, we redesign popular architectures with more heads and fewer layers. This trade-off reduces parameter counts by up to 30--50\\% while preserving accuracy, yielding leaner yet equally effective models. We validate our approach across a range of transformer-based architectures and scales, showing consistent benefits on tasks in computer vision (ImageNet-1k) and language and sequence modeling (GLUE, TinyStories, and the Long-Range Arena benchmark).", "tldr": "", "keywords": ["multi-head attention", "theoretical approach to efficient transformers"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07e8bd236b7bc3f0de0e3dbe9d792e9b77968e16.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper primarily investigates the role of attention heads in Transformer architectures, analyzing how the number of heads affects model performance across different Transformer variants. It explores the trade-off between the number of heads and the number of layers, demonstrating that appropriately scaling these components can lead to improved performance and higher accuracy. Furthermore, the paper shows that increasing the number of heads enhances the conditioning of the Jacobian of the attention matrix, an effect supported by empirical results on Vision Transformers (ViT). The authors also conduct experiments on different domains like Vision Transformer architectures as well as GPT and LRA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Strengths\n\nThe paper have multiple strengths:\n\n* **Clarity in Presentation:** The main results are presented clearly and effectively, with key findings highlighted in well-designed boxes that make them easy to follow and visually accessible.\n* **Jacobian Analysis:** The analysis of the Jacobian matrix of the attention mechanism is a valuable and insightful direction. It provides a deeper understanding of the conditioning of the attention matrix and aligns well with the objectives outlined in the related literature.\n* **Comprehensive Vision Experiments:** The paper conducts extensive and thorough experiments on Vision Transformer models, including both base and large-scale variants, demonstrating a strong experimental design and analysis."}, "weaknesses": {"value": "While the experimental setup in the paper is extensive, there are several notable shortcomings:\n\n**1) Simplistic Preliminary Section**\nThe preliminary section is overly simplified. For instance, *Equation (2)* omits the layer normalization steps between layers. Additionally, in the paragraph following *Equation (3)*, the similarity metric for attention is denoted as $\\phi$, which should ideally be expressed as $\\phi(q, k)$. However, the paper describes it as *softmax($\\phi(q, k)$)*, a formulation that lacks clarity and may lead to ambiguity in interpretation.\n\n---\n\n**2) “Why Scale the Number of Depths?”**\nThis section argues that increasing model depth leads to richer feature representations. Although this is intuitively reasonable, the authors provide neither visualizations nor quantitative evidence to substantiate the claim. For example, visualizations of attention maps could demonstrate whether additional heads actually learn more diverse or complementary features. Furthermore, while deeper models can indeed capture higher-level abstractions, the paper does not rigorously analyze or quantify this effect.\n\n---\n\n**3) Increasing the Number of Heads leads to more parameters**\nAs mentioned on page 5, increasing the number of attention heads substantially raises the model’s parameter count. To better understand the trade-off between *depth* and *heads*, the authors could conduct experiments where model depth is increased while keeping the number of heads constant, so maintaining approximately the same parameter budget. Such an ablation would provide clearer insights into whether performance gains arise from additional heads or simply from a larger model capacity.\n\n---\n\n**4) Weakness of the LRA Benchmark**\nThe Long Range Arena (LRA) benchmark is no longer considered a strong evaluation framework for transformers. Recent works have demonstrated that State Space Models (SSMs) achieve significantly higher accuracy on these tasks, while transformers can perform well when pre-trained with self-supervised objectives (see [1]). Moreover, improvements reported on LRA tasks are often marginal and may not reflect meaningful advancements in model architecture.\n\n---\n\n**5) Incomplete Experimental Comparisons**\nAlthough the experiments span multiple tasks, they lack comprehensive one-to-one comparisons. The study focuses primarily on increasing the number of heads, without adequately comparing this to scaling model *depth* or *width*. A more systematic analysis—both numerical and theoretical, would strengthen the conclusions and provide a deeper understanding of how these factors influence model performance.\n\n---\n\n**Typos and Formatting Issues**\n\n* *Equation (11)* contains a typographical error in the parentheses, one bracket is larger than the other.\n* In *Section 4.1.1*, the patch size is written as *16!x!16*, which appears to be a formatting error (the exclamation marks should be removed).\n\n---\n\n### Reference\n\n[1] Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors  Ido Amos, Jonathan Berant, Ankit Gupta"}, "questions": {"value": "The main questions are mentioned in the weakness section but can be summarized as follows:\n\n1) Is there any theoretical proof that, under the same parameter regime, increasing depth or the number of heads is more beneficial than the other?\n\n2) If not, are there any experimental results showing that increasing the number of heads (which leads to more parameters) performs better than increasing the depth? A one-to-one comparison would help clarify how scaling depth versus the number of heads contributes to the model’s overall performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gOwdNWvXPU", "forum": "Exk9KSgMVl", "replyto": "Exk9KSgMVl", "signatures": ["ICLR.cc/2026/Conference/Submission17693/Reviewer_QqCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17693/Reviewer_QqCC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668368542, "cdate": 1761668368542, "tmdate": 1762927540314, "mdate": 1762927540314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the overparameterization trend in Transformers and proposes a new design principle: “More heads, less depth.” The authors provide a theoretical analysis showing that increasing the number of attention heads improves the conditioning of the attention Jacobian, which should stabilize optimization. They demonstrate empirically that one can reduce network depth while maintaining or improving accuracy, achieving 30–50% parameter reduction across vision (ImageNet), language (GLUE, TinyStories), and long-context (LRA) benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a mathematically grounded reinterpretation of multi-head attention’s role in optimization.\n- Consistent empirical validation across domains (vision, NLP, long-sequence).\n- Strong practical message toward leaner Transformer design.\n- The combination of theoretical insight and empirical validation across different domains makes the findings broadly relevant and practically actionable."}, "weaknesses": {"value": "- Core assumptions (independent Gaussian heads, isotropy, bounded singular values) are unrealistic during actual training; their validity beyond initialization is unclear.\n- The theoretical link between Jacobian conditioning and global optimization/generalization remains unproven.\n- Experiments are limited to mid-scale models (<200M parameters); it remains unclear whether the same depth-head trade-off persists under large-scale pretraining or diverse data distributions."}, "questions": {"value": "- Can the authors empirically measure Jacobian conditioning or NTK spectra after training to verify that it remains improved, not just at initialization?\n- Can you provide quantitative results linking improved Jacobian conditioning to faster convergence or better generalization performance?\n- As model and data scales grow, the assumptions of independent Gaussian heads, isotropy, and bounded Jacobian singular values may become less valid. How do the authors expect these assumptions and the resulting conditioning behavior to evolve in larger Transformer regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V6RbIKwn0m", "forum": "Exk9KSgMVl", "replyto": "Exk9KSgMVl", "signatures": ["ICLR.cc/2026/Conference/Submission17693/Reviewer_5iHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17693/Reviewer_5iHU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886431596, "cdate": 1761886431596, "tmdate": 1762927539851, "mdate": 1762927539851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors first present theoretical results that show that increasing the number of heads in a transformer improves the conditioning of the Jacobian of the attention block. They then design new architectures with more heads and fewer layers to exploit this insight, reducing parameter count whilst still maintaining high accuracy across a range of tasks that include vision, language, and sequence modelling."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The mathematical and empirical analysis is clearly presented\n2. The mathematical analysis generates interesting insights which then strongly motivate the architectural changes explored\n3. The empirical analysis is extensive, covering several different domains and task types\n4. The empirical analysis is detailed, aiding reproducibility, and conducts lots of different experiments to validate the hypotheses of the paper\n5. The empirical results are strong, and give good evidence to the hypotheses of the paper"}, "weaknesses": {"value": "1. The authors may wish to cite further prior empirical work showing that one can make this tradeoff between attention head count and layer size while still preserving accuracy, such as https://arxiv.org/abs/2210.00640, or https://proceedings.neurips.cc/paper_files/paper/2023/hash/3504a4fa45685d668ce92797fbbf1895-Abstract-Conference.html\n2. Given the dominance of scaling GPT-style transformers and language-modelling in today's AI landscape, it would be interesting to see how the improvements from this architectural change are affected when used in larger LMs. A plot or analysis of train loss on a large language corpus across model sizes from 100s of millions to 10s of billions parameters would be very interesting to see.\n\n#### Typos (Score given assuming these will all be fixed):\n1. It's not material to the rest of the paper, but equation (1) should be (ignoring normalisation): $\\mathbf{T}(X) = \\mathbf{F}(\\mathbf{A}(X) + X) + \\mathbf{A}(X) + X$\n2. On line 146/147 it's stated \"and $h$ and thus $h$ must divide $D$.\" Presumable one of these $h$'s should be $d$?\n3. Line 291, \"$16! \\times !16$\", presumably the !s shouldn't be there?\n4. Line 338/339 \"number of heads (left) ... head dimension (left)\". Both plots list number of heads on the x-axis. Same patter for figure 3 (line 357). These captions and plot labels are inconsistent with the main text, though it can be easily inferred what is meant (hence typo classification for these errors)\n5. Line 367/368 \"Row is the standard ViT-B...\" which row?"}, "questions": {"value": "1. For the language modelling experiments, how do you think your results will be affected as the model size is scaled up into billions of parameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CoTAETTV7T", "forum": "Exk9KSgMVl", "replyto": "Exk9KSgMVl", "signatures": ["ICLR.cc/2026/Conference/Submission17693/Reviewer_NMme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17693/Reviewer_NMme"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927208384, "cdate": 1761927208384, "tmdate": 1762927539222, "mdate": 1762927539222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the convention that larger vision transformers are required for strong performance and shows that many transformers are unnecessarily oversized. The authors present a theoretical principle that redefines the role of multi-head attention and redesign popular architectures with more heads and fewer layers. Empirical results shows that this trade-off reduces parameter counts by up to 30--50% while preserving accuracy, and experiments on multiple tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The direction of reducing the depth of vision transformers seems interesting and indeed worths exploration. The depth of deep networks plays a more essential role compared to width in terms of practical efficiency and it is appreciated that the authors have provided theoretical insights to validate the proposed scheme. \n2. The writing is clear and the paper is easy to follow. The overall structure is well organized and the idea is presented in a coherent manner.\n3. It is appreciated that the authors have conduct experiments on different backbones and various tasks, spanning from computer vision to language and sequence modeling."}, "weaknesses": {"value": "1. The technical contribution of the paper seems limited. The core idea, i.e., more heads and fewer layers leads to better trade-off between efficiency and accuracy, looks like the empirical finding which can hardly be regarded a systematic methodology. Although the authors have provided theoretical insights, it does not provide a systematic guideline of how to adjust the trade-off between heads and layers. Is the trade-off chosen from empirical results per architecture?\n2. While the rationale has been validated on high level understanding tasks, its efficacy on dense prediction tasks like sementic segmentation or object detection remains unverified. It would better if the authors could validate the proposed method on dense prediction tasks as well.\n3. The ablation is missing. It seems that the authors have different criterion on choosing the trade-off between heads and layers for different specific models, and the ablation on this part is missing.\n4. Although the authors have shown advantage of the proposed method in model parameters and memory usage, the practical inference speedup is unknown and it is better to compare the CPU and GPU latency of the proposed architecture with the conventional structure."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0wA7YhfDLa", "forum": "Exk9KSgMVl", "replyto": "Exk9KSgMVl", "signatures": ["ICLR.cc/2026/Conference/Submission17693/Reviewer_qDS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17693/Reviewer_qDS2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972760167, "cdate": 1761972760167, "tmdate": 1762927538725, "mdate": 1762927538725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper takes a step back and questions design habit of Transformers: the tendency to scale mainly by depth.\nThe authors develop a simple argument that increasing the number of attention heads improves the conditioning of the attention Jacobian, which in turn stabilize optimization. Based on that, they explore a trade-off between heads and layers. They show that, for the same FLOPs, a shallower network with more heads can match or even slightly exceed the performance of deeper ones. They demonstrate this on ImageNet (ViT), GLUE, TinyStories, and LRA benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The theoretical idea is fresh and intuitive. It offers a new perspective on why multi-head setups are easier to train than single-head variants.\n- The empirical validation is broad. I like that the authors show the results in both vision and language tasks. The Imagenet and GLUE results are convincing. The savings in memory and parameters are impressive. \n- Many transformer related papers are technically heavy and having unclear design guidance. But this paper contributes a simple rule of thumb that practitioners can test immediately."}, "weaknesses": {"value": "- The theoretical section assumes independence and isotropy among heads, which probably doesn't hold in trained models. It would help to at least show empirical correlation plots or conditioning statistics from real networks to bak this up.\n- The experiments are all in the mid-scale regime (ViT-B/L, BERT-base size). I would have liked to see a preliminary results on a > 1B-parameter model to check whether the rule survives modern LLM training dynamics. \n- I'm not entirely convinced the performance improvements stem directly from the better conditioning. It could just be that redistributing parameters across heads changes the inductive bias or regularization behavior. Some ablations around learning rate or normalization might clarify that. \n- The paper could discuss inference speed a bit more carefully. Fewer layers should reduce sequential latency, but many-head attention can be bandwidth-limited."}, "questions": {"value": "1. You assume independence and isotropy among head outputs in the conditioning analysis. Could you include empirical evidence, e.g., inter-head correlation or Jacobian spectra from trained models, to verify that real Transformers approximately satisfy these assumptions?\n2. Have you tried this head–depth trade-off on larger models (hundreds of millions to billions of parameters)? It would be interesting to see whether the same effect holds once training dynamics and regularization at scale come into play.\n3. Could the observed benefit come from changed regularization or parameter distribution rather than better Jacobian conditioning? For instance, have you compared against simply redistributing parameters into wider MLP blocks?\n4. Prior work (e.g., Michel et al., 2019) showed that many heads can be removed post-training with little effect. How do you reconcile your finding that additional heads help optimization with evidence that many heads are redundant at convergence?\n\nReference:\n[Michel et al., 2019] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JnxKKIJGlW", "forum": "Exk9KSgMVl", "replyto": "Exk9KSgMVl", "signatures": ["ICLR.cc/2026/Conference/Submission17693/Reviewer_pNzE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17693/Reviewer_pNzE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036105576, "cdate": 1762036105576, "tmdate": 1762927538204, "mdate": 1762927538204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}