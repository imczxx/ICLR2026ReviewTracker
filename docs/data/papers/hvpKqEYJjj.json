{"id": "hvpKqEYJjj", "number": 19648, "cdate": 1758297966257, "mdate": 1763752518819, "content": {"title": "A Theoretical Analysis of Mamba’s Training Dynamics: Filtering Relevant Features for Generalization in State Space Models", "abstract": "The recent empirical success of Mamba and other selective state space models (SSMs) has renewed interest in non-attention architectures for sequence modeling, yet their theoretical foundations remain underexplored. We present a first-step analysis of generalization and learning dynamics for a simplified but representative Mamba block: a single-layer, single-head selective SSM with input-dependent gating, followed by a two-layer MLP trained via SGD. Our study adopts a structured data model with tokens that include both class-relevant and class-irrelevant patterns under token-level noise and examines two canonical regimes: majority-voting and concentration-type sequences. We prove that the model achieves guaranteed generalization by establishing asymptotic sample complexity and convergence rate bounds, which improve as the effective signal increases and the noise decreases.\nFurthermore, we show that the gating vector aligns with class-relevant features while ignoring irrelevant ones, thereby formalizing a feature-selection role similar to attention but realized through selective recurrence. Numerical experiments on both synthetic and real-world data justify our theoretical results. Overall, our results provide principled insight into when and why Mamba-style selective SSMs learn efficiently, offering a theoretical counterpoint to Transformer-centric explanations.", "tldr": "", "keywords": ["Training Dynamics", "Feature Learning", "Generalization", "Mamba", "Learning Theory", "Selective State Space Models"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65557062017a3bf36d35de940fc0b13a878ed643.pdf", "supplementary_material": "/attachment/c3883063f66818aeb7ba87b4ed954ca64ac13f2b.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a formal theoretical study of the Mamba architecture. It analyzes a simplified single-layer Mamba followed by a two-layer MLP trained with gradient descent on two synthetic data regimes — a majority-voting model and a locality-structured model — where class labels depend on subsets of relevant features within the input sequence. It derives non-asymptotic convergence and generalization guarantees, proving that Mamba’s gating mechanism learns to amplify class-relevant features while suppressing irrelevant ones. The analysis links learning efficiency and generalization to interpretable data parameters such as signal-to-noise ratio and the fraction of informative tokens. Synthetic experiments confirm these trends qualitatively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, clearly structured, and theoretical results are supported by rigorous proofs with proof intuitions given in the main text.\n2. The paper is well-situated in the literature. I would only suggest to add [1,2], when discussing the connection of SSMs and Transformers in L147-154, for additional reading on how Mamba relates to attention, altough not critical.\n3. The theoretical analysis of the learning process of Mamba is novel and timely, as Mamba has become an alternative to Transformers.\n4. The results are convincing, however their generality is not assessed yet.\n5. The paper does a good job in linking the empirical findings to the theoretical results, supporting each statement with empirical evidence.\n\n[1] Dao & Gu (2024), \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\", https://arxiv.org/abs/2405.21060\n\n[2] Sieber et al. (2024), \"Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks\", https://arxiv.org/abs/2405.15731"}, "weaknesses": {"value": "1. The main drawback of the paper is its limited scope, both in its model selection and the application setting. While chosing a single model (in this case Mamba) for such a study is reasonable, I would have expected a broader application setting, i.e., not limiting the study to binary classification.\n2. The investigated data models are purely theoretical and are not representative of more realistic data models, e.g. the Gaussian distribution assumption is not realistic for practical applications. Having a discussion on this point in the paper would be appreciated.\n3. The theoretical results are not novel in itself and are mainly extensions to the Mamba specifics. Altough I think this is not a major problem, but it reduces the originality of the contribution.\n4. The paper could do a better job at clarifying its limitations, e.g. with a paragraph in the conclusion.\n\n\nMinor points:\n- The short paragraphs before lemmas/theorems already discuss the results of the lemmas/theorems before they are stated. It would be better for the flow to reserve this discussion for after the lemma/theorem and only give a brief introduction to which results is shown next.\n- The remarks after the results are nice, but should not be wrapped in a proof environment, i.e., should not have a square at the end.\n- the empirical results provide no quantitative comparison of actual to theoretical convergence rates and are purely qualitative. Including, such a comparison/result would strengthen the paper."}, "questions": {"value": "1. Have you observed any empirical evidence suggesting these theoretical results hold beyond your simplified setup? Or how do you think these results fair in practice?\n2. Can your analysis or conclusions be generalized to more complex or non-Gaussian data distributions (e.g., correlated, heavy-tailed, or structured inputs as in language or vision)? If not, what aspects of Mamba’s behavior might your theory fail to capture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0bRayXUiMQ", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Reviewer_T7xH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Reviewer_T7xH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924600568, "cdate": 1761924600568, "tmdate": 1762931501018, "mdate": 1762931501018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims to provide the first theoretical study of the learning and generalization dynamics of the Mamba architecture, a selective state space model (SSM) that has recently emerged as an efficient alternative to Transformers. The authors analyze a simplified single-layer Mamba block with input-dependent gating followed by a two-layer MLP trained via gradient descent under two structured data regimes—majority-voting and locality-structured sequences containing both class-relevant and irrelevant tokens. They prove non-asymptotic sample complexity and convergence guarantees, showing that Mamba generalizes efficiently when class-relevant signals are strong or locally concentrated, and that its gating vector naturally aligns with informative features while suppressing noisy ones, formalizing its feature-selection role. Synthetic experiments align with theoretical insights."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s primary strength lies in its original theoretical treatment of Mamba, offering the first formal generalization and convergence analysis for selective state space models—an area that has so far been dominated by empirical studies. In terms of originality, it introduces a novel analytical framework for gated architectures trained with gradient descent, bridging ideas from feature-learning theory and state-space modeling, and extending prior analyses of Transformers to a fundamentally different recurrence-based mechanism. In terms of technical quality, the authors provide rigorous, non-asymptotic sample complexity and convergence bounds under well-defined data models, supported by clear mathematical reasoning and alignment with empirical trends. The paper could benefit from an improved exposition of the theoretical results. In general, the paper is well structured, with intuitive explanations preceding formal results. Regarding significance, the work fills a key theoretical gap by formalizing how Mamba’s gating acts as a feature selector that prioritizes class-relevant signals, thus deepening understanding of why such models generalize efficiently and under what data conditions they outperform attention-based architectures. While it is unclear whether this specific architecture will stand the test of time, this reviewer celebrates the introduction of rigorous analysis of architectural components."}, "weaknesses": {"value": "While the paper makes a valuable theoretical contribution, several weaknesses limit its impact and clarity. First, the analysis is restricted to a highly simplified single-layer Mamba block and may not generalize to deeper or multi-head architectures used in practice; extending the framework to multi-layer dynamics or coupling between blocks would strengthen its relevance. While this is hard to do, authors could comment on whether these results extend to multi-layer settings. Second, the structured data assumptions (majority-voting and locality-structured) are somewhat idealized and may not capture the complexity of real-world sequences; providing evidence that these regimes approximate practical scenarios (e.g., via ablations on natural datasets) would improve the paper’s applicability. Third, the empirical validation is minimal—limited to synthetic data—and lacks comparisons to other theoretically analyzed architectures (e.g., gated linear attention), making it hard to assess the distinct benefits of Mamba’s gating mechanism. Fourth, while the paper claims to establish generalization guarantees, the proofs rely on strong assumptions such as balanced datasets, small token noise, and specific initialization schemes, which could limit robustness. Finally, the connection to prior theoretical work on Transformers and SSMs could be deepened by discussing how Mamba’s multiplicative gating fundamentally changes training dynamics beyond attention-style weighting. Strengthening these aspects would make the work more comprehensive and impactful."}, "questions": {"value": "Please address weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DA2HQIkZBY", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Reviewer_rcjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Reviewer_rcjx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975491469, "cdate": 1761975491469, "tmdate": 1762931500626, "mdate": 1762931500626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "GR1: general Response on \"additional experimental results.\" [@Reviewer Sfca, @Reviewer 8FgG, @Reviewer rcjx, @Reviewer T7xH]"}, "comment": {"value": "We thank all reviewers for their careful reading of our paper and their constructive feedback. We have revised the manuscript accordingly. We are encouraged that all four reviewers highlighted the novelty and significance of our theoretical contributions, particularly the generalization guarantees, the characterization of Mamba’s gating mechanism, and our framework for understanding feature selection in selective state-space models. We also appreciate the reviewers' feedback that our empirical experiments are closely tied to the theory  and provide supporting evidence for our analytical findings.\n\nBelow, we first provide general responses (GRs) addressing the main concerns raised across reviews. We begin with a unified GR summarizing **all additional experiments** requested by the reviewers. Subsequent GRs then address concerns regarding **the simplified model and its connection to real-world Mamba architectures** and the **validity of our data assumptions**.\n\nPoint-by-point responses are provided after each reviewer’s comment.\n\n# GR1: general Response on \"additional experimental results.\" [@Reviewer Sfca, @Reviewer 8FgG, @Reviewer rcjx, @Reviewer T7xH]\n\nWe thank all reviewers for emphasizing the importance of strengthening the empirical connection between our theoretical analysis and practical Mamba architectures. In response, we conducted additional experiments using the multi-layer, multi-head Mamba model from [R7] trained on synthetic datasets that follow the same structured data regimes as in our theory. We emphasize that these experiments are conducted to further support the theoretical insights developed in this work and do not alter the main contributions of the study.\n\n**First, we conducted experiments using the Mamba2 block, which includes residual connections and RMSNorm.** Our tests included a stacked multi-layer Mamba block ($2$ and $5$ blocks), both configured with $4$ heads. Overall, the empirical results support our theoretical findings: (i) the cosine similarity between the learned gating vectors and both class-relevant and class-irrelevant features matches the predicted behavior from our theory [Figure A1](https://imgur.com/a/MrYJSR6), and (ii) the MLP weights in practical Mamba blocks consistently align with class-relevant feature directions [Figure A2](https://imgur.com/u4NmIXI), exactly as our analysis suggests.\nAdditional experiment results on $5$ blocks Mamba model are summarized in the following table. \n\n\n**Table 1: Cosine similarity alignment in the 5-layer Mamba model.**\n\n\n| |Class-relevant |Class-irrelevant |  \n|----------------|-------------------------------|-----------------------------|   \n|Gating vector |0.53 |0.00 |  \n|MLP weights |0.73|0.00|\n\n\n**Next, we evaluated the effect of the gating mechanism by comparing models trained with and without gating on both structured data regimes.** For the majority-voting data, the gated model consistently outperforms the ungated variant [Figure A3](https://imgur.com/a/an2yH9c). For the locality-structured data, gating becomes essential, since the ungated model fails to learn the task while the gated model converges reliably [Figure A4](https://imgur.com/a/5o3GVpq).\n\n**Thirdly, following the suggestion of Reviewer@Sfca, we conducted two additional ablations.** First, we varied the feature dimension $d \\in \\{32, 64, 128\\}$ and observed that the results are consistent with those reported in the paper [Figure A5](https://imgur.com/a/POgId2e). Second, we varied the data distribution parameter $\\alpha_c$, the fraction of confusion features in the majority-voting data, across three settings, and the empirical results again align closely with our theoretical predictions [Figure A6](https://imgur.com/a/i4Am2nk).\n\n**Finally, to address the concern raised by Reviewer@8FgG regarding the comparison with attention-based baselines, we extended our empirical study to ensure that the attention models operate under their strongest possible configuration.** In the original experimental setup, all architectures, including Mamba, Transformer, and local-attention models, were trained under identical optimizer, learning-rate, and hyperparameter settings. Following the reviewer’s suggestion, we further incorporated positional encodings and tuned the relevant hyperparameters for the Transformer and local-attention baselines to give them the best possible performance. The updated results can be seen in [Figure A7](https://imgur.com/a/3I8Pmy5). The findings remain consistent with our earlier figure, and the Mamba model continues to perform favorably when the class-relevant features are more locally concentrated.\n\n[R7] Dao, Tri, et al. “Transformers are SSMs: Generalized Models and Efficient Algorithms Through\nStructured State Space Duality,” arXiv:2405.21060, 2024."}}, "id": "8HXIkJ7rSq", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763745431139, "cdate": 1763745431139, "tmdate": 1763746066524, "mdate": 1763746066524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical analysis of learning and generalization for a simplified Mamba block (single-layer selective SSM with input-dependent gating followed by a 2-layer MLP) trained by gradient descent on structured sequence data. Two scenarios are studied: majority-voting and locality-structured sequences with token-level noise. The authors prove non-asymptotic sample complexity and convergence guarantees under width and noise conditions, show that the gating vector aligns with class-relevant features while suppressing irrelevant ones, and provide synthetic experiments supporting the theory. They also position Mamba’s dynamics relative to Transformers, arguing that Mamba can match the majority-voting setting and outperform attention baselines on locality-structured data due to selective recurrence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper contains a clear novel theoretical results. In particular, two formal and non-asymptotic bounds are presented for both sample complexity and iteration complexity in two scenarios (Theorems 1 and 2), with interpretable dependence on signal gap, locality separations, step size, sequence length, and noise.\n\n- In addition, the authors provide a rigorous characterization that the gate prioritizes class-relevant features and suppresses confusing ones; connects to attention’s feature-selection role while highlighting differences arising from multiplicative recurrence.\n\n- Synthetic experiments are closely tied to theory, including comparisons where Mamba outperforms Transformer/local attention for locality."}, "weaknesses": {"value": "- My first main concern is the simplified assumptions (single layer, single head, simplified recurrence), which make the considered setting far from practical Mamba stacks (with depth, multi-head structure, residuals, normalization, and learned discretization). However, I understand that a simplified setting is commonly used in theoretical analysis, as the practical setting is usually too complicated to allow for a thorough analysis.\n\n- My second concern is the strong assumptions required in Theorems 1 and 2: the width $m$ must be larger than or equal to the square of the dimension $d$, and the noise level must satisfy $\\tau = O(1/d)$. These assumptions weaken the novelty of the theoretical results. A justification for these assumptions is needed.\n\n- The experimental results are quite limited. The empirical comparison to Transformers and local attention is restricted to synthetic settings, with no controlled ablations for optimizer choice, learning rate scaling, or positional encoding. It is unclear whether attention could close the gap with appropriately tuned locality biases."}, "questions": {"value": "- What would the theoretical results be if a more complex setting of the Mamba block were used? For example, what happens if a normalization layer is added or if the Mamba block is repeated?\n\n- Are the assumptions in Theorems 1 and 2 regarding width and small noise required only for the proofs to work, or are they fundamentally unavoidable? What happens if these assumptions do not hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ak28zAMTOW", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Reviewer_8FgG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Reviewer_8FgG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001327981, "cdate": 1762001327981, "tmdate": 1762931500240, "mdate": 1762931500240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "GR2: general Response to \"the simplified model and the connection between our theoretical setting and real-world Mamba models.'' [@Reviewer Sfca, @Reviewer 8FgG, @Reviewer rcjx, @Reviewer T7xH]"}, "comment": {"value": "# GR2: general Response to \"the simplified model and the connection between our theoretical setting and real-world Mamba models.'' [@Reviewer Sfca, @Reviewer 8FgG, @Reviewer rcjx, @Reviewer T7xH]\n\nWe sincerely thank the reviewers for their thoughtful comments and suggestions. We appreciate that all reviewers recognized the difficulty of providing a full theoretical analysis of the complete Mamba architecture, which is precisely why simplified models are commonly used in theoretical studies. We also agree that experiments on the full model would strengthen the paper and offer additional support for our theoretical insights. Following your suggestions, we have carried out additional experiments on the full model and observed behavior fully consistent with our theoretical predictions, further reinforcing our findings.\n\n\n**First, we conducted additional experiments on multi-layer, multi-head Mamba models, and the results were fully consistent with our theoretical findings.** As suggested by Reviewer@Sfca, we evaluated a multi-layer, multi-head Mamba model under our synthetic data setting. In these experiments, (i) the learned gating vectors aligned with class-relevant features, and (ii) the hidden-layer (MLP) weights likewise converged toward the class-relevant feature directions. These findings further demonstrate that the theoretical mechanisms identified in our simplified analysis extend to practical Mamba architectures. Please see [GR1](https://openreview.net/forum?id=hvpKqEYJjj&noteId=8HXIkJ7rSq) for the detailed results.\n\n\n**Second, we clarify that working with a simplified shallow neural network model is standard practice in the theoretical analysis \nof generalization and convergence.** Most existing results in this area rely on simplified architectures, precisely because full practical models are too complex to admit rigorous, non-asymptotic analysis. This does not prevent these works from offering many interesting and insightful conclusions. Please refer to some recent works [R1, R2, R3, R4, R5] for examples. As highlighted in [R1] and [R2], even in these simplified settings, the optimization landscape remains highly non-convex, and establishing convergence and generalization guarantees is still technically challenging and non-trivial.\n\n\n**Finally, some degree of simplification is not only common but necessary in the theoretical study of modern neural networks.** The training dynamics analysis of the complete Mamba architecture is currently infeasible due to its substantial architectural complexity, including multi-head gating, multiplicative recurrence, discretization, residual pathways, and normalization layers. As noted by Reviewer@Sfca and Reviewer@8FgG, removing these simplifications is practically impossible at present. Similar abstractions are adopted in prior theoretical works on Transformers, CNNs, and GNNs to make the analysis tractable while preserving the essential mechanisms under investigation.\n\n[R1] Li, Hongkang, et al. “A Theoretical Understanding of Shallow Vision Transformers: Learning,\nGeneralization, and Sample Complexity,” International Conference on Learning Representations\n(ICLR), 2023.\n\n[R2] Zhang, Shuai, et al. “Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural\nNetworks,” International Conference on Learning Representations (ICLR), 2023.\n\n[R3] Shi, Zhenmei, et al. “A Theoretical Analysis on Feature Learning in Neural Networks: Emer-\ngence from Inputs and Advantage over Fixed Features,” International Conference on Learning\nRepresentations (ICLR), 2022.\n\n[R4] Li, Hongkang, et al. “How Do Nonlinear Transformers Learn and Generalize in In-Context\nLearning?” International Conference on Machine Learning (ICML), 2024.\n\n[R5] Allen-Zhu, Zeyuan, et al. “A Convergence Theory for Deep Learning via Over-Parameterization” International Conference on Machine Learning (ICML), pp. 242–252, 2019."}}, "id": "rm0XF7CM4I", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763745883953, "cdate": 1763745883953, "tmdate": 1763746190038, "mdate": 1763746190038, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically analyzes the training dynamics of a single Mamba layer wrapped with a two-layer MLP trained via gradient descent. Under assumptions like majority-voting or locally structured data, the authors establish generalization guarantees with improved sample complexity and convergence bounds derived from the characterization of the final solution. The analysis centers on the gating mechanism, a key innovation in modern recurrent models, and is supported by empirical evidence validating the theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "W.1. Expanding the theoretical understanding of Mamba and its gating mechanism is important, as these components are popular in both transformers and modern RNNs.\n\nW.2. The analysis is novel, non-trivial, and sheds light on the training dynamics and the role of critical components in modern architectures."}, "weaknesses": {"value": "**W.1. Connection between theoretical settings and the real world:**\n\nWhile the analysis of the simplified model is interesting, its connection to standard practices in the domain remains vague. I understand that providing theoretical proofs for the full model is challenging and that simplifications (such as using a single-layer model or specific datasets) are standard. I’m not expecting a full theoretical proof for the complete model, but I believe the authors should better link key aspects of the theoretical analysis to real models.\n\nFor example, they could run synthetic tasks inspired by the theoretical analysis on the full model and investigate whether the behavior in which MLP weights align with class-relevant features also appears in the complete models. Similarly, they could examine whether the cosine similarity between the gating vector and both class-relevant and class-irrelevant features is consistent with the theoretical (or single layers)  findings.\n\n**W.2. Related work is missing.**\n\n Several studies analyzing the training dynamics, optimization issues, and generalization properties of Mamba are not mentioned. Examples include [1, 2, 3, 4, 5]. In this sense, some of the claims made in the paper (see ’To the best of our knowledge, this work presents the first theoretical generalization analysis of the Mamba architecture.’ and ‘To the best of our knowledge, we are the first to theoretically analyze the learning and generalization performance of the trained model with gradient descent (GD)’) may be overstated.\n\n**W.3. The experimental analysis is limited:** \n\n(i) Some information is missing or insufficiently detailed. For example, it is unclear which model was used (simplified, single-layer, with or without convolution, and whether parameters were frozen as in Eq. 15).\n\n(ii) The experiments could be improved, for instance, one could empirically analyze multiple models across different data regimes (varying numbers of features, data distribution parameters, etc.). In addition, several architectural ablations could be performed to better understand the dynamics. For example, training the model without gating to see whether it struggles, as the theory predicts, could strengthen the connection between theory and practice.\n\nW.4. Minor: The visibility of Figure 1 should be improved. For example, it would be better to use figures of the same size (the one on the far right is smaller), and the space between the first two figures is too narrow, causing the caption text to almost overlap.\n\n**W.5. Generality can be improved:**\n\n Some parts of the theoretical analysis could be applied to other modern gated RNNs, such as Gated Linear Attention, Gate Delta-Net, RWKV, and RetNet. It would be good to explain which parts of the analysis are relevant to each architecture, and which components in those architectures improve or negatively impact the optimization dynamics or generalization.\n\n___\n\n**References** \n\n[1] Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention  . Honarpisheh. Nips 2024\n\n[2] The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels. Slutzky∗ et al. NIps 2025. \n\n[3] REVISITING ASSOCIATIVE RECALL IN MODERN RECURRENT MODEL. Okpekpe et al.\n\n[4] Repeat after me transformers are better than state space models at copying. Jelassi et al."}, "questions": {"value": "Q.1. I think $v_i$​ in Equation 6 is not introduced before it appears in the equation, and the authors should make this clearer.\n\nQ.2. The visibility of Figure 1 should be improved. For example, it would be better to use figures of the same size (the one on the far right is smaller), and the space between the first two figures is too narrow, causing the caption text to almost overlap.\n\nQ.3. Which aspects of the proof are not relevant for gated linear attention but are relevant only to Mamba layers?\n\nQ.4. Are $W_B$​ and $W_C$​ trained, or is there a typo with the superscript 0 in Eq. 15 for those weights? It seems that the generalization guarantees are derived for a model where some parameters are frozen."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LfpjsW0XFF", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Reviewer_Sfca"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Reviewer_Sfca"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762261027518, "cdate": 1762261027518, "tmdate": 1762931499666, "mdate": 1762931499666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "GR3: general Response to \"validity of our data assumptions'' [@Reviewer rcjx, @Reviewer T7xH]"}, "comment": {"value": "# GR3: general Response to \"validity of our data assumptions'' [@Reviewer rcjx, @Reviewer T7xH] \n\n\nWe thank the reviewers for this important question regarding the practicality of our data models. \n\n**First, we would like to clarify a misunderstanding from Reviewer@T7xH.** The paper does not assume that the data itself follows a Gaussian distribution since our framework is based on a structured data model. Only the initialized weights follow a Gaussian distribution, which is standard and natural in practice. In general, the structured data model is widely used in the theoretical analysis of deep learning and has been shown to capture key properties of real data, as elaborated below.\n\n**Second, structured data assumptions are widely used in theoretical analyses of modern architectures.** These data models can be viewed as abstractions of real data that capture the essence of the studied problem and help formalize important hypotheses observed in practice. The majority-voting model in particular is a standard assumption in recent theoretical studies (e.g., [R1, R2, R3]), including analyses of Transformers and attention mechanisms. \n\n\n**Third, we believe our majority-voting and locality-structured data models introduced in this work capture the structure of many real-world tasks.** On the one hand, the majority-voting data model reflects a common practical pattern in which the label is determined by the aggregate contribution of multiple discriminative sources. For example, in image classification, the class label often reflects evidence from several foreground patches (class-relevant tokens), in contrast to background patches that may contain confusing or irrelevant patterns. On the other hand, the locality-structured data model corresponds to tasks where semantic meaning is concentrated in spatially or temporally localized clusters, while background features are more dispersed. This structure is characteristic of vision tasks such as object detection, localization, and image captioning, as well as audio, speech, and genomics, where decisive information is confined to short, contiguous regions that strongly correlate with the label. **We would like to clarify further that this content was originally presented in Section 4.2 of the paper and is explicitly highlighted in the revised version.**\n\n[R1] Li, Hongkang, et al. “A Theoretical Understanding of Shallow Vision Transformers: Learning,\nGeneralization, and Sample Complexity,” International Conference on Learning Representations\n(ICLR), 2023.\n\n[R2] Zhang, Shuai, et al. “Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural\nNetworks,” International Conference on Learning Representations (ICLR), 2023.\n\n[R3] Shi, Zhenmei, et al. “A Theoretical Analysis on Feature Learning in Neural Networks: Emer-\ngence from Inputs and Advantage over Fixed Features,” International Conference on Learning\nRepresentations (ICLR), 2022."}}, "id": "bclldrQMeS", "forum": "hvpKqEYJjj", "replyto": "hvpKqEYJjj", "signatures": ["ICLR.cc/2026/Conference/Submission19648/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19648/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19648/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763746516867, "cdate": 1763746516867, "tmdate": 1763746516867, "mdate": 1763746516867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}