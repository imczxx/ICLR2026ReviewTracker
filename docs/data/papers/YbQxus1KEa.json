{"id": "YbQxus1KEa", "number": 13357, "cdate": 1758216944956, "mdate": 1763714186780, "content": {"title": "Neural Hamilton--Jacobi Characteristic Flows for Optimal Transport", "abstract": "We present a novel framework for solving optimal transport (OT) problems based on the Hamilton--Jacobi (HJ) equation, whose viscosity solution uniquely characterizes the OT map. By leveraging the method of characteristics, we derive closed-form, bidirectional transport maps, thereby eliminating the need for numerical integration. The proposed method adopts a pure minimization framework: a single neural network is trained with a loss function derived from the method of characteristics of the HJ equation. This design guarantees convergence to the optimal map while eliminating adversarial training stages, thereby substantially reducing computational complexity. Furthermore, the framework naturally extends to a wide class of cost functions and supports class-conditional transport. Extensive experiments on diverse datasets demonstrate the accuracy, scalability, and efficiency of the proposed method, establishing it as a principled and versatile tool for OT applications with provable optimality.", "tldr": "", "keywords": ["Optimal Transport", "Hamilton--Jacobi Equations", "Method of Characteristics", "Class-Conditional Optimal Transport"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd36e376a3297cc8dda84f0c01decf241fab86de.pdf", "supplementary_material": "/attachment/a695bba671ae2d5a2ef9a5e342114fb02f9554a2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a Neural Characteristic Flow (NCF) method for static optimal transport (OT) with translation-invariant costs. Despite solving static OT, the method’s theory begins with the equivalent dynamical formulation, whose optimality condition is governed by the Hamilton–Jacobi (HJ) equation.\nThe core idea is to enforce an implicit solution formula (obtained via the method of characteristics) for the HJ PDE, whose viscosity solution uniquely determines the OT map.\nTo tie the PDE solution to the data (i.e., to satisfy boundary conditions), the authors add an MMD loss between the transported source and the target, combined with a data-driven HJ implicit-solution loss.\nThis design has several advantages: (1) it is a pure minimization approach that avoids the min–max optimization typical of dual methods and trains a single network; (2) although grounded in a dynamical viewpoint, it does not require ODE/SDE simulation; and (3) it jointly learns forward and backward maps from the same potential.\nThe authors also show that the framework extends to class-conditional OT by applying class-wise MMD within the same HJ formulation.\nEmpirically, with the standard quadratic cost, the method is shown to work in low-dimensional settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Implementation of a novel approach via an implicit HJ formulation (from paper Park & Osher, 2025).\n2. Efficient training (no minimax optimization; no ODE/SDE simulation).\n3. Theoretical analyses and guarantees (primarily for the quadratic-cost case)."}, "weaknesses": {"value": "1. **Lack of scalability evidence or discussion.**\nAuthors evaluate the proposed NCF only on low-dimensional datasets; even for MNIST they work in a low-dimensional VAE latent space. The method raises subtle potential scalability issues. First, it requires backpropagation through Jacobian–vector product, which could lead to instability and high memory consumption in higher dimensions. Second, the assumptions required by the theory—global bijectivity, a positive-definite Jacobian everywhere, and a strictly positive sampling distribution—may not hold in practical settings. \n\n2. **Missed state-of-the-art baselines.** \nThe experimental evaluation would be strengthened by including comparisons with recent and relevant static Optimal Transport methods, specifically DIOTM [1] and ENOT [2]. These approaches have demonstrated superior performance and rapid convergence, and a direct comparison is crucial for a comprehensive assessment of the proposed method's contributions.\n\n[1] Choi et al. Improving Neural Optimal Transport via Displacement Interpolation, ICLR 2025\n\n[2] Buzun et al. ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport, NeurIPS 2024"}, "questions": {"value": "1. Why use a VAE latent space for MNIST experiments? Did you try solving the task in the original pixel space? If so, how do performance, stability, and runtime compare? If not, what specifically fails in pixel space?\n2. Why does your method perform best in dimensions d≤16 but lose to MM-v1 at d=32 and d=64? Could you, please, discuss scalabilty of your method taking into account concerns from the weaknesses?\n3. Could you provide an experimental comparison between NCF and DIOTM [1] and ENOT [2] ?\n4. What are some real-world applications for the class-conditional OT method?\n5. What is the advantage of using the Implicit Solution Formula (9) over the original HJB equation (5) (like in DIOTM [1] or HOTA [3])? Were any ablation studies conducted on this point?\n\n[3] Buzun et al. HOTA: Hamiltonian framework for Optimal Transport Advection. https://arxiv.org/abs/2507.17513"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vwz7OhbQFS", "forum": "YbQxus1KEa", "replyto": "YbQxus1KEa", "signatures": ["ICLR.cc/2026/Conference/Submission13357/Reviewer_r22u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13357/Reviewer_r22u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684265909, "cdate": 1761684265909, "tmdate": 1762924004950, "mdate": 1762924004950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm for computing optimal transport maps between the distributions based on the usage of Hamilton-Jacobi (HJ) equation. For this purpose, the authors, first, represent the OT maps using the characteristics of the HJ equations; second, consider an implicit formula for the viscosity solution of HJ equation; third, parametrize the viscosity solution using neural networks and learn it by optimizing the  loss derived from the implicit solution formula and additional MMD regularizer. As a result, this allows for deriving the approximations of forward and backward OT maps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In general, the paper is quite well-written and suggests some theoretical results supporting the constructed algorithm. The proposed algorithm allows for computation of OT maps without the need of min-max optimization or numerical integration of ODEs."}, "weaknesses": {"value": "Some of the mathematical concepts introduced in the paper lack necessary details. First, the definition of Hamilton-Jacobi (HJ) equation given in section 2.2 lacks reference to the literature where it was introduced. The continuity equation mentioned in line 102 is not written directly in the text. Besides, the characterization of the viscosity solution by the system of characteristic ordinary differential equations (CODE) should be supported by the relevant reference. \n\nMeanwhile, the intuition behind some of the ideas which appear in the derivation of the main objective is not sufficiently clear. In lines 204-209, the authors write that since the initial condition in the HJ equation is not known analytically, they introduce an additional loss term to ensure that this condition is appropriate. However, this loss term does not deal with the initial condition directly but rather enforce the solutions of the OT problem derived from the HJ (viscosity) solution to satisfy the marginal condition, i.e., to map the source distribution to the target. Thus, it seems that the addition of this loss term to the main objective serves another goal than the initially stated one, see questions section for my additional concerns on this topic. Moreover, the intuition behind the weighting function $\\rho$ introduced  in the equation (13) is not clarified.\n\nStill, while the authors provide some theoretical results justifying their approach, my major concerns correspond to the **error in the experimental comparisons** provided by the authors in section 6.1. The authors perform testing of their approach on the benchmark providing the ground-truth OT maps between the specific constructed pairs of distributions (Korotin, 2021a) and claim that their approach leads to the best performance. However, one of the competitive approaches (NOT) seems to be not tested correctly. Indeed, NOT approach actually corresponds to [MM:R] which was tested in the benchmark paper (Korotin, 2021a) and outperformed all other methods considered there. However, according to Table 2 in the paper under review, the NOT ([MM:R]) approach leads to awful results in comparison to another [MM-v1] method from the benchmark. This, actually, contradicts the results from the benchmark where the NOT method leads to the best results as I already said. Thus, I kindly suggest the authors check their results and implementation of the NOT approach which probably contains errors. These errors might also have affected the experimental results on the considered color transfer task. Meanwhile, the experimental results in section 6.2.2 show that the proposed approach did not beat the competitor GNOT according to FID metric. Thus, overall practical validity of the approach is unclear until the stated errors are resolved.\n\n**In summary**, I am mostly confused by the revealed errors in the experimental comparisons provided in the paper. These errors raise doubts regarding the results reported for the experiments with high-dimensional Gaussians and the color transfer task. Thus, I could not assign a positive score to the paper until the issues are resolved."}, "questions": {"value": "- Does your method have connections with the unbalanced optimal transport? This question appears since your loss (14) uses a regularization which motivates the learned OT maps to push input distribution to the target one. Such a strategy of softening the marginal constraint of the OT problem is usually exploited in unbalanced OT field. And the related question is – is it necessary to use MMD loss in this  regularization? Or maybe it can be replaced by some f-divergences?\n- Is it possible to generalize your theoretical results in section 5.1 for general types of costs (other than quadratic)?\n- Does the result of your algorithm depend on the choice of the parameter $\\lambda$?\n- Please explain the intuition behind the weighting function $\\rho$ introduced in equation (13)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7L3KwtqyA9", "forum": "YbQxus1KEa", "replyto": "YbQxus1KEa", "signatures": ["ICLR.cc/2026/Conference/Submission13357/Reviewer_nTXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13357/Reviewer_nTXZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840780514, "cdate": 1761840780514, "tmdate": 1762924004356, "mdate": 1762924004356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Single-network and pure minimization training for conditional OT maps. The proposed method eliminates the need for adversarial training or saddle-point optimization that many \"OT-GAN\" or dual potential methods use. NCF uses a single neural network and a single loss function to compute bidirectional maps. This is a significant simplification: prior approaches often involve two networks (e.g. a generator and a discriminator or dual potentials) and tricky min–max optimization, which can be unstable and require extensive hyper-parameter tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very well written and easy to follow. The theoretical motivation, from the dynamical formulation of OT and the Hamilton-Jacobi (HJ) equation to the method of characteristics, is presented clearly. The core idea of leveraging this to derive closed-form, bidirectional maps is elegant.\n\nThe proposed Neural Characteristic Flow (NCF) framework is compelling. Its main advantages using a single neural network and, crucially, avoiding adversarial min-max optimization  are significant practical contributions over common dual-formulation approaches.\n\nThe algorithm is supported by theoretical arguments. The authors provide a consistency analysis (Theorem 5.1) showing that a zero loss recovers the true OT map and a stability analysis (Theorem 5.4), but demonstrating that a small loss guarantees convergence in the Gaussian setting"}, "weaknesses": {"value": "The HJ formulation for OT is well-established. The \"implicit solution formula\" (Equation 9) , which is the foundation for the proposed loss , is explicitly cited from a very recent work (Park & Osher, 2025). The main contribution appears to be the application of this new formula to the OT problem, combined with a standard MMD loss, which was already considered in (Asadulaev, et. al. 2024).\n\nSimilar to other papers in this area, the primary class-conditional experiment is transporting Fashion MNIST to MNIST. This setup (e.g., mapping a \"Trouser\" to a \"1\") feels artificial and does not provide a compelling, practical use case for the class-conditional framework. The colour transfer setup is also very simple and is often easier than mapping between mixtures of Gaussians. \n\nThe experimental validation, while broad, has a significant weakness in the class-conditional setting. In Table 4 (Fashion MNIST → MNIST) , while NCF achieves the highest accuracy (83.42%), its FID score of 18.27 is substantially worse than GNOT (5.26), NOT (7.51), and MUNIT (7.91). The paper attempts to explain the poor FID by blaming the VAE decoder. The authors then provide a \"re-calculated\" FID of 2.73, computed between NCF outputs and VAE-decoded images, but no comparison with other methods are done."}, "questions": {"value": "**Q1**: The Theorem 5.1 assumes regularity conditions. How does the use of an infinitely-smooth neural network reconcile with approximating a potentially non-smooth solution? Does this implicit regularization bias the solution away from the true, non-smooth solution?\n\n**Q2**: Could you please clarify, is \"Optimality\" in Table 1 means guaranteed convergence to the optimum? Why Dual Formulation and Dynamical Models are mentioned as \"No\"? \n\n**Q3**: The GNOT paper considered an image-to-image paired cost setup. Is your method applicable to high-dimensional paired data, forward and backward? Regarding the class-conditional results in Table 4: Given the poor FID score (18.27) compared to baselines like GNOT (5.26), can the method truly be considered competitive for high-fidelity generative tasks without using any decoders? \n\n\n**Minor**: \n\n*Q4*: For Figure 4, it would be interesting to see how the model maps the generated forward samples backward.\n\n*Q5*: The 'integration-free' nature of the model is a key feature. By deriving a closed-form map from the HJ characteristics, the method avoids the need to solve ODEs. However, are  ODEs a real bottleneck for dynamical OT models? There is a well-known trilemma between coverage, sampling and 'accuracy' of generative models (https://arxiv.org/pdf/2112.07804). It would be interested to see a dynamic formulation of the proposed method. Can be obtain bidirectional maps in dynamical OT formulation? Maybe the authors have conducted some experiments using an ODE-like formulation, as done by (https://arxiv.org/pdf/2507.17513) using the HJB framework? The comparison would be very interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D7c7k4ZMFN", "forum": "YbQxus1KEa", "replyto": "YbQxus1KEa", "signatures": ["ICLR.cc/2026/Conference/Submission13357/Reviewer_9wfo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13357/Reviewer_9wfo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986311972, "cdate": 1761986311972, "tmdate": 1762924003986, "mdate": 1762924003986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors' response to all reviewers"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful and constructive feedback. We greatly appreciate the time and effort they devoted to evaluating our work. In response, we have carefully revised the manuscript to address all comments. Changes in the revised manuscript are highlighted in blue for clarity.\n\nBelow, we provide detailed responses to each reviewer’s questions and comments. We believe these revisions have further improved the quality and presentation of our paper."}}, "id": "UqR0X9jtB9", "forum": "YbQxus1KEa", "replyto": "YbQxus1KEa", "signatures": ["ICLR.cc/2026/Conference/Submission13357/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13357/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission13357/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763713678120, "cdate": 1763713678120, "tmdate": 1763713678120, "mdate": 1763713678120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}