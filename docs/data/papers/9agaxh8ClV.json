{"id": "9agaxh8ClV", "number": 15107, "cdate": 1758247828895, "mdate": 1759897328344, "content": {"title": "CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval", "abstract": "We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.", "tldr": "", "keywords": ["Feed-Forward 3D; Dynamic Scene Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45b3a756c581165af56cb8bea54fcaea37312751.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CogniMap3D, a bio-inspired framework for dynamic 3D scene understanding that mimics human cognition by maintaining a persistent memory of static environments. The core innovation lies in integrating three capabilities: a multi-stage motion detection system to identify dynamic objects, a cognitive mapping system for storing and recalling static scenes across multiple visits, and a factor graph optimization for refining camera poses. Evaluations show state-of-the-art performance in depth estimation, pose reconstruction, and 3D mapping, enabling continuous scene understanding over extended periods and repeated visits to the same location."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The cognitive 3D mapping is intuitive and novel, which effectively decouple static and dynamic parts of the scene.\n\n2. The writing quality, experimental results and visualization are good."}, "weaknesses": {"value": "1. The performance of CogniMap3D is close to VGGT in all experiments, but the inference speed is much lower than VGGT. More discussion on the performance is required.\n\n2. What is the memory footprint of the proposed method. Does the proposed memory mechanism require high GPU memory?"}, "questions": {"value": "In Table 1, the proposed method belongs to 'FF' category. However, different from CUT3r and VGGT which are eng-to-end, the proposed method requires an optimization process to refine camera trajectory. Is the optimization feed-forward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3127b1cjBE", "forum": "9agaxh8ClV", "replyto": "9agaxh8ClV", "signatures": ["ICLR.cc/2026/Conference/Submission15107/Reviewer_tg31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15107/Reviewer_tg31"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468061317, "cdate": 1761468061317, "tmdate": 1762925430278, "mdate": 1762925430278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a feed-forward 3D network for dynamic scenes. It separates static and dynamic regions using dynamic mask and introduces a memory system. The results showed more stable camera pose and depth estimation under motion compared with baselines.\n\nContributions:\n\n1. A multi-stage motion-cue module that progressively refines the dynamic mask using geometry and optical flow, which is more stable than the MonST3R masks.\n2. A memory system for mapping, recall, and update, enabling robust relocalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides clear writing and figures. The dynamic mask pipeline is easy to follow.\n2. Geometry and flow cues plus global mean move refine masks and stabilize tracking. The results showed that 3D reconstruction is more stable than baselines in metrics and visualization.\n3. Using 2D/3D features supports fast indexing, and voting + ICP verifies matches. The memory further supplies constraints that help the global optimization of camera trajectories."}, "weaknesses": {"value": "1. VGGT comparison gap. The model initializes pose and depth with VGGT, yet lacks direct, controlled comparisons against VGGT in the visualizations; reconstruction metrics are also close. Given VGGT is not specifically optimized for dynamic scenes, should include more dynamic motion benchmarks and report VGGT vs. your method to substantiate the value of dynamic-mask extraction.\n2. Lack evidence for the memory module. While the paper states that memory provides stronger constraints for trajectory optimization, the ablation focuses on 3D reconstruction only. The specific contribution of memory to pose accuracy/consistency remains under-substantiated. Consider testing memory on longer sequences and evaluating improvements on tasks/settings where VGGT might struggle (e.g., long-horizon drift, relocalization)."}, "questions": {"value": "1. Can you provide **head-to-head comparisons with raw VGGT outputs** on scenes with **more dynamic objects**, and analyze the module’s concrete gains on moving regions?\n2. Can you report **memory-induced improvements** across **additional benchmarks** (eg. pose esitimation)?\n3. Can you **evaluate memory on longer sequences** to demonstrate practicality and its effect on drift/relocalization beyond VGGT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k2E3t6gCV4", "forum": "9agaxh8ClV", "replyto": "9agaxh8ClV", "signatures": ["ICLR.cc/2026/Conference/Submission15107/Reviewer_RgAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15107/Reviewer_RgAW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909890628, "cdate": 1761909890628, "tmdate": 1762925429868, "mdate": 1762925429868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CogniMap3D, a biologically inspired framework for the problem of understanding and reconstructing dynamic 3D scenes from monocular videos. The core contribution is the design of an integrated pipeline, including a multi-stage motion cue separation module, a cognitive mapping system, and a camera trajectory optimization strategy. The cognitive system is capable of creating, storing, recalling, and updating persistent \"memories\" of static scenes and is designed to mimic the spatial cognitive ability of humans to revisit familiar environments. Experimental results on several standard datasets such as Sintel, KITTI, TUM-dynamics, and 7-Scenes show that the framework achieves competitive performance on multiple tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematically introducing the concept of \"cognitive memory\" into dynamic scene reconstruction is a novel and visionary attempt, which directly addresses the key challenge of transitioning from processing isolated video clips to achieving long-term, persistent environmental perception.\n2. The paper is clearly articulated, effectively conveying its complex system architecture and core ideas through high-quality illustrations, enabling readers to clearly understand its workflow and contributions."}, "weaknesses": {"value": "1. Although 14.32 FPS is reported in Table 1, this speed is quite amazing considering the complexity of the whole system (integrating VGGT, RAFT, DINOv2, PointNet++, SAM2, etc.). It is recommended that the authors more clearly state which modules are covered by this FPS test.\n2. The multi-stage, cascaded architecture of the framework raises a key concern that small biases in upstream modules may be amplified later, suggesting that the authors briefly discuss the robustness of the system to initial estimation errors.\n3. The paper claims that its key advantage is to handle long-term sequences and scene revisits, but the relevant quantitative evaluation is only carried out on small-scale relocation datasets such as 7-Scenes. This framework lacks validation in larger scale and more challenging long-term real environments.\n4. Ablation of memory systems has not been adequately studied, and it would be more compelling to provide a more nuanced analysis of what contributions 2D visual features and 3D geometric features make in scene recall and validation."}, "questions": {"value": "1. With regard to the reported 14.32 FPS, the authors should clarify exactly what modules are included in the metric and clarify whether it covers the complete, computationally intensive back-end flow, from memory retrieval and geometry validation to factor graph optimization.\n2. The authors need to supplement the discussion on the robustness of the cascaded framework under initial estimation errors. If the system design includes a specific mechanism to mitigate this error propagation, the principle needs to be explicitly stated.\n3. The authors should conduct experiments to quantify the specific contribution of the 3D geometry validation link. For example, how does the system's performance change after using only 2D visual features for scene recall and relocation (i.e., removing 3D validation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mZq3I74Znb", "forum": "9agaxh8ClV", "replyto": "9agaxh8ClV", "signatures": ["ICLR.cc/2026/Conference/Submission15107/Reviewer_rprw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15107/Reviewer_rprw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980070220, "cdate": 1761980070220, "tmdate": 1762925429078, "mdate": 1762925429078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CogniMap3D, a cognitively inspired framework for long-term 3D scene understanding from monocular videos. The method emulates human spatial cognition by maintaining a persistent memory of static environments while filtering dynamic elements and refining camera poses through recalled geometric anchors. It integrates multi-stage motion cues for dynamic object suppression, a dual 2D–3D embedding memory for scene recall and update, and a factor-graph–based trajectory optimization constrained by static landmarks. Experiments on diverse datasets, including Sintel, KITTI, and 7-Scenes, demonstrate that CogniMap3D achieves state-of-the-art or comparable performance in depth estimation, camera pose reconstruction, and 3D mapping, offering a robust and efficient solution for continual scene understanding under dynamic conditions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Conceptual originality. The paper introduces a cognitively inspired formulation of 3D scene understanding that explicitly models long-term memory, recall, and update—bridging human cognitive mapping theories with modern video foundation models. This conceptual framing is both novel and timely for the community’s growing interest in continual, memory-based perception.\n2. Technical coherence. The pipeline is well structured: multi-stage motion cues, a dual-modality memory, and factor-graph optimization are tightly integrated, yielding a clear causal chain from dynamic-object filtering to stable pose refinement. Each component is motivated by a specific limitation in existing VFMs and validated through comprehensive experiments.\n3. Empirical thoroughness. Experiments span multiple datasets and tasks (depth, pose, reconstruction) with detailed ablations demonstrating the contribution of each module. The method achieves state-of-the-art or comparable performance while maintaining efficiency, highlighting both the practicality and scalability of the proposed framework."}, "weaknesses": {"value": "While the paper is conceptually strong and empirically well-supported, several technical limitations remain that constrain its general applicability and robustness.\n\n1. The proposed multi-stage motion cue framework heavily relies on the accuracy of the underlying Video Foundation Model (VFM), particularly the depth and pose priors obtained from VGGT. Since these priors are directly used to compute geometric residuals and dynamic masks, any failure of the VFM in low-texture or high-illumination-variance regions could propagate errors through the entire pipeline, suggesting limited robustness to imperfect geometric initialization.\n2. The memory recall mechanism lacks explicit safeguards against false-positive retrievals. Although the paper mentions geometric verification via ICP, it does not clarify how mismatched recalls are detected or handled. In the absence of clear rejection or fallback strategies, incorrect memory associations could introduce erroneous landmarks into the factor graph optimization, potentially leading to catastrophic trajectory drift.\n3. The experimental validation focuses mainly on small- to medium-scale or short video sequences, such as 7-Scenes and Sintel, which do not fully reflect the challenges of large-scale, long-term, and highly dynamic environments. The paper’s claim of “long-term cognitive mapping” would be more convincing if evaluated under extended temporal settings or realistic outdoor sequences exhibiting significant environmental changes."}, "questions": {"value": "CogniMap3D shares conceptual similarities with prior methods such as Neural Map, iMAP, and NICE-SLAM, all emphasizing scene memory and revisitation-based localization. While its “cognitively inspired” design is reasonable, the paper does not clearly explain how the proposed dual-modality memory differs from these works in map representation, retrieval, or update strategy. A clearer articulation of these distinctions would better highlight the paper’s unique contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IRHSratTiy", "forum": "9agaxh8ClV", "replyto": "9agaxh8ClV", "signatures": ["ICLR.cc/2026/Conference/Submission15107/Reviewer_GYbG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15107/Reviewer_GYbG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993551363, "cdate": 1761993551363, "tmdate": 1762925428586, "mdate": 1762925428586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}