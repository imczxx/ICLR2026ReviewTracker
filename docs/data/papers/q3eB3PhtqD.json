{"id": "q3eB3PhtqD", "number": 13473, "cdate": 1758218335445, "mdate": 1759897435025, "content": {"title": "Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding", "abstract": "Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams.\nTo address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning.\nWe use Chimera to measure the presence of three types of shortcuts in visual question answering: \n(1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns;\n(2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and\n(3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly.\nThese findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.", "tldr": "", "keywords": ["vision-language model", "diagram understanding", "multimodal reasoning", "visual question answering", "shortcut learning", "knowledge grounding", "benchmark dataset", "multimodal evaluation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f92e46808334228be1f33c8c27812a84e924f889.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Chimera, a large-scale benchmark for evaluating visual-language models (VLMs) on diagram comprehension. The authors argue that current benchmarks overestimate model understanding because they fail to detect shortcut behaviors.  \nCHIMERA consists of 7,500 Wikipedia diagrams annotated with semantic triples and four levels of comprehension questions (entity recognition, relation understanding, knowledge grounding, and visual reasoning). Using this benchmark, the authors evaluate 15 open-weights VLMs across 7 model families, analyzing three shortcut types: visual-memorization, knowledge-recall, and Clever-Hans.  \nTheir findings suggest that Clever-Hans shortcuts significantly influence model performance, while visual-memorization and knowledge-recall shortcuts have smaller to moderate effects."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important gap in multimodal reasoning evaluation, distinguishing true comprehension from shortcut exploitation.\n\n- Proposes a structured framework for diagram understanding grounded in semiotic theory.\n    \n- Builds a large-scale dataset (Chimera) with hierarchical question design, enhancing diagnostic evaluation.\n    \n- Evaluates a broad range of open-weights VLMs, offering valuable comparative insights.\n    \n- The identification of three shortcut types provides a clear conceptual taxonomy."}, "weaknesses": {"value": "- **Human evaluation and visual dependency**: The authors conduct a human evaluation on 300 diagrams to assess visual dependency, QA correctness, and triple completeness. However, they do not report inter-annotator agreement, and standard deviations across categories suggest significant variability. This raises concerns about the reliability of the visual-dependency measure, which is critical for assessing the Clever-Hans shortcut.\n    \n- **Ambiguity in fine-tuning**: The paper does not specify whether the 15 evaluated models were fine-tuned on CHIMERA. Fine-tuning could strongly influence performance and shortcut measurement, while zero-shot evaluation might reveal different behaviors.\n    \n- **Statistical rigor in shortcut measurement**: The visual-memorization shortcut is claimed based on a small mean difference (~2%) between original diagrams and visualized triples. No standard deviations or significance tests are reported. Given that human evaluation found triples to be only 74–86% fully sufficient, this small difference may reflect dataset quality rather than model memorization.\n    \n- **Task difficulty not human-validated**: Knowledge-recall shortcuts are inferred from ER (Entity Recognition) vs other tasks, assuming ER questions should be easier to respond for a VLM. However, no human study confirms relative difficulty. Automatically generated questions may introduce artifacts, so the 5% difference may not reliably indicate shortcut behavior. This can be assessed with a human evaluation which answer a sample of questions. If human performance doesn't align with VLM results on ER vs other tasks, it could reveal the influence of the knowledge-recall shortcut.\n    \n- **Clever-Hans overstatement**: Clever-Hans shortcuts are measured only on ER questions. Larger models show minimal differences when diagrams are removed, suggesting that the observed effect is primarily driven by smaller models. Therefore, the claim that all VLMs suffer from Clever-Hans may overgeneralize.\n    \n- **Unusual paper structure**: The paper merges introduction, related work, dataset description, and results into a single section, which makes it harder to clearly follow the flow of motivation, prior work, and methodology."}, "questions": {"value": "**Questions for the Authors**\n\n1. Were any of the evaluated models fine-tuned on CHIMERA? If yes, please describe the training procedure and specify whether comparisons include both fine-tuned and zero-shot results.\n2. What was the inter-annotator agreement for the 300 manually evaluated diagrams?\n\n**Actionable Feedback**\n\n1. Conduct a human study to verify that ER questions are indeed easier than the other three tasks. This would strengthen claims about knowledge-recall shortcuts.\n2. Provide standard deviations or confidence intervals to assess reliability. Consider analyzing fully sufficient vs partially insufficient triples separately.\n3. Discuss potential biases introduced by automatically generated questions (Gemini) and how they may influence shortcut detection.\n4. Consider reorganizing the manuscript to separate introduction, related work, dataset description, and results for better readability and clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LPjBJZSV0s", "forum": "q3eB3PhtqD", "replyto": "q3eB3PhtqD", "signatures": ["ICLR.cc/2026/Conference/Submission13473/Reviewer_JpPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13473/Reviewer_JpPG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662723526, "cdate": 1761662723526, "tmdate": 1762924091677, "mdate": 1762924091677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CHIMERA is a visual benchmark suite designed to evaluate vision-language models (VLMs) on diagram comprehension. It contains 6,000 training and 1,500 test diagrams. The authors identify three shortcut behaviors commonly exhibited by VLMs: (1) visual information memorization, (2) knowledge-recall shortcut, and (3) Clever-Hans shortcut. The latter two primarily arise from the models’ reliance on linguistic cues rather than genuine visual understanding. To address this, each CHIMERA entry integrates three modalities: visual, semantic, and textual. This enables a comprehensive assessment of model behavior. The benchmark defines four task levels: entity recognition, relation understanding, knowledge grounding, and visual reasoning. The authors evaluate across 15 VLMs revealing that much of their performance can be attributed to language bias rather than true multimodal reasoning. In particular, the Clever-Hans shortcut experiment exposes cases where models achieve high accuracy even when the visual input is omitted entirely."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe authors identify three prominent shortcut behaviors and base their analysis on these.\n\n•\tThe findings from the Clever-Hans shortcut experiment are interesting, demonstrating that models do not utilize information from images for visual questions.\n\n•\tThe construction of the test suite is well explained: the authors describe starting with Wiki Web2M and ultimately filtering 7,500 instances for CHIMERA through a semi-autonomous process.\n\n•\tThe paper is well-structured, clearly written, and includes appropriate figures and tables."}, "weaknesses": {"value": "•\tMost models achieve high scores (above 80%) across the majority of tasks. This raises the question of whether the primary purpose of the benchmark is simply to show that models rely more heavily on textual modality. Since the questions themselves do not appear to pose substantial challenges, the benchmark’s diagnostic value seems limited.\n\n•\tThe authors claim that it is surprising that models perform better on the visual modality than on the semantic modality (lines 334-338). However, given that most training data heavily represent visual and textual modalities, with comparatively limited exposure to semantic diagrams, such results are expected rather than surprising.\n\n•\tLLaMA3.2 and Gemini were used to construct the dataset, while the evaluation includes models from the LLaMA3.2 and Gemma3 families (also developed by Google). This overlap introduces a potential source of bias.\n\n•\tThe authors show in Figure 4 and Table 2 that there is a 6–8% gap between the textual modality and the visual or semantic modalities, concluding that models perform best on text (line 333). However, I believe this conclusion is influenced by a few weaker models. After recalculating Table 2 using the data from Table 5 in the Supplementary, excluding LLaMA3.2-11B, LLaVA1.6-7B, LLaVA1.6-13B, and BLIP3-4B, the observed gap becomes much smaller, suggesting that the overall trend may not be as pronounced as reported.\n\nThe recalculated average scores are: Visual - 90.7, Semantic - 89.3, and Linguistic - 93.3. As observed, the gaps become much less prominent, under 3% between the Visual and Textual modalities. This raises the question of whether the authors’ reported findings are broadly generalizable or primarily driven by a few underperforming models."}, "questions": {"value": "•\tThe bottom part of Figure 3 requires more clarity. Also at first glance, the Human Evaluation component is not clear.\n\n•\tIn Table 1, Annotator C’s assessments show high percentages for visual dependency – partially dependent and triple completeness – marginally insufficient. What is the reason for this? The difference with other annotators is not minor.\n\n•\tThe authors present model-wise performance in Table 5 of the Supplementary. I recommend that they include a concise visual summary of this information, or at least aggregate the results by model family within the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z3BItQWg7v", "forum": "q3eB3PhtqD", "replyto": "q3eB3PhtqD", "signatures": ["ICLR.cc/2026/Conference/Submission13473/Reviewer_RvGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13473/Reviewer_RvGE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873564080, "cdate": 1761873564080, "tmdate": 1762924091357, "mdate": 1762924091357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CHIMERA, a benchmark for diagnosing shortcut learning in VLMs on diagram understanding tasks. The dataset provides 7,500 Wikipedia-sourced diagrams, automatically annotated with semantic triples and four levels of multiple-choice questions. The authors evaluate 15 open-source VLMs and claim their performance is largely driven by three types of shortcuts: visual memorization, knowledge recall, and Clever-Hans. While the work addresses a relevant problem, significant methodological and conceptual limitations undermine its contributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper states a good question of investigating why VLMs succeed, focusing on shortcut behaviors rather than just performance metrics.\n\n- The four-level task hierarchy (ER → RU → KG → VR) provides a good framework for analyzing different aspects of diagram comprehension.\n\n- Evaluating across three modalities (visual diagrams, semantic graphs, and textual descriptions) is a well-structured approach for isolating modality-specific biases.\n\n- The dataset includes 7,500 diagrams with 20% human validation, demonstrating reasonable quality control efforts."}, "weaknesses": {"value": "- The finding that VLMs rely on language priors and exhibit superficial pattern matching is well-established in many works, such as VQA, IconQA, VLMs are biased, etc. The paper does not sufficiently differentiate its contributions from these existing works. \n\n- The near 2% gap between visual and semantic modalities is within noise margins and insufficient to claim memorization effects, and it is not a powerful claim. It can be changed in another experiment.\n\n- ER performance itself consists of two steps: OCR text extraction and visual element extraction. It should be discussed and investigated which one affects the low performance. Moreover, successful reasoning fundamentally depends on accurate entity recognition. If ER fails in one of those tasks, then subsequent reasoning operates on corrupted inputs, making high performance on KG/VR tasks despite poor ER performance inherently suspicious and indicative of shortcut behavior.\n\n- The superior performance of the models in Wikipedia data can be caused by the models' training on the same data and memorizing it.\n\n- In Clever-Hans Shortcut, while the blank-image experiment is interesting, the analysis is incomplete. There is a need for an analysis of question-answer correlation biases; there is also a need for a comparison with random baseline or majority class baseline beyond the 25% chance level; and lastly, the evaluation also misses an analysis of whether performance correlates with question linguistic features.\n\n- The paper is purely diagnostic with no proposed methods to mitigate identified shortcuts. As an example, a work published in ICLR 2025, \"Chain-of-Region,\" used the OpenCV library to extract the visual data and feed it as text to the models."}, "questions": {"value": "- How does CHIMERA differ from IconQA, ChartQA, and ERBench? These benchmarks also decompose visual reasoning into levels and test chart/diagram understanding. What unique contribution  of CHIMERA compared to those other works?\n\n- If entity recognition provided accurate diagram content to the model, wouldn't reasoning tasks become trivial? An ablation study can be done in this matter by showing that fixing ER errors would improve downstream task performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TvvbHiJRdF", "forum": "q3eB3PhtqD", "replyto": "q3eB3PhtqD", "signatures": ["ICLR.cc/2026/Conference/Submission13473/Reviewer_HN5s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13473/Reviewer_HN5s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882302277, "cdate": 1761882302277, "tmdate": 1762924090950, "mdate": 1762924090950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CHIMERA, a benchmark designed to test whether vision-language models actually understand diagrams or just rely on shortcuts. It includes thousands of Wikipedia diagrams annotated with questions that probe different levels of understanding, using Gemini model. The authors use the same model to filter out samples that are not accurate and visually grounded. By evaluating 15 major models, the authors conclude that strong performance often comes from exploiting memorized visuals, stored knowledge, or linguistic patterns rather than true comprehension."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This paper tries to present a benchmark that has an important goal: Diagnosing whether models perform true visual understanding and reasoning, or they use other cues, called shortcuts by the authors. This is a benchmark that is relevant and needed for performance evaluation, however, the process of building the dataset has fundamental flaws that undermines the value of it. I will elaborate more in weaknesses.\n- The presentation of the paper and the results is good and clear.\n- The authors observation of decreased performance on more complex tasks (Lines 370-377) is intriguing and supported by previous observations [1] where the same observation is called \"easier-worse anomaly\".\n[1] Visual Graph Arena, ICML 2025"}, "weaknesses": {"value": "1) The most important weakness of the current work, is the use of LLMs for generating the annotations and answers. Although automation is used in literature for benchmark curation, the current work's goals specifically requires use of no model in generating the ground truth responses. One major flaw is when the authors use the Gemini model for \"discarding the examples if questions can be answered without the image\" (Line 240). This process is removing the very biases that the models may have because of being exposed to the data in their training, which is the goal of paper to assess. \n\nWhile time and resource consuming, the benchmark for this specific goal must be carefully created by human annotators. As a sidenote, one does not need to have 7500 samples for this benchmark.1000 samples, but high quality, would suffice for the goal.\n\n2) The hypothesis in Line 324 is wrong, and hence the experiment based on that to evaluate visual memorisation is so. Why do the authors expect models perform better on the semantic modality, which is actually generated by another model using the original image! If anything, the image must contain more information helping the models to answer the questions. \n\n3) Another issue is the use of wikipedia diagrams for the benchmark. Although the authors try to justify this (Line 88), it is notable that the 3rd shortcut presented (Clever-Hans shortcut) which is concluded to be the most prominent one, actually requires a benchmark of out of distribution diagrams to be fairly evaluated. Moreover, if a benchmark is generated to be out of distribution by design (for instance, by newly generated plots and diagrams from textual/tabular data), it clearly will be a sound benchmark for evaluating memorisation. (Newly generated diagrms -> Not exposed to during training -> suitable to test memorisation shortcuts)"}, "questions": {"value": "1) Since the dataset is not provided, I ask the authors to add a few more examples of their samples to the appendix (Like Figure 1).\n2) Did you see the other anomaly observed by [1], the middle score anomaly, in any of the tasks? For instance, the tasks involving detecting the colors (like the example in Figure 8)\n3) Since the benchmark contains a train and test split, have you trained (fine-tuned) any models on the training set? If no, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K9JPOVp8jz", "forum": "q3eB3PhtqD", "replyto": "q3eB3PhtqD", "signatures": ["ICLR.cc/2026/Conference/Submission13473/Reviewer_bqMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13473/Reviewer_bqMB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177085658, "cdate": 1762177085658, "tmdate": 1762924090015, "mdate": 1762924090015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}