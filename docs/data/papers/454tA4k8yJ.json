{"id": "454tA4k8yJ", "number": 21286, "cdate": 1758315915709, "mdate": 1759896930735, "content": {"title": "FIRE-Bench: Evaluating Research Agents on the Rediscovery of Scientific Insights", "abstract": "Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery, but rigorously evaluating their capacity for genuine discovery remains a critical challenge. Current evaluation benchmarks face a dilemma: they either rely on LLM-as-judge evaluations of auto-generated papers, which raise concerns about validity and circularity, or focus on optimizing single performance metrics that serve as a coarse proxy for genuine discovery. To address this, we introduce FIRE-Bench (\\textbf{F}ull-cycle \\textbf{I}nsight \\textbf{R}ediscovery \\textbf{E}valuation). Our benchmark reframes evaluation by tasking agents with the verifiable rediscovery of established scientific findings from recent, high-impact ML research. We provide agents only with the high-level research question from a published study, requiring them to autonomously design experiments, implement code, execute their plan, and derive a conclusion from the evidence. We evaluate a suite of state-of-the-art agents with frontier model backbones (e.g., GPT-5) on FIRE-Bench. Our findings paint a sobering picture of current capabilities: even the most advanced agents struggle profoundly, exhibiting low success rates, high variance, and a spectrum of recurring failure modes ranging from flawed experimental design to ungrounded conclusions. FIRE-Bench provides a rigorous, diagnostic framework for measuring and driving progress towards AI agents capable of genuine scientific discovery.", "tldr": "", "keywords": ["Research Agents", "Autonomous AI Agents", "Research Automation", "Benchmarking", "AI for Science", "Scientific Discovery"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eadf94f2d30282b883c2bd8708a56a68327b7bdb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces URSA (Universal Research and Scientific Agent), a modular agent-based research ecosystem that leverages large language models’ capacities for reasoning, planning, and tool invocation alongside high-fidelity physical simulators (e.g., Helios) to enable autonomous research workflows and automated design optimization; URSA demonstrates superior performance to Bayesian optimization on inertial confinement fusion (ICF) design tasks. The study also presents FIRE-Bench, a verifiable, fine-grained evaluation benchmark that requires agents to reproduce published findings, employs claim-level F1 scoring, and diagnoses failures across four stages—Planning, Implementation, Execution, and Analysis. Experimental results reveal persistent limitations of current agents, including hallucinations, insufficient environment isolation, limited task generalizability, and dependence on specific LLMs, indicating the need for human verification and more rigorous environment management."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes the FIRE-Bench benchmark, which evaluates agents by reproducing published scientific findings rather than generating novel conclusions, thereby avoiding circular-reasoning issues associated with LLM-as-judge evaluations (e.g., subjectivity and reliability concerns). Uses claim-level F1 scoring to quantify the agreement between agent outputs and ground-truth findings, enabling fine-grained analysis.\n\n2. Decomposes failures into four stages—Planning, Implementation, Execution, and Analysis—revealing weak points in experimental design and code implementation (e.g., 60.3% of errors originate in the planning stage). Combines human–LLM hybrid analysis to ensure accurate error attribution.\n\n3. Applies strict paper selection criteria: public data, lightweight computation, and verifiable conclusions (e.g., 15 papers from ICLR/ICML 2024–2025). Benchmarks multiple state-of-the-art agents (e.g., OpenHands, Claude Code), covering both open-source and commercial models, yielding representative results."}, "weaknesses": {"value": "1. Covers only lightweight LLM behavioral analyses (e.g., bias detection, reasoning) and does not address domains requiring long-running experiments (e.g., computational biology) or resource-intensive tasks. Evaluates only closed tasks of reproducing known conclusions and does not test agents’ open-ended scientific discovery capabilities (e.g., proposing new questions or serendipitous findings).\n\n2. Only 15 papers from ICLR/ICML 2024–2025 were chosen, which may not cover diverse research paradigms.\n\n3. Most tasks show $F_1$ standard deviation > ±20 (e.g., Claude Code reaches ±57.7 on the Awareness Detection task), indicating unstable agent performance (Table 1). Agents perform well on linear tasks (e.g., Lost in the Middle) but fail on tasks requiring causal experimental design (e.g., LLM Racial Bias) (§5.1)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gkj3Utt9nY", "forum": "454tA4k8yJ", "replyto": "454tA4k8yJ", "signatures": ["ICLR.cc/2026/Conference/Submission21286/Reviewer_ZqJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21286/Reviewer_ZqJM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533099995, "cdate": 1760533099995, "tmdate": 1762941672451, "mdate": 1762941672451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **FIRE-Bench**, a benchmark designed to evaluate research agents by testing their ability to rediscover verifiable scientific findings from recent, high-impact machine learning papers. Agents are provided only with high-level research questions and are required to autonomously design, implement, and execute experiments to derive conclusions. Each original paper is represented as a hierarchical research-problem tree consisting of root, intermediate, and leaf nodes, corresponding respectively to overarching questions, subproblems, and empirical tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe benchmark tried to design a benchmark without the LLM evaluator and single performance metrics. \n2.\tThe paper is clearly written, and the figures effectively illustrate the benchmark design and agent evaluation results."}, "weaknesses": {"value": "1.\tAlthough the authors claim to avoid LLM-based evaluation, the benchmark still uses RAGChecker, which depends on LLMs (GPT-4o) for claim extraction and verification. This still inherits potential issues of bias, circularity, and reliability that motivated their critique of LLM-as-judge methods.\n2.\tPotential data leakage: the benchmark uses analysis papers from 2024–2025, yet some evaluated agents (e.g., GPT-5) were trained using data possibly overlapping with these sources. It remains unclear how the benchmark ensures that target findings were unseen during training.\n3.\tFrom my perspective, rediscovery can partially reflect a model’s capacity for genuine discovery; however, it is not sufficient on its own. The paper evaluates research agents using precision, recall, and F1 scores between the agent’s and the ground-truth conclusions, but this metric does not align well with the goal of scientific discovery. For example, a high F1 score merely indicates textual or semantic overlap rather than genuine reasoning ability or discovery quality. Conversely, low precision does not necessarily mean the agent is weak—it may arise because the model proposes novel conclusions not yet found in the literature. As a result, this evaluation design rewards conformity and penalizes creativity, measuring agreement rather than epistemic soundness or originality.\n4.\tSome grammar errors appear in the examples (e.g., “Do this LLM contain biases?”).\n5.\tLLM use and reproducibility claim are not included."}, "questions": {"value": "1.\tHow do the authors ensure RAGChecker’s LLM-based evaluation does not reintroduce the same reliability issues they aim to avoid?\n2.\tCan the authors clarify whether measures were taken to prevent data leakage between training data and benchmark source papers?\n3.\tCan the authors clarify the validity of the evaluation protocol?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y0TLryyXcb", "forum": "454tA4k8yJ", "replyto": "454tA4k8yJ", "signatures": ["ICLR.cc/2026/Conference/Submission21286/Reviewer_27ba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21286/Reviewer_27ba"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737917406, "cdate": 1761737917406, "tmdate": 1762941672005, "mdate": 1762941672005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that genuine scientific discovery remains a central challenge. Existing evaluation benchmarks often either rely on LLM-as-judge assessments of automatically generated papers or optimize single metrics that are merely coarse proxies for discovery. To address this, the authors introduce FIRE-Bench, a benchmark that evaluates research agents on their ability to rediscover established scientific insights. Empirical studies reveal high variance and recurring failure modes, ranging from flawed experimental design to ungrounded conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper’s focus on evaluating the capability for genuine scientific discovery has substantial practical value.\n- The benchmark follows a clear, well-reasoned process by curating high-impact ML research and constructing a research-problem tree.\n- The benchmark’s findings highlight the current limitations of research agents."}, "weaknesses": {"value": "- The benchmark’s metrics for assessing agents’ performance in genuine scientific discovery are not sufficiently comprehensive.\n- The benchmark lacks an evaluation of the quality of the problem-solving process itself, beyond merely checking success or failure.\n- The amount of data used in the paper to evaluate capabilities is insufficient.\n- The paper’s evaluation methodology is not sufficiently comprehensive; it is limited to only four methods."}, "questions": {"value": "- More specific metrics are needed to assess each agent’s process-level capabilities, for example, metrics that evaluate the quality of generated code.\n- Could the authors add additional papers of the same type for each task to expand the test set? This would improve the stability of the conclusions.\n- Could the evaluation include metrics beyond Accuracy and F1? When results are close, how will ties be adjudicated or the winner determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gJlXyxBawd", "forum": "454tA4k8yJ", "replyto": "454tA4k8yJ", "signatures": ["ICLR.cc/2026/Conference/Submission21286/Reviewer_poxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21286/Reviewer_poxW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810433116, "cdate": 1761810433116, "tmdate": 1762941671541, "mdate": 1762941671541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FIRE-Bench, a benchmark of 15 end-to-end LLM behavior research tasks to evaluate research agents’ capabilities in rediscovering published findings. Evaluation results of three agent scaffolds with different LLMs show that the rediscovery success rates have high variance, and the agents show different failure modes."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper adapts a clear problem formulation by breaking research papers down to research problem trees.\n2. The paper has a relatively clear presentation."}, "weaknesses": {"value": "1. One of the claimed major contributions of this paper is falsifiable. The authors claim to “obviate the need for unreliable LLM judges.” However, LLMs are implicitly used in two stages of the benchmark’s evaluation process: (1) LLMs are used to parse the source papers into problem trees, and (2) both claim extraction and verification steps in RAGChecker are implemented by LLMs. While the authors mentioned robustness of RQ tree parsing with LLMs, no further details and human evaluation results are presented to support the claim. Thus, this paper is not sound and subject to overclaiming issues.\n2. The proposed benchmark does not take potential data contamination or agent shortcut issues into serious consideration. Instead of simply attributing the high evaluation variance to LLMs, the authors should carefully analyze whether some of the exceptionally high scores are due to contamination or shortcuts, and implement corresponding safeguards if found any.\n3. The benchmark size and scope is narrow in comparison to existing literature. It specifically targets LLM behavior analysis tasks and only includes 15 of them, which may not be sufficient to meaningfully support the results and findings. As a potential consequence, the error analysis presented remains relatively superficial and lacks deeper insights, e.g., any more fine-grained error categorization and analysis for “misunderstood the core objective, failed to design necessary control conditions, or omitted critical steps entirely.”"}, "questions": {"value": "The paper incorrectly uses \\citet instead of \\citep for in-text citations at many places, e.g., introduction, which needs a round of proofreading and correction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOs19BarZp", "forum": "454tA4k8yJ", "replyto": "454tA4k8yJ", "signatures": ["ICLR.cc/2026/Conference/Submission21286/Reviewer_ehB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21286/Reviewer_ehB6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21286/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922908690, "cdate": 1761922908690, "tmdate": 1762941671035, "mdate": 1762941671035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}