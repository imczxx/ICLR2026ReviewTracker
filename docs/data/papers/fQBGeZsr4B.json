{"id": "fQBGeZsr4B", "number": 8634, "cdate": 1758093080118, "mdate": 1759897772564, "content": {"title": "An Efficient Structural Pruning for Spiking Neural Networks by Balancing Accuracy and Sparsification", "abstract": "The increasing scale of spiking neural networks (SNNs) poses significant challenges for deployment on resource-constrained neuromorphic hardware, necessitating lightweight and learnable structural solutions. Interestingly, biological neural systems employ an efficient organizational strategy—hierarchical structural reorganization around functional clusters, where new connections grow\northogonally to existing ones to expand representational capacity. Inspired by this mechanism, we propose a dynamic pruning and regrowth framework with channel-level orthogonality for SNNs (DPRC-SNNs) to enable scalable and efficient structural learning for SNNs. DPRC-SNNs introduce the spiking column subset selection mechanism for SNNs, which integrates channel-level pruning with orthogonality-driven regrowth, selectively restoring diverse and complementary channels to minimize information loss from aggressive pruning. Through iteratively pruning redundant channels and regrowing orthogonal ones, DPRC-SNNs preserve functional diversity while enhancing sparsity at the channel level. Extensive evaluations on CIFAR10, DVS-Gesture, and DVS-CIFAR10 demonstrate that DPRC-SNNs achieve high compression rates and computational efficiency without compromising accuracy, showing strong potential for neuromorphic deployment.", "tldr": "", "keywords": ["spiking neural networks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aef68cc8c35401f2e52b9caf5ed8ad58d874d329.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DPRC-SNNs, a Dynamic Pruning and Regrowth with Channel-level Orthogonality framework for spiking neural networks. Motivated by the hierarchical structural reorganization observed in biological neural circuits, the method enables adaptive structural learning by iteratively pruning redundant channels and regrowing new, orthogonal ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Structured and hardware-friendly sparsity"}, "weaknesses": {"value": "1. The overall pruning–regrowth paradigm has been explored in several works. The main innovation here—orthogonality-driven regrowth—while conceptually appealing, may require stronger empirical justification to establish substantial novelty.\n2. This paper does not specify how orthogonality between channels is measured or enforced.\n3. This paper should benchmark against modern event-based or structured sparsity approaches.\n4. Although the method is motivated by neuromorphic deployment, the results are limited to simulation-level metrics (accuracy, FLOPs). Energy measurements or mapping to actual chips (e.g., Loihi 2, Tianjic) would substantiate the hardware efficiency claim."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VOwgrRWves", "forum": "fQBGeZsr4B", "replyto": "fQBGeZsr4B", "signatures": ["ICLR.cc/2026/Conference/Submission8634/Reviewer_nFzZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8634/Reviewer_nFzZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791362525, "cdate": 1761791362525, "tmdate": 1762920461502, "mdate": 1762920461502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DPRC-SNNs, a novel structural learning framework for Spiking Neural Networks (SNNs) that unifies dynamic channel pruning and orthogonality-driven regrowth to balance accuracy and sparsification. Unlike prior weight-level or static pruning strategies, the proposed spiking column subset selection mechanism enables channel-level optimization guided by temporal spiking dynamics, while the orthogonality-based regrowth restores diverse and complementary channels to preserve functional representation. The framework dynamically reorganizes network topology during training, achieving compact yet expressive SNN architectures. Extensive experiments on three benchmark datasets consistently demonstrate that DPRC-SNNs achieve substantial reductions in both parameters and computational cost while maintaining competitive accuracy. This highlights the model's strong potential for efficient neuromorphic deployment and scalable event-driven learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes DPRC-SNNs, a novel structural learning framework that integrates channel-level pruning with orthogonality-driven regrowth, inspired by biological neural reorganization. By introducing the spiking column subset selection (SCSS) mechanism, it effectively captures temporal dependencies in SNN pruning, bridging the gap between fine-grained sparse optimization and hardware-efficient structured pruning.\n2. The method is technically sound, with well-founded formulations for channel importance and orthogonality-based regrowth. Its dynamic pruning–regrowth strategy ensures stable optimization, and extensive experiments on both static and neuromorphic datasets validate its robustness and effectiveness.\n3. The paper presents a coherent progression from biological motivation to algorithmic design and validation. The neuroscience-computation link is well-supported, with figures effectively illustrating channel evolution and pruning. The polished \npresentation is accessible to both neuromorphic and machine learning audiences."}, "weaknesses": {"value": "1. The ablation study could more clearly isolate the effects of SCSS and orthogonality-based regrowth.\n2. The paper provides limited analysis of computational overhead.\n3. The experimental validation lacks support from larger datasets, which would strengthen the empirical evidence for the proposed approach.\n4. The article's layout exceeds the margins."}, "questions": {"value": "1. Can this pruning approach be extended to other mainstream neural network frameworks? If not, what do you consider to be the primary limitations or bottlenecks that may hinder its generalization?\n2. The visualizations of pruning and regrowth dynamics are insightful but somewhat difficult to interpret. Would the authors consider improving the figure annotations or providing simplified visual aids to enhance clarity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lZiSPoEv89", "forum": "fQBGeZsr4B", "replyto": "fQBGeZsr4B", "signatures": ["ICLR.cc/2026/Conference/Submission8634/Reviewer_KAkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8634/Reviewer_KAkT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810224176, "cdate": 1761810224176, "tmdate": 1762920460896, "mdate": 1762920460896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by the brain's ability to reorganize around functional clusters, this paper proposes a Dynamic Pruning and Regrowth framework with Channel-level orthogonality for SNNs (DPRC-SNNs). The goal is to enable scalable and efficient structural learning. The core innovation is a \"spiking column subset selection mechanism\" that iteratively performs pruning and regrowth."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "​​The inspiration from hierarchical structural reorganization in the brain is a compelling and well-articulated foundation for the work.\n\nThe integration of dynamic pruning with an orthogonality-driven regrowth mechanism is a novel and interesting approach to structural learning in SNNs.\n\nThe paper is generally well-written and easy to follow."}, "weaknesses": {"value": "The discussion on structural/unstructured learning in SNNs is lacking. A clear comparison with existing unstructured sparsity methods for SNNs is needed to position this work's unique contribution (structural/channel-level pruning) and its advantages/disadvantages.\n\nThe absence of results on a large-scale dataset like ImageNet is a significant gap. It is difficult to assess the scalability and true effectiveness of the method without this standard benchmark.\n\nThe purpose of comparing against TET (a method focused on improving accuracy, not sparsity) is confusing. Comparisons should primarily be against other state-of-the-art sparsity-inducing methods.\n\nIt is unclear if the proposed dynamic pruning/regrowth framework is specific to SNNs or a general technique. Showing its performance on standard CNNs would help demonstrate the generality and robustness of the concept.\n\nThe paragraph preceding the list of contributions largely repeats the items that follow. This section should be shortened and made more concise."}, "questions": {"value": "How does DPRC-SNNs compare quantitatively against state-of-the-art methods that reduce unstructured sparsity in SNNs, particularly in terms of the trade-off between accuracy, sparsity level, and training/inference cost?\n\nCan the DPRC framework be directly applied to standard CNNs (Artificial Neural Networks not SNNs)? If so, what are the results on a benchmark like ImageNet? If not, what aspects are specific to the dynamics of spiking neurons?\n\nWhat is the specific rationale for including TET, a non-sparsity method, in the comparisons? Is the goal to show that DPRC-SNNs can alsoachieve high accuracy? This should be clarified in the text.\n\nHave you conducted any experiments on ImageNet-scale datasets? If not, do you anticipate any challenges in scaling the dynamic pruning and regrowth process to such large networks?\n\nWhat is the computational overhead of the iterative pruning-and-regrowth process during training compared to a standard one-shot pruning pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q418HpvJ0p", "forum": "fQBGeZsr4B", "replyto": "fQBGeZsr4B", "signatures": ["ICLR.cc/2026/Conference/Submission8634/Reviewer_kXKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8634/Reviewer_kXKa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833952092, "cdate": 1761833952092, "tmdate": 1762920460441, "mdate": 1762920460441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DPRC-SNNs, a dynamic channel-level pruning and regrowth framework for Spiking Neural Networks inspired by biological neural reorganization mechanisms. The method introduces a spiking column subset selection mechanism that integrates channel-level pruning with orthogonality-driven regrowth to preserve functional diversity while enhancing sparsity. The experimental results on CIFAR and DVS datasets demonstrate competitive performance with significant parameter reduction."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper shows strong innovation in bridging biological principles with efficient SNN compression, and the orthogonality-based regrowth strategy represents a novel contribution to hardware-friendly neural network optimization."}, "weaknesses": {"value": "1.The SCSS formulation would benefit from a clearer comparison to related methods in matrix approximation or subspace selection.\n\n2.The evaluation scope is narrow; testing on more architectures or datasets could better establish generality."}, "questions": {"value": "question 1: How does the adaptive sparsity mechanism, driven by batch normalization and spike activity, influence training stability and convergence?\n\nquestion 2: The proposed method is only evaluated on convolutional architectures (ResNet, VGG). Can the proposed framework scale effectively to larger or more complex datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4EUV6DtZ7E", "forum": "fQBGeZsr4B", "replyto": "fQBGeZsr4B", "signatures": ["ICLR.cc/2026/Conference/Submission8634/Reviewer_QNRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8634/Reviewer_QNRN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881451675, "cdate": 1761881451675, "tmdate": 1762920459654, "mdate": 1762920459654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}