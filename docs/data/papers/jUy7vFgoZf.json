{"id": "jUy7vFgoZf", "number": 17665, "cdate": 1758278975163, "mdate": 1759897161940, "content": {"title": "How Neural is a Neural Foundation Model?", "abstract": "Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for understanding brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as physiologist might, characterizing each ‘neuron’ based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding manifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by “pushing apart” the representations of different temporal stimulus patterns. Our novel metric of “tubularity” quantifies this stimulus-dependent development of neural activity as biologically plausible. The readout module achieves high fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, this study provides a window into the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons’ joint temporal response patterns. Our findings suggest design changes that could bring neural foundation models into closer alignment with biological systems: introducing recurrence in early encoder stages, and constraining features in the readout module.", "tldr": "This paper analyzes the internal representations of a neural foundation model using manifolds and a new “tubularity” metric showing divergence from biological representations.", "keywords": ["Foundation Models", "Neuroscience", "Interpretability", "Mouse Visual System", "Manifolds"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d06ea53ddbd065852a6de4b514b1c98dc2b5a2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper provides a novel technique to analyze representations from the intermediate layers of the neuronal foundational model from Wand et al 2025, suggesting that recurrent layers play a core role in the topology of internal representations.\nThey construct a neural encoding and neural decoding manifolds and try to capture the relation between two manifolds by the temporal evolution of neuronal activity.\nThey also introduce a tubularity metric to quantify the relationship between artificial and biological neural response trajectories."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper raises a very important question of interpreting neural foundational models, especially for biological plausibility, and do it in a novel and original way.\n2. The paper uses state-of-the-art models and diagnostic tools, also using biologically interesting stimuli, etc\n3. Careful statistical analysis is highly appreciated (Bonferroni corrections, etc)"}, "weaknesses": {"value": "1. **The paper is not clearly written**. The paper is not written in a self-sufficient manner. To understand and critically evaluate the central claims and contributions of the paper it is essential to understand the architecture of the Wang et al 2025 foundational model encoder. However, neither the original paper no the current paper provide a diagram / schematics of the encoder - are there skip connections? how many 3D convolutional blocks are there? Are they full 3D convolutions or are they factorized? What is the maximum temporal horizon, which could be seen by the temporal convolutions? Same applies for the tools they use for the analysis, the introduction even in the appendix is not enough for critical evaluation.\n2.  **Lack of connection to the current interpretations of the data-driven networks**. The paper attempts to contribute towards interpretability of the neural networks, however, it seems like the authors have not exhaustively studies the current interpretations of the data-driven neural predictive networks. \nSpecifically, data-driven networks [1-3] with Gaussian readout [4] (Wang et al 2025 uses a variation of it) would interpret the encoder as  deconstructing visual stimuli into a set of leanrt basis functions and then the readout learns the position of per-neuron receptive field and a 1-dim vector. Under this interpretation, these 1-d vectors form the neural encoding manifold. This is not reflected anywhere in the text and I am not sure if the way the authors construct the neural encoding manifold is consistent with this perspective. This perspective and prior work this does not diminish the value of studying the intermediate representations in the encoder, but it might crucially change the interpretation of the results. \n3. **The paper lacks clear explanations of technical details, which is crucial for interpretability studies**. See questions below\n\nReferences:  \n[1] Klindt, David, et al. \"Neural system identification for large populations separating “what” and “where”.\" Advances in neural information processing systems 30 (2017).  \n[2] Turishcheva, Polina, et al. \"Reproducibility of predictive networks for mouse visual cortex.\" Advances in Neural Information Processing Systems 37 (2024): 7930-7956.   \n[3] Nellen, Nina S., et al. \"Learning to cluster neuronal function.\"Advances in Neural Information Processing Systems 37 (2025)  \n[4] Lurz, Konstantin-Klemens, et al. \"Generalization in data-driven models of primary visual cortex.\" ICLR 2021"}, "questions": {"value": "1. See questions in Weakness 1\n2.  Do I get it correctly that all of the analysis was done on a single session? (Appendix A2, line 835 *\"We used the FNN from session 8, scan 5.\"*)\n3. Wrt to Weakness 2 - In appendix A3, lines 851-852 say *\"50 neurons were sampled from each feature map\"* - how exactly do you do it? are things sampled independently from each feature map or is not then what exactly do you do and why? How exactly *\"the sampling probabilities of feature maps and neurons were set to be proportional to their activation strength\"* (lines 852-853)? \n4. How exactly have you adapted the stimuli to have 37 frames (lines 846-848)? Did you repeated some frames, put some preliminary buffer or just changed the shape?\n5. In lines 162-163 you say *\"We compare with biological decoding trajectories using the experimental data from Dyballa et al. (2024a).\"*. Have you also finetuned the network from Wang et al for the new neurons? if yes - how (e.g. which components have been frozen if any)?\n6. In which space (and dimensionality) tubularity is measured? time, amplitude of responses and repeats? Or is is the latent space after PCA transforms?\n7. What do you mean by *\"The encoding manifold for the readout layer is highly disconnected (Figure 4A), with each cluster corresponding almost exclusively to neurons sampled from a single feature map.\"* in lines 371-373? If this is about learning the receptive field  (*\"* $p^n \\in R^2$  *denote the spatial position (x, y)\"* see Wang et al 2025 Methods section, *\"Readout module\"* paragraph) or do you mean a single dimension along the  $w^n \\in R^C$  that *\"denotes the feature weights for that neuron\"*?\nIf first, than the clusters you report in *\"Figure 4A\"* under the papers from weakness 2 would be interpreted as grouping by receptive field rather than \"cell subtypes\".\n8. What are the limitations of tubularity metric that you have introduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hb7Ax3LkSs", "forum": "jUy7vFgoZf", "replyto": "jUy7vFgoZf", "signatures": ["ICLR.cc/2026/Conference/Submission17665/Reviewer_5PbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17665/Reviewer_5PbF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760544727392, "cdate": 1760544727392, "tmdate": 1762927518062, "mdate": 1762927518062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the biological plausibility of a foundation neural network (FNN) trained to predict mouse visual cortex activity. The authors analyze the model’s internal representations using three neuroscience-inspired approaches: decoding manifolds, encoding manifolds, and temporal trajectory analysis. They introduce a new geometric metric tubularity to quantify the organization of neural trajectories and compare artificial versus biological neural dynamics. Results show that the FNN’s recurrent module introduces biologically plausible temporal dynamics, while feedforward and readout modules deviate significantly from biological organization. The authors argue for architectural adjustments (e.g., early recurrence, fewer redundant readout maps) to improve biological alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of probing foundation models through a neuroscience lens is both original and timely, offering a valuable bridge between computational neuroscience and modern AI.\n\n2. The proposed tubularity metric is a creative and potentially useful approach for quantifying the geometric structure of neural trajectories."}, "weaknesses": {"value": "1. The experimental analysis lacks ablation or sensitivity studies. It remains unclear how stable the conclusions are with respect to hyperparameter settings or architectural choices.\n\n2. The discussion of model mechanisms is descriptive rather than mechanistic. Related work such as “What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models” offers a more fundamental exploration of inductive biases.\n\n3. The validation of the “tubularity” metric is limited. There is little theoretical grounding, and no comparison with alternative geometric or dynamical measures.\n\n4. Figure 1 could be more informative and visually integrative. A clearer schematic that unifies the core workflow (encoding/decoding manifolds, tubularity computation, and main findings) would greatly improve readability and help the reader grasp the conceptual flow at a glance."}, "questions": {"value": "1: On the core premise:\n\na. The paper equates tubularity with biological plausibility, but the argument relies almost entirely on the geometry of encoding/decoding manifolds and trajectories. Could these geometric measures alone truly capture brain-like computation?\n \n2: On the definition and robustness of “tubularity”:\na. How is tubularity theoretically grounded? Could concepts from topology or knot theory (e.g., linking number, curvature-based tightness) provide a more rigorous mathematical basis?\n \nb. Can properties or invariants be derived analytically from the current metrics to demonstrate robustness or internal consistency?\n \nc. How sensitive are the proposed metrics to hyperparameter settings—for example, clustering thresholds, quantile, or binning strategy?\n \nd. Would a bin-wise normalization approach yield a more stable, scale-free comparison across models or datasets?\n \ne. Have you considered alternative distance measures, such as Dynamic Time Warping (DTW), for comparing trajectory similarity? What advantages or disadvantages does your approach have?\n\nf. Conceptually, how do we know that higher tubularity truly correlates with better computational function?\n \n \n3: On model design and experimental interpretation:\n\na.  The encoder’s inability to separate stimuli seems expected, as early layers mainly act as low-level feature extractors. In biological vision, early retinal processing (e.g., from photoreceptors to bipolar cells) lacks strong recurrent connections; each stage performs relatively modular, feedforward computation. Given this biological organization, why does the paper suggest introducing recurrence into earlier encoder layers as a way to make the FNN more biologically plausible?\n \nb. The “intensity arm” is attributed to convolutional padding artifacts, but might this reflect a deeper architectural discrepancy between CNNs and biological vision—where visual space is continuous and boundary-free? Is “fixing padding” merely treating the symptom rather than the cause?\n \nc. The paper reuses parameters (e.g., tensor decomposition rank) from Dyballa et al. (2024a) without justification. Given the difference between sparse, discontinuous biological responses and continuous activations in FNNs, could this distort manifold topology?\n \nd. The neuron sampling strategy prioritizes highly active units (2000 neurons total), excluding low-response neurons that may play inhibitory or regulatory roles. Could this bias the manifold geometry toward excitatory-like representations?\n \ne. Given that the recurrent module is preceded by an attention layer, how can we disentangle their respective contributions? Without ablation or control experiments, can we confidently attribute the observed improvements solely to recurrence?\n \n4: Given the conceptual density of the paper, a comprehensive schematic figure that consolidates the key elements, such as model architecture, experimental pipeline (encoding/decoding manifolds, trajectory analysis), definition of tubularity, and the main conclusions, would greatly enhance accessibility. This could replace or extend the current Figure 1 to serve as a visual summary of the study’s logic and findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "31TobEvs3y", "forum": "jUy7vFgoZf", "replyto": "jUy7vFgoZf", "signatures": ["ICLR.cc/2026/Conference/Submission17665/Reviewer_h5wS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17665/Reviewer_h5wS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629568920, "cdate": 1761629568920, "tmdate": 1762927517648, "mdate": 1762927517648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper compares a recently published foundation model of neural responses (FNN) in mouse visual areas (Wang 2025) to data from mouse retina and V1 (Dyballa 2024) recorded while viewing different visual flows. They use decoding and encoding manifolds to visualize high-dimensional activations (possibly across time) and feature selectivity in both systems then evaluate the extent to which the FNN aligns with the biological visual system. To make these comparisons more concrete the authors introduce “tubularity” which roughly measures the tightness of different temporal trajectories in activation space. They conclude that the recurrent module of the FNN produces responses which best align with recorded neural data. Most significantly, this is the first layer where they found robust decodability of stimulus type. Decodability corresponded to tighter trajectories according to their tubularity measure, though tubularity in the FNN recurrent module was still “sub-biological.” By contrast the activity in the FNN readout module was a poor match for biological data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The strongest point of this paper is establishing that the very intuitive correspondence from feedforward encoder to retina and recurrent module to V1 doesn’t actually hold. It is well written and the subject matter is timely given the proliferation of foundation models across the field."}, "weaknesses": {"value": "I think after reading I now know where in the FNN representations qualitatively resemble the mouse visual system - the recurrent module and not encoder stage - but I’m not sure why this is the case, how encoder computations support this similarity in the recurrent module, whether or which parts of the recurrent module is more similar to V1 or retina etc. Given the lack of deeper insights here the significance and impact to the community isn’t obvious and I’m leaning reject. \n\n1. The biggest weakness of the paper is that there is very little analysis that follows up the surprising mismatch between FNN modules and mouse visual system. Given that there is now a very large literature on mapping large artificial networks to brain data I think something like this would have vastly improved the paper. For example, take the fact that only the recurrent module shows the tubularity and decodability present in retina and V1 data. A natural follow up (I think within the stated goals of the paper) would be to ask if there are parts of the recurrent module that fit retinal data better than V1 and vice versa (via regression of FNN activations onto neural data and an examination of the regression coefficients). Also along these lines, it would be interesting to know if there are subpopulations in the recurrent module that are more sensitive to the feedforward input (this could be checked simply by plotting the distribution of input weights to the recurrent module). Can you fit retinal data well just with these subpopulations? Some more analyses along these lines would go towards actually explaining how the FNN and biological visual areas process inputs. \n\n2. I didn’t find that the measure of tubularity was very well motivated. Since it was the main novel method introduced in the paper I think a comparison with other metrics would’ve significantly strengthened the paper. For example, does tubularity allow us to conclude anything beyond what the simple decoding analysis revealed? I take the author's point that measuring tubularity shows that though recurrent modules can decode stimulus type, the trajectories are less “tight” than biological trajectories. But again, this conclusion suffers from a lack of further analyses which show that this is an important difference from the perspective of computation and representation. Why should we care about relative tightness if across the entire noisy data set both systems can linearly separate stimulus type? \n\n3. I was a bit confused about why the readout was included in the analyses. If my understanding is correct, the authors are analyzing a readout module from the original Wang paper, session 8 scan 5. The weights of this module are trained to predict data from an individual mouse. They then compare this module with recordings from a completely different animal. Given how the training of the FNN is set up one would very strongly expect there to be no correspondence. Could the author clarify why we might expect a correspondence given that this part of the system was trained exclusively to predict activity from cells that aren't present the dataset examined?\n\n4. I found some of the recommendations a bit speculative and confusing. The authors suggest introducing recurrence earlier on to make activations more biologically plausible, yet point out elsewhere that the retina is itself not strongly recurrent. There’s a tension here between introducing biologically non-plausible circuit mechanisms to introduce more biological plausible activation patterns that I think deserved a bit of attention."}, "questions": {"value": "See above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "53yFD7Ky7u", "forum": "jUy7vFgoZf", "replyto": "jUy7vFgoZf", "signatures": ["ICLR.cc/2026/Conference/Submission17665/Reviewer_ppwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17665/Reviewer_ppwn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823905556, "cdate": 1761823905556, "tmdate": 1762927517133, "mdate": 1762927517133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors use a computational neuroscientist's toolbox to analyse the FNN, a foundational model of the mouse visual cortex\n\nNamely, they find so-called decoding and encoding manifolds using the kinds of simple stimulus videos one would expect to see in neuroscience experiments, and characterise them with a novel 'tubularity' metric.\n\nThey find that the feedforward encoder lacks biologically plausible stimulus-dependent temporal patterns despite having temporal convolutions, and that the recurrent module learns seperable stimulus representations. However, V1 trajectories are shown to be more tubular than those developed by FNNs."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "This is a good, principled approach to understanding foundational models, and certainly the correct flavour of research to do with them now that we have obtained them. The paper goes on to provide concrete architectural suggestions (early-stage recurrence, feature dimensionality constraints). Similarly, the identification of padding artifacts as a source of non-biological behavior is a valuable practical finding."}, "weaknesses": {"value": "Figures don't do the approach justice, and more time spent on styling would make the paper much more visually pleasing to read. e.g. PSTH were too small/low resolution to really inspect.\n\nTubularity is not well motivated - it is mentioned as a similarity mteric for biological and artifical systems, but the reasoning for this specific form is not provided. e.g., why not just use straightness of manifolds? Overall, there was a lack of comparison to the wider range of measures to characterise neural trajectories.\n\nWording was difficult to parse in multiple places, namely when discussing the encoding manifold 'arms' on page 5."}, "questions": {"value": "Why only analyze L1 and L13 from the 15-layer encoder? A more systematic analysis across all layers would reveal how representations gradually develop. What guided this choice?\n\nThe tubularity metric is novel and mathematically defined, but lacks comparison to alternative measures. Have you considered simpler metrics like trajectory straightness, smoothness, or variance?\n\nYou suggest adding recurrence to early encoder stages - can you be more specific about how this could be achieved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lZJDnudrcf", "forum": "jUy7vFgoZf", "replyto": "jUy7vFgoZf", "signatures": ["ICLR.cc/2026/Conference/Submission17665/Reviewer_pp5o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17665/Reviewer_pp5o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028322651, "cdate": 1762028322651, "tmdate": 1762927516714, "mdate": 1762927516714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to study the internal representations of a specific, recently proposed digital-twin–style neuro-foundation model that predicts neural activity responses to visual stimuli, in an attempt to understand the computations in such models, and also whether the computations performed by this model or its learned representations resemble those of the brain itself. The authors approach this question by looking at neural encoding and decoding manifolds, where they analyse for several layers or modules in the model, the similarity in encoding properties of neurons across stimuli (using the neuron factor from tensor decompositions to indicate shared or similar circuit properties) and trial-wise analysis of neural population dynamics (condition-averaged low-dimensional projections of neural activity trajectories) respectively. The authors also introduce a metric, tubularity, through which they quantitatively compare trajectories extracted from the model and trajectories in neural state space (mouse neural activity). Overall, the authors find that the earlier encoder components of the model do not show much similarity to biological networks, but that recurrence and additional constraints on output modules could starkly improve the bio-plausibility of these foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* I think the paper is focused on an important endeavour. Interpretability is an important research direction – so we have an understanding of how models arrive at their predictions – and adopting a dynamical systems approach inspired by computational neuroscience seems like the most promising candidate to me for obtaining useful insights into model function.\n* The authors have introduced a metric, tubularity, which can be compared across contexts (i.e., artificial neural trajectories vs. biological ones), which directly gets at the question of comparing models to brains in a quantitative way."}, "weaknesses": {"value": "* One of the main issues I have with this paper is its lack of clarity and poor presentation. The figures are nowhere near publication-ready and it is very hard to really understand what's going on from them – which severely impacts my rating as the main focus of the paper is interpretability. I would highly encourage the authors to take inspiration from other computational neuroscience papers and spruce up their figures, and also improve the writing:\n    * The introduction lacks a clarity, doesn't have a clear flow or story, and does not do a good job of motivating the study. I was left asking, \"why should a foundation model that is a strong predictive model of neural activity actually be brain-like?\" – maybe it doesn't have to be, and the authors are simply curious whether the representations are akin to those of the brain, but this wasn't made clear. Furthermore, what does it mean when the authors say that neuro-foundation models \"generalise\" to OoD scenarios, especially in reference to Fig. 1A which seems unclear? Similarly, what does it mean for them to be \"supportive\" or \"asking questions\"?\n    * Some acronyms, e.g., FNN, PCA, are never defined fully prior to using the acronym. In general, several paragraphs explain results with jargon and assume familiarity on the reader's part with the techniques and how to interpret them. I found it quite hard to understand what exactly was being conveyed, e.g., in Section 3 – and some of the claims did not seem very apparent to me from the figures. Furthermore, details on the FNN model being considered are quite sparse, both in the main text and appendix – this should be described better as it is central to the paper.\n    * Overall, I wasn't able to clearly evaluate whether the claims were supported by clear, unambiguous evidence due to a combination of the figures and writing (see also my point below on lack of quantitative analyses).\n* Another key issue with the paper is the lack of quantitative analyses apart from the one tubularity-based comparison. There is no attempt to compare the dynamics of the FNN's trajectories to those of actual neural activity based on metrics such as Dynamical Similarity Analysis (DSA) or Canonical Correlation Analysis (CCA). These have been used in many works in the literature to compare the dynamics and representational geometry of models and actual neural activity, e.g., https://www.biorxiv.org/content/10.1101/2024.10.04.616712v3 and https://www.biorxiv.org/content/10.1101/2025.02.07.637062v1. Without comprehensive quantitative analysis, unfortunately interpretability becomes akin to reading tea leaves based on visualisations, and is quite far-removed from the real data. As we are looking at reduced-dimensional spaces extracted using specific methods, we are limited by the assumptions or flaws of those methods.\n* Only specific layers from each \"module\" in the FNN architecture were chosen for the analysis. Why were these chosen, e.g., why only L1 and L13 from the encoder? Could the authors not analyse the evolution of representations through the layers of the network? This goes back to my point about quantifying their results and showing something concrete through their analyses and figures, e.g., if separation by stimuli increases through layers (and quantifying separation through clustering metrics, for example).\n* It's important to note that there are different kinds of \"foundation models\" for neuroscience, and not all of them have to do with predicting neural activity from stimuli. There are several other neural decoding and joint encoding + decoding \"foundation model\" approaches, e.g.:\n  * Decoding, spikes: NDT-2 (https://openreview.net/forum?id=CBBtMnlTGq), POYO (https://openreview.net/forum?id=sw2Y0sirtM), POSSM (https://openreview.net/forum?id=1i4wNFgHDd, incorporates recurrence – important as discussed in this work), NDT-3 (https://openreview.net/forum?id=utXSSdD9mt)\n  * Decoding, Calcium imaging, transformer-based: POYO+ (https://openreview.net/forum?id=IuU0wcO0mo)\n  * Encoding and decoding, spikes: NEDS (https://openreview.net/forum?id=vOdz3zhSCj)\n\n  There is no discussion on these complementary or related approaches, and I think it would be important to provide an overview of the field and clarify what kind of models the focus is on here."}, "questions": {"value": "* It is unclear to me whether a digital-twin–style neuro-foundation model _must_ itself have computations/representations that resemble the brain – wouldn't strong predictive machine, even if not brain-like, still be useful in the experimental loop? I feel this point lacks justification and could be motivated better.\n* On a related note, is it very surprising that these models don't resemble the brain much? They weren't trained on interventional data or with specific architectural details, constraints, or bottlenecks to force their representations to be similar to the brain's. Why then would we expect them to be similar? I would further encourage quantitative analyses as mentioned in my listed weaknesses to substantiate points such as recurrence bringing about better bio-plausibility.\n* How were the specific layers analysed selected? Could the authors not look at all layers and how representations/computations evolve through them?\n* Could the authors introduce better quantitative comparisons between neural data and models, rather than just qualitative comparisons, such as through CCA, DSA, etc.?\n* Why are there no results/figures on encoding manifolds for the neural data itself? I thought, based on the introduction, that a key point was to compare the FNN model's neurons to biological ones on the basis of encoding properties as well?\n* Why was the input specification to the FNN model modified, as described in Section A.3? Would the results not be affected by using fewer frames overall, and was there any attempt to finetune the FNN model or provide longer sequences as inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w5NCp1YsXZ", "forum": "jUy7vFgoZf", "replyto": "jUy7vFgoZf", "signatures": ["ICLR.cc/2026/Conference/Submission17665/Reviewer_MUj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17665/Reviewer_MUj9"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061977743, "cdate": 1762061977743, "tmdate": 1762927516155, "mdate": 1762927516155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}