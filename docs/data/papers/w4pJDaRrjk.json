{"id": "w4pJDaRrjk", "number": 2435, "cdate": 1757085428227, "mdate": 1759898148292, "content": {"title": "MegaScience: Pushing the Frontiers of Open Post-Training Datasets for Science Reasoning", "abstract": "Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present **TextbookReasoning**, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce **MegaScience**, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance (e.g., +3.24\\% for Qwen3-30B-A3B). In addition, **MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning**. We release our data curation pipeline, evaluation system, datasets, and nine trained models to the community to advance scientific reasoning research.", "tldr": "We introduce MegaScience, a large-scale, high-quality scientific reasoning dataset that significantly improves model performance and scaling for AI in science.", "keywords": ["Scientific Reasoning", "Post-Training Datasets", "Open Science", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f305c7081ae63217fa7305952b0ba0d5d0615f78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two new datasets designed to advance scientific reasoning in AI systems:\n\n1. TEXTBOOKREASONING: A large, open-source, university-level scientific dataset comprising 650K challenging questions and step-by-step solutions. These are derived from over 12,000 scientific textbooks across a wide range of domains, including mathematics, biology, physics, economics, and more.\n2. MEGASCIENCE: A comprehensive collection of high-quality, open-source datasets containing over one million data points.\n\n\n**Key Contributions**\n\n1. The release and open-sourcing of the TEXTBOOKREASONING and MEGASCIENCE datasets.\n2. A detailed presentation and open-sourcing of the curation pipeline used to construct both datasets.\n3. A thorough empirical evaluation demonstrating the effectiveness of these datasets in enhancing scientific reasoning capabilities of LLMs. The authors show that base models fine-tuned on these datasets  outperform their corresponding instruction-tuned counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Relevance** The paper tackles the important problem of improving the scientific reasoning on LLM through the creation and open-sourcing of two very valuable resources.\n2. **Impact/Significance** The open-sourcing of the new datasets and, more importantly, of the curation pipeline has the potential to spur more rapid progress in LLM-based scientific reasoning\n3. **Presentation** Overall the paper is well organized, well written, and easy to read and follow.\n4. **Experimental Evaluation** The ablation study clearly shows the importance of key components of the curation pipeline."}, "weaknesses": {"value": "1. **Incorrect or Exaggerated Claims**\nThe authors make several incorrect or overstated claims regarding the strength of their empirical evaluation. For instance, in the abstract, they state:\n\n> \"Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MEGASCIENCE, which *significantly* outperform the corresponding official instruct models in average performance (e.g., +3.24% for Qwen3-30B-A3B)\"\n\nSimilarly, in Section 4.2, they write:\n\n> \"Table 4 shows that Qwen2.5-7B, all Qwen3 models, and Llama3.1-8B trained on MEGASCIENCE *substantially* outperform their official instruction-tuned counterparts, demonstrating MEGASCIENCE ’s effectiveness in pushing the frontier in science.\"\n\nThese statements are not fully supported by the results presented in Table 4:\n\n- **First**, Table 4 shows that *Qwen2.5-1.5B-instruct* and *Qwen2.5-3B-instruct* outperform their MEGASCIENCE-trained counterparts in overall performance.\n- **Second**, *Llama3.1-8B-megascience* is only marginally better than its instruct variant in terms of overall average (+1.6%), and actually performs worse in *specific-avg* and *math-avg* metrics.\n- **Finally**, while the Qwen3 MEGASCIENCE models do consistently outperform their instruct counterparts, the margins are relatively modest (+1%, +0.4%, +2%, +2.8%, and +3.2%). These improvements do not substantiate the use of terms like “*significantly*” or “*substantially*” as claimed by the authors.\n\n2. **Heavy Reliance on LLMs**  \nIn the introduction, the authors critique the heavy reliance on LLMs in key aspects of existing scientific datasets. However, a central component of their own data curation pipeline (the Q-A Pair Refinement module) relies heavily on LLMs. Specifically:\n\n- *DeepSeek-V3* is used to refine Q-A pairs with corresponding source documents to ensure that “questions include all necessary contextual information and answers provide comprehensive explanations with clear reasoning processes.”\n- *Llama3.3-70B-Instruct* is employed to identify Q-A pairs lacking reasoning.\n- *DeepSeek-V3* is again used to “enrich them with explanations and reformat their answers.”\n\nGiven the critical role of the Q-A Pair Refinement module (highlighted by the ablation study showing a 45% performance drop when it is removed), it would be prudent to rigorously evaluate how well these LLMs perform in the refinement tasks. A human evaluation of a small, randomly selected subset of the refined Q-A pairs could provide valuable insights into the quality and reliability of the refinement process."}, "questions": {"value": "Why didn't you evaluate the performance of LLMs involved in various part of the curation pipeline? It seems that this could be done with a limited human evaluation on a small subset of a few hundred randomly selected instance data points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S63AGhwSTF", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_Dmgy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_Dmgy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526943298, "cdate": 1761526943298, "tmdate": 1762916236461, "mdate": 1762916236461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical gap in open-source, high-quality datasets for scientific reasoning. It proposes high-quality TextbookReasoning dataset which is curated from large-scale scientific textbooks through rigorous data pipeline. Furthermore, it absorb high-quality data from existing open-source dataset to construct a large-scale and high-quality post-training dataset MegaScience. Extensive SFT experiments on MegaScience and TextbookReasoning demonstrates the good quality of the proposed datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing and structure of this paper are clear and easy to understand.\n2. The data curation process is rigorous and effective, which not only obtains high-quality data but also conduct strict deduplication and decontamination policies.\n3. The experimental results are strong and convincingly demonstrate the high quality of the proposed datasets."}, "weaknesses": {"value": "1. My main concern is that, in the refinement (Section 2.3) and solution annotation (Section 3.4) processes, the authors employ DeepSeek-V3 to generate solution trajectories. However, given that DeepSeek-V3 performs only moderately on scientific benchmarks (e.g., 59.1 on GPQA-D), its responses are likely to contain many errors and hallucinations. I think the authors should at least conduct some human verification to estimate the proportion of erroneous responses from DeepSeek-V3 to provide a warning for future users."}, "questions": {"value": "1. Could you provide specific details on the LLM prompts or criteria used to filter \"strictly copyrighted\" textbooks?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors are encouraged to provide more details about filtering \"strictly copyrighted\" web sources."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yFtMxtyWQK", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_ZKqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_ZKqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790028476, "cdate": 1761790028476, "tmdate": 1762916236196, "mdate": 1762916236196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces *TextbookReasoning*, an open-source scientific post-training dataset containing over 650k reasoning questions derived from 12k university-level textbooks, and *MegaScience*, a large-scale mixture of open-source datasets with 1.2 million instances.\n\nThe work discussed existing critical challenges in scientific reasoning data, including unreliable benchmark evaluation, less rigorous decontamination, low-quality reference answers, and superficial knowledge distillation. Systematic ablation studies help being clear with effectiveness of data selection. Experiments demonstrate strong performance and training efficiency. The resources are released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written with well-motivated research goals.\n- The ablation study on data creation and combination is comprehensive.\n- The evaluation covers diverse reasoning-intensive tasks across various domains.\n- The data, prompts, and models are fully released."}, "weaknesses": {"value": "- The paper needs more discussion and comparison with relevant works. For example, OpenThoughts[1] and S1.1[2] have released data and models, but their performance is not compared here. It would be valuable to compare with works that do not use MegaScience or TextbookReasoning.\n- The paper focuses on post-training, but current paradigms commonly apply RL as post-training as well. This work lacks discussion of RL approaches for improving general reasoning performance, like General-Reasoner[3]. While it's acceptable to focus on SFT, comparing TextbookReasoning for RL or benchmarking against other RL-based works would be valuable.\n- Since refinement quality is important (as shown in this work), the dataset creation assumes access to a strong model for data generation.\n- Claims like \"web content is now saturated with AI-generated text\" should include quantitative evidence and citations.\n- Copyright of textbook data might be a concern, but the author explained in the ethical statement.\n\n[1] OpenThoughts: Data Recipes for Reasoning Models\n\n[2] s1: Simple test-time scaling\n\n[3] General-Reasoner: Advancing LLM Reasoning Across All Domains"}, "questions": {"value": "See the weaknesses section.\n\nAnd I may have missed this—how was the subsample size in Table 1 determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bv9uOdgDM0", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_BVV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_BVV2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944050898, "cdate": 1761944050898, "tmdate": 1762916236034, "mdate": 1762916236034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper made two contributions: \n1. It collects scientific textbook PDFs online, extracting questions from the textbooks, and curates a dataset called textbook reasoning with 650k examples of question and answer pairs. \n2. It conducts extensive ablations and proposes a data mixture called megascience that combines existing sources and textbook reasoning for post-training LLMs for scientific tasks. \n\nThe authors show that fine tuning on the proposed mixture can improve the performance on various scientific question answering and reasoning tasks; and it conducted extensive ablation studies to validate their data filtering and mixing strategies for constructing MegaScience."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper made two contributions: \n1. It collects scientific textbook PDFs online, extracting questions from the textbooks, and curates a dataset called textbook reasoning with 650k examples of question and answer pairs. \n2. It conducts extensive ablations and proposes a data mixture called megascience that combines existing sources and textbook reasoning for post-training LLMs for scientific tasks. \n\nThe authors show that fine tuning on the proposed mixture can improve the performance on various scientific question answering and reasoning tasks; and it conducted extensive ablation studies to validate their data filtering and mixing strategies for constructing MegaScience."}, "weaknesses": {"value": "While this paper makes some good contribution, I think there are some limitations: \n1. I don’t think there’s significant novelty in this work – it uses standard methods to collect and clean the collected dataset, and constructs the proper data mixture. \n2. I think the performance improvement is somewhat limited: for example, in table 3 and 4, training on a new million scale corpus only yields 3 absolute point improvements, while one would expect bigger improvements (e.g., see the dataset scaling study in the openthoughts paper https://arxiv.org/abs/2506.04178)"}, "questions": {"value": "- Can you provide detailed stats for the TextbookReasoning dataset – i.e., breakdown of the domains, the average input and output token length."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I think there are potential copyright concerns as the authors directly scrape the textbook PDFs and I don’t think they’ve reported reliable measures to address the copyright issues (line 493, the primary copyright filtering is through LLM detected copyright information, which can be unreliable.)"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MA7eg5IrUJ", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_vNaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_vNaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128219116, "cdate": 1762128219116, "tmdate": 1762916235827, "mdate": 1762916235827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}