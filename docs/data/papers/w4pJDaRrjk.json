{"id": "w4pJDaRrjk", "number": 2435, "cdate": 1757085428227, "mdate": 1763753283889, "content": {"title": "MegaScience: Pushing the Frontiers of Open Post-Training Datasets for Science Reasoning", "abstract": "Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present **TextbookReasoning**, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce **MegaScience**, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance (e.g., +3.24\\% for Qwen3-30B-A3B). In addition, **MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning**. We release our data curation pipeline, evaluation system, datasets, and nine trained models to the community to advance scientific reasoning research.", "tldr": "We introduce MegaScience, a large-scale, high-quality scientific reasoning dataset that significantly improves model performance and scaling for AI in science.", "keywords": ["Scientific Reasoning", "Post-Training Datasets", "Open Science", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d66de26b94cddb29fb70dc83e5aafae36985f84.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two new datasets designed to advance scientific reasoning in AI systems:\n\n1. TEXTBOOKREASONING: A large, open-source, university-level scientific dataset comprising 650K challenging questions and step-by-step solutions. These are derived from over 12,000 scientific textbooks across a wide range of domains, including mathematics, biology, physics, economics, and more.\n2. MEGASCIENCE: A comprehensive collection of high-quality, open-source datasets containing over one million data points.\n\n\n**Key Contributions**\n\n1. The release and open-sourcing of the TEXTBOOKREASONING and MEGASCIENCE datasets.\n2. A detailed presentation and open-sourcing of the curation pipeline used to construct both datasets.\n3. A thorough empirical evaluation demonstrating the effectiveness of these datasets in enhancing scientific reasoning capabilities of LLMs. The authors show that base models fine-tuned on these datasets  outperform their corresponding instruction-tuned counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Relevance** The paper tackles the important problem of improving the scientific reasoning on LLM through the creation and open-sourcing of two very valuable resources.\n2. **Impact/Significance** The open-sourcing of the new datasets and, more importantly, of the curation pipeline has the potential to spur more rapid progress in LLM-based scientific reasoning\n3. **Presentation** Overall the paper is well organized, well written, and easy to read and follow.\n4. **Experimental Evaluation** The ablation study clearly shows the importance of key components of the curation pipeline."}, "weaknesses": {"value": "1. **Incorrect or Exaggerated Claims**\nThe authors make several incorrect or overstated claims regarding the strength of their empirical evaluation. For instance, in the abstract, they state:\n\n> \"Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MEGASCIENCE, which *significantly* outperform the corresponding official instruct models in average performance (e.g., +3.24% for Qwen3-30B-A3B)\"\n\nSimilarly, in Section 4.2, they write:\n\n> \"Table 4 shows that Qwen2.5-7B, all Qwen3 models, and Llama3.1-8B trained on MEGASCIENCE *substantially* outperform their official instruction-tuned counterparts, demonstrating MEGASCIENCE ’s effectiveness in pushing the frontier in science.\"\n\nThese statements are not fully supported by the results presented in Table 4:\n\n- **First**, Table 4 shows that *Qwen2.5-1.5B-instruct* and *Qwen2.5-3B-instruct* outperform their MEGASCIENCE-trained counterparts in overall performance.\n- **Second**, *Llama3.1-8B-megascience* is only marginally better than its instruct variant in terms of overall average (+1.6%), and actually performs worse in *specific-avg* and *math-avg* metrics.\n- **Finally**, while the Qwen3 MEGASCIENCE models do consistently outperform their instruct counterparts, the margins are relatively modest (+1%, +0.4%, +2%, +2.8%, and +3.2%). These improvements do not substantiate the use of terms like “*significantly*” or “*substantially*” as claimed by the authors.\n\n2. **Heavy Reliance on LLMs**  \nIn the introduction, the authors critique the heavy reliance on LLMs in key aspects of existing scientific datasets. However, a central component of their own data curation pipeline (the Q-A Pair Refinement module) relies heavily on LLMs. Specifically:\n\n- *DeepSeek-V3* is used to refine Q-A pairs with corresponding source documents to ensure that “questions include all necessary contextual information and answers provide comprehensive explanations with clear reasoning processes.”\n- *Llama3.3-70B-Instruct* is employed to identify Q-A pairs lacking reasoning.\n- *DeepSeek-V3* is again used to “enrich them with explanations and reformat their answers.”\n\nGiven the critical role of the Q-A Pair Refinement module (highlighted by the ablation study showing a 45% performance drop when it is removed), it would be prudent to rigorously evaluate how well these LLMs perform in the refinement tasks. A human evaluation of a small, randomly selected subset of the refined Q-A pairs could provide valuable insights into the quality and reliability of the refinement process."}, "questions": {"value": "Why didn't you evaluate the performance of LLMs involved in various part of the curation pipeline? It seems that this could be done with a limited human evaluation on a small subset of a few hundred randomly selected instance data points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S63AGhwSTF", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_Dmgy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_Dmgy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526943298, "cdate": 1761526943298, "tmdate": 1762916236461, "mdate": 1762916236461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical gap in open-source, high-quality datasets for scientific reasoning. It proposes high-quality TextbookReasoning dataset which is curated from large-scale scientific textbooks through rigorous data pipeline. Furthermore, it absorb high-quality data from existing open-source dataset to construct a large-scale and high-quality post-training dataset MegaScience. Extensive SFT experiments on MegaScience and TextbookReasoning demonstrates the good quality of the proposed datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing and structure of this paper are clear and easy to understand.\n2. The data curation process is rigorous and effective, which not only obtains high-quality data but also conduct strict deduplication and decontamination policies.\n3. The experimental results are strong and convincingly demonstrate the high quality of the proposed datasets."}, "weaknesses": {"value": "1. My main concern is that, in the refinement (Section 2.3) and solution annotation (Section 3.4) processes, the authors employ DeepSeek-V3 to generate solution trajectories. However, given that DeepSeek-V3 performs only moderately on scientific benchmarks (e.g., 59.1 on GPQA-D), its responses are likely to contain many errors and hallucinations. I think the authors should at least conduct some human verification to estimate the proportion of erroneous responses from DeepSeek-V3 to provide a warning for future users."}, "questions": {"value": "1. Could you provide specific details on the LLM prompts or criteria used to filter \"strictly copyrighted\" textbooks?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors are encouraged to provide more details about filtering \"strictly copyrighted\" web sources."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yFtMxtyWQK", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_ZKqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_ZKqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790028476, "cdate": 1761790028476, "tmdate": 1762916236196, "mdate": 1762916236196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[Part 2/2] General Response to Copyright Concern"}, "comment": {"value": "Our copyright-related keyword list is as follows:\n```\ncopyright_related_keywords = {\n    'all_rights_reserved': [\n        r'all\\s*rights\\s*reserved',\n        r'without.*permission.*prohibited',\n        r'reserved\\s*rights\\s*of',\n        r'rights\\s*reserved\\s*by'\n    ],\n    'no_reproduction': [\n        r'no\\s*part.*may\\s*be\\s*reproduced',\n        r'reproduction.*strictly\\s*prohibited',\n        r'may\\s*not\\s*be\\s*copied',\n        r'may\\s*not\\s*be\\s*duplicated',\n        r'copying\\s*without\\s*permission',\n        r'unauthorized\\s*copying'\n    ],\n    'no_commercial_use': [\n        r'commercial\\s*use\\s*prohibited',\n        r'no\\s*commercial\\s*use',\n        r'strictly\\s*non-?commercial',\n        r'not\\s*for\\s*sale',\n        r'not\\s*for\\s*commercial\\s*distribution'\n    ],\n    'no_distribution': [\n        r'distribution.*prohibited',\n        r'no\\s*distribution',\n        r'may\\s*not\\s*be\\s*shared',\n        r'unauthorized\\s*distribution'\n    ],\n    'no_modification': [\n        r'modification.*prohibited',\n        r'no\\s*modification',\n        r'alteration.*prohibited',\n        r'may\\s*not\\s*be\\s*edited',\n        r'may\\s*not\\s*be\\s*modified'\n    ],\n    'no_derivative_works': [\n        r'derivative\\s*works.*prohibited',\n        r'no\\s*derivative\\s*works',\n        r'adaptations.*prohibited',\n        r'transformation.*prohibited'\n    ],\n    'publisher_exclusive': [\n        r'exclusive\\s*property\\s*of',\n        r'exclusively\\s*owned\\s*by',\n        r'property\\s*of\\s*(the\\s*)?publisher',\n        r'belongs\\s*to\\s*(the\\s*)?publisher'\n    ],\n    'written_permission_required': [\n        r'written\\s*permission.*required',\n        r'prior\\s*written\\s*consent',\n        r'obtain\\s*permission\\s*before\\s*(use|reproduction)',\n        r'permission\\s*in\\s*writing\\s*from'\n    ],\n    'copyright_violation_warning': [\n        r'copyright\\s*violation',\n        r'infringement.*legal\\s*action',\n        r'unauthorized\\s*use.*prosecuted',\n        r'violation.*of\\s*copyright\\s*law',\n        r'copyright\\s*protected\\s*material'\n    ],\n    'educational_only_strict': [\n        r'educational\\s*use\\s*only.*no\\s*other',\n        r'strictly\\s*educational',\n        r'for\\s*classroom\\s*use\\s*only',\n        r'academic\\s*use\\s*only'\n    ],\n    'internal_use_only': [\n        r'internal\\s*use\\s*only',\n        r'confidential.*internal',\n        r'for\\s*internal\\s*circulation',\n        r'not\\s*for\\s*public\\s*release'\n    ],\n    'no_ai_training': [\n        r'artificial\\s*intelligence.*prohibited',\n        r'machine\\s*learning.*prohibited',\n        r'ai\\s*training.*prohibited',\n        r'may\\s*not\\s*be\\s*used\\s*to\\s*train\\s*(ai|machine\\s*learning)',\n        r'not\\s*authorized\\s*for\\s*(ai|ml)\\s*training'\n    ],\n    'no_data_mining': [\n        r'data\\s*mining.*prohibited',\n        r'text\\s*mining.*prohibited',\n        r'scraping.*prohibited',\n        r'may\\s*not\\s*be\\s*harvested',\n        r'no\\s*automated\\s*(scraping|analysis|collection)'\n    ],\n    'drm_protected': [\n        r'drm\\s*protected',\n        r'digital\\s*rights\\s*management',\n        r'copy\\s*protection',\n        r'encrypted\\s*content',\n        r'digital\\s*watermark'\n    ],\n    'redistribution_limited': [\n        r'not\\s*authorized\\s*for\\s*redistribution',\n        r'may\\s*not\\s*be\\s*posted\\s*online',\n        r'posting\\s*on\\s*the\\s*internet.*prohibited',\n        r'not\\s*to\\s*be\\s*shared\\s*electronically'\n    ],\n    'legal_notice': [\n        r'legal\\s*notice',\n        r'governed\\s*by\\s*copyright\\s*law',\n        r'subject\\s*to\\s*copyright\\s*restrictions'\n    ]\n}\n```"}}, "id": "6ATXOqFXim", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705803004, "cdate": 1763705803004, "tmdate": 1763724277734, "mdate": 1763724277734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces *TextbookReasoning*, an open-source scientific post-training dataset containing over 650k reasoning questions derived from 12k university-level textbooks, and *MegaScience*, a large-scale mixture of open-source datasets with 1.2 million instances.\n\nThe work discussed existing critical challenges in scientific reasoning data, including unreliable benchmark evaluation, less rigorous decontamination, low-quality reference answers, and superficial knowledge distillation. Systematic ablation studies help being clear with effectiveness of data selection. Experiments demonstrate strong performance and training efficiency. The resources are released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written with well-motivated research goals.\n- The ablation study on data creation and combination is comprehensive.\n- The evaluation covers diverse reasoning-intensive tasks across various domains.\n- The data, prompts, and models are fully released."}, "weaknesses": {"value": "- The paper needs more discussion and comparison with relevant works. For example, OpenThoughts[1] and S1.1[2] have released data and models, but their performance is not compared here. It would be valuable to compare with works that do not use MegaScience or TextbookReasoning.\n- The paper focuses on post-training, but current paradigms commonly apply RL as post-training as well. This work lacks discussion of RL approaches for improving general reasoning performance, like General-Reasoner[3]. While it's acceptable to focus on SFT, comparing TextbookReasoning for RL or benchmarking against other RL-based works would be valuable.\n- Since refinement quality is important (as shown in this work), the dataset creation assumes access to a strong model for data generation.\n- Claims like \"web content is now saturated with AI-generated text\" should include quantitative evidence and citations.\n- Copyright of textbook data might be a concern, but the author explained in the ethical statement.\n\n[1] OpenThoughts: Data Recipes for Reasoning Models\n\n[2] s1: Simple test-time scaling\n\n[3] General-Reasoner: Advancing LLM Reasoning Across All Domains"}, "questions": {"value": "See the weaknesses section.\n\nAnd I may have missed this—how was the subsample size in Table 1 determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bv9uOdgDM0", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_BVV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_BVV2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944050898, "cdate": 1761944050898, "tmdate": 1762916236034, "mdate": 1762916236034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[Part 1/2] General Response to Copyright Concern"}, "comment": {"value": "We sincerely thank all reviewers for their careful reading of our manuscript and for raising important concerns regarding copyright and data sourcing. We greatly appreciate the opportunity to clarify our procedures and provide additional details.\n\nIn terms of copyright detection, we have built a dedicated test set along with a rule-based + LLM-based detection pipeline. While we cannot guarantee perfect detection across all data, on our test set, our pipeline successfully identifies **100% of the textbooks with copyright restrictions**, and 10% of unrestricted samples are mistakenly flagged as restricted. This demonstrates the **high recall** and the **strictness** of our detection procedure.\n\nIn our **Ethics Statement**, we clearly emphasize the following points:\n\n1. Our dataset is **for research use only**, and when released, it will be distributed under the **CC BY-NC-SA 4.0 license**, which explicitly prohibits commercial use.\n\n2. We will **open a Feedback and Removal Request channel**. After the dataset is released, if we receive any feedback indicating that certain data may violate copyright or related rights, we will promptly remove the corresponding content.\n\nTo construct our **test set**, we manually curated **50 textbooks with known copyright restrictions** as positive samples and **50 textbooks without any copyright limitations** as negative samples. This dataset is used to evaluate and validate the accuracy of our detection pipeline. Our pipeline successfully identifies **100% of the textbooks with copyright restrictions**, and 10% of unrestricted samples are mistakenly flagged as restricted. This demonstrates the **high recall** and the **strictness** of our detection procedure.\n\nThe detailed **rule + LLM detection procedure** is as follows:\n\n1. We manually construct a **copyright-related keyword list**. Using regular expressions, we scan all documents and flag those containing these keywords as **candidate documents** potentially subject to copyright restrictions.\n\n2. For each book, we also include its **first 5 and last 5 segments** (each being a 4096-token chunk) in the candidate list, since copyright information is typically found at the beginning or end of a book.\n\n3. We then use Llama-3.3-70B-Instruct to determine whether each document is **permitted for public use and redistribution** and **does not prohibit AI training**, outputting either “Yes” (allowed) or “No” (not allowed). If **any document** within a book is classified as “No” the entire book is excluded from our dataset. To improve robustness, each candidate document is evaluated **twice** independently, and only documents labeled “Yes” in both passes are retained.\n\nThe LLM detection prompt we use is as follows:\n```\nYou are an expert analyst specializing in copyright, licensing, and terms of use for textual documents. Your task is to determine the permissions granted for a given document.\n\nTask: Analyze the provided document text and determine whether it is explicitly permitted for public use and redistribution **AND** does not explicitly prohibit its use for Artificial Intelligence training.\n\n**Guidelines:**\n\n1.  **Focus on Explicit Statements:** Pay close attention to words and phrases related to:\n    *   **Permission:** \"public domain,\" \"open access,\" \"free to share,\" \"distribute,\" \"Creative Commons\" (e.g., CC BY, CC0), \"permitted,\" \"allowed,\" \"you may use this for...\"\n    *   **Prohibition/Restriction:** \"all rights reserved,\" \"prohibited,\" \"may not,\" \"for personal use only,\" \"not for redistribution,\" \"requires permission,\" \"no derivative works,\" and crucially, any mention of \"AI,\" \"machine learning,\" \"ML,\" \"training,\" \"data mining,\" or \"algorithmic use\" in a restrictive context.\n\n2.  **Decision Logic:**\n    *   Your decision should be **\"YES\"** only if the document **clearly indicates it is allowed for public use/redistribution** AND **contains no explicit prohibition against AI/ML training**.\n    *   If the document is silent on AI training but allows redistribution, default to \"YES\" unless other restrictive terms contradict it.\n    *   Your decision should be **\"NO\"** if the document is clearly restricted (e.g., all rights reserved, for personal use only) OR if it **explicitly forbids use for AI/ML training**, even if it seems publicly available.\n\nOutput Format: You MUST output your response in the following exact format. Do not add any other text before or after.\n\nAnalysis:\n<Provide a concise, step-by-step explanation of your reasoning based on the document's text. Point to specific phrases or the lack thereof that led to your decision.>\n\nDecision:\n<YES/NO>\n- YES: The document is permitted for public use and redistribution and does not prohibit AI training.\n- NO: The document is NOT permitted for public use and redistribution OR it explicitly prohibits AI training.\n\nInput:\nThe document:\n`<DOCUMENT>`\n```"}}, "id": "kJYC0vT5Wu", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705865314, "cdate": 1763705865314, "tmdate": 1763727161778, "mdate": 1763727161778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper made two contributions: \n1. It collects scientific textbook PDFs online, extracting questions from the textbooks, and curates a dataset called textbook reasoning with 650k examples of question and answer pairs. \n2. It conducts extensive ablations and proposes a data mixture called megascience that combines existing sources and textbook reasoning for post-training LLMs for scientific tasks. \n\nThe authors show that fine tuning on the proposed mixture can improve the performance on various scientific question answering and reasoning tasks; and it conducted extensive ablation studies to validate their data filtering and mixing strategies for constructing MegaScience."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper made two contributions: \n1. It collects scientific textbook PDFs online, extracting questions from the textbooks, and curates a dataset called textbook reasoning with 650k examples of question and answer pairs. \n2. It conducts extensive ablations and proposes a data mixture called megascience that combines existing sources and textbook reasoning for post-training LLMs for scientific tasks. \n\nThe authors show that fine tuning on the proposed mixture can improve the performance on various scientific question answering and reasoning tasks; and it conducted extensive ablation studies to validate their data filtering and mixing strategies for constructing MegaScience."}, "weaknesses": {"value": "While this paper makes some good contribution, I think there are some limitations: \n1. I don’t think there’s significant novelty in this work – it uses standard methods to collect and clean the collected dataset, and constructs the proper data mixture. \n2. I think the performance improvement is somewhat limited: for example, in table 3 and 4, training on a new million scale corpus only yields 3 absolute point improvements, while one would expect bigger improvements (e.g., see the dataset scaling study in the openthoughts paper https://arxiv.org/abs/2506.04178)"}, "questions": {"value": "- Can you provide detailed stats for the TextbookReasoning dataset – i.e., breakdown of the domains, the average input and output token length."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I think there are potential copyright concerns as the authors directly scrape the textbook PDFs and I don’t think they’ve reported reliable measures to address the copyright issues (line 493, the primary copyright filtering is through LLM detected copyright information, which can be unreliable.)"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MA7eg5IrUJ", "forum": "w4pJDaRrjk", "replyto": "w4pJDaRrjk", "signatures": ["ICLR.cc/2026/Conference/Submission2435/Reviewer_vNaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2435/Reviewer_vNaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128219116, "cdate": 1762128219116, "tmdate": 1762916235827, "mdate": 1762916235827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}