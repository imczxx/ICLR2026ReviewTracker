{"id": "CmyqMn3HeJ", "number": 12404, "cdate": 1758207592140, "mdate": 1759897512250, "content": {"title": "Process Reinforcement through Implicit Rewards", "abstract": "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", "tldr": "Scalable RL solution with implicit dense rewards for advanced reasoning of language models", "keywords": ["LLM", "Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74bd4e3d9ffa79002c4ae35962c7118c4f29df26.pdf", "supplementary_material": "/attachment/12af42a91b73227bc3a7a9be3ee48616d927c7d4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a method for directly initializing process reward models from base or SFT models. These models are then incorporated into the reinforcement learning process. The authors also introduce an online update for the process reward models using cross-entropy loss. They evaluate their training pipeline on mathematics and programming tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is clearly written and easy to follow.\n\n- It addresses the important challenge of annotation-efficient process reward modeling in RL.\n\n- The authors propose PRIME, a method that eliminates the need for a dedicated reward modeling stage by simply initializing the process reward model from the SFT or base model."}, "weaknesses": {"value": "- **Limited Novelty**\n\nThere is a major concern about the novelty of the proposed methods for addressing the practical challenge of incorporating dense rewards into online RL, as outlined in section 2.2. The methods introduced in this paper appear to have been well studied in prior research. For instance, for Challenge 1, previous work [1,2,3] has already provided well-defined and theoretically sound token-level rewards. For Challenge 2, prior research such as [4] has adopted an online update strategy for reward modeling using MCTS and rule-based verification. For Challenge 3, existing studies such as [5] have established a theoretical foundation for deriving token-level rewards from pre-trained or SFT LLMs, which can then serve as training-free reward models for RL.\n\n- **Lack of Clarification of Core Objective**  \n\nThere is another major concern about the clarity of Equation 5 in the paper (in \"RLOO with implicit process rewards\"). The rationale for subtracting the sequence-level baseline $\\frac{1}{K-1}\\sum_{j\\neq i}r_\\phi(\\mathbf{y}^j)$ from the token-level reward $r_\\phi(y^i_s)$ is unclear.  Given that a token-level advantage function evaluates an action relative to other actions at the same state, using a baseline computed over entire sequences (with non-identical states) seems theoretically unsound. Furthermore, the definition of $r_\\phi(\\mathbf{y}^j)$ is ambiguous; I assume it represents  $\\beta\\log\\frac{\\pi_\\phi(\\mathbf{y}^j)}{\\pi_\\text{ref}(\\mathbf{y}^j)}$, but this should be explicitly stated.\n\n- **Lack of Justification for the CE Loss**\n\nThe paper provides insufficient theoretical and empirical justification for using CE loss to update the PRIME model at Algorithm 1. A more rigorous explanation is needed to clarify why this specific loss function is appropriate for the proposed framework.\n\n- **Limited Experimental Baselines**\n\nThe experimental comparisons are limited. The main results only compare PRIME against an outcome verifier, while several highly relevant baselines [2,3,5,6] are missing. Furthermore, other related work [7,8] has also explored incorporating process reward models into RL. The omission of key comparisons raises concerns about the claimed effectiveness of the proposed framework. \n\n[1] From r to Q: Your Language Model is Secretly a Q-Function, COLM 2024.\n\n[2] Discriminative Policy Optimization for Token-Level Reward Models, ICML 2025.\n\n[3] DPO Meets PPO: Reinforced Token Optimization for RLHF, ICML 2025.\n\n[4] ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search, NuerIPS 2024.\n\n[5] Generalist Reward Models: Found Inside Large Language Models, Arxiv 2025.\n\n[6] Preference-Grounded Token-Level Guidance for Language Model Fine-Tuning, NeurIPS 2023.\n\n[7] Dense Reward for Free in Reinforcement Learning from Human Feedback, ICML 2024.\n\n[8] Let's Verify Step by Step, Arxiv 2023."}, "questions": {"value": "- Question about Equation 3. Why is this defined as a process reward? I think the authors should refer to the conclusion from some previous work (e.g., [1]) and provide further explanation in the paper.\n\n- This work shares great similarity with previous work [2]. More discussion is needed to clarify this in the Related Work section.\n\n[1] From r to Q: Your Language Model is Secretly a Q-Function, COLM 2024.\n\n[2] Free Process Rewards without Process Labels, ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0z4BVbmDXM", "forum": "CmyqMn3HeJ", "replyto": "CmyqMn3HeJ", "signatures": ["ICLR.cc/2026/Conference/Submission12404/Reviewer_PFTD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12404/Reviewer_PFTD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760493361152, "cdate": 1760493361152, "tmdate": 1762923302063, "mdate": 1762923302063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds on the idea of implicit process rewards from Yuan et al (2024), which is a DPO-like transformation of outcome rewards into per-token process rewards through a DPO-like function $\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_t | y_{<t}}{\\pi_{ref}(y_t | y_{<t})})$ for \"positive\" examples (correct outcome) and $\\log (1-\\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_t | y_{<t}}{\\pi_{ref}(y_t | y_{<t})}))$ for negative samples (incorrect outcome).\n\nThis process reward is then converted into an advantage for PPO training, yielding improvements in overall performance and in efficiency, all in mathematical reasoning. Experiments are based mainly on Eurus-2-7b (which I think is just a fine-tuned variant of Qwen2.5-math-7b-Inst? unclear), and in comparison with RLOO (leave-one-out advantage estimation).\n\nAdditional experiments quantify the effect of online updates (figure 4 and 5), training efficiency (table 2), alternative RL algorithms (fig 7), and additional rollouts (fig 6).\n\n\nYuan et al 2024: https://arxiv.org/abs/2412.01981"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The method seems relatively straightforward to implement, and seems to yield significant improvements on mathematical reasoning tasks.\n\nThe supplementary experiments comprehensively evaluate several aspects of the application of the method."}, "weaknesses": {"value": "Many of the claimed contributions seem like they are really attributable to Yuan et al 2024 rather than this submission. While the paper cites Yuan et al repeatedly, it could be clearer about what specifically is novel in this submission.\n\nDespite the extensive discussion of the motivation for the approach in 2.2, I'm still left uncertain as to why implicit process rewards work, given that there is no new information about which parts of the rollout were actually impactful. I think it would probably be possible to say more about this, though that may be out of scope for this paper."}, "questions": {"value": "- Sorry if I missed it, but what is Eurus-2-7b-sft and how does it relate to Qwen?\n- What specifically is the contribution of this paper wrt Yuan et al 2024?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uJdD5KHrQh", "forum": "CmyqMn3HeJ", "replyto": "CmyqMn3HeJ", "signatures": ["ICLR.cc/2026/Conference/Submission12404/Reviewer_v7hP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12404/Reviewer_v7hP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761006274443, "cdate": 1761006274443, "tmdate": 1762923301717, "mdate": 1762923301717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Process Reinforcement through Implicit Rewards (PRIME)” proposes a scalable reinforcement learning framework for large language models (LLMs) that uses implicit process rewards instead of sparse outcome rewards. PRIME updates process reward models online using only outcome labels, eliminating costly step-level annotations and reducing reward hacking. It integrates token-level dense and outcome rewards into standard RL algorithms like PPO, REINFORCE, and RLOO. Experiments on math and coding benchmarks show significant gains—up to 15% improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, well structured, and effectively communicates its methods, results, and insights.\n\n2.  PRIME is practically designed, requiring no separate reward model training or step-level labeling. It can  straightforward to integrate into existing LLM training pipelines"}, "weaknesses": {"value": "1. The experiments focus mainly on mathematical reasoning and coding, which, although challenging, represent a narrow set of structured tasks.  \n\n2.  The proposed method only evaluated on Qwen based method. However, a few of works have been pointout that  qwen based method have serious test data leak problem. \n\nWu, Mingqi, et al. \"Reasoning or memorization? unreliable results of reinforcement learning due to data contamination.\" arXiv preprint arXiv:2507.10532 (2025).\n\n3. Lots of implicit reward formulation have been proposed from various perspective. This paper  omits discussion on possible bias or divergence between implicit and true process rewards."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ie59jhfUvJ", "forum": "CmyqMn3HeJ", "replyto": "CmyqMn3HeJ", "signatures": ["ICLR.cc/2026/Conference/Submission12404/Reviewer_CxT8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12404/Reviewer_CxT8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445654426, "cdate": 1761445654426, "tmdate": 1762923301357, "mdate": 1762923301357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PRIME, a novel reinforcement learning framework for LLMs that derives dense process-level rewards implicitly from outcome-level supervision. Instead of relying on costly, explicitly annotated process reward models, PRIME learns token-level feedback signals online by contrasting the current policy with a reference model, effectively bridging the gap between sparse and dense reward formulations. The proposed approach integrates seamlessly with existing RL algorithms and demonstrates strong empirical results across a variety of reasoning benchmarks. Overall, the method is elegant, efficient, and empirically validated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-organized, with clear motivation, algorithmic description, and empirical validation.\nThe implicit reward formulation is also conceptually sound and well-motivated, allowing dense process feedback without explicit annotations.\nMoreover, PRIME consistently improves accuracy and sample efficiency across multiple reasoning benchmarks, often surpassing larger or more heavily trained baselines.\nThe method introduces minimal complexity and can be readily integrated into standard RL pipelines, making it both accessible and impactful for the community.\nThe experimental evaluation is also comprehensive.\nThe results are consistently strong and demonstrate both the effectiveness and robustness of the proposed method."}, "weaknesses": {"value": "I did not find any major issues in the paper, but I still have a few questions that I hope the authors could clarify.\n\nFirst, the authors mention that the proposed method can mitigate reward hacking. However, in domains such as math reasoning, where predefined correctness rules may already provide reliable outcome-based rewards, the advantages of implicit process rewards are less evident. \nI think this paper would benefit from a more detailed discussion or empirical analysis on whether the proposed approach is indeed more effective in more general or less structured tasks.\n\nThe second concern is somewhat related, regarding generalization. \nThe experiments focus mainly on reasoning tasks, where both the task structure and the verifier are well defined. It would strengthen the paper to discuss or test PRIME’s potential applicability to open-ended or weakly structured domains (e.g., dialogue generation or summarization), where defining correct outcomes is more subjective.\nI would be happy if the authors engage in a discussion on these points, and I am open to revising my score based on discussions."}, "questions": {"value": "Please refer to the concerns in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zn9YoWJ2j0", "forum": "CmyqMn3HeJ", "replyto": "CmyqMn3HeJ", "signatures": ["ICLR.cc/2026/Conference/Submission12404/Reviewer_sryj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12404/Reviewer_sryj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001285146, "cdate": 1762001285146, "tmdate": 1762923300884, "mdate": 1762923300884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}