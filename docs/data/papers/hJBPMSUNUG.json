{"id": "hJBPMSUNUG", "number": 3320, "cdate": 1757401137613, "mdate": 1759898096166, "content": {"title": "Discovering heterogeneous synaptic plasticity rules via large-scale neural evolution", "abstract": "Synaptic plasticity is a fundamental substrate for learning and memory, yet how functional behaviors emerge from heterogeneous synaptic plasticity mechanisms remains poorly understood. Here, we introduce a computational framework that harnesses Darwinian evolutionary principles to discover biologically plausible, heterogeneous synaptic plasticity rules within a biologically realistic model of the mouse primary visual cortex. Specifically, we parameterize several key factors related to synaptic plasticity, including pre- and post-synaptic spikes, their associated eligibility traces, and neuromodulatory signals. By integrating these factors via a truncated Taylor expansion, we construct a large-scale search space of candidate plasticity rules, with each rule containing over 2.6k optimizable parameters. Each rule is subsequently evaluated in a multitask learning setting according to both task performance and biological validity. Leveraging a multi-objective evolutionary algorithm, we effectively navigate this high-dimensional search space to identify plasticity rules that are both biologically plausible and yield high task performance. Experimental results demonstrate that the discovered plasticity rules can attain high task performance with orders of magnitude fewer training samples than the gradient-based rule, while also maintaining biological plausibility and exhibiting scalability across various network sizes. Collectively, these findings offer insights into potential mechanisms by which the brain may implement heterogeneous synaptic plasticity rules and provide a foundation for the development of more efficient and adaptive artificial intelligence systems.", "tldr": "", "keywords": ["Synaptic plasticity", "Evolutionary computation", "Computational neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c66aa5e6bbd244d902fda85276ce5a96c3e44179.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a computational framework that uses Darwinian evolution to automatically discover biologically plausible, heterogeneous synaptic plasticity rules in a realistic model of mouse primary visual cortex (V1). By parameterizing plasticity as a Taylor expansion of pre- and postsynaptic spikes, eligibility traces, and neuromodulators, the authors create a compact space plasticity rules. A multi-objective evolutionary algorithm optimizes rules for both task performance and biological validity. The resulting rules achieve high performance on visual change-detection tasks with far fewer samples than gradient-based methods, generalize to networks of different sizes, and maintain biological constraints, offering insights into how the brain might implement efficient, heterogeneous plasticity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- First large-scale attempt to let evolution simultaneously optimize thousands of plasticity coefficients inside a biological cortical model.\nIntroduces a compact closed-form plasticity rules that keeps rules local and therefore biologically testable (Up to 25 terms, far surpassing existing plasticity rules containing 2 to 4 terms).\n- Multi-objective fitness to control both the task performance and biological plausibility.\n- If I understand it correctly, the exact same rules work after rescaling on networks from 1 k to 5k neurons without re-evolution."}, "weaknesses": {"value": "- The work is an impressive proof-of-concept that evolutionary search can find local plasticity rules which learn quickly and remain biologically plausible, but it stops short of asking whether those rules generalize to new and practical tasks. My main concern after reading this paper is “what should the readers take away?” Sample efficiency of Hebbian plasticity has already been proved by many previous works. According to my knowledge, the real problem for Hebbian plasticity is “are there general plasticity rules that generalize any tasks & networks?” So far, the generalization of those local learning rules can not compare with that of gradient descent.\n- The paper contains no statistical comparison with other biologically plausible learning rules."}, "questions": {"value": "- The paper should explain the Evaluation Tasks (visual change detections) more clearly at least in the appendices, which is less clear for readers who are not familiar enough\n\n- Can you explain where did the $c^k$ in equation 7 go in Table 1? Are Table 1 cluster different plasticity rules by $g^k$ only? I think $c^k$ is real number and is non-homogeneous for different synapses. Why is it possible to collapse to 4000 unique rules?\n\n- Did the paper study the problem of generalization? What is the relation between training/testing and training/validation trials? Are those tasks unchanged? Or are the tasks different? How different they are?\n\n- It seems to me that evolution are done on a fixed V1 model, while the evolved plasticity rules can be applied to train different networks / networks of different size? Please clarify more on the experiment settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XqddvL4XJR", "forum": "hJBPMSUNUG", "replyto": "hJBPMSUNUG", "signatures": ["ICLR.cc/2026/Conference/Submission3320/Reviewer_t465"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3320/Reviewer_t465"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651342392, "cdate": 1761651342392, "tmdate": 1762916668017, "mdate": 1762916668017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a synaptic plasticity evolutionary method that achieves a balance between task performance and biological interpretability through heterogeneous plasticity. This method is trained by finding an interpretable search space. Also, the plasticity algorithm they discovered has better generalization and the ability to learn from few shot learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1.** It's a very interesting job with intriguing ideas. The ideas presented offer fresh perspectives and will likely stimulate further academic discussion.\n\n**2.** The thesis is well-written, logical, and clear. The manuscript demonstrates good academic writing standards.\n\n**3.** The research question is clearly defined."}, "weaknesses": {"value": "**1.** The article is not very clear on how it assesses biological interpretability. Is it through the values related to firing rate in the metrics? Why would similar firing rates be more biologically similar? The methodology for evaluating biological interpretability requires clarification. \n\nFor example, How model units are aligned or matched with biological neurons, How the neural recordings from LGN were preprocessed?\n\n**2.** There is no paragraph explaining how the similarity with the LGN regions is calculated and how the neural data is utilized.\n\n**3.** Many formulas are presented without explaining the meanings of all the symbols. It is recommended to provide a complete explanation."}, "questions": {"value": "See Weakness Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GpeEa15xOI", "forum": "hJBPMSUNUG", "replyto": "hJBPMSUNUG", "signatures": ["ICLR.cc/2026/Conference/Submission3320/Reviewer_Txin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3320/Reviewer_Txin"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893308762, "cdate": 1761893308762, "tmdate": 1762916667723, "mdate": 1762916667723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale framework for evolving heterogeneous synaptic plasticity rules within a model of mouse V1. This work is ambitious, and seeks to unify many observed biological features of synaptic plasticity by using a multi-objective optimization across biological and task metrics and exploring a vast parameter space of rules. The approach is technically impressive and represents a major engineering effort. However, the biological and computational insights drawn from the discovered rules remain limited: the evolved rules are simple, the tasks are minimal, and there is little analysis of why heterogeneity helps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Ambitious attempt to systematically explore a large rule space under biological constraints.\n- Clear and reproducible description of the evolutionary framework.\n- A tour de force of synaptic plasticity modelling and related works."}, "weaknesses": {"value": "The scientific hypothesis being tested isn’t clearly stated. Filling in gaps between existing approaches isn’t a hypothesis. I think stemming from this, the paper is somewhat unfocused in its choice of related literature and methods description throughout. For example I think much of section 2 could go in the appendix. As a result, there is no space for details of the V1 model in the main text. \n\nIt reads as if many of the modelling choices are included because they exist in the literature and field, rather than being focused behind testing a hypothesis.  For example, it is not clear why the network needs to be so large: if general principles are being discovered, a reduced network would likely be a more tractable and informative place to start than beginning with 17 neuron types. Also, to this point, it is not shown clearly that heterogeneous complexity is even needed. This is especially pertinent given the simplistic task nature, and simplicity of the rank1 and rank2 rules found. \n\nIn my opinion the primary weakness that needs to be addressed is the empirical results. These results, as currently presented, do not support the claims in the abstract. The single task (image change detection) is overly simple and does not support the general claim of “high task performance”. Moreover, I am not convinced by the comparison to Adam. From Figure 6, it looks as if there is an initialisation issue with Adam. At the very least there should be a hyperparameter search over Adam’s parameters. It may also be that without some regularisation (e.g. the metrics in 3.3) the network struggles to learn. Also, cutting off training when task accuracy is ~%70 is strange. Neither learning rule appears to have been run to convergence and again, this is important for the claims made.\n\nI would find the claims of superior performance over GD more convincing if the authors were to take an existing SANN task/dataset, for example Heidelberg Digits, and compare their framework to an optimized surrogate gradient benchmark. Ideal would be a small set of tasks that required the network to implement different computational strategies. \n\nFinally, there is minimal mechanistic insight or discussion into how the rules support learning."}, "questions": {"value": "Questions\n\n1. The Adam baseline in Fig. 6 seems poorly tuned and underperforms due to a long period of no learning. Have the authors tested alternative optimiser settings, initialisations, and standard regularisations?\n2. Is the heterogenous nature of plasticity rules required? Have the authors tested reduced models with fewer cell types? It also appears like very simple rules are sufficient for the task - is that the correct interpretation?\n3. How important is the task choice for the conclusions? Have the authors tested this framework on other tasks?\n4. What insight is gained about why certain terms (pre/post, eligibility, reward) occur in the best rules? \n\nMinor questions: \n1. What is n in (2)? Also, l_i is a little odd for a trial index? \n2. Eq 7, g^k is learned by evolutionary search, correct?\n3. I don’t think the implementation of metrics in 3.3 is explained? They might also be suitable as regularisers for GD. \n4. Are the authors making an implicit assumption that plasticity rules are learning rules? I think a clear statement or discussion on the relationship between plasticity rules and learning rules could be valuable to the reader (but do not think this is a must). My perspective is that synaptic plasticity rules are often components of a larger learning rule or process. But I’d be interested in their perspective. The authors touch on this in paragraph 393 with the observation that reward free rules succeeded.   \n5. Dale’s law, maximum & minimum weights, adaptive scaling (appendix B.2) might be valuable aspects that sculpt which plasticity rules are found. It reads as a little odd that these features were chosen to be explicitly dealt with if the whole point is to discover rules that fit biology. Do the authors have a justification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XUKBtqorpg", "forum": "hJBPMSUNUG", "replyto": "hJBPMSUNUG", "signatures": ["ICLR.cc/2026/Conference/Submission3320/Reviewer_XrNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3320/Reviewer_XrNa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994901400, "cdate": 1761994901400, "tmdate": 1762916666536, "mdate": 1762916666536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to find candidate plasticity rules that could be implemented in the brain using numerical optimisation (meta-learning, “learning the learning rule”). This is an important problem, as direct recordings of weights in vivo during learning are currently impossible, thus computational methods are the only option to integrate all the indirect evidence at our disposal (neural activity during learning, general circuit knowledge, task performance…). \n\nThe authors consider a biologically detailed model of mouse V1 cortex inspired by previous work with 289 synapse types, each evolving with their own, tunable plasticity rule. Plasticity rules are parameterised as Taylor expansions up to the third order on the presynaptic activity, postsynaptic activity and reward. To navigate this high-dimensional search space (~2.6k parameters in total), they use a multi-objective evolutionary strategy, refined from previous work that maintains and evolves a population of 4000 rules.\n\nThey apply this method to a working memory task (image change detection), for which there exists mouse recordings during learning. They select rules based on several objective: task performance, a bias towards simpler rules (fewer non-zero terms), matching of the in silicon and in vivo neural activity, and general biological plausibility (e.g. physiological weight values, individual firing rates). Each candidate plasticity rule (i.e. 289 individual rules) is evaluated by exposing the network to a 100 trials with plasticity on, while performance is evaluated on test trials with plasticity turned off. This means that task needs to be performed with a fixed connectivity learned by plasticity during training, and not via online weight changes during individual trials.\n\nThey find rules that solve the task with comparable performance to the animals. They show that the rules are scale-invariant via simulations in smaller and bigger networks of similar architecture. Their set of 70 best-performing rules display similar fit to data yet meaningfully different expressions (degeneracy). Notably, non-reward modulated rules, and simple, pre-only rules can solve the task, showing that the task at hand can be solved in an unsupervised way.  Finally, they show that their best rules learn the task faster than an implementation of gradient based optimization (Adam)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method presented has many strong points compared to existing work: (i) flexible, high dimensional search space with co-active rules across many synapse types, (ii) large scale, biologically detailed network model which enables (iii) direct comparison with experimental data (iv) multi-objective optimisation which allows to select meaningful candidate rules (v) some level of a population view: recovering several candidate rules, reporting that many different rules are equally consistent with data. Though there exists work with the ingredients mentioned above, this is, to my knowledge, the first study to put all of these together. The only work that comes close, [2], is 1/ very recent and 2/ trades off the flexibility of the search space for a more exhaustive survey of the space using numerical inference instead of local optimisation, and thus provides a complementary instead of redundant perspective. Thus this work is novel and interesting as it adresses many limitations of existing work, and worthy of being shared with the community.\n\n- This work is a technical tour de force, the sheer number of simulations of large-scale, biologically detailed models with so many co-active rules simulated in parallel on GPUs using modern ML libraries lays the groundwork for the field to embrace such large scale simulations more easily in the future. Navigating and optimising such a landscape is notoriously hard, with most previous work effectively failing to do so. The method presented here appears to work at a scale large enough for direct experimental comparison. Moreover, the biological model, search space and optimisation procedure are very well explained. This is remarkable, given the complexity of the method with nested loops of learning systems and evaluations. Overall, I see the methodological and technical side of this work as exceptionally strong, and the evidence that this method could be useful for neuroscience is convincing (provided a few clarifications and additional plots, see question 1)."}, "weaknesses": {"value": "However, despite the exciting aspects above (for which the authors should be commended), I found the results drawn from the method currently lackluster and underdeveloped, especially given how promising their setup is.\n\n1/ The paper lacks a discussion and limitations section, and at present is convincing only as a method paper, not as much as a work that attempts to make actual predictions on which plasticity rules are in the brain. I could still recommend this paper for acceptance for the methods alone, provided some clarifications and some claims to be adjusted accordingly. A much more exciting route would be to dig deeper on the current results and extract some new neuroscientific insights (more on this, with hopefully actionable items in question 3).\n\n2/ Position wrt prior work: though the section comparing this work to prior work is thorough, I think it needs more precision and nuance. Right now, many different papers are bunched together with general statements on limitations for which the reader needs to investigate applies to which reference mention all potential weakness. e.g. for the paragraph line 115: consider a small search space (….), or simple tasks (…). I also think 2 important papers are missing: [1] which is also evolutionary search in an effectively infinite dimension search space, though it is applied to ~single neuron spiking models and simple tasks, without direct comparison to experimental data. [2] which considers 4 co-active rules and a smaller search space, but also in large scale spiking models, with direct comparison to data and a multi-objective selection of rules. Importantly, this paper also finds degeneracy of rules, which is interesting to relate somewhere in discussion to the current paper’s claims. To summarise: 1/ the precise method used in the paper is novel 2/ there exists work that has each of the advantages ascribed by the method, but not all at once. 3/ the combination of the advantages allows for unique capabilities of the method presented (e.g. visualize the trade-offs between different objectives).\n\n3/ Overclaims and paper’s lack of a main message.\n- Line 53 (and similarly Line 58): “systematically”: I don’t think that the authors can claim that at present they are systematically exploring the 2.6K parameter space with an evolutionary strategy, almost by design given how big this space is. It’s both a strength (large, very flexible space) and a weakness (hard to survey systematically) of the paper. Despite this limitation that should be acknowledged somewhere, I think it’s very nice that this paper still makes a point about the observed degeneracy. I am pointing this out mainly in contrast to the SBI line of work [2] which infers posteriors over rules to be able to more systematically explore the search space and degeneracy. In their case, the search space has to be much smaller dimensional.\n- Can you really call this a “multitask setting”, when it’s 2 tasks, both image change detection? Why not simply image detection task with either natural images or gratings. Currently this makes the rules inferred look like they solve a wider range of tasks than they probably do, and makes especially the simple rules found seem suspicious.\n- The comparison with gradient descent. Line 24: “Experimental results demonstrate that the discovered plasticity rules can attain high task performance with orders of magnitude fewer training samples than the gradient-based rule”. I think this statement is vastly overinflated, and I’m not sure how useful this point is, given how underdeveloped the analysis of the results is otherwise. Is the claim destined to alert machine learning practicioners that this paper proposes rules more efficient better than gradient descent? Then many additional results are required to show the results generalises across different tasks and architectures. The gradient descent claim can be rephrased into a more fair sentence acknowledging this is probably heavily dependent (overfit?) on the task and network model considered here (and would require more results on the GD hyperparameter search). I’m not sure of the value of adding the gradient descent debate into this paper, and would rather see the space used to analyse and reverse engineer more thoroughly the rules found to make predictions for neuroscience (see questions). Overall, I think this GD point risks being a distractor on what your framework offers, and I would keep this paper “AI4neuro”, focused at understanding the rules in the brain, and not “Neuro4AI”, aiming to develop novel learning rules for ML inspired by neuro, at least in this submission.\n- The final section of the paper brushes too fast over the interpretation of the plasticity rules and implications for neuroscience (see questions)."}, "questions": {"value": "(1) Fig 4, I would add a panel about the optimisation itself, not only after convergence (for instance the performance of the best rule at each step). I’m especially curious wrt how no plasticity performs on the task, as well as randomly drawn rules from the search space (step 0 of the optimisation). Right now, this is crucial evidence missing to show that your method efficiently navigates the space of rules. It would be a red flag that most rules in the search space are good at this task.\n\n(2) How reproducible is the optimization run shown in the study? I understand that within one run, you keep a population of rules (4000), but this is very small wrt to the dimensionality of the whole search space, so I would expect that you are “missing” many good candidate rules. For instance, starting from a different initialisation, does a run converge to a completely different population of rules? Or do you see regularities within the inferred rules? Apologies if I missed this experiment. And I understand this is a lot of compute to ask for. But at least this should be mentioned in limitations or in the discussion section. This method is not really fit to properly explore degeneracy (given the dimensionality of the space, probably no method can, but the claims in the paper (line 283) imply the opposite).\n\n(3) I would appreciate more work on analysing the solutions obtained by the optimisation. How come simple rules (pre-only or reward-free rules) are able to solve the task by themselves? How are the networks able to keep a memory of the previous stimulus? Since weights are frozen during testing, it has to be in the activity (or is there short term plasticity in the model?)? Can you decode the identity of previous stimulus from the activity? How does this change across time? Can you compare how a static network (without plasticity) behaves in this task, and what changes do a successful plasticity rule brings, especially the simplest ones you found in the optimization? If the evaluation trials are indeed a good metric of generalization performance, this would mean there is a connection between extremely simple, unsupervised rules and generic working memory abilities, which is exciting, and to my knowledge, novel. Can you analyse and compare the solutions that use/don’t use reward, and make predictions on how we could distinguish these two in future experiments. But as it stands, your results instead question the task choice, if reward is not necessary to learn. My worry is that the advantage of using such a complex search space is lost if simple, reward-free rules can solve the task. At least this warrants proper reverse engineering efforts to understand what these rules do to the network and how they solve the task. Nice insights for neuro could fall from this.\n\nSmaller points:\n\n- Why shut off plasticity during test trials? Presumably the candidate rules are restricted to be slow, or at least comparable to the brain. If you leave plasticity enabled during testing, do you learn radically different plasticity rules? Can the task be fairly recasted as novelty detection? In that case, it doesn’t have to be working memory and it would be a nice link between the working memory literature and the novelty detection literature.\n\n- Line 426: distributions of firing rates on the violin plots do look quite different to data still (mainly, a larger variance in exp data). Normally that wouldn’t be a concern for a network of identical neurons like are standard in the field, but given your much more refined model with many neuron types, how do you interpret the inability to better fit the distribution of firing rate (which is objective 6, so part of the fitting procedure if I understand correctly).\n\n- Define heterogenous: when reading initially I understood heterogenous as one rule per synapse, not one per synapse-type. Maybe co-active is better suited?\n\n- Fig 2: I would say from where the data was taken in the caption, not only in the main text.\n\n- I’m confused by the 2 timescales for reward: one is the window over which past rewards are averaged (20, not learned), and the other is the time constant in eq 4. Why not have a single timescale? And if 2, why not learn both?\n\n- Limitation section: besides the points raised above, I would acknowledge there that in experiments, people are moving away from pairwise rules with for instance btsp [4], which are not taken into account by your model.\n\n- Introduction has a whole paragraph on related work, followed by a section entirely dedicated to related work. Maybe merge the two for space, now it functions a bit like a repeat.\n\n- Line 55: “broad but interpretable candidate space of heterogeneous plasticity rules” Are Taylor expansions really a guarantee of interpretability? Especially when there are so many terms and so many co-active rules? By interpretable I mean that can give rise to theoretical insights or experimental predictions. This is a more general point, so does not affect my assessment of the paper.\n\n- Can you explain the choice for a single g for all synapses (which I understood to be tunable)? Wouldn’t the simplicity criterion introduced in the optimization already remove unnecessary terms, and the g could only be there to remove the non-sensical terms in the Taylor expansion once and for all (and potentially removed from the main text for clarity)?\n\n- Enforcing Dale’s law and maximum values: how often do your weights attempt to cross 0? I.e. how often is this condition not met by the rule itself but instead by your added bound. Same question for the upper bound. Ideally these bounds are not reached often.\n\n-  Given that one of the key conclusion is about degeneracy, a discussion of other work studying degeneracy should be added, for instance [2].\n\n- Please reformat the citations so that they are either in parenthesis or numbered (if ICLR allows this).\n\n- This is not really a question, but I found the scale-free result very nice and reassuring on the solutions that you find.\n\n[1] Jordan et al, Evolving interpretable plasticity for spiking networks, eLife, 2021.  \n[2] Confavreux et al, Memory by a thousand rules: Automated discovery of multi-type plasticity rules reveals variety & degeneracy at the heart of learning, bioRxiv, 2025.  \n[3] Bittner et al, Behavioral time scale synaptic plasticity underlies ca1 place fields, Science 2017"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LLU2b6n3ar", "forum": "hJBPMSUNUG", "replyto": "hJBPMSUNUG", "signatures": ["ICLR.cc/2026/Conference/Submission3320/Reviewer_N15c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3320/Reviewer_N15c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001299573, "cdate": 1762001299573, "tmdate": 1762916666080, "mdate": 1762916666080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}