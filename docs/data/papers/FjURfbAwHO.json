{"id": "FjURfbAwHO", "number": 18611, "cdate": 1758289483554, "mdate": 1759897092020, "content": {"title": "SOCIA: Joint Structure–Parameter Co-Optimization for Automated Simulator Construction", "abstract": "Building credible simulators from data is difficult because structure design, parameter calibration, and out-of-distribution (OOD) robustness are tightly coupled. We introduce SOCIA (Simulation Orchestration for Computational Intelligence with Agents), a framework that treats simulator construction as joint structure–parameter co-optimization: it elicits mechanism-rich blueprints, exposes explicit tunable parameters, and instantiates a calibration schema, producing an executable simulator with built-in calibration hooks. SOCIA couples Bayesian Optimization for sample-efficient point calibration with Simulation-Based Inference for uncertainty-aware fitting; diagnostics trigger targeted structural edits in an outer refinement loop to co-optimize design and parameters under tight budgets. Across three diverse tasks, SOCIA consistently outperforms strong baselines, excelling on both in-distribution (ID) fitting and OOD shift. Ablations that weaken structure, calibration design, or tuning yield near-monotone degradations, underscoring the necessity of unified structure–parameter optimization. SOCIA’s code and data are available here.", "tldr": "This paper presents SOCIA, an LLM-guided framework that unifies structure generation and parameter calibration to automatically construct high-fidelity social simulations.", "keywords": ["Structure–Parameter Optimization", "Agentic LLM", "Social Simulation", "LLM-guided Bayesian Optimization", "Parameter Calibration"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aefe69542fe3c9981c2678fdef937849638cf994.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The article introduces a framework for building and tuning simulation models (primarily agent based models). The chain starts with a chain-of-structure specification to encapsulate the situation being modelled. An LLM is used to encode this framework into code, and a combination of Bayesian optimisation for parameter tuning, and further LLM methods for model updating, are used to fine tune the model. Experiments are given in which, of course, the current method provides higher performance than the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I really liked this article's full spectrum approach to modelling, going all the way from the problem specification to fine tuning and testing the resulting simulation model.\nThe paper is on a different topic to most machine learning conference papers (at least the ones I read) but uses lots of highly relevant techniques and addresses problems that machine learning researchers address, using a slightly different toolset. I think it would be a strong contribution to the conference."}, "weaknesses": {"value": "There are some gaps in the presentation. Primarily, the key \"Algorithm 1\" is actually in the Appendix, which I think is verging on cheating the page limit unfairly.\nHowever there were also lots of details of the method that I could not ascertain from reading the article. The most frustrating one for me was the iterative refinement of model structure. Only one sentence used to present this (lines 314-315) but it is a complex and interesting part of the method."}, "questions": {"value": "Please can you give some more detail on each of the components of your method (but not the Bayesian optimisation, which is completely standard). I think they're all interesting but will struggle to accept a paper when so much of the method is not described at all."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SPWy4ua3rW", "forum": "FjURfbAwHO", "replyto": "FjURfbAwHO", "signatures": ["ICLR.cc/2026/Conference/Submission18611/Reviewer_3qgX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18611/Reviewer_3qgX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301835226, "cdate": 1761301835226, "tmdate": 1762928326295, "mdate": 1762928326295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an agentic workflow for automatically building multi-agent simulations based on observational data. The main novelty seems to be the joint optimization of the simulator structure and its hyperparameters."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "n.a."}, "weaknesses": {"value": "[motivation and introduction] While the paper tackles a significant and complex challenge with state-of-the-art methods, the introduction is highly generic. It does not adequately establish what specific task SOCIA performs or what kind of simulators it produces. After multiple readings of the intro, it remains unclear to me what the system’s concrete task and output is (see Figure 1: is SOCIA generating executable code, calibrated data, forecasts, or decision policies?). This becomes a bit clearer after browsing the appendix. \n--> Providing an early, concrete example of a representative simulator and a clear definition of the modeling task would give readers a much clearer entry point into the paper.\n\n[readability] The paper’s notation is inconsistent and often undefined, which makes it hard to understand the central methodology. For instance, $\\lambda$ is described as a “mechanistic blueprint” (l161) while $B$ is later introduced as a “simulator blueprint” taking $\\lambda, \\omega$ as input (l212). Furthermore, $T$ denotes a textual task description and a data count (line 183). These ambiguities, combined with very long and nested sentences (e.g., single sentences spanning lines 219-229, 241-247), make the content difficult to follow. \n--> Clearer notation and more concise writing would significantly improve readability and make the paper accessible to a broader audience."}, "questions": {"value": "This paper lies somewhat outside my core area of expertise (I was likely assigned since I am very familiar with BO), and I found it difficult to grasp the task definition and central methodology fully (see comments below). As a result, I could not thoroughly review the entire paper and appendix, and my feedback focuses on the introduction, technical problem setup, and methodology. While I recognize the potential relevance of the work, I believe the paper requires a major revision. Even for readers outside the immediate subfield (like me), the introduction should more clearly motivate the task/application and clearly define the problem and main ideas to make the paper accessible to a broader audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ziJwYRYK9i", "forum": "FjURfbAwHO", "replyto": "FjURfbAwHO", "signatures": ["ICLR.cc/2026/Conference/Submission18611/Reviewer_AoLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18611/Reviewer_AoLv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920501024, "cdate": 1761920501024, "tmdate": 1762928325924, "mdate": 1762928325924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SOCIA, a framework for joint structure–parameter co-optimization of\nsimulators using LLM-based agents. The system orchestrates a pipeline consisting of a\nChain-of-Structure (CoS) generator, a Code Generation Agent (CGA), a Simulation\nExecution Agent (SEA) for calibration via Bayesian Optimization (BO) and\nSimulation-Based Inference (SBI), and a Feedback Generation Agent (FGA) for structure\nrefinement. The overall goal is to automate the design, implementation, and calibration of\nsimulation models, particularly agent-based models, by coupling structural synthesis\nwith parameter learning and uncertainty estimation. Experiments span three tasks, user\nmodeling, mask adoption, and personal mobility, evaluating both in-distribution (ID) and\nout-of-distribution (OOD) regimes. SOCIA variants outperform or match existing LLM-based\nbaselines (e.g., GSIM, AI Scientist). The work positions itself as a holistic framework for simulation-based science and a\nstep towards self-calibrating simulation agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Ambitious and holistic framing.** SOCIA tackles an important and underexplored\n  question: how to couple structural design and parameter inference in simulation-based\n  modeling using AI agents. The modular pipeline with separate agents working on different parts of the pipeline is\n  well-motivated.\n- **Novel integration.** The combination of LLM-driven structure synthesis\n  with parameter tuning via both GP-based BO and SBI is a novel and interesting\n  direction. The use of SBI for posterior estimation and OOD robustness is particularly\n  compelling.\n- **Empirical coverage.** The experiments span multiple tasks (ID, OOD, intervention),\n  and ablation results illustrate that removing CoS or calibration steps substantially\n  degrades performance.\n- **Clear motivation for uncertainty-aware calibration.** The contrast between SOCIA-BO\n  and SOCIA-SBI provides useful insight into where each approach excels, and supports\n  the authors’ argument that uncertainty helps under regime shifts.\n- **Potentially impactful vision.** If properly extended and validated, the system could\n  form the basis for practical LLM-assisted modeling workflows—especially for\n  agent-based social or behavioral simulations."}, "weaknesses": {"value": "### Conceptual and methodological clarity\n\n- **Implicit focus on agent-based models.** Although the abstract and introduction\n  suggest generality (“simulation-based modeling”), the formulation, notation, and\n  examples are clearly tailored to ABMs (agents, policies, exogenous inputs). The paper\n  should explicitly state this scope and discuss how (or whether) SOCIA can generalize\n  to domains with complex scientific simulators (e.g., neuroscience, physics).\n- **Technical definitions remain vague.** Core components in Section 3.1–3.2 are\n  underdefined: the aggregation operator $A$, the objectives $J_{\\text{train}},\n  J_{\\text{val}}$, and metrics like CRPS appear without formal definitions. The\n  relationship between variables $x, y, u, \\omega$ is unclear across sections (e.g.,\n  the SBI posteriors are defined conditioned on x, not y). \n- **Ambiguity between “agents.”** The term *agent* refers both to simulated entities and\n  to LLM components orchestrating the workflow. This dual meaning often causes confusion\n  and should be disambiguated consistently (e.g., “simulation agent” vs “AI agent”).\n\n### Empirical evaluation and baselines\n\n- **Missing comparison to standard manual workflows.** Evaluation focuses on LLM-based\n  systems (GSIM, AI Scientist) but omits the central baseline of *human-designed\n  simulators calibrated with BO or SBI*. This is critical to assessing whether LLM-based\n  orchestration meaningfully assists existing modeling pipelines.\n- **No expert validation of generated simulators.** Results rely exclusively on\n  numerical metrics. It would be informative to include domain-expert evaluation of\n  simulator plausibility (e.g., whether generated structures make physical or behavioral\n  sense).\n- **Limited compute transparency.** The paper does not report simulator call counts,\n  wall-clock time, or resource usage for BO and SBI calibration loops. Given that\n  structural edits can trigger full re-runs of calibration, compute requirements are\n  important for assessing scalability and practical feasibility.\n- **Section 4.3 lacks orientation and purpose clarity.** The section introduces\n  additional experiments that modify the main tasks, but it lacks a short introductory\n  statement explaining *why* these experiments are run (e.g., testing generalization,\n  intervention robustness, or pipeline autonomy).\n- **Related work coverage is narrow.** The paper cites task-specific SBI/BO works but\n  omits standard references such as Cranmer et al. (2019) or general introductory papers\n  like Deistler & Boelts (2025), which would help situate the work for non-specialist\n  readers.\n\n### Presentation and structure\n\n- The introduction and Section 3 remain abstract; a running example or concrete case\n  study would help anchor the reader.  \n- Figure 1 is referenced late (p. 6) despite being essential for understanding the\n  architecture—earlier guidance to it would improve readability.  \n- Minor confusion arises from describing SBI as “simulate–compare–learn”, which reads\n  like ABC but actually describes conditional density estimation via NPE.  \n\n### Overall contribution\n\nThe main contribution of the paper lies in system integration rather than algorithmic novelty. SOCIA presents an ambitious and well-engineered orchestration of LLM-based structure generation, simulator code synthesis, and calibration via standard BO and SBI components. This is a valuable step toward automating simulation-based modeling workflows, particularly for agent-based settings.\n\nHowever, the paper does not yet articulate a clear technical algorithm underlying the structure–parameter co-optimization loop. The proposed “structural refinement” process appears to operate heuristically, e.g., driven by diagnostics and LLM proposals, without a formally specified objective, acceptance rule, or convergence behavior. While this is understandable given the non-differentiable nature of the involved components, the lack of algorithmic detail makes it difficult to assess the method’s reliability or theoretical grounding.\n\nIn its current form, SOCIA should thus be viewed primarily as a proof-of-concept system demonstrating how modern language models can coordinate established inference and optimization techniques. The idea is promising and potentially impactful, but the technical contribution would be significantly strengthened by a more precise definition of the optimization loop, explicit evaluation traces of structure edits, and a discussion of what (if anything) can be guaranteed or bounded within this framework."}, "questions": {"value": "1. **Aggregation operator \\(A\\):** How exactly is \\(A\\) defined and implemented (line\n   188)? Does it perform statistical aggregation or empirical mapping from micro-level\n   simulations to macro-level indicators?  \n2. **Objectives and metrics:** What is $J$? How is CRPS computed and used in the loss?\n   (line 191)  \n3. **Notation consistency:** Posteriors are defined as $p(\\omega\\mid x)$, but $x$ is\n   described as system state while training data involve $(y,u)$. Please clarify the\n   variable roles.  \n4. **Initialization heuristics:** (line 320) What heuristics justify assuming that CoS\n   provides a near-optimal starting point? How robust is this when the constructed\n   simulator poorly fits the data?  \n5. **Calibration strategy:** How does the CGA or SEA decide between running BO and SBI?\n   Are both always executed, and if so, how are point and posterior estimates combined?  \n6. **Compute scaling:** Each structural edit triggers new simulations and retraining of\n   BO/SBI models. What are the wall-clock costs, simulator call budgets, and\n   computational resources per task?  \n7. **Posterior sampling:** SBI inference reportedly draws 50–100 posterior samples. Why\n   such a low number, given that NPE allows essentially free sampling?  \n8. **Baselines:** How exactly do the Random Search and LR baselines operate? Which\n   simulator are they tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l6xul6gmQe", "forum": "FjURfbAwHO", "replyto": "FjURfbAwHO", "signatures": ["ICLR.cc/2026/Conference/Submission18611/Reviewer_t7Ju"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18611/Reviewer_t7Ju"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077039850, "cdate": 1762077039850, "tmdate": 1762928325477, "mdate": 1762928325477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}