{"id": "aJd636PxP1", "number": 19914, "cdate": 1758300569287, "mdate": 1759897012433, "content": {"title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies", "abstract": "We derive a novel PAC-Bayesian generalization bound for reinforcement learning (RL) that explicitly accounts for Markov dependencies in the data, through the chain’s mixing time. This contributes a step to overcoming challenges in obtaining generalization guarantees for RL where the sequential nature of data does not meet independence assumptions underlying classical bounds. Our bound provides non-vacuous certificates for modern off-policy algorithms like Soft Actor-Critic. We demonstrate the bound’s practical utility through PB-SAC, an algorithm that optimizes the bound during training to guide exploration. Experiments across continuous control tasks show that our approach provides meaningful confidence certificates while maintaining competitive performance.", "tldr": "Traditional generalization bounds assume independent data, but RL trajectories are sequential and dependent, making classical bounds inapplicable or vacuous for reinforcement learning.", "keywords": ["Reinforcement Learning", "PAC-Bayes"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/133a15d14b27d2fa586a6f88e8bbfc40320b597b.pdf", "supplementary_material": "/attachment/0ccb4cf0be29980578125672222accfa9838488b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a PAC Bayesian formulation to bound the discounted returns of reinforcement learning policies with high probability. The original aspect of the bound is to characterize the effect of a single-step change in a trajectory on the subsequent time-steps using prior results developed for Markov chains. This bound developed on trajectory variations is then converted into a PAC Bayes bound using the standard recipe of change of measure inequality. The paper proposes an algorithm to implement this bound in a deep actor-critic training pipeline and evaluates its performance on four standard continuous control benchmarks from the MuJoCo physics engine."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The allocation of the bounded differences formula in Lemma 3.1 and incorporation of it into a PAC Bayes building process is novel and interesting.\n * The results reported in Figure 1 indeed demonstrate that the bound is not only non-vacuous but also tight."}, "weaknesses": {"value": "The paper has a number of major weaknesses I list below and investigate further via my questions in the next section: \n \n * The presentation has many points that welcome an improvement. For instance, I have a very hard time to follow Sections 4.2, 4.3, and 4.4. Section 4.2 points to different PAC Bayes bound types until ending up with Equation 9 with an overly brief justification of each logical step. It reads very much like a random walk within the literature. Section 4.3 is only about the generic applicability of the very well known log-trick. A single-sentence pointer to the issue would be sufficient. Section 4.4 uses concepts such as \"curriculum\" and \"posterior syncing\" which are either not established or not used in the proposed way in prior work. The high-rate posterior sampling phrase likewise misses a reference. Whether 512 samples is high depends on the context. The experiments section misses to explain the rationale behind the particular design choices.\n * The paper has a large number of statements whose correctness is questionable. For example the **improved scaling** paragraph in Section 3.3 compares the suggested bound to two earlier bounds that have been developed actually on the Bellman error in the L_2 space, while the paper studies the signed difference between population and empirical estimates of the return. They are two different quantities and the investigation of each follows separate motivations. For example, the suggested bound cannot be used for uncertainty-aware policy search, i.e. model selection, as suggested in Fard et al., AISTATS, 2012 because it can't be used to guarantee a contraction mapping. This confusion about which exact quantity is being bounded diffuses into the whole storyline, which I don't think can be fixed within the scope of a rebuttal.\n * The experiment results do not really support the intended claim. Except for Half Cheetah, actually the results only indicate that none of the models, both baselines and the proposed model, is coming anywhere close solving the task. Relatedly, the reported SAC results do not match its known performance profiles in the very hyperparameter setting it has been trained. The authors for instance can see in the following paper that much higher reward scores should have been observed especially in Ant and Walker [1].\n * The paper lacks focus, which makes the interpretation of the outcomes not possible. Is the goal to build the tightest PAC Bayes bound for continuous control setups? Then I would expect to see what a simple McAllester bound would be doing, but the paper doesn't provide any comparison. Is it to reach highest performance while providing \"some\" generalization guaranteest. Then I'd say the performances are significantly behind the state of the art. So the goal has not been achieved. Is the goal to do directed exploration as claimed in Section 4.1? Then I would expect to see results in a proper sparse-reward setup. Only in this case a comparison to PBAC would be sensible. However, the experiments appear to have been studied in classical MuJoCo scenarios with dense rewards where directed exploration is simply not required, even should not be done. All the demonstrated improvement over PBAC stems from this simple and well-known fact. The paper attempts to solve all these difficult problems and ends up with solving none of them.\n\n[1] Hui et al., Double Gumbel Q-Learning, NeurIPS, 2023"}, "questions": {"value": "* Is there a theoretical justification of the $\\epsilon-$Thompson exploration technique used in Line 4 of the pseudo-code? It reads like a mix of $\\epsilon-$greedy exploration and Thompson sampling. Hence it inherits the properties of both. But as it is actually none of the two, it is questionable how much the theoretical guarantees of any of the are maintained.\n * Why do we need the actor freezing approach in Line 2 of the pseudo-code? It is not a common practice in applied reinforcement learning, at least not in the way prescribed by this paper. Is it a prerequisite to make the suggested algorithm work? If no, what do the results look like without it? If yes, where does this fragility come from?\n * What does Line 27 of the pseudo-code do? Does it truly collect a number of complete trajectories with a frozen model? If yes, does this not generate a huge sample complexity? Furthermore, if we assume to have a budget of taking such full roll-outs repeatedly, why should not an ordinary PAC Bayes bound fit on first-visit Monte Carlo estimates extracted from these roll-outs be enough? Note that the return calculated from each of them will be independent samples. Furthermore, in the studied continuous control cases, most probably first-visit and every-visit samples will be equivalent as it is probability zero for the same state to be visited more than once on a continuous state space.\n * SAC is performing maximum-entropy RL. Hence its reward function is appended an entropy bonus. The critic network is trained to predict this modified reward function. Has this fact been taken into account while computing and interpreting the PAC Bayes bounds? Are the results in Figure 1 truly discounted return results with respect to environment reward or max-ent reward? If second, we can't conclude much from them as the entropy score is generated from the model itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0KKZyCtaBH", "forum": "aJd636PxP1", "replyto": "aJd636PxP1", "signatures": ["ICLR.cc/2026/Conference/Submission19914/Reviewer_GWMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19914/Reviewer_GWMr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037076615, "cdate": 1761037076615, "tmdate": 1762932082325, "mdate": 1762932082325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new PAC-Bayesian bound for RL that, for the first time, is able\nto account for temporal dependencies in policy-induced Markov chains, avoiding\nthe independence assumptions needed by prior work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-written and clearly motivated paper\n- Provides the first PAC-Bayesian bound for RL that is able to handle temporal dependencies\n- The derived bound is tighter than prior work by Tasdighi et al. (2025) and Fard et al. (2012)\n- The experiments provide initial evidence that the method is applicable in practice\n- A public implementation is available"}, "weaknesses": {"value": "- Despite the bound relying on $\\tau_\\text{min}$ as an important parameter, its estimation is left rather vague in line 275f, without further details or any discussion in the experiments\n- The practical application in PB-SAC requires several further modifications (Secs. 4.1–4.4), without any ablations on their respective necessities or the sensitivity of PB-SAC with respect to them\n- The experiments are limited to four continuous MuJoCo environments with dense rewards against just two baselines. SAC and PBAC were originally proposed for sparse rewards. The authors acknowledge “carefully select[ing]” the hyperparameters for PB-SAC without tuning the others leaving the comparison unbalanced.\n\n### Minor weaknesses\n- The bibliography is broken, with many incomplete references; e.g., Amit et al. were published at NeurIPS; Fard et al. (2012) is a UAI publication; some references include the venue editors, others don’t, etc.\n- Equation (6) should be $\\neq$ instead of $=$ in the identity function\n- Several hyperparameters are unclear; e.g., what is $\\epsilon_\\text{explore}$?"}, "questions": {"value": "- Q1: How sensitive is the approach to each of the adaptations discussed in Secs. 4.1–4.4?\n- Q2: How does the runtime compare against vanilla SAC?\n- Q3: What is the theoretical justification for using a moving-average update on the prior? Does that not violate its data-independence requirement?\n- Q4: Can the authors speculate on how easily the approach is transferable to other actor-critic methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3uxJfIV29", "forum": "aJd636PxP1", "replyto": "aJd636PxP1", "signatures": ["ICLR.cc/2026/Conference/Submission19914/Reviewer_Yysq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19914/Reviewer_Yysq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761247546311, "cdate": 1761247546311, "tmdate": 1762932079539, "mdate": 1762932079539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a major theoretical and algorithmic advance in certified reinforcement learning. The authors derive a new PAC-Bayesian generalization bound for RL that explicitly accounts for temporal dependencies in trajectories via the mixing time of the policy-induced Markov chain. This is a significant step forward compared to prior work, which either relied on martingale-based concentration (often requiring unnatural assumptions) or used bounds that become vacuous in long-horizon, high-discount settings. The authors then propose PB-SAC, a practical deep RL algorithm that leverages this PAC-Bayesian bound as a live, optimizable performance certificate. The algorithm alternately optimizes the posterior distribution (via a PAC-Bayes-κ objective) and the exploration policy, using posterior-guided exploration and an adaptive sampling curriculum to stabilize training. Experiments on standard MuJoCo continuous control benchmarks (HalfCheetah, Ant, Hopper, Walker2d) show that the PAC-Bayesian bound tightens over time as the policy improves, providing meaningful confidence certificates. Crucially, PB-SAC maintains competitive performance with or even exceeds SAC, demonstrating that theoretical guarantees can be achieved without sacrificing learning efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Theoretical Novelty and Correctness:** The derivation of a PAC-Bayesian bound with explicit mixing time dependence is a significant theoretical contribution. The use of Paulin’s (2018) McDiarmid-type inequality for Markov chains is well-motivated and correctly applied. The bound improves on prior work by avoiding the (1−γ)⁻⁴ scaling, making it non-vacuous even for γ = 0.99.\n\n2. **Practical Algorithm Design:** PB-SAC is remarkably well-designed and practical. The use of a diagonal Gaussian posterior, moving average prior updates, and the adaptive sampling curriculum are all clever, stable, and empirically effective solutions to the \"posterior syncing shock\" problem.\n\n3. **Integration of Theory and Practice:** This is one of the rare works that successfully translates a theoretical bound into a practical training algorithm. The bound is not just a passive certificate but an active component of the learning process, guiding exploration and optimization."}, "weaknesses": {"value": "1. **Computational Overhead:** The PAC-Bayes update cycle (every 20k steps) and the need to collect fresh rollouts and estimate mixing time (via autocorrelation) introduce significant computational overhead. The paper does not discuss how this scales to larger networks or more complex environments. It would be helpful to include a brief discussion of computational cost and potential optimizations.\n\n2. **Mixing Time Estimation:** The method for estimating mixing time (using reward autocorrelation) is robust to overestimation but sensitive to underestimation. The paper acknowledges this, but more discussion on the reliability and variability of this estimation across different environments would strengthen the work. For instance, how much does the bound fluctuate with different autocorrelation estimates?\n\n3. **Generalization to Other RL Frameworks:** The paper focuses on actor-critic methods (SAC). It would be valuable to briefly discuss how the proposed bound and algorithm might be extended to value-based methods (e.g., DQN) or model-based RL.\n\n4. **Limitation of KL Divergence:** The paper acknowledges that the use of KL divergence can be unstable when posteriors diverge. While this is not a flaw in the current work, future work could explore alternative divergence measures (e.g., Wasserstein distance), which are more stable and geometry-aware."}, "questions": {"value": "1. The mixing time estimation via reward autocorrelation is a key component of your algorithm. Could you provide a quantitative analysis of the variance and bias in this estimation across different environments (e.g., HalfCheetah vs. Ant)? How sensitive is the final bound to this estimation?\n\n2. In Section 5.2, the authors show that the PAC-Bayesian bound tightens over time. Could you visualize the evolution of the KL divergence term in the bound over training? Does this track the tightening of the bound?\n\n3. The adaptive sampling curriculum (512 samples during critic adaptation) is crucial for stability. How does the performance of PB-SAC degrade if you reduce the sampling rate during adaptation (e.g., 64 or 128 samples)? Is this a critical hyperparameter?\n\n4. The paper focuses on continuous control tasks. How do you expect the proposed method to scale to episodic, sparse-reward tasks (e.g., Atari games)? Would the mixing time estimation still be reliable?\n\n5. The use of a diagonal Gaussian posterior is a simplifying assumption. How would the algorithm behave if you used a more expressive posterior approximation (e.g., normalizing flows)? Would the bound still be tight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ciyZvidj93", "forum": "aJd636PxP1", "replyto": "aJd636PxP1", "signatures": ["ICLR.cc/2026/Conference/Submission19914/Reviewer_z6cf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19914/Reviewer_z6cf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976359982, "cdate": 1761976359982, "tmdate": 1762932070772, "mdate": 1762932070772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}