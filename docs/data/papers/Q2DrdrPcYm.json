{"id": "Q2DrdrPcYm", "number": 9264, "cdate": 1758116690302, "mdate": 1759897734472, "content": {"title": "CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models", "abstract": "Watermarking algorithms for Large Language Models (LLMs) effectively identify machine-generated content by embedding and detecting hidden statistical features in text. However, such embedding leads to a decline in text quality, especially in low-entropy scenarios where performance needs improvement. Existing methods that rely on entropy thresholds often require significant computational resources for tuning and demonstrate poor adaptability to unknown or cross-task generation scenarios. We propose $\\textbf{C}$ontext-$\\textbf{A}$ware $\\textbf{T}$hreshold watermarking (CATMark), a novel framework that dynamically adjusts watermarking intensity based on real-time semantic context. CATMark partitions text generation into semantic states using logits clustering, establishing context-aware entropy thresholds that preserve fidelity in structured content while embedding robust watermarks. Crucially, it requires no pre-defined thresholds or task-specific tuning. Experiments show CATMark improves text quality in cross-tasks without sacrificing detection accuracy.", "tldr": "", "keywords": ["watermark", "large language model", "cross-task"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0864416925b1f8eff9a9d6353ad3f385f5c38b3b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CATMARK, a dynamic watermarking framework designed to improve watermark robustness across heterogeneous, mixed-modality text generation tasks (e.g., code interleaved with natural language). The method builds on red/green list watermarking but introduces context-aware entropy thresholding: it clusters tokens based on logit similarity to learned prototypes, computes per-cluster entropy thresholds using historical entropy distributions, and applies watermarking only to tokens exceeding those thresholds. This allows adaptive control over watermark strength without manual tuning. Experiments on HumanEval, MBPP, MATH-500, and StackEval show improved cross-task watermark robustness and text quality relative to static-threshold baselines such as SWEET and EWD. CATMARK also reports strong resilience under paraphrase and back-translation attacks with minimal runtime overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The algorithmic description is detailed, with explicit mathematical definitions for clustering, thresholding, and detection weighting.\n* Extensive cross-task benchmarks (HumanEval, MBPP, MATH-500, StackEval) convincingly demonstrate improved robustness without quality degradation.\n* The focus on mixed-modality sequences (code + text) is valuable and more realistic than single-domain watermark studies.\n* The inclusion of hyperparameter sensitivity, attack robustness (back-translation, paraphrasing), and computational overhead analysis adds credibility and empirical rigor to the work.\n* The clear pseudo-code and definitions (Algorithms 1–2) make reproduction feasible."}, "weaknesses": {"value": "* The method is largely an elaboration of entropy-weighted or adaptive watermarking. Prior works — Context-Aware Watermark with Semantic Balanced Green-Red Lists (EMNLP 2024), Adaptive Text Watermark for LLMs (Liu & Bu, 2024), From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models (Wang et al., 2025), and Invisible Entropy (Gu et al., 2025) — already modulate watermark strength or detection weighting by entropy or local context. CATMARK’s main distinction (logit-based clustering and historical entropy quantiles) could be viewed as a more complex way to approximate entropy-based token grouping, and it is unclear whether this is significantly better.\n* Although Liu & Bu (2024) are cited, the discussion of how CATMARK avoids their “external estimation module” limitation is brief. The paper would benefit from a clearer justification of why maintaining per-cluster entropy histories is better than directly conditioning watermark strength on instantaneous entropy values.\n* Categorizing tokens via logit-vector similarity is presented as “semantic state clustering,” but in practice this may simply track entropy gradients (high vs low predictability). Empirical analysis of whether the clusters truly capture distinct semantic contexts (e.g., code vs prose) is lacking.\n* The related work section does not presently include several very similar adaptive or hybrid watermarking papers (see above). A deeper comparison of methodological overlap would improve the paper’s positioning."}, "questions": {"value": "Critical questions:\n* Could the authors provide empirical evidence that the logit-based clustering corresponds to meaningful context boundaries (e.g., code vs comment) rather than just high vs low entropy regions?\n* Why is historical entropy quantile computation superior to using the instantaneous token entropy (as in Guo et al.'s context-aware schemes from EMNLP 2024)? A direct comparison on the same benchmarks between CATMARK and a simpler entropy-adaptive delta baseline would clarify whether the added complexity of maintaining entropy histories and clusters yields meaningful benefits. \n\nOptional additions:\n* CATMARK combines three mechanisms — clustering by logits, quantile-based thresholding, and entropy-weighted detection — but the ablations don’t fully disentangle their contributions. Could the authors report results for each in isolation (e.g., quantile + no clustering; clustering + fixed threshold; etc.) to verify which component drives the gains?\n* Since the method relies on online clustering of logits, how well do the learned prototypes transfer when task boundaries shift (e.g., unseen code domains or mixed natural-language/code datasets)? A discussion of cross-dataset stability would strengthen the generalization claim.\n* Some reported gains are relatively small (<1pp pass@1). Did the authors perform significance testing or multiple-seed runs to ensure the improvements are robust and not due to stochasticity in sampling or clustering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6VSHfst3x8", "forum": "Q2DrdrPcYm", "replyto": "Q2DrdrPcYm", "signatures": ["ICLR.cc/2026/Conference/Submission9264/Reviewer_7zUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9264/Reviewer_7zUY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930119264, "cdate": 1761930119264, "tmdate": 1762920914444, "mdate": 1762920914444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CATMARK, a novel watermarking framework for large language models. It's designed to solve the problem of watermarking in cross-task scenarios, where LLMs switch between generating structured, low-entropy text like code and unstructured, high-entropy text like comments. The key idea is to dynamically cluster tokens based on their logits to detect the current semantic context. Then, CATMARK calculates a context-specific entropy threshold for each cluster. This method robustly watermarks high-entropy text while preserving the quality of low-entropy text. The experiments are strong, showing CATMARK achieves 82.3% pass@1 on HumanEval, matching the best baseline, while also getting a 97.0% AUROC. This effectively overcomes the typical trade-off between text quality and detection robustness."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses a very important and practical problem. Static watermarking thresholds fail when models mix different text types, and this work provides a robust solution for these real-world, heterogeneous generation tasks.\n    \n2. The core mechanism is genuinely innovative. Using on-the-fly logit clustering to create adaptive, context-aware thresholds is a smart way to bypass the limitations of a single, fixed threshold.\n    \n3. The empirical results are excellent. Table 1 shows that CATMARK achieves top-tier performance in both quality and detection. For instance, on StackEval, it scores a 2.72 average score and 97.5% acceptance rate, far exceeding all baselines, while also hitting a perfect 100% AUROC.\n    \n4. The theoretical analysis in Theorem 1 adds solid formal backing. It helps explain why adaptively filtering out low-entropy tokens provides a better detection z-score bound than methods that include them."}, "weaknesses": {"value": "1. The detection algorithm seems fundamentally misaligned with the generation process. The generator uses fine-grained, cluster-specific thresholds, but the detector reverts to a single, global threshold. It's unexplained how this mismatch can reliably detect the context-aware watermark.\n    \n2. I'm concerned about the practical stability of the method. The paper claims to be robust, but it doesn't provide a quantitative analysis of how sensitive the model is to its key hyperparameters, alpha and rho. This suggests the results might require careful tuning.\n    \n3. The method introduces new clustering and entropy tracking steps at each token. This seems computationally expensive, but the paper lacks any analysis of the latency overhead compared to baselines. This makes it difficult to assess its real-world efficiency."}, "questions": {"value": "1. Regarding the dynamic clustering, what mechanism prevents the number of clusters K from growing indefinitely? It seems a new cluster could be created for any slightly novel token.\n    \n2. I'm confused by the logic of the $f(x) = e^{-x}$ threshold function. It implies that a high-entropy cluster gets a stricter, lower threshold, which seems to mean _less_ watermarking. Isn't this the opposite of the stated goal?\n    \n3. Theorem 1 justifies excluding a single low-entropy token. How does this theoretical result support your specific choice of using the quantile-based $\\tau_k$ as the cutoff for _all_ tokens in that cluster? The connection isn't clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NjAoFSMOhb", "forum": "Q2DrdrPcYm", "replyto": "Q2DrdrPcYm", "signatures": ["ICLR.cc/2026/Conference/Submission9264/Reviewer_HYtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9264/Reviewer_HYtk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959435630, "cdate": 1761959435630, "tmdate": 1762920914060, "mdate": 1762920914060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CATMARK (Context-Aware Threshold Watermarking) — a dynamic framework that adaptively adjusts watermarking strength based on semantic context, enabling robust, cross-task watermarking for large language models without degrading text quality or requiring task-specific tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research problem is crucial, as not all generation modes correspond to high-entropy scenarios; effective watermarking must address the challenges of cross-task and mixed-task generation.\n2. Experimental results are clearly stated and visualized so that help to understand\n3. Figure 1 is clear and easy for readers to understand the workflow"}, "weaknesses": {"value": "1. It does not clearly cite a source or define how entropy high or low levels are quantitatively classified or thresholded.\n2. During generation, CATMARK depends on context-based clustering, yet the detection stage cannot reconstruct these clusters and instead relies on global entropy weighting—raising questions about consistency between embedding and detection.\n3. The experimental results heavily rely on the Qwen family models, and a broader evaluation across other LLM architectures would be necessary to validate generalization."}, "questions": {"value": "1. Can author provide more experimental results on more SOTA LLMs?\n2. Please state how to define the best hyperparameters for example, those for high and low entropies so that to enhance the reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bI8v1afFm7", "forum": "Q2DrdrPcYm", "replyto": "Q2DrdrPcYm", "signatures": ["ICLR.cc/2026/Conference/Submission9264/Reviewer_2SAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9264/Reviewer_2SAR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998143769, "cdate": 1761998143769, "tmdate": 1762920913696, "mdate": 1762920913696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}