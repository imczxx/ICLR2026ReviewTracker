{"id": "qK4JKOu0Gx", "number": 11334, "cdate": 1758196580449, "mdate": 1759897586764, "content": {"title": "Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models", "abstract": "Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems.\nHowever, recent studies reveal a sharp performance drop in *reasoning hop generalization* scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. \nThe internal mechanisms driving this failure remain poorly understood.\nIn this work, we conduct a systematic study tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. \nCloser inspection reveals that these token-level erroneous predictions stem from internal *competition mechanisms*: certain attention heads, termed *erroneous processing heads* (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. \nNotably, removing individual ep heads during inference can often restore the correct predictions.\nMotivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. \nExtensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.", "tldr": "This work sets out to explore the patterns and mechanisms for reasoning hop generalization errors, and secondly leverages these insights to develop strategies for correcting these errors.", "keywords": ["large language models", "chain-of-thought", "reasoning hop generalization", "mechanistic interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9536343b5b492f50a86f8755c3c6d2900e1eb582.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studied language model tasks that involve “reasoning hop generalization”. They localize errors in the reasoning chains of LMs by identifying predicted token positions which erroneously diverge from the expected token positions. The authors study the role of activation heads in correct vs. incorrect text generation trajectories. They identify a key set of attention heads which play a role in erroneous prediction via a knock-out mechanism. They find that there is substantial overlap between the critical attention heads for both erroneous and correct completion trajectories. They use this finding to motivate the design of test-time correction of reasoning (TRC). They show that applying TRC improves performance across many models across many benchmark tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method, TRC, does improve model performance on a wide range of tasks across a reasonable number of models (table 2).\n2. It is interesting to show that simply intervening on a trained model can substantially improve model performance. It indicates that models often have the knowledge needed to complete long-context tasks, but simply struggle to do so due to length of context. This validates well known phenomena of LMs struggling to be performant to varying context lengths.\n3. I felt the tables and figures were easy to understand.\n4. I appreciate the appendix, I found myself needing to reference it several times while reading this paper.\n5. Lines 176-178 are a good observation. Existing interpretability tools do struggle to be applied across vast token ranges.\n6. The procedure to determine when to intervene (lines 408-417) are both light-weight and reasonable."}, "weaknesses": {"value": "1. I feel like there is a conflation between what this paper dubs “reasoning hops generalization” and a commonly studied NLP task of: “multi-hop reasoning”. Multi-hop reasoning is a task which may require the successful completion of many intermediate reasoning steps to arrive at the final answer [1]. Chain-of-though is a prompting strategy that can be employed with any LM task (including and not limited to: multi-hop reasoning). You use the term “n-hop” problem on line 179, but this is almost identical terminology to the multi-hop reasoning literature. It's fine to use the term since it's defined, but, like me, I suspect it may confuse readers who are doing a quick skim between your problem setting and the canonical multi-hop setting.\n2. On lines 184-185 you use erroneous token-positions as proxies for identifying reasoning errors within the prompt completion. By this extension, in figure 2, is each reasoning hop number simply another erroneous token? If so, this is an overstatement of “reasoning hops” and rather should just be plainly labeled as erroneous token predictions. It makes more sense to predetermine all of the reasoning hops the model needs to make before identifying if there are errors at those positions; here you are using known errors to post-hoc identify reasoning hops? A related question: in figure 2, if you observe an error at a later reasoning hop number (e.g., later token position), was this after you corrected all previous errors at previous token positions, or is this a setting with cascading auto-regressive errors? If it is the autoregressive setting, this is not a surprising – models continue to make mistakes when fed erroneous prompts.\n3. In response to lines 49-52, it seems like you are missing some related work which has studied this issue [1].\n4. You discuss how you use the knockout mechanism to identify answer-writing heads in line 245-254, but it is not clear to me what the “answer” is here. Is it the expected-next-token immediately following the error token position you are analyzing? Is the answer the expected-next-token for the entire prompt? From formula 4, it looks like you are measuring the relative impact of an attention head on the vocabulary token associated with the erroneously predicted token t (shouldn’t you be measuring the impact on the correct/expected-next-token if you are looking to analyze the head’s relevance to the answer and not the error?\n5. From line 258-259 it is  not clear to me what is Scorr  vs Serr? What are the correction and erroneous prediction sets? Are these the erroneously predicted tokens and anticipated correct tokens? What about the token selections is “random”?\n6. You have a single competing baseline which you report (DoLa). DoLa is a decoding-based method, and your work is focused on activation interventions. I think your work could benefit from a direct comparison to LoFiT [2], or at least a discussion of what distinguishes both approaches.\n7. Typos: Line 17-18: Do you mean “...systematic study {of} tasks…”\n\n[1] Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. BlackboxNLP Workshop at EMNLP 2023.\n\n[2] LOFIT: Localized Fine-tuning on LLM Representations. NeurIPS, 2024."}, "questions": {"value": "1. On line 182 you formalize the n-hop problem, but in practice how do you decompose a CoT response into individual “hops”? Does a human have to go in and annotate all of the “hops”?\n2. Along the lines of the previous question, how do you identify errors within reasoning chains as per the error-types outlined in Appendix E.1? For example on line 300 you write for the 50-hop parity-NL tasks, ~79% of errors are type 2; how did you calculate this? Did a human go and read through all of the reasoning chains for the 50-hop parity tasks and label errors?\n3. Suggestion for clarity: You use an example to detail the errors you track for all tasks in Appendix E.1, but I would prefer a clear table that outlines the names of all of the tasks and the names of all of the corresponding error types in addition to the examples you provide.\n4. In section 5, I am always left wondering if the premise of training a model (e.g., knockout classifier model) to intervene on a model (e.g., language model) is maybe a bit convoluted. Why not just train your language model further on the same dataset you used to train the knock-out model? How would the results compare on the same test dataset? Upon reading further it looks like your knockout classifier model is  Qwen2.5-0.5B w/ a classifier head…since this model is the same size as the models you are intervening on (E.g., table 1 models are all roughly 7-8B parameter counts), then it is not like you are saving compute cost by training a smaller classifier model; it is about the same size as the base model itself. \n5. I appreciate that once the classifier is trained, the test-time intervention (TRC) is computationally efficient and light-weight. What is the cost of training the classifier as opposed to just training the base model itself?\n\n[1] Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models. BlackboxNLP Workshop at EMNLP 2023.\n\n[2] LOFIT: Localized Fine-tuning on LLM Representations. NeurIPS, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pjc2cK8YFB", "forum": "qK4JKOu0Gx", "replyto": "qK4JKOu0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission11334/Reviewer_C6Fu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11334/Reviewer_C6Fu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761174905186, "cdate": 1761174905186, "tmdate": 1762922470544, "mdate": 1762922470544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why LLMs fail to generalize in multi-hop reasoning tasks, finding that errors are concentrated in the several error types and dominated by few attention heads. Using residual stream analysis and attention head interventions, the authors uncover a competition mechanism between correct and erroneous reasoning signals within the model. Based on these insights, they propose Test-time Correction of Reasoning (TCR), a lightweight intervention that dynamically deactivates erroneous heads to significantly improve hop generalization performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I think overall this is a good paper; it not only provides a mechanistic analysis but also proposes an improved algorithm based on these insights.\n\n2. The paper is presented in a clear and logical manner, making it easy to follow and understand."}, "weaknesses": {"value": "1. Although the TCR method effectively improves performance, considering that it requires prior attention-head selection and training an additional small model, I am unsure whether this workflow would be computationally acceptable in practice.\n\n2. I think reporting additional metrics on the consistency of model outputs after applying TCR would strengthen the paper. For example, showing whether the predictive entropy is effectively reduced could further support the method’s effectiveness.\n\n3. I noticed that Equation (4) uses normalization to represent relative change trends. One concern is that the final averaged results may be overly influenced by cases where the denominator is small. For example, if the change $f(h) - f(h-a) = 0.2$, the resulting $s(a)$ will be 100% if $f(h) = 0.2$ and only 0.25% if $f(h) = 0.8$. I am unsure whether removing this normalization would affect the final head localization. Reporting the change $f(h) - f(a)$ or the distributions of $f(h)$ and $f(h) - f(a)$ would be somewhat helpful.\n \n4. In locating the answer-writing heads, the paper uses the Logit Lens interpretability tool (mentioned in Section 2). I understand that this is a common approach in interpretability studies. However, different layers and heads in the model may operate in entirely different semantic spaces, which do not necessarily align with the semantic space of the final unembedding matrix. Therefore, this mapping may not fully carry the intended meaning. I think a discussion of the limitations and some justification for its use would be helpful.\n\n5. While the logic of the content is clear, Section 4 is very dense in text. It might be helpful to simplify it or break it into smaller subsections, making it easier for readers to follow."}, "questions": {"value": "see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RgZ2kQZgZD", "forum": "qK4JKOu0Gx", "replyto": "qK4JKOu0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission11334/Reviewer_VfVH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11334/Reviewer_VfVH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728960143, "cdate": 1761728960143, "tmdate": 1762922469884, "mdate": 1762922469884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recent studies reveal a sharp performance drop in reasoning hop generalization scenarios. To understand these failures, the authors first investigate two research questions: 1) Where do errors occur? 2) Why do these errors arise? They found that these failures concentrate on token positions of a few key error types, and the error trajectories can suppress the correct ones. To address this issue, the authors proposed Test-time Correction of Reasoning (TCR), which trains a classifier to dynamically identify and deactivate the erroneous heads during inference. Experimental results prove the effectiveness of the proposed method, TCR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper investigates the reasoning hop generalization problem,  and provides valuable insights to understand where and why the problem happens. The findings may contribute to the recent reasoning research community.\n2. Based on their findings, the authors propose an effective solution."}, "weaknesses": {"value": "1. The proposed method needsto  train a classifier for tasks. Although the results achieve some ood performance. However, there are still marginal gaps between id and ood. \n2. Lack experiments on larger models, like 13B, 32B. Only 7B is not convincing enough."}, "questions": {"value": "1. The classifier is an interface to LLMs. Can we directly improve the LLM's ability by teaching it to stop searching the error trajectories?\n2. See weakness 2. How does the method perform on large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GH4cSc8wtx", "forum": "qK4JKOu0Gx", "replyto": "qK4JKOu0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission11334/Reviewer_iLuH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11334/Reviewer_iLuH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957808495, "cdate": 1761957808495, "tmdate": 1762922469085, "mdate": 1762922469085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies reasoning-hop generalization in LLMs. It decomposes chain-of-thought into hop-level positions, defines task-specific error types, and then uses logit-lens / knockout to identify answer-writing heads and processing heads. The core claim is that correct and erroneous trajectories coexist internally, and hop failure largely comes from a small set of erroneous processing heads dominating late layers. A test-time head-selection / knock-out procedure improves accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The angle is genuinely new. Instead of saying “model can’t reason”, it argues that correct reasoning traces often coexist but get overshadowed by competing erroneous heads. This produces a novel mechanistic framing of hop failure.\n- The paper is extremely well-structured. The narrative from “error type → internal mechanism → test-time correction” is clean and easy to follow."}, "weaknesses": {"value": "**W1.** “error types” are human-defined task-specific taxonomies (Appendix E.1), not emergent internal state clusters of the model, so the mapping from annotation layer → mechanism layer is not guaranteed.\n\n**W2.** Evaluation covers only synthetic / programmatic hop-style tasks. No natural-language reasoning benchmarks (e.g., GSM8K, MATH, AIME, Leetcode).\n\n**W3** No experiments on reasoning-style models (RL-based, verifier-based, or scratchpad-policy optimized models), whose non-linear reasoning chain dynamics are known to differ substantially from vanilla SFT LLMs. So universality claims are untested."}, "questions": {"value": "**Q1.Error-type identification in real-world NL reasoning.**\n\nIn this work, the key step for mechanistic probing is that “critical error types” are first localized (Appendix E.1) and then those token positions are used as entry points for head attribution. For general NL benchmarks, error boundaries are not syntactically aligned to deterministic step indices, and model outputs are free-form.\n\nHow do you envision the error-type mining procedure to scale to such settings? Would this require a learned error tagger? a verifier? or a retrieval oracle to define step boundaries?\n\n**Q2. Applicability to non-linear reasoning policies (with backtracking / exploring).**\n\nYour entire head-tracing argument implicitly assumes a linear monotonic accumulation of residual signal along a single chain (hop 1 → hop 2 → hop 3 …).\nRL-style models perform backtracking, branch scoring, and dynamic re-evaluation of earlier states.\n\nDo you believe the cp vs ep competition in late layers is still a meaningful concept in a policy that does dynamic branch selection?\nIf yes, is the right object to knock out still an “attention head” or does the unit become a search operator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "94NISa4eaC", "forum": "qK4JKOu0Gx", "replyto": "qK4JKOu0Gx", "signatures": ["ICLR.cc/2026/Conference/Submission11334/Reviewer_ZZNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11334/Reviewer_ZZNy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980570116, "cdate": 1761980570116, "tmdate": 1762922468544, "mdate": 1762922468544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}