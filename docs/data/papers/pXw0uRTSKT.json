{"id": "pXw0uRTSKT", "number": 21835, "cdate": 1758322434969, "mdate": 1759896900812, "content": {"title": "Can we generate portable representations for clinical time series data using LLMs?", "abstract": "Deploying clinical ML is slow and brittle: models that work at one hospital often degrade under distribution shifts at the next. In this work, we study a simple question -- can large language models (LLMs) create portable patient embeddings i.e. representations of patients enable a downstream predictor built on one hospital to be elsewhere with minimal-to-no retraining and fine-tuning.\nTo do so, we map from irregular ICU time series onto concise natural language summaries using a frozen LLM, then embed each summary with a frozen text embedding model to obtain a fixed length vector capable of serving as input to a variety of downstream predictors.\nAcross three cohorts (MIMIC-IV, HIRID, PPICU), on multiple clinically grounded forecasting and classification tasks, we find that our approach is simple, easy to use and surprisingly competitive with in-distribution with grid imputation, self-supervised representation learning, and time series foundation models, while exhibiting smaller relative performance drops when transferring to new hospitals.\nWe study the variation in performance across prompt design, with structured prompts being crucial to reducing the variance of the predictive models without altering mean accuracy. We find that using these portable representations improves few-shot learning and does not increase demographic recoverability of age or sex relative to baselines, suggesting little additional privacy risk.\nOur work points to the potential that LLMs hold as tools to enable the scalable deployment of production grade predictive models by reducing the engineering overhead.", "tldr": "Explore the ability of LLMs to generate portable and transferrable representations for ICU time-series", "keywords": ["Machine Learning for Healthcare", "ICU Time-series", "LLMs", "Representation Learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/903cbbdfc1b98442e1f8867c920f129f25f0da10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper asks a crisp, deployment-relevant question: Can frozen large language models (LLMs) produce portable patient representations from irregular ICU time series that permit downstream predictors trained on one hospital to generalize to another with minimal re-training?\n\nTo address this, the authors propose Record2Vec, a novel \"summarize-then-embed\" pipeline. This approach first maps a 48-hour irregular patient record to a concise natural-language summary by using a frozen LLM. This summary is then converted into a fixed-length vector using a frozen text embedder. The authors evaluate Record2Vec across three distinct cohorts (MIMIC-IV, HiRID, and PPICU) and seven clinical tasks (including forecasting, length of stay (LOS), mortality, and predictions of treatment or lab values). The method is compared against multiple established baselines, including classical grid imputation, a time-series diffusion embedding (TSDE), and a time-series foundation model (TimesFM). Record2Vec is reported to be competitive with these baselines for in-distribution performance. Crucially, the approach demonstrates substantially greater robustness under cross-site transfer. Furthermore, the authors show that it is highly data-efficient in few-shot settings and, importantly, does not increase demographic recoverability relative to the baselines. The full set of empirical claims, the experimental setup, and the detailed results are documented in the submitted draft."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- treat language as a harmonizing interface between heterogeneous EHR encodings and generic predictors. Though some other works have described similar trends. Please cite (https://www.nature.com/articles/s41746-025-01777-x). Also cite MEDS which is trying to build transferable data schema in tabular domain (https://openreview.net/forum?id=IsHy2ebjIG) \n- The multi-site empirical evaluation and transfer (public and private cohorts) is a major asset so the cross-hospital experiments carry real weight. \n- The authors also do a good job exploring multiple LLM variants, prompt styles, and downstream tasks; the finding that summaries reduce token costs by ~25× (page 6) while improving transfer is practically important. \n- Methodologically, the pipeline is simple, engineering-light, and compatible with frozen models, which is attractive for real systems where re-training large models is infeasible. \n- The inclusion of privacy probes and sanity checks about demographic leakage shows the authors are thinking beyond pure accuracy"}, "weaknesses": {"value": "- A number of ablations and diagnostics could strenghten the work that would make the story rigorous rather than suggestive. The first is a controlled ablation separating the effects of (a) canonicalization of names/units, (b) summarization (compression of time series), and (c) the choice of text embedder. Without this, one cannot conclude whether “language” per se is necessary, or whether a structured canonicalizer plus a shallow encoder would suffice.\n\n- the privacy evaluation is narrowly scoped to demographic inference for age/sex. This is insufficient to claim \"no additional privacy risk.\" Embedding inversion, membership inference against the summarizer+embedder pipeline, and probing for rare/highly identifying events (e.g., unusual lab values, timestamps, or free-text tokens) are missing. Additionally, the LLM summarizer itself could hallucinate or introduce artifacts; there is no human evaluation of summary faithfulness to the numeric inputs, nor an automated fidelity metric (e.g., check that numeric extrema referenced in summaries match the underlying series).\n\n- operational costs and latency are discussed qualitatively but lack quantitative benchmarks. Reporting inference latency (ms per window), token counts, embedding dimensionality, and cost per 1k patients would give readers a concrete sense of real-world viability.\n\n- figures can be of higher quality."}, "questions": {"value": "- How much of Record2Vec’s transfer advantage remains if you replace the LLM summarizer with a deterministic template based approach that normalizes variable names, units, and missingness and emits a short structured template? This would clarify whether LLM semantic abstraction is essential or merely convenient and be a direct comparison with previous literature (https://www.nature.com/articles/s41746-025-01777-x)\n- Why is TSDE excluded from few-shot adaptation? Could you adapt TimesFM and TSDE with the same small labeled target budgets (or simulate a light adaptation procedure) to make the few-shot comparison symmetric?\n- Can you report concrete latency and cost numbers (tokens, GPU time, ms per example) for Gemini-2.0, MedGemma, Llama-3.1 summarizers and for Qwen3 embeddings, so practitioners can weigh the tradeoffs?\n- Have you looked at worst-group performance (e.g., by race/ethnicity, rare diagnoses, or admission types)? The aggregate AUROC can mask severe degradation for clinically important subpopulations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dey3BIvP2L", "forum": "pXw0uRTSKT", "replyto": "pXw0uRTSKT", "signatures": ["ICLR.cc/2026/Conference/Submission21835/Reviewer_rGkM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21835/Reviewer_rGkM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510994886, "cdate": 1761510994886, "tmdate": 1762941949539, "mdate": 1762941949539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful, detailed, and constructive feedback on our work.\n\nSeveral reviewers highlighted that our paper addresses a practically important problem of cross-hospital portability in clinical ML, that the proposed summarize-then-embed Record2Vec pipeline is a simple yet novel and deployment-oriented use of frozen LLMs, and that our experimental design is comprehensive, with multi-site evaluation across three ICU cohorts, seven tasks, systematic ablations, few-shot adaptation studies, and privacy/representation diagnostics.\nThe reviewers also emphasized our empirical findings that summarization improves transfer while substantially reducing token and computational costs.\n\nOverall, our contribution is to show that a frozen summarize-then-embed pipeline can already produce portable ICU time-series representations that are competitive in-distribution, markedly more robust under cross-site transfer, and effective in realistic few-shot settings, without increasing demographic leakage relative to strong non-language baselines.\n\nIn this rebuttal, we have clarified our positioning relative to recent LLM for EHR work, refined our claims around distribution shift, privacy, and added experiments on baselines, ablations, and operational cost estimates.\n\n#### Clarifications Summary\n\n- **Significance of transferability.** While utility is critical for clinicians, many sites lack large, long-term datasets like HiRID, MIMIC, and PPICU. Models trained only on small local cohorts may have even lower utility. To illustrate this, we replicate baseline methods using 1,000 PPICU samples and compare them with our 16-sample finetuning results.\n\n- **Distribution shift and explanation of gains.** We explicitly identify three sources of distribution shift and detail how our method addresses each. We also show that improvements are not due to simple format matching by comparing against a no-summarization, fixed-template variant, where our method still achieves substantially larger gains.\n\n- **Role of LLM summarization and experimental setup.** The features used for LLM summarization are not pre-selected for the LLM; they arise from standard ICU preprocessing pipelines developed with inputs from clinical specialists. These procedures are common to ICU studies, not unique to ours.\n\n- **Classifier choice.** In addition to PatchTSMixer, we include other strong time-series classifiers (MLP, LSTM, TimeMixer). Their results, reported in the appendix, show that our conclusions are not tied to a single downstream model.\n\n- **Finetuning sample selection.** All finetuning samples are randomly selected and shared across methods to ensure a fair and controlled comparison.\n\n---\n\n#### Experiments Summary\n\n- Replicate GenHPF with minor adaptations to better fit ICU datasets and quantify the additional performance contributed by our method.\n- Replace the original mortality models with strong mortality baselines, demonstrating both improved utility and portability.\n- Conduct embedder ablations over pooling strategies (mean, CLS, last) and normalization schemes (L2, none), and compare against one additional SOTA and one non-SOTA embedding model.\n- Finetune TSDE under our transfer setting to assess compatibility with our framework.\n- Report latency and cost statistics to contextualize operational feasibility."}}, "id": "sRnQ39jSek", "forum": "pXw0uRTSKT", "replyto": "pXw0uRTSKT", "signatures": ["ICLR.cc/2026/Conference/Submission21835/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21835/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21835/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763706659855, "cdate": 1763706659855, "tmdate": 1763706659855, "mdate": 1763706659855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Record2Vec, a novel \"summarize-then-embed\" pipeline designed to create portable representations of clinical time-series data from the Intensive Care Unit (ICU). The central claim is that by first using a frozen Large Language Model (LLM) to generate a natural language summary of serialized patient data, and then embedding this summary into a fixed-length vector, the resulting representation enables downstream predictive models to generalize across different hospital systems with minimal retraining. The authors evaluate this approach on three distinct ICU cohorts (MIMIC-IV, HiRID, and PPICU) across a range of forecasting and classification tasks. They demonstrate that their method achieves competitive in-distribution performance while significantly outperforming baselines in cross-site transfer and few-shot learning settings, without introducing additional privacy risks related to demographic recoverability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Important and Practical Problem: The paper tackles the problem of model portability in clinical machine learning. The degradation of model performance across different hospital systems is a major barrier to the real-world deployment of AI in healthcare.\n\nNovel \"Summarize-then-Embed\" Architecture: The main contribution is the two-stage process that uses an LLM for summarization as an intermediate step. The paper provides compelling evidence (Figure 3) that this summarization step is crucial for achieving portability, acting as an intelligent compression and harmonization layer that abstracts away site-specific noise. \n\nRigorous Evaluation of Portability: The experimental design is strong, with a clear focus on testing cross-site transfer as the primary outcome."}, "weaknesses": {"value": "The primary weakness of the submission is its failure to properly contextualize itself within the rapidly evolving literature on using LLMs for structured EHR data. Several highly relevant, recent works (see below) that explore similar \"textualization\" and embedding-based approaches are not cited or discussed. This significantly overstates the novelty of the paper's general premise and misses a critical opportunity to highlight its more specific, architectural contribution. This prior works also also demonstrates cross-site generalization with a direct embedding approach, which directly challenges some of the implicit claims in this paper. By not engaging with several recent papers, the authors overstate the novelty of their core idea and fail to properly frame their specific contributions. A substantial revision of the manuscript is necessary to accurately position this work in the current landscape. \n\nThe paper's core narrative sometimes feels repetitive, and the most novel and important findings, specifically, the crucial role of the summarization step for portability and efficiency, can get buried. Additionally, several figures in the submission (e.g., Figures 3 and 4) are low-resolution.\n\nSimon A. Lee, Sujay Jain, Alex Chen, Kyoka Ono, Arabdha Biswas, Akos Rudas, Jennifer Fang, and Jeffrey N. Chiang. \"Clinical decision support using pseudo-notes from multiple streams of EHR data\". npj Digital Medicine, 8(1):394, 2025.\n\nStefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, and Benjamin Wild. \"Large Language Models are Powerful EHR Encoders\". arXiv preprint arXiv:2502.17403, 2025.\n\nYanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A. Miller, Danielle Bitterman, Matthew Churpek, and Majid Afshar. \"When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?\". In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 5414–5428, 2024.\n\nContreras, M., Kapoor, S., Zhang, J., Davidson, A., Ren, Y., Guan, Z., Ozrazgat-Baslanti, T., Nerella, S., Bihorac, A., Rashidi, P.: DeLLiriuM: A large language model for delirium prediction in the ICU using structured EHR. arXiv. arXiv:2410.17363 [cs] (2024).\n\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. \"TabLLM: Few-shot Classification of Tabular Data with Large Language Models\". In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR 206:5549-5581, 2023."}, "questions": {"value": "Recent work, such as Hegselmann et al. (2025), has shown that a direct \"serialize-then-embed\" approach can also achieve strong generalization across different cohorts using off-the-shelf embedding models. Your \"no-summary\" baseline, however, shows poor transfer performance. Could you please discuss this discrepancy? \n\n--- \n\nIn general I think this is a well motivated and nicely executed work, but I'm not sure how much novelty is left once other recent works are properly taken into account. Can the authors please clearly state how their work goes beyond the current state of the art?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UZOnBrKfbY", "forum": "pXw0uRTSKT", "replyto": "pXw0uRTSKT", "signatures": ["ICLR.cc/2026/Conference/Submission21835/Reviewer_xPpy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21835/Reviewer_xPpy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564551820, "cdate": 1761564551820, "tmdate": 1762941949093, "mdate": 1762941949093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Record2Vec a pipeline that first asks an LLM to write a brief handoff of an ICU patient’s irregular time‑series record and then feeds that text to a text embedder to obtain a fixed‑length vector for standard predictors. Across three ICU cohorts and seven tasks, the authors report competitive in‑distribution results, improved cross‑site transfer, better few‑shot adaptation, and no increase in demographic leakage compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, deployment‑motivated idea summarize irregular numeric streams into natural language, then embed once to standardize inputs across hospitals, figure 2 nicely explains the study.\n- Extensive benchmarking across three datasets and seven various outcomes, with record2vec winning most in‑distribution tasks and most cross‑site transfer columns.\n- Systematic ablations on summarizer choice and prompt design, good findings: summarization generally boosts portability and structured prompts modestly reduce variance.\n- Few‑shot study shows that with only 16 labeled target examples, finetuning on record2vec closes much of the cross‑site gap for mortality. \n- Privacy and representation diagnostics are included: demographic recoverability stays comparable or lower than baselines, and UMAP/silhouette analyses provide an embedding‑quality proxy."}, "weaknesses": {"value": "-The paper does not quantify information loss end‑to‑end or analyze the embedding geometry needed for downstream tasks, proxies like silhouette and task‑relative gains are informative but do not measure retained mutual information or per‑feature fidelity, and both the summarizer and embedder remain frozen rather than being optimized for downstream geometry. This makes it unclear whether key clinical signals are systematically discarded or distorted during the summarize-embed pipeline.\n\n- Absolute performance on mortality is mixed: in‑distribution AUROC reaches about 0.90 on HiRID but is lower on MIMIC and PPICU, and cross‑site mortality AUROC is ~0.72, which may be below what many deployments would select.\n\n- The embedder is used off‑the‑shelf and frozen, the paper does not test task‑aware metric learning, embedder finetuning, or geometry‑regularized objectives that could improve linear probe performance and calibration."}, "questions": {"value": "- Please position results against task-specific SOTA (e.g., strong ICU mortality baselines) so readers can judge absolute utility, not only portability. You do include mortality among your tasks; a direct SOTA comparison table would help. \n\n\n- How sensitive are outcomes to the choice and dimensionality of the frozen text embedder? You mainly note Qwen3, an embedder ablation (dim, normalization, pooling) would clarify it. \n\n\n- The normalization choice differs across baselines (grids normalized with train-split stats, language keeps raw magnitudes/units). Can you justify fairness here or add a control where text summaries are unit-standardized too? \n\n\n- Figures 3–4 rely on “rank distributions.” Please report the exact metric used to compute ranks in the captions and include a small table of corresponding absolute numbers per method/prompt.   \n\n\n- Few-shot: what was the selection protocol for the 16 examples (random vs stratified), and how variable are results across seeds? A CI/SD per setting would help.\n\n\nMinor:\n- In line 21, “surprisingly competitive with in-distribution with grid imputation …” consider “competitive in-distribution with grid imputation …”. \n\n\n- In line 1088 the sentence is slightly off: “All clinical features are selected in discussion with real clinician have prominent experience in ICU units.\n\n\n- In Figure 6 caption, “Age results have less than 0.5% gap and can be inferred from appendix 4”, should be table 4? \n\n\n- Figure captions especially figures 4 and 5 could explicitly state the evaluation metric used for ranking/few-shot plots, right now the captions don’t name a metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WjuEHbqIYY", "forum": "pXw0uRTSKT", "replyto": "pXw0uRTSKT", "signatures": ["ICLR.cc/2026/Conference/Submission21835/Reviewer_xPBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21835/Reviewer_xPBC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706812955, "cdate": 1761706812955, "tmdate": 1762941948863, "mdate": 1762941948863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for transfer learning clinical machine learning models to different hospitals without a harmonization process, addressing the problem of data format differences that arise during deployment. The authors designed the 'Record2Vec', which utilizes LLMs to generate 'portable' patient representations. This methodology transforms irregular ICU time-series data into a natural language summary (a clinical handoff-style summary) using LLM, and then extracts text embeddings from this summary to be used for training predictive models.\n\nThe authors claim that this representation vector overcomes the heterogeneity of data formats between hospitals, allowing a downstream predictive model trained at one hospital to operate at another with minimal-to-no retraining. Experiments conducted on multiple prediction tasks across three ICU cohorts (MIMIC-IV, HIRID, PPICU) showed that the proposed method achieves in-distribution performance comparable to existing methods, while exhibiting less performance degradation in cross-site transfer and greater efficiency in few-shot learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) Well motivaed and novel approach: It is timely and important that the paper directly addresses the practical bottleneck of inter-hospital portability in clinical ML model deployment. Unlike traditional model-centric domain adaptation or schema standardization (OMOP/FHIR), the approach of using an LLM as an 'information transformation layer' to generate semantically aligned input representations is novel and interesting.\n\n(2) Comprehensive Experimental Design: The authors attempted to validate the core claim (portability) of the proposed methodology in various scenarios, including In-distribution, cross-site transfer, and few-shot learning, using three different ICU cohorts.\n\n(3) Practicality and Privacy Considerations: The 'Record2Vec' pipeline is conceptually simple. Furthermore, the evaluation of privacy risks—specifically, the recoverability of sensitive information (age, sex) from the embeddings—and the finding that it does not introduce additional risk is a commendable aspect for a study utilizing clinical data."}, "weaknesses": {"value": "1. Inadequate Baseline Comparison: To demonstrate the superiority of the proposed representation, the paper compared the performance of various representation methods by feeding them into the same downstream classifier (PatchTSMixer). However, this approach omits comparisons with key baselines in the field of EHR prediction.\n(a) Traditional EHR prediction models that directly utilize medical code sequences, such as BEHRT (Li et al., 2020) or MedBERT (Ramsy et al., 2021).\n(b) Models like GenHPF (Hur et al., 2023), which are trained end-to-end by converting EHR time-series inputs into text.\n\nA direct performance comparison with these end-to-end models is necessary. Merely comparing input representations while keeping the PatchTSMixer classifier head fixed is insufficient to demonstrate the practical performance advantages of the proposed method.\n\n2. Practicality of Transfer Learning: In real clinical settings, hospitals will likely prioritize achieving the highest accuracy by training models optimized for their own in-distribution datasets, even if it requires significant cost and time. For the portability offered by the proposed transfer learning approach to be meaningful, the performance of the model transferred from a source to a target (transfer learning performance) must be superior to, or at least comparable to, the performance of a model trained directly on the target hospital's data (in-distribution performance). The current results do not provide a clear incentive for hospitals to choose portability at the expense of accuracy.\n\n3. Overstatement on Resolving Distribution Shift: While it is true that LLM summarization standardizes heterogeneous data formats into a common text input, the claim that this resolves the 'core' issue of cross-site distribution shift is an overstatement. The primary cause of distribution shift is not merely that the same patient data is recorded in different formats, but that the patient populations themselves are fundamentally different between hospitals (e.g., differences in severity, prevalence rates). Generating summaries can alleviate input format differences to some extent, but it seems that author overclaimed it.\n\n4. Unclear Role of LLM Summarization and Limitations of Experimental Setup: The purpose of LLM summarization in this study is ambiguous. While reducing the input length is a general advantage of converting time-series data to text, this experiment was conducted using 'pre-selected' common features (60-75) across datasets. This presupposes that the key shared variables between datasets are already aligned. It is questionable whether performing this pre-alignment step before summarization is an appropriate experimental setup to validate the purpose of LLM summarization (without aligning process EHRs).\n\nIf the role of the LLM is 'generating a standardized format,' a comparison should have been made with recent studies that use LLMs to convert EHRs to a standard data model (CDM) like OMOP and then apply models such as BEHRT (e.g., Adams et al., 2025)."}, "questions": {"value": "1. Lack of Explanation for Performance Improvement: There is an insufficient explanation of 'how' standardizing the input format into a unified summary leads to performance benefits in prediction. An analysis is needed on how clinically important information is better preserved or extracted during the summarization process—beyond simply matching formats—and how this positively impacts downstream tasks.\n\n2. Lack of Justification for Classifier Choice: The paper does not provide a reason or justification for choosing PatchTSMixer as the downstream predictive model for comparing the different representations. An explanation is needed as to whether this model is suitable for the task, or if the same conclusions would hold if a different classifier were used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LzK4FpT9cs", "forum": "pXw0uRTSKT", "replyto": "pXw0uRTSKT", "signatures": ["ICLR.cc/2026/Conference/Submission21835/Reviewer_t4Cv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21835/Reviewer_t4Cv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974846718, "cdate": 1761974846718, "tmdate": 1762941948582, "mdate": 1762941948582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}