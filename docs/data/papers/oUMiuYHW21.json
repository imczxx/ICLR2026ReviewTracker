{"id": "oUMiuYHW21", "number": 5468, "cdate": 1757912820251, "mdate": 1759897972698, "content": {"title": "Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning", "abstract": "Foundation models pretrained on various and unlabeled data have demonstrated significant success in natural language and vision, but their application to electroencephalography (EEG) remains challenged due to the signal's unique properties. Existing brain foundation models that inherit architectures designed for text or images lead to three limitations in pre-training: 1) conflating time-domain waveform patterns with frequency-domain rhythmic features in a single processing stream, 2) ignoring the critical spatial topology of  electrodes with different standards, and 3) reliance on the inflexible, dense network to process functionally distinct EEG patterns. To address these challenges, we introduce the \\textbf{Unified Neural Topological Foundation Model (Uni-NTFM)}, which is designed based on neuroscience principles to produce universal and interpretable representations. Uni-NTFM integrates three core innovations: 1) A decoupled architecture parallelly encodes time, frequency, and raw signal representations before performing cross-domain feature integration; 2) A topological embedding mechanism to unify electrodes from different international standards and generate structured input sequences for brain regions; and 3) a  Mixture-of-Experts neural Transformer that efficiently scales model capacity by routing signal patterns to specialized subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B parameters and was pretrained on over 28,000 hours of diverse EEG data via a dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms existing task-specific methods and foundation models across nine distinct downstream tasks under both linear probing and fine-tuning settings, demonstrating a superior ability to learn universal representations of brain activity. Our code is available at \\url{https://anonymous.4open.science/r/Uni-NTFM-0924}.", "tldr": "", "keywords": ["Brain Computer interface", "Foundation Model", "Electroencephalography"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3a1634e2c463f2cd66a19c00e6883d5caff0013.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose UNI-NTFM which consists of double branch encoding in both frequency- and time-domain, which then merge again with the embedded raw data stream for computing correspondences across natural and spectral features. This is further enhanced by topological embedding where brain region level variations are explicitly modeled. The final MOE / RoPE augmented Transformer performs token mixing. These architectural contributions are evaluated across several representative EEG databases, against both task-specific and foundation baselines, revealing competitive performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive experiments with confidence level clearly shown.\n- Well motivated set of modules used as the backbone, proved to be quite effective.\n- Beautiful figures\n- Complete ablation studies and comparisons."}, "weaknesses": {"value": "- Limited novelty: joint frequency- and time-domain encoding is not new. MoE and RoPE are not new in the EEG foundation model domain either. Topological embedding adds novelty, yet with this alone, model side contribution might be weak.\n\n- Limited performance gains: performance difference across baselines and within model size variations is statistically not significant in most cases. I'd suggest to tone down the significance statement in the introduction and conclusion.\n\n- Model-size scalability is limited. The reason for this in unclear."}, "questions": {"value": "- I like the topological embedding idea. However, analyses and the written information on this invention are somewhat limited. What are the brain area / region that you considered? Depending on the disease type and its characteristic functional connectivity the embedding might vary a lot. Have you tried to differentiate this?\n\n- I appreciate the confidence intervals that show clearly the gain from the proposed method. However, most results don't seem to be statistically significant compared to second best, on both baseline comparison and ablations. How is the confidence computed? Perhaps it's in the paper but I cannot identify it. Can you show the P-values, for instance, the best performance against the second best performance? If this is not significant under like 95% confidence interval, then the authors may have to tone down the performance gain.\n\n- On a similar line, statistical significant tests within different model sizes might reveal that they are not very different. Would this reveal a model / data side scalability?\n\n- Did you use the MU transfer? How do you know the convergence in model size scalability at the billion parameter regime is due to limited data, or suboptimal hyperparameter tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ABRPIikEu4", "forum": "oUMiuYHW21", "replyto": "oUMiuYHW21", "signatures": ["ICLR.cc/2026/Conference/Submission5468/Reviewer_HBFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5468/Reviewer_HBFG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789879972, "cdate": 1761789879972, "tmdate": 1762918080672, "mdate": 1762918080672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Uni-NTFM, which is a neuro-inspired foundation model on EEG brain signal data, which separates and then fuses time-signal, frequency-signal and raw-signal views, and then injects explicit spatial topology, which makes the electrodes from different montages become unified and then tries to scale capacity with a MoE Transformer. Uni-NTFM is a pretrained model with a dual-domain masked reconstruction objective, which is the combination of reconstructing time and frequency targets on around 28k hours of EEG signals. Across 9 downstream datasets, this model beats task-specific models and prior EEG foundation models under both probing and fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I found the originality of this work very good, which rethinks Foundation models on EEG around the physics of the signal and tries to decouple time, frequency and raw views cleanly, and they're also explicitly unifying electrode montages with a hierarchical topological embedding, and also scaling with a sparsely-activated MoE, instead of importing a single-stream and dense transformer. \nAs to the quality, I found the proposed method interesting and strong, as the pretraining corpus is large and heterogeneous, and the evaluation spans 9 downstream tasks with both linear probing and fine-tuning. \nThe clarity of the work is also easy to follow, and the significance is notable."}, "weaknesses": {"value": "While I found the proposed method strong and interesting, I'd like to raise some weaknesses that came to my mind after reading the work. From my understanding, the authors already claimed the montage unification, but I don't see any direct test on unseen layouts or on systematically missing channels, so at this point, I'd suggest considering a small cross-montage transfer and k-missing-channel stress test, which could make the proposed topology more concrete. \n\n---\nAnother point that I'd like to make is about the Mixture of Experts (MoE), which I think is well-motivated, but from my understanding, the work does not show that different experts specialize on distinct EEG patterns, so I'd suggest considering some diagnostics, such as gate distributions and entropy by class, per-expert token-routing heatmaps, or maybe an ablation that freezes the most-used experts, which will help in demonstrating real specialization and justifying your MoE claim. \n\n---\nOne additional thing that I'd like to raise is about the scaling part, which I found super interesting, and the authors have done a great job on it. I just suggest making it more explicit by showing how accuracy changes as size grows, normalized by compute and/or data, and try to report training cost and inference speed. \n\n---\nOne last thing that I'd like to raise. I see that the pretraining mixes 9 EEG datasets, then the authors evaluate on datasets from the same families. From my understanding, this can blur whether gains come from true cross-corpus generalization or from seeing very similar data. I'd suggest considering leave-one-corpus-out, somehow similar to leave-one-out cross-validation (LOOCV), but I don't mean LOOCV at this point; I mean removing one family during pretraining and then testing on e.g., TUAB to prove the robustness to unseen corpora, which could rule out easy distribution matches."}, "questions": {"value": "I'd suggest that the authors engage with the points that I raised in the Weaknesses section. As to the scaling part, I'd suggest downstream accuracy vs pretraining hours (e.g., 10%, 25%, 50%, 100%) plots and vs labeled fraction used in fine-tuning (1%, 5%, 10%, 100%) on a few tasks, so there would be a possibility to see if benefits come from data, model size or both and how label-efficient Uni-NTFM really is. \n\n---\nI have another question and would be curious to know the authors' comments. Do you really need 3 separate streams, or would a single well-designed stream be enough? I was thinking maybe adding an equal-compute baseline that feeds stacked Short-Time Fourier Transform (STFT) patches (magnitude + phase) into one Transformer with MoE, plus early-fusion and late-fusion variants, and compare head-to-head with Heterogeneous Feature Projection Module (HFPM) + Dual-domain Cross-attention Module (DCM). Also, I'd suggest reporting matched parameters/FLOPs and an identical training budget for a fair test."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hhf1hQhGJ7", "forum": "oUMiuYHW21", "replyto": "oUMiuYHW21", "signatures": ["ICLR.cc/2026/Conference/Submission5468/Reviewer_YF8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5468/Reviewer_YF8s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886924522, "cdate": 1761886924522, "tmdate": 1762918080293, "mdate": 1762918080293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Uni-NTFM, a large-scale EEG foundation model designed to unify temporal, spectral, and spatial representations of brain signals. The model incorporates three key innovations: (1) a Heterogeneous Feature Projection Module that decouples time-, frequency-, and raw-signal representations; (2) a Topological Embedding mechanism that encodes spatial priors across different electrode standards; and (3) a Mixture-of-Experts (MoE) Transformer that scales efficiently and captures heterogeneous EEG patterns. Uni-NTFM achieves state-of-the-art performance across nine downstream tasks (clinical, cognitive, and BCI datasets) under both linear probing and fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized and clearly written, making it easy to follow and understand.\n2. The experimental evaluation is comprehensive, with systematic ablations of each module and thorough benchmarking against strong baselines under both linear-probing and full fine-tuning settings.\n3. Overall the model achieved better performance compared with other baselines."}, "weaknesses": {"value": "1. The paper claims “zero-shot transfer” performance, but the evaluation setup corresponds to linear probing, where a supervised linear classifier is trained on the downstream data with the pretrained backbone frozen. While this setup measures representation quality and transferability, it does not constitute true zero-shot inference. The terminology should be clarified to avoid confusion.\n2. The paper claims that the proposed MoE design enables efficient scaling of model capacity. However, this claim is not well substantiated by the current ablation study (if I understand correctly, the current ablation simply removes the MoE module). I would recommend replacing the MoE with a comparably sized dense Transformer and conducting a matched-compute comparison to more convincingly demonstrate the efficiency benefit. Since efficient scaling is listed as one of the main contributions, a more detailed analysis or ablation is necessary to support this claim.\n3. As mentioned above, it would be more informative to include the parameter counts of both the proposed model and the baselines in Tables 1, 2, and 3. This would make it easier to assess model efficiency and fairly support the efficiency-related claims.\n4. From an architectural perspective, although the current design and combination of each module reflects thoughtful consideration, the novelty of the proposed framework appears limited. The overall design builds upon standard Transformer and MoE backbones, and the introduced modules (e.g., heterogeneous feature projection, topological embedding, domain calibration) largely combine existing ideas from temporal–spectral decomposition and spatial encoding. While the integration is well executed and practically useful, it represents more of a system-level consolidation rather than a fundamentally new architectural contribution."}, "questions": {"value": "1. How would performance change without EEG data augmentation?\n2. Could you provide ablations on the different position embedding components (channel-wise, region-wise, relative index within region) combination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxRJkcSb6H", "forum": "oUMiuYHW21", "replyto": "oUMiuYHW21", "signatures": ["ICLR.cc/2026/Conference/Submission5468/Reviewer_zMGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5468/Reviewer_zMGJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973519447, "cdate": 1761973519447, "tmdate": 1762918079656, "mdate": 1762918079656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Uni-NTFM, a foundation model for EEG that addresses domain-specific challenges through a decoupled time-frequency architecture, topological electrode embeddings across different standards, and a Mixture-of-Experts Transformer, pretrained on 28,000+ hours of diverse EEG data. The model demonstrates \"superior performance\" over existing task-specific methods and foundation models across nine downstream tasks under both linear probing and fine-tuning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The decoupled time-frequency architecture, topological electrode embeddings, and MoE-based routing represent thoughtfu\n\n- Extensive experiments across nine diverse downstream tasks under both linear probing and fine-tuning.\n\nOutstanding presentation featuring clear writing and accurate usage of technical terminology, effectively applying concepts such as foundation models, zero-shot learning, and fine-tuning, which are often misused in the community.\n\n- The 1.9B parameter scale, 28,000+ hours of diverse pretraining data, and superior performance establish a meaningful benchmark for EEG foundation models with potential clinical and neuroscience applications.\n\nAll necessary code, data, and information are provided to ensure reproducibility."}, "weaknesses": {"value": "- The ablation study is conducted on a near-saturation task [1](e.g., TUAB), where the baseline model shows low performance; even a lightweight model such as EEGNet (~2k parameters) could outperform it (see [2]).\n\n- The reported improvement over existing methods is marginal, limiting the practical significance of the contribution.\n\nThe novelty of this approach is limited, as it appears to be a combination of existing methods [3,4].\n\n[1] Kiessner, A. K., Schirrmeister, R. T., Boedecker, J., & Ball, T. (2024). Reaching the ceiling? Empirical scaling behaviour for deep EEG pathology classification. Computers in Biology and Medicine, 178, 108681.\n\n[2] Darvishi-Bayazi, M. J., Ghaemi, M. S., Lesort, T., Arefin, M. R., Faubert, J., & Rish, I. (2024). Amplifying pathological detection in EEG signaling pathways through cross-dataset transfer learning. Computers in biology and medicine, 169, 107893.\n\n[3] Zhang, X., Zhao, Z., Tsiligkaridis, T., & Zitnik, M. (2022). Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in neural information processing systems, 35, 3988-4003.\n\n[4] Kuruppu, G., Wagh, N., & Varatharajah, Y. (2025). Eeg foundation models: A critical review of current progress and future directions. arXiv preprint arXiv:2507.11783."}, "questions": {"value": "- Given that you have already provided different model sizes, would it be possible to create a performance scaling plot versus the number of parameters?\n\nDo we really need billions of parameters for the current size of EEG data?\n\n- Would it be better to plot the ablation tables for a clearer and faster understanding?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yumGT436TG", "forum": "oUMiuYHW21", "replyto": "oUMiuYHW21", "signatures": ["ICLR.cc/2026/Conference/Submission5468/Reviewer_BGmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5468/Reviewer_BGmc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029643748, "cdate": 1762029643748, "tmdate": 1762918079432, "mdate": 1762918079432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}