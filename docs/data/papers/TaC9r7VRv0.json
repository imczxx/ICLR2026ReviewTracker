{"id": "TaC9r7VRv0", "number": 5220, "cdate": 1757869274054, "mdate": 1759897987537, "content": {"title": "Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation", "abstract": "Diffusion models have revolutionized generative modeling with their exceptional ability to produce high-fidelity images. However, misuse of such potent tools can lead to the creation of fake news or disturbing content targeting individuals, resulting in significant social harm. In this paper, we introduce Anti-Reference, a novel method that protects images from the threats posed by reference-based generation techniques by adding imperceptible adversarial noise to the images. We propose a unified loss function that enables joint attacks on fine-tuning-based customization methods, non-fine-tuning customization methods, and human-centric driving methods. Based on this loss, we train a Adversarial Noise Encoder to predict the noise or directly optimize the noise using the PGD method. Our method shows certain transfer attack capabilities, effectively challenging both gray-box models and some commercial APIs. Extensive experiments validate the performance of Anti-Reference, establishing a new benchmark in image security.", "tldr": "", "keywords": ["Diffusion model", "Adversarial attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a40db01a3196ae670c49dbdba810dd69551cd7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Anti-Reference, a universal image protection framework designed to defend against both fine-tuning-based (e.g., DreamBooth, LoRA) and reference-based (e.g., IP-Adapter, Magic Animate, EMO) generative models. The method perturbs user images with imperceptible adversarial noise, either by training a DiT-based Adversarial Noise Encoder (ANE) or by optimizing noise directly via PGD. The unified loss jointly disrupts diffusion and conditional generation processes, while data augmentations enhance robustness to real-world transformations. Additionally, the authors explore gray-box transferability by attacking proxy models with architectures resembling proprietary APIs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to explicitly target reference-based generation methods (e.g., IP-Adapter, Magic Animate, EMO), while simultaneously offering a universal perturbation framework that generalizes across both fine-tuning-based and non-finetuning generative pipelines.\n2. The paper conducts quantitative and qualitative experiments across 7 customized generation tasks, evaluates robustness to prompt and transformation variations, and provides human evaluation studies - showing methodological rigor and practical reliability."}, "weaknesses": {"value": "1. As shown in Tables 1-3, the ANE variant consistently underperforms compared to baselines such as SimAC, AdvDM, and PhotoGuard across most target generative models. For example, under the ISM Score metric (Table 1), ANE lags behind baselines in both fine-tuning-based and reference-based settings. Also, while the paper highlights that ANE runs ~1000× faster than baseline methods, its lower protection performance raises doubts about the practical utility of such speed. Conversely, the PGD variant achieves stronger performance but incurs at least 4× higher computational cost than the baselines. This trade-off is not fully addressed or analyzed, leaving the core value of ANE somewhat unclear.\n2. The paper evaluates performance using ISM, Aesthetic Score, and CLIP-IQA. However, prior works like PhotoGuard and AdvDM typically report FID and Precision, which are more widely adopted and allow for better comparability. The use of unconventional metrics hinders direct evaluation against existing approaches.\n3. The comparison in Table 5 shows that the proposed methods operate under different PSNR and SSIM levels than baselines (e.g., PSNR of the proposed method is about 30 and that of  AdvDM is about 38). Without equalizing the perturbation strength, it's difficult to fairly assess whether improvements stem from better framework design or simply from allowing more visible (and thus stronger) noise.\n\nNote: Weaknesses 1-3 correspond directly to Questions 1-3."}, "questions": {"value": "1. What is the practical justification for introducing ANE, given that its protection performance is consistently lower than both the baselines and your own PGD variant? Does the runtime gain outweigh the significant drop in protection quality?\n2. Can the authors report results using widely adopted evaluation metrics, such as FID and Precision, to enable more direct comparison with prior works?\n3. Can the authors report additional experiments where PSNR/SSIM are matched across methods, to ensure the perturbation budgets are comparable? For instance, increasing ANE's regularization weight could improve its perceptual quality. Were such variants tested? \n4. In Figure 5 (EMO, 5th column), it is unclear whether the clean and perturbed outputs are meaningfully different. Also, if the protection effect relies on generating visible artifacts, couldn’t an attacker simply denoise the generated image? As universality is a central claim, quantitative gray-box results would strengthen the argument.\n5. According to Section 5.1, the unified loss employs fixed weight values. Could the authors clarify the rationale behind these choices and provide ablation studies to validate their impact on performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mt0ENannEX", "forum": "TaC9r7VRv0", "replyto": "TaC9r7VRv0", "signatures": ["ICLR.cc/2026/Conference/Submission5220/Reviewer_F22m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5220/Reviewer_F22m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945014064, "cdate": 1760945014064, "tmdate": 1762917956664, "mdate": 1762917956664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Anti-Reference, a universal defense framework designed to protect personal images from misuse in reference-based diffusion models. The authors propose an Adversarial Noise Encoder (ANE) that efficiently generates protective perturbations in a single forward pass, offering much faster inference than PGD-based methods, though with somewhat weaker protection performance. Extensive experiments across multiple reference-based generation tasks demonstrate that Anti-Reference effectively degrades identity fidelity and visual consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduce a universal and model-agnostic defense framework applicable to DreamBooth, IP-Adapter, MagicAnimate, and Ecomimic without per-model retraining.\n- The authors propose an Adversarial Noise Encoder (ANE), a ViT-based module that generates protective noise in a single forward pass, which is faster than PGD and demonstrates strong cross-model transferability by effectively performing gray-box attacks on unseen models.\n- The experiments provide comprehensive quantitative and qualitative evaluations across seven reference-based generation tasks and several datasets."}, "weaknesses": {"value": "- The paper presents an empirical approach with limited theoretical justification.\n- The authors claim a universal framework but only validate on human-centric datasets, limiting generalizability.\n- Section 4.3 lacks depth and insight, and it reads like a simple combination of loss terms without analysis behind the design choices.\n- No ablation study on the contribution or sensitivity of each loss component.\n- No failure case analysis to understand when and why the protection fails.\n- The gray-box evaluation (Section 5.3) is only supported by qualitative examples (Figure 5) without quantitative metrics.\n- Baseline configurations are under-specified; it is unclear whether comparisons are conducted under fair and consistent settings.\n\n### Minor\n- Figure 5 is not referenced or explained in the main text.\n- The formatting of Table (Table x / Tab. x) and Figure (Figure x / Fig. x) references should be unified for consistency."}, "questions": {"value": "- The inclusion of a human evaluation is commendable; could the authors provide statistical significance tests to strengthen the conclusions?\n- Is the proposed defense easily bypassed or neutralized through **fine-tuning** of the target model?\n- In Section 5.1, could the authors explain the rationale behind their chosen experimental parameter settings?\n- Could the authors provide training curves for the total loss to illustrate how each component evolves during training and to better demonstrate the optimization stability and convergence behavior?\n- Other questions are discussed in the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j6Dj1vQRbz", "forum": "TaC9r7VRv0", "replyto": "TaC9r7VRv0", "signatures": ["ICLR.cc/2026/Conference/Submission5220/Reviewer_ySbQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5220/Reviewer_ySbQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484781609, "cdate": 1761484781609, "tmdate": 1762917956424, "mdate": 1762917956424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Anti-Reference, a new defense method designed to protect user images from being exploited by reference-based generation techniques in diffusion models (e.g., IP-Adapter, ReferenceNet, DreamBooth, LoRA). Such methods are widely used for customized image and video generation, including talking-face and human animation, but can also be abused to create harmful or misleading content such as deepfakes, fake news, or explicit imagery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important and emerging issue of defending against misuse of diffusion models for reference-based generation, which is socially relevant.\n\n2. he method attempts to unify defenses across fine-tuning, non-fine-tuning, and human animation methods.\n\n3. The Adversarial Noise Encoder (ANE) is designed to be much faster than PGD-based baselines, which is practically attractive."}, "weaknesses": {"value": "1. The paper frames its method as a universal defense, yet all experiments are constrained to Stable Diffusion 1.5 and related architectures. Transferability to SD-XL or newer backbones is minimal, undermining the universality claim. Attacks on commercial APIs are anecdotal and lack quantitative rigor, making the “gray-box” transferability evidence weak. The evaluation is narrowly scoped to curated datasets (LAION, CelebA, TikTok) and does not convincingly prove robustness in real-world, diverse image-sharing platforms.\n\n2. The technical core of the paper is simply adding adversarial noise through a Vision Transformer–based encoder and combining existing loss terms with weighted sums. This is more of an engineering adjustment than a conceptual innovation. The “unified loss” function is not theoretically justified and provides little new understanding of why the method should generalize. Without rigorous analysis or principled novelty, the contribution feels incremental, essentially presenting an efficiency-oriented version of existing adversarial perturbation work.\n\n3. The choice of baselines is questionable, and comparisons are not entirely fair. For example, Anti-DreamBooth is dismissed on the grounds of CLIP encoder fine-tuning, but no robust alternative baseline is included. Metrics such as ISM, Aesthetic Score, and CLIP-IQA are convenient but insufficient to capture real-world usability or perceptual quality. Human evaluation is limited to a small, non-diverse group, undermining the strength of user-centered claims. Time cost comparisons are also overstated: PGD is portrayed as impractically slow, yet the authors ignore optimized adversarial methods that could serve as stronger baselines.\n\n4. The method concedes that its noise introduces more visible artifacts than baselines but downplays this critical usability flaw. For real users, any perceptible distortion undermines adoption, making the defense questionable in practical contexts. Moreover, the robustness tests are limited to trivial augmentations like JPEG compression and cropping, leaving open whether the defense would survive complex transformations applied by real platforms. Finally, the ethical discussion is superficial, consisting of boilerplate acknowledgments without engaging with the broader societal consequences of deploying adversarial noise protections at scale."}, "questions": {"value": "I will raise my score if the authors address W3 and W4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6mwEWwtO3p", "forum": "TaC9r7VRv0", "replyto": "TaC9r7VRv0", "signatures": ["ICLR.cc/2026/Conference/Submission5220/Reviewer_P5V7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5220/Reviewer_P5V7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535036897, "cdate": 1761535036897, "tmdate": 1762917956043, "mdate": 1762917956043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Anti-Reference, a defense mechanism that protects personal or artistic images from being exploited in reference-based or fine-tuning-based diffusion model generation. The paper claims strong performance, transferability to gray-box commercial APIs, and orders-of-magnitude faster inference compared to PGD-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a relevant problem of defending images from misuse in reference-based generation by using an adversarial perturbation framework that is reasonably well-engineered and empirically validated. \n2. the paper is well written and well presented"}, "weaknesses": {"value": "1. The main empirical results are reported on Stable Diffusion 1.5 (SD 1.5), which is increasingly outdated in the context of generative models. Given that SD 3 and other more advanced architectures are now available and widely used.  Without experiments on these newer models, it is uncertain whether the proposed methods generalized to current-state generation systems.\n2. Gray-box setting: The gray-box setting used in this paper is only partially convincing. While it is reasonable from a practical standpoint, the experiments mainly involve models that share the same SD 1.5 backbone, making the evaluation closer to an intra-family transfer rather than a true gray-box or black-box setting. Consequently, the claimed “universality” is overstated.\n3. Limited technical novelty: While the paper presents an interesting unification of adversarial defense mechanisms against both fine-tuning–based and reference-based diffusion models, the underlying techniques rely heavily on established adversarial training principles. The primary novelty lies in combining these components into a single framework rather than introducing a fundamentally new algorithmic insight.\n4. Lack of baselines:  The comparison set of defense baselines is incomplete. While the paper includes several prior works, it omits other recent or conceptually related defenses such as data-free universal watermarking methods."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cBjZ5JTbio", "forum": "TaC9r7VRv0", "replyto": "TaC9r7VRv0", "signatures": ["ICLR.cc/2026/Conference/Submission5220/Reviewer_JeS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5220/Reviewer_JeS9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842988708, "cdate": 1761842988708, "tmdate": 1762917955681, "mdate": 1762917955681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}