{"id": "g3FLeSCmgJ", "number": 19517, "cdate": 1758296915266, "mdate": 1759897034881, "content": {"title": "WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale", "abstract": "Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex real-world tasks. We introduce WildLong, a framework for generating diverse, scalable, and realistic instruction-response datasets tailored to long-context tasks. WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation. Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data. By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context instruction tuning.", "tldr": "WildLong is a framework for scalably generating diverse, realistic long-context instruction datasets that boost LLMs’ performance on long-context tasks.", "keywords": ["long-context", "synthetic data", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc9165717bcafda9e7341a186afa606bc7fcfcae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces WildLong, a benchmark designed to evaluate LLMs' capabilities in long-context understanding and reasoning. The authors aim to address the limitations of existing long-context benchmarks, which often rely on synthetic or simplified data. WildLong is constructed from real-world datasets—such as long documents, GitHub repositories, and code bases—processed into multi-turn question-answering tasks. The benchmark is intended to simulate realistic long-context scenarios while maintaining high data quality and diversity. The paper also provides extensive experiments across various model sizes and context lengths to illustrate the benchmark’s utility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and timely problem: the lack of realistic and diverse long-context benchmarks for LLM evaluation.\n2. WildLong is built from real-world sources rather than artificial data, which increases its ecological validity compared to existing synthetic benchmarks.\n3. The experiments cover a wide range of models and configurations, providing a comprehensive empirical view of long-context performance trends."}, "weaknesses": {"value": "1. The paper lacks substantial methodological innovation. The main contribution is dataset construction rather than a novel algorithm, framework, or evaluation methodology. The graph-based construction method has been widely applied by several studies. While the dataset is valuable, the overall technical depth is limited.\n2. The data synthesis pipeline** largely follows established practices (e.g., sampling, chunking, and QA generation using existing LLMs) without introducing new insights into data generation or evaluation strategies.\n3. Although the benchmark uses \"real-world data\", it remains unclear how the realism of synthesized QA pairs is validated beyond basic filtering and prompt design. More rigorous human evaluation or ablation analysis would strengthen the claims.\n4. The experimental section primarily provides descriptive statistics and performance comparisons, but lacks deeper analysis (e.g., error categorization, reasoning failure types, or task-level breakdowns) that could yield more meaningful findings."}, "questions": {"value": "Please check my comments in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hu4fMUvHfP", "forum": "g3FLeSCmgJ", "replyto": "g3FLeSCmgJ", "signatures": ["ICLR.cc/2026/Conference/Submission19517/Reviewer_rxh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19517/Reviewer_rxh6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568709023, "cdate": 1761568709023, "tmdate": 1762931411187, "mdate": 1762931411187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WildLong, a framework designed to create large-scale, diverse, and realistic datasets for long-context SFT. \nWildLong first extracts 13 types of \"meta-information\" (e.g., user intent, task type, document type) from real-world user-chatbot conversations. It then models the relationships between these elements using a graph. By performing random walks on this graph, the framework generates novel and realistic instruction templates. These templates are paired with documents from a large corpus (SlimPajama) to generate final instruction-response pairs using GPT-4. The framework is also extended to create tasks that require reasoning across multiple documents.\nThe authors fine-tune two open-source models, Mistral-7B and Llama-3.1-8B, on 150,000 examples generated by WildLong. Their results show that these models significantly outperform existing open-source long-context models on benchmarks like RULER and HELMET, and perform competitively with much larger models, all while maintaining strong performance on short-context tasks without needing extra short-context data."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel and Scalable Data Generation: The core strength of the paper is its innovative, graph-based approach to data synthesis. By modeling the co-occurrence of meta-information from real user queries, WildLong can generate a massive and diverse set of instructions that are more realistic and complex than those from previous methods.\n- Strong Empirical Results: The fine-tuned models show significant performance gains on multiple long-context benchmarks. The Llama-3.1-8B model trained with WildLong data is competitive with models that are nearly 10 times larger (e.g., Llama-3.1-70B), demonstrating the high quality of the synthetic data.\n- Preservation of Short-Context Abilities: A notable achievement is that the models improve on long-context tasks without degrading their performance on short-context tasks. This is a common trade-off, and WildLong avoids it by creating a high-quality, generalist long-context dataset, eliminating the need to mix in short-context data."}, "weaknesses": {"value": "- Limited Scope of \"Real-World\" Scenarios: The initial meta-information is extracted from the WildChat dataset, which consists of user-ChatGPT conversations. While large, this outdated dataset (before 2024.5) may not capture the full spectrum of long-context reasoning in specialized domains like agentic tasks and coding tasks.\n- Potential for Inherited Bias: The framework synthesizes data based on patterns in existing conversations (WildChat) and generates responses using an LLM (GPT-4). Any biases present in these source materials are likely to be inherited and potentially amplified in the resulting dataset."}, "questions": {"value": "- Can the WildLong constructed data evolved with more up-to-date log data?\n- The framework resamples the SlimPajama dataset to match the document type distribution from WildChat. Could this step potentially limit the diversity of document types the model is exposed to, and have you considered alternative approaches to pairing instructions with documents?\n- The paper focuses on single-turn conversations. How could the WildLong framework be adapted to generate instruction data for multi-turn, long-context dialogues where the context is built up over several turns?\n- Figure 5 shows that performance on the RULER and HELMET benchmarks continues to improve as the dataset scales to 150,000 samples. Do you have a hypothesis on where performance might saturate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aq3Q5W9AVU", "forum": "g3FLeSCmgJ", "replyto": "g3FLeSCmgJ", "signatures": ["ICLR.cc/2026/Conference/Submission19517/Reviewer_C458"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19517/Reviewer_C458"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806479080, "cdate": 1761806479080, "tmdate": 1762931410688, "mdate": 1762931410688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called WildLong, designed to synthesize large-scale, high-quality long-context instruction data to improve the performance of large language models on long-text reasoning tasks. The overall idea is clear, the methodology is well structured, and the experimental results are convincing. WildLong demonstrates its ability to enhance long-context reasoning while maintaining solid short-context performance.\n\nAs a researcher specializing in human-computer interaction, this study clearly falls outside my area of expertise. Following the Area Chair’s instructions, I have selected “1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers” and submitted my review accordingly. Therefore, I will not be participating in the rebuttal stage for this manuscript. Thank you for your understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "First, the paper combines graph-based modeling with meta-information extraction and uses random walks to generate diverse long-context tasks. This data synthesis idea is both innovative and scalable.\n\nSecond, it validates performance improvements across multiple well-known benchmarks such as RULER, HELMET, and LongBench, with thorough comparisons against various baselines, including models specifically optimized for long contexts.\n\nThird, by extracting meta-information from real dialogue data like WildChat, the synthesized instructions better reflect real-world user scenarios, and the paper shows that the data distribution closely matches actual user needs."}, "weaknesses": {"value": "First, the data generation process heavily relies on closed-source models such as GPT-4. Although a Qwen-based experiment is included, reproducibility and sustainability remain limited.\n\nSecond, the analysis focuses mainly on quantitative improvements but lacks a deeper look into instruction diversity, semantic authenticity, and interpretability.\n\nThird, even though the dataset is large, there is no demonstration of its effectiveness or transferability in real downstream applications such as legal analysis or academic research.\n\nFourth, it is not entirely clear whether the diversity of the synthesized data might be limited in some way."}, "questions": {"value": "First, since meta-information extraction depends on GPT-4, has the paper evaluated how using different extraction models might affect the final data quality?\n\nSecond, are the definitions of nodes, edges, and weight calculations in the graph sensitive to the results? Has any stability analysis been done?\n\nThird, although the authors describe safety filtering and ethical review of the synthetic instructions, was there any external auditing or open evaluation process?\n\nFourth, since the experiments are mainly conducted on 7B and 8B models, can the approach generalize to larger models or different architectures?\n\nFifth, in the RoPE base adjustment experiments, how was the statistical significance of performance trade-offs evaluated? Was any variance or confidence interval analysis performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "13wuc8GHsX", "forum": "g3FLeSCmgJ", "replyto": "g3FLeSCmgJ", "signatures": ["ICLR.cc/2026/Conference/Submission19517/Reviewer_xyou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19517/Reviewer_xyou"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813540949, "cdate": 1761813540949, "tmdate": 1762931410153, "mdate": 1762931410153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WildLong, a scalable framework for synthesizing diverse and realistic instruction–response pairs to enhance long-context reasoning in large language models (LLMs). The method integrates meta-information extraction from real user–chatbot conversations, graph-based modeling of meta-information relationships, and adaptive instruction–response generation using GPT-4. The authors fine-tune Mistral-7B and Llama-3.1-8B on 150K synthetic samples and demonstrate strong performance on long-context benchmarks (e.g., RULER, HELMET, LongBench-Chat), often matching or exceeding larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The graph-based meta-information modeling and path sampling strategy enable scalable and diverse instruction generation, grounded in real user interactions.\n- The paper includes extensive experiments across multiple long-context benchmarks (RULER, HELMET, LongBench-Chat) and compares against a wide range of proprietary, open-source, and specialized long-context models.\n- The authors thoroughly ablate key components (e.g., path sampling strategy, path length, teacher model impact) and demonstrate that performance improves with increasing data size up to 150K samples."}, "weaknesses": {"value": "- While the framework supports multi-document tasks, the evaluation does not clearly distinguish whether the gains come from single- or multi-document supervision. A more fine-grained breakdown of multi-document task performance (e.g., cross-document reasoning, synthesis) would better validate the extension.\n- The use of GPT-4 as the primary teacher model for instruction–response generation raises concerns about reproducibility and accessibility. \n- The paper does not include a qualitative analysis of where the fine-tuned models still fail—especially in complex reasoning tasks like aggregation or citation generation—which would help identify remaining challenges."}, "questions": {"value": "How does the graph-based sampling strategy ensure that generated instructions are not only diverse but also challenging enough to improve reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0eYkPZ6FnR", "forum": "g3FLeSCmgJ", "replyto": "g3FLeSCmgJ", "signatures": ["ICLR.cc/2026/Conference/Submission19517/Reviewer_2B3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19517/Reviewer_2B3G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762191831641, "cdate": 1762191831641, "tmdate": 1762931409503, "mdate": 1762931409503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}