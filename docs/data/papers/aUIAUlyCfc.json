{"id": "aUIAUlyCfc", "number": 15089, "cdate": 1758247605816, "mdate": 1763372418969, "content": {"title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting", "abstract": "As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CYBERTEAM, a benchmark designed to guide LLMs in blue teaming practice. CYBERTEAM constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CYBERTEAM integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CYBERTEAM against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the  limitations of open-ended reasoning in  real-world threat hunting.", "tldr": "This work benchmarks LLMs in blue team threat hunting tasks through a standardization design", "keywords": ["Cyber Threat Intelligence (CTI)", "Large Language Models (LLMs)", "Blue Teaming", "Benchmarking and Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfe01ed92790d0d3596aa59e920a8ba2d55df62b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Cyberteam, a benchmark for evaluating Large Language Models (LLMs) in blue team threat-hunting workflows. Cyberteam constructs a standardized workflow modeling realistic threat-hunting dependencies across 30 tasks organized into four stages, with each task addressed through 9 operational modules. In this paper, the authors comprehensively evaluated the performance of multiple LLMs on Cyberteam, with actionable insights on llm-aided cybersecurity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Coverage of benchmark: This is the major strength from reviewer’s point of view. Cyberteam  exceeds existing cybersecurity benchmarks on task coverage such as CTIBench, SevenLLM-Bench and SWE-Bench which provides comprehensive coverage across threat-hunting lifecycle stages with real-world heterogeneity.\n\nThe presentation of the paper is joyful to read: The paper is clearly structured, especially for readers who does not have a deep knowledge on blue teaming. For example Fig. 2 shows dependency chain of analytical tasks in details which I really appreciate.\n\nGood empirical validation: This work combines both evaluation and benchmarking with case studies, which is a comprehensive validation of the effectiveness of the benchmark."}, "weaknesses": {"value": "Limited failure analysis: Even the paper covers a wide range of blue teaming modules, it lacks detailed characterization of failure patterns. For example, when does NER miss critical entities? When does RAG retrieve irrelevant context? etc. This work would be more solid with this analysis.\n\nData contamination: Some model’s training data may already have some data sample contaminated, that is the one of the weakness for LLM selection, but from reviewer’s point of view, a minor point."}, "questions": {"value": "Refer to the first point of weakness, any insights on failure analysis?\n\nHow is data contamination mitigated, particularly for closed models that may have pretraining exposure to these original data?\n\nA question regarding model selection: I noticed some models support or may support thinking mode, for example Claude 4 Sonnet, or Gemini 2.5 (it is not clear if this work uses Flash or Pro). So how is the thinking mode setup for this work? How does these thinking impact the benchmarking results?\n\nA question about Human-In-The-Loop: I found in appendix, this work may use Human-In-The-Loop with human analysts. How would analysts interact with Cyberteam in operational practice? Would they validate module outputs sequentially, override module decisions when expertise contradicts outputs, or use outputs purely as suggestions?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The authors stated in ethics statement about the license and copyright, so I think it would be a minor issue."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rUhNtZxqtD", "forum": "aUIAUlyCfc", "replyto": "aUIAUlyCfc", "signatures": ["ICLR.cc/2026/Conference/Submission15089/Reviewer_piEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15089/Reviewer_piEC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761169237240, "cdate": 1761169237240, "tmdate": 1762925412989, "mdate": 1762925412989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark for evaluating how effectively LLMs can assist in blue team threat-hunting workflows. Unlike prior cybersecurity benchmarks that focus on narrow, isolated tasks, CYBERTEAM models the full lifecycle of threat hunting, from threat attribution and behavior analysis to prioritization and mitigation, by translating this it into a structured, modular reasoning process. It integrates 30 tasks and 9 operational modules (e.g., NER, RAG, summarization) to guide LLMs through standardized analytical steps.  Experimental results show that CYBERTEAM outperforms open-ended reasoning methods such as Chain-of-Thought and Tree-of-Thought, producing more accurate, interpretable, and robust results in real-world scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "a novel framework for LLM-based cyber threat hunting"}, "weaknesses": {"value": "- Lack of description of design rationale\n- Lack of comparison with non-LLM approach\n- Lack of human evaluation"}, "questions": {"value": "### **Design Rationale**\n\nMy main concern with this paper lies in the motivation and rationale behind the framework’s design.\n\nWhile in MITRE attack matrix, the techniques employed by attackers to launch an attack is separated into phases, these techniques combined in specific ways can form a complete attack. For example, the attackers need to first compromise the network to deploy malware, then exploit vulnerabilities to escalate privilege, and finally launch exfiltration attacks. However, the current framework seems to evaluate the task completely separately, and each task's success metric listed in Table 2 is too simplistic. It is difficult to understand how a similarity metric can be directly translated into a successful executed attack step. Let alone connecting multiple steps to form an investigation of a complete attack. The paper makes an over-simplistic  assumption of a cyber attack, and the benchmark cannot present too much practical value in using LLMs to defend or detect attack steps. \n\n\n### **Hallucination**\n\nThe entire framework is automated by LLMs, but many of its components could instead leverage existing deep learning–based classifiers, such as those for *Attack Vector Classification* or *Severity Scoring*, which already perform well. Similarly, for NER and REX, it would make sense to use existing NER toolkits or regex-based patterns. Therefore, I believe a tool-using agentic framework would be more effective. I would also like to see comparisons with these non-LLM approaches at each step. The current evaluation, which only compares different in-context learning or prompt engineering settings, is not meaningful.This is also attributed to the motivation of the proposed benchmark that simply assumps every attack step should be analyzed by LLMs rather than alternative techniques. \n\n### **Utility**\n\nThe authors claim that “*This design is inspired by blue team practices*.” Therefore, I believe a human evaluation is necessary to validate this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WSqsoSOueZ", "forum": "aUIAUlyCfc", "replyto": "aUIAUlyCfc", "signatures": ["ICLR.cc/2026/Conference/Submission15089/Reviewer_GcYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15089/Reviewer_GcYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542203175, "cdate": 1761542203175, "tmdate": 1762925412574, "mdate": 1762925412574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CyberTeam, a benchmark for evaluating LLMs in blue team cybersecurity workflows. CyberTeam models real-world threat-hunting as a modular process of 30 interdependent tasks. It aggregates data from 23 major cybersecurity databases and platforms, enabling LLMs to perform standardized, stepwise reasoning rather than open-ended reasoning. Experiments comparing this structured approach to open-ended reasoning methods (like CoT and ToT) show that CyberTeam's modular design significantly improves performance, interpretability, and reliability in threat-hunting scenarios, while also highlighting LLM weaknesses under noisy or ambiguous inputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The structured reasoning approach is an important technical contribution\n- The evaluation covers multiple LLMs to highlight the benefit of CyberTeam across a variety of models"}, "weaknesses": {"value": "- No implementation details are provided for the 9 modules. For instance, what database and indexing mechanism does RAG use? These details are not even present in Appendix B.6 for RAG. There is similar lack for remaining modules too.\n- It seems that every task of the CyberTeam flow invokes an LLM prompt, possibly implying that CyberTeam may consume higher tokens than CoT. Appendix E.1 provides run time details where CyberTeam exceeds all methods across all tasks, indicating that it may also consume higher tokens. More balanced comparison of CoT and ToT will be when they are allowed multiple attempts such that they have a similar token consumption as CyberTeam.\n- Except for sources, no details about the benchmark are provided in terms of how they were processed, or if any deduplication was performed among the data from similar sources. It is also not clear whether the #Data column of Table 2 counts single threat-hunting report multiple times as each report may involve multiple tasks."}, "questions": {"value": "- How were the benchmark samples collected and validated across 23 data sources? Were any heuristics, deduplication, or human verification steps applied?\n- How is the data imbalance in the tasks countered? How does it affect the evaluation?\n- How are ground-truth labels generated for complex tasks like *Patch Code Generation* or *Patch Tool Suggestion*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzkur9Nxnt", "forum": "aUIAUlyCfc", "replyto": "aUIAUlyCfc", "signatures": ["ICLR.cc/2026/Conference/Submission15089/Reviewer_1odD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15089/Reviewer_1odD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975866086, "cdate": 1761975866086, "tmdate": 1762925412075, "mdate": 1762925412075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CYBERTEAM, a benchmark for evaluating LLMs in blue team threat-hunting scenarios. It models threat hunting as a standardized, modular workflow encompassing 30 analytical tasks and 9 operational modules that span four phases: threat attribution, behavior analysis, prioritization, and response & mitigation. The benchmark integrates data from 23 cybersecurity sources and evaluates both general-purpose and cybersecurity-specialized LLMs under standardized versus open-ended reasoning paradigms. Empirical results show that standardized workflows substantially outperform open-ended reasoning methods such as CoT and ToT in accuracy, robustness, and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Originality: Comprehensive benchmark to model end-to-end blue team threat hunting through modular LLM reasoning.\n\n- Quality: Thorough evaluation across multiple models and reasoning paradigms, grounded in realistic threat-hunting lifecycles.\n\n- Clarity: Clear motivation, strong contextualization within prior cybersecurity and LLM-agent work.\n\n- Significance: Enables standardized, reproducible research in an emerging area of AI-driven cyber defense, with clear potential for community impact."}, "weaknesses": {"value": "- Limited discussion of generalization to unseen attack types or adversarially perturbed data beyond the controlled noise study.\n\n- Human evaluation of interpretability and operational usefulness is absent. Practical blue team deployment scenarios are only discussed qualitatively.\n\n- Ablation studies on module interaction (e.g., contribution of SUM vs. RAG) are not fully explored.\n\n- Some experimental results (e.g. Table 3, Figure 4) could include variance or confidence intervals to better reflect robustness.\n\n- The paper lacks efficiency metrics, such as inference latency, computational overhead, or resource utilization of the modular workflow compared to open-ended reasoning. Limits understanding of its practical deployability in real-time threat-hunting."}, "questions": {"value": "- How are task dependencies dynamically handled when upstream modules fail or produce uncertain outputs?\n\n- Could CYBERTEAM be adapted for red team or mixed adversarial-defense benchmarks?\n\n- How sensitive is the modular reasoning pipeline to updates in data sources (e.g., CVE schema changes)?\n\n- Is the data type limited to textual logs and code snippets, or can the framework be extrapolated to other data modalities (e.g., network telemetry, images, binaries)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bjjVs4HAXK", "forum": "aUIAUlyCfc", "replyto": "aUIAUlyCfc", "signatures": ["ICLR.cc/2026/Conference/Submission15089/Reviewer_cf9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15089/Reviewer_cf9B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027473483, "cdate": 1762027473483, "tmdate": 1762925411645, "mdate": 1762925411645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}