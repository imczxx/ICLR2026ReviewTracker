{"id": "Lu7WPPkmOq", "number": 16179, "cdate": 1758261073168, "mdate": 1759897256194, "content": {"title": "Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics", "abstract": "Simulating physically plausible trajectories toward user-defined goals is a fundamental yet challenging task in fluid dynamics. While particle-based simulators can efficiently reproduce forward dynamics, inverse inference remains difficult, especially in dissipative systems where dynamics are irreversible and optimization-based solvers are slow, unstable, and often fail to converge. In this work, we introduce the Reversible Graph Network Simulator (R-GNS), a unified framework that enforces bidirectional consistency within a single graph architecture. Unlike prior neural simulators that approximate inverse dynamics by fitting backward data, R-GNS does not attempt to reverse the underlying physics. Instead, we propose a mathematically invertible design based on residual reversible message passing with shared parameters, coupling forward dynamics with inverse inference to deliver accurate predictions and efficient recovery of plausible initial states. Experiments on three dissipative benchmarks (Water-3D, WaterRamps, and WaterDrop) show that R-GNS achieves higher accuracy and consistency with only one quarter of the parameters, and performs inverse inference more than 100× faster than optimization-based baselines. For forward simulation, R-GNS matches the speed of strong GNS baselines, while in goal-conditioned tasks it eliminates iterative optimization and achieves orders-of-magnitude speedups. On goal-conditioned tasks, R-GNS further demonstrates its ability to complex target shapes (e.g., characters “L” and “N”) through vivid, physically consistent trajectories. To our knowledge, this is the first reversible framework that unifies forward and inverse simulation for dissipative fluid systems.", "tldr": "R-GNS unifies forward and inverse fluid simulation—accurate, efficient and reversible.", "keywords": ["physics simulation", "fluid simulation", "reversible problem", "inverse neural network"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0db014685d0e7a063ff214dd2ac98144c03b995d.pdf", "supplementary_material": "/attachment/d9c6cc4480e16315605a504fad71d4b6c5de0e18.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents \"reversible-GNS\", a graph architecture based on Graph Network Simulator (GNS) able to predict both forward and backwards dynamics. The methods relies on the composition of several invertible mappings: (1) linear encoding and pseudo-inverse decoder, and (2) reversible message passing based on RevNet, resulting in an end-to-end invertibile process. This enables efficient solutions to inverse problems using a standard forward pass, avoiding costly iterative optimization. They provide several examples of dissipative fluid simulations in forward and inverse inference, and goal-conditioned tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The adaptation of RevNet to message passing GNNs is novel and interesting. \n* The writting is clear and easy to read.\n* Comparisons over other state-of-the-art methods are provided, including EGNN, NeuralSPH and GNS baselines."}, "weaknesses": {"value": "* The examples are not convenient for the method. Learning reversible mappings for irreversible processes is conceptually impossible (more on questions sections). \n* The source code is not ready for review.\n* There is a lack of details about the baselines hyperparameters and the dataset physical properties and size."}, "questions": {"value": "* What is the advantage of this approach compared to using a fully expressive forward model combined with a separate model to learn the inverse mapping? This question is motivated by the design of autoencoders, whose encoder–decoder architecture is typically asymmetric, even when trained to approximate the identity mapping. Such a setup is often simpler to implement and can be more expressive overall.\n\n* Lines 79-80: This is more like a general comment rather than a question. I think that branding the method as \"reversible-GNS\" is a poor choice. I understand that the authors refer to forward/inverse flexibility, but in the context of physics simulations solving clearly irreversible dynamics with a \"reversible\" network is very counterintuitive. I think that the correct term mathematically speaking would be \"invertible-GNS\".\n\n* Section 4.1: The paper provides no dataset details. The original GNS datasets were generated with MPM/SPH simulators, yet the fluid regime is unspecified: is it low-viscosity (Euler-like) or high-viscosity (Navier–Stokes)? This distinction is crucial, since for turbulent or highly viscous flows, reversibility breaks down and the method is unlikely to hold.\n\n* Related to the previous question, I have many concerns about the uniqueness and validity of the proposed approach. As already mentioned in the paper, irreversible dynamics such as diffusion inherently destroys information, making inversion ill-posed. The paper claims applicability to any dissipative fluid, but this is simply impossible. I suspect that the examples provided have low dissipation and are quasi-reversible. I've found the architecture very interesting but the results are presented very naively from the physics perspective. I would have preferred to have a couple of truly reversible dynamics.\n\n* Section 4.2.3: How far in the time horizon can the network inference a state from a given target? Does the error accumulation on integration affect the results? Even though the overall process in reversible, the integration noise is not modelled and might break the reversibility ssumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I have no ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nxdII4ahXV", "forum": "Lu7WPPkmOq", "replyto": "Lu7WPPkmOq", "signatures": ["ICLR.cc/2026/Conference/Submission16179/Reviewer_S3n7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16179/Reviewer_S3n7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139601631, "cdate": 1761139601631, "tmdate": 1762926343995, "mdate": 1762926343995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon the work of Sanchez-Gonzalez et al. (2020) [Learning to Simulate Complex Physics with Graph Networks], which introduced the use of Graph Neural Networks (GNNs) for learning physics simulations from trajectory data. The original goal was to train a model capable of predicting future simulation states (rollouts) from an initial condition, without relying on the original simulator (which is only used for obtaining the training trajectories).\n\nThe authors extend this framework by introducing a reversible message-passing mechanism to GNNs. Specifically, they partition node features and design a forward update rule that admits an analytically derived inverse (up to numerical precision). This leads to their proposed model, R-GNS, which they evaluate across a range of tasks.\n\nInterestingly, R-GNS not only supports inversion but also improves forward simulation accuracy. The authors, that for \"dissipative fluids, where inversion is inherently ill-posed and optimization-based solvers break down\", R-GNS enables \"accurate and efficient recovery of initial states\" and try to show this by experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core idea seems conceptually sound, and the most compelling result (in my opinion) is the improved forward simulation performance. Another promising aspect seems to be the semi-symmetric input-output design, which appears to leverage multiple past states via masking. However, this mechanism is underexplained and would benefit from clearer exposition."}, "weaknesses": {"value": "- The authors emphasize the applicability of R-GNS to dissipative fluids. While their experiments suggest improved invertibility in such systems, it is unclear why this should be possible in principle. This raises concerns about the physical plausibility of their results.\n- Several parts of the manuscript are difficult to follow, particularly in Section 4.2.3. It is unclear whether Sep-GNS uses a separately trained reverse model and whether R-GNS uses its inverse mode to predict initial states, followed by forward inference using the respective forward modules.\n- The paper does not provide variance estimates for all reported metrics and describe how they are computed.\n- The manuscript lacks a thorough discussion of prior reversible GNN architectures, e.g., Li et al. (2021) [Training Graph Neural Networks with 1000 Layers]. A comprehensive literature review would help contextualize the novelty of R-GNS."}, "questions": {"value": "## Major Remarks\n\n### Claims Regarding Dissipative Fluids\n\nThe authors emphasize the applicability of R-GNS to dissipative fluids. While their experiments suggest improved invertibility in such systems, it is unclear why this should be possible in principle. This raises concerns about the physical plausibility of their results.\n\nIn particular, the inversion shown in Figure 4b appears to contradict fundamental thermodynamic principles. If such inversion were truly possible, it would imply either:\n(a) the system is not genuinely dissipative, or\n(b) the authors would have found a method to circumvent the second law of thermodynamics.\n\nA more plausible explanation is that the training, validation, and test sets are biased toward a narrow class of initial conditions, e.g., rectangular \"water blocks.\" It would be valuable to explore greater diversity in the \"water block\" shapes used in the experiments. For instance, one could consider using circular shapes in the test set while keeping the training set restricted to rectangles, or vice versa. My expectation is that, in the absence of circular shapes during training, R-GNS would likely default to predicting rectangular water blocks when presented with circles in the test set. If this is not observed, I would be very interested in the authors' explanation of how R-GNS manages to generalize effectively in such inversion scenarios.\n\nAdditionally, applying R-GNS to non-dissipative systems could help clarify whether its performance is specifically only superior in the dissipative regime or also helps under other conditions.\n\n### Clarity of Experimental Setup\n\nSeveral parts of the manuscript are difficult to follow, particularly in Section 4.2.3. It is unclear whether Sep-GNS uses a separately trained reverse model and whether R-GNS uses its inverse mode to predict initial states, followed by forward inference using the respective forward modules.\n\nIf this interpretation is correct, the following ablations would be valuable:\n- Use the **ground-truth simulator** for forward inference from predicted initial states.\n- also check for DiffTaichi+SPH whether the inverted \"N\" and \"L\" shapes can be recovered using the ground-truth simulator.\n- cross-initial state inference: apply R-GNS forward on Sep-GNS inversion outputs, and vice versa.\n\nIn Section 4.2.2, the source of the initial state for inference is also not clear. Is it derived from the ground-truth trajectory or from a model-generated forward rollout? If the latter, an ablation using the ground-truth forward state would be informative.\n\nRegarding MSE:\n- Is the reported MSE averaged over all test trajectories?\n- How comparable are trajectories in terms of particle count? / How comparable are trajectories at all for aggregation?\n- How meaningful is MSE as a metric compared to alternatives like optimal transport based ones? Table 4 in the appendix touches on this, but it's unclear whether those results pertain to forward or inverse predictions.\n\n### Variance and Robustness\n\nPlease provide variance estimates for all reported metrics and describe how they are computed. How sensitive are results to different training re-runs?\n\n### Experimental results reported\n\nDid the authors run all the experiments in the tables from the different methods themselves or did they copy the results from other papers?\n\nThe manuscript lacks clarity on how results compare to Sanchez-Gonzalez et al. (2020). Discrepancies in reported metrics (e.g., Table 1 vs. Appendix C.4 in Sanchez-Gonzalez et al.) should be explained.\n\n### Relation to Reversible GNN Literature\n\nThe manuscript lacks a thorough discussion of prior reversible GNN architectures, e.g., Li et al. (2021) [Training Graph Neural Networks with 1000 Layers]. How does R-GNS relate to this and other reversible GNNs? A comprehensive literature review would help contextualize the novelty of R-GNS.\n\n### Bidirectional Training\n\nThe concept of bidirectional training is introduced but not well explained. Key questions include:\n- How does it differ exactly from standard training regimes? Pseudocode could help here.\n- Is bidirectional training applied within the same batch?\n- Is there a random use of forward mode or inverse mode training on the provided training trajectories?\n\nClarifying the training protocol is essential for understanding the method's mechanics and reproducibility.\n\n## Minor Remarks\nThere furthermore seem to be several typos:\n- I guess the initial state in line 138/139 should be $\\tilde{\\chi}^{0}$ instad of $\\chi^{0}$?\n- The indexing in Figure 2 is strange: I guess \"t:t-k+1\" means that states from \"t-k+1\" up to \"t\" are taken? Please explain in the manuscript if you introduce such non-standard notation.\n- Appendix B and Appendix C are \"unattached\" to the main text.\n- A.0.1 and A.0.2 have strange chapter names. There is no proof at all. \n- The \"*\" in R-GNS* in Table 4 is unexplained.\n- The notation {e} in A.0.2 is strange.\n- \"Theoretical guarantees of reversibility are formally stated in the Appendix A.\" ==> I don't see where theoretical guarantees would be shown. A.0.1 seems to be an empirical \"guarantee\". But even for this further details would be needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6mdoudOgNG", "forum": "Lu7WPPkmOq", "replyto": "Lu7WPPkmOq", "signatures": ["ICLR.cc/2026/Conference/Submission16179/Reviewer_9h4c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16179/Reviewer_9h4c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910970046, "cdate": 1761910970046, "tmdate": 1762926343405, "mdate": 1762926343405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reversible Lagrangian model for learning the inverse dynamics of asymmetric Lagrangian systems. This is a particularly challenging problems and existing works either focus on forward dynamics or rely on computationally expensive optimization based models. In contrast this paper proposes a reversible GNS model to learn the inverse formulation. To handle assymetry the authors propose a semi-symmetric input-output design where the node dynamic quantity is predicted while the other features are masked. The inverse operation is defined as the psuedo inverse of a linear transformation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This is a novel approach to model inverse problems in Lagrangian systems. \n2. The paper is well motivated, with clear explanation of key concepts (reversibility, inverse problem formulation, psuedo-inverses, goal-conditioned generation)\n3. The figures are well designed and intuitive to understand."}, "weaknesses": {"value": "1. The paper does not provide sufficient training details and sufficient ablations on hyperparameters. \n2. Many experiments are presented in a tabular manner, but it's not clear if all the models were trained sufficiently. If R-GNS outperforms GNS with fewer training steps, it should be mentioned. GNS requires 1 million steps to stabilize and 20 million steps to faithfully generalize. It would be good to have a discussion on training stability of R-GNS."}, "questions": {"value": "1. Figure 3 is a little confusing to me. Unidirectional R-GNS is identical to GNS is it not? In which case, I don't understand why the behavior deviates from GNS. \n2. GNS uses random walk noise to stabilize long-horizon rollouts. In the inverse setting, is it still necessary to add random walk noise? How does noise affect the performance? GNS is extremely sensitive so it would be interesting to see whether the reversible layers affect the noise dependency. \n3. It is not clear to me under what scenarios R-GNS strictly outperforms GNS. Due to high sensitivity to noise, training steps, hyper-parameters etc., it would be good to have a discussion specifying at what point R-GNS starts to outperform GNS, rather than only providing tabular numbers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bp8zOYIO68", "forum": "Lu7WPPkmOq", "replyto": "Lu7WPPkmOq", "signatures": ["ICLR.cc/2026/Conference/Submission16179/Reviewer_5LXv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16179/Reviewer_5LXv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947365939, "cdate": 1761947365939, "tmdate": 1762926342802, "mdate": 1762926342802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an unified differentiable framework for forward and inverse simulation of fluids. Unlike traditional optimization-based inverse solvers or feed-forward neural simulators that approximate backward dynamics, the proposed approach (R-GNS) enforces bidirectional consistency through a reversible design. The approach combines three main components: a semi-symmetric input-output structure, an invertible linear projection (ILP) encoder-decoder, and a residual reversible message-passing (RRMP) network. The model achieves comparable speed to standard GNS for forward simulation while enabling inverse inference more than 100x faster than optimization-based methods. A few experiments demonstrate improved accuracy, forward-inverse consistency, and stable goal-conditioned control tasks, such as shaping fluids into target geometries. The authors claim this is the first reversible framework to unify forward and inverse simulation for dissipative systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- R-GNS achieves lower rollout errors than competing neural simulators (GNS, DMCF, EGNN) across the tested datasets, indicating that enforcing bidirectionality can also improve forward dynamics.\n- The paper proposes a coherent framework integrating forward and inverse simulation with a single reversible GNN architecture, reducing parameter count and ensuring consistent physical behavior.\n- The idea of enforcing mathematical reversibility (without assuming physical reversibility) is conceptually interesting and well-justified for tackling ill-posed inverse problems in fluids."}, "weaknesses": {"value": "- The paper lacks detail on how the model was trained relative to the evaluation benchmarks. It is not explicitly stated whether the R-GNS is trained separately for each dataset (WaterDrop, WaterRamp, Water_3D) or jointly across them. This ambiguity makes it difficult to assess generalization capabilities.\n- The evaluation focuses on a narrow set of examples: two for forward/inverse simulation and a single goal-conditioned task. While quantitative metrics are provided, the diversity of test cases and ablation analysis is limited, raising concerns about the robustness and scalability of the method beyond these specific datasets.\n- The paper does not present applications that would motivate the need for reversible inference in real-world settings. Typical use cases for inverse solvers (e.g., shape optimization, control, or parameter estimation) are absent, making it hard to gauge the broader utility of the approach.\n- The paper does not clearly describe the training setup, including dataset composition and train/validation/test splits. Important aspects such as training duration, convergence behavior, and the use (or absence) of data augmentation are missing, making it difficult to assess the reproducibility and robustness of the reported results."}, "questions": {"value": "- I don't understand why edge features are fixed across time-steps. I assume that they encode the relative positions between particles, and these could be changing (albeit less than compared with the absolute positions) over the simulation. Is this correct or is there something I'm missing?\n- Could you better elucidate how the data was prepared for training? How long did the model took to converge compared to the other approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5s4Fcl9hKX", "forum": "Lu7WPPkmOq", "replyto": "Lu7WPPkmOq", "signatures": ["ICLR.cc/2026/Conference/Submission16179/Reviewer_b1NG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16179/Reviewer_b1NG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185678998, "cdate": 1762185678998, "tmdate": 1762926342242, "mdate": 1762926342242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}