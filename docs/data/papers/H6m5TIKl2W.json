{"id": "H6m5TIKl2W", "number": 9473, "cdate": 1758123695464, "mdate": 1763705287008, "content": {"title": "TreeRPO: Tree Relative Policy Optimization", "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce TreeRPO, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, TreeRPO directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, TreeRPO innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows TreeRPO to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our TreeRPO algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0% to 35.5%. Furthermore, TreeRPO significantly outperforms GRPO by 2.9% in performance while simultaneously reducing the average response length by 18.1%, showcasing its effectiveness and efficiency.", "tldr": "We propose TreeRPO, a GRPO variants to training LLMs with dense process reward without using PRMs.", "keywords": ["RLVR", "Reasoning with LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4a065499e53140fb32cccecce8550b90617f268a.pdf", "supplementary_material": "/attachment/85be25283c575ba6f8254665f9680030041df402.zip"}, "replies": [{"content": {"summary": {"value": "TREERPO addresses the crucial limitation of trajectory-level model-free RL methods like GRPO: dence rewards. It introduces a novel approach for fine-grained intermediate step reward estimations for training LLM in deterministic settings. It leverages tree sampling to construct step-level groups for estimation of step-level rewards, extending the group-relative policy optimization (GRPO) framework. The method improves reasoning accuracy on mathematical benchmarks, outperforming GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Reward model-free step-level reward estimation: TREERPO addresses the crucial limitation of trajectory-level RL: dense rewards.\n\n2. Efficiency and performance gains: experimental results demonstrate that TREERPO significantly boosts Pass@1 accuracy by 2.9% over GRPO on multiple mathematics benchmarks and provide improved computational efficiency"}, "weaknesses": {"value": "1. There is no theoretical grounding provided for the proposed method: does it yield more effective estimates for the loss? Can we compare the variance of these estimates with those of GRPO? There should at least be some intuition, examples, formulas, or simulation studies. Since GRPO can be seen as a special case of the TreeRPO algorithmically (when the branching factor is 1), why does a greater branching factor lead to better results? To me, there is no clear intuition supporting this.\n\n2. In addition to the tree sampling approach used to estimate rewards, the authors introduce a KL divergence term in the objective function (Section 3.4). Without ablation studies, it is unclear which of these two factors contributes most to the model improvements observed in experiments. Could you provide such details?\n\n3. TREERPO does not appear to be the first method for dense step-level rewards in the model-free deterministic setting. What about \"Exploiting Tree Structure for Credit Assignment in RL Training of LLMs\" by Tran et al.? This paper was published two days before the ICLR deadline. Given the conference's high standards, a comparison between this submission and that paper is requested.\n\n4. The empirical evidence seems unconvincing: there is evaluation on only one architecture, Qwen2.5-Math (with two model sizes), and improvements are smaller for the larger model. Why does the improvement diminish with the larger model? The explanation at lines 317-318 states: \"the MATH training data for Qwen2.5-Math-7B is too simple.\" This sounds vague and unconvincing because (1) no references are provided, (2) how does this correspond with the larger Pass@1 for the 7B model? (3) More importantly, how is this expected to influence the difference in effectiveness between GRPO and TreeRPO in theory — or is it an orthogonal factor?"}, "questions": {"value": "See Weaknesses for explicit questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mWRWzr1iQE", "forum": "H6m5TIKl2W", "replyto": "H6m5TIKl2W", "signatures": ["ICLR.cc/2026/Conference/Submission9473/Reviewer_g2qw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9473/Reviewer_g2qw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767594350, "cdate": 1761767594350, "tmdate": 1762921060561, "mdate": 1762921060561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "JtTsIOjtiV", "forum": "H6m5TIKl2W", "replyto": "H6m5TIKl2W", "signatures": ["ICLR.cc/2026/Conference/Submission9473/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9473/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705286057, "cdate": 1763705286057, "tmdate": 1763705286057, "mdate": 1763705286057, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TreeRPO, which uses tree-based sampling to estimate step-level rewards without requiring an explicit reward model e.g. PRM. The method is an extension of GRPO, generating a tree at each decoding step to construct step-level groups. The contributions are as follows: introducing the first reward model-free method that provides dense reward signals through tree sampling, demonstrating significant performance improvements over GRPO, and showing improved token efficiency. \n\nThe strengths of the work are as follows. First, the paper touches on an important subject in which GRPO is a critical part of training today's LLMs. Second, the technical approach is generally sound, and the sampling approach is well-motivated. Finally, the paper is empirically strong and shows a large performance improvement on Qwen-2.5-Math-1.5B and outperforms GRPO.\n\nThe weaknesses are as follows. First, the sampling approach is quite computationally expensive. Although the approach is model free, it still requires a large cost for sampling. Second, the ablation studies could be improved e.g. with additional analysis on the branching factor or the depth of the tree. Finally, the model size tested is a bit small, and it's curious about how the method would do on larger model families."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the work are as follows.\n- First, the paper touches on an important subject in which GRPO is a critical part of training today's LLMs. - Second, the technical approach is generally sound, and the sampling approach is well-motivated.\n- Finally, the paper is empirically strong and shows a large performance improvement on Qwen-2.5-Math-1.5B and outperforms GRPO."}, "weaknesses": {"value": "The weaknesses are as follows.\n- First, the sampling approach is quite computationally expensive. Although the approach is model free, it still requires a large cost for sampling.\n- Second, the ablation studies could be improved e.g. with additional analysis on the branching factor or the depth of the tree.\n- Finally, the model sizes (1.5B, 7B) tested is a bit small, and it's curious about how the method would do on larger model families. Additionally, the performance gain is not as great with the 7B model, which is a potential concern."}, "questions": {"value": "Has TreeRPO been tested on non-math tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQCu6qoW2z", "forum": "H6m5TIKl2W", "replyto": "H6m5TIKl2W", "signatures": ["ICLR.cc/2026/Conference/Submission9473/Reviewer_1JLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9473/Reviewer_1JLT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867104898, "cdate": 1761867104898, "tmdate": 1762921060035, "mdate": 1762921060035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TreeRPO introduces a reinforcement-learning algorithm that supplies dense, step-level reward signals for fine-tuning LLMs on mathematical reasoning tasks without requiring a learned process-reward model.\nThe key technical idea is to (i) sample an N-ary tree of reasoning continuations up to a fixed depth, (ii) label every leaf with a verifiable outcome reward, and (iii) back-propagate expected returns bottom-up to obtain a pseudo-reward for every intermediate node.\nThese per-step returns are then used inside a GRPO-style group-relative policy objective.\nOn Qwen2.5-Math-1.5B the method improves Pass@1 from 19.0% → 35.5% on a 4-bench suite (MATH-500, OlympiadBench, Minerva, AIME24) while shortening average outputs by 18.1% relative to the GRPO baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: First reward-model-free RL approach that delivers per-step (process-style) supervision via tree sampling rather than learned PRMs or Monte-Carlo roll-outs\n- Empirical gains: Consistent +2–3% absolute accuracy improvements over a strong GRPO baseline on four maths benchmarks, with lower token cost\n- Clarity of method: The recursive reward back-propagation and group-level filtering are easy to implement and clearly described.\n- Well-scoped: Stays within verifiable-reward domains (math with known answers), avoiding annotation bottlenecks that plague PRM training."}, "weaknesses": {"value": "Missing baselines\n- No comparison with any PRM-based method (e.g., Math-Shepherd, Wang et al. 2024) or step-level RL (Step-DPO, Lai et al. 2024). Hence the claim “first to provide dense signals without a reward model” is not sufficient to establish superiority over existing PRM pipelines.\n\nStatistical rigor\n- Results are reported as single-run curves (Fig. 3) without standard deviations or confidence intervals. With only 500–1k test questions, variance can be high.\n\nAblations incomplete\n- Tree depth D=3, branching N=8, and pruning τ=0.1 are fixed without ablation.\n- Advantage re-normalisation  is motivated with a toy example but not ablated; it is unclear how much it contributes.\n\nLimited scale and scope of experiments\n- Main claims rely mostly on Qwen2.5-Math-1.5B; 7B results are limited and not systematically analyzed (sec. 4.1 mentions 7B but claims limited gains). The benefit may not hold at larger scales.\n\nStep segmentation heuristic\n- Steps are split by token length L_step=384 (sec. 3.1) which may break semantic boundaries and bias the per-step rewards."}, "questions": {"value": "- What is the compute overhead (GPU-hours) of tree sampling relative to GRPO under the same sample budget?\n- Did you try D=1 (i.e., GRPO with N roll-outs) and N=1 while keeping total samples fixed? This would isolate the tree back-prop contribution from mere sample diversity.\n- Why were PRM-based baselines not included in the experiments?\n- How sensitive are results to the pruning threshold τ? A quick sweep plot would suffice.\n- Do gains persist when response length is forced to be identical (e.g., via length penalty)? This checks whether accuracy lift is partly due to shorter, less noisy outputs.\n- What is the exact verification function implementation (rationale and code) used to evaluate leaf nodes? Is it identical for TreeRPO and GRPO?\n- Does TreeRPO still outperform GRPO on larger models (7B, 32B) or broader reasoning tasks beyond math? If not tested, can you provide preliminary results to underdstand the generality of the approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZFAg3VWY7A", "forum": "H6m5TIKl2W", "replyto": "H6m5TIKl2W", "signatures": ["ICLR.cc/2026/Conference/Submission9473/Reviewer_vCgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9473/Reviewer_vCgU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027357754, "cdate": 1762027357754, "tmdate": 1762921059692, "mdate": 1762921059692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}