{"id": "JAOOOgzVUl", "number": 21666, "cdate": 1758320295398, "mdate": 1759896909678, "content": {"title": "From Predictors to Samplers via the Training Trajectory", "abstract": "Sampling from trained predictors is fundamental for interpretability and as a compute-light alternative to diffusion models, but local samplers struggle on the rugged, high-frequency functions such models learn. We observe that standard neural‑network training implicitly produces a coarse‑to‑fine sequence of models. Early checkpoints suppress high‑degree/ high‑frequency components (Boolean monomials; spherical harmonics under NTK), while later checkpoints restore detail. We exploit this by running a simple annealed sampler across the training trajectory, using early checkpoints for high‑mobility proposals and later ones for refinement. In the Boolean domain, this can turn the exponential bottleneck arising from rugged landscapes or needle gadgets into a near-linear one. In the continuous domain, under the NTK regime, this corresponds to smoothing under the NTK kernel. Requiring no additional compute, our method shows strong empirical gains across a variety of synthetic and real-world tasks, including  constrained sampling tasks that diffusion models are unable to handle.", "tldr": "Use a predictor's training checkpoints as an annealing schedule to improve sampling", "keywords": ["sampling", "energy based models", "discrete sampling", "synergistic interactions", "markov chain monte carlo"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f45f9ac5191607e947b950f79ad23b2f4426a022.pdf", "supplementary_material": "/attachment/4d0eee762774f6226760146f6cc7577545f6e0c0.zip"}, "replies": [{"content": {"summary": {"value": "### Summary\n\nThis paper leverages a well-celebrated theoretical idea, i.e. the coarse to fine, spectral learning dynamics of gradient based learning to help create a series of landscape with different smoothness. Then use these landscapes as annealed landscapes to help sampling from the final complex energy landscape. They conducted theory on simple boolean settings showing the learning order effect in the degree of polynomial, and then validated the idea in various discrete and continuous sampling set up and showed significant improvement upon simply temperature annealed sampling and other MC methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "### Strength\n\n- The paper is tackling an interesting problem with a very creative solution, connecting ideas from spectral bias, training dynamics and energy based model and sampling. I’m very convinced of the idea.\n- The authors noted significant agreement with the theory in FCNN and MLP setting, and noted the deviation in transformer setting. We commend their honesty about the limitation of the theory.\n- The experimental testing of the idea is very comprehensive and convincing, showing univocal benefit of this idea."}, "weaknesses": {"value": "### Weakness\n\n- Often it’s not clear which training time check point the authors used in sampling experiments.\n    - e.g. in 4.1.2, it’s not clear from writing which training time checkpoint was used and what 1K and 10K steps denote. is it that 10K steps checkpoint is better intermediate landscape than 1K step?\n    - More generally, I feel there is a lot of heuristics and design space for which checkpoint(s) are best suit for these intermediate landscapes, and how do the authors decide on them?"}, "questions": {"value": "- C.f. Sec. 3.2, the theory / motivation of the paper also aligns closely with the learning dynamics of score-based diffusion models, i.e. learned score vector fields are simpler, smooth earlier in the training, usually better approximated by a linear vector field. There is a nice spectral ordering for the learning of vector field and distribution. [^1], [^2]\n\n[^1] Wang, & Vastola, (2024). The unreasonable effectiveness of gaussian score approximation for diffusion models and its applications. TMLR\n\n[^2] Wang, & Pehlevan (2025). An analytical theory of power law spectral bias in the learning dynamics of diffusion models. NeurIPS\n\n- For the idea of sampling leveraging a sequence of landscapes from smooth to rugged to help find tricky spiky solutions, the authors could also mention this recent work [^3], which shares a very similar picture, but did not use the learning dynamics to help. I feel [^3] could use many intuition / results from this paper to help learn their sequence of landscapes.\n\n[^3] Du, Y., Mao, J., & Tenenbaum, J. B. (2024). Learning iterative reasoning through energy diffusion. \n\n- In the continuous sampling experiments, is it relevant to compare to or report reference values for evolutionary algorithms? since these are also common functions used to benchmark those, e.g. CMAES.\n\n- Discussing how to best heuristically pick the intermediate checkpoint is quite interesting and useful for the reader of the paper.\n\nMinor\n\n- The Table 2 format is a bit confusing…. \nthe right column (step count) should not be compared on the same table with the mid and left column (success rate). A separatrix should be added or different table should be used to present the median step result. \nFrom the first glance, it’s confusing to see 4.00 and 2.00 in the right column…. and median step is an integer so why do we have 2 decimal points here\n\n- Table 5 and Sec. 4.1.2 are not very clear, which value is the temperature annealed GWG? why does it have two values as authors say they used the final checkpoint."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r5dtI3wDEp", "forum": "JAOOOgzVUl", "replyto": "JAOOOgzVUl", "signatures": ["ICLR.cc/2026/Conference/Submission21666/Reviewer_v2vM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21666/Reviewer_v2vM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794358177, "cdate": 1761794358177, "tmdate": 1762941881546, "mdate": 1762941881546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method called trajectory annealing, which uses intermediate training checkpoints of a predictor to guide sampling. The main idea is that during training, neural networks naturally evolve from coarse to fine representations, early checkpoints smooth high-frequency variations, while later ones add detail. By running MCMC samplers (GWG for discrete and MALA for continuous variables) sequentially across these checkpoints, the method achieves better mixing and sampling efficiency, especially in rugged or synergistic landscapes. Experiments on Boolean, MNIST-EBM, DNA design, and materials datasets show strong empirical gains over standard temperature annealing. The theory part connects this to SGD’s hierarchical learning of low- to high-degree monomials."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is conceptually simple yet powerful, requires no retraining or architectural change, and leverages an inherent property of neural network learning. The connection to hierarchical degree learning and NTK smoothing is novel. Results are extensive and consistent across very different domains. The Boolean case study is particularly compelling, showing exponential-to-linear mixing improvements."}, "weaknesses": {"value": "Theoretical results rely on strong assumptions (e.g., degree-wise alignment checkpoints) that may not generalize. Experiments are numerous but some seem cherry-picked to highlight advantages. Limited comparison with recent diffusion-based or amortized samplers. Some derivations could be more formal, and the transition from discrete to continuous domains feels hand-wavy. Also, claims of \"no extra compute\" ignore checkpoint storage and evaluation overhead."}, "questions": {"value": "How sensitive is the performance to checkpoint spacing or number of steps per checkpoint? Could this be integrated with modern optimizer schedules or adaptive checkpoint selection? How does it perform on large-scale, non-convex tasks like ImageNet classifiers? Is there any insight on when the coarse-to-fine property breaks down, e.g., transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "57Iyz34YaY", "forum": "JAOOOgzVUl", "replyto": "JAOOOgzVUl", "signatures": ["ICLR.cc/2026/Conference/Submission21666/Reviewer_45Q3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21666/Reviewer_45Q3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946711995, "cdate": 1761946711995, "tmdate": 1762941881030, "mdate": 1762941881030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes trajectory annealing: instead of sampling only from the final trained predictor $f^*$, the sampler runs short MCMC updates across saved training checkpoints $\\{f_t\\}$, exploiting the observed coarse‑to‑fine learning dynamics (early checkpoints damp high‑degree/high‑frequency components, later checkpoints restore detail)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Leveraging the existing training trajectory as an annealing schedule is elegant and creative and requires no re‑training or auxiliary generative model. The coarse to fine picture is illustrated empirically and theoretically (Apps A-B).\n- The method works for discrete (GWG) and continuous (SMC) domains. \n- The Hamming‑ball constraint is handled naturally within the MCMC framework and yields large gains on DNA (Table 7).\n- The $O(d\\log d)$ mixing is theoretically supported on degree-1/2 surrogates."}, "weaknesses": {"value": "- Compute parity (App J) is defined as matching the number of MALA steps, but SMC adds resampling/weight computations and multi‑checkpoint bookkeeping. MNIST also saves hundreds of checkpoints (50k training epochs saving every 100 steps). Reporting the wall‑clock time and memory would substantiate the “no additional compute” claim from the abstract.\n- For the discrete synthetic tasks the only baseline is GWG (authors state this choice explicitly), leaving out stronger informed/non‑local samplers (e.g., locally balanced and discrete‑Langevin families). This narrows the comparison.\n- The method does not apply to transformers (stated in the paper), which constrains scope for many modern applications.\n- Several evaluations measure best‑of‑run (DNA keeps the best of 60 steps per run), which is an optimization metric. Mixing diagnostic or distributional metrics would be helpful."}, "questions": {"value": "- With only a few steps at the final checkpoint, how biased are samples relative to the target distribution?\n- How do performance and compute vary with the number and placement of checkpoints (uniform vs geometric; early‑only vs full trajectory)? MNIST uses 500 checkpoints, but discrete synthetic uses just a few.\n- Have you tried saving checkpoints based on a performance metric instead of time? \n- The paper states diffusion “cannot” handle Hamming‑ball constraints (p. 8). Could you qualify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tw4ltnSqlb", "forum": "JAOOOgzVUl", "replyto": "JAOOOgzVUl", "signatures": ["ICLR.cc/2026/Conference/Submission21666/Reviewer_wkfn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21666/Reviewer_wkfn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185399740, "cdate": 1762185399740, "tmdate": 1762941880584, "mdate": 1762941880584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use intermediate checkpoints from training a predictor model (used to define an energy function) to anneal MCMC from easy to sample distributions to the target distribution, which may have pathological structure that makes it difficult to directly sample from. They provide hierarchical learning analysis to validate the intuition, and provide some experiments to verify that the method works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The idea of using the training checkpoints to perform some kind of annealing is novel to the best of my knowledge. I agree with their argument on how early stage checkpoints focus on high level features, late checkpoints focus on details, providing a sampling path. \n\nThe use of confidence intervals demonstrates thorough evaluation metrics. \n\nThe hierarchical learning analysis is also quite interesting."}, "weaknesses": {"value": "**Literature Review**: The paper does not mention [2] or [3], which are focused on accessing modes that are difficult to reach. [3] is more recent so the omission is understandable. \n\n**Base sampler**: Gibbs with gradient changes one coordinate at a time (at least in the default version), which makes it very slow. Are the benefits of annealing via model checkpoints preserved when using samplers proposed in [1, 2, 3]? Or does performance improvement from using this annealing strategy saturate with stronger base samplers? While GWG was chosen due to simplicity, it is important to note that more recent methods may solve this problem without the need for annealing across checkpoints. \n\n**Breadth of Metrics**: It would be nice to include metrics that show sampling accuracy v.s number of sampling steps. For the MNIST experiment, it could take the form of log maximum mean divergence v.s sampling steps. Also, the paper does not include (Effective Sample Size), which is an important metric for evaluating the efficiency [1]. \n\n**Characterization of Diffusion**: They characterize diffusion as requiring training over the entire trajectory instead of single step MLE. However, I am not sure that this is a fair characterization: in practice, the score matching objective is trajectory free [4]. Even in discrete diffusion, the training objective takes the form of corrupting the input and then predicting the clean input [5, 6, 7, 8]. For obtaining SOTA results on CIFAR-10, it might take up to 15 hours. But it is entirely possible that within the first 15 minutes, the model is capable of generating reasonable images. \n\nThis is important because the intuition behind the proposed method is extremely similar to the core idea of diffusion: start from a distribution that is easy to sample, and gradually anneal it to the target distribution. The difference is that diffusion enables this with a single checkpoint, whereas the proposed method requires several checkpoints. Furthermore, diffusion directly supervises learning of the score (which is what GWG requires). \n\nThis is perhaps my largest concern with the submission: it seems to be capturing the intuition of diffusion, but via a more indirect path. If the focus of this paper is small models (as discussed in the introduction), is training a diffusion model really that expensive? And if the focus is on larger models where diffusion training is expensive, then it would also be expensive to store multiple checkpoints of the model and run analysis across all the checkpoints to determine which ones to use for annealing the sampler. \n\nAlso, I do not see a reference to [9], which directly incorporates the logic of diffusion into MCMC to improve mixing via the same intuition presented in this submission. While [9] is focused on the continuous domain, it may be worth considering how to extrapolate their method to the discrete space via gradient based discrete samplers. \n\n[1] A Langevin-like Sampler for Discrete Distributions. Zhang et al. ICML 2022. \n\n[2] Gradient-based Discrete Sampling with Automatic Cyclical Scheduling. Pynadath et al. NeurIPS 2024. \n\n[3] Reheated Gradient-based Discrete Sampling for Combinatorial Optimization. Li, Zhang. TMLR 2025. \n\n[4] Elucidating the Design Space of Diffusion-Based Generative Models. Karras et al. NeurIPS 2022. \n\n[5] Simplified and Generalized Masked Diffusion for Discrete Data. Shi et al. NeurIPS 2024. \n\n[6] Simple and Effective Masked Diffusion Language Models. Sahoo et al. NeurIPS 2024. \n\n[7] Simple Guidance Mechanisms for Discrete Diffusion Models. Schiff et al. Preprint 2024. \n\n[8] The Diffusion Duality. Sahoo et al. ICML 2025. \n\n[9] Diffusive Gibbs Sampling. Chen et al. ICML 2024."}, "questions": {"value": "- How does this method perform when using DLP sampler? Are the gains of using checkpoints / using just the final checkpoint just as large? \n- In the introduction, it is stated that it takes 15 hours to train a diffusion model on CIFAR-10. Is there a citation for this? Are there plots of the loss v.s training time of diffusion methods for the tasks considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8EeLvlBSNn", "forum": "JAOOOgzVUl", "replyto": "JAOOOgzVUl", "signatures": ["ICLR.cc/2026/Conference/Submission21666/Reviewer_zyoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21666/Reviewer_zyoC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187310749, "cdate": 1762187310749, "tmdate": 1762941880160, "mdate": 1762941880160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}