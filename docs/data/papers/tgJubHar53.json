{"id": "tgJubHar53", "number": 14684, "cdate": 1758241669731, "mdate": 1759897355156, "content": {"title": "Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench", "abstract": "We present HumorBench, a benchmark designed to evaluate large language models’ (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.", "tldr": "Our novel humor comprehension benchmark shows that \"reasoning\" transfers to non-STEM domains.", "keywords": ["humor comprehension", "large language models", "reasoning evaluation", "benchmark dataset", "transfer learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80bf05112e069127d34c9f103264928260c1a115.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HumorBench, a benchmark designed to evaluate LLM ability to reason about and explain sophisticated humor in cartoon-caption settings. It highlights the limitations of existing humor datasets, which often conflate subjective funniness with objective joke comprehension, and instead proposes a framework focused on objective, verifiable joke elements. To address this gap, the authors curate about 300 cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock, annotate each with core joke elements, and build an autograder to assess whether LLM explanations capture those elements. Through experiments with frontier models, the study shows (1) strong transfer of reasoning skills from STEM benchmarks to humor comprehension, (2) surprising competence of models trained only on STEM reasoning tasks, and (3) mixed effects of test-time scaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper identifies a real gap in LLM evaluation: existing benchmarks overemphasize STEM reasoning, while humor demands cultural, linguistic, and inferential reasoning.\n\n2. The dataset is constructed with rigorous curation, including expert validation and removal of inconsistent annotations, contributing to dataset quality and reliability.\n\n3.\tIt provides systematic evaluation, tests across many models, compares base vs. reasoning optimized variants, and analyzes correlations with other benchmarks."}, "weaknesses": {"value": "1.\tThe size of the dataset may be limited. Although curated carefully, about 300 cartoon-caption pairs (499 elements) is relatively small compared to other reasoning benchmarks, potentially limiting generalization.\n2.\tNeed further justification for the dataset diversity, especially w.r.t. cultural background and humor type. Considering the original sources are largely Western (New Yorker, Cartoonstock), and the dataset may not test models across diverse humor traditions.\n3.\tThe validation shows a leniency bias (higher false positive rate), meaning scores are optimistic upper bounds. The reliance on LLM-as-judge could compromise fairness and transparency.\n4.\tWhile the benchmark shows transfer from STEM reasoning, the interpretation sometimes overstates the connection without fully probing why abstract reasoning transfers across domains.\n5.\tThe explanations of why models fail certain jokes remain somewhat descriptive rather than deeply diagnostic."}, "questions": {"value": "1.\tThe benchmark currently relies on textual descriptions of cartoons rather than the original images. How might removing the visual modality bias the evaluation? Do you plan to extend HumorBench into a multimodal setting?\n2.\tHow do you think HumorBench generalizes across different cultures and humor traditions? Do you envision multilingual or cross-cultural extensions?\n3.\tDo you have hypotheses about why this “inverse scaling” occurs, and what it implies about LLM reasoning dynamics?\n4.\tWould you incorporate both objective and subjective dimensions in future benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bazZeRfkSG", "forum": "tgJubHar53", "replyto": "tgJubHar53", "signatures": ["ICLR.cc/2026/Conference/Submission14684/Reviewer_ATma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14684/Reviewer_ATma"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799367900, "cdate": 1761799367900, "tmdate": 1762925053369, "mdate": 1762925053369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HumorBench, a new benchmark designed for evaluating humor understanding in models. HumorBench comprises 300 unique cartoon–caption pairs collected from the New Yorker Caption Contest and Cartoonstock.com, each annotated with human-defined rubrics. Unlike prior humor understanding benchmarks, HumorBench decomposes humor into several objective elements and assesses a model’s generated explanations against these elements using an automatic grading system. This approach aims to measure objective comprehension of humor rather than subjective interpretations. The authors evaluate several recent multimodal large language models (MLLMs) on HumorBench and present analyses that reveal their relative performance and limitations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The introduction of a rubric-based evaluation framework for humor understanding is novel and well-motivated.\n- The proposed HumorBench benchmark has the potential to offer valuable insights and guide future research in multimodal humor understanding.\n- The experimental design and analyses are comprehensive, effectively highlighting the limitations and challenges faced by current MLLMs in this domain."}, "weaknesses": {"value": "- The benchmark size is relatively small, containing only about 300 cartoons drawn from a limited range of sources (mainly The New Yorker Caption Contest and Cartoonstock.com). This restricted scope may limit the dataset’s generalizability and future applicability;\n- While the authors argue that using humor elements as rubrics enables objective humor understanding, this claim raises concerns. The definition and categorization of these humor elements may be subjective and culturally biased, as different annotators or readers could conceptualize these humor elements differently. The paper would benefit from a clearer justification of how these elements were defined and whether they adequately capture the diversity of humor mechanisms. Moreover, important annotation details are missing—including the number and backgrounds of annotators (particularly their cultural contexts), and whether inter-annotator agreement was measured to ensure annotation reliability;\n- The paper reports a correlation between STEM reasoning ability and humor understanding, but this interpretation may be confounded. The observed correlation could simply reflect that stronger MLLMs perform better across both STEM and Humor reasoning domains, rather than implying a causal link. Without a controlled experiment, for instance, fine-tuning the same base model on STEM data and comparing its humor understanding performance, the claim that enhancing STEM reasoning improves humor comprehension remains unsubstantiated;\n- Regarding Ethics Statement: Including the background of annotators and how you ensure the annotations are not biased."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J2MSSTyscN", "forum": "tgJubHar53", "replyto": "tgJubHar53", "signatures": ["ICLR.cc/2026/Conference/Submission14684/Reviewer_eZVy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14684/Reviewer_eZVy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808762537, "cdate": 1761808762537, "tmdate": 1762925052849, "mdate": 1762925052849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HumorBench, a benchmark designed to evaluate the ability of large language models (LLMs) to reason about and explain sophisticated humour in cartoon captions. It includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. The authors also conducted a rich experiment by benchmarking current SOTA models and provided several insights for future work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. HumorBench introduces a new non-STEM reasoning task that isolates objective humour understanding, avoiding confounding subjective funniness and providing a valuable probe of high-level reasoning.\n\n2. Through extensive experiments on frontier LLMs, the study reveals clear transfer from STEM reasoning to humour comprehension and mixed effects of test-time scaling, demonstrating both the benchmark’s sensitivity and the cross-domain generality of reasoning abilities."}, "weaknesses": {"value": "1. Besides identifying individual elements, the evaluation should also consider the interactions or causal relations among them. Humour understanding depends on how well the model connects these elements, not just mentions them. Element-based evaluation can serve as a supplement. In fact, using an LLM as a judge to compare the response with the golden label can already effectively reflect joke understanding.\n\n2. This conclusion is reasonable, but the paper does not explain why reasoning models improve humour understanding. The causes should be analysed further: Is the gain due to longer thinking tokens, potential training data leakage, or emergent self-search/retrieval behaviour of the model?\n\n3. Although the text-only setup makes the task more controllable and annotations clearer, it may oversimplify the problem. Under this setting, GPT-4o already achieves around 92% accuracy, suggesting that the task ceiling may be too low to differentiate between stronger models.\n\n4. Models with access to external tools (e.g., web search) may perform better in humour reasoning. Since the paper mentions that many hard cases involve hidden concept jumps or obscure references, evaluating models equipped with such tools would provide more meaningful insights."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SFkmt94wVy", "forum": "tgJubHar53", "replyto": "tgJubHar53", "signatures": ["ICLR.cc/2026/Conference/Submission14684/Reviewer_Ruae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14684/Reviewer_Ruae"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903197977, "cdate": 1761903197977, "tmdate": 1762925052517, "mdate": 1762925052517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a new benchmark based on the New Yorker Cartoon Contest and CartoonStock datasets, through re-annotation and the removal of images. Unlike previous datasets, HumorBench relies solely on objective elements that aid in humor understanding and excludes images, making it a cleaner benchmark for evaluating humor comprehension."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- During the annotation of the dataset, elements that could subjectively influence humor understanding were deliberately removed.\n- The authors avoid using multiple-choice and ranking formats, as these can limit the model’s reasonable divergence in humor understanding. Additionally, fixed options may inadvertently hint at the actual punchline, making it unclear whether the model is reasoning or simply guessing. This is a reasonable and effective improvement.\n- The authors designed a multi-round data refinement method that uses two large language models to alternately evaluate the data."}, "weaknesses": {"value": "- Some steps in the dataset construction process are not described in detail. (See Questions.)\n- The absence of images in the dataset may weaken its impact, as the main limitation of current multimodal large models lies in their inability to effectively understand humor from images."}, "questions": {"value": "- What is the criterion you use to distinguish between subjective and objective elements?\n- Why did you choose “whether it contains a joke element” as the evaluation criterion for the autogravder, rather than evaluating the complete output?\n- Do you have plans to extend the dataset to include a version with images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bR9m4oynTX", "forum": "tgJubHar53", "replyto": "tgJubHar53", "signatures": ["ICLR.cc/2026/Conference/Submission14684/Reviewer_rg5y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14684/Reviewer_rg5y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14684/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975357901, "cdate": 1761975357901, "tmdate": 1762925052140, "mdate": 1762925052140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}