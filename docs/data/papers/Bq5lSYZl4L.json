{"id": "Bq5lSYZl4L", "number": 10972, "cdate": 1758185883554, "mdate": 1763108330923, "content": {"title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "abstract": "Conversational agents must translate egocentric utterances (e.g., “on my right”) into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.", "tldr": "We introduce the Conversational Orientation Reasoning (COR) benchmark and propose a multimodal chain-of-thought framework for egocentric-to-allocentric orientation reasoning.", "keywords": ["conversational AI", "multimodal reasoning", "chain-of-thought", "spatial reasoning", "egocentric navigation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2f77af2d1fa2c0af3e89e935529eb29f33977f80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a Conversational Orientation Reasoning (COR) benchmark for Traditional Chinese conversational navigation. To solve this languages based, tabular egocentric-to-allocentric navigation task, authors further proposed a multi-modal chain-of-thought (MCoT) framework for fine-tuning a Taiwan-LLM-13B-v2.0-Chat. The proposed framework can solve the egocentric-to-allocentric with high success rate and showed robustness to noise in the conversation input.\n\nNevertheless, the benchmark itself consisted a simple 10x10 table, which could oversimplify the realistic task. One can imagine that the realistic task could involve high-dimensional input, e.g., images captured from the user or vectorized map. A multi-model LLM could have the capability to solve this high-dim to orientation mapping task, rather than tabular setup."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposed a novel problem and a solution on conversational orientation reasoning from language and semantic map. Experiments demonstrated the effectiveness and robustness of the proposed solution. The paper presented the problem and the method clearly."}, "weaknesses": {"value": "The major weakness is the proposed problem oversimplified real-world scenarios. This surprised the capability of the method as well as multi-model LLM’s capabilities. The current method seems can only solve an in-distribution 10x10 table and another novel 10x10 table. Reasoning the orientation from a table seems can be solve using hard-code."}, "questions": {"value": "What is the advantage of the proposed method over a set of hard-coded rules?\nCould proposed method reason from high-dimensional input, for example, from the photos captured from a user?\nHow capable is the method generalize to novel tables or even open-world language/landmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "CQmOKlbf17", "forum": "Bq5lSYZl4L", "replyto": "Bq5lSYZl4L", "signatures": ["ICLR.cc/2026/Conference/Submission10972/Reviewer_Kvna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10972/Reviewer_Kvna"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761352725548, "cdate": 1761352725548, "tmdate": 1762922163756, "mdate": 1762922163756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "tl1UEcTrv2", "forum": "Bq5lSYZl4L", "replyto": "Bq5lSYZl4L", "signatures": ["ICLR.cc/2026/Conference/Submission10972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10972/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763108330147, "cdate": 1763108330147, "tmdate": 1763108330147, "mdate": 1763108330147, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors propose a benchmark for conversational navigation that is in Chinese, a multimodal CoT framework, and a curriculum learning strategy to try and achieve high performance on this benchmark. Overall, the authors are able to achieve strong results on the proposed benchmark by fine-tuning an open LLM using their proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors achieved strong performances on the proposed benchmark.\n- The figures are clear."}, "weaknesses": {"value": "- Nothing proposed in the paper is unique or *novel*, everything done was a standard fine-tuning technique with an added curriculum and standard multimodal reasoning steps.\n- The central research questions are not particularly exciting, important, or impactful for the field\n- The model achieves 100% reasoning performance on on the proposed egocentric spatial orientation task, which signals two things to me:\n\t- First, the model is likely overfitting this task after being fine-tuned, resulting in a loss of much of its prior knowledge.\n\t- Second, the proposed benchmark is too easy.\n- While some of these ideas are briefly mentioned in the limitations, I see them as notable large problems. \n\t- The models are evaluated in an unrealistic environment (a 10x10) grid.\n\t- The authors only focus on a single language.\n- The proposed approach is complex, and not easy to implement."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5VdUvU7e3A", "forum": "Bq5lSYZl4L", "replyto": "Bq5lSYZl4L", "signatures": ["ICLR.cc/2026/Conference/Submission10972/Reviewer_zCUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10972/Reviewer_zCUP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868294798, "cdate": 1761868294798, "tmdate": 1762922163084, "mdate": 1762922163084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's topic is translating egocentric utterances (e.g., on my right) into allocentric (N/E/S/W) orientation in conversational navigation. It introduces COR, a Traditional-Chinese benchmark drawn from real urban layouts projected onto a 10×10 Manhattan grid, where inputs are ASR-transcribed speech plus landmark coordinates and outputs are cardinal orientations. The authors propose a multimodal chain-of-thought (MCoT) framework with a structured three-step reasoning recipe: (1) relation extraction, (2) coordinate to absolute direction mapping, (3) final orientation inference, which is trained via curriculum on Taiwan-LLM-13B-v2.0-Chat. On COR, MCoT reports 100% accuracy on clean transcripts and 98.1% with ASR transcripts, with additional robustness under linguistic variation, cross-domain transfer (Taipei Station), and referential ambiguity. The paper argues that structured reasoning improves both accuracy and interpretability for resource-constrained, GPS-limited scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The task is formalized cleanly with explicit mapping rules and a Manhattan grid, focused on the neglected egocentric to allocentric transformation rather than high-level action prediction, and does so in a non-English, ASR-noisy setting (Traditional Chinese). The three-step MCoT plus curriculum addresses reasoning stability and interpretability.\n2. The method addresses GPS-denied environments and resource-constrained deployments. The reported accuracy suggests potential utility for speech-driven navigation assistants where full maps/sensors are unavailable."}, "weaknesses": {"value": "1. The task on a 10 x 10 Manhattan grid with axis-aligned landmarks and a fixed rule table (Table 1) looks algorithmically solvable by a simple deterministic program: parse the relation and landmark, compute $\\Delta$, take `AbsDir(Δ)` by comparing $|\\Delta_x|$ vs $|\\Delta_y|$, then rotate by the relation. Without a strong non-neural baseline, the 100% clean accuracy is hard to contextualize.\nI suggest adding (i) a rule-based solver and (ii) a probabilistic variant robust to noisy extraction, and comparing accuracy and latency.  \n2. The ASR noise is introduced via a TTS to ASR loop on clean, templated text, not real user speech. CER-based severity is reported, but real conversational prosody, disfluency, and OOVs can be harsher.\n3. The grid is small, axis-aligned, and excludes diagonal cases; mapping rules are deterministic and known.\n4. Baseline choices and numbers look unusually weak.\n   Few-shot (with/without CoT) and fine-tuned no CoT baselines perform near chance or worse, which raises questions about prompt/format mismatches rather than inherent task difficulty.\n5. Data are programmatically generated from the same mapping rules used by the model’s step-2/3 supervision. Curriculum-tuning on these deterministic recipes could lead to memorization of rule templates rather than robust reasoning.\n6. Reasoning quality is reported as a match rate of intermediate steps and format error as schema violations, but neither measures *faithfulness* (whether the trace genuinely causes the answer).\n7. The scope of robustness is still narrow.\n   Cross-domain stays on a 10 x 10 Manhattan grid with the same rule table. Strong performance may reflect data homogeneity.\n8. The work motivates resource-constrained, GPS-limited settings but evaluates offline on a mid-size 13B chat model with LoRA.\nI suggest providing latency/memory profiles, and a small-footprint variant (e.g., 1–3B or distilled student) to substantiate edge deployment claims."}, "questions": {"value": "1. What is the performance and latency of a deterministic solver that (a) extracts the relation via a regex/IE component and (b) applies Table 1 + rotation? If omitted, why? \n2. How did you ensure the programmatic generation templates and the model’s step-wise supervision do not leak distributional shortcuts that trivialize the task?\n3.  Is the reasoning trace *required* to be correct for the final answer to be accepted (e.g., by an external checker), or can the model produce a correct label with an incorrect intermediate step?\n4. Beyond the TTS to ASR loop, do you have evaluations on spontaneous human speech (code-switching, disfluency, accents) recorded in the target locales?\n6. How would MCoT handle diagonal/off-axis landmarks, continuous coordinates, or non-orthogonal street plans? \n8. How sensitive is performance to removing supervision on *one* of the three steps (e.g., only supervise steps 1+3)? Any evidence that curriculum (vs. joint training) is the key driver?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pd0onPVjy7", "forum": "Bq5lSYZl4L", "replyto": "Bq5lSYZl4L", "signatures": ["ICLR.cc/2026/Conference/Submission10972/Reviewer_dPzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10972/Reviewer_dPzU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969993236, "cdate": 1761969993236, "tmdate": 1762922162688, "mdate": 1762922162688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}