{"id": "w7jkX7FfZ5", "number": 25415, "cdate": 1758367856850, "mdate": 1759896721794, "content": {"title": "Formal-Lagrangian Policy Optimization for Safe Reinforcement Learning in Code Generation with Differentiable Verification", "abstract": "\\begin{abstract}\nWe propose Formal-Lagrangian Policy Optimization (FLPO), an original framework of safe reinforcement learning (RL) in code generation that combines safe image inspection and policy optimization through a Lagrangian multiplier mechanism. The major bottleneck to RL-based code synthesis, however, is to ensure the constraints of hard safety, such as memory safety or type correctness, without losing the flexibility of generative models. FLPO addresses this by adding to the reward function a Lagrangian to dynamically penalise constraint violations, the penalty weight of which is adapted using the dual ascent to decrease the importance of safety issues downwards. Moreover, we propose a differentiable formal verification layer to approximate the verification results into a continuous value gradient so that the policy network can also learn straight from formal feedback. \n\\end{abstract}", "tldr": "", "keywords": ["Code Generation"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eca150f63d4f8ff01a5c7f0e6a9f4f1e5d598224.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Formal-Lagrangian Policy Optimization (FLPO), a framework for safe reinforcement learning (RL) in code generation that integrates formal verification with policy optimization through a Lagrangian multiplier. The approach models code generation as a constrained Markov decision process, where safety properties, such as type correctness and memory safety, are enforced as differentiable constraints. FLPO dynamically balances reward maximization and safety using adaptive Lagrangian penalties and a differentiable verification layer that approximates formal proof outcomes, enabling gradient-based learning from verification feedback. Empirical evaluations on Python and Solidity code generation benchmarks show that FLPO significantly reduces safety violations compared to prior RL methods while maintaining comparable functional correctness."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper explores a relevant topic in code generation.\n\n- The use of constrained reinforcement learning combined with differentiable verifiers is conceptually sound."}, "weaknesses": {"value": "- The paper contains insufficient content density; many sections have large unused spaces due to sparse figures and equations. These could be better utilized to improve clarity and presentation, see examples below.\n- The description of the verifier design lacks sufficient detail, and it is unclear why a Graph Neural Network is an appropriate choice for code snippet verification.\n- The reported training curves show an almost perfectly linear increase with training episodes, which appears implausible to me. I am unsure what additional sanity checks could be performed to clarify this behavior, but verifying it is important."}, "questions": {"value": "- The computational setup is unspecified, details such as GPU type, number of devices, and estimated wall-clock time are missing. Running RL for a 350M-parameter model over 100K episodes seems extremely large-scale and warrants clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7sosPnkXhR", "forum": "w7jkX7FfZ5", "replyto": "w7jkX7FfZ5", "signatures": ["ICLR.cc/2026/Conference/Submission25415/Reviewer_CmAJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25415/Reviewer_CmAJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796760140, "cdate": 1761796760140, "tmdate": 1762943426552, "mdate": 1762943426552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FLPO, a constrained reinforcement learning framework for safety that combines a Lagrangian formulation with dual ascent on safety multipliers, a differentiable verification estimator for predicting constraint violations, and a logarithmic barrier integrated into the policy-gradient update. The authors show that the model reduces safety violations while preserving functional correctness on safety-constrained tasks such as code generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper presents a clear constrained‐RL formulation and integrates a logarithmic barrier into the policy‐gradient update, which is interesting.\n\n+ The empirical results demonstrate favorable safety–correctness trade‐offs on Python code generation and Solidity smart‐contract tasks, performing competitively with post‐hoc approaches."}, "weaknesses": {"value": "-- Even with the reduced unsafe generations during training, a single unchecked failure can be critical when deployed. Can more details about this provide insight into how these kinds of safety failures can affect the model performance?\n\n-- The “novel constraints” evaluation (Table 3) is under‐specified. For example, a precise definition of what qualifies as novel, how these constraints differ from those seen in training, and a compute‐fair comparison against PHV are needed to clarify how FLPO’s differentiable barrier improves upon post‐hoc validation."}, "questions": {"value": "see above; also, \n\nCould more details be provided on the experimental setup? What safety constraints were used for the smart contracts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IbeDAvxs0S", "forum": "w7jkX7FfZ5", "replyto": "w7jkX7FfZ5", "signatures": ["ICLR.cc/2026/Conference/Submission25415/Reviewer_xuK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25415/Reviewer_xuK6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857798999, "cdate": 1761857798999, "tmdate": 1762943426354, "mdate": 1762943426354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a policy-optimization approach for safe reinforcement learning (RL) using Lagrangian multipliers."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Differentiable verification is a promising direction for efficiently improving the safety of RL."}, "weaknesses": {"value": "The manuscript requires substantial revision. The overall flow and clarity should be improved. The background, main approach, and experimental sections need much more explanation and discussion; in its current form, the paper reads more like an outline."}, "questions": {"value": "How do you plan to improve the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KjiONFSbNc", "forum": "w7jkX7FfZ5", "replyto": "w7jkX7FfZ5", "signatures": ["ICLR.cc/2026/Conference/Submission25415/Reviewer_4LQu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25415/Reviewer_4LQu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889016764, "cdate": 1761889016764, "tmdate": 1762943426078, "mdate": 1762943426078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}