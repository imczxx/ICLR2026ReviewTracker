{"id": "j6Y1ugNykX", "number": 7136, "cdate": 1758009084121, "mdate": 1759897871038, "content": {"title": "Can Large Language Models Express Uncertainty Like Human?", "abstract": "Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0–100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we 1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and 2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we 3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we 4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction. The code and dataset are anonymously available at \\url{https://anonymous.4open.science/r/Linguistic-Uncertainty-Dataset-051E}", "tldr": "", "keywords": ["linguistic confidence", "uncertainty estimation", "large language model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2e5dd61b989974bf1969518d31f07e0b1f9109e.pdf", "supplementary_material": "/attachment/1ba5b25042377354ed4f3398a8f459b508a2d936.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studies linguistic confidence (LC) in LLMs and aims to make LLMs express LC more reliably. Specifically, the authors (1) release a human-annotated dataset of hedging expressions, and (2) train a confidence mapper on the dataset to map hedging language to confidence scores. The authors also study different approaches to improve LC, including (3) carefully designed prompting and (4) supervised fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores different approaches to effectively improve LC in LLMs through extensive empirical evaluation. \n2. This paper introduces a new human-annotated dataset that includes diverse hedging expressions."}, "weaknesses": {"value": "1. While the authors claim in the introduction that “users do not typically phrase queries with explicit instructions”, their main empirical improvement relies on LC+ (with additional instructions in queries) in the experiments. This appears to contradict the previous claim. If LC only performs well with additional instructions, it makes the proposed confidence mapper less practical.\n2. The proposed dataset is heavily biased towards “moderate” level confidence expressions, likely because the authors take the average score of the human annotations. For example, the confidence score of phrases like “I believe” or “I think” can be inherently very subjective, and it can be either viewed as high or low confidence. Averaging these values would concentrate scores around a “moderate” level. This not only makes the dataset less reliable but also potentially makes the confidence mapper less effective. \n3. More assessment is needed for the fine-tuning framework. The proposed framework only performs well in one dataset. More empirical evidence is necessary to evaluate whether the proposed method generalizes to other datasets."}, "questions": {"value": "1. In the setting of the fine-tuning framework, the authors mention that they sample 200 questions from SimpleQA to construct the fine-tuning dataset. However, the fine-tuned model turns out to perform poorly on SimpleQA but much better on NQ-Open. Could the authors provide insights into this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3x2icnzPGN", "forum": "j6Y1ugNykX", "replyto": "j6Y1ugNykX", "signatures": ["ICLR.cc/2026/Conference/Submission7136/Reviewer_MG3s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7136/Reviewer_MG3s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761201096326, "cdate": 1761201096326, "tmdate": 1762919301653, "mdate": 1762919301653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs are increasingly human-facing. Human users need to be able to understand whether they can trust the responses. However, there is little work understanding people's percepts of linguistically-expressed uncertainty --- much less using this to inform ways to train models to better \"hedge\" their bets, in natural language. The authors contribute a new dataset of human uncertainty ratings, a classifier to map from natural language utterances to confidence level, and a new fine-tuning method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a substantive, solid paper! The authors make several contributions --- collecting a new dataset of human uncertainty interpretation/judgements; training a classifier on these data to map from naturalistic confidence scores to ratings; and the fine-tuning models to give better calibrated uncertainty scores. \n\nThe introduction is very well-written and the motivation is strong. I believe the authors' work can have a substantial contribution and hopefully spur more work around LLMs' uncertainty -- and particularly their uncertainty expression relative to human users!"}, "weaknesses": {"value": "While I think the author's work has the potential to be a strong and valuable contribution for the broader ICLR community, I do have a few questions on the human data processing and results. \n\nIn particular, I'm a bit concerned about the filtering the authors applied. Step 4 is based on the mean and stdev of 5 annotators’ responses? That’s really not many annotators to get good signal…. At least in my experience in human data annotation and cognitive science studies. Especially if it then led to just 3 annotators for many. Are you certain that the ones filtered out are noise, not genuine disagreement?\n- Is it possible to spot-check a random subset of these to assess agreement rates with your own team?\n- Especially given one (great) contribution of this work is producing a benchmark, it’s good to make sure the variability is genuine. \n\nI was also concerned about the lack of error bars for many of the results. How stable are the evaluations?"}, "questions": {"value": "In addition to the questions in my Weaknesses response: \n\n-  There are 5 validation checks — when did they appear in the 105? All at the beginning? Mixed throughout? 105 is a LOT of trials! How long did participants take on average? \nRelated to the above, it's potentially worth including a bit more detail on any training raters went through (can be in the Appendix); right now, that's only mentioned in a footnote.\n- Where did the decisiveness score come from in the prompts in D6? (e.g., the 0.6 one) Are these author-written or from existing datasets? \n\nMinor point (more of a comment)  The title asks — \"can LLMs express uncertainty like a human?\" But the paper, to my understanding, is more can LLMs express uncertainty in a way that is interpretable to people. They are related! But the current title could demand collecting human uncertainty expressions (but to my understanding, all expressions were LLM-generated; just rated by people)."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors conducted a new human study. It was not clear whether they had ethics review?"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FGV3lmIQSc", "forum": "j6Y1ugNykX", "replyto": "j6Y1ugNykX", "signatures": ["ICLR.cc/2026/Conference/Submission7136/Reviewer_wG75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7136/Reviewer_wG75"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609254916, "cdate": 1761609254916, "tmdate": 1762919300618, "mdate": 1762919300618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses primarily on Linguistic Confidence (LC), which is the approach of eliciting confidence of models via natural language modifiers (such as “I think that..” or “I am not sure, but…”). The paper makes the following contributions: first, it creates a dataset of LLM-generated uncertainty expressions, which are then scored by humans with regard to the level of certainty expressed. Next, it trains a lightweight mapper model on this dataset, that is then able to be used to convert general answers to uncertainty scores. This mapper is then evaluated for ECE and AUROC on a range of models, and compared to a variety of prior methods in the literature, with mixed results. In general, direct use of the mapper performs worse than preexisting state of the art on these metrics, but results improve with auxiliary prompting. Finally, the authors fine-tune a model to express linguistic uncertainty as measured by the best performing prior method, semantic uncertainty.\n\nOverall, although each individual component of the paper is reasonable in isolation, I struggle to glean a coherent overall motivation of the work. This is outlined in the weaknesses section below, particularly in points 1-3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organised and generally easy to follow.\n2. The range of experiments conducted – in particular, the number of models tested in Section 5 – is broad and appropriate\n3. The pipeline for LC dataset construction is well thought through, and the dataset is released publicly. I think this is likely to be a welcome contribution to researchers in this area."}, "weaknesses": {"value": "1. Although the motivation for LC is clearly stated (e.g. lines 50-53) to be in contrast with verbal elicitation methodologies such as “Please output your confidence along with the answer”, the LC+ method is essentially doing just that – though asking the model to do so in hedging language form rather than numerical form. Similarly, lines 49-50 claim certain prior confidence elicitation methods require “auxiliary networks, limiting their practicality”; but this is also true for the LC approach adopted by the authors (i.e. the mapper). And in comparison to logit-based methods, the authors state: “These methods are simple and inexpensive but require access to model logits, which are typically unavailable in commercial LLM APIs.” This is not typically true; most of the major closed-source APIs do provide sufficient logit access to perform the logit-based elicitation methods well.\n2. Section 6 attempts to directly address the point above of requiring extra verbal elicitation, by fine-tuning such behavior into the model. However, it does so using semantic uncertainty and an entirely newly-created dataset from strong model completions, not by building on Sections 3 and 4. Moreover …\n3. … it is extremely unclear in Section 6 exactly what the authors wish to claim about the efficacy of each method. First, they state that “Compared with the base model (LC) and its fine-tuned variant (LC (SFT)), our framework consistently improves both calibration and discriminability”. What is ‘our framework’ here? Is it LC+? If so, is the claim that LC SFT is not as good as LC+, and therefore that Sections 3 and 4 do constitute an important contribution? This claim however is not borne out by the results on NQ-Open, where LC SFT clearly outperforms LC+. So is the claim instead that LC SFT outperforms LC and LC+? Well, this is contradicted by the results of LC+ on SimpleQA; and moreover, this then begs the question of what the tangible contribution of Sections 3 and 4 are.\n4. Although the LC dataset constructed and released is claimed to be ‘diverse’, it consists only of questions from SimpleQA. Although I understand the costs associated are significant, it must be remarked that including more question variety would significantly improve the utility of the dataset.\n5. Related to point 2) above, evaluations are performed only on Q&A style datasets similar to SimpleQA. It is not clear if the results of the paper can be generalized to other settings, e.g. more reasoning-heavy evaluations.\n6. Line 48 claims that the P(true) method (Kadavath et al, 2022) is based on multiple generations, but it is actually a logit-based method.\n7. Appendix D.2 which gives detail on the P(true) implementation says that true-false token normalization is omitted. I do not understand the rationale for this; even if the other tokens have negligible probabilities as claimed (which is not always the case in my experience), there is no downside to performing the normalization. This design choice likely weakens the baseline performance.\n\nNote: The title of the paper, “Can Large Language Models Express Uncertainty Like Human?” is grammatically malformed."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tGfxorQlJX", "forum": "j6Y1ugNykX", "replyto": "j6Y1ugNykX", "signatures": ["ICLR.cc/2026/Conference/Submission7136/Reviewer_o6xa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7136/Reviewer_o6xa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739277784, "cdate": 1761739277784, "tmdate": 1762919299853, "mdate": 1762919299853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}