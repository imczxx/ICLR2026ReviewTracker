{"id": "vr36DTbxT9", "number": 14002, "cdate": 1758226712841, "mdate": 1759897397039, "content": {"title": "House, G.P.T.: Diagnosing Pathological Chain-of-Thought in Reasoning Models", "abstract": "Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. If models are incapable of performing harmful actions without reasoning efforts in the CoT, monitoring the CoT becomes a valuable tool for implementing safety guardrails. However, CoT reasoning may have properties which prevent it from being used for monitoring; we call these properties \"pathologies\". Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we present a systematic set of novel health metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop ``model organisms'', models deliberately trained to exhibit specific CoT pathologies, and demonstrate that our metrics can reliably diagnose these conditions. Crucially, we show that each pathology produces a distinct signature across our metric suite, enabling differential diagnosis between different types of pathologies. We apply our diagnostic framework to multiple open-weight frontier models, revealing their CoT health signatures and the prevalence of these pathologies in current systems. Our work provides the first practical toolkit for assessing CoT pathologies at scale, with direct implications for model interpretability, scalable oversight, and AI alignment.", "tldr": "We develop metrics to measure how pathological chain-of-thought reasoning is, and model organisms to measure metric effectiveness", "keywords": ["Chain of Thought", "Chain of Thought Metrics", "Model Organisms", "Pathological Reasoning", "Encoded Reasoning", "Internalized Reasoning", "Post-Hoc Rationalization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/276a34064009a5ef45e5608952f86923189a1e0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces three task-agnostic health metrics (reliance, paraphrasability, and substitutability) to diagnose pathological chain-of-thought (CoT) reasoning in large language models. The authors create \"model organisms\" - models deliberately fine-tuned to exhibit specific CoT pathologies (encoded reasoning, internalized reasoning, and post-hoc reasoning) - and demonstrate that their metrics can successfully differentiate between these pathologies. They apply their diagnostic framework to several open-weight models, finding that most display relatively healthy CoT signatures on GSM8K."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem for AI safety and model interpretability. The systematic approach to creating model organisms for each pathology type is well-structured, and the metrics are computationally inexpensive and model-agnostic, making them practical for deployment. The experimental methodology clearly demonstrates that each pathology produces distinct metric signatures, enabling differential diagnosis. The paper is generally well-written with clear explanations of the pathologies and metrics."}, "weaknesses": {"value": "My main concern is that the model organisms are created through highly artificial training procedures, which may not reflect how these pathologies would naturally emerge in practice. For the encoded reasoning organism, training on a fixed animal-themed codebook is far removed from realistic steganographic encoding that might develop during training (such as in [1]). I also notice a lack of references to existing LLM steganography literature. Similarly, the internalized reasoning organism is trained explicitly on lorem ipsum text, which seems unlikely to occur naturally. The post-hoc organism simply appends answers to questions, which is definitional rather than emergent. This raises questions about whether the metrics would successfully detect naturally-occurring pathologies versus these synthetic versions.\n\nFurthermore, the wild-type results are difficult to interpret. For instance, GPT-OSS-20B shows extremely high reliance scores (18.9), which the authors attribute to memorization of GSM8K answers. However, high reliance is supposed to be a sign of healthy CoT according to Table 1. This suggests the metrics may not be capturing what they intend to measure, or that the expected signatures in Table 1 are incomplete.\n\nThe metrics may also conflate training distribution effects with true pathologies. As the authors acknowledge, \"interventions like removing CoT or substituting it with random text may push some model families further off-distribution than others due to differences in their training procedures\" (Section 6). The paper does not provide a clear way to distinguish between low metric scores caused by genuine pathologies versus those caused by distribution shift, particularly for the substitutability metric.\n\nOther relatively minor weaknesses:\n- Despite using the word \"causal interventions\" (such as in Section 7 Conclusion), the metrics infer pathology from output statistics but do not verify internal causal mechanisms. This contrasts interpretability's normal definition of causal interventions.\n- The paraphrasability metric relies on another LLM (Gemini-1.5-flash) to generate paraphrases, introducing an additional source of potential bias and error. Different paraphrasers might produce different results, and there is no analysis of how robust the metric is to the choice of paraphrasing model. Additionally, the paper mentions using \"styles\" (short, polite, negative, verbose, typos, reversal) but provides limited justification for why these particular styles were chosen or how they relate to detecting encoded reasoning.\n\n---\n\n[1] Motwani, Sumeet, et al. \"Secret collusion among ai agents: Multi-agent deception via steganography.\" Advances in Neural Information Processing Systems 37 (2024): 73439-73486."}, "questions": {"value": "The paper lacks comparison to existing faithfulness metrics from prior work [Lanham et al., 2023; Tutek et al., 2025]. How do these metrics perform relative to or in combination with existing approaches? Are they complementary or redundant? This comparison would strengthen the contribution.\n\nThe paper acknowledges a critical limitation but does not adequately address it: \"Qwen3-8b should not be interpreted as a baseline for non-pathological CoT since various forms of unfaithfulness have been reported within reasoning models\" (Section 6). If the baseline model used to validate the metrics may itself have pathologies, how can we trust that the metrics are correctly identifying pathological versus healthy CoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0MRZ01Dh40", "forum": "vr36DTbxT9", "replyto": "vr36DTbxT9", "signatures": ["ICLR.cc/2026/Conference/Submission14002/Reviewer_CQyF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14002/Reviewer_CQyF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961524389, "cdate": 1761961524389, "tmdate": 1762924497575, "mdate": 1762924497575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers whether CoT explanations from large language models genuinely reflect their reasoning. It identifies three failure modes---post-hoc rationalization, encoded reasoning, and internalized reasoning---and proposes three corresponding diagnostic metrics: 1) reliance, 2) paraphrasability, and 3) substitutability. The metrics are validated on synthetically finetuned models (\"model organisms\") exhibiting these pathologies and applied to real open-weight models on GSM8K. The paper is timely and introduces a clear framework. However, its empirical scope is narrow (mostly GSM8K), interpretation of metric outputs can be unclear (large variance, unclear thresholds), and practical deployment challenges (e.g., intervention off-distribution effects, API constraints) are underexplored. Overall, it's an interesting contribution but would benefit from broader experiments without overclaiming and clearer guidance for practical use."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a timely and important problem: whether CoT is trustworthy for monitoring reasoning in LLMs.\n\n2. Clear taxonomy of three CoT pathologies and corresponding diagnostic metrics (reliance, paraphrasability, substitutability).\n\n3. Use of fine-tuned \"pathological\" models provides controlled validation of metrics.\n\n4. Empirical results on real open-weight LLMs add practical relevance.\n\n5. Writing is clear and limitations are explicitly acknowledged."}, "weaknesses": {"value": "1. Overbranding / overstated novelty: The term “model organisms” is unnecessary rebranding of controlled/synthetic fine-tuning, and the paper overclaims originality.\n\n2. Evaluation is very narrow---focused primarily on GSM8K math reasoning. unclear generalization to other reasoning tasks or domains.\n\n3. Metric interpretations are unstable (high variance, unclear thresholds), making \"healthy vs pathological\" judgments ambiguous.\n\n4. Practical deployment challenges (API-accessible models, off-distribution interventions, computational cost) are underexplored.\n\n5. No ground truth for \"pathology\" in real models/claims of healthiness are inferred, not verified."}, "questions": {"value": "1. Why is the term \"model organism\" preferable to simply \"controlled fine-tuned model\"? What conceptual or methodological value does the rebranding add?\n\n2. How do the metrics perform outside GSM8K (e.g., commonsense reasoning, multi-hop QA, or open-ended tasks)?\n\n3. Can you provide guidance or thresholds for interpreting metric values in practice? When is a model “pathological” vs “healthy”?\n\n4. How sensitive are the metrics to paraphrase quality, model size, or prompt design?\n\n5. For real-world deployment (especially black-box APIs), how feasible is it to compute these metrics without access to full log-probabilities or ability to modify CoT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySm790WCVC", "forum": "vr36DTbxT9", "replyto": "vr36DTbxT9", "signatures": ["ICLR.cc/2026/Conference/Submission14002/Reviewer_vNma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14002/Reviewer_vNma"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985293171, "cdate": 1761985293171, "tmdate": 1762924497131, "mdate": 1762924497131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an evaluation framework for detecting problematic CoTs in large reasoning models. The authors first characterize problematic CoTs as “pathologies” and divide them into three categories based on previous work: post-hoc rationalization, encoded reasoning, and internalized reasoning. They propose three metrics for evaluating these issues: reliance, paraphrasability, and substitutability. The measurement relies on comparing answer log probabilities before and after interventions on the CoT. To validate their metrics, they create “model organisms” by fine-tuning models to deliberately induce the pathological behaviors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper proposes new metrics to evaluate problematic behaviors in long CoTs from reasoning models. \n- The proposed metrics are model-agnostic, task-agnostic, and computationally cheap (just log probability comparisons). \n- The metrics capture different aspects of CoTs."}, "weaknesses": {"value": "**1. Model organisms may not reflect pathologies.**\n- I find the primary method of validating the metrics - fine-tuning capable models to exhibit some problematic behaviors - not convincing. It creates artificial cases that do not imply real unfaithful or pathological CoT cases in the wild. Real pathologies might emerge more subtly and be harder to detect. \n- All wild-type models tested appear healthy (Table 3), so we don't know if metrics work on real pathologies. \n- How well could the metrics detect pathologies \"in the wild\"?\n\n**2. Interpretations of the metrics are unclear.**\n- Is there a range (or bounds) for the metrics? Table 1 simply shows the metrics can be \"high\" or \"low\", but there is no way to quantify how high is too high. \n- Why are both high and low values possible for some pathologies? \n- Table 3: The use of log probs can make the metric values very extreme and less interpretable. Why are the values not normalized? \n\n**3. The pathologies are all based on prior work, making the paper's contribution weak.**\n- The described pathologies are all not new - CoT unfaithfulness is a longstanding problem that has been studied before reasoning models (https://aclanthology.org/2020.acl-main.386/, https://aclanthology.org/2023.ijcnlp-main.20.pdf). \n- The main contribution of the paper seems to be merely characterizing existing CoT behaviors in a different way. The metrics are only describing behaviors that have already been shown, so I find there is a lack of insights from the proposed evaluation scheme."}, "questions": {"value": "1. What does \"House, G.P.T.\" mean in the title?\n\nFor other questions, please refer to Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k34gW1Q3ah", "forum": "vr36DTbxT9", "replyto": "vr36DTbxT9", "signatures": ["ICLR.cc/2026/Conference/Submission14002/Reviewer_GfiK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14002/Reviewer_GfiK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997156922, "cdate": 1761997156922, "tmdate": 1762924496790, "mdate": 1762924496790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies several types of unfaithful reasoning, where the chain of though (CoT) of a reasoning language model (RLM) is misleading in some way. The three types studied are:\n1. Post-hoc rationalization - The response is already determined and the RLM constructs a rationale rather than reasoning to reach an answer.\n1. Encoded reasoning - The CoT is expressing some information, but in a way that the RLM understands and monitors do not.\n1. Internalized reasoning - Some or all of the true reasons for an RLM's response are not expressed in the CoT.\n\nThe paper proposes several metrics to test for these types of unfaithful reasoning. Since understanding the true relationship between a CoT and a final response is very difficult, the paper proposes to validate the metrics with RLMs that have been specifically fine-tuned to exhibit these behaviors. Experiments show that the metrics are accurate at recovering known instances of unfaithful reasoning. The paper then applies these metrics to popular RLMs and argues that they are mostly faithful in their reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper takes an interesting approach to studying a very important problem. The idea of creating deliberately unfaithful RLMs to validate metrics is novel to my knowledge.\n\n2. The metrics are straightforward and easy to implement.\n\n3. The metrics could see wide use as validation tools in the development and application of RLMs."}, "weaknesses": {"value": "1. I really do not care for dressing up the paper in the language of biology and medicine. I think referring to types of unfaithful reasoning as \"pathologies\" and fine-tuned RLMs as \"model organisms\" does not add anything to the paper. It only risks exaggerating hype around AI.\n\n2. As far as I can tell, all experiments were done on GSM8k. (Section 4.1 called \"Models and Datasets\" does not mention any datasets, so it is unclear.) This dataset seems likely to be similar to the training data for many RLMS. (In fact Section 4.3 guesses that some models have memorized this dataset.) It seems premature to declare these models mostly \"healthy\" if they haven't been evaluated on anything more challenging or novel to the models."}, "questions": {"value": "1. How different are the abilities between the \"model organisms\" and \"wild type\" models? Are the model organisms as accurate? Perhaps they are either not that realistic as models or perhaps poor performance would also reveal these \"pathologies?\"\n\n2. The text in Figure 3 is too small to read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "It9PmqazEF", "forum": "vr36DTbxT9", "replyto": "vr36DTbxT9", "signatures": ["ICLR.cc/2026/Conference/Submission14002/Reviewer_stHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14002/Reviewer_stHw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130542864, "cdate": 1762130542864, "tmdate": 1762924496406, "mdate": 1762924496406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}