{"id": "F4TgVGhxJc", "number": 6950, "cdate": 1758003204468, "mdate": 1762926868783, "content": {"title": "Towards Human-Preferences Chinese Rewriting Evaluation: Prompt-Based Scoring with Large Language Models", "abstract": "Sentence rewriting is a core task in natural language processing, encompassing paraphrasing, translation, and summarization. Despite its importance, existing evaluation metrics often rely on superficial similarity measures (e.g., BLEU, ROUGE), which fail to capture deep semantic fidelity. In this work, we propose a principled, multi-dimensional framework for evaluating rewriting quality based on semantic consistency, syntactic structure, lexical variation, and stylistic fidelity. We design a prompt-based scoring method with the QWQ-32B language model, achieving a Spearman correlation of $\\rho = 0.6121$ with human judgments, which is comparable to inter-human agreement ($\\rho = 0.6076$). We further benchmark popular rewriting strategies using this metric and introduce a multiround generation pipeline that improves rewriting quality by 9.66\\%. Our results show that large language models, when paired with structured evaluation and guidance, can robustly assess and generate high-quality rewrites.", "tldr": "", "keywords": ["Sentence Rewriting", "Semantic Consistency", "Large Language Models", "Text Generation Control"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9e1b84f1993c36a5d3a17d3bee5d4e2dc441f433.pdf", "supplementary_material": "/attachment/11199cd08eb47503c51ae33169a15f52327dc268.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a prompt-based, multi-dimensional framework for evaluating and guiding Chinese sentence rewriting. The method integrates semantic consistency, syntactic variation, lexical diversity, and stylistic fidelity into a unified evaluation scheme using large language models (LLMs), particularly QWQ-32B. The authors report that their LLM-based scoring achieves a Spearman correlation (ρ = 0.6121) with human annotation, comparable to inter-human agreement (ρ = 0.6076). The study also introduces a multi-round rewrite generation pipeline, demonstrating a 9.66% improvement in rewriting quality over traditional methods such as back translation or summarization. Overall, the paper addresses a relevant gap in LLM evaluation for rewriting tasks but could benefit from deeper theoretical analysis and broader generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and underexplored problem—evaluation of rewriting quality—by moving beyond superficial n-gram similarity toward structured, semantically grounded scoring.\n2. The proposed four-dimensional evaluation criteria (semantic, syntactic, lexical, and stylistic) are well-motivated and clearly defined, providing a systematic framework for human-like assessment.\n3. The empirical results are convincing, demonstrating that the QWQ-32B model aligns closely with human judgment, achieving near human-level agreement on a manually annotated dataset.\n4. The proposed score-guided multi-round rewriting pipeline is a practical and well-validated contribution that improves rewrite quality across multiple evaluation models (QWQ, GPT-5, DeepSeek)."}, "weaknesses": {"value": "1. While the framework is well-engineered, it remains primarily empirical and methodological, lacking theoretical justification for how the four evaluation dimensions interact or should be weighted within the overall scoring process.\n2. The current experiments focus solely on Chinese rewriting tasks, without demonstrating transferability to English or multilingual contexts, which limits the framework’s generalization and broader applicability.\n3. Human–model bias analysis is somewhat shallow; incorporating qualitative case studies of where model and human evaluations diverge would help identify systematic errors or strengths in the scoring model.\n4. Quantitative gains from the multi-round rewriting pipeline are promising, yet it is unclear whether these improvements reflect true semantic enhancement or overfitting to the model’s own scoring bias.\n5. Prompt design exploration remains limited; analyzing why multi-reason prompting reduces performance in smaller models and how to optimize prompt granularity would provide more actionable insights.\n6. Visualization and interpretability could be strengthened by presenting concrete rewriting examples, detailed score breakdowns, and side-by-side comparisons of human, BLEU, and LLM-based assessments to illustrate practical advantages."}, "questions": {"value": "1. How robust is the scoring framework across different rewriting tasks, such as summarization or simplification, which may have distinct semantic–syntactic trade-offs?\n2. Has the proposed metric been tested on out-of-domain or noisy data, such as user-generated text, to verify its stability and bias?\n3. Could a hybrid metric combining QWQ-based scoring and embedding-based similarity (e.g., BERTScore) further enhance reliability?\n4. How computationally intensive is the multi-round feedback pipeline, and is it scalable for large-scale rewriting datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mCKfEbkuSF", "forum": "F4TgVGhxJc", "replyto": "F4TgVGhxJc", "signatures": ["ICLR.cc/2026/Conference/Submission6950/Reviewer_2c2a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6950/Reviewer_2c2a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801090493, "cdate": 1761801090493, "tmdate": 1762919179191, "mdate": 1762919179191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qFbVMAgiDA", "forum": "F4TgVGhxJc", "replyto": "F4TgVGhxJc", "signatures": ["ICLR.cc/2026/Conference/Submission6950/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6950/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762926867059, "cdate": 1762926867059, "tmdate": 1762926867059, "mdate": 1762926867059, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission focusses on the evaluation of Chinese sentence rewriting. The authors compare humans and LLMs at the task of scoring rewrites with respect to 5 different criteria. The rewrites are generated using 4 different prompts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The 5 criteria are well-motivated\n- The superiority over symbolic/heuristic metrics is demonstrated effectively"}, "weaknesses": {"value": "My largest concern is that this work seems quite detached from related work. The weaknesses of symbolic metrics in many NLP areas are well-known and learnt / neural-network-based metrics have been used for years now. For example, the main MT metrics are learnt (BLEURT, COMET, etc.), not BLEU. Some of these do not use the source sentence (e.g. BLEURT), so they could serve as baselines in this work.\n\nEven more related, LM-as-a-judge is an active and quite large area of research, but the paper contains little reference to it (except a short note at the end of Sec. 2). This work could be a nice addition to this existing line of research, but unfortunately it does not position itself as such, and does not compare against relevant baselines from that area.\n\nMinor comments:\n- Formatting of citations: Use brackets\n- Tiny figures - I feel that this has become more common recently, but imo it should be a reason for desk-rejection if there is absolutely no chance of reading it on a print-out."}, "questions": {"value": "- How do you see your work in context of existing work around LM-as-a-judge or learnt NLP (e.g. MT) metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T8aRjSQVLV", "forum": "F4TgVGhxJc", "replyto": "F4TgVGhxJc", "signatures": ["ICLR.cc/2026/Conference/Submission6950/Reviewer_u3SR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6950/Reviewer_u3SR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901472456, "cdate": 1761901472456, "tmdate": 1762919178674, "mdate": 1762919178674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a prompting strategy for evaluation of the Sentence Rewriting task. As part of this research, they have collected a novel human-annotated dataset for this task, achieving moderate ~0.6 Spearman correlation between annotators. The effectiveness of the proposed approach is evaluated on the collected dataset and compared to baselines consisting of lexical overlap metrics and semantic similarity-based ones. The authors additionally propose a sentence rewriting pipeline, which incorporates the proposed evaluation approach to iteratively deliver better rewritings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The authors proposed effective prompting strategies for reasoning and non-reasoning LLMs to evaluate sentence rewriting task, focusing on Chinese language. According to the results, there is indeed a boost in evaluation quality as compared to the mentioned baselines.\nS2. The authors conducted extensive experiments on models of different sizes, which allows us to draw conclusions about scaling behavior of the proposed evaluation metric."}, "weaknesses": {"value": "W1. The related work section fails to mention a plethora of recently (and not so) published trained MT evaluation metrics, such as BLEURT, COMET, xCOMET, MetricX, GEMBA-DA/MQM/ESM or more general, such as Prometheus and M-Prometheus. Given that authors' motivation for this research is built upon flaws of existing evaluation metrics, it is a substantial weakness that they haven't discussed the most recent and most performant (as per benchmarks) of those metrics.\n\nW2. Stemming from W1, lack of comparison of the proposed method with most recent baselines is a flaw. This paper would benefit from comparison with such baselines as Prometheus or M-Prometheus, as they allow to customize evaluation rubrics.\n\nW3. Authors claim in L502-503, that the datasets used in this paper are publicly available, yet they are in fact not. There are no links nor access mode explanations for any datasets anywhere in the paper, including authors-collected evaluation dataset."}, "questions": {"value": "Q1. How well does your method perform in comparison to any of the recently published metrics (both MT-specific and more general)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BFDhYEx19u", "forum": "F4TgVGhxJc", "replyto": "F4TgVGhxJc", "signatures": ["ICLR.cc/2026/Conference/Submission6950/Reviewer_FUM3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6950/Reviewer_FUM3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986606986, "cdate": 1761986606986, "tmdate": 1762919178177, "mdate": 1762919178177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests an LLM based evaluation strategy for sentence rewrites. Specifically, it suggest a prompt that works by addressing the issues with current evaluation metrics like BLEU, ROUGE-L, which typically do not capture semantics very robustly and word-embeddings, Sentence-BERT which don’t capture variations very robustly. The provided method evaluates across 4 particular dimensions; semantic consistency, syntactic structure, lexical variation and style length. By designing prompts for scoring with models like QWQ-32B, the authors report a high Spearman correlation with human judgments (0.6121), close to inter-annotator reliability. \n\nSuggestions:  \n1\\. Line 078: This is just the rewrite of the previous paragraph. No need to separate it out if one immediately follows the other.\n\n2\\. Line 094-098: Check bracketing for the citations in this section.\n\n3\\. Line 359: Provide some explanations as well about why Qwen3, LLaMA3 show weaker and inconsistent results.. Give examples where it fails, do a thorough analysis.\n\n4\\. Line 448-451: What is the generation pipeline exactly ? How does it do it? This part is not clear from the figure and this paragraph. Can you give an example and elaborate on this?\n\n5\\. Line 459-461: It makes strong sense to continue and see if and where it saturates - at what iteration. Is CRE = 5 ideal, always, or are there situations where one might not present CRE so high? Does it not then become a thresholding based situation incase CRE is not equal to 5."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies some critical weaknesses in current metrics, highlighted in Table2, and proposes a prompt to mitigate those.\n    \n2. Comprehensive analysis on Chinese rewriting via multiple LLMs."}, "weaknesses": {"value": "1. Lack of clarity: Presentation of key parts of the papers in convoluted.\n    \n2. Code link is not available, weak rationales for parameter and design choices (see summary)\n    \n3. The rationale for observed weaknesses in models like Qwen3 and LLaMA3 is insufficient; the paper points out their inconsistency but does not provide concrete examples or thorough failure analysis."}, "questions": {"value": "1) Line 075: $\\rho$ = 0.593 here but in the summary it is mentioned as 0.6121. Why is there a discrepancy?\n\n2) Section 3.1: This section is very confusing. Is the task - rewriting semantically similar sentences or annotating them?  \nIn line 139 (Requirement 1) it is written \"express in different words..\"  \nIn line 129 it says \"..experts rated\"  \nIn line 161 it is \"we\" re-annotated 730 samples - Who are \"we\"? What was the original dataset and annotation, if they are being re-annotated?\n\n3) Line 137: What does the title of this table mean? Are you rewriting the quality scoring criteria  \nAlso Requirement 1 asks the annotator to rewrite the sentence, does it mean that they are scoring what they are writing? Should it not be just a scoring mechanism instead given a pair of sentences ?\n\n4) Line 424: Why was 5 chosen as the number of iterations? What was the deciding criteria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WS0jpLunOn", "forum": "F4TgVGhxJc", "replyto": "F4TgVGhxJc", "signatures": ["ICLR.cc/2026/Conference/Submission6950/Reviewer_9Fq9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6950/Reviewer_9Fq9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100100325, "cdate": 1762100100325, "tmdate": 1762919177768, "mdate": 1762919177768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}