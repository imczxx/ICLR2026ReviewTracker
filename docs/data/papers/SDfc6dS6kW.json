{"id": "SDfc6dS6kW", "number": 23665, "cdate": 1758346889573, "mdate": 1759896802287, "content": {"title": "RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and Behavior Regularization", "abstract": "In safety-critical domains  where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value conservatism and restricted policy classes, whereas expressive policies are only used in risk-neutral settings. Here, we address this gap by introducing the **Risk-Aware Multimodal Actor-Critic (RAMAC)** framework, which couples an *expressive generative actor* with a distributional critic. The RAMAC differentiates composite objective combining distributional risk and BC loss through the generative path, achieving risk-sensitive learning in complex multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching actors and observe consistent gains in $\\mathrm{CVaR}_{0.1}$ while maintaining strong returns on most Stochastic-D4RL tasks.", "tldr": "We guide expressive diffusion/flow policies with CVaR from a distributional critic, achieving safer offline RL (better lower tails) while keeping mean returns high and reducing OOD visitation.", "keywords": ["offline reinforcement learning", "risk-averse RL", "CVaR", "distributional RL", "generative policies", "diffusion model", "flow matching", "behavior cloning", "multimodal actions", "out-of-distribution (OOD)"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2be201768d08782d64e7138a3676d17e864e940.pdf", "supplementary_material": "/attachment/7eb1c62181432030eae0e6bd759ae6c02c5af943.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates risk-aware offline reinforcement learning (RL). The authors employ a distributional critic with quantile regression to capture value distributions, which are then used to compute the Conditional Value at Risk (CVaR) metric. Unlike conventional methods that optimize for expected values, this approach leverages CVaR to guide policy optimization toward high-return regions while simultaneously controlling risk. To enhance the expressiveness of the policy class, the paper further adopts diffusion and flow-based policies, which can effectively model complex, multi-modal distributions and thereby improve the quality of behavior constraints. Experimental evaluations on a stochastic variant of the D4RL benchmark show that the proposed method can improve expected returns while also reducing risk."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Adopting a distributional critic to improve not only the expectation of returns but also the long-tail returns is important\n2. The proposed idea is easy to follow and straightforward. \n3. The proposed method is easy to be implemented, can be a simple baseline for future researches on this direction."}, "weaknesses": {"value": "1. `Novelty` This paper seems to be a direct combination of 2 components: Diffusion Q Learning and distributional critic, which somehow lacks of novelty. Especially, this paper puts lots of efforts on Section 4 with 2 pages to emphasize the importance of utilizing expressive policy classes to conduct behavior regularization, but this has been widely studied in existing offline RL literatures, where using diffusion model or flow matching to model data distribution has become a popular choice. \n\n2. `Soundness` The results in Figure 3 are strange. To the best of my knowledge, we can easily tune the conservatism strength in diffusion Q Learning or Flow Q Learning to achieve the similar behavior of RADAC. So, Figure 3 (c, d) looks like lacking of hyperparameter tuning. Also, it seems that Figure3 (f-g) produces lots of OOD actions, which can be easily addressed through a small scale of pertubation. Considering the strange results in Figure 3, I cannot gaurantee the soundness of this paper.\n\n3. Figure 4 can somehow demonstrate that RADAC can produce safe behaviors, but I cannot see why. The objective of RADAC is to maximize the CVaR of `REWARD` and imposes no gaurantee of `safty`. It would be better to present the return distributions of the rollouted trajectories, which can directly reflect the training results of RADAC objective.\n\n4. `Theory` The authors take theories as a core contribution of this paper, but the theoretical insights have been widely studied in existing offline RL literature. For example, utilizing expressive policies can produce better results than unimodal policies."}, "questions": {"value": "Please see weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RebZk4gHrV", "forum": "SDfc6dS6kW", "replyto": "SDfc6dS6kW", "signatures": ["ICLR.cc/2026/Conference/Submission23665/Reviewer_dseA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23665/Reviewer_dseA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545532085, "cdate": 1761545532085, "tmdate": 1762942755540, "mdate": 1762942755540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RAMAC for offline risk-aware RL.  The RAMAC methodology combines an expressive generative actor with a distributional critic.  The actor-critic approach is trained to optimize CVaR for tail-based risk-sensitivity.  The loss function avoids out-of-distribution actions by incorporating a regularizer that encourages behavior cloning.  Experiments on stochastic D4RL tasks show improved CVaR in many settings, while simultaneously maintaining high average returns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "One of the biggest strengths of this paper is the empirical evaluation in Sec. 5.  The authors chose a strong set of baselines to compare with, and report, both, CVaR and average return across challenging instances.  The authors provide good insights on the behavior of OOD action selection, and provide some theoretical analysis that justifies why their method works well."}, "weaknesses": {"value": "Overall, my opinion is that this paper is not quite ready for publication in its current form.  The presentation requires a bit of polishing.  Indeed, the paper briefly covers many concepts without adequate and thorough discussion.  These include concepts of offline RL, risk-sensitive decision making, OOD actions, distributional RL, behavior regularization, and one of the most terse descriptions of diffusion methods that I have come across.  \n\nIn some ways RAFMAC feels like a bit of an afterthought in the way that it is presented.  There is almost no discussion of RAFMAC throughout the paper, with most attention being paid to RADAC.  The example result in Fig. 3 only evaluated RADAC (I realize that there is some evaluation of RAFMAC in the appendix).  Only Table 1 evaluates RAFMAC in any capacity, and frankly the results are not compelling for it.  To what extent should we evaluate RAFMAC as a core contribution of the paper?  If it is not intended to be a core contribution, and indeed doesn't perform that well, then perhaps it is better to relegate it to the appendix?\n\nThe authors claim that \"theoretical insight\" is a core contribution of this work.  However, the analysis amounts to a single Proposition and a conjecture, which is a bit limited as far as theoretical analysis is concerned.  In terms of the proposition, I am a bit unclear on the implications of this result, and in my mind the authors don't adequately explain.  The conjecture, on the other hand, is quite obvious--namely, that a multimodal policy should be more flexible (in terms of lower KL) than a unimodal policy.  Note that the authors don't properly define either of these policies.\n\nThe experimental setup is a bit unclear.  First, it is not clear how the 2-D contextual bandit experiment in Sec. 4.3 is designed.  From Fig. 3 it is unclear to the reader whether the RADAC result is desireable or not.  In fact, the authors state that \"RADAC concentrates near the safe center without losing multimodality,\" but by failing to capture the outer ring I would argue that it has failed to capture multimodality.  For Sec. 5 the description of the environments is somewhat lacking.  I am unclear on the risk signal in these environments without diving deeper into Fu et al. (2020).  Looking at the qualitative distribution plots in Fig. 4 I would argue that there is limited signal that RAMAC learns to avoid unsafe regions relative to baselines.\n\nFinally, because there are so many components to RAMAC, the authors should include a careful ablation study in the main text.  There are limited ablation studies in the appendix, but I think these need to be strengthened and included in the main text.\n\nMinor issues:\n* In the last paragraph on Page 1 you have double parenthesis on many references.\n* L139-140 : I am not sure what the following statement actually means \"our framework instead inject distributional risk signals.\"\n* The behavior cloning loss $\\mathcal{L}_{BC}$ is not explicitly defined"}, "questions": {"value": "See Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UmdmkvPgtM", "forum": "SDfc6dS6kW", "replyto": "SDfc6dS6kW", "signatures": ["ICLR.cc/2026/Conference/Submission23665/Reviewer_ktys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23665/Reviewer_ktys"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686944623, "cdate": 1761686944623, "tmdate": 1762942755034, "mdate": 1762942755034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles offline RL for safety-critical settings where one must learn purely from logged data but still avoid “catastrophic” outcomes. The authors propose RAMAC (Risk-Aware Multimodal Actor-Critic): pair an expressive, generative policy (either a diffusion model or a flow-matching model) with a distributional critic that models the full spread of possible returns instead of just the average. They then optimize a combined objective that (a) behavior-clones the dataset to stay “on-manifold” and (b) pushes the policy away from the lower tail of the return distribution by maximizing CVaR (a standard risk measure). This way, the policy remains expressive enough to capture multimodal behaviors but becomes explicitly risk-aware. Across Stochastic-D4RL benchmarks, the two instantiations generally improve upon baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the topic itself is important. I like that the paper targets risk in offline RL, because it seems directly relevant to safety-critical applications where exploration is impossible. \n\nThe idea of threading CVaR gradients through the full generative path of diffusion/flow actors seems a clean way to marry expressiveness with risk shaping. \n\nIt is interesting to see the Stochastic-D4RL study and the OOD-action analysis, it seems the methods improve CVaR on several tasks and show lower OOD rates than prior-anchor methods."}, "weaknesses": {"value": "On the diffusion/flow side, can you clarify how your approach differs from concurrent “distributional critic + diffusion” works that still optimize expectation—what concrete failure case of those methods does RAMAC fix (beyond the conceptual argument)?\n\nIt seems there is a missing related work. “A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization” by Luo et al. proposes a mixture-policy parameterization specifically to improve CVaR optimization efficiency by blending a risk-neutral and an adjustable policy to shape the lower tail. The risk-neutral part is trained by offline RL. It’s squarely in your problem space, even if their mechanism (mixture policy for CVaR) differs from yours (distributional critic + generative actor).\n\nSome more experiments: \n\nFirst critical thing. Your theory motivates expressive actors for lower forward-KL on multimodal data; can you test an explicit-likelihood actor (e.g., CVAE with improved decoding) under the same BC+CVaR objective to isolate whether the gains truly come from trajectory-based generation vs. expressiveness alone? \n\nFor fairness, you relabel rewards with the same hazard model used in evaluation; could you expand on potential label leakage risks and justify why this procedure does not advantage RAMAC versus baselines? A short sensitivity study to hazard severity would strengthen the claim. \n\ncould you add CODAC-CVAR and CQL-CVAR variants tuned specifically for lower-tail metrics, and also include high-confidence policy improvement (e.g., SPIBB-style) to show how RAMAC compares to explicit safety-first policies? This would better cover the design space you survey. \n\nYour results currently fix α=0.1; a risk-return frontier (vary α) would make the safety/return trade-off more transparent and would help practitioners pick α. I think this sensitivity should be important for risk-averse RL. \n\nIt is known that diffusion backprop through denoising steps can be heavy; it would be interesting to report computation time, and inference latency (vs. Flow) to guide adoption, and consider your suggested distilled one-step head as an ablation.  \n\nOOD detection robustness: the 1-NN detector is simple and sensible; could you replicate with kernel density or ensemble-based novelty measures to ensure the OOD trend (RADAC < ORAAC) is detector-agnostic?"}, "questions": {"value": "see above weaknesses. I will adjust my score depending on the author response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1G0v6eU3rX", "forum": "SDfc6dS6kW", "replyto": "SDfc6dS6kW", "signatures": ["ICLR.cc/2026/Conference/Submission23665/Reviewer_JUg6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23665/Reviewer_JUg6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866145900, "cdate": 1761866145900, "tmdate": 1762942753988, "mdate": 1762942753988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAMAC (Risk-Aware Multimodal Actor-Critic), a framework for offline reinforcement learning that integrates a generative actor with a distributional critic estimating return quantiles. The method combines behavior cloning to stay close to the data distribution with a CVaR-based risk term that shifts probability mass away from low-return outcomes while maintaining multimodal high-reward behaviors. Theoretical analysis links BC regularization to lower out-of-distribution probability, and experiments on Stochastic-D4RL tasks show improved CVaR performance and competitive mean returns compared to strong baselines such as CQL, CODAC, ORAAC, DiffusionQL, and FlowQL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Propagating distributional (CVaR) signals through diffusion/flow trajectories to directly reshape policy mass is an elegant way to combine expressiveness with tail-risk control; this is a nontrivial architectural idea with plausible benefits.\n2. The paper identifies an important gap in using expressive generative policies for risk-aware offline reinforcement learning and explains that previous approaches either restricted policy expressiveness or remained risk-neutral. The motivation and taxonomy of prior mechanisms are well organized.\n3. The paper identifies an important gap in using expressive generative policies for risk-aware offline reinforcement learning and explains that previous approaches either restricted policy expressiveness or remained risk-neutral. The motivation and taxonomy of prior mechanisms are well organized."}, "weaknesses": {"value": "1. Backpropagating CVaR through a multi-step diffusion or flow trajectory may be computationally expensive; the paper does not quantify training time, memory, or step cost relative to baselines (DiffusionQL / FlowQL) which could matter in practice.\n2. The ablation shows alternatives (Wang, CPW) but more analysis is needed on when CVaR is preferable and trade-offs (variance, mean vs tail). The role of K (tail samples) and N (quantiles) on estimator variance could be expanded."}, "questions": {"value": "See weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wHUvTxDA4V", "forum": "SDfc6dS6kW", "replyto": "SDfc6dS6kW", "signatures": ["ICLR.cc/2026/Conference/Submission23665/Reviewer_CBrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23665/Reviewer_CBrd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965122906, "cdate": 1761965122906, "tmdate": 1762942753653, "mdate": 1762942753653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}