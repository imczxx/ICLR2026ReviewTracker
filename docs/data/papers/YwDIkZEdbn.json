{"id": "YwDIkZEdbn", "number": 14988, "cdate": 1758246536498, "mdate": 1759897337398, "content": {"title": "Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting", "abstract": "Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities. Existing solutions attempt to mitigate this tension via regularization, selective parameter updates, or data-centric replay, but each imposes significant costs in computation, data access, or adaptability. Recent work has shown that training signals can be compressed to subsets of logits without severe accuracy loss, suggesting a path toward efficient adaptation. However, naïve truncation destabilizes optimization and exacerbates forgetting. \n\t\n\tWe introduce Logits Replay + MoClip, a two-stage framework that compresses supervision in the logit space and stabilizes optimization at the update level. In Stage0, we record dynamic Top-$K$ token subsets that cover a probability threshold, always including the gold label. In Stage1, we replay these compact subsets to compute exact renormalized losses, avoiding full softmax computation and implicitly regularizing. To ensure stability, we design MoClip, an optimizer that caps gradient–momentum rotation and applies an $\\arctan2$-based rescaling of updates. Empirically, our method improves domain performance on Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over 40\\%. Together, these contributions offer a scalable, architecture-agnostic path for domain adaptation of LLMs without sacrificing generalization.", "tldr": "We propose Logits Replay + MoClip, combining dynamic top-K supervision with a stability-aware optimizer. It boosts domain accuracy, preserves general skills, and cuts training cost by 40%+.", "keywords": ["Logits Replay", "Domain Adaptation", "Dynamic Logits Collection", "Restricted Softmax", "Fine-tuning Efficiency", "Catastrophic Forgetting Mitigation", "Stability-aware Optimization", "Gradient–Momentum Angle Clipping", "Atan2-based Scaling"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9b4b51d5b5eb006a60e6b6027abdc4b0c2b03a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a finetuning technique for LLMs for a domain specific data in such a way that it prevents forgetting on the domain(s) included in pretraining. Their post training framework has two stages. The first stage runs a forward pass through the model for the finetuning data and collects the top-k pre-softmax logits and their indices. The second stage does the actual training over the same data but the softmax operation is restricted to the top-k indices selected in the first stage. For this training, they introduce an optimizer which they name MoCLIP. MoCLIP is a variant of ADAM with two differences : they use Gradient-angle momentum scaling and an Atan2 scaling on the update for the second moment. These ensure that the direction of the update does not cause forgetting on the original data and also bounds large updates whenever the variance is close to zero. They perform experiments on 3 datasets on the QWEN models and compare their results to baselines like MoFO. Ablations show the impact of different hyperparameters for MoCLIP as well as different logits selection techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea, experimental settings, baseline comparisons, ablations as well as overhead analysis is clearly presented and I was able to follow the mathematical details outlined in the paper for MoCLIP. I think that in general, the idea of trying to balance the amount of adaptation while  not moving too far away in weight space to forget pretraining skills is worth studying and the approach certainly tries to do that. Their experimental results demonstrate that their clipped optimizer along with a sparse vocab does mitigate forgetting while performing well on the new datasets."}, "weaknesses": {"value": "My main issue with the paper is the improvement over one of the baseline methods presented in the paper especially when it comes to the delta in forgetting on the pretraining benchmarks. In most cases the delta is < 1\\% which seems to suggest that previous methods work pretty well. Also, in my opinion, what the authors classify as catastrophic forgetting is a bit of an overstatement. A degradation of a couple of percentage points or more does not amount to catastrophic forgetting. Experiments are limited to QWEN models for which there is a limited understanding of the pretraining data. I'm not sure whether the datasets used for finetuning are standard datasets for evaluating performance on out-of-distribution tasks."}, "questions": {"value": "Don't logits collected in Stage 0 become stale once you begin training and the model updates? Is there something to be done about that and perhaps extract more performance out of FTing?\n\n\"Stage 0 records dynamic Top-K logits per position\". The word logits in this paper is sort of assumed to be the pre-softmax activations in the vocab projection layer. However, logits are usually just referred to in the literature as activations at any layer. Please clarify this. \n\n\"rescaling to bound step sizes without relying on ϵ\". ϵ is not defined. Please make it clear that it is the learning rate"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bmQs2yKgtt", "forum": "YwDIkZEdbn", "replyto": "YwDIkZEdbn", "signatures": ["ICLR.cc/2026/Conference/Submission14988/Reviewer_wmfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14988/Reviewer_wmfT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760729530731, "cdate": 1760729530731, "tmdate": 1762925316719, "mdate": 1762925316719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Logits Replay+MoClip, a two-stage post-training recipe to stabilize and cut the cost of fine-tuning LLMs. Specifically, stage-0 records per-token Top-K candidate sets (always include the gold token) and stage-1 trains only on these subsets with a re-normalized cross-entropy;  It also uses MoClip, an AdamW variant with hard gradient–momentum angle clipping and an atan-based step-size bounding. Experiments on Qwen3-4B and 8B shows improvements on CT and NL2SQL tasks over strong baselines, mitigates forgetting on MMLU/BBH/GPQA/MATH, and cuts end-to-end training time by roughly 40%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive experiments and ablation studies\n  - The paper tests across different model sizes and a mix of tasks (CT, NL2SQL, MMLU/BBH/GPQA/MATH), beating strong baselines. It also runs clean ablations on dynamic Top-K, keeping the gold token, re-normalizing the subset loss, and MoClip’s angle vs. step controls, so it’s easy to see which parts boost stability, accuracy, and efficiency.\n- Practical efficiency and easy for adoption\n  - In this paper, Logits Replay reduces expensive full-vocab softmax computations by training on Top-K subsets, and MoClip is a drop-in AdamW variant requiring only minimal code changes. So the approach is easy for adoption. In practice, it delivers great wall-clock savings and preserves general capabilities.\n- Overall, I think this paper is of good quality."}, "weaknesses": {"value": "- Lack of comparsions with logits-based baselines. \n  - As far as I know, the paper mainly compares ite performance against optimizer/momentum related baselines, missing the comparsion with logits-based methods (like Baichuan4-Finance as you mentioned).\n- MoClip hyperparameters add tuning complexity.\n  - The best settings for MoClip may vary across tasks and model sizes; the method’s sensitivity and robustness to these choices are unclear. A systematic sensitivity study would be better."}, "questions": {"value": "- Could you provide results of some logits-based methods (e.g., teacher top-K distillation, KL to a reference model like Baichuan4-Finance)? Or if it's not easy to conduct, what do you expect these logits-based methods perform and why?\n- Do the best settings of MoClip transfer across model sizes (smaller→4B→8B→larger) and tasks? Is there a better approach to selecting the best settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dRxB0UnwTK", "forum": "YwDIkZEdbn", "replyto": "YwDIkZEdbn", "signatures": ["ICLR.cc/2026/Conference/Submission14988/Reviewer_Ngvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14988/Reviewer_Ngvc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929107603, "cdate": 1761929107603, "tmdate": 1762925316284, "mdate": 1762925316284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose LOGITS REPLAY + MOCLIP (LRM): a combination of self-distillation, Adam A-tan2, and torque-aware momentum. The authors combine these techniques to solve the problem of continual learning in LLM pre-training settings. In their empirical study, they show that LRM outperforms a number of fine-tuning baselines from the literature, including adaptive Muon and Muon clip across a number of benchmarks. They also show that LRM leads to fewer or the same amount of loss spikes as other methods and that their method is significantly more computationally efficient."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I like the idea of leveraging distillation and alignment-aware updates for continual learning. \n- I like the author’s comparison of stability through the number of loss spikes. Training stability is seldom measured in existing work but is essential to practical training.\n- The authors' experiments clearly illustrate the problem of forgetting and how Logits Replay + MoClip can help to combat it."}, "weaknesses": {"value": "- My main concern is the lack of a strong baseline to compare against. While the authors train a number of baselines, none leverage standard continual learning techniques, such as replay [1]. For instance, the authors could easily add replay by using standard high-quality web-scraped data (e.g. the high-quality portion of nemotronCC [4]), which would be comparable to replaying the pre-training data of QWEN 4B and 8B. This would help to strengthen the claims that Logit Replay + MoClip reduces forgetting.\n- Following from my previous concern, it seems that AdaMuon with no replay has nearly as much retention as Logit Replay + MoClip. This leads me to wonder how well AdaMuon would work if it were equipped with replay.\n- My second strongest concern is the lack of clearly reported hyperparameters. No hyperparameter tuning is reported for baselines in section 3, and only limited details are provided for Logit Replay + MoClip. Without understanding how well the hyperparameters were tuned, it is difficult to draw conclusive results.\n- Finally, my last concern is about novelty. Distillation to prevent forgetting[5], torque-aware momentum[1], and Adam-arctan2 [2] are already well-known techniques in the literature. Combining them certainly yields a novel technique (e.g., one not previously evaluated in the literature), but the lack of strong baselines and reported hyperparameters makes me wonder if the combination is warranted. \n\n\n[1][TORQUE-AWARE MOMENTUM]\n\n[2][Scaling Exponents Across Parameterizations and Optimizers]\n\n[3][Simple and Scalable Strategies to Continually Pre-train Large Language Models]\n\n[4][Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset]\n\n[5][iCaRL: Incremental Classifier and Representation Learning]"}, "questions": {"value": "- I don't understand how your technique speeds up training. Could you elaborate on this? \n- Were hyperparameters swept for all baselines? Was each optimal value selected an interior point of the values considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4YsuccdzgU", "forum": "YwDIkZEdbn", "replyto": "YwDIkZEdbn", "signatures": ["ICLR.cc/2026/Conference/Submission14988/Reviewer_Et4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14988/Reviewer_Et4X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318885552, "cdate": 1762318885552, "tmdate": 1762925315881, "mdate": 1762925315881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}