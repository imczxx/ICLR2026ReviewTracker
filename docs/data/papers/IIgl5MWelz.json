{"id": "IIgl5MWelz", "number": 15290, "cdate": 1758249854207, "mdate": 1759897315963, "content": {"title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?", "abstract": "Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a \\emph{prosperity-before-collapse} phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Proxy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.006% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six model scales (1.7B–32B) and eight reasoning benchmarks shows that M2PO delivers stable off-policy training even with data stale by *at least 256 model updates* and matches on-policy performance.", "tldr": "", "keywords": ["Off-policy RL; LLM; Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82b31c494e0d477ff8e22ae2814f3d64866c94e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces M2PO (Second-Moment Trust Policy Optimization), a novel off-policy reinforcement learning algorithm designed to address the challenges of training large language models (LLMs) with stale data. Current on-policy RL methods, while stable, are inefficient and unscalable due to their reliance on fresh rollouts. Off-policy methods, which decouple rollout generation from training, often suffer from performance degradation or collapse when data staleness is high.\n\nThe authors identify a \"prosperity-before-collapse\" phenomenon where stale data can initially be as informative as on-policy data if properly exploited, but existing methods like GRPO fail due to excessive clipping of informative high-entropy tokens. M2PO tackles this by constraining the second moment of importance weights, effectively suppressing extreme outliers while preserving valuable informative updates. This approach dramatically reduces token clipping under high staleness, leading to stable optimization.\n\nThe paper demonstrates that M2PO achieves performance comparable to on-policy GRPO even with data stale by at least 256 model updates, across six models (1.7B to 32B) and eight math reasoning benchmarks. M2PO is shown to be robust to its threshold hyperparameter, simplifying its practical application. This work highlights the potential of stale data in RL for LLMs and provides a more stable and efficient training solution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents several strengths that would be beneficial for reviewers to consider:\n\n- **Addresses a Critical Problem in LLM Training:** The paper tackles the significant challenge of efficiently training LLMs with reinforcement learning. The reliance of traditional RL on fresh data (on-policy) is a major bottleneck for scalability, and M2PO offers a practical solution to leverage stale data.\n- **Identifies a Novel Phenomenon:** The \"prosperity-before-collapse\" phenomenon is an intriguing observation that sheds light on the potential of stale data. This insight forms the foundation of their proposed method.\n- **Proposes a Well-Motivated Solution (M2PO):** M2PO directly addresses the identified problem of existing methods inappropriately clipping informative tokens. By constraining the second moment of importance weights, the algorithm effectively differentiates between noisy outliers and valuable high-entropy tokens, leading to more stable and efficient learning.\n- **Strong Empirical Evidence:** The extensive evaluation across six different LLMs (ranging from 1.7B to 32B parameters) and eight math reasoning benchmarks provides compelling evidence of M2PO's effectiveness. The consistent performance parity with on-policy methods, even under extreme data staleness, is a key highlight."}, "weaknesses": {"value": "1. Computational Overhead of M2PO and Masking: The computational overhead associated with calculating KL divergence, which can involve operations at the scale of $VocabularySize \\times TokenNumber$, and the iterative nature of the masking algorithm (Algorithm 1) are not thoroughly detailed. More explanation is needed regarding the practical implications of these computations, including tensor storage requirements and time complexity, especially when dealing with very large language models and extensive vocabularies.\n\n2. Limited Evaluation on Small-Degree Staleness and Practical Relevance of Extreme Staleness: The paper primarily focuses on evaluating M2PO under large-degree staleness (e.g., 256 model updates), while several real-world RL training setups and recent works (e.g., DAPO with 16 gradient updates per rollout, or AREAL uses even smaller degree of staleness) demonstrate that smaller degrees of staleness (e.g., 4, 8, or 16) can be sufficient for maximizing efficiency. This raises two important questions:\n\n   - How does M2PO perform under these more commonly encountered small-degree staleness settings? A comparison in these scenarios would provide a more complete picture of its practical utility.\n   - What is the necessity of large-degree staleness (e.g., 256) in real-world RL training for LLMs? Further exploration of this aspect could clarify the specific contexts in which such high staleness is beneficial or required."}, "questions": {"value": "In my understanding, the experiments only involves using stable data (i.e., data sample with old behavior models) but doesn't involve any reusing of sampled data (e.g., one sample prompt can be used more than once in gradient updating), am I right?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XgWrZqBxdO", "forum": "IIgl5MWelz", "replyto": "IIgl5MWelz", "signatures": ["ICLR.cc/2026/Conference/Submission15290/Reviewer_8kZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15290/Reviewer_8kZV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634263834, "cdate": 1761634263834, "tmdate": 1762925588083, "mdate": 1762925588083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of performance degradation in Reinforcement Learning (RL) for Large Language Models (LLMs) when trained on stale, off-policy data. The authors identify a \"prosperity before collapse\" phenomenon, where training without a trust region initially performs well but eventually fails.\n\nTo harness this potential while ensuring stability, they propose M2PO (Second-Moment Trust Policy Optimization). This algorithm constrains the second moment of importance weights, which selectively masks only extreme, high-variance tokens while preserving informative updates from stale data.\n\nExtensive experiments show that M2PO enables stable training even with data stale by 256 model updates, matching on-policy performance and significantly outperforming existing methods across various model scales and reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an interesting observation that training without clipping on stale data can initially outperform clipped training, highlighting a \"prosperity-before-collapse\" phenomenon.\n\n- The proposed method, M2PO, is intuitive and easy to follow. It selectively excludes tokens that cause extreme increases in the batch-level second-moment metric during loss propagation."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "- Regarding the \"stale-k\" setup: Is the model using a policy from k training steps earlier to generate the current training batch, or is it directly using data collected k steps ago? If it’s the former, I suggest using the term \"stale model\" for clarity.\n\n- Why is such a large staleness (e.g., s=256) used? While off-policy training is an important challenge in RL post-training, techniques like Truncated Importance Sampling (TIS) [1] can help maintain on-policy behavior. Could the authors compare M2PO with TIS under similar settings?\n\n- Including an ablation study with different staleness levels for M2PO would help better understand its robustness and performance across varying off-policy gaps.\n\n[1] Your Efficient RL Framework Secretly Brings You Off-Policy RL Training. https://fengyao.notion.site/off-policy-rl"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DvY2UAXAEt", "forum": "IIgl5MWelz", "replyto": "IIgl5MWelz", "signatures": ["ICLR.cc/2026/Conference/Submission15290/Reviewer_D8e7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15290/Reviewer_D8e7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901559358, "cdate": 1761901559358, "tmdate": 1762925587718, "mdate": 1762925587718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a critical challenge in scaling RL for LLMs: performance degradation when training with stale data in off-policy settings. The authors identify a \"prosperity-before-collapse\" phenomenon, where training without a trust region initially performs well on stale data before eventual collapse, suggesting that stale data contains valuable but hard-to-harness information. To address this, they propose M2PO, a novel policy optimization algorithm that constrains the second moment $M_2$ of the importance weights. M2PO employs an adaptive masking strategy to suppress only extreme, high-variance tokens, thereby preserving informative updates. The method is evaluated extensively across 6 model scales (1.7B to 32B) and 8 math reasoning benchmarks, demonstrating that M2PO achieves performance comparable to on-policy baselines even with data stale by 256 model updates, while significantly reducing the token clipping ratio and maintaining training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed M2PO algorithm is built upon a well-motivated and sound theoretical insight. It identifies the limitations of KL divergence in measuring distribution shift — specifically its insensitivity to outliers due to cancellation effects — and introduces the second moment ($M_2$) as a superior alternative. The $M_2$ metric is both an effective measure of distribution shift and inherently sensitive to the high-variance tokens that are most likely to destabilize training.\n\n- The fact that a single threshold hyperparameter $\\tau_{M_2} = 0.04$ works robustly across all model sizes and tasks is a major practical advantage, greatly enhancing the method's usability.\n\n- The empirical evaluation is extensive and compelling. The consistent success of M2PO across six model families and scales under extreme staleness $s=256$ provides strong evidence for its robustness and scalability, demonstrating performance on par with on-policy baselines."}, "weaknesses": {"value": "- While the paper compellingly demonstrates M2PO's superiority under extreme staleness $s=256$, it remains unclear if this advantage holds under more common, lower-staleness regimes (e.g., $s=8$ or $s=16$). A critical comparison in a moderate-staleness setting would be highly informative.\n\n- The empirical evaluation is currently confined to mathematical reasoning benchmarks. While this domain is a valid testbed, it features verifiable rewards and potentially specific characteristics in its token-wise importance ratio $r$ distribution. The claims regarding M2PO's general superiority and the universal robustness of its threshold $\\pi_{M_2}$ would be significantly strengthened by evaluation on other domains, such as code generation."}, "questions": {"value": "- The paper convincingly shows the superiority of M2PO over standard GRPO with symmetric clipping. However, a natural alternative is to use GRPO with asymmetric clipping bounds. Could the authors discuss the theoretical and practical relationship between M2PO and such an asymmetric GRPO variant? Why is M2PO's adaptive masking fundamentally different from simply tuning the clipping boundaries?\n\n- We note that the KL divergence calculation in DeepSeek-R1's GRPO implementation, $\\frac{\\pi_{new}}{\\pi_{behav}} - \\log \\frac{\\pi_{new}}{\\pi_{behav}} - 1$, also possesses the property of being more sensitive to large ratios compared to the standard KL. Given this, what is the distinct advantage of M2PO over such an improved, large-ratio-sensitive KL divergence used within the existing GRPO framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nkDN8ks689", "forum": "IIgl5MWelz", "replyto": "IIgl5MWelz", "signatures": ["ICLR.cc/2026/Conference/Submission15290/Reviewer_vuPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15290/Reviewer_vuPK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944802733, "cdate": 1761944802733, "tmdate": 1762925587098, "mdate": 1762925587098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why off‑policy RL for LLMs degrades when trained on stale data (rollouts from earlier policies). It reveals a clear “prosperity‑before‑collapse” phenomenon: removing the trust region yields substantially higher performance than standard GRPO with epsilon‑clipping for a period, sometimes matching on‑policy results, but eventually becomes unstable and collapses. The authors diagnose that GRPO performs poorly under staleness because stale‑data training exhibits a much higher clipping rate, disproportionately affecting informative, high‑entropy tokens. To address this, they propose M2PO, a variance‑sensitive trust region that constrains the second moment of importance weights. M2PO suppresses extreme outliers while preserving signal on high‑entropy tokens, achieves stable training, matches on‑policy performance even under high staleness (e.g., 256 updates on Qwen‑2.5‑32B), dramatically reduces clipping, and is reported to be insensitive to its threshold."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Clear, timely empirical finding: prosperity‑before‑collapse highlights that stale data can be as informative as on‑policy trajectories, shifting the focus from data quality to algorithm design.\n+ Concrete diagnosis of GRPO’s failure mode under staleness: elevated clipping, especially on high‑entropy tokens, plausibly explains lost training signal.\n+ M2PO’s second‑moment constraint provides a simple, principled, variance‑sensitive trust region that stabilizes off‑policy training.\n+ on Qwen‑2.5‑32B with staleness 256, M2PO matches on‑policy performance while reducing clipping events and avoiding collapse。"}, "weaknesses": {"value": "No formal analysis of why the unclipped regime prospers then collapses, or why second‑moment constraints deliver stability.\nIncomplete experimental detail: definitions of staleness, collapse criteria, and full baseline configurations are not specified in the provided materials, making external validity hard to assess.\nIt is unclear how results vary across model sizes, tasks, reward models, and different staleness levels beyond the highlighted case.\nCompute matching, training schedules, and hyperparameter tuning parity between on‑policy GRPO, GRPO without trust region, and M2PO are not fully documented.\nDirect token‑level gradient/credit assignment analyses are not shown here to firmly establish the masking/clipping mechanism."}, "questions": {"value": "How exactly is “staleness of 256 model updates” defined? Is it the number of policy updates between rollout generation and training, or another measure?\nDo you track a policy‑distance metric (e.g., KL between rollout policy and training policy) and how does M2PO perform as this distance changes?\nWhat operational criteria define “collapse”? Reward crash, divergence of losses, KL blow‑up, or mode collapse?\nWhat stability diagnostics did you monitor, and can you show trajectories (e.g., gradient norms, importance‑weight second moments) leading to collapse?\nHow do the results vary across tasks, reward models, and model sizes? Can you share performance under different staleness levels beyond 256?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XdEXqWeUbL", "forum": "IIgl5MWelz", "replyto": "IIgl5MWelz", "signatures": ["ICLR.cc/2026/Conference/Submission15290/Reviewer_GUjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15290/Reviewer_GUjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184996153, "cdate": 1762184996153, "tmdate": 1762925586555, "mdate": 1762925586555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}