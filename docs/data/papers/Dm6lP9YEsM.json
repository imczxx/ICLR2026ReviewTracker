{"id": "Dm6lP9YEsM", "number": 20368, "cdate": 1758305204646, "mdate": 1759896981547, "content": {"title": "Pay-Per-Search Models Are Abstention Models", "abstract": "LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.\n\nWe run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by $7.6$%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leveraged for making abstention decisions.", "tldr": "We propose MASH, a training framework that straight-forwardly elicits abstentions from LLMs by training them to selectively seek for outside help, such as by search tool-use, via reinforcement learning.", "keywords": ["abstention", "selective help-seeking", "efficient search", "knowledge boundaries"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3dd5b9bb6c02a664e12b10ab5b2bb765e4efe15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MASH, a reinforcement learning framework that models abstention through selective help-seeking. During training, the model learns when to use external tools; at inference, removing the tools turns help-seeking into abstention, improving reliability and efficiency on QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear framework for learning abstention via selective help-seeking. Its design—training with retrieval tools and inferring without them—elegantly links tool-use efficiency to abstention behavior. Across multiple QA datasets, MASH delivers strong empirical gains, including higher tool productivity and a 7.6% accuracy improvement on multi-hop QA."}, "weaknesses": {"value": "1. The method optimizes a proxy objective—binary correctness multiplied by a search penalty—while abstention is induced post hoc by removing tools at inference rather than being learned directly. The paper offers no theoretical account of why this proxy should produce a stable abstention boundary; the mechanism remains unclear and appears incidental.\n2. In the oracle-helper setting, the optimal policy is trivially to always ask for help, since help deterministically returns the gold answer. A model will therefore query every time. This biased environment cannot substantiate the claim that “this setting with the oracle helper is equivalent to explicitly training for abstention using RL”; it conflates environmental bias with algorithmic behavior.\n3. The approach appears tailored to QA with a specific retrieval setup, and its generalization beyond that scope is unproven. Across OOD and several QA datasets, the method does not show a clear advantage over DPO, which directly optimizes abstention behavior. This suggests that the observed gains may depend heavily on dataset characteristics rather than reflecting a genuinely more general or reliable abstention mechanism.\n\nIf my questions are resolved, I will consider raising the score."}, "questions": {"value": "1. Why should the “correctness × search-penalty” proxy and constraining tool usage induce meaningful “abstention”? Provide a theoretical rationale or explicit assumptions\n2. For each dataset, could you report answer rate, abstention rate, and recall to provide a more complete analysis of the model’s abstention behavior?\n3. Wouldn’t it be more direct to train abstention explicitly with a ternary reward (e.g., Correct = +1, Abstain = 0, Wrong = −1), rather than relying on the binary correctness × search-penalty proxy? Could you try continuing RL from a Search-R1 checkpoint using this objective to see whether it leads to a clearer or more consistent abstention behavior?\n4. To rule out possible dataset coincidence, could you expand the experiments to more QA datasets and evaluate them, to examine whether the model still learns a stable and interpretable abstention boundary across different distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fITd32WE1w", "forum": "Dm6lP9YEsM", "replyto": "Dm6lP9YEsM", "signatures": ["ICLR.cc/2026/Conference/Submission20368/Reviewer_b1mW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20368/Reviewer_b1mW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908174692, "cdate": 1761908174692, "tmdate": 1762933820433, "mdate": 1762933820433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies model's ability to abstain, including cases where models seek external search tools to answer a question. The authors propose a method named MASH seeking to minimize help seeking from external sources while abstaining when search is not available. The method uses GRPO with an additional penalty term for search. The authors train a Qwen 2.5- 3B model using MASH on 3 separate QA datasets. The authors also evaluate generalization of the best method across datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors tackle the important practical question of abstention—the authors' definition of abstention as cases where LLMs seek external help due to the questions' answers lying outside the model's knowledge boundary is a refreshing, new perspective. \n\nThe authors propose MASH a novel method that outperforms existing baselines with a clear objective building on GRPO. The authors evaluate abstention across several scenarios: where external help is avaiable, not available, as well as generalization across datasets. Several of the insights such as the severe penality needed for models to use parameteric knoweldge (line 312), importance of SFT warmp, and difference in performance for multihop questions are all valuable insights. I also found the study of out-of-distribution generalization quite important as generalization across datasets is key to advancing abstention."}, "weaknesses": {"value": "- clarity of the synthetic data generation pipeline: I did not find the presentation of the synthetic data generation pipeline to be very clear. A diagram or explicit example would help clarify this setp.\n- Given, Qwen Base 2.5 3B is used as the basis for the specialized abstention training, it's unclear how this training affects the general capabilities of the LLM and whether abstention can be learned on top of the more commonly used chat variants. Can the authors comment on this choice and consider adding more general LLM benchmarks to capture how specialized abstention training affects general capabilities? \n- How does the Qwen 2.5 3B chat model perform out of the box? This would be a key baseline to include in all the tables."}, "questions": {"value": "- Why Exponential Decay for natural questions is different from other datasets (line 212)\n- How is the Abstention Classification (Table 3) performed? Is this based on the use of search for a given question?  (line 384)\n- Can you provide details on how the exact match reward used (183) is performed? Is this too strict of a criteria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f9Lag22dnw", "forum": "Dm6lP9YEsM", "replyto": "Dm6lP9YEsM", "signatures": ["ICLR.cc/2026/Conference/Submission20368/Reviewer_2Hdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20368/Reviewer_2Hdt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005710708, "cdate": 1762005710708, "tmdate": 1762933819874, "mdate": 1762933819874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MASH (Modeling Abstention via Selective Help-seeking), which is a reinforcement learning framework that trains LLMs to selectively invoke external search tools under a pay-per-search penalty. The key idea is that selective help-seeking implicitly learns abstention behavior: if the model wants to search, that indicates it cannot answer with parametric knowledge. Removing the search tool at inference thus converts the model into an abstention model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clever reframing of help-seeking as a proxy for abstention. Avoids need for labeled “known/unknown” training data.\n* The writing is clear. The paper is easy to follow."}, "weaknesses": {"value": "* The link between search invocation and calibrated abstention is intuitive but not formally analyzed.\n* Focuses solely on short-form QA; unclear applicability to reasoning-heavy or generative tasks, e.g., mathematical problems."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I07dp4aXKl", "forum": "Dm6lP9YEsM", "replyto": "Dm6lP9YEsM", "signatures": ["ICLR.cc/2026/Conference/Submission20368/Reviewer_wZVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20368/Reviewer_wZVe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103293637, "cdate": 1762103293637, "tmdate": 1762933819127, "mdate": 1762933819127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting insight: that training LLMs for selective help-seeking (knowing when to search) naturally induces abstention behavior (knowing when to say \"I don't know\"). The authors propose MASH, which uses reinforcement learning with pay-per-search penalties to train models that invoke search only when needed. The key observation is that when search access is removed at inference, search invocations become abstention signals. The authors demonstrate improvements over baselines on three QA datasets and analyze the behavior of the trained LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* **Novel perspective**. The paper reframes the abstention problem from direct boundary detection through tool use. This shift from \"teaching what not to know\" to \"learning when to seek help\" opens a different approach to modeling uncertainty in LLMs. Unlike existing abstention methods that require oracle knowledge of model capabilities to construct training data, This method discovers knowledge boundaries through RL optimization. The model self-identifies its limitations via the help-seeking reward signal, making it more scalable and realistic.\n* **Empirical validation of non-obvious transfer**. The successful transfer from search behavior to abstention is insightful. The knowledge boundary identification ability without privileged information required can achieve a comparable performance with doing SFT on a specially curated abstention dataset."}, "weaknesses": {"value": "1. **Necessity of warm-start trajectory construction**. The paper lacks justification for the complex warm-start procedure over simpler alternatives, such as rejection sampling with format constraints. Table 4 shows models trained without warm-start, but no direct comparison between warm-start trajectory construction and rejection sampling is provided. The rationale for using a base model rather than an instruct model for synthetic data generation remains unclear and may introduce unnecessary complexity.\n2. **Inadequate handling of partial knowledge and reward exploitation**. MASH primarily addresses extreme cases (Abs(0) and Abs(1)) while neglecting the critical middle ground where models have partial knowledge. Several issues arise: (1) In multi-hop reasoning, models may just happen to generate intermediate entities without retrieval (Pass@K). It causes MASH to penalize all other trajectories in the GRPO group, leading to unstable RL training. (2) For multiple-choice or binary questions, models can achieve rewards through random guessing with incorrect reasoning, a problem observed in Search-R1 that MASH's strict penalties may exacerbate. (3) The aggressive penalty structure makes integration with other reward signals challenging without careful balancing.\n3. **Limited benchmark coverage**. The evaluation omits MuSiQue, a widely used benchmark that supports at most 4-hop reasoning, which would better test the approach's scalability for complex multi-hop queries.\n4. **Insufficient model scale evaluation**. Experiments exclusively use Qwen2.5-3B-base, leaving open the question of whether findings generalize to larger models (7B, 14B) or newer architectures (Qwen3), where different dynamics may emerge.\n5. **Incomplete baseline analysis**. Table 2 omits Search-R1's detailed search distribution. The consistent TC=3.0 for Search-R1 on multi-hop tasks (Table 1) seems weird. The model should sometimes answer directly or perform additional searches if it fails to generate appropriate queries.\n6. **Presentation clarity issues**. The paper's flow and readability need improvement. For instance, the abstract's key idea is difficult to follow. Moreover, the paper suffers from excessive use of dashes, disrupting readability."}, "questions": {"value": "Line 177. What is \"random correct?\" Does it mean rolling out multiple trajectories that contain the correct answer, and then randomly picking one? And why use \"shortest answer\" if we cannot get one correct trajectory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b2MOxLVcmp", "forum": "Dm6lP9YEsM", "replyto": "Dm6lP9YEsM", "signatures": ["ICLR.cc/2026/Conference/Submission20368/Reviewer_xSsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20368/Reviewer_xSsG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762495090717, "cdate": 1762495090717, "tmdate": 1762933818213, "mdate": 1762933818213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}