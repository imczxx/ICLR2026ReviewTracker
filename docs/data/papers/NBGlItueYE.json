{"id": "NBGlItueYE", "number": 16755, "cdate": 1758268354658, "mdate": 1763642706898, "content": {"title": "Don’t Lose the Thread: Empowering Long-Horizon LLM Agents with Cognitive Resource Self-Allocation", "abstract": "Agents powered by large language models (LLMs) have demonstrated remarkable progress in solving complex reasoning tasks. However, LLM agents often falter on long-horizon tasks due to cognitive overload, as their working memory becomes cluttered with expanding and irrelevant information, which dilutes their attention and hinders effective planning and reasoning. To mitigate this challenge, we introduce **CO**gnitive **R**esource Self-**AL**location (**CORAL**), a novel reasoning paradigm that empowers agents to proactively optimize their context. Implemented as an agent-callable working memory management toolset, CORAL allows an agent to maintain crucial checkpoints of its progress within its working memory and adaptively initiate a new problem-solving episode by purging cluttered working memory and resuming its reasoning from the most recent checkpoint, effectively reallocating agentic cognitive resources by implicitly sharpening their attention on the checkpoints. We further enhance the agent's checkpoint capabilities using a Multi-episode Agentic Reinforced Policy Optimization algorithm. On several long-horizon task benchmarks, CORAL significantly outperforms standard LLM agent methods. Notably, analysis of the LLMs' attention distribution reveals that CORAL substantially optimizes agentic RL dynamics, which in turn ensures agents maintain a focused cognitive resource allocation, thereby continuously amplifying performance gains.", "tldr": "", "keywords": ["Agent", "Large Language Model", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1249cc185cdfd67861bdb23589b5d020597a4c69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose COgnitive Resource Self-ALlocation (CORAL), a reasoning paradigm that allows LLM agents to dynamically manage and optimize their own context during multi-step tasks. CORAL equips agents with a working-memory management toolset, consisting of a memory management tool and a context optimization tool. These mechanisms allow agents to periodically “reboot” their reasoning—focusing attention on recent progress and freeing cognitive resources. For training enhancements, authors use supervised fine-tuning (SFT) to teach checkpointing behavior using high-quality trajectories, and present multi-episode agentic reinforced policy optimization (Multi-episode ARPO) to refine this behavior by treating multi-episode trajectories as cohesive training units, propagating rewards from final outcomes to earlier checkpoints. The authors show that, on the GAIA benchmark (a test for reasoning and research agents), CORAL significantly outperforms existing frameworks such as ReAct, WebThinker, and DeepSeek-R1—especially on long-horizon tasks (Levels 2–3)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-structured and readable.\n2. It reframes context management as a self-allocation problem, drawing an explicit analogy to human working-memory control — a fresh and underexplored perspective in LLM research. The underlying principle — that LLMs should manage their own cognitive bandwidth — is a conceptual contribution to the future of autonomous reasoning agents. \n3. The CORAL framework creatively combines existing elements — the ReAct agent loop, memory tools, and reinforcement learning — into a coherent, self-optimizing reasoning paradigm."}, "weaknesses": {"value": "1. While CORAL is presented as a novel reasoning paradigm, several of its core ideas overlap with existing work on memory-aware and context-optimization agents, such as Thread (Schroeder et al., 2024), MemAgent (Yu et al., 2025), and Mem1 (Zhou et al., 2025).\n2. The empirical evaluation relies almost exclusively on the GAIA benchmark, which, while widely used, focuses on text-based research and reasoning tasks with synthetic structure. This narrow scope raises questions about the generality of CORAL’s advantages. Specific issues:\nNo evidence is provided on multi-modal, embodied, or tool-heavy environments (e.g., ALFWorld, WebArena, or XBench-DeepSearch);\nResults use proprietary models (Claude-4, GPT-4o) for prompting-based experiments, limiting reproducibility; The paper does not explore context length scaling: it claims to mitigate “cognitive overload” but never tests tasks that explicitly push the context window (e.g., 100K–1M tokens).\n3. The paper introduces Multi-episode Agentic Reinforced Policy Optimization (Multi-episode ARPO), which propagates rewards from the final episode backward across all earlier ones. However it lacks comparison to established reinforcement frameworks like DAPO (Yu et al., 2025) and GRPO (Shao et al., 2024) on convergence stability."}, "questions": {"value": "see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y4JLViPn8t", "forum": "NBGlItueYE", "replyto": "NBGlItueYE", "signatures": ["ICLR.cc/2026/Conference/Submission16755/Reviewer_z3nr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16755/Reviewer_z3nr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704255289, "cdate": 1761704255289, "tmdate": 1762926801043, "mdate": 1762926801043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QWla5QJmHM", "forum": "NBGlItueYE", "replyto": "NBGlItueYE", "signatures": ["ICLR.cc/2026/Conference/Submission16755/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16755/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763642706220, "cdate": 1763642706220, "tmdate": 1763642706220, "mdate": 1763642706220, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CORAL, a reasoning framework designed to alleviate cognitive overload in long-horizon LLM agents. The key idea is to grant the agent metacognitive control over its working memory through two callable tools that enable it to create checkpoints of verified facts and task progress, and to periodically reset its context to refocus on essential information. CORAL integrates these mechanisms into the ReAct loop and introduces a multi-episode Agentic Reinforced Policy Optimization (ARPO) algorithm to train memory management policies that optimize checkpointing and reset decisions across multiple reasoning episodes. Experiments on the GAIA benchmark demonstrate substantial gains on long-horizon tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Metacognitive framing: CORAL goes beyond context-window optimization and positions the LLM agent as a self-regulating cognitive system. The notion of \"cognitive resource self-allocation\" aligns well with human working-memory theory and opens a promising direction for computational metacognition.\n* Memory management as actions: Implementing self-regulation through explicit callable tools for memory management and context optimization is elegant and compatible with current agentic frameworks. The separation between context and working memory provides a clear conceptual foundation for scalable reasoning.\n* Multi-episode ARPO: The Multi-episode ARPO formulation effectively models credit assignment across episodes, providing a learning mechanism for when to checkpoint or reset. This is a clever extension of PPO-style optimization to agentic reasoning.\n* Empirical evidence of attention re-focusing: The attention-map analysis convincingly shows sharper focus on checkpoint tokens after fine-tuning, supporting the cognitive-resource argument."}, "weaknesses": {"value": "* Limited evaluation: The authors used only the GAIA benchmark for evaluation, which limits the generalizability of CORAL’s validity. Would CORAL remain effective in other domains like embodied reasoning, tool-use planning, or knowledge-based QA? Isn't the observed gain specific to GAIA’s web-search and reasoning tasks? I suggest embodied planning e.g. AlfWorld would be a nice domain to test the validity of the proposed framework.\n* Ambiguous definition of “verified facts”: The paper states that \"the agent can autonomously invoke memory tools to create checkpoints of its progress and verified facts.\", but it never defines how \"verified facts\" are detected and filtered. Are these based on positive observations, confidence thresholds, or LLM judgments? Without a clear criterion for forming verified facts, checkpoint quality and reproducibility are uncertain.\n* Limited description of checkpoint summarization: The process of converting ongoing reasoning into concise checkpoints is crucial but underspecified. What summarization prompts or heuristics were used? Different summarization strategies could yield drastically different outcomes.\n* Partial reporting of results: Table 1 omits level-wise scores for certain settings (e.g., ReAct + Hard Optimization with Claude). Even if those experiments were limited, aggregated numbers alone obscure performance trends across difficulty levels."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JZUIh6g9T5", "forum": "NBGlItueYE", "replyto": "NBGlItueYE", "signatures": ["ICLR.cc/2026/Conference/Submission16755/Reviewer_Nj4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16755/Reviewer_Nj4W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705746999, "cdate": 1761705746999, "tmdate": 1762926800437, "mdate": 1762926800437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CORAL, a agentic method that manages its context while attempting to solve a query. The authors equip the agent with tools for managing its context and optimize it using both SFT and RL. They present results on GAIA, comparing with a range of agent and direct interface baselines, showing that the prompt-based system, that + STF, and that + STF + RL successively improve models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Originality. The context management tool ideas (memory management and context optimization) are to my knowledge new ways of approaching managing decision-making as an agent’s history grows.\n\nQuality. The experiments involve a substantial set of reasonable baselines, all done on a reasonable benchmark, with a useful ablation study.\n\nClarity. I found that I could generally follow what was going on (with some exceptions, see below).\n\nSignificance. The paper demonstrates the efficacy of the various parts of their technique, showing promising performance comparable with strong baselines."}, "weaknesses": {"value": "I found the presentation quite hard to follow in various important places.\n\n1. I couldn’t precisely understand what Memory Management is doing exactly — I have a decent idea but I could not replicate it. What is a “knowledge unit”?\n2. I think I know what’s going on with ARPO (episodes share the reward at the end but are optimized separately — but otherwise DAPO?), but several key points for how it is described (”a separate optimization unit while maintaining trajectory-level coherence”, “all episodes participate in the group computation to get an advantage”) are vague when they could be said exactly.\n3. The experiments presentation is difficult to follow. That section doesn’t use the method name given in the rest of the paper. One of the critical baselines (indeed the one that outperforms your method overall, CK-Pro-8B) is as far as I can tell never mentioned in the main text. Claude doesn’t have score breakdowns into Level 1, 2, 3, without explanation.\n\nThe experimental results seem promising but overall do not beat a baseline (and this isn’t discussed at all, just that the paper’s approach has “superior performance”). I might also suggest, especially your top qwen model does comparably less well (relative to baselines) on Level 1, to do an error analysis on each level.\n\nMore minor points: what is o in equation 6? You mean that x_i is masked for any observation, right? And section 3.2 seems largely redundant given 3.1."}, "questions": {"value": "I think all important questions were asked in talking through weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JT2grdvMRT", "forum": "NBGlItueYE", "replyto": "NBGlItueYE", "signatures": ["ICLR.cc/2026/Conference/Submission16755/Reviewer_uaYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16755/Reviewer_uaYK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964144408, "cdate": 1761964144408, "tmdate": 1762926799025, "mdate": 1762926799025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CORAL, which lets an LLM web agent manage \"working memory\" via two tools: Memory Management and Context Optimization (CO), where CO performs a hard context reset that preserves essentials (system prompt, original query, working memory) and segments reasoning into episodes. Reset timing is learned: a multi-episode ARPO objective treats each episode as an optimization unit, with a reward that multiplies a format check and an LLM judge answer score. Experiments use the GAIA showed CORAL used purely as a prompting framework on a strong closed model performs competitively with leading systems on a reasoning benchmark. With a smaller open-source base model, adding SFT and RL transforms brittle tool calling into more reliable multi-step behavior and improves performance over a standard ReAct-style baseline. The evaluation, however, is concentrated on a single benchmark and one web environment, and some comparisons leave compute-parity details unspecified."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed mechanism including multi-episode RL setup with a learned context reset is sound. A reset segments trajectories into episodes with clear initial contexts and recurrence (previous CO output becomes the next episode’s seed), which cleanly supports learning when to reset, and might effectively deal with long context issues."}, "weaknesses": {"value": "- The authors claim CORAL \"demonstrates substantial improvements\", especially on harder levels, and also describes a prompting-only run on a strong closed model that’s \"comparable\" to a claude. Those narrative claims are present, but some supporting table cells are missing (no per-level breakdowns for key rows), so the evidence is incomplete in places. For example:\n  - Table 1 seems incomplete. Several entries are left unreported, and the \"+HARD OPTIM\" rule-based variant isn't described-so it's hard to judge how learned resets compare against strong heuristics. Also there is no justification ever provided for why these results were excluded.\n  - Comparisons span heterogeneous stacks without clear parity on budgets, step limits, and decoding/stopping settings aren’t detailed enough to guarantee apples-to-apples conclusions.\n\n- Although improvements on GAIA would be valid, it would be better to see if this method generalizes across domain.\n\n- It's unclear what the \"take-away\" should be from Figure 3. The paper shows a single case study attention map and says it focuses more on prior checkpoint messages after fine-tuning, but it doesn’t clearly explain how heads were chosen, how attention was aggregated, or how to read axes and color scales."}, "questions": {"value": "Do you have analysis on when CO is invoked, including the distribution of episode counts, triggers preceding resets, or correlation between reset timing and success?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "medZRPkupV", "forum": "NBGlItueYE", "replyto": "NBGlItueYE", "signatures": ["ICLR.cc/2026/Conference/Submission16755/Reviewer_NPZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16755/Reviewer_NPZM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975342882, "cdate": 1761975342882, "tmdate": 1762926798656, "mdate": 1762926798656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}