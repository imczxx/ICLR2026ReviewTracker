{"id": "qcz3g6mH3L", "number": 24895, "cdate": 1758361657741, "mdate": 1759896743321, "content": {"title": "ROSARL: Reward-Only Safe Reinforcement Learning", "abstract": "An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, designing reward or cost functions is non-trivial and can increase with the complexity of the problem. To address this, we investigate the concept of a *Minmax* penalty, the smallest penalty for unsafe states that leads to safe optimal policies, regardless of task rewards. We derive an upper and lower bound on this penalty by considering both environment *diameter* and *controllability*. Additionally, we propose a simple algorithm for agents to estimate this penalty while learning task policies. Our experiments demonstrate the effectiveness of this approach in enabling agents to learn safe policies in high-dimensional continuous control environments.", "tldr": "", "keywords": ["Reinforcement Learning", "Deep Reinforcement Learning", "Safe RL", "Constrained RL", "Reward shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bafc582802c24fa147a79b2d0826bb27e97cea11.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers the problem of RL penalty reward design for safe reaching tasks. The setting is without discounted factors. The authors derive the calculation for the minimax penalty, the smallest penalty for unsafe states that leads to safe optimal policies. The authors first show the upper and lower bounds for this minimax penalty term based on task solvability and diameter, as well as the minimum and maximum rewards, then give a model-free practical estimate for the minimax penalty term using value estimates. The validity of their approach is first tested on the LAVA GRIDWORLD environment. Then they conduct experiments on the Safety Gym environment, and show that their proposed method can work better (less failure rate or cumulative cost with longer episode length) than baselines such as constrained RL, Lagrangian methods, and SauteTRPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written. The way the authors introduce these concepts is like a textbook, where abundant figures and examples provide a walkthrough for the key concepts.\n2. The proposed method has strong theoretical guarantees.\n3. Strong empirical results achieved compared to baselines. The experiments are conducted over 10-20 random seeds, showing statistical significance."}, "weaknesses": {"value": "1. Missing baseline: the work does not compare with epigraph-based methods [1].\n2. The simulation environment is just in 2D space (though observation state is 60D): not sure how the proposed method will behave in 3D space or for manipulation tasks.\n3. Minor issue in writing: some citation issue at L193-194 and L316-317.\n\n\nReferences:\n1. So, Oswin, and Chuchu Fan. \"Solving stabilize-avoid optimal control via epigraph form and deep reinforcement learning.\" arXiv preprint arXiv:2305.14154 (2023)."}, "questions": {"value": "The paper doesn't describe the limitations of the proposed method (beyond its applicability only to environments with unsafe terminal states). It will be great if the authors can comment on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QoHOtns06W", "forum": "qcz3g6mH3L", "replyto": "qcz3g6mH3L", "signatures": ["ICLR.cc/2026/Conference/Submission24895/Reviewer_GG4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24895/Reviewer_GG4v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760592448088, "cdate": 1760592448088, "tmdate": 1762943237588, "mdate": 1762943237588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a minmax penalty within TRPO algorithm, which applies the smallest penalty for unsafe states to generate safe policies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed security reinforcement learning algorithm has certain theoretical significance."}, "weaknesses": {"value": "1. It has not been compared with existing reachability methods, which adopt the idea of minimax optimization. \n2. The proposed method cannot guarantee absolute security of the strategy in theory or practice, which is crucial for secure reinforcement learning. \n3. The comparison algorithm is relatively outdated."}, "questions": {"value": "1. If Theorem 1 follows from the convergence guarantee of policy evaluation (Sutton & Barto, 1998), what is its significance? \n2. What does Theorem 3 actually prove? \n3. Can the proposed framework be combined with other reinforcement learning methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GEZ8JvsI94", "forum": "qcz3g6mH3L", "replyto": "qcz3g6mH3L", "signatures": ["ICLR.cc/2026/Conference/Submission24895/Reviewer_ci8A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24895/Reviewer_ci8A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484902432, "cdate": 1761484902432, "tmdate": 1762943237324, "mdate": 1762943237324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for Safe RL, whose core argument is that traditional CMDP methods, such as those using cost functions and Lagrangian multipliers, are unnecessary. Instead, the authors posit that the safety problem can be reformulated as a reward design problem.\nThe authors theoretically define a \"Minmax penalty\", which if assigned to all unsafe terminal states, ensures that the optimal policy of any standard, reward-maximizing RL algorithm will automatically be safe.\nThe authors derive theoretical upper and lower bounds for Minmax penalty, which depend on the \"Diameter\" (D) and \"Solvability\" (C). As this theoretical bound is difficult to compute in practice, the authors further propose a simple and model-free practical algorithm. This algorithm adaptively learns a sufficiently large penalty value by estimating the bounds of the value function online. Experiments demonstrate that combining this algorithm with standard RL algorithms achieves strong safety performance on benchmarks like Safety Gym, outperforming traditional constrained-optimization methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Reframing the episodic safety problem from a complex \"constrained optimization\" framework (CMDPs) back to a \"reward design\" problem is an insightful perspective. \n2. The proposed practical algorithm is simple and easy to implement. It does not require manual tuning of hyperparameters. It can be used as a \"plug-in\" with any off-the-shelf, value-based RL algorithm, offering strong generality.\n3. The method shows excellent performance in the Safety Gym experiments, particularly under high-noise settings."}, "weaknesses": {"value": "1. The entire theoretical framework  is explicitly built on \"undiscounted stochastic shortest path\" (SSP) MDPs. However, the core experiments used to validate the algorithm  are conducted in \"discounted,\" continuous-control, non-SSP environments. This makes the connection between the theoretical derivations and the experimental results weak.\n2. The practical algorithm completely omits the solvability factor C from the theoretical bound. The authors claim the adaptive nature of the algorithm \"implicitly\" compensates for this, but this claim is not supported by any theory or ablation.\n3. It is not at all clear how this method would generalize to the non-terminating CMDP setting. Figure 21 in the appendix seems to suggest the method's performance degrades in such a non-terminating setting."}, "questions": {"value": "1. Given that the theory is for undiscounted SSPs, while the experiments are in discounted, continuous environments, can the authors provide deeper insight or a theoretical argument as to why Algorithm 1, which is derived from SSP theory, remains effective and robust in a discounted setting?\n2.  Could the authors provide an ablation study to justify the omission of C? For example, in a simple tabular gridworld where C can be computed, how does Algorithm 1 (omitting C) compare to a policy trained using the \"oracle\" theoretical penalty that includes C?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1qgPmjJrwo", "forum": "qcz3g6mH3L", "replyto": "qcz3g6mH3L", "signatures": ["ICLR.cc/2026/Conference/Submission24895/Reviewer_PL6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24895/Reviewer_PL6h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830418465, "cdate": 1761830418465, "tmdate": 1762943236973, "mdate": 1762943236973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an alternative to traditional constrained policy optimization methods by learning a penalty term for states that violate constraints, resulting in the learning of safe optimal policies. The paper derives its penalty term from the concepts of diameter and solvability, which are explained in the paper. The derived penalty requires knowledge of the environment's dynamics; thus, the paper provides a practical algorithm that sidesteps this issue and presents some experiments demonstrating the performance of their algorithm. The paper also provides an analysis of performance in cases where the assumptions hold and in other cases where they do not."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's strengths lie in providing an alternative to unstable policy optimization methods by introducing a penalty term that eliminates the need for such approaches. The paper is well-presented and generally sound, albeit with some flaws. The analysis in a lower-dimensional environment, as well as the comparison between the performance of the practical algorithm and the environment where the method's assumption holds, is quite helpful."}, "weaknesses": {"value": "The paper derives its penalty term using the concepts of diameter and solvability, which require knowledge of the dynamics. In the practical implementation of their method, which does not require knowledge of the dynamics. The empirical experiments are on the weaker side. The method underperforms Lagrangian TRPO in task performance. Also, the paper compares their method only with a single threshold; further, the method does not compare their approach with the PID Lagrangian method, which is SOTA in constrained policy optimization. Further, there's a serious flaw in how the paper motivates its approach; the authors provide reasoning that their approach offers an alternative to shaped constraint costs. However, in many constrained RL problems, the cost is considered to be sparse. In my view, this approach provides an alternative to the issue of constrained policy optimization, which can be unstable. This is a significant distinction.\n\nMinor errors: \n- \"minimized when reaching\"  in the abstract\n- references in lines 193 and 215, 316\n\nStooke, Adam, Joshua Achiam, and Pieter Abbeel. \"Responsive safety in reinforcement learning by pid lagrangian methods.\" International Conference on Machine Learning. PMLR, 2020."}, "questions": {"value": "- Can you compare your approach to Lagrangian TPO under different constraint thresholds?\n- Can you compare your algorithm to PID Lagrangian approaches?\n- Can you run experiments on other safety gym domains?\n\nI would be willing to raise my score if the authors can provide answers to my questions.\n\nStooke, Adam, Joshua Achiam, and Pieter Abbeel. \"Responsive safety in reinforcement learning by pid lagrangian methods.\" International Conference on Machine Learning. PMLR, 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Omi3Ix7vZ", "forum": "qcz3g6mH3L", "replyto": "qcz3g6mH3L", "signatures": ["ICLR.cc/2026/Conference/Submission24895/Reviewer_yhcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24895/Reviewer_yhcf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997558124, "cdate": 1761997558124, "tmdate": 1762943236703, "mdate": 1762943236703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}