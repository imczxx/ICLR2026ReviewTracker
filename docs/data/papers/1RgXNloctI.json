{"id": "1RgXNloctI", "number": 1721, "cdate": 1756911960103, "mdate": 1759898191911, "content": {"title": "CoT-RVS: Zero-Shot Chain-of-Thought Reasoning Segmentation for Videos", "abstract": "Reasoning Video Object Segmentation is a challenging task, aiming at  generating a mask sequence from an input video given a complex and implicit text query. While existing works finetune Multimodal Large Language Models (MLLM) for the task, they still fail in video inputs given complex temporally-sensitive queries, indicating their lack of temporal and spatial integration in complex scenarios. In this paper, we propose **CoT-RVS**, a novel framework employing the zero-shot Chain-of-Thought (CoT) capability of MLLM to address these complex challenges by **temporal-semantic reasoning**: CoT-RVS analyzes the visible objects within a given frame that possibly match the language query (semantic), and chooses a corresponding keyframe for each object that can be observed effortlessly among all frames (temporal). Notably, the CoT-RVS framework is training-free and compatible with closed-source MLLMs, which can be applied to Reasoning Video Instance Segmentation. Our framework's training-free feature further allows its extension to process online video streams, where the CoT  is used at test time to update the object of interest when  a better target starts to emerge and becomes visible. We conduct extensive experiments on video object segmentation with explicit and implicit queries. The results show that CoT-RVS significantly outperforms previous works in both cases, qualitatively and quantitatively.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Reasoning Video Object Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9880eb50aa08b4f7575d1d43f1737c9362ce28a5.pdf", "supplementary_material": "/attachment/65e8510a380aa1f902b69e90a12f8dc5efd71f60.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces CoT-RVS, a zero-shot, and training-free framework for Reasoning Video Object Segmentation (Reasoning VOS). The primary goal is to segment objects in a video based on complex, implicit, or temporally-sensitive text queries.The core idea is to decouple high-level temporal-semantic reasoning from low-level segmentation and tracking. The method uses a modular, multi-agent pipeline:MLLM Keyframe Selector, Reasoning Image Segmenter, Video Processor."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A key aspect of the paper's design is its problem decomposition. Instead of using a single model to handle both temporal reasoning and pixel segmentation, it decouples the task into two stages: 1) First, an MLLM ($F_{key}$) performs high-level temporal-semantic reasoning to identify the most suitable keyframe and target description ; 2) Subsequently, specialized vision models ($F_{seg}$, $F_{vid}$) execute segmentation and tracking based on the clear instructions provided by the MLLM . This architecture shifts the challenge of complex temporal understanding from the pixel domain to the semantic reasoning domain.\n2. The paper provides sufficient experiments to validate its method. It was tested not only on four standard benchmarks (MeViS, Refer-DAVIS-17, ReVOS, ReasonVOS) but also on a specially constructed, temporally-sensitive T-ReasonVOS dataset to validate its core hypothesis. The experimental results show that the method achieves performance exceeding the compared SOTA methods on these benchmarks, particularly on T-ReasonVOS \n3. The framework is designed to be training-free and modular. This design allows for the replacement of the MLLM component (e.g., GPT-4o, Gemma3) without retraining. The architecture also supports an extension from single-object (VOS) to multi-instance (VIS) segmentation (via the MLLM outputting a list) and is refactored for an online version (via periodic keyframe updates )."}, "weaknesses": {"value": "1. the paper's most compelling conclusion—superiority on temporally-sensitive tasks—relies heavily on the newly created T-ReasonVOS dataset . Given that this dataset was manually filtered by the authors and is not yet publicly released, this raises concerns regarding reproducibility and potential selection bias.\n\n2. the ablation studies are incomplete. While they validate the importance of the CoT process and sampling rate, the paper fails to fully explore its claimed modularity by, for instance, swapping the $F_{seg}$ (Seg-Zero) or $F_{vid}$ (SAM2) components to test the framework's generalizability\n\n3. the system is highly dependent on the MLLM's adherence to specific prompt formats.\n\n4. The framework's practical viability is questionable due to high computational latency and cost."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jr492FP4Hr", "forum": "1RgXNloctI", "replyto": "1RgXNloctI", "signatures": ["ICLR.cc/2026/Conference/Submission1721/Reviewer_EGe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1721/Reviewer_EGe3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788634952, "cdate": 1761788634952, "tmdate": 1762915869048, "mdate": 1762915869048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoT-RVS, a training-free framework for Reasoning Video Object Segmentation. Instead of end-to-end finetuning, the framework is modular. It decomposes the R-VOS task into three distinct stages: an MLLM-based Keyframe Selector that uses zero-shot Chain-of-Thought prompting to perform temporal-semantic reasoning, an off-the-shelf Reasoning Image Segmentation Model to generate a key mask, and a Video Processor (i.e., SAM2) to track the object throughout the video. The paper demonstrates that this framework achieves state-of-the-art performance on several R-VOS benchmarks, with extensions to Reasoning Video Instance Segmentation and online streaming video."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of a training-free, modular framework is compelling. This approach cleverly composes the strengths of large pre-trained models to bypass the need for task-specific finetuning.\n2. The quantitative results are strong, showing the framework outperforms prior SOTA methods on multiple benchmarks.\n3. The framework is shown to be flexible. The authors demonstrate its applicability beyond standard R-VOS, with extensions for Instance Segmentation and Online Reasoning VOS."}, "weaknesses": {"value": "1. The primary weakness is that the framework is an integration of existing, powerful components rather than a new technical method. It \"stitches together\" a large MLLM, an image segmenter, and a video tracker. The system-level design is a valid engineering contribution, but the paper would be stronger if it more clearly articulated this as a novel contribution beyond \"stitching\".\n2. The paper's central claim, embedded in its title, is the power of the Chain-of-Thought reasoning process. However, this claim is supported almost exclusively by a qualitative ablation in Figure 8. The paper lacks a quantitative ablation study regarding CoT."}, "questions": {"value": "1. What is the key difference between this work and ThinkFast?\n2. According to Table A4 in Appendix G, LLaVA-1.5-7B and Qwen2.5-VL-3B are poor at key frames selection. How are the results achieved in the main paper?\n3. Why only CoT-RVS-LLaVA supports online mode, while Gemma and GPT-4o do not support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KXFCxUtvQ4", "forum": "1RgXNloctI", "replyto": "1RgXNloctI", "signatures": ["ICLR.cc/2026/Conference/Submission1721/Reviewer_bGVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1721/Reviewer_bGVR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891963332, "cdate": 1761891963332, "tmdate": 1762915868431, "mdate": 1762915868431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a novel training-free framework called CoT-RVS. This method leverages the zero-shot CoT capability of MLLM to select the optimal keyframes for segmentation. This framework outperforms existing methods both qualitatively and quantitatively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed CoT-RVS architecture can be seamlessly extended to online video stream processing (Online Reasoning VOS), showing strong practical potential.\n2.\tThe method primarily relies on the zero-shot capability of MLLMs, requiring no additional training and exhibiting good generalization ability."}, "weaknesses": {"value": "1.\tFrom a pipeline perspective, the proposed method shows limited distinction from prior training-free works such as AL-Ref-SAM. The explicit introduction of Chain-of-Thought reasoning has also been explored in previous reasoning segmentation studies, indicating limited methodological novelty.\n2.\tAlthough the introduction emphasizes the importance of reasoning over temporal-semantic correlation, this motivation is not clearly reflected in either the method design or the experiments.\n3.\tThe MLLM keyframe selector and reasoning image segmentation model communicate through text, which may lead to inconsistencies between the described instance and the segmented target when multiple similar objects exist.\n4.\tThe method requires long-chain reasoning on each sampled frame, limiting the length and complexity of the input video, and potentially causing issues such as forgetting or hallucination. This restricts scalability to complex or long-duration videos.\n5.\tUsing GPT-4o as the base model makes the comparison with other methods unfair; parameter amount, inference cost, and latency introduced by the CoT mechanism should be reported.\n6.\tThe performance on Reasoning VOS tasks using LLaVA-1.5 and Gemma-3 is not significant, as shown in Table 4."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5I8WoXSBNa", "forum": "1RgXNloctI", "replyto": "1RgXNloctI", "signatures": ["ICLR.cc/2026/Conference/Submission1721/Reviewer_7g6P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1721/Reviewer_7g6P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931081760, "cdate": 1761931081760, "tmdate": 1762915868297, "mdate": 1762915868297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free referring video object segmentation by chain-of-thought capability of multimodal LLM in multi-agent framework, termed as CoT-RVS. CoT-RVS contains a multimodal LLM keyframe selector, a reasoning image segmentation model, and a video processor that track the masked selected object instances over the entire video. The CoT-RVS can be applied to offline video settings as well as online video settings. Experimental results show CoT-RVS outperforms prior methods on reasoning VOS benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) This paper proposes a framework to use the chain-of-thought capabilitity of pretrained MLLMs with prompting to perform reasoning first and segmentation then. Experiments show this design outperforms prior approaches and maintain training-free, being versatile with proprietary models.\n\n2) The author also proposes a simple adaptation of this framework to online causal video settings, which is relatively underexplored by previous reasoning VOS methods but with great potential values."}, "weaknesses": {"value": "There are some major concerns as listed follows:\n\n1) For online video settings, could the authors please report the inference FPS and latency of this pipeline? It seems requiring many components' working to finish segmentation and might take too much time when handling online videos.\n\n2) I was wondering whether this is pipeline agentic enough to be called an agent framework? The steps in this pipeline seem highly fixed and rule-based and lacking being autonomous enough. I would like to hear some author's discussion on this.\n\n3) To what extent the performance gain is credited to the SAM2 as a video processor? As one knows, SAM2 excels at video object segementation given prompts. Is this pipeline still effective (and outperforming prior methods) when using SAM1 as a segmentation model?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jfJCmCh0X6", "forum": "1RgXNloctI", "replyto": "1RgXNloctI", "signatures": ["ICLR.cc/2026/Conference/Submission1721/Reviewer_bVYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1721/Reviewer_bVYo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012599698, "cdate": 1762012599698, "tmdate": 1762915868175, "mdate": 1762915868175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}