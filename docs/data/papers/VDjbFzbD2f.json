{"id": "VDjbFzbD2f", "number": 19764, "cdate": 1758299147910, "mdate": 1759897020868, "content": {"title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "abstract": "As large language models (LLMs) are pretrained on massive web corpora, careful selection of data becomes essential to ensure effective and efficient learning. While perplexity (PPL)-based filtering has demonstrated strong performance, it suffers from drawbacks: substantial time costs and inherent unreliability of the model when handling noisy or out-of-distribution samples. In this work, we propose a simple yet powerful alternative: a prior-based data filtering method that estimates token priors using corpus-level term frequency statistics, inspired by linguistic insights on word roles and lexical density. Our approach filters documents based on the mean and standard deviation of token priors, serving as a fast proxy to PPL while requiring no model inference. Despite its simplicity, the prior-based filter achieves the highest average performance across 20 downstream benchmarks, while reducing time cost by over 1000× compared to PPL-based filtering. We further demonstrate its applicability to symbolic languages such as code and math, and its dynamic adaptability to multilingual corpora without supervision.", "tldr": "Token frequency stats can replace perplexity for LLM data filtering—1000× faster, equally effective.", "keywords": ["LLM", "large language models", "pretraining", "data filtering", "data pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7d0286493f54fc8a95bd24cb39185fe3a27dd9e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a prior-based data filtering approach that replaces perplexity-based filtering with a simple frequency-based proxy. By leveraging corpus-level token priors (term frequencies) and their document-level mean and variance, the method identifies noisy or ill-formed data efficiently. It achieves comparable or better downstream performance than PPL-based methods while being 1000x faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Efficient and elegant alternative to PPL filtering.\n- Strong empirical results across 20 benchmarks and two model sizes.\n- Grounded in interpretable linguistic insights (lexical density, word frequency).\n- Scalable and robust, it extends to code and math data."}, "weaknesses": {"value": "- Results mostly on English and related datasets; generalization to other scripts unclear: The experiments are focused almost entirely on English or similar languages, so it’s hard to tell how well the method would hold up for other scripts or morphologically rich languages. Since the approach depends on token frequencies, it might behave very differently for languages with complex segmentation or compounding (like Japanese or Turkish). Even a small multilingual check would help back up the broader claims.\n- Some claims (e.g., automatic \"learnability\" detection) lack quantitative rigor: The paper argues that the method can automatically detect “learnable” data, but this idea isn’t really supported by clear evidence. There aren’t concrete metrics or controlled experiments showing that the filtered data is actually easier for a model to learn from. As it stands, the claim feels more intuitive than proven.\n- May remove low-frequency, high-value data: Because the method penalizes documents with rare token patterns, it could unintentionally throw away useful or unique data like text from small domains, rare languages, or creative writing. This might reduce the diversity of the pretraining corpus, especially in the long tail where valuable but infrequent patterns live. A short analysis of this trade-off would make the results more convincing.\n- Some sections read more as post-hoc rationalization (linguistic grounding) than rigorous analysis.\n- Limited novelty from a machine learning standpoint"}, "questions": {"value": "-How sensitive are results to tokenizer choice or token granularity?\n- Could the method over-filter rare but meaningful content?\n- How does this interact with deduplication or semantic filtering pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dW8r6eQyt8", "forum": "VDjbFzbD2f", "replyto": "VDjbFzbD2f", "signatures": ["ICLR.cc/2026/Conference/Submission19764/Reviewer_8chv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19764/Reviewer_8chv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727767024, "cdate": 1761727767024, "tmdate": 1762931606791, "mdate": 1762931606791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a prior-based data filtering method as a simple, fast, and powerful alternative to perplexity (PPL) for cleaning massive web corpora used in LLM pretraining. Inspired by linguistic insights on lexical density, the method estimates token priors using corpus-level term frequency and filters documents based on the mean and standard deviation of these priors, requiring no model inference. Despite its simplicity, the prior-based filter achieves state-of-the-art average performance, outperforming PPL-based filtering across 20 downstream benchmarks. The proposed approach is also over 1000 times faster than PPL filtering and demonstrates robust applicability to multilingual corpora and symbolic languages like code and math."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper proposes a simple yet effective data filtering method that achieves efficient information filtering using only statistical features.\n* Experiments show that the method is not only fast but also outperforms PPL-based approaches in terms of performance.\n* The authors demonstrate the generality of the method, showing that it can be applied to text, code, and math data."}, "weaknesses": {"value": "* The experiments are conducted only on a 3B-scale dataset, which is too small to be convincing. I believe the authors should use a larger dataset (e.g., over 100B) and test on more complex tasks to better demonstrate the superiority of the proposed data filtering method.\n* In terms of writing, Chapter 2 includes excessive discussion of basic NLP concepts such as word frequency. I suggest that the authors reduce such background explanations or move them to the appendix.\n* The authors only test on unigrams and do not extend the analysis to n-grams. I believe they should further discuss the filtering effectiveness for n-grams.\n* The paper lacks experiments on the choice of filtering thresholds and filtering ratios. I suggest that the authors conduct additional experiments on these two hyperparameters to determine the optimal settings."}, "questions": {"value": "* Have the authors considered combining different methods to achieve better filtering performance?\n* Is there any scaling experiment on data filtering? The authors should test with larger data proportions and model sizes to fully demonstrate the effectiveness of the filtering approach.\n* Why did the authors choose word frequency instead of n-grams or more complex statistical metrics? Previous work in statistical NLP has shown that n-grams are often more effective than using word frequency alone.\n* Is there a detailed experiment on the filtering ratio to determine the optimal filtering threshold?\n* Would different tokenizers significantly affect the filtering results? Moreover, can the filtering method remain effective across different tokenizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OtqgSvt2Ko", "forum": "VDjbFzbD2f", "replyto": "VDjbFzbD2f", "signatures": ["ICLR.cc/2026/Conference/Submission19764/Reviewer_edXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19764/Reviewer_edXW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914760758, "cdate": 1761914760758, "tmdate": 1762931606020, "mdate": 1762931606020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a text data filtering method based on word frequency prior to replace mainstream perplexity-based filtering strategies. Drawing upon statistical patterns of word frequency and lexical density in linguistics, the method identifies and filters anomalous documents deviating from the overall distribution by calculating the mean and standard deviation of the logarithm of word frequency within documents. Experimental results demonstrate that this method achieves performance superior to or equivalent to perplexity-based filtering across 20 downstream tasks, while reducing filtering time to approximately 0.1% of the original method, exhibiting remarkable computational efficiency. Furthermore, the method shows good applicability in both symbolic languages and multilingual corpora, without requiring pre-trained models or manually annotated data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This method employs word frequency statistics for filtering, eliminating the need for training or invoking reference models, thereby reducing implementation complexity and resource dependency.\n\n2. Across 20 downstream tasks encompassing world knowledge and common-sense reasoning, the model demonstrated superior average performance compared to baseline models.\n\n3. This method is linguistically grounded in word frequency and lexical density, possessing an interpretable statistical foundation."}, "weaknesses": {"value": "1. The paper should thoroughly discuss the advantages of PPL in capturing semantic coherence and contextual dependencies. Emphasizing only its computational cost and noise sensitivity issues may be misleading.\n\n2. The author should consider incorporating additional lightweight or baseline metrics (such as character repetition rate and language model scores) for comparison to enhance the persuasiveness of the experimental results.\n\n3. The authors should strengthen the discussion on the scaling relationship between computational efficiency and corpus size. Since experiments were conducted only at a 6B-word scale, they should demonstrate feasibility at larger scales.\n\n4. Although the hypothesis of a priori approximate PPL is proposed, it lacks rigorous theoretical derivation or boundary condition analysis."}, "questions": {"value": "1. The method uses the median as the central estimate for “normal documents.” For corpora with highly skewed distributions or mixed multimodal distributions (e.g., a blend of professional forums and news), is the median still a robust estimator? Have other robust central measures been considered?\n\n2. The experiment demonstrated the effectiveness of processing mixed Chinese-English corpora. However, for variants within the same language (such as academic English versus colloquial English), word frequency distributions may differ. Can the method distinguish stylistic variations within the same language from genuine noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TYZgwNJ7Yv", "forum": "VDjbFzbD2f", "replyto": "VDjbFzbD2f", "signatures": ["ICLR.cc/2026/Conference/Submission19764/Reviewer_HKmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19764/Reviewer_HKmF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922564746, "cdate": 1761922564746, "tmdate": 1762931604431, "mdate": 1762931604431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prior-based filter that estimates a token's context free probability from corpus term frequencies (priors). For each document, it computes the mean and standard of token priors and drops outliers far from corpus medians, motivated by (i) frequency $\\approx$ function/content role and (ii) well-formed text having a stable lexical density. This method approximates PPL filtering while being $~1000\\cdot$ cheaper (no model inference) and often outperforms PPL-based and DSIR baseline across  nearly 20 downstream tasks. The code is reproducible."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical and speedy. This method requires only frequency counts and reported wall-clock is orders of magnitude lower than PPL-based pipelines.\n2. Clear, language agnostic intuition: This paper links token frequency to function/content roles; uses document-level $\\mu, \\sigma$ to capture lexical density.\n3. It covers 2 model sizes with 20 tasks. Extensive experiments."}, "weaknesses": {"value": "1. Frequency-driven criteria can over-filter rare but valuable content (named entities, dialects, minority languages). This paper discusses some multi-lingual behavior in Section 3.4.2 but they use Chinese and English as an example. These two languages are very common in web. Broader language settings might be needed.\n2. Baseline scope might be limited. Comparisons focus on PPL and DSIR; other lightweight filters (classifier-based, memorization/dup measures, n-gram LM perplexity, quality/explicit toxicity heuristics) are not covered.\n3. Scaling to large models: The authors report results for 137M-1.5B GPT-2 like models. It's unclear whether the advantage persists at 7B-70B scales or with instruction-tuning datasets."}, "questions": {"value": "Can the authors add more baselines to compare and see whether other lightweights methods have shorter time and also good performance? For instance, baselines listed in related work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ix7yYPRjKB", "forum": "VDjbFzbD2f", "replyto": "VDjbFzbD2f", "signatures": ["ICLR.cc/2026/Conference/Submission19764/Reviewer_533u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19764/Reviewer_533u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964217208, "cdate": 1761964217208, "tmdate": 1762931603063, "mdate": 1762931603063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}