{"id": "R96mXloI0e", "number": 18779, "cdate": 1758290810696, "mdate": 1759897081596, "content": {"title": "DR-CFGNN: A Completion-Aware Framework for Counterfactual Explainability in Graph Neural Networks", "abstract": "In this study, we propose a novel framework for counterfactual explainability in graph neural networks (GNNs). To the best of our knowledge, this is the first generic, model-agnostic method for local-level GNN explainability that considers both edge removal and edge assertion. The approach takes advantage of the progress achieved in factual explainability, coupling it with an encoder-decoder deep learning model to learn valid and robust graph expansions. In addition to standard benchmark datasets, we evaluate our method on a new variant of a popular synthetic dataset to study how explainability is influenced by data incompleteness, a common characteristic of real-world graph data. A multi-faceted experimental analysis with both established metrics from relevant literature and novel ones aimed at assessing the validity and the quality of explanations, demonstrates the advancement that our proposed approach brings to state-of-the-art baselines.", "tldr": "A novel framework for counterfactual explainability in graph neural networks (GNN) that considers both edge removals and edge additions.", "keywords": ["Explainable AI", "Counterfactual Explanations", "Graph Neural Networks"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0713651a351f6cb37cd6596a3d290c74b00a6062.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a generic, model-agnostic method for local-level GNN explainability that considers both edge removal and edge assertion. The approach takes advantage of the advanced explanation objectives through an ensemble design. \nExperiments show that the proposed method not only achieves state-of-the-art performance on standard benchmark datasets but also generates more robust explanations when data incompleteness occurs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a novel ensemble framework for counterfactual explainability in graph neural networks (GNNs).\n- The study of the robustness of counterfactual explanations based on noise is interesting.\n- The paper overall is easy to follow."}, "weaknesses": {"value": "1. The method proposed in the paper resembles an ensemble explainer, as it integrates candidate explanations from multiple explanatory objectives. Reporting the best-performing one among these will always yield results no worse than the best explanations from a single objective, and this approach can intuitively improve performance across metrics. However, each of these objectives can be found in existing works such as [1], so the contribution to explanatory methods is not strong.  \n\n2. Regarding the paper’s contributions to the evaluation system, they mainly come from the integration of existing metrics. Although the authors are the first to evaluate the impact of counterfactuals in dealing with incomplete graph data through a noisy-based metric, the relevant theories and implementation methodologies also originate from existing works such as [2]. As one of the key contributions claimed in the paper, this noisy-based metric lacks discussion and analysis on the necessity of its introduction for GNN explanation. This necessity determines whether it can be adopted as a new general metric for subsequent GNN explanation works.  \n\n3. Eq. (3), Eq. (4), and Eq. (5) are confusing. The right-hand side of the equal sign represents the edited adjacency matrix, yet the graph label does not appear in the equations. I am confused about how it optimizes the objective described in the context.  \n\nIn summary, when considering the paper’s contributions to explanatory methods and evaluation methods in isolation, they are not strong. Therefore, it is more necessary to supplement analyses that previous works have not conducted, such as discussing the necessity of introducing the new metric, and explaining why an ensemble is necessary and why a unified objective cannot be used to extract explanations in an end-to-end manner. However, the paper lacks such analyses and fails to provide new impactful insights.\n\n- [1] Joint factual and counterfactual explanations for top-k gnn-based recommendations.\n- [2] Robust counterfactual explanations in machine learning: a survey."}, "questions": {"value": "Please refer to the above weakness section for suggestions and questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PZFMjgfMZ0", "forum": "R96mXloI0e", "replyto": "R96mXloI0e", "signatures": ["ICLR.cc/2026/Conference/Submission18779/Reviewer_2PZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18779/Reviewer_2PZj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760947655875, "cdate": 1760947655875, "tmdate": 1762999995627, "mdate": 1762999995627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for constructing counterfactual graphs through edge addition and edge removal, leveraging a graph classification model and a factual graph explainer. \n\nTo identify minimal edge removals, the method first extracts a subgraph that preserves the original classification result using a factual graph explainer, then exhaustively searches candidate subgraphs by removing edges from this factual subgraph to disrupt the pattern.\n\nFor edge addition, the approach classifies graphs with incomplete motifs and those with complete motifs into two categories, and trains a GNN to predict the edges required to complete the motif so that the graph’s class label changes accordingly.\n\nThe main contribution of this paper lies in exploring the potential of decoupling edge addition and edge removal, and in analyzing their respective roles in generating counterfactual examples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new framework for identifying counterfactual examples in post-hoc graph neural network (GNN) analysis.\n\n2. It explores the potential of separating edge addition and edge removal, and analyzes their respective effects on discovering counterfactual examples.\n\n3. The proposed model demonstrates advantages in specific scenarios, such as recognizing graph incompleteness."}, "weaknesses": {"value": "1. The paper lacks methodological novelty. The edge removal component essentially performs an exhaustive search based on the results of existing factual GNN explainers, while the edge addition component closely resembles traditional link prediction methods. Overall, the contribution is insufficient to meet the novelty standards expected for ICLR.\n\n2. Several ambiguous descriptions appear in key sections, making the paper difficult to follow. More detailed comments are provided in the following section.\n\n3. The reported performance improvement over CFX primarily arises from the experimental setting involving incomplete motifs, which is specifically tailored to favor the proposed edge addition approach. Therefore, this advantage cannot be considered a general one."}, "questions": {"value": "In Definition 3.1, the graph is inconsistently represented as ${A, X}$ and ${M, F}$. Please clarify whether these notations represent different formulations.\nIn Section 4.1, the loss function is expressed as a matrix, which is an unconventional formulation in machine learning. It would be helpful to include an explanation or derivation of how this matrix loss is computed.\nIn Equation (6), $L_{+/-}$ appears to contain the same term as $L_{+}$ or $L_{-}$. Readers may find it confusing why these terms need to appear again in the final loss formulation without a clarification addressing this overlap.\nThe feature mask introduced in the edge-removal method and the feature noise used in the experiments do not seem directly related to the proposed approaches. Please clarify their roles and how they contribute to the main methodology.\nIn Section 5.3, it is unclear why $r = k = 0$ is considered a trivial case. Although the total number of edges remains unchanged, different edge combinations could still alter or disrupt structural patterns in the graph.\nIn Table 1, no explanation size is reported for the CFE models. Since explanation size and PN are typically considered together in a trade-off, including explanation size would allow for a fairer and more complete comparison.\nIn RQ3, the paper notes that adding noise could result in invalid molecular graphs, yet similar concerns are not raised for textual or semantic noise in sentimental analysis. Please clarify why these two types of noise are treated differently."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MukxxAD9Hj", "forum": "R96mXloI0e", "replyto": "R96mXloI0e", "signatures": ["ICLR.cc/2026/Conference/Submission18779/Reviewer_ZXSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18779/Reviewer_ZXSc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680571441, "cdate": 1761680571441, "tmdate": 1762999994764, "mdate": 1762999994764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for local counterfactual explainability that jointly considers edge removals and additions. The experiments indicate that the proposed method outperforms the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated, and the problem is clearly defined.\n- The method is explained clearly."}, "weaknesses": {"value": "- The presentation of tables and figures is difficult to read; some tables are very small and not visually clear. The overall visual presentation needs improvement.\n- Novelty appears limited; the paper does not convincingly position its methodological contribution.\n- The number of baselines is limited, focusing on only one baseline and its variants.\n- The algorithm’s computational complexity is not discussed, and running times are not reported.\n- The graphs used in experiments are small; please provide, preferably in a table (Appendix is fine), the average number of nodes and edges for each dataset.\n- The shared code includes a requirements.txt that references local paths and lacks version specifications, which hinders reproducibility."}, "questions": {"value": "- A random algorithm outperforms the main algorithm on one of the most important metrics. Can you elaborate on why this occurs? Is it due to small graph sizes? If so, why should one prefer the ML-based approach over a random baseline?\n- Can you provide a general requirements.txt with pinned package versions and the Python version? Also, please share details of your runtime environment (e.g., CPU/GPU, memory, and machine specifications) to support reproducibility.\n- Can you report the algorithm’s time complexity and empirical running times?\n\nAdditional suggestions for readability and improvements:\n- Please refer explicitly to the table for RQ4 in Section 5.4.\n- Consider enlarging tables, adopting consistent formatting, and adding visual cues (e.g., row/column grouping, margins, fonts) to improve readability.\n- Strengthen the novelty narrative by clearly contrasting your approach with existing methods and articulating the specific scenarios where your framework offers unique advantages.\nExpand the baseline set to include diverse and stronger comparators to better contextualize performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A62cQnaUIE", "forum": "R96mXloI0e", "replyto": "R96mXloI0e", "signatures": ["ICLR.cc/2026/Conference/Submission18779/Reviewer_FeuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18779/Reviewer_FeuL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800347326, "cdate": 1761800347326, "tmdate": 1762999995194, "mdate": 1762999995194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a counterfactual explanation framework for graph neural networks that aims to identify important subgraphs and provide interpretability through a decomposition-and-masking procedure. While the motivation is sound, the paper suffers from weak novelty justification, and insufficient experimental rigor. Moreover, several key methodological and presentation issues significantly hinder readability and the evaluation of contributions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper addresses the important topic of counterfactual explanations in graph models, which is relevant and timely.\n2.\tThe experimental section provides some visualization and ablation, which indicates implementation effort."}, "weaknesses": {"value": "1. Weak and Redundant Challenge Definition\nThe listed “challenges” are not convincing.\n*\tThe first two challenges both focus on subgraph reasoning or graph expansion, which have already been extensively explored in prior works such as GCFExplainer. Therefore, the motivation for re-stating these as novel challenges is weak.\n*\tFurthermore, the paper does not analyze the limitations of existing “graph expansion” methods (e.g., GCFExplainer), nor explain what unique challenge this work addresses beyond them.\n*\tThe fourth challenge mentions that “most models implement costly search methods,” yet the proposed paper provides no discussion or quantitative analysis regarding computational complexity or runtime cost, which makes this challenge invalid.\n2. Problem Formulation Unclear:  The problem formulation section is confusing and not well aligned with existing definitions of counterfactual explanation. The authors cite Guo et al. and CFExplainer (the baseline), but these works already provide a formal and widely accepted definition of the counterfactual explanation task on graphs. Since the current paper does not define a new problem, the formulation should directly follow this established setting instead of introducing vague “decomposition” and “mask” operations, which belong to the method design, not the problem formulation.\n3. Unclear Method Innovation: Due to the challenges are vague and overlap with existing work, it is difficult to understand the design motivation of the proposed framework. The method seems to combine existing decomposition and masking steps without a clear theoretical or algorithmic novelty. The paper does not demonstrate why these design choices are necessary or how they overcome specific weaknesses of prior methods.\n4. Baseline Selection Insufficient: The experiments only compare with CFExplainer, which is inadequate. At minimum, the authors should include Grad-CAM, or GCFExplainer, the relative methods mentioned in your paper or the paper of CFExplainer, as additional baselines. Without these comparisons, the claimed improvement lacks credibility.\n5. Missing Discussion of Hyperparameters: Key hyperparameter choices are not justified. For example, in RQ3, the perturbation ratio is fixed to 4%, but the paper provides no explanation for this choice, nor any sensitivity analysis. Such details are essential to assess the robustness and fairness of the method.\n6. Experimental Presentation and Formatting Issues: \nThe presentation quality is poor and significantly affects readability:\n*\tSeveral figures (e.g., Figure 2) contain multiple subplots in a single image, making the contents unreadable due to small font and low resolution.\n*\tTable formatting is inconsistent across the paper, with misaligned columns and missing captions.\n*\tFigure labels and axis annotations are too small to interpret.\n*\tOverall, the visual quality and layout do not meet publication standards."}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kr7rz2HRje", "forum": "R96mXloI0e", "replyto": "R96mXloI0e", "signatures": ["ICLR.cc/2026/Conference/Submission18779/Reviewer_R19j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18779/Reviewer_R19j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913867988, "cdate": 1761913867988, "tmdate": 1762999995307, "mdate": 1762999995307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}