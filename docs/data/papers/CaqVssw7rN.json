{"id": "CaqVssw7rN", "number": 24751, "cdate": 1758359956736, "mdate": 1759896751005, "content": {"title": "Particles Don’t Care About Z: Towards Scaling Entropy Estimation of Unnormalized Densities", "abstract": "Computing the differential entropy of distributions known only up to a normalization constant is a long-standing challenge with broad theoretical and practical significance. While variational inference is the most scalable approach for density approximation from samples, its potential in settings where only the unnormalized density is available remains largely underexplored. The central difficulty lies in constructing variational distributions that simultaneously ($i$) exploit the structure of the unnormalized density, ($ii$) are expressive enough to capture complex target distributions, ($iii$) remain computationally tractable, and ($iv$) support efficient sampling. Recently, \\citet{messaoud2024s} introduced P-SVGD, a particle-based variational method leveraging Stein Variational Gradient Descent dynamics, which satisfies all of these constraints and demonstrates promising results in low-dimensional setups. We show, however, that P-SVGD does not scale to high dimensions due to fundamental algorithmic flaws: ($i$) misdiagnosed sensitivity to SVGD hyperparameters, ($ii$) violation of the global invertibility assumption in entropy derivation, and ($iii$) omission of a critical trace-of-Hessian term, along with sub-optimal heuristics, including a divergence-based sampling check that induces mode collapse and loose informal bounds with no practical value. These issues severely limit both correctness and scalability. We propose MET-SVGD, a principled extension of P-SVGD that addresses these flaws, providing a general framework for SVGD hyperparameters selection with global invertibilty and convergence guarantees. This enabled accurate and more scalable entropy estimation in high-dimensional set-ups. Empirically, on entropy estimation benchmarks, MET-SVGD achieves up to a 12$\\times$ and 16$\\times$ accuracy improvement over, respectively, P-SVGD and the most scalable baselines from the SVGD literature. On CIFAR-10 Energy-Based image generation, it improves FID by $80.4\\%$ compared to P-SVGD and achieves 64$\\times$ improved stability. In Maximum-Entropy reinforcement learning, MET-SVGD yields up to $16\\%$ better returns than P-SVGD. We will make our code publically available: \\url{https://tinyurl.com/2esyfx8j}.", "tldr": "We propose a variational method to estimate the entropy of distributions known-up-to a normalization constant.", "keywords": ["Stein Variational Gradient Descent", "Sampling", "Variational Inference", "Entropy"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48f48ad56db58f3a1456f00150313be1405c54db.pdf", "supplementary_material": "/attachment/a5990295b4eef99e4ff350d91ab1291f4108d03b.pdf"}, "replies": [{"content": {"summary": {"value": "The paper introduces MET-SVGD, a nonparametric method for entropy estimation that combines the kernelized Stein discrepancy with a Metropolis–Hastings correction. The approach is motivated by shortcomings in entropy estimation using P-SVGD. The authors identify a missing term in the log-determinant as the true source of divergence in P-SVGD—contrary to prior claims attributing it to violations of the invertibility assumption. To address this, the paper proposes a correction based on Hutchinson’s trace estimator, along with adaptive strategies for step size and kernel bandwidth, and a new convergence criterion."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Careful analysis supports the claim that invertibility, per se, is not the cause of P-SVGD diverging.  \n- The computable Stein-identity–based convergence check is broadly useful beyond MET-SVGD.  \n- Empirical studies on both synthetic and real problems convincingly validate MET-SVGD claim to improve upon P-SVGD.\n- The appendix (preliminaries and derivations) is clear and well organized."}, "weaknesses": {"value": "- The article does not elaborate on the magnitude or interpretation of $L_C$ in the evaluation."}, "questions": {"value": "## Questions\n- What exactly is meant by *global* invertibility of the SVGD map? Does this refer to the composed transport $f$ being globally invertible?  \n- If Proposition 3.1 requires $\\epsilon \\nabla \\phi(x^\\ell)$ to be contractive (see Technicality below), what ensures that this assumption holds throughout optimization?  \n- In Figure 4, it is not clear that $L_C$ corresponds to the step-size $\\to 0$ limit. Could you clarify the expected asymptotic behavior?  \n- In the experiments, do the MET-SVGD runs satisfy the Stein-identity (SI) convergence criterion in practice?\n\n## Technicalities\n- In Proposition 3.1, are you assuming that $\\phi^\\ell$ is contractive? Otherwise, it is unclear how item (iii) in the appendix derivation follows.  \n- Could you elaborate on the final equality on line 2066 and the assumptions required for it to hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wtNA1OzdeR", "forum": "CaqVssw7rN", "replyto": "CaqVssw7rN", "signatures": ["ICLR.cc/2026/Conference/Submission24751/Reviewer_Cp7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24751/Reviewer_Cp7b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584304942, "cdate": 1761584304942, "tmdate": 1762943186058, "mdate": 1762943186058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MET-SVGD, a principled extension of P-SVGD (Messaoud et al., 2024) for entropy estimation of distributions known only up to a normalization constant. The authors identify and correct fundamental algorithmic flaws in P-SVGD that limit its scalability to high dimensions, including: (1) misdiagnosed sensitivity to SVGD hyperparameters, (2) violation of global invertibility assumptions, (3) omission of a critical trace-of-Hessian term, and (4) suboptimal divergence control heuristics. MET-SVGD addresses these issues through several innovations: a unified sufficient condition for global invertibility and log-det approximation;  end-to-end learning of kernel bandwidth and step-size; adaptive determination of sampling steps via Stein Identity;  efficient restoration of the missing trace-of-Hessian term; and  replacement of heuristic divergence control with Metropolis-Hastings correction. The method provides both theoretical guarantees and empirical improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the specific problem of estimating differential entropy for distributions known only up to a normalization constant, a critical challenge with broad theoretical and practical significance. While entropy estimation from samples has been extensively studied, the unnormalized density setting is central to important applications like Energy-Based Models and Maximum Entropy Reinforcement Learning, yet remains underexplored due to scalability challenges.\n\nThe paper effectively articulates the longstanding challenge of entropy estimation for unnormalized densities and clearly identifies limitations in prior work (P-SVGD). It provide rigorous mathematical analysis of P-SVGD's limitations and derive principled solutions with formal guarantees (Propositions 3.1-3.6). The unified condition for global invertibility (Corollary 3.3) is particularly elegant. The evaluation spans multiple domains (Gaussian/GMM entropy estimation, EBM training, MaxEnt RL) with thorough ablation studies demonstrating each component's contribution. Results consistently show substantial improvements over baselines."}, "weaknesses": {"value": "While the paper provides a thorough comparison with SVGD-based methods, it offers minimal comparison to alternative entropy estimation approaches, such as normalizing flows or other MCMC variants capable of handling unnormalized densities.\n\nAdditionally, I strongly recommend including numerical experiments using data from known models, along with well-designed ablation studies that examine the effects of model complexity and dimensionality.\n\nThe paper is quite dense and technical. It would be helpful to include a dedicated section or subsection that clearly explains the proposed method and its algorithm/implementation."}, "questions": {"value": "How does MET-SVGD compare to non-SVGD approaches for entropy estimation from unnormalized densities, such as normalizing flows or other MCMC variants?\nCould you quantify the MH acceptance rate in your CIFAR-10 experiments (e.g., average rate across training)? What strategies might mitigate high rejection rates for complex high-dimensional targets?\nHow should a user choose the number of particles M based on problem dimensionality or complexity? Are there empirical or theoretical guidelines for this parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q6lJ25xqw2", "forum": "CaqVssw7rN", "replyto": "CaqVssw7rN", "signatures": ["ICLR.cc/2026/Conference/Submission24751/Reviewer_TwdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24751/Reviewer_TwdB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904647438, "cdate": 1761904647438, "tmdate": 1762943185738, "mdate": 1762943185738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a direct improvement on the work of Messaoud et al. (2024). The proposed methodology of both works aims to solve the following problem: given an unnormalized density $p$ known only up to normalization constant, produce i) a variational approximation $q$ to $p$, and ii) an estimate of the entropy of $p$.The main improvements are 1) formalization of step size conditions that ensure an invertibility result, 2) an on-the-fly approach to tune necessary sampling hyperparameters, 3) a second-order correction to the trace term that improves empirical (non-asymptotic) sampling, and 4) a sampling-within-Metropolis scheme that corrects for deviations along the sampling trajectory formally compared to the heuristic of Messaoud et al."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is portrayed as a direct extension of previous related work. The concreteness of this formulation simplifies understanding of the contribution being made relative to Messaoud et al.\n- Regarding the contributions, to my knowledge #1 and #3 are unique results that I have not seen before. #2 and #4 are more algorithmic tweaks, but I still recognize the contribution of changes such as these that improve empirical results\n- I appreciate the framing of the approach from a more general perspective, namely the variational inference perspective. This makes the suite of potential problems for which the proposed approach is usable much broader than merely within a reinforcement learning context."}, "weaknesses": {"value": "- I find the paper fairly messy and disorganized. The authors’ contribution does not really begin until the top of page 5, which is quite late. And there are some obvious typos (e.g., line 174, maximizing (not minimizing).\n- The framing relative to Messaoud is clear enough as written, but less obvious to me when I go read Messaoud et al., which is primarily framed as a reinforcement learning paper. That paper does not explicitly propose a SVGD gradient as such, but rather how to apply SVGD to the RL problem. This paper could do a much better job motivating why invertibility and entropy are important from this perspective, as this is mostly taken for granted. \n- Given the above, I find the comparison and discussion with respect to the rest of the SVGD literature to be lacking. It seems to me the others should be comparing their approach to variants of SVGD, such as \\beta-SVGD (Sun and Richtarke, 2022), or at least framing their method with respect to SVGD discretization, which seems poorly understood in the literature relative to the continuous formulation.\n- The experiments sometimes raise more questions than answers: for example, the divergence seen in Figure 7 seems to suggest MET-SVGD can be quite sensitive to some of the settings. Am I reading it correctly that the two best configurations (peach and bright pink) do not utilize the MH adjustment?"}, "questions": {"value": "- Can you elaborate on the use of normalizing flows for entropy estimation (line 94)? Does the user fit the flow and then estimate the entropy empirically with a Monte Carlo estimate?\n- (Line 250) is the velocity $\\phi$ known to be Lipschitz? If so, can you provide a reference or citation?\n- (Line 391) When $q$ is defined as a mixture in this way, does this affect the ease of using $q$ for downstream applications?\n- (Line 399) Convergence of what, precisely? The distribution, the entropy estimator? Convergence in what sense (in distribution, in probability, of real numbers, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RFjZ5oglkQ", "forum": "CaqVssw7rN", "replyto": "CaqVssw7rN", "signatures": ["ICLR.cc/2026/Conference/Submission24751/Reviewer_dBCy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24751/Reviewer_dBCy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955265888, "cdate": 1761955265888, "tmdate": 1762943185380, "mdate": 1762943185380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper deals with estimating differential entropy of a distribution whose density is known only up to the normalization constant. Specifically, a method called MET-SVGD is proposed, which is a refinement of the first-order approximation of the particle distribution in Stein Variational Gradient Descent (SVGD) due to Messaoud et al. (2024, Theorem 3.3). The refinement includes 1) conditions on the step-size for invertibility and log-det approximation, 2) optimized SVGD parameters, 3) corrected derivation OF the particle distribution, and 4) divergence control via Metropolis-Hastings. As a results, the MET-SVGD improved upon Messaoud et al. (2024) by 12 folds in high-dimensional differential entropy estimation benchmarks, 80.4% in FID for image generation, and 16% in maximum entropy reinforcement learning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Resolves the sensitivity of step sizes in Messaoud et al. (2024), including global invertibility conditions.\n2. Adaptively selecting the step sizes and kernel bandwidth by neural networks.\n3. Refines the first-order approximation of Messaoud et al. (2024) by second-order approximation, yet avoiding explicit Hessian computation. \n4. Finite-sample convergence guarantee by employing the Metropolis-Hastings algorithm."}, "weaknesses": {"value": "1. As the authors point out in L396, MET-SVGD is a Metropolis-Hasting (MH) algorithm with a SVGD-based proposal distribution. Therefore it brings SVGD into the arena of Markov chain Monte Carlo (MCMC), which precisely the SVGD framework intends to avoid. \n\n2. By merging the refined approximation of the SVGD-based particle distribution with MH, it is difficult to discern which component is critical in the empirical performance improvement. It may is plausible that if the method of Messaoud et al. (2024) is incorporated into MH to design a proposal distribution, then the performance improves. On the other hand, with the second-order  refinement alone, will it perform better than Messaoud et al. (2024)? In short, is it the refinement or MH that makes MET-SVGD successful?\n\n3. Proposition 3.5 is not precise. In the proof of Proposition 3.5, the MH acceptance probability is only approximated to the first-order. Convergence of MH usually requires the target density is bounded away from zero. To claim the convergence guarantee, conditions for convergence must be carefully checked. \n\n4. Likewise, in Proposition 3.2 the equality is only approximate. \n\n5. The presentation is the paper is too busy and it is hard to follow. I understand that the authors wanted to put as much as details within the page limit, focusing on the main contributions in an expository manner may work better."}, "questions": {"value": "1. In avoiding explicit Hessian calculation by using Hutchinson's scheme, how the additional random variable $v$ is sampled? and what is the recommended sample size? I think this determines the accuracy of approximation. If done coarsely, then it won't be as good as Messaoud et al. (2024).\n\n2. The method of Messaoud et al. (2024) is called parametric SVGD (P-SVGD). Messaoud et al. (2024) never calls their method P-SVGD. Why is it \"parametric\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QiKSa0Gnhz", "forum": "CaqVssw7rN", "replyto": "CaqVssw7rN", "signatures": ["ICLR.cc/2026/Conference/Submission24751/Reviewer_vDmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24751/Reviewer_vDmA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762311479901, "cdate": 1762311479901, "tmdate": 1762943184948, "mdate": 1762943184948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}