{"id": "hav7s0ACAI", "number": 18715, "cdate": 1758290357601, "mdate": 1759897085652, "content": {"title": "Leveraging NLLB for Low-Resource Bidirectional Amharic – Afan Oromo Machine Translation", "abstract": "We present a bidirectional machine translation system for Amharic and Afan Oromo, two low-resource Ethiopian languages critical for cultural and linguistic accessibility. Leveraging pre-trained transformer models, we curated a high-quality parallel corpus of 667,021 human-edited sentence pairs, preprocessed through text normalization, non-target language filtering and dividing the data into training, validation, and test sets in a stratified way.. Using the Hugging Face Transformers library, we fine-tuned a sequence-to-sequence transformer architecture, optimized for the linguistic nuances of Ethio-Semitic and Cushitic languages, with tokenized input and dynamic padding for efficient batch processing.\nOur model significantly outperforms baselines, including Google Translate and NLLB models (600M, 1.3B, 3.3B parameters), which represent industry and research state-of-the-art for low-resource translation. For Amharic-to-Afan Oromo, it achieves a BLEU score of 42.19, surpassing Google Translate’s 9.6. For Afan Oromo-to-Amharic, it scores 42.82, exceeding NLLB-3.3B’s 5.72. Additional metrics (CHRF++, BERTScore) and low loss values confirm its robustness. These results highlight the efficacy of tailored fine-tuning for low-resource language pairs, advancing cross-lingual communication and digital accessibility in multilingual societies.", "tldr": "We optimize NLLB for low-resource Amharic–Afan Oromo translation, achieving large gains over Google Translate and baseline models, with BLEU scores above 42 in both directions.", "keywords": ["Neural Machine Translation (NMT)", "No Language Left Behind (NLLB)", "Natural Language Processing (NLP)", "Multilingual NLP"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbd3c1bc0cd3bd1ea8364d9fe224876990638fdf.pdf", "supplementary_material": "/attachment/68e3d1f1592694930ee9a61d19589a614b1adb10.zip"}, "replies": [{"content": {"summary": {"value": "The authors created a high quality parallel corpus (human translated) for Amharic and Afan Oromo, low resource Ethiopian languages. Further, they finetune the NLLB model on that data after doing some filtering. Their finetuned model for both directions beat NLLB and Google Translate. They evaluated their models using BLEU, chrF++, and BERTScore."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is very well written and experimental setup is clearly explained.\n- I absolutely loved the fact that authors used humans in the loop to create the dataset to ensure quality.\n- The data preprocessing techniques used are also good enough.\n- Their models perform really well compared to NLLB and Google Translate.\n- Qualitative comparison is quite helpful."}, "weaknesses": {"value": "- One of the major weaknesses of the paper is that authors don’t evaluate their models with FLORES200 (it’s called FLORES+ on HF Datasets). I’d request authors to do that.\n- I’m wondering why did they not train models for English<->Amharic and English<->Afan Oromo? This could have helped them translate a lot of things on the web in their native language for education purposes.\n- The authors don’t mention how they create synthetic data of 667K sentences, i.e., data sources.\n- The authors should have also done an ablation study by including all the data (which was not cleaned) to see how much impact it would have created. This would have helped explain the impact of each filtering step.\n- The train/val/test set is created using stratified sampling to ensure robustness."}, "questions": {"value": "- It'd be great if authors can include public link for the model they have trained and the dataset. It will be great for the community. \n- I’d recommend adding Gemini 2.5 Pro as your baseline instead of Google Translate.\n- The abstract says it’s human edited but section 3.1 says it’s human translated. Can you please clarify that?\n - Suggestion: Please cite the Hugging Face Datasets library. I think you used it for data processing in the Loading and Tokenisation section.\n - In section 3.3 Training process, you don’t mention the NLLB model you finetune has how many parameters? I assume it’s 1.1B?\n- Assuming you finetuned NLLB 1.1B model, you could have also done SFT on a small enough LLM (<1B parameters) to see if it does better than encoder model or not.\n- The authors fine tuned the NLLB model in bf16 or fp32?\n- Typos: It’s chrF++ and not CHRF++. Please fix all instances of CHRF++ to chrF++. chrF++  combines both character n-grams and word n-grams to evaluate machine translation output (assuming they used it because sec 3.4 says chrF)\n- Did authors try any other preprocessing filtering steps? For eg, src tgt ratio, lexical diversity for cleaning corpora?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zprO46t23T", "forum": "hav7s0ACAI", "replyto": "hav7s0ACAI", "signatures": ["ICLR.cc/2026/Conference/Submission18715/Reviewer_pVA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18715/Reviewer_pVA3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760653275292, "cdate": 1760653275292, "tmdate": 1762928422720, "mdate": 1762928422720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a bidirectional machine translation system for Amharic and Afan Oromo based on fine-tuning Meta’s No Language Left Behind (NLLB) model. The authors curated a new 667k human-edited parallel corpus, implemented detailed preprocessing (normalization, filtering, stratified data split), and achieved remarkable BLEU scores exceeding those of NLLB and Google Translate by large margins. The paper reports that high-quality, language-specific data and careful fine-tuning can substantially improve translation performance for low-resource languages."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.Excellent empirical performance, with BLEU improvements exceeding 30 points over strong baselines.\n\n2.Meticulous data collection and cleaning, which can serve as a valuable public resource.\n\n3.Comprehensive evaluation using multiple metrics and both translation directions.\n\n4.Important social relevance: the work supports linguistic accessibility and resource creation for underrepresented African languages."}, "weaknesses": {"value": "1.The work lacks algorithmic novelty. The model architecture, training method, and evaluation pipeline follow established practices without introducing new algorithms, loss functions, or optimization techniques.\n\n2.The contribution is primarily data-centric. While the dataset is valuable, the conference typically expects conceptual or methodological innovation beyond dataset construction.\n\n3.No analysis is provided that would yield new insights into model behavior, transferability, or low-resource learning theory.\n\n4.Human or linguistic expert evaluation is absent, limiting confidence in real-world applicability.\n\n5.Missing ablation studies prevent understanding of which design choices most contributed to the gain."}, "questions": {"value": "1.What specific linguistic adaptations or architectural changes were introduced beyond fine-tuning?\n\n2.Could the proposed corpus and pipeline enable new algorithmic insights, or is it mainly an engineering contribution?\n\n3.Would integrating low-rank adaptation (LoRA) or parameter-efficient fine-tuning techniques achieve similar gains more efficiently?\n\n4.Did you test reduced-size fine-tuning to estimate data efficiency?\n\n5.How can this approach be generalized to other low-resource pairs without bespoke corpora?\n\n6.Is the corpus or model publicly released to ensure lasting research impact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dOhnlv41oH", "forum": "hav7s0ACAI", "replyto": "hav7s0ACAI", "signatures": ["ICLR.cc/2026/Conference/Submission18715/Reviewer_9LdM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18715/Reviewer_9LdM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524557995, "cdate": 1761524557995, "tmdate": 1762928422096, "mdate": 1762928422096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds a translation system for Amharic and Oromo, two low-resource languages of Ethiopia. The core contribution involves building a translation corpus of 600k parallel sentences, that is then used for fine-tuning an LLM. The resulting model beats Google Translate and NLLB by a large margin according to multiple metrics.\n\nWhile this is a respectable effort, and surely a very meaningful contribution for the state of NLP technologies in Ethiopia, I don't think that the paper is a good fit for ICLR.\nThe main technical contribution is the data - but the process of curating and creating the data, and assuring its quality, is not actually described in the paper - so it's not clear what other researchers can learn from this effort. The paper is closer to a tech report for this particular model. I want to be very clear that this is very important work, but it doesn't have enough substance for a research paper at ICLR because it is missing insights into the data, ablations and relevant baselines, and more robust testing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a challenging problem: translation between two low-resource languages. This is rarely an area of focus but globally speaking a very relevant objective.\n- The paper measured performance with multiple evaluation metrics, which is a good idea since probably neither of them are optimal.\n- The resulting quality of translations, as evaluated on their own evaluation set, is outperforming existing models by far.\n- The paper contains many experimental details that could help reproducing the experiments (if the data was public)."}, "weaknesses": {"value": "- The data collection is key to the success of the model (and also defines the evaluation distribution), but lacks a lot of information (see questions below) The paper reports sentence length, which is on average rather short (10 words) - which will bias the model towards such sentences.\n- Comparisons with prior work are missing: While previous attempts at this problem with NMT models are cited, they are not used as baselines. With 600k instances, it is very reasonable to expect decent NMT performance (and much cheaper to train since smaller), especially when starting from e.g. M2M-100 (https://aclanthology.org/2022.naacl-main.223.pdf). There should be at least one comparison that utilizes the same data, otherwise it is not clear how much gains are attributed to the data rather than the modeling choices.\n- Evaluation metrics are likely not a great fit since not optimized for the target language. But there is a custom evaluation metric optimized for African languages, including this language pair (https://huggingface.co/masakhane/africomet-stl-1.1). It would be great to also report human evaluation, albeit this is costly.\n- There are no out-of-distribution tests. The model is only tested on data held-out from the training distribution, which is not a public benchmark. Therefore it is hard to calibrate the reported wins. Flores200 (https://github.com/facebookresearch/flores/blob/main/flores200/README.md) would be a standardized benchmark that would be nice to add in addition to the held-out data to test generalization to domains beyond the collected data (and lengths!)."}, "questions": {"value": "- What are sources of data, what type of data, what domains, how does the interaction with human annotators look like and what are their skill levels and how are they remunerated/incentivized, how is the quality controlled for?\n- What do the examples mean? Please provide English descriptions and explanations of the examples in Fig 3 + Fig 4.\n- Could there be any detrimental side effects of the English filtering? Aren't English terms borrowed (adequately) in some contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DXFb28OsrN", "forum": "hav7s0ACAI", "replyto": "hav7s0ACAI", "signatures": ["ICLR.cc/2026/Conference/Submission18715/Reviewer_bVW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18715/Reviewer_bVW4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966241586, "cdate": 1761966241586, "tmdate": 1762928421598, "mdate": 1762928421598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a bidirectional machine translation system for Amharic and Afan Oromo by fine-tuning pre-trained NLLB models on a curated corpus of 667,021 parallel sentence pairs. The authors report significant improvements over existing baselines, achieving BLEU scores of 42.19 and 42.82 for the respective translation directions, substantially outperforming Google Translate and untuned NLLB models. The work emphasizes careful data curation, preprocessing, and language-specific adaptations for these Ethiopian languages."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- **Marginal Empirical Results**:   \nThe reported performance improvements, while predictable, demonstrate that targeted fine-tuning can enhance translation quality for specific language pairs. The multi-metric evaluation approach using BLEU, CHRF++, and BERTScore provides adequate coverage of translation quality dimensions.​​  \n\n- **Data Resource Contribution** :  \nThe dataset would serve as a valuable data resource contribution and would be instrumental in building more high-quality translation models for these low-resource languages.  \n\n- **Thorough Preprocessing Pipeline**:   \nThe authors implement a systematic preprocessing approach addressing orthographic variations, character standardization, and language-specific challenges including Amharic's Fidel script and Afan Oromo's morphological complexity."}, "weaknesses": {"value": "- **Critical Model Specification Omissions**: \nThe paper lacks clear specification of which NLLB model variant serves as the base for fine-tuning. While multiple NLLB sizes are evaluated as baselines, the base model architecture for the fine-tuned system remains unclear, affecting reproducibility.\n\n- **Limited Technical Novelty**:   \nWhile the empirical results are valuable, the methodology consists primarily of standard fine-tuning procedures without architectural innovations or novel training techniques. For a venue like ICLR, stronger technical contributions would typically be expected. Further, within their existing setup a lot of interesting analysis along low-resource languages and cross-lingual transfer could have been conducted which is not present.  \n\n - **Data Contamination Analysis**:   \nGiven that the authors evaluate on their own curated dataset, the absence of word-level overlap statistics, n-gram contamination analysis, or decontamination procedures between training and test sets represents a methodological concern. Such analysis is standard practice when using custom evaluation benchmarks.​​  \n\n - **Missing Cross-Dialectal Evaluation**:   \nThe paper does not explore cross-lingual transfer to related varieties. Given that FLORES-200 includes West Central Oromo, evaluating the Afan Oromo fine-tuned model on this related dialect would provide valuable insights into cross-dialectal generalization capabilities. Further, zero-shot performance between English - Afan Oromo, should also have been tracked to understand the effects.  \n\n- **Limited Baseline Analysis**: \nThe comparison focuses on untuned models versus the fine-tuned system. Including fine-tuned baselines or ablation studies examining different NLLB model sizes as base models would strengthen the experimental design. Further, authors should consider the use of PEFT methods.​\n\n - **Insufficient Analysis of Synthetic Data**:   \nWhile the paper mentions synthetic data augmentation via back-translation, the methodology, scale, and quality validation of this synthetic data are not adequately described.​  \n\n- **Generalizability Claims:**   \nThe assertion that this approach provides \"a scalable framework for other low-resource languages\" requires stronger empirical support beyond a single language pair."}, "questions": {"value": "Please read the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pxdSpKqq8j", "forum": "hav7s0ACAI", "replyto": "hav7s0ACAI", "signatures": ["ICLR.cc/2026/Conference/Submission18715/Reviewer_gVVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18715/Reviewer_gVVr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991897007, "cdate": 1761991897007, "tmdate": 1762928421116, "mdate": 1762928421116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}