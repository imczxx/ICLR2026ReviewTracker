{"id": "43YIq34nYw", "number": 14832, "cdate": 1758244483147, "mdate": 1759897346755, "content": {"title": "Audio-FLAN: An Instruction-Following Dataset for Unified Understanding and Generation of Speech, Music, and Sound", "abstract": "Instruction tuning has generalized well in language and vision, yet audio remains siloed by domain (speech, music, environmental sound) and by task type (understanding vs. generation). We present Audio-FLAN, a large-scale instruction-following corpus that unifies heterogeneous audio sources under a unified instruction schema with instruction, input, and output. It supports both understanding (audio→text) and generation (text/audio/(audio, text)→audio) across speech, music, and general audio. The dataset contains 108.5M instances spanning 23 major and 80 minor tasks drawn from 52 datasets. Instruction tuning on a small subset of Audio-FLAN yields consistent gains on diverse understanding tasks, including zero-shot generalization. We further evaluate the existing generation model and validate Audio-FLAN as an effective benchmark. Hallucination probes inform future data and training design. In summary, Audio-FLAN serves as both an effective training resource and a unified, extensible benchmark for instruction-following audio–language models. We release the dataset on HuggingFace (https://huggingface.co/datasets/Audio-FLAN/Audio-FLAN-Dataset).", "tldr": "This paper introduces Audio-FLAN, a large-scale instruction-following dataset designed to unify audio understanding and generation tasks across speech, music, and sound, enabling zero-shot learning for audio-language models.", "keywords": ["Instruction following", "instruction tuning", "audio-language model", "large language model", "zero-shot learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ea57e3e81f18e07b4f835023342c100b7b4fb20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper released a large (108.5M) speech instruction following dataset, including both speech generation and understanding. Each example follows the schema of (instruction, input, output). Specifically, there are a total of 23 major and 80 minor tasks, which are drawn from 52 datasets. Their experiments on Qwen2-Audio-7b-Instruct further show the model’s improvement on unseen tasks after tuning on a subset of their dataset, indicating the usefulness and validity of their work."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provided a large and diverse speech instruction tuning dataset, which makes a large contribution to the field. To my knowledge, there is no dataset with this scale in the speech domain that exists before.\n2. The authors conducted detailed experiments on their dataset, including zero-shot performance on unseen tasks and hallucination analysis."}, "weaknesses": {"value": "1. If I understand it correctly, the so-called “unseen” tasks are collected using the same pipeline, same LLM. I’m not fully convinced by it as totally “unseen” or out of distribution.\n2. The paper would be more robust if the author showed the improvement on other speech benchmarks after tuning on the proposed dataset.\n3. It is frustrating that there are no results for audio and music generation. The author does mention that their fin-tuned model fails on these tasks. However, this makes me wonder whether there are problems with the curated data for music and audio generation tasks in this proposed dataset.\n4. Lacking a topline(e.g., GPT-4/Gemini…) for all tasks. This makes it hard for the reader to judge the quality of the curated data.\n5. Lacking a random baseline (probably without giving the SpeechLLM audio/speech/music input) in Table 1, which makes people question whether the model really learns non-trivial knowledge after fin-tuning on seen tasks. Especially for some tasks, the performance is quite low after tuning on seen tasks."}, "questions": {"value": "1. Why is the author using HuBERT-Large for evaluating the WER in generation tasks. Why not Whisper models? Whisper models should be more robust in this case.\n2. It would be better to add an explanation for “/” in the caption of Table1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4KXXfFf8u4", "forum": "43YIq34nYw", "replyto": "43YIq34nYw", "signatures": ["ICLR.cc/2026/Conference/Submission14832/Reviewer_eRX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14832/Reviewer_eRX3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761235246955, "cdate": 1761235246955, "tmdate": 1762925182753, "mdate": 1762925182753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AUDIO-FLAN, a large-scale instruction-following dataset that unifies understanding and generation across speech, music, and general audio under a single schema (instruction, input, output). It reports 108.5M instances spanning 23 major and 80 minor tasks aggregated from 52 sources, and provides train/dev/test splits plus zero-shot (unseen task) settings. The authors show performance gains after finetuning Qwen2-Audio."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a large-scale, unified instruction-following dataset for audio, speech, and music understanding and generation. This lays a strong foundation for training large audio-language models.\n\n2. The paper is easy to follow."}, "weaknesses": {"value": "Despite recognizing the motivation and contribution of this paper, the designed and conducted experiments fail to demonstrate the quality of the proposed dataset.\n\n1. The paper claims it is the first benchmark to integrate understanding and generation. However, the evaluation method described cannot evaluate all the tasks provided. Compared to existing benchmarks like Dynamic SUPERB Phase 2, the novelty lies mainly in the generation aspect. However, many proposed tasks—such as Emotional TTS, Descriptive Speech Synthesis, Speech-to-Speech Translation, Emotion Conversion, Text-to-Music Generation, and Text-to-Audio Generation—lack automatic evaluation methods. This also raises the question of how the proposed dataset helps models improve on these tasks.\n2. Why was the zero-shot evaluation only conducted on UniAudio? Experiments on SOTA models like Qwen-2.5-omni, Qwen3-omni, GPT-4o, Gemini, Mimo-Audio, or Kimi-audio would help readers realize the shortages of current State-of-the-Art (SOTA) models.\n3. The paper experiments on outdated models like Qwen2-Audio. I’m wondering if the dataset can provide improvements on other SOTA models like Qwen-2.5-Omni? Please adopt Qwen-2.5-Omni or other SOTA models. it would greatly strengthen this paper.\n4. The experiments are conducted on a 10% training subset. There are no experiments on the full training set to validate the effect of full data, and no ablation on models trained on different subset sizes.\n5. Discussions about dataset licenses are lacking. \n6. The conclusion mentions current limitations (e.g., the dataset mainly focuses on speech-related tasks), how would the authors tackle this issue?\n7. Section 3.6 indicates current issues with the model release. Why didn't the authors take action to solve these problems, such as yes-bias?"}, "questions": {"value": "### Typos\n1. If prior work has been accepted (e.g., AISHELL-3 at Interspeech 2021; Dynamic SUPERB Phase 2; MMAU at ICLR 2025), please cite the conference versions rather than preprints.\n\n### Questions\n1.  Is the human evaluation during dataset construction is sample-based rather than inspecting all instances? If so, please report an analysis of the human evaluation, such as the percentage of failure cases."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Discussions about dataset licenses are lacking. (Conclusion says there are still some constraints)."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gk1Lf2ASes", "forum": "43YIq34nYw", "replyto": "43YIq34nYw", "signatures": ["ICLR.cc/2026/Conference/Submission14832/Reviewer_dKuG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14832/Reviewer_dKuG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359174287, "cdate": 1761359174287, "tmdate": 1762925182379, "mdate": 1762925182379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present Audio-FLAN, a large-scale instruction-following dataset unifying speech, music and general audio tasks under a single schema supporting both understanding (audio->text) and generation (text/audio->audio). The dataset comprises over 100M audio-instruction pairs spanning 80 tasks. Empirical validation demonstrates that instruction-tuning on Audio-FLAN is able to improve performance on understanding and generation, respectively."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This work contributes to the community by creating the largest unified instruction-following dataset to date, comprising 108.5M samples spanning speech, music, and audio domains. The dataset covers 80 diverse tasks, including both well-studied tasks and underexplored areas such as beat-level music reasoning, thereby expanding the scope of research in this field.\n* While not originally proposed, the Self-Instruction [1] pipeline enables a semi-automated approach to scale up and diversify instruction collection while maintaining semantic consistency. This work represents the first application of this pipeline for generating audio instructions.\n\n[1] Wang, Yizhong, et al. \"Self-Instruct: Aligning Language Models with Self-Generated Instructions.\" ACL 2023, 2023."}, "weaknesses": {"value": "* The purpose of creating a unified dataset is unclear if no model is jointly trained on both understanding and generation tasks. It would be valuable to see how these two training paradigms interact and affect each other’s performance.\n* The experimental setup demonstrates limited evidence for the effectiveness of generation tasks, as there is no baseline comparison and the results for text-to-audio or text-to-music generation are missing.\n* The evaluation design for understanding tasks is questionable, where the model is trained on only 10% of Audio-FLAN while designating certain tasks as “unseen” seems arbitrary and may not meaningfully test generalization.\n* The paper does not provide any comparison with existing audio-instruction datasets [2,3] (as well as comparison with existing benchmarks), making it difficult to assess the incremental value of Audio-FLAN relative to prior efforts.\n\n\n[2] Lu, Ke-Han, et al. \"DeSTA2. 5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment.\" 2025.\n\n[3] Goel, Arushi, et al. \"Audio flamingo 3: Advancing audio intelligence with fully open large audio language models.\" 2025."}, "questions": {"value": "* Are there any preliminary experimental results demonstrating the effectiveness of the domain imbalance mitigation strategies mentioned in Section 2.6? What specific approach was actually applied in the current experiments?\n* The Self-Instruct pipeline was originally proposed to enhance the diversity of generated instructions. How diverse are the instructions produced by the current pipeline? Has any text-based analysis (such as lexical or semantic diversity) been conducted to quantify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQKH6Ws7pq", "forum": "43YIq34nYw", "replyto": "43YIq34nYw", "signatures": ["ICLR.cc/2026/Conference/Submission14832/Reviewer_D7te"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14832/Reviewer_D7te"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902895267, "cdate": 1761902895267, "tmdate": 1762925181924, "mdate": 1762925181924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper releases Audio-FLAN, an instruction-following corpus that unifies speech, music, and general audio for both understanding (audio-->text) and generation (text/audio-->audio). Each example follows a Unified Task Template with {instruction, input, output}; audio spans are delimited by <|SOA|>…<|EOA|>. The authors claim 108.5M instances across 23 major / 80 minor tasks from 52 sources, and provide train/dev/test plus zero-shot (unseen-task) splits. They build Self-Instruct paraphrases (GPT-4o --> seed pool; Llama-3.1-70B --> constrained rewrites) and validate automatically + spot-check manually. Experiments: (i) instruction-tune Qwen2-Audio-7B-Instruct on 10% of data (seen-task only), reporting big gains; (ii) evaluate UniAudio for generation tasks; (iii) zero-shot tests; (iv) a hallucination probe showing severe yes-bias and >80% unsupported mentions. Ethics: no raw audio is redistributed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- First benchmark with training data for unified audio models in my understanding.\n- Unified, cross-domain schema that spans speech/music/sound and both directions (U<-->G) with a single JSONL template and audio markers; this lowers friction for multi-task training.  \n- A large task space (23/80) intended to exercise time-sequential reasoning (e.g., beat-level MIR).\n- 108.5M instruction instances spanning from 52 sources is a very novel contribution for the community.\n- Although that speech dominates (100.42M speech vs 2.17M music / 5.91M general-audio) in the dataset, but the domain tagging plus per-domain stats make it feasible to rebalance sampling downstream. \n- For messy, instruction-following responses, the LLM-based extractor + expert review provides a workable normalization method; they also log extraction failures for transparency."}, "weaknesses": {"value": "- Some human verification of the generated data using Llama would definitely strengthen the paper.\n- The authors use LLM as a judge for evaluating open ended generations -  a human - llm correlation is missing.\n- Constrained Llama rewrites “must not alter labels/spans/timestamps,” but only spot-checks are reported."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qe1oNps5Ca", "forum": "43YIq34nYw", "replyto": "43YIq34nYw", "signatures": ["ICLR.cc/2026/Conference/Submission14832/Reviewer_BHhw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14832/Reviewer_BHhw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762412416568, "cdate": 1762412416568, "tmdate": 1762925180981, "mdate": 1762925180981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AUDIO-FLAN, a massive 108.5M-sample instruction-following dataset that unifies speech, music, and general audio under the same instruction template. Unlike most existing corpora that are either (i) speech-only or (ii) understanding-only, AUDIO-FLAN supports both understanding (audio→text) and generative tasks (text/audio/{audio,text}→audio) using a single JSON schema. It covers 23 major and 80 minor tasks collected from 52 datasets, and uses a Self-Instruct-style pipeline to produce stylistically diverse paraphrased instances while preserving semantic correctness.\n\nThe authors show (on Qwen2-Audio) that instruction tuning with just a 10% subset of AUDIO-FLAN yields large gains across speech, music, and general audio — including previously unparseable tasks becoming solvable — and that these gains transfer to zero-shot unseen tasks (especially MIR tasks). They also evaluate UNIAUDIO generation under a unified protocol, showing AUDIO-FLAN can act as a benchmark for instruction-conditioned audio generation. A hallucination study demonstrates that despite improved instruction adherence, hallucination remains severe, motivating augmentation with hard negatives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- first OPEN dataset to unify all three audio domains + both directions (understanding + generation) under one schema\n- extremely large, broad coverage; well-engineered templating and variation\n- strong empirical evidence that small FT on it boosts instruction understanding and zero-shot transfer (more on this below)\n- benchmark design is compatible with preexisting models without architectural changes"}, "weaknesses": {"value": "- extremely speech-heavy distribution (≈ 100M/108M) — imbalance acknowledged\n- relies on LLM-assisted normalization + human adjudication in eval (adds subjectivity and cost)\n- Related to the above, I do not see how hallucinations in generations were handled or reduced. or negative generations were handled \n- generation evaluation only meaningfully covers speech — music generation eval lacking\n- additionally only one model has been used to eval\n- I do not see any insights on the actual data, like difficulty level etc. This makes it hard to determine how useful the data is for frontier model training.\n- hallucination still extreme — dataset alone does not fix it\n- I am not fond of the eval setup, while I acknowledge that fine-tuning from scratch is hard, but fine-tuning an open-weights model (where training data is not known) says little about the efficacy of the data. The additional problem is LoRA fine-tuning. LoRA fine-tuning does not give a model new capabilities (mentioned in several papers) -- so how is FT on the model actually helping on downstream tasks? \n- I dont se e"}, "questions": {"value": "- What is the philosophy of the eval setup? how was it determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "etRloNcRRm", "forum": "43YIq34nYw", "replyto": "43YIq34nYw", "signatures": ["ICLR.cc/2026/Conference/Submission14832/Reviewer_kEqo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14832/Reviewer_kEqo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762453122260, "cdate": 1762453122260, "tmdate": 1762925179076, "mdate": 1762925179076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}