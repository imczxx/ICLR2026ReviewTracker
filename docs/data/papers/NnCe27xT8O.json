{"id": "NnCe27xT8O", "number": 6670, "cdate": 1757991731860, "mdate": 1759897901964, "content": {"title": "MoVie: Multimodal Video Compression with Text Guidance", "abstract": "Recent advances in deep video compression have significantly improved rate-distortion performance. Compared to traditional codecs that rely on handcrafted motion estimation and block-based prediction, deep learning-based methods can learn more flexible and content-adaptive representations, leading to better compression efficiency. However, most existing approaches still focus primarily on low-level pixel motion modeling and lack semantic awareness, which limits their ability to preserve perceptual quality in complex scenes. In this paper, we propose **MoVie**, a **M**ultim**o**dal **Vi**d**e**o compression framework built upon a Text-guided Video Transformer–CNN mixed block(Text-VideoTCM). Instead of relying on image-oriented feature extractors that ignore temporal cues, we design a video-focused network, jointly modeling local spatial structures and temporal dynamics, achieving a remarkable trade-off between computational cost and perceptual performance. To enhance semantic perception, a dual-stage text fusion mechanism is introduced: Extractor modules distill text-aware features at early layers, while Injector modules inject refined semantics in deeper stages. We also introduce a new recipe history-conditioned coding that adaptively leverages both previous and aggregated historical frames, alongside a spatial-channel factorized entropy model tailored for window-based Transformer, which jointly captures local spatial structures and inter-channel dependencies. Averaged over the UVG and MCL-JCV datasets, MoVie achieves substantial BD-rate reductions relative to HM: **-50.23%** for FID and **-28.27%** for LPIPS. While maintaining superior perceptual quality, our method substantially reduces computational cost, requiring only **55.76%** of the per-pixel kMACs of DCVC-FM.", "tldr": "MoVie is a text-guided multimodal video codec that unifies video-centric Transformer–CNN blocks with history-conditioned coding, achieving large perceptual BD-rate gains (−50.23% FID/−28.27% LPIPS vs. HM) with ~44% less compute than DCVC-FM.", "keywords": ["Video Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59d8427cc883040c2134a9755f68d3ea91c5faeb.pdf", "supplementary_material": "/attachment/2676eb4fd43eecfc7d93c4787b56155a602d63f8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MoVie, a multimodal video compression framework that integrates textual\nguidance into the video compression process. The model introduces a Text-guided Video\nTransformer–CNN Mixed block (Text-VideoTCM), which combines a Video Swin Transformer and\n3D CNN for spatiotemporal modeling while incorporating semantic cues from CLIP-derived text\nembeddings via Extractor–Injector modules. Furthermore, the authors design a spatial–channel\nfactorized entropy model with history-conditioned coding to improve long-term temporal\nprediction and rate control. Experiments on UVG, MCL-JCV, and HEVC benchmarks show\nsubstantial BD-rate gains in perceptual metrics (FID, LPIPS), while maintaining low\ncomputational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of textual semantics into learned video compression is timely. By leveraging\nCLIP-based text embeddings, MoVie bridges semantic understanding and rate–distortion\noptimization, moving toward perceptual and content-aware compression. The dual-stage\nExtractor–Injector mechanism provides a new way to incorporate text guidance into\ntemporal modeling.\n2. Compared with Transformer-heavy models (e.g., VCT, DCVC-TCM), MoVie achieves a\nremarkable reduction in per-pixel MACs (only 55.76% of DCVC-FM) while maintaining\ncompetitive perceptual performance. This shows that the hybrid VideoTCM design is efficient\nand practical for deployment. The model attains a strong balance between compression\nefficiency and perceptual fidelity.\n3. The paper is well organized, the writing is professional, and all figures and tables are clear,\nallowing readers to understand the model pipeline instantly."}, "weaknesses": {"value": "1. Lack of comprehensive comparison. The current evaluation primarily compares MoVie with\ndistortion-oriented models such as DCVC, DCVC-TCM and VCT in Figure 4. However, it does\nnot include perceptual-optimization baselines like PLVC [r1] or GLC-Video [r2], which are\ndirectly relevant for perception–distortion trade-off analysis. Without such comparisons, it\nremains unclear whether MoVie truly outperforms existing perceptual video codecs in\nbalancing visual quality and bitrate efficiency.\n2. Unclear motivation for Text-VideoTCM design. As a core contribution, the design rationale of\nthe Text-VideoTCM module is underexplained. The idea of using two successive stages for\nextraction and injection is presented as an architectural choice but lacks clear theoretical or\nempirical justification. It is not evident why this two-step process is superior to a simpler\nsingle-stage cross-attention mechanism. The current explanation appears heuristic rather\nthan principle-driven.\n3. Limited novelty beyond prior multimodal compression works. While MoVie emphasizes the\nuse of textual priors, this idea was first introduced in TACO for text-guided image\ncompression. MoVie does not clearly extend this concept to address video-specific challenges\nsuch as temporal semantic consistency or cross-frame alignment. Consequently, its\nmultimodal contribution seems incremental rather than substantially innovative.\n4. Minor technical issue. In Figure 2, the notation “x” is overloaded — it denotes both the\ninput/output in the left figure and the representaions after fusion conv in the right part.\n[r1] Yang, Ren, Radu Timofte, and Luc Van Gool. \"Perceptual Learned Video Compression with\nRecurrent Conditional GAN.\" IJCAI. 2022.\n[r2] Qi, Linfeng, et al. \"Generative latent coding for ultra-low bitrate image and video\ncompression.\" IEEE Transactions on Circuits and Systems for Video Technology (2025)."}, "questions": {"value": "1. How does MoVie perform compared to PLVC and GLC-Video in terms of perception–distortion\ntrade-off?\n2. What motivated the use of a two-stage Extractor–Injector design instead of a unified fusion\nlayer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJhoCWA5uq", "forum": "NnCe27xT8O", "replyto": "NnCe27xT8O", "signatures": ["ICLR.cc/2026/Conference/Submission6670/Reviewer_6zig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6670/Reviewer_6zig"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776388268, "cdate": 1761776388268, "tmdate": 1762918977005, "mdate": 1762918977005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called MoVie, a perception-oriented video compression method. MoVie introduces textual information at the encoder to enhance perceptual compression quality. Compared to previous objectively oriented compression methods, it achieves a significant improvement in perceptual quality while also substantially reducing complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves a significant improvement in perceptual quality compared to previous approaches.\n2. The proposed method achieves a significant reduction in computational complexity.\n3. The design of the text information introduction module and the entropy model has some innovations.\n4. This paper is easy to understand."}, "weaknesses": {"value": "1. The necessity of introducing text information is not adequately verified.\n2. The methods used in the comparisons are all objective compression methods, which cannot determine whether the improvement in perception quality is due to excellent model design or simply the introduction of a loss function related to perception quality."}, "questions": {"value": "1. The necessity of introducing text still requires further validation. For example, keeping the complexity identical during training but replacing the text input with a constant, and then training another set of models, would provide more convincing evidence. The paper only presents comparisons with different text inputs after training is completed, as well as comparisons where complexity is not kept consistent (I understand that the complexity of \"+VideoTCM+S-C\" and \"+VideoTCM+S-C-Text\" should be different, due to the \"Extractor\" and \"Injector\"?). Therefore, I still have doubts about the actual contribution of the textual information to the final perceptual quality.\n2. I can understand introducing textual information at the decoder side, as in compression scenarios the decoder can be viewed as video generation, and using text for a generation task is quite natural. However, due to causality considerations, the authors only introduce textual information at the encoder side. Since the encoder's typical role is de-correlation, I am uncertain whether the textual information truly contributes to this process. Does the encoder in perception-oriented video compression have functions beyond de-correlation? Could the authors further explain the rationale for introducing textual information at the encoder side, beyond just causality concerns?\n3. Are there any other perception-oriented video compression methods available for comparison? For example, “Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Dz1h6M2hc", "forum": "NnCe27xT8O", "replyto": "NnCe27xT8O", "signatures": ["ICLR.cc/2026/Conference/Submission6670/Reviewer_MK3V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6670/Reviewer_MK3V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801546233, "cdate": 1761801546233, "tmdate": 1762918976593, "mdate": 1762918976593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoVie, a multimodal video compression framework using text guidance to improve semantic awareness and perceptual quality. The core contribution is the Text-guided Video Transformer-CNN Mixed block, combining a Video Swin Transformer and a 3D CNN to efficiently model temporal dynamics and spatial details. The framework employs a dual-stage text fusion mechanism to integrate semantic priors. It also introduces a history-conditioned coding strategy and a spatial-channel factorized entropy model to capture long-term dependencies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a video compression framework that leverages textual priors and cross-frame fusion to achieve semantically guided and perceptually coherent compression. Experimental results demonstrate that the proposed method substantially improves perceptual quality metrics (e.g., FID, LPIPS) compared to mainstream approaches."}, "weaknesses": {"value": "**Limited Novelty**\n\nThe central idea of incorporating text guidance in the encoder is largely derived from prior work in image compression, particularly TACO [1], without introducing substantial modifications for video compression. Moreover, the Transformer–CNN hybrid architecture is not novel, as similar designs have been explored in [2]. The spatial–channel entropy model is also well established in the video codec research domain.\n\n**Incomplete Experimental Comparisons**\n\nThe paper positions itself as a strong perceptual compression model, yet it does not compare against other specialized baselines designed explicitly for perceptual quality. The comparison is limited to standard SOTA codecs (VCT, DCVC) rather than dedicated perceptual models [3,4,5]. This omission makes the claim of state-of-the-art perceptual quality less robust. Examples of such omitted baselines include:\n\n\n**High Sensitivity to Text Accuracy** \n\nThe model’s performance heavily depends on the quality and relevance of input text. As shown in the authors’ stability analysis (Section 4.4, Table 2), irrelevant or missing captions markedly degrade all metrics (bpp, PSNR, FID), raising concerns about robustness in real-world scenarios where captions are generated by error-prone VLMs rather than verified by humans.\n\n**Complexity issues**\n\nSince the captions are generated by large language models (LLMs), the computational complexity of these models should also be taken into account in the overall encoding process.\n\n\n[1] Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity (Lee et al., ICML 2024) \n\n[2] Learned Image Compression with Mixed Transformer-CNN Architectures (J. Liu et al., CVPR 2023)\n\n[3] \"Perceptual Learned Video Compression with Recurrent Conditional GAN\" (Yang et al.)\n\n[4] \"Extreme Video Compression with Pre-trained Diffusion Models\" (Li et al.)\n\n[5] \"I2VC: A Unified Framework for Intra-& Inter-frame Video Compression\" (Liu et al. )"}, "questions": {"value": "- Could the authors provide a specific breakdown of the decoder's complexity (kMACs or actual FPS/latency)? The proposed spatial-channel factorized entropy model appears to have sequential dependencies (Section 3.3) that could create a decoding bottleneck, which is not captured by the reported autoencoder-only MACs.\n\n- Why was the image-based Fréchet Inception Distance (FID) used to evaluate video perceptual quality instead of the more standard and appropriate Fréchet Video Distance (FVD), which is designed to assess temporal consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NPcchscZoe", "forum": "NnCe27xT8O", "replyto": "NnCe27xT8O", "signatures": ["ICLR.cc/2026/Conference/Submission6670/Reviewer_S5kW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6670/Reviewer_S5kW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986904374, "cdate": 1761986904374, "tmdate": 1762918976229, "mdate": 1762918976229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a text-guided learned video compression scheme (termed MoVie). Like VCT, MoVie skips low-level motion estimation and motion compensation, in the hopes of preserving perceptual quality in complex scenes. To this end, the proposed feature extractor incorporates 3D window-based Transformer and 3D CNN to create a video-focused latent generator (i.e. encoder). This is distinct from the image-based feature extractor adopted by VCT. Moreover, to enhance semantic preservation, the proposed feature extractor adopts a dual-stage text fusion mechanism. Reportedly, it outperforms the SOTA DCVC codecs in terms of FID and LPIPS. Furthermore, it requires only half of the kMAC/pixels of DCVC-FM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The performance results look interesting. \nThe dual-stage text fusion mechanism appears novel. \nThe low kMAC/pixel is another striking point of this work. \nThis is one of the few early attempts at textual-guided video compression for perceptual quality optimization."}, "weaknesses": {"value": "1. The comparison with the DCVC series is unfair in that they are not really optimized for perceptual quality. Neither does VCT. I understand that there are only few perceptually optimized learned video codecs, the code of which may not be available. For a fair comparison, it would be instructive to see how MoVie compares with the DCVC series of work if its training objective involves only MSE or MS-SSIM as the only distortion metric. That is, I would like to see the performance of MSE-optimized MoVie and MS-SSIM-optimized MoVie for a fair comparison with the other baselines, given that it is nearly impossible to re-train these codecs for perceptual quality. \n2. Line 202: While I understand that the textual description is a form of global information, it does not make much sense to me that such global information is only used on the encoder side but not on the decoder side. The argument that injecting it into entropy coding or decoding would break frame-wise causality is NOT very convincing. The textual information can be signaled in the bitstream with a minimal impact on the compressed bit-rate. Since it can be easily made available on both sides as causal contextual information, why not use it? It would be instructive to see the performance of the following 4 variants: Encoder (Text-VideoTCM vs. VideoTCM) + Decoder (Text-VideoTCM vs. VideoTCM). \n3. Please report encoding and decoding times as the block-wise spatial AR modeling is involved. Note that VCT allows multiple blocks to be decoded simultaneously. This does not seem possible with the spatial AR modeling.  \n4. Like VCT, the proposed method does NOT perform explicit motion estimation and compensation. It would be instructive to see how the proposed method works on those fast- or complex-motion sequences. In addition, to compensate for motion, VCT has an extended search window in the latent space (y space). With VideoTCM (the left part of Fig. 3) for fusing y_{i-1} and y_{<i}, it appears that there is no notion of extended window. This design reinforces my impression that motion compensation is done implicitly via VideoTCM."}, "questions": {"value": "Line 247: I suppose that the textual description is a sentence composed of a sequence of words. However, it appears that only ONE global textual token of 512 dimensions serves as the query to aggregate information from visual tokens extracted from each video frame. Why only ONE token? Or there are actually multiple textual tokens. If this is the case, the description needs updating. In addition, to my surprise, only one linear layer is sufficient to project visual tokens into the textual space of CLIP and vice versa. \nIn Fig. 2, it is unclear whether only one textural description is used for the entire test sequence (of 96 frames long or even longer). \nPlease clarify how many frames are processed together as a whole. How does the proposed method impact the coding delay? Most learned codecs process video on a frame-by-frame basis for both feature extraction and coding. They could support low-delay or random-access applications depending on the temporal prediction structure. But, with the proposed scheme, this part becomes unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CitOxWIL4b", "forum": "NnCe27xT8O", "replyto": "NnCe27xT8O", "signatures": ["ICLR.cc/2026/Conference/Submission6670/Reviewer_pyyR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6670/Reviewer_pyyR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132648400, "cdate": 1762132648400, "tmdate": 1762918975305, "mdate": 1762918975305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MoVie, a multimodal video compression framework that integrates textual guidance into a spatiotemporally-aware deep autoencoder. The core contribution is a Text-guided Video Transformer-CNN Mixed block (Text-VideoTCM) that fuse CLIP-derived textual semantics with localized spatiotemporal dependencies using a dual-branch Video Swin Transformer and 3D CNN architecture. MoVie also proposes a history-conditioned, spatial-channel factorized entropy model to better capture temporal and channel dependencies for improved rate control and perceptual quality. Extensive experiments on UVG, MCL-JCV, and HEVC datasets show substantial BD-rate reductions (notably on perceptual metrics) and compute efficiency compared to strong learned and traditional baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1) Compelling integration of multimodal guidance:**  MoVie leverages CLIP text features through a well-motivated two-stage fusion (Extractor and Injector), improving semantic coherence in reconstructions.\n\n**2) Innovation in architecture:** The mixed Video Swin Transformer and 3D CNN block (Text-VideoTCM) cleverly balances localized detail and high-level semantics.\n\n**3) Sound entropy modeling:** The proposed spatial-channel factorized entropy model conditions on both prior and historical frames, which is well justified for videos."}, "weaknesses": {"value": "**1) Limited theoretical transparency in entropy model:** Although the spatial-channel factorized entropy model is formally described (Figure 3, Section 3.3), the concrete mathematical instantiation, especially how channel groupings interact with window-based spatial splits and how sequential channel entropy coding unrolls over temporally cached states, is not fully specified. For example, Equation (in Section 3.3) does not clarify the per-slice sampling or thresholding mechanism, which is vital for reimplementability and for peers to verify training stability. \n\n**2) Potential overclaiming on flexibility to imperfect or absent semantic guidance:** Table 2 and Figure 5’s analysis shows performance degrades—or at least shifts—substantially with irrelevant or missing text, calling into question the practical robustness. The fallback template approach is a start, but the method is clearly sensitive to real-world inconsistencies in text annotation. This concern is acknowledged in the conclusion, but should be emphasized more clearly throughout as a limitation.\n\n**3) Empirical comparison for computational efficiency could be more granular:** While Table 1 and Figure 1 communicate MAC reductions, there’s no breakdown by network component (e.g., SwinT vs. 3D CNN paths, Extractor/Injector overhead, patch merging, and entropy module). Additionally, for Table 3’s ablation, reporting wall-clock time or GPU memory usage in addition to kMACs would provide better practical insight for deployability."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e99n1w27rs", "forum": "NnCe27xT8O", "replyto": "NnCe27xT8O", "signatures": ["ICLR.cc/2026/Conference/Submission6670/Reviewer_ytLC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6670/Reviewer_ytLC"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135471849, "cdate": 1762135471849, "tmdate": 1762918974755, "mdate": 1762918974755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}