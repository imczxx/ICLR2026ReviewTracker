{"id": "G6UoTWi5oC", "number": 10585, "cdate": 1758176634540, "mdate": 1763015289727, "content": {"title": "Random Effect Bandits using h-Likelihood", "abstract": "Stochastic multi-armed bandit (SMAB) is a fundamental framework for sequential decision-making in reinforcement learning, where an agent must balance exploration and exploitation to maximize cumulative rewards. Recently, random effect SMAB has been proposed where reward feedback is modeled as random effect. However, it has not been well formulated yet in likelihood perspectives. Furthermore, individual noise variance can be arm-dependent. We propose a novel random effect upper confidence bound\n(ReUCBHL) algorithm, based on h-likelihood. The likelihood approach is conceptually easy and can be implemented by simply minimizing the loss (negative h-likelihood). The algorithm can be applied to SMAB with univariate and multivariate rewards under arm-dependent noise variances. It can be further extended to  contextual multivariate bandit. Theoretical justification and simulation studies demonstrate that ReUCBHL consistently achieves better regret performance compared to the baseline algorithms. These results highlight the effectiveness of the proposed algorithm.", "tldr": "We propose random effect upper confidence bound algorithm based on h-likelihood procedure for stochastic multi-armed bandit with arm-dependent noise variance.", "keywords": ["h-likelihood", "random effect model", "reinforcement learning", "stochastic multi-armed bandit"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/871bf98a0c931a69f846fdba361fd2f19cba71e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates random effect bandits where each arm's expected reward is sampled i.i.d from the distribution $N(\\mu_0, \\sigma_0^2)$. The paper's novel contribution is the consideration of arm-dependent noise, and achieves a variance-dependent regret guarantee. Simulation results validate the efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully derives a variance-dependent regret guarantee, achieved by explicitly modeling arm variance.\n\n2. Simulation results confirm the proposed algorithm's effective performance."}, "weaknesses": {"value": "1. Though the author provides a variance-dependent regret guarantee, there lacks a crucial comparison with the results from the more general stochastic multi-arm bandit (MAB) problem. In fact, Random Effect Bandits is a sub-class of the stochastic MAB problem, as the expected reward for all arms must be sampled from the same distribution $N(\\mu_0, \\sigma_0^2)$, whereas in the general MAB setting, each arm can have an arbitrary expected reward $\\mu_k$.\n\nIt is generally expected that the regret is much smaller when the problem setting is more restricted. However, it seems that the classic analysis for the instance-dependent regret for the multi-armed bandit problem can directly yield an improved result. (See Theorem 8.1 in the classic textbook for bandit theory by Lattimore and Szepesvári [1]).\n\n[1] Lattimore, Tor, and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.\n\nIn detail, when all expected rewards are sampled from $N(\\mu_0, \\sigma_0^2)$, with high probability, the gap between the optimal reward and the sub-optimal reward is at least $\\Omega(\\sigma/\\sqrt{\\log K})$. Under this situation, Theorem 8.1 directly implies an $\\mathcal{O}(\\log T)$ regret, while the ReUCBHL algorithm only achieves an $\\mathcal{O}(\\sqrt{T})$ regret.\n\nIn summary, this work proposes a new algorithm for a more restrictive setting, but the regret guarantee is worse than the result obtained by applying a classic bandit algorithm for a general environment to this specific setting. This highly impacts the theoretical contribution of this work.\n\n2. On the other hand, the experiments only provide simulation results. Even though these results support the efficiency of the proposed algorithm, simulation results cannot individually support the contribution of this paper without validation on real-world or complex benchmark datasets."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GwCwRJNvZW", "forum": "G6UoTWi5oC", "replyto": "G6UoTWi5oC", "signatures": ["ICLR.cc/2026/Conference/Submission10585/Reviewer_Hp8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10585/Reviewer_Hp8H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933304424, "cdate": 1760933304424, "tmdate": 1762921854312, "mdate": 1762921854312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "IN7OIJEnsa", "forum": "G6UoTWi5oC", "replyto": "G6UoTWi5oC", "signatures": ["ICLR.cc/2026/Conference/Submission10585/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10585/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763015288919, "cdate": 1763015288919, "tmdate": 1763015288919, "mdate": 1763015288919, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recently, random effect stochastic multi-armed bandit (SMAB) has been proposed where reward feedback is modeled as random effect. However, it has not been well formulated yet in likelihood perspectives. Furthermore, individual noise variance can be arm-dependent. This paper proposes a novel random effect upper confidence bound (ReUCBHL) algorithm, based on h-likelihood. The likelihood approach is conceptually easy and can be implemented by simply minimizing the loss (negative h-likelihood). The algorithm can be applied to SMAB with univariate and multivariate rewards under arm-dependent noise variances. It can be further extended to contextual multivariate bandit. Theoretical justification and simulation studies demonstrate that ReUCBHL consistently achieves better regret performance compared to the baseline algorithms. These results highlight the effectiveness of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The studied problem, stochastic multi-armed bandit (SMAB), is a fundamental problem in online learning, and has various applications such as clinical trials, recommendation systems and robotics.\n2. This paper proposes to formulate random effect SMAB from the likelihood perspective and consider arm-dependent noises.\n3. This paper designs a novel random effect upper confidence bound (ReUCBHL) algorithm, based on h-likelihood, and extends it to the multivariate SMAB setting. Both regret guarantees and empirical evaluations are provided."}, "weaknesses": {"value": "1. The authors should discuss more on the motivation of the proposed random effect SMAB formulation by connecting it with real-world applications, especially when compared to existing random effect SMAB formulations. For example, what are the advantages of considering arm-dependent noises?\n2. Can UCB, BUCB and TS be applied to the proposed random effect SMAB problem with arm-dependent noises? In other words, is the comparison in empirical evaluations a fair comparison? The authors should discuss more on this point.\n3. Minor comment: The format of the algorithm pseudo-codes can be improved."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2knon4XTBX", "forum": "G6UoTWi5oC", "replyto": "G6UoTWi5oC", "signatures": ["ICLR.cc/2026/Conference/Submission10585/Reviewer_6unJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10585/Reviewer_6unJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829335690, "cdate": 1761829335690, "tmdate": 1762921853504, "mdate": 1762921853504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies two variants of MABs, the random effect SMAB with arm-dependent noise and multi-dimensional contextual random effect SMAB with arm-dependent noise. Extremely simple methods, maximizing the corresponding h-likelihood functions, are introduced to select"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I couldn't find strengths in this submission. Please see the Weaknesses part."}, "weaknesses": {"value": "- The paper did not justify the necessity and the challenges of the new problem settings and their importance. Formulations (3) and (6) appear more structural than the conventional settings, hence making them easier problems.\n\n- The paper is poorly written\n  - Equations (2) vs (3).\n  - Section titles for Section 2.3 and Section 3.\n  - Notations \\Sigma and D_K are not defined.\n  - Disconnection from the main algorithm (Table 1) and the key notion h-likelihood (line147).\n  - Duplicate “The simple joint maximization …” (lines 150 and 355) without further justifications.\n  - The forms of Theorems 1 and 2 differ drastically, but the formulations (3) and (6) are close. There is no discussion of the difference.\n  - The curves in Figures 1 and 2 do not appear to be experimental outcomes.\n  - Conflicting claims: line 247 vs line 482.\n\n- There is no LLM Usage section in this paper, but the paper contains several mistakes that a human will not make.\n  - Reversed names in two references: Shipra Aggarwal and Goyal Navin\n  - In consistent names in the link of the MovieLens dataset\n  - The Kyungbok Lee et. al paper published in AAAI cannot be PMLR.\n  - The reading experience is locally coherent, but many mismatches, even conflicts, globally. (This can be too objective.)"}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xt1vSWPGeQ", "forum": "G6UoTWi5oC", "replyto": "G6UoTWi5oC", "signatures": ["ICLR.cc/2026/Conference/Submission10585/Reviewer_gcQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10585/Reviewer_gcQm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896079232, "cdate": 1761896079232, "tmdate": 1762921853112, "mdate": 1762921853112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the h-likehood approach to derive UCB for stochastic multi-armed bandits with arm-dependent noise model. The proposed algorithm, ReUCBHL, enjoys the same regret bounds as prior baselines, and outperform the baselines when the arm noises are arm-dependent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Better empirical performance: The proposed algorithm enjoys better empirical performance. \n2. New Statistical Technique: The paper introduces the h-likelihood, an MLE method considering the arm-dependent noise, to construct the UCB index."}, "weaknesses": {"value": "1. The writing of this paper is hard to follow. In the intro’s last paragraph, the author mentioned h-likelihood multiple times, but the definition or its intuition is not explained.\n    - In Table 1, Line 2, the $k$ should be $K$.\n    - Line 205, it is not exactly MLE that UCB uses. It uses concentration inequalities. \n    - Table 2, Line 8, should it be equation (7)?\n2. Lack of novelty: Although the algorithm performs better, it relies on a strict Gaussian parametric assumption (both the reward and the variance of the noise). Therefore, the new technique, h-likelihood, for UCB construction seems to be an application of the MLE together with the property of the Gaussian distribution itself. If that is true, then the techniques used in this paper are basic and difficult to extend to more general settings.\n    1. What’s the difference between h-likelihood and the likelihood calculated by MLE?\n3. It would be better to derive the instance-dependent bounds, as is a common practice for applying UCB-like algorithms to stochastic bandits.\n4. The proof of Theorem 2 is informal, and the regret definition used in the proof is different from that in the main paper (Section 4.3)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qkrvdk28tu", "forum": "G6UoTWi5oC", "replyto": "G6UoTWi5oC", "signatures": ["ICLR.cc/2026/Conference/Submission10585/Reviewer_xSg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10585/Reviewer_xSg4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033405880, "cdate": 1762033405880, "tmdate": 1762921852677, "mdate": 1762921852677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}