{"id": "rBFDvZu6pb", "number": 875, "cdate": 1756821699060, "mdate": 1759898237670, "content": {"title": "Towards Spatial Supersensing in Video", "abstract": "We frame spatial supersensing in video as an overarching goal for multimodal intelligence and argue that progress requires a shift from long-context brute force to predictive sensing. Using a four-level taxonomy: semantic perception, streaming event cognition, implicit 3D spatial cognition, and predictive world modeling, we audit existing benchmarks and show they focus heavily on the first tier, with only partial coverage of streaming and spatial cognition, and almost never test true world modeling. To ground these gaps, we introduce VSI-Super, a two-part benchmark for continual spatial sensing: VSO (long-horizon spatial observation and recall) and VSC (continual counting under changing viewpoints and scenes). These tasks admit arbitrarily long video inputs and are specifically built so that simply scaling tokens or context length isn’t enough. Within the current paradigm, we push spatial cognition by curating VSI-590K and training a new family of video MLLMs that deliver 30% absolute on VSI-Bench without sacrificing general semantic perception. Yet these models still underperform on VSI-Super, exposing a paradigm gap. We then prototype predictive sensing: a self-supervised next latent-frame predictor whose surprise (prediction error) drives long-horizon memory and event segmentation. On VSI-Super, this approach substantially outperforms leading video MLLMs, evidencing that advancing spatial supersensing requires models that not only see but also anticipate, select, and organize experience.", "tldr": "", "keywords": ["Multimodal Large Langauge Model", "Super Sensing Model", "Spatial Understanding", "Video Understanding", "Memory"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e336d2d2c252d5bb899ed8a194e4379ad49fa5c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes spatial supersensing as a framework for video understanding with four levels: semantic perception, streaming event cognition, implicit 3D spatial cognition, and predictive world modeling. It contributes: (1) VSI-SUPER, a benchmark for continual spatial sensing with two tasks (VSO for long-horizon recall, VSC for continual counting), (2) VSI-590K, a large-scale spatial instruction-tuning dataset combining real, simulated, and pseudo-annotated data, (3) Cambrian-S, a family of video MLLMs achieving SOTA on spatial tasks, and (4) a predictive sensing approach using latent frame prediction for memory management and event segmentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: It correctly and timely identifies that current video benchmarks focus on semantic perception while neglecting spatial/temporal reasoning.\n\nS2: It creates a Comprehensive dataset, VSI-590K which combines real annotated, simulated, and pseudo-annotated data with clear methodology.\n\nS3: Strong Baseline as Cambrian-S and novel and creative application of prediction error/\"surprise\" for both memory compression and event segmentation."}, "weaknesses": {"value": "W1: Claims \"data alone isn't enough\" doesn't have proper justification. VSI-590K focuses on spatial cognition, not streaming/continual tasks. So, the failure on VSI-SUPER doesn't prove data is insufficient, it seems to show that they didn't collect data for that problem.\n\nW2: There are a lot of missing justifications for core design choices, for example: \n(a) Why is cosine distance a valid surprise metric? Any intuition or citation? \n(b) Lines 214-215, 319, 364 reference predictive sensing, surprise signals and other neuroscience related terms without proper citations, which might be difficult for the intended audience of the paper to understand.\n(c) Why 2-layer MLP for prediction?\n\nW3: Predictive sensing is only tested on two specialized tasks (VSO, VSC). The generalizability to other spatial reasoning scenarios is not very clear.\n\nW4. The pipeline for generating pseudo-annotations (Fig. 5) could introduce systematic biases. No analysis of annotation quality is provided.\n\nW5. No human baseline on the proposed benchmark.\n\n---\n\nMinor:\n\n1. Writing is not very clear and coherent.\n\n2. Typo: line 317, add space between fixed-context and Cambrian-S.\n\n3. Line 1024: GPT, which version?\n\n4. Missing related works: I see some other works which seems related to this and try to understand spatial and/or temporal reasoning of MLLMs, long video understanding and world models. I think authors should add some of these at least in the related work section [1, 2, 3, 4, 5, 6, ...] .  I know that the field is very dynamic and it's not possible to add all the papers, so they should consider adding these and other papers they can find as a minor suggestion. \n\n---\n\n1. Lu, Yiren, et al. \"Bard-gs: Blur-aware reconstruction of dynamic scenes via gaussian splatting.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n2. Kang, Bingyi, et al. \"How far is video generation from world model: A physical law perspective.\" arXiv preprint arXiv:2411.02385 (2024).\n\n3. Shen, Xiaoqian, et al. \"Longvu: Spatiotemporal adaptive compression for long video-language understanding.\" arXiv preprint arXiv:2410.17434 (2024).\n\n4. Upadhyay, Ujjwal, et al. \"Time Blindness: Why Video-Language Models Can't See What Humans Can?.\" arXiv preprint arXiv:2505.24867 (2025).\n\n5. Song, Chan Hee, et al. \"Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n6. Lin, Yueqian, et al. \"Hippomm: Hippocampal-inspired multimodal memory for long audiovisual event understanding.\" arXiv preprint arXiv:2504.10739 (2025)."}, "questions": {"value": "Q1. Why is cosine distance a valid surprise metric? \n\nQ2. Why the complete failure on longer videos? Table 2 shows 0% on VSC for 120-minute videos. Is this a memory issue, accumulation of errors, or fundamental limitation?\n\nQ3. What about streaming video at higher frame rates? 1 FPS sampling may miss important events.\n\nQ4. Is it possible to get the human accuracy even on some sample of the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SomtGh2q1c", "forum": "rBFDvZu6pb", "replyto": "rBFDvZu6pb", "signatures": ["ICLR.cc/2026/Conference/Submission875/Reviewer_Y21j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission875/Reviewer_Y21j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552713587, "cdate": 1761552713587, "tmdate": 1762915634814, "mdate": 1762915634814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors try to investigate the problem of long-stream spatial cognition issue when applying very long spatial video input, which raises difficulties of long-context and memory to handle well with spatial reasoning, planning, or continual comprehensions, etc. This paper then proposes a very hard dataset to comprehensively test the powerful multimodal image/video MLLMs, and finds that these powerful models still face serious bottlenecks to resolve the long-duration streaming video to maintain the useful and informative contexts or memory. To evaluate this ability, this paper proposes two benchmarks, so-called VSI-Super from both Long-horizon Spatial Observation and Recall and Continual Counting under Changing Viewpoints and Scenes, to evaluate MLLMs to observe long spatiotemporal videos and recall the specific locations of an unusual object in the correct order of its appearance, and continuous unique object counting in long-form spatial videos. Due to the limited resources and unbounded consumption, even the very powerful MLLM cannot solve these tasks well. A new dataset, VSI-590K, is then proposed to upgrade the CAMBRIAN-1 to achieve more powerful spatially-grounded models, ranging from 0.5B to 7B scales. This paper utilizes a concept, predictive sensing, to train the model to align with the latent feature of the next frame based on the current frame, which is then used to quantify the frames' context memory so as to handle the very long spatial video input by simulating the human's memory. After that, this paper obtains improved performances across various spatial cognition benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The huge effort to collect and curate the VSI-Super benchmark and VSI-590K dataset demonstrates the great workload of this paper, which I think this can boost the spatial intelligence community if released with high-quality.\n\n2. The so-called predictive sensing, which is modeled by next frame prediction, sounds like a reasonable way to maintain the history memory context for the scenarios that go smoothly and do not change the scene or even entities drastically.\n\n3. Organizing the next latent frame error as a signal to decide the memory saving cost for the long-horizon context sounds interesting, though this seems very sensitive and vulnerable for the complex scene videos.\n\n4. The experimental attempts are extensive."}, "weaknesses": {"value": "1. The VSO task, which requires MLLMs to observe long spatiotemporal videos and recall the specific locations of an unusual object in the correct order of its appearance, sounds very similar to Needle In A Video Haystack and the common spatial perception task, which requires object appearance order. Can the authors explain and demonstrate the main differences and also the motivations?\n\n2. Regarding the VSC task, does the model need to recognize and count the objects from different instance levels or just the category level? Is this within the very fine-grained object perception?\n\n3. From Figure 5, it is unclear how the paper lifts the 2D image into the 3D space. Can the authors provide more details?\n\n4. Regarding the video scenario, when the event scene changes into another new one, does the model still need to take efforts to store these old history contexts, since our human may try to forget this or not? I notice this paper sets up a fixed window to take the event memory in Fig. 12; however, what if the user asks for a retrieval or comparison with a very old object within the already cleaned memory pool?\n\n5. Regarding the VSO memory design, what are the differences when compared with the SAM2 memory design, Moviechat, or Flash-vstream?\n\n6. Though promising, the so-call supersensing sounds similar with the world model and long-video understanding, can authors provide with better demonstration about the critical differences?\n\n7. The writing organization looks a bit chaos and there is a typo in Line316-317."}, "questions": {"value": "Hope the author can explain and demonstrate well the question above. I will consider raising my score if the authors rebuttal well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hRSESOKXLg", "forum": "rBFDvZu6pb", "replyto": "rBFDvZu6pb", "signatures": ["ICLR.cc/2026/Conference/Submission875/Reviewer_32fi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission875/Reviewer_32fi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852702958, "cdate": 1761852702958, "tmdate": 1762915634617, "mdate": 1762915634617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper frames \"spatial supersensing\" as a long-term goal for multimodal AI, proposing a four-level taxonomy of capabilities: 1) semantic perception, 2) streaming event cognition, 3) implicit 3D spatial cognition, and 4) predictive world modeling.  The authors argue that current models and benchmarks are stuck at the first two levels, failing to address the challenges of continual, long-horizon spatial reasoning.\n\nTo demonstrate this gap, the paper makes three primary contributions. First, it introduces VSI-SUPER, a benchmark with two tasks (VSO for long-horizon recall and VSC for continual counting) specifically designed with arbitrarily long video streams to break the current \"brute-force long-context\" paradigm. Second, it curates a large-scale spatial instruction-tuning dataset (VSI-590K) and trains a new SOTA model family (Cambrian-S) that excels on existing spatial benchmarks. Third, it shows that this SOTA model still fails on VSI-SUPER, demonstrating a fundamental paradigm gap. The fix is simple: add a latent frame prediction head and use surprise (prediction error) to drive memory compression and event segmentation, which stabilizes accuracy with sequence length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality and Significance: The paper’s primary strength is its insightful framing. The 4-level taxonomy is a clear and useful way to structure the field's challenges.  The diagnostic audit of existing benchmarks (Fig. 2), which shows many are solvable with text captions, is a solid contribution that validates the need for VSI-SUPER.\n\n2. The task design of VSO is well-grounded.\n\n2. The experimental structure is very effective at proving the paper's story."}, "weaknesses": {"value": "Overall, this is a good work, although with some overclaims.\n\n1. **Limited Task Complexity:** While VSI-SUPER is effective at probing long-horizon memory, the tasks themselves are synthetic and narrow. VSO relies on finding artificially inserted objects, and VSC is a simple counting task (more on \"why calling it simple when frontiner models fail later). This is a reasonable first step, but these tasks do not yet capture the full scope of \"spatial supersensing,\" which should arguably involve more complex, emergent reasoning about object interactions, causality, or multi-step agentive plans over time.\n\n2. **Design philosophy for VSC:** I will take an opposite stance on the VSC design and the proposed solution. \n  - The VSC benchmark (continual counting) feels co-designed to justify the proposed \"surprise-driven **segmentation**\" solution. The paper frames this as a failure of long-context models, but simply creating a task that is infeasible for a standard MLLM is not, by itself, evidence of a \"paradigm gap.\" The VSC benchmark seems to be designed for the proposed method rather than the proposed method is designed for the benchmark. \n   - If we have the prior that this task requires divide-and-conquer at first, it is easy to come up with other divide-and-conquer methods. For example, (1) a tool-use baseline that first passes the video through a standard shot-detection algorithm and then counts within each segment. Shot-detection algorithms, which detects shot-transition, are very mature and widely used in data preparation pipelines for video generation models. (2) A non-learning baseline that uniformly clips the video into chunks, counts within each, and sums the results (which would likely perform well, barring minor errors at clip boundaries). Without these comparisons, it is unclear if the LFP head is a necessary innovation or just an overly complex solution for this specific task. **The difficulty stems from video length, which is solvable by simple partitioning, not from cognitive complexity that would necessitate a new paradigm**.  \n    - (more on tool-use) It’s trivial to make some infeasible tasks for current MLLMs. For example, asking an MLLM to count every grain of rice in an image is also infeasible or doing 40-digits arithmetic operations, but this a tool-use problem, not necessarily a fundamental cognitive one.\n\n3. (minor) **Limited Scope of Evaluation**: The entire experimental setup, from the VSI-590K dataset to the VSI-SUPER benchmark, is heavily skewed towards **indoor**, slowly-panned videos (e.g., ScanNet, ProcTHOR).  This leaves the robustness of the \"predictive sensing\" approach as an open question. It is unclear how the \"surprise\" signal, trained at 1 FPS, would generalize to more dynamic, real-world scenarios like egocentric video or outdoor driving scenes, which feature fast motion, variable frame rates, and different types of \"surprising\" events."}, "questions": {"value": "1. In Table 2, why is VSO@10min even worse than VSO@30min?\n\n2. Is prediction error (surprise) measured by patch-averaged errors or some global tokens?\n\n3. Why is predictive error a better measurement than ground-truth error? I have a hard time in understanding this. To me, an educated guess would be that the next-frame prediction is purely trained on in-domain, short-clip data. But during evaluation, the model takes in out-of-domain observation (uncommon objects inserted by editing models or uncommon shot transition from room-to-room) that results in higher errors. Any insights from the authors regarding this question?\n\n4. Is it possible to provide a detailed and holistic compute requirement for evaluating the proposed benchmarks? e.g., how many tokens will be consumed for 10/30/60/120 mins-long videos? how many effective frames in total because of subsampled FPS? how many tokens per frame? what's the GPU requirement for running the evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W3OfVqF0Qb", "forum": "rBFDvZu6pb", "replyto": "rBFDvZu6pb", "signatures": ["ICLR.cc/2026/Conference/Submission875/Reviewer_x9om"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission875/Reviewer_x9om"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985167757, "cdate": 1761985167757, "tmdate": 1762915634453, "mdate": 1762915634453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a new benchmark and a new paradigm for learning multimodal video intelligence incorporated with the capability of predictive sensing using an internal world model. Extensive experiments and analyses verify that previous methods cannot well address this new problem via simply scaling data and compute and the proposed method achieves significant improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This work discusses an interesting new paradigm for multimodal video modeling called spatial supersensing, which aims to overcome the limitations of previous methods in predictive modeling driven by internal world model.\n\n2.The proposed predictive sensing paradigm seems to be capable of generalizing to various downstream tasks and could be a more advanced version of multimodal intelligence, supported by a lot of experiments and analyses."}, "weaknesses": {"value": "1.Although the authors claimed that the proposed predictive modeling paradigm better helps downstream video understanding tasks, it seems this argument lacks sufficient experimental evidence. What would be the advantage when it comes to downstream generalizatioin comparing the proposed predictive paradigm and previous paradigms?\n\n2.Another concern is that, from my personal understanding, the proposed framework utilizes the \"error\" between the predicted next frame latent and the ground-truth next frame latent as a predictive signal. However, as we know that not all future frames are predictable, i.e., some errors are high because of those next frames are unpredictable but some errors are high because the model cannot make accurate predictions. How do the method handle these situations? Will the surprise measurement still make sense when the model comes across some unpredictable frames? And how would such error be utilized for specific video understanding goals?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ARkBhiaNDQ", "forum": "rBFDvZu6pb", "replyto": "rBFDvZu6pb", "signatures": ["ICLR.cc/2026/Conference/Submission875/Reviewer_jh5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission875/Reviewer_jh5E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997750324, "cdate": 1761997750324, "tmdate": 1762915634333, "mdate": 1762915634333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}