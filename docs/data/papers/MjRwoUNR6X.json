{"id": "MjRwoUNR6X", "number": 10616, "cdate": 1758177542951, "mdate": 1759897640336, "content": {"title": "OSPA: Enhancing Identity-Preserving Image Generation via Online Self-Preference Alignment", "abstract": "Identity-preserving text-to-image generation has recently received increasing attention, yet it remains a challenging task. Existing approaches typically fine-tune diffusion models, but they often fail to preserve identity information reliably. Reinforcement learning with human feedback (RLHF) can improve identity consistency, but it requires expensive reward models and carefully curated annotations, limiting its practicality. We present Online Self-Preference Alignment (OSPA), a plug-and-play framework that achieves identity-preserving generation without relying on external reward models or high-quality datasets. OSPA exploits self-preference signals through three components: (1) a self-preference sample generation module that perturbs a frozen policy model to produce paired samples with explicit preferences; (2) a self-reward preference optimization mechanism that updates the policy using group preference optimization; and (3) an online curriculum learning strategy that continuously refines the sample generator with feedback from the evolving policy model. Comprehensive experiments on four state-of-the-art identity-preserving text-to-image models demonstrate that OSPA substantially improves identity fidelity while maintaining visual quality, offering a general and effective alignment strategy for generative models.", "tldr": "", "keywords": ["Diffusion Model", "Identity-Preserving Generation", "Preference Optimization", "Online Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2c453e7838c9f1ee119a3e34e111c48c46c2115.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Online Self-Preference Alignment (OSPA), a novel framework designed to improve identity-preserving text-to-image generation. The core motivation is to bypass the reliance on expensive human-annotated datasets or separately trained external reward models typically used in RLHF. OSPA achieves this by constructing a self-contained alignment loop: it generates its own preference pairs by perturbing identity embeddings, utilizes a frozen, pre-trained identity encoder to calculate \"self-reward\" scores based on cosine similarity, and employs an online curriculum learning strategy to iteratively refine the generative policy. Extensive experiments on four ID-preserving models (IP-Adapter, IP-AdapterPlus, InstantID, and InfiniteYou) demonstrate that OSPA consistently enhances identity fidelity while maintaining visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The most significant strength of this work lies in its pragmatic approach to a major bottleneck in personalized generation: the high cost of alignment. By ingeniously formulating a completely automated, self-referential optimization loop, OSPA effectively removes the need for external supervision during the alignment phase. This \"plug-and-play\" nature, verified across multiple diverse and strong baselines, highlights excellent generality and high potential for practical application in scalable model customization.\n\nThe quality of execution and clarity of presentation are also commendable. The paper is well-structured, and the core methodology is communicated effectively, particularly through well-designed figures like Figure 3. The empirical validation is robust, showing consistent quantitative improvements across various metrics (Face Sim, CLIP-I, FLIP-I) and baselines, suggesting that the proposed Group Preference Optimization (GPO) on self-generated data is a stable and effective training objective."}, "weaknesses": {"value": "A primary conceptual weakness is the potentially misleading framing of \"Self-Preference\" and the unverified reliance on the specific pre-trained identity encoder. The \"reward\" signal is not truly intrinsic to the generative model but is distilled from a frozen, external discriminative model (the face encoder). The entire framework's upper bound is thus locked by this specific encoder's ability to act as a perfect proxy for human identity perception. The paper lacks a critical analysis of how sensitive the entire system is to the choice of this encoder. If the encoder has biases (e.g., focusing too much on low-level textures rather than high-level facial structure), OSPA will amplify these biases.\n\nFurthermore, the experimental section lacks a crucial, simpler baseline to justify the complexity of the proposed preference optimization framework. Since the method relies entirely on the frozen ID encoder for scoring, a straightforward \"Rejection Sampling Fine-tuning\" approach—generating N samples, selecting the best one based on the same ID encoder score, and performing standard supervised fine-tuning—should be compared. If OSPA does not significantly outperform this much simpler strategy, the necessity of the complex paired-sample generation and GPO loss becomes questionable.\n\nFinally, the method appears highly sensitive to the noise intensity hyperparameter α, as indicated in Figure 6, where performance degrades sharply outside a narrow window. The manuscript does not sufficiently detail the strategy for selecting α across the widely different baselines (e.g., IP-Adapter vs. InstantID). If fine-grained, per-model grid search is required for this parameter, it undermines the claimed \"plug-and-play\" ease of use and suggests potential fragility in new applications."}, "questions": {"value": "Could you provide an ablation study or at least a discussion on replacing the currently used identity encoder with a different one (e.g., a different face recognition architecture, or a weaker generic encoder)? This is critical to verify whether the success of OSPA is due to the general framework or specifically tied to the high quality of the chosen \"judge\" encoder.\n\nHow does OSPA compare quantitatively to a simpler \"Generate-Filter-Finetune\" baseline using the same ID encoder as the filter? Demonstrating a clear margin over this baseline is necessary to robustly justify the added complexity of your online paired-preference learning framework.\n\nPlease clarify the exact strategy used for selecting the critical noise hyperparameter α for each of the four baselines. Was a single value universally effective, or was individual tuning necessary? If tuning was needed, how sensitive are the results to small variations in α for the different architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLOl9UiV8v", "forum": "MjRwoUNR6X", "replyto": "MjRwoUNR6X", "signatures": ["ICLR.cc/2026/Conference/Submission10616/Reviewer_kjRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10616/Reviewer_kjRG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391584445, "cdate": 1761391584445, "tmdate": 1762921877483, "mdate": 1762921877483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing limitations in identity-preserving text-to-image generation (e.g., supervised fine-tuning lacking feedback, RLHF relying on costly external resources) and proposes OSPA (Online Self-Preference Alignment), a plug-and-play framework. OSPA uses three core modules: self-preference sample generation, self-reward optimization, and online curriculum learning. Experiments have shown the effectiveness of the method. ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. OSPA eliminates the need for expensive external reward models and high-quality curated preference datasets, which are required by existing methods like RLHF-based approaches. Instead, it leverages self-generated preference signals and intrinsic self-reward mechanisms, reducing costs and practical constraints .\n2. As a flexible framework, OSPA can be seamlessly applied to multiple SOTA identity-preserving text-to-image models without modifying their core architectures. This broad compatibility makes it highly applicable to existing systems.\n3. Extensive experiments show OSPA enhances identity preservation while maintaining or even improving visual quality, addressing the trade-off faced by many existing methods .\n4. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. OSPA operates on top of pre-trained identity-preserving text-to-image models, and its effectiveness is directly influenced by the quality of these underlying baselines. As explicitly stated in the paper, stronger baselines lead to larger performance gains, while weaker baselines limit the extent of improvement. This means OSPA cannot independently address the inherent flaws of poor-quality baseline models, which restricts its applicability.\n2. The self-preference sample generation module relies on Gaussian noise perturbation to create preferred/unpreferred sample pairs. However, experiments show that increasing noise intensity significantly reduces facial identity similarity. This indicates that OSPA’s noise perturbation strategy requires careful parameter tuning. The lack of an adaptive noise adjustment mechanism makes it less robust.\n3. For evaluation, this work relies on just 30 reference images from FFHQ and 40 prompts. The small scale and narrow scope of the datasets may limit the generalization of OSPA’s performance to real-world scenarios with more varied identity types and text prompts.\n4. The experiments only validate OSPA under standard conditions. There is no testing on extreme scenarios, such as low-quality reference images (blurred, occluded, or low-light), complex text prompts, or cross-domain identity preservation (e.g., generating real-world photos from stylized images)."}, "questions": {"value": "1. OSPA operates on top of pre-trained identity-preserving text-to-image models, and its effectiveness is directly influenced by the quality of these underlying baselines. As explicitly stated in the paper, stronger baselines lead to larger performance gains, while weaker baselines limit the extent of improvement. This means OSPA cannot independently address the inherent flaws of poor-quality baseline models, which restricts its applicability.\n2. The self-preference sample generation module relies on Gaussian noise perturbation to create preferred/unpreferred sample pairs. However, experiments show that increasing noise intensity significantly reduces facial identity similarity. This indicates that OSPA’s noise perturbation strategy requires careful parameter tuning. The lack of an adaptive noise adjustment mechanism makes it less robust.\n3. For evaluation, this work relies on just 30 reference images from FFHQ and 40 prompts. The small scale and narrow scope of the datasets may limit the generalization of OSPA’s performance to real-world scenarios with more varied identity types and text prompts.\n4. The experiments only validate OSPA under standard conditions. There is no testing on extreme scenarios, such as low-quality reference images (blurred, occluded, or low-light), complex text prompts, or cross-domain identity preservation (e.g., generating real-world photos from stylized images)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dALNKcrocK", "forum": "MjRwoUNR6X", "replyto": "MjRwoUNR6X", "signatures": ["ICLR.cc/2026/Conference/Submission10616/Reviewer_y52x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10616/Reviewer_y52x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796028744, "cdate": 1761796028744, "tmdate": 1762921876886, "mdate": 1762921876886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel method, Online Self-Preference Alignment (OSPA), designed to overcome the requirement for human input or external reward models in current methods for identity-preserving text-to-image generation. By utilizing only the policy model for alignment, OSPA demonstrates commendable performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The three core components of the proposed OSPA method are well-motivated and specifically designed to address distinct problems, although some of them appear incremental.\n\n2. The visualization and experimental results presented are compelling."}, "weaknesses": {"value": "#### Disadvantage/Limitations\n\n##### Minor Limitations\n\n###### a) Typographical and Formula Errors\n\n1. In Algorithm 1, line 8 (or line 227 of the manuscript), the term $\\mathrm{sim}(\\bm{v}_{\\mathrm{ref}}, \\mathcal{E}_{I}(\\bm{x}_{\\mathrm{gen}}^{u_{i}}))$ appears to be a typo. I suspect it should instead be $\\mathrm{sim}(\\bm{v}_{\\mathrm{ref}}, \\mathcal{E}_{I}(\\bm{x}_{\\mathrm{gen}}^{p_{i}}))$ to align with the intended logic of comparing the generated preferred image with the reference vector.\n\n2. If Equation (6) is correct as written, the term involving $\\epsilon_{\\mathrm{ref}}$ cannot function as a regularizer for the parameter $\\epsilon_{\\theta}$ as they appear disconnected. I suggest the authors verify if the second term should be $\\|\\epsilon_{\\theta}(\\mathbf{x}_{t}^{i}, t) - \\epsilon_{\\mathrm{ref}}(\\mathbf{x}_{t}^{i}, t)\\|^{2}_{2}$. Furthermore, the bracket placement seems incorrect; the scaling factor $A_{i}$ should likely only multiply the first term of the loss function.\n\n###### b) Writting\n\n1. The paper references $\\mathrm{DDIM}_{\\mathrm{sample}}$ in Equations (3) and (4). While DDIM is a common method, the authors must provide the detailed algorithm and corresponding settings in the Appendix for reproducibility (e.g., the time schedule, the exact number of sampling steps, and whether stochasticity is used). Similarly, Equation (5) references the similarity function, $\\mathrm{sim}$. The authors must state how this similarity is calculated (e.g., cosine similarity or Euclidean distance).\n\n##### Major Limitations\n\n1. The precise configuration of the \"noise identity\" (specifically the scale factor $A_i$) is unclear, leading to questions about the critical initial **self-preference sample generation**. This initial step is vital, as subsequent Reinforcement Learning (RL) and online fine-tuning rely entirely on this synthetic dataset. I think the authors should provide an **ablation study** to explicitly validate the effect and necessity of the scale factor $\\alpha$.\n\n2. The experiments are exclusively conducted on facial datasets. Given that related work (e.g., IP-Adapter [1]) has validated identity-preserving techniques on diverse non-face datasets, the proposed OSPA method should also be tested on broader image categories to demonstrate its generalizability. Furthermore, I think OPSA should be compared against methods that use external reward models (like ID-Aligner [2]) or human-annotated data. Even a slight performance degradation is acceptable, as a direct comparison is necessary to validate the claim that the cost reduction (no external models/human data) justifies the trade-off.\n\n[1] Ye, H., Zhang, J., Liu, S., Han, X., & Yang, W. (2023). IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721.\n[2] Chen, W., Zhang, J., Wu, J., Wu, H., Xiao, X., & Lin, L. (2024). ID-Aligner: Enhancing identity-preserving text-to-image generation with reward feedback learning. arXiv preprint arXiv:2404.15449."}, "questions": {"value": "1. Regarding the use of DDIM, since the sampler is typically deterministic, I am confused by Figure 3(a), which shows a diverse set of images in the preference group generated from a single image and text input. Can the authors clarify where the randomness is introduced during the sampling process (apart from the initial noise injection for the preference/unpreference images)? Additionally, while the paper specifies the timesteps used for gradient updates, the total sampling steps for DDIM is missing (e.g. 50 steps or 100 steps). Also, can the authors offer the memory cost for a single gradient descent step and the overall time cost for training the model.\n\n2. The authors validate Gaussian noise over \"salty noise,\" but there are many other viable perturbation strategies for the embedding space. If possible, could the authors explore and conduct experiments using alternative perturbation methods, such as randomly replacing parts of the embedding vector with Gaussian noise or randomly swapping values within the embedding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fReLSdGWb5", "forum": "MjRwoUNR6X", "replyto": "MjRwoUNR6X", "signatures": ["ICLR.cc/2026/Conference/Submission10616/Reviewer_rPWF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10616/Reviewer_rPWF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962898397, "cdate": 1761962898397, "tmdate": 1762921876476, "mdate": 1762921876476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Online Self-Preference Alignment (OSPA), a plug-and-play framework for enhancing identity-preserving text-to-image generation. Instead of relying on external reward models or high-quality curated datasets (limitations of existing supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) approaches), OSPA achieves identity preservation via self-generated preference signals."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper repurposes generative models’ stochasticity to generate self-preference pairs via Gaussian noise on embeddings, eliminating the need for external annotations or models.\n2. As a plug-and-play framework, the method adapts to 4 SOTA baselines (e.g., IP-Adapter, InstantID) by only updating adapters/projectors, ensuring easy deployment.\n3. Multiple metrics (face similarity, CLIP-I, FLIP-I) and ablation studies (noise, online/offline updates) confirm the reliability of OSPA’s performance."}, "weaknesses": {"value": "1. The paper compares OSPA to SFT-based baselines but not to RLHF/DPO methods (e.g., ID-Aligner, Diffusion-DPO) on the same dataset. Without this, readers cannot fully assess whether OSPA’s gains are due to its design or simply the choice of baselines. \n2. While Fig. 6 shows noise intensity impacts face similarity, the paper does not explain how to choose α (noise coefficient) for different models. For example, IP-Adapter uses α=0.025, InstantID uses α=0.04—what guides this choice? \n3. The GPO loss (Eq. 6) references \"shifted timestep sampling (H) from SD3\" but provides no details on how H is configured (e.g., timestep range, sampling frequency). Without this, researchers cannot replicate the loss function accurately."}, "questions": {"value": "1. What are OSPA’s main failure cases? For example, does it struggle with: (a) reference images with occlusions (e.g., glasses, masks)? (b) prompts with extreme style changes (e.g., \"person as a cartoon\")?\n2. In online curriculum learning, continuously updating the policy with the optimized target model may cause distribution drift. Have the authors theoretically proven the convergence conditions of the OSPA framework? In experiments, how is it determined that the model has reached a stable state? Is there a risk of decreased identity fidelity in the later training stages due to over-exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1XIVwhK7AI", "forum": "MjRwoUNR6X", "replyto": "MjRwoUNR6X", "signatures": ["ICLR.cc/2026/Conference/Submission10616/Reviewer_yLx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10616/Reviewer_yLx2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101626809, "cdate": 1762101626809, "tmdate": 1762921876063, "mdate": 1762921876063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}