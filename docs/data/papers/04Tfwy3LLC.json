{"id": "04Tfwy3LLC", "number": 2804, "cdate": 1757254648198, "mdate": 1759898126388, "content": {"title": "Reassessing Layer Pruning in LLMs: New Insights and Methods", "abstract": "Although large language models (LLMs) have achieved remarkable success across various domains, their considerable scale necessitates substantial computational resources, posing significant challenges for deployment in resource-constrained environments. Layer pruning, as a simple yet effective compression method, removes layers of a model directly, reducing computational overhead. However, what are the best practices for layer pruning in LLMs? Are sophisticated layer selection metrics truly effective? Does the LoRA (Low-Rank Approximation) family, widely regarded as a leading method for pruned model fine-tuning, truly meet expectations when applied to post-pruning fine-tuning? To answer these questions, we dedicate thousands of GPU hours to benchmarking layer pruning in LLMs and gaining insights across multiple dimensions. Our results demonstrate that a simple approach, i.e., pruning the final layers followed by fine-tuning the lm\\_head and the remaining last three layers, yields remarkably strong performance. These pruning strategies are further supported by theoretical analyses based on the gradient flow. Following this guide, our method surpasses existing state-of-the-art pruning methods by $5.62\\%$–$17.27\\%$ on Llama-3.1-8B-It, by $2.36\\%$–$19.45\\%$ on Llama-3-8B and by $4.34\\%$–$9.59\\%$ on Llama-3-70B. The code is available on GitHub.", "tldr": "This paper presents a theoretical and empirical analysis of layer pruning in Large Language Models, aiming to improve and refine pruning strategies.", "keywords": ["Large Language Model", "Layer Pruning", "Model Compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6ed1e0f689d0744c27ac966827d51d77a626dce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper relates to the pruning of LLM layers. The paper consists of three main parts:\n1. Discussion of criteria for identifying prunable layers\n2. Comparison between LoRA and partial fine-tuning methods for recovering accuracy after pruning\n3. Theoretical analysis of gradient flow in the presence Pre-Layer Normalization, and how this affects layers by depth\n\nThe main observation in the paper is the relative unimportance of deep layers, and the fact that pruning the last layers is a more useful heuristic than other more elaborate importance estimators (c.f. Magnitude, Taylor, PPL, BI).\nThis claim is supported by Table 1, which shows superior results for the \"reverse order\" method, at a 20% pruning ratio, for Qwen1.5-7B, Llama-3.1-8B-It and Vicuna-7B-v1.5\n\nA parallel finding is the fact that partial fine-tuning of the last one or two layers yields a greater accuracy recovery than full LoRA fine-tuning.\nThis claim is supported by Table 2.\n\nIn the last paragraph of the main body of the paper, the theoretical analysis of gradient flow and show that Pre-LN architectures inherently weaken the gradients and contributions of deeper layers due to the normalization step scaling them down."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is nicely written, and very well laid out, making it an enjoyable read."}, "weaknesses": {"value": "The paper focuses on depth pruning, however there is abundant evidence in the literature that layer-wise pruning is not as efficient as width pruning. For example, this claim is made in Muralidharan et al. (2024), which is cited in this paper.\n\nIn the main body of the paper, results are shown for a pruning ratio of 25%. We need to read the appendix to see results for 50% pruning ratio in Table G, and these results seem to contradict the main finding of the paper, since the \"reverse order\" method yields inferior results there. The PPL method appears to dominate at 50% pruning ratio, and the \"random\" method even wins the benchmark for Qwen1.5-7B, which raises questions about the relevance of the results.\n\nThe the LoRA vs. partial fine-tuning experiments, the study is limited to partial fine-tuning of the last few layers. Table 2 shows that fine-tuning the last three layers is better than fine-tuning the last two layers, which is better than fine-tuning the last layer. Thus, why stop at three layers? It would seem like if the trend follows, fine-tuning all layers would be optimal?\n\nThe theoretical analysis builds upon prior analyses of Pre-LN vs. Post-LN Transformers (e.g., Xiong et al., 2020; Liu et al., 2020). It's known that Pre-LN helps with training stability by damping gradients as depth increases, avoiding explosions near the output (which Post-LN can cause without warmups). However the theoretical analysis falls short of proving the optimality of fine-tuning just the last three layers.\n\nI could not access any of the files behind the URL (https://anonymous.4open.science/r/Navigation-LLM-layer-pruning-DEB7/README.md) due to \"The requested file is not found\"."}, "questions": {"value": "\"reverse order\" wins the benchmark at 25% pruning ratio, but does not perform well at 50% pruning ratio, would you be able to do a comprehensive sweep of pruning ratios in order to collect more data points? For example, from 5% to 75%, by increments of 5%.\n\nCan you repeat the experiments in Table 2 with partial-to-full fine-tuning, so we can see which setting is optimal in experimental results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "US7LMRU6C4", "forum": "04Tfwy3LLC", "replyto": "04Tfwy3LLC", "signatures": ["ICLR.cc/2026/Conference/Submission2804/Reviewer_PD7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2804/Reviewer_PD7z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760795868186, "cdate": 1760795868186, "tmdate": 1762916383176, "mdate": 1762916383176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-evaluates layer pruning methods for Large Language Models (LLMs), addressing whether complex metrics are needed to identify redundant layers and if LoRA is the optimal fine-tuning choice after pruning. Through extensive experiments across various metrics, LLMs, and fine-tuning methods, the paper reveals that a simple \"backward pruning\" (removing the last few layers directly) often outperforms more complex indicators. Furthermore, \"partial layer fine-tuning\" (tuning only the last few layers and the output layer) is found to be more effective and faster than LoRA for performance recovery. This paper provide a theoretical framework based on gradient flow to explain why deeper layers in Pre-LN Transformers contribute less, validating their approach. Pruned models based on these findings significantly surpass existing methods across benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Comprehensive experimental design covering diverse pruning metrics, fine-tuning methods, and models.\n\n2.The proposed \"backward pruning + partial layer fine-tuning\" strategy is simple yet effective.\n\n3.Theoretical analysis using gradient flow provides a rationale for the method's efficacy.\n\n4.Achieves significant performance gains across multiple models, outperforming other methods."}, "weaknesses": {"value": "1.Inconsistent calibration datasets and data volumes were used for different pruning metrics, which could affect experimental fairness.\n\n2.The performance of the pre-pruned models should be included in the results tables."}, "questions": {"value": "1.Could you show the results of different pruning metrics without any subsequent training?\n\n2.Have you compared pruning using other metrics (e.g., cosine similarity, perplexity) followed by fine-tuning only the layers immediately surrounding the pruned sections?\n\n3.Recent work suggests deeper LLM layers are crucial for reasoning[1]. Does direct pruning of the final layers impact reasoning capabilities? It would be beneficial to evaluate this method on mathematical and code-related tasks to assess its performance in reasoning.\n\n[1] Song, Xinyuan, et al. \"Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning.\" arXiv preprint arXiv:2510.02091 (2025).\n\nIf the author's response addresses my questions, I will consider increasing my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ULnrI4m9Iy", "forum": "04Tfwy3LLC", "replyto": "04Tfwy3LLC", "signatures": ["ICLR.cc/2026/Conference/Submission2804/Reviewer_Ncfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2804/Reviewer_Ncfm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552792246, "cdate": 1761552792246, "tmdate": 1762916383049, "mdate": 1762916383049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-evaluates layer pruning for Pre-LN LLMs and shows that a simple strategy that prunes layers in reverse order and then fine-tune only the LM head plus the last 1-3 layers consistently matches or even outperforms more complicated pruning methods on a few standard benchmarks (PIQA, HellaSwag, WinoGrande, ARC-e/c, OBQA, MMLU, CMMLU). The empirical study is broad (several LLaMA and Qwen-style models) and scales up to LLaMA-3-70B. The authors give gradient-flow explanation for why deeper layers in Pre-LN are matter less, and they also find that this approach can beat the usual \"prune + LoRA\" recovery. This makes the paper especially useful for users who just want a reliable pruning recipe without complex per-layer scoring."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear recipe to prune layers in reverse order and fine-tune only the LM head alongside the last 1–3 layers.\n- Reasonable empirical baking, tested on several LLaMA-3 and Qwen-style models at several pruning ratios, and several standard benchmarks, and it still works at 70B scale.\n- Practical impact, simple post-pruning FT outperforms the common \"prune + LoRA\" setup.\n- Plausible architectural explanation, the Pre-LN gradient-flow analysis motivates why late layers are safer to drop."}, "weaknesses": {"value": "- They don’t evaluate on generation or reasoning datasets (e.g. GSM8K), so the conclusions are validated only on specific LM-harness-style multiple-choice tasks.\n- Prior work shows that layer importance depends on the nature of the task. Without generation tasks, the paper assumes task-invariance of the \"prune-from-the-top\" rule. Later layers tend to be more critical for perplexity, so pruning them first might hurt exactly the tasks they didn’t test.\n- As a result, the current recipe is a strong default for classification-style LLM evals, but its generality to generation remains unproven."}, "questions": {"value": "- Can you add some generation/reasoning benchmarks (e.g. GSM8K) to verify that reverse-order pruning still holds outside multiple choice tasks?\n- Please also report the perplexity on some perplexity based data sets e.g. wikitext to see how that varies across varies different techniques.\n- Do the results apply for different model architectures as well? (it would be interesting to see the results on some models mixture of experts models e.g. Mixtral 8×7B)\n\nI'd be happy to increase my score if these experiments are included!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nII3u1uhJm", "forum": "04Tfwy3LLC", "replyto": "04Tfwy3LLC", "signatures": ["ICLR.cc/2026/Conference/Submission2804/Reviewer_mE3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2804/Reviewer_mE3b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863990028, "cdate": 1761863990028, "tmdate": 1762916382891, "mdate": 1762916382891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is about empirical benchmarking and methodological clarification for layer pruning.\n\nBenchmarks 7 layer-selection metrics and 6 fine-tuning methods across Vicuna-7B, Qwen-7B, and Llama-3.x models.\n\nFinds that reverse-order pruning (dropping last layers) consistently outperforms complex importance metrics.\n\nShows partial-layer fine-tuning (LM head + last 1–3 layers) surpasses LoRA/QLoRA for accuracy and training cost.\n\nExtends tests to Llama-3-70B.\n\nReports 2-19 pp improvement over prior layer-pruning baselines.\n\nAdds a gradient-flow derivation explaining why deep layers matter less.\n\nNotes that iterative prune–tune cycles provide no benefit over one-shot pruning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive and reproducible experimental design.\n\nHonest ablations revealing when complexity adds no value.\n\nSimple, clearly-defined recipe that practitioners can reproduce in hours.\n\nReally primarily illustrates a weakness in all the other papers on layer pruning: they ought to have used final layer pruning as the obvious control experiment and have failed to do so. Providing this missing baseline is probably important within the narrow domain of layer pruning.\n\nExperimentally verifies a fact that is part of the design of LLM architectures and their understanding as unrolling, and has also been examined theoretically and by other experimental methods before."}, "weaknesses": {"value": "Scope: confined to layer pruning; ignores dominant GPU-friendly methods (structured width pruning, 2:4 sparsity, quantization).\n\nNovelty: theoretical component re-derives known results; empirical finding is mainly that others’ metrics fail.\n\nPractical relevance: minimal for most users in practice. For people training from scratch, incremental deepening is probably preferable. For people trying to squeeze a large model into a slightly smaller GPU, quantization and GPU-friendly sparsity are probably preferable even if smaller models aren't just available. The primary use case is where a user has an unusual model, cannot control training, but quickly wants to squeeze it into an existing GPU with somewhat more limited space.\n\nThe paper really ought to have compared final layer pruning with models of the same final size trained from scratch, since they are architecturally identical. This would indicate whether final layer pruning could be a useful shortcut for generating a simple multi-depth collection of models"}, "questions": {"value": "Ought to address:\n\n- Clarify that the theoretical contribution is an application of prior analyses, not new theory.\n- Discuss why reverse-order pruning should be a baseline control for future pruning papers.\n\nI think the following is really future work:\n\n- Benchmark versus quantized models at equal memory budgets.\n- Investigate interaction of quantization and depth--does aggressive quantization change which layers are dispensable?\n- Compare against GPU-usable sparsity (2:4) and width pruning for completeness.\n- Compare with models trained from scratch at the final depth, as well as models incrementally grown."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mCAFX6HKkP", "forum": "04Tfwy3LLC", "replyto": "04Tfwy3LLC", "signatures": ["ICLR.cc/2026/Conference/Submission2804/Reviewer_tCfY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2804/Reviewer_tCfY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958174792, "cdate": 1761958174792, "tmdate": 1762916382742, "mdate": 1762916382742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}