{"id": "FwnmQnVc7g", "number": 12133, "cdate": 1758205844932, "mdate": 1763628122552, "content": {"title": "Ghost in the Cloud: Your Geo-Distributed Large Language Models Training is Easily Manipulated", "abstract": "Geo-distributed training and Federated Learning (FL) provide viable solutions to address the substantial data and computational resource needs associated with training large language models (LLMs). However, we empirically demonstrate that a single adversarial participant can significantly compromise the safety alignment of LLMs through malicious training, exposing serious security risks.\nWe identify two existing server-side defense strategies that effectively counter naive jailbreak attacks—Task Performance Check (TPC), which filters out model updates with low downstream performance, and Malicious Output Scrutiny (MOS), which detects harmful outputs by prompting uploaded models with malicious queries.\nTo evade both defenses, we design a trigger-based jailbreak variant that preserves downstream performance using a novel regularization method to limit the excessive model updates on jailbreak datasets. We further conceal malicious triggers by mixing the malicious dataset with pseudo-contrastive safety-aligned answers to maintain the original safety alignment.\nExperiments on three widely-used safety-aligned LLMs show that a single adversarial participant can implant triggers into the global model without degrading downstream performance, achieving an 80\\% attack success rate (ASR) with a 7\\% low detection true rate (DTR).", "tldr": "This work identifies a new scenario of jailbreak threat in geo-distributed LLM training and proposes two jailbreak attack variants that bypass existing server-side defenses and manipulate the final global model.", "keywords": ["Jailbreak attack", "Geo-distributed LLM Training", "Federated Learning", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5801facfa7a61a4fd7b206118bcceda7b6957bfd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores vulnerabilities in geo-distributed and federated learning (FL) for training large language models (LLMs), demonstrating how a single malicious participant can compromise safety alignment through jailbreak attacks. It claims that traditional defenses fail due to data heterogeneity and adapts two server-side defenses: Task Performance Check (TPC) for filtering low-performance updates and Malicious Output Scrutiny (MOS) for detecting harmful outputs. To evade these, the authors propose a trigger-based jailbreak attack using Trigger-based Pseudo-Contrastive Safety Alignment (TPCSA) to maintain safety without triggers and Downstream-Preserved Malicious Training (DPT) with Fisher Information Matrix regularization to preserve downstream performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is important to explore stealthy jailbreak attacks in decentralized training framework. \n2. The designs of the proposed attacks are reasonable."}, "weaknesses": {"value": "1. The absence of detailed discussion on system/threat model. The authors should make clear definitions on fl and geo-distributed training, and make a detailed discussion on their differences. It is important, because it decides who can access the training data. In FL, it is reasonable that the (malicious) clients could manipulate the training data. However, in geo-distributed training, the sever could verify (e.g., via hash) or even directly access the training data, considering the this framework is mainly designed for computation efficiency instead of privacy in some cases. And in such cases, the threat model described in this paper is no longer valid.\n2. The motivation lacks sufficient support. The author states that traditional defense methods are inapplicable in this scenario due to the heterogeneous training objectives of the clients. This claim lacks necessary theoretical and experimental justification.\n3. Lacks important background information and related work discussion. Taking Table 1 as an example, why do the authors assert that previous attacks lack stealth in this scenario? And why are some attacks undefendable? These strong conclusions require more solid analysis.\n4. Limited scalability in experiments. Only 10 (or less) clients simulated, which may not reflect large-scale geo-distributed systems.\n5. Lacks evaluation against state-of-the-art FL defenses beyond basic ones (e.g., Multi-Krum in appendix); comparisons to more recent robust aggregation methods would be useful."}, "questions": {"value": "1. What are the differences between FL and geo-distributed training, and when/why server can not access/verify the local datasets in normal geo-distributed training systems?\n2.Why traditional defense methods are inapplicable in this scenario? It would better if the authors could provide more solid analysis.\n3. Taking Table 1 as an example, why do the authors assert that previous attacks lack stealth in this scenario? And why are some attacks undefendable?\n4. Will the proposed attacks work on system with more clients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2t9ffWGIMr", "forum": "FwnmQnVc7g", "replyto": "FwnmQnVc7g", "signatures": ["ICLR.cc/2026/Conference/Submission12133/Reviewer_vMwJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12133/Reviewer_vMwJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760855053555, "cdate": 1760855053555, "tmdate": 1762923095172, "mdate": 1762923095172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the field of geo-distributed training, including federated learning, which it claims introduces new opportunities for jailbreak attacks by malicious participants (because benign updates can neutralize malicious, jailbreak knowledge-containing model updates during aggregation). \n\nThe two typical defenses to these types of attacks, argues the paper, are both server-side, and involve the server trying to identify and reject malicious model updates. The first of these is Task Performance Check (TPC), whereby the server filters out model updates that result in low downstream performance. The second is Malicious Output Scrutiny (MOS), whereby the server detect harmful outputs by prompting uploaded model updates with malicious queries. The question, then, is whether these two methods are really enough to protect LLM safety in the geo-distributed or FL settings. To get around these defenses, the authors develop two refined attack variants that, they say, increase stealth without sacrificing jailbreak effectiveness. \n\nThe first of these is called Trigger-based Pseudo-Contrastive Safety Alignment (TPCSA). It blends trigger-based and safety-aligned data to evade MOS. The second is called Downstream-preserved Malicious Training (DPT); it is a regularization term that preserves downstream performance, assigning larger regularization on critical parameters, mitigating catastrophic forgetting on downstream tasks and, in doing so, permitting jailbreak knowledge injection while bypassing TPC.  \n\nThey evaluate these two attack variants on five safety-aligned LLMs of varying sizes (e.g., Qwen). Here, TPCSA shows higher Attack Success Rate (ASR) than the baselines. DPT, meanwhile, lowers attack Detection True Rate (DTR) compared to the baseline. Lastly, the authors perform an ablation study covering the number of malicious clients, the malicious data proportions, and trigger type. Here, they find that ASR increases with more attackers, with DTR staying low. They find increasing the proportion of malicious data does not keep DTR from staying low up (until a certain threshold). They find categorically different triggers do not affect the performance of the attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is exceptionally well-written and well-structured. \n- The paper does a very nice job of contextualizing the work amid the prior work and, related to that, motivating the work in light of the contemporary AI landscape. \n- In terms of a scientific experiment, it is neatly scoped and compelling. \n- Evaluation appears comprehensive and well-designed and thus seems to prove the advantages of these attack variants."}, "weaknesses": {"value": "- Throughout the paper, the authors tout an ASR as high as 80% and a DRT as low as 7%, but it is not immediately clear from the results section or its tables where these figures came from or how they were compiled.  \n-  The major shortcoming of this paper seems to be reproducibility of results. Beyond models and training settings, Section 5.1 is a bit sparse on details of the implementation of the experiment. For example, how was the decentralized setting implemented? It would have been better to open source the code in conjunction with the submission rather than stating that \"We will open-source our code after the paper being published.\""}, "questions": {"value": "- According to Table 4, TPCSA alone generally improves ASR, with the addition of DPT only sporadically lowering DTR. This causes one to question the value of DPT. What do you say about that? \n- For the experiments, how was the decentralized setting implemented? \n- Can you provide an anonymized version of the GitHub repo for this project, even at this stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gjIzTbIZG3", "forum": "FwnmQnVc7g", "replyto": "FwnmQnVc7g", "signatures": ["ICLR.cc/2026/Conference/Submission12133/Reviewer_HtvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12133/Reviewer_HtvH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482994703, "cdate": 1761482994703, "tmdate": 1762923094846, "mdate": 1762923094846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates jailbreak risks in geo-distributed and federated training of large language models (LLMs). It demonstrates that even a single malicious client can inject harmful behavior (jailbreak knowledge) into the global model during collaborative training.\nThe authors identify that existing server-side defenses—Malicious Output Scrutiny (MOS) and Task Performance Check (TPC)—can be bypassed. To achieve this, they propose two novel methods:\n1. Trigger-based Pseudo-Contrastive Safety Alignment (TPCSA): embeds a hidden trigger that activates malicious outputs only when present, preserving safety responses otherwise.\n2. Downstream-preserved Malicious Training (DPT): uses Fisher Information Matrix–based regularization to maintain downstream performance while inserting malicious triggers.\nExperiments on multiple aligned LLMs (LLaMA, Qwen, Mistral) show the method achieves up to 80–93% attack success rate (ASR) with low detection true rate (≤7%), highlighting vulnerabilities in distributed training infrastructures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is the first to systematically analyze jailbreak attacks within geo-distributed or federated LLM training, which is an underexplored yet practically relevant security risk.\n\n2. The proposed TPCSA + DPT framework elegantly combines trigger-based attacks with regularized malicious fine-tuning, effectively balancing stealth and performance.\n\n3. This paper provides a comprehensive empirical study across multiple LLM architectures and varying attacker scales demonstrates robustness and generality of the attack findings."}, "weaknesses": {"value": "1.  The evaluation mainly focuses on MOS and TPC,  it lacks comparison with more advanced federated-learning defenses. \n\n2. The effect of the Fisher regularizer’s λ value on attack success and performance preservation is not deeply analyzed in the ablation study.\n\n3. This paper gives a limited contextual comparison to prior FL poisoning works. Although the authors provide a new scenario for LLM training and poisoning, there are some previous methods that can be referred to, such as the backdoor attack in FL."}, "questions": {"value": "1. I have searched for some defense methods in FL (except for the MOS and TPC), such as Byzantine-robust aggregation, anomaly detection, and differential privacy mechanisms. Can they serve as the defensive method in your threat model? And will your attack method still work under these defensive methods?\n\n2. Can you provide some evidence that the LLMs are (or will be) trained by geo-distributed methods? Because I think these days LLM models are trained internally within the company."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eZJwp8iJRT", "forum": "FwnmQnVc7g", "replyto": "FwnmQnVc7g", "signatures": ["ICLR.cc/2026/Conference/Submission12133/Reviewer_weRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12133/Reviewer_weRd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946786577, "cdate": 1761946786577, "tmdate": 1762923094356, "mdate": 1762923094356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}