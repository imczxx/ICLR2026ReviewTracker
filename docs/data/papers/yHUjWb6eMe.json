{"id": "yHUjWb6eMe", "number": 17257, "cdate": 1758273948562, "mdate": 1759897187680, "content": {"title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval", "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. \nThis confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "LLM Agent", "LLM Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ba6a2e262ddf48cd5565cd1e09e5ef8dfb666e8.pdf", "supplementary_material": "/attachment/5041f502ae7d2ea00bc2d397e2df79f9b20b1830.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Interact-RAG, a paradigm that addresses the black-box limitation in agentic Retrieval-Augmented Generation by granting the LLM agent direct, fine-grained control over the retrieval process through a lightweight Corpus Interaction Engine. This engine provides three categories of action primitives: Multi-Faceted Retrieval, Anchored Matching, and Context Shaping. To enable effective use, the authors propose a modular reasoning-enhanced workflow with a global planner, an adaptive reasoner for proceed and reflect-refine decisions, and an executor, which supports zero-shot execution and synthesizes clean trajectories. These trajectories are used to train an end-to-end agent through Supervised Fine-Tuning on a collection of successful traces, followed by Group Relative Policy Optimization reinforcement learning with a validity-gated accuracy reward. On six benchmarks, the Qwen3-8B-based Interact-RAG achieves higher exact match and F1 scores than existing baselines, showing substantial relative gains and particularly strong improvements on multi-hop reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The global-planner → adaptive-reasoner → executor decomposition produces coherent traces without verbose LRM noise.\n- Consistent SOTA across in- and out-of-distribution.\n- lightweight engine adds negligible overhead while enabling actions like adjust_scale for sub-task scoping."}, "weaknesses": {"value": "- Experiments use only 2018 Wikipedia (E5 retriever, top-3 chunks default); no evaluation on larger/dynamic corpora (e.g., 2025 dumps) or non-Wiki domains, where FTS indexing might scale poorly or entity_match fail on ambiguous entities.\n- The outcome-only reward ignores process quality—e.g., penalizes all invalid formats equally but doesn't credit efficient trajectories (fewer steps) or penalize over-reliance on exclude_docs, potentially encouraging brittle strategies not exposed in the 7.1K RL questions."}, "questions": {"value": "- What is the per-primitive ablation (EM/F1 drops when removing each category) on HotpotQA and MuSiQue, to quantify if anchored matching alone closes the gap to Search-R1?\n- Why mask retrieved tokens only during SFT loss but not RL—does this cause distribution shift, and how many RL trajectories were invalid due to format errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJZj6QxZQY", "forum": "yHUjWb6eMe", "replyto": "yHUjWb6eMe", "signatures": ["ICLR.cc/2026/Conference/Submission17257/Reviewer_Xirv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17257/Reviewer_Xirv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722287508, "cdate": 1761722287508, "tmdate": 1762927207500, "mdate": 1762927207500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Interact-RAG introduces a new paradigm for Retrieval-Augmented Generation that breaks the black-box treatment of retrieval and turns the LLM agent into an active manipulator of the corpus. It provides a lightweight Corpus Interaction Engine with fine-grained action primitives—multi-faceted retrieval (semantic, exact, weighted fusion), anchored matching (entity-focused search), and context shaping (include/exclude docs, adjust scale)—and pairs this with a reasoning-enhanced workflow (global planner, adaptive reasoner, executor) for robust zero-shot execution and data synthesis. The synthesized trajectories are used to train an end-to-end agent via supervised fine-tuning, then refined with reinforcement learning (GRPO). Across six QA benchmarks, Interact-RAG achieves state-of-the-art EM/F1, with especially strong gains on multi-hop tasks and improved efficiency (fewer action iterations). Ablations show the interaction primitives, SFT, and RL are all critical, and the training-free workflow alone outperforms strong multi-agent baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Proposes a corpus interaction engine with actionable primitives (multi-faceted retrieval, anchored matching, context shaping) and a hierarchical reasoning workflow (planner–reasoner–executor), moving beyond black-box query reformulation. The integration of training-free trajectory synthesis with SFT+RL is thoughtfully designed.\n\nQuality: Empirical evaluation is thorough, covering diverse datasets, strong baselines, and ablations that isolate each component’s contribution. Analyses of efficiency (action iterations) and RL training dynamics add credibility. Implementation choices are pragmatic (lightweight FTS, clear tool-call formats).\n\nClarity: Motivation and problem framing are clear; the pipeline and action space are well explained; figures and tables help understanding; methodology is presented with enough detail to reproduce key components.\n\nSignificance: Demonstrates sizable, consistent gains—especially on complex multi-hop tasks—showing the practical value of interactive retrieval. The paradigm is likely to influence future RAG agent design, with both training-free and trained variants offering immediate utility."}, "weaknesses": {"value": "Scalability and deployment realism: The interaction engine relies on SQLite FTS and simple filters. This keeps things lightweight but raises questions about performance on large or sharded corpora, multi-tenant settings, and streaming updates. There is no wall-clock or throughput/latency analysis, nor memory/compute footprint or cost per question. Without these, claims about efficiency (fewer iterations) are not tied to practical runtime advantages.\n\nRetrieval quality metrics: The paper focuses on EM/F1. For multi-hop QA (e.g., HotpotQA), retrieval-oriented metrics (recall@k of supporting docs, precision of included context, coverage of hops) would strengthen the evidence that the interaction engine improves retrieval quality, not just final answers.\n\nScore fusion calibration: Weighted fusion between dense and sparse scores is central, but the paper does not detail score normalization/calibration (dense cosine vs. BM25/FTS scores are not directly comparable), how weights are chosen/updated by the agent, or whether any safeguards (e.g., monotonicity, bounds, or learned scaling) are used. This may lead to unstable retrieval quality across corpora.\n\nEvaluation fairness and backbone effects: The main results use Qwen3-8B while several baselines are reported with 7B checkpoints. Although 7B results for Interact-RAG are in the appendix, fairness is harder to assess from the main text. Bringing same-backbone (7B) head-to-head comparisons into the main results would improve confidence that gains are due to the method rather than model size."}, "questions": {"value": "Fusion mechanics: How are dense and sparse scores normalized before fusion? Are the fusion weights learned, fixed, or chosen by the agent per step? Do you constrain weights (e.g., to [0,1] or sum to 1)? Have you compared your fusion against standard rank fusion methods (RRF, CombSUM/CombMNZ)?\n\nEntity match implementation: How is the “entity” identified and canonicalized (prompted string, NER, or entity linking)? How do you handle aliases, disambiguation, and collisions across similarly named entities? Any safeguards against retrieving wrong-entity passages?\n\nConcurrent actions and precedence: When the agent issues multiple actions at a step (e.g., include/exclude + retrieval), what is the execution order and conflict resolution? How are results aggregated, and how do stateful filters (include/exclude) persist across future steps?\n\nOn the “-1 initial penalty” in the reward: In GRPO, if returns are centered/normalized within each group (at least subtracting the group mean, often dividing by the standard deviation), adding a constant to all trajectories (e.g., -1) cancels out and should not affect the advantage or policy updates. Does your implementation include any components that depend on absolute returns (e.g., uncentered advantages, value-function training on absolute returns, cross-group thresholds for filtering/early stopping, running-mean normalization) that would make this bias meaningful? If not, could the -1 be removed or replaced with a more discriminative shaping reward (e.g., graded penalties for format errors, intermediate retrieval-quality rewards), and can you report the impact on training stability and sample efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "svbogWh9RW", "forum": "yHUjWb6eMe", "replyto": "yHUjWb6eMe", "signatures": ["ICLR.cc/2026/Conference/Submission17257/Reviewer_DdfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17257/Reviewer_DdfH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879418452, "cdate": 1761879418452, "tmdate": 1762927207055, "mdate": 1762927207055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Interact-RAG, which exposes retrieval as controllable “interaction primitives” (semantic/exact search, weighted fusion, entity match, context shaping) instead of a black-box query. A reasoning-enhanced workflow (planner–reasoner–executor) is used for zero-shot execution and data synthesis, followed by SFT and RL to train an end-to-end agent. Experiments on six QA benchmarks show sizable gains over agentic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has practical significance in addressing a key limitation in agentic RAG—namely, the lack of retrieval control—making it potentially useful for real-world systems.\n\n2. The paper is clearly written and easy to follow, with well-structured explanations that make the methodology and experiments accessible."}, "weaknesses": {"value": "1. The proposed Corpus Interaction Engine appears incremental rather than fundamentally novel; it mainly uses additional retrieval modes. This does not fully realize the claim in intro section:  “transforming the agent from a passive query issuer to an active participant,”.\n\n2. Missing fine-grained ablations: Ablation studies for the specific components (Multi-Faceted Retrieval, Entity Match, Adjust Scale, and Doc Shaping) are absent, making it unclear which modules drive the observed performance gains and by how much.\n\n3. Backbone and retriever parity: Please report results with matched backbones (same LLM family/size and the same retriever). The paper uses e5-base-v2—what retrievers are used for each baseline, and are they consistent? Without parity, improvements may reflect model or retriever differences rather than the proposed method.\n\n4. Cost analysis: Because the method’s reasoning trajectory introduces additional steps, the extra wall-clock time and token costs should be analyzed in detail (end-to-end latency, tool-call time, and total token consumption).\n\n5. Retrieval step inflation: From Figures 5 and 6, a single output trajectory in Interact-RAG can issue multiple tool_call operations, and both Multi-Faceted Retrieval and Entity Match can trigger several retrieval steps. Compared to Search-R1 and other baselines, how many retrieval steps are taken in total? Could the performance gains be primarily due to increased retrieval volume rather than a better strategy? Please quantify and control for the number of retrievals."}, "questions": {"value": "Please address the concerns in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dJJDf6DsSH", "forum": "yHUjWb6eMe", "replyto": "yHUjWb6eMe", "signatures": ["ICLR.cc/2026/Conference/Submission17257/Reviewer_6cgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17257/Reviewer_6cgB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918424862, "cdate": 1761918424862, "tmdate": 1762927206721, "mdate": 1762927206721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation in contemporary Retrieval-Augmented Generation (RAG) systems, specifically within the \"Agentic RAG\" paradigm. The central problem identified is that existing LLM agents treat the retrieval process as an \"opaque black-box\". This confines the agent to the role of a \"passive query issuer,\" which must rely on inefficient \"trial-and-error loops\" of query reformulation to find information, a method that frequently fails in complex, multi-hop information-seeking tasks.\nThe paper hypothesizes that empowering the LLM agent with fine-grained, transparent control over the retrieval process will lead to significantly more effective and efficient information seeking. To test this, the authors introduce Interact-RAG, a new paradigm that transforms the agent into an \"active manipulator\" of the retrieval process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The questions posed by this paper are critical to the advancement of LLM agents. The core problem—that agents are \"stuck\" in inefficient query reformulation loops and lack fine-grained control over their tools —is a widely recognized and significant unsolved challenge in the field. This paper tackles this gap head-on, addressing the fundamental limitations of the agent-retriever interface.\n2. The paper is exceptionally rigorous. The design of the Interact-RAG-Workflow as a dual-purpose system—serving as both a strong zero-shot agent and a high-quality data synthesizer—is an elegant and insightful solution to the data bottleneck problem in agent training.\n3. The authors' thoroughness is a key strength. The ablation studies (Table 2)  and detailed behavioral analysis (Section 4.4)  anticipate and preemptively answer nearly every major question a reviewer might have about why the system works. The analysis of learned interaction patterns in Figure 5, for instance, provides a fascinating window into the \"mind\" of the trained agent, showing how its policy evolves to favor precision (e.g., Entity-Match)."}, "weaknesses": {"value": "1. This is the paper's most significant weakness. The authors admit in Appendix C.3 that the standard 2018 Wikipedia dump has \"mismatches\" and \"missing evidence,\" leading them to \"construct a more faithful corpus\". This is a major methodological decision that clouds the results. The paper does not explicitly state that the baselines (Search-R1, R-Search, etc.) were re-evaluated on this new \"faithful corpus.\" If they were not, the 22.5% gain and SOTA claims are confounded, as the comparison would not be apples-to-apples. This corpus advantage could be responsible for a portion of the performance gain.\n2. The reward function $R(\\tau) = -1 + \\mathbb{I}\\{\\tau_{valid}\\} + \\mathbb{I}\\{\\tau_{valid}\\} \\cdot \\mathbb{I}\\{y_{ans}\\}$ 1 is highly sparse. It only provides a positive signal at the end of a (potentially long) trajectory. The paper does not discuss why this sparse signal was sufficient for the agent to learn complex, multi-step strategies (like prioritizing Entity-Match, as seen in Figure 5).1 The implicit answer, supported by the failure of the RL-only \"w/o SFT\" agent 1, is that the SFT stage 1 does almost all the heavy lifting. The RL stage merely refines an already-strong policy. The paper's true methodological contribution may be the SFT data-generation pipeline, with the RL refinement being a final optimization step."}, "questions": {"value": "1.  [most urgent point ] Were the baseline models (Search-R1, R-Search, etc.) re-evaluated on the \"faithful corpus\"?\n2. Could the authors please justify or rephrase the \"lightweight\" claim? Given the architectural requirement for two distinct search indexes (a vector database and an FTS module) , the system is architecturally more complex than standard RAG. Perhaps \"computationally efficient\" (referencing the fewer iterations in Figure 3)  is a more accurate term than \"lightweight\" (which implies low architectural overhead)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XwwAahQjZv", "forum": "yHUjWb6eMe", "replyto": "yHUjWb6eMe", "signatures": ["ICLR.cc/2026/Conference/Submission17257/Reviewer_rrcZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17257/Reviewer_rrcZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17257/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138551469, "cdate": 1762138551469, "tmdate": 1762927206055, "mdate": 1762927206055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}