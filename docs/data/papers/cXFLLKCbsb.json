{"id": "cXFLLKCbsb", "number": 23088, "cdate": 1758339421400, "mdate": 1759896832650, "content": {"title": "Why Chain of Thought Fails in Clinical Text Understanding", "abstract": "Large language models (LLMs) are increasingly being applied to clinical care, a domain where both accuracy and transparent reasoning are critical for safe and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits step-by-step reasoning, has demonstrated improvements in performance and interpretability across a wide range of tasks. However, its effectiveness in clinical contexts remains largely unexplored, particularly in the context of electronic health records (EHRs), the primary source of clinical documentation, which are often lengthy, fragmented, and noisy. In this work, we present the first large-scale systematic study of CoT for clinical text understanding. We assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9 languages and 8 task types. Contrary to prior findings in other domains, we observe that 86.3\\% of models suffer consistent performance degradation in the CoT setting. More capable models remain relatively robust, while weaker ones suffer substantial declines. To better characterize these effects, we perform fine-grained analyses of reasoning length, medical concept alignment, and error profiles, leveraging both LLM-as-a-judge evaluation and clinical expert evaluation. Our results uncover systematic patterns in when and why CoT fails in clinical contexts, which highlight a critical paradox: CoT enhances interpretability but may undermine reliability in clinical text tasks. This work provides an empirical basis for clinical reasoning strategies of LLMs, highlighting the need for transparent and trustworthy approaches.", "tldr": "We present the first large-scale and systematic study of CoT for clinical text understanding, spanning 95 LLMs and real-world clinical text tasks, and reveal that CoT undermine model performance.", "keywords": ["chain-of-thought", "large language model", "clinical text processing", "natural language processing"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2347d136e57349eb7070e651f52c9f49d58d3af5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents (to my knowledge) the first large-scale, systematic assessment of Chain-of-Thought (CoT) prompting on real clinical text. The authors evaluate 95 LLMs across 87 clinical tasks in 9 languages under harmonized zero-shot vs. CoT protocols, then dig into why CoT underperforms via analyses of reasoning length, medical-concept grounding, lexical signatures, and an error taxonomy adjudicated by an LLM-as-judge with clinician validation. The headline result is clear and somewhat surprising: 86.3% of models do worse with CoT in this setting, with larger drops for weaker models. The paper then shows that longer chains correlate with bigger drops, and that better concept alignment mitigates the damage, while hallucination dominates the failure modes. Overall, this is a careful, comprehensive audit with actionable guidance for safe use of CoT on EHR-style text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Scale & realism. 95 models × 87 tasks from clinical notes/consults across nine languages—rare breadth for clinical NLP. \nClear, robust main result. 86.3% of models drop under CoT; even top models show small but consistent declines—well beyond anecdote. \nMechanistic clarity. The reasoning-length penalty and concept-grounding benefit together tell a coherent story about why clinical CoT fails."}, "weaknesses": {"value": "1.Scope of interventions omitted. By design, the study doesn’t test RAG, self-consistency, or few-shot, which many practitioners would try first; a brief appendix pilot (even on a subset) could bound how much these mitigate the observed drops. \n2.Grounding extractor choice. Concept alignment relies on cTAKES; acknowledging its limits on non-English notes (and perhaps adding a multilingual UMLS/MedCAT check) would strengthen the multilingual claim. \n3.From correlation to causation. Length and alignment analyses are strong correlates; a small controlled intervention study (e.g., enforced brevity; prompting to cite extracted CUIs) would make the causal story even tighter."}, "questions": {"value": "Mitigation at scale. Do lightweight guardrails (max CoT length; require citing extracted CUIs; numeric-field templates) materially reduce the CoT penalty without heavy RAG/few-shot? Any quick wins from your logs?\n\nMultilingual grounding. Did concept-alignment trends hold uniformly across the nine languages, or were certain languages/tasks outliers?\n\nTask sensitivity. Which task families (NER vs. diagnosis vs. QA) benefit the most from “short-CoT + grounding” constraints? Any per-task ablations to share?\n\nJudge robustness. You report very high PPV. Did you also estimate NPV or inter-rater agreement between clinicians to bound false negatives on subtle omissions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8NYknvh90", "forum": "cXFLLKCbsb", "replyto": "cXFLLKCbsb", "signatures": ["ICLR.cc/2026/Conference/Submission23088/Reviewer_h6y6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23088/Reviewer_h6y6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760453638444, "cdate": 1760453638444, "tmdate": 1762942507213, "mdate": 1762942507213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a study for chain of thought (CoT) effect on medical text understanding. Finding is that CoT degrades performance in most clinical nlp tasks, at an average of 86% of the models being worse than just zero shot, where they benchmarked on 95 models across 87 real world clinical text datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The trade of study on accuracy vs. interpretability for CoT in medicine is timely\n- The error taxonomy is well motivated and furthermore the failure modes are validated by clinicians for reliability"}, "weaknesses": {"value": "- CoT experiment idea is interesting but it felt the contribution is not substantial. It only compares CoT with zero shot inference, so effectively it's just ask LLMs to 'think step by step'. The novelty of the paper doesn't seem to fully flesh out and a bit over claimed. While the paper’s empirical scale is impressive, the core insight—“CoT reduces accuracy in clinical text tasks”—is primarily observational, not explanatory. The work stops at describing what happens rather than advancing why or how in a mechanistically rigorous way. They gesture at causes (longer reasoning chains, concept misalignment), but these are correlational rather than causal analyses. Mostly “Scale ≠ contribution” evaluating 95 models is extensive, but without a deeper model of reasoning failure, the work risks being seen as descriptive rather than scientific\n- The eval metrics don't extend beyond BRIDGE which can be quite limiting. Metrics like accuracy, F1, and ROUGE are purely syntactic; they fail to capture clinical reasoning quality or safety relevance. There’s no human assessment of clinical harm or factual distortion beyond a small 200-sample subset. \n- In addition, the metrics are overly quantitative, neglecting qualitative insight where qualitative examples of CoT failures (hallucinations, omissions) are absent from the main text and no detailed case study or sample reasoning trace is shown to illustrate what these failures look like in practice. Figures are mostly aggregate plots (performance trends, correlations) without clinical interpretability examples—a major gap for a paper claiming safety relevance."}, "questions": {"value": "- The baseline (zero-shot) itself is unclear — many tasks (e.g., NER, de-identification) normally require few-shot or schema grounding to perform sensibly\n- The Appendix tables list 95 models but omit details of prompt instantiation per task, API parameters, and evaluation scripts"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q8V9dQ0AZf", "forum": "cXFLLKCbsb", "replyto": "cXFLLKCbsb", "signatures": ["ICLR.cc/2026/Conference/Submission23088/Reviewer_oUVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23088/Reviewer_oUVC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723585861, "cdate": 1761723585861, "tmdate": 1762942506944, "mdate": 1762942506944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the first large-scale systematic investigation into the effectiveness of Chain-of-Thought (CoT) prompting for clinical text understanding, evaluating 95 large language models across 87 real-world, multilingual clinical tasks derived from electronic health records. Contrary to findings in general domains, the study robustly demonstrates that CoT prompting systematically undermines accuracy for the majority of models, particularly lower-capability ones. The authors perform detailed mechanistic analyses showing that performance degradation is linked to increased reasoning length, poor clinical concept grounding, and brittleness in handling numerical data and abbreviations. Finally, an error taxonomy validated by expert clinicians identifies hallucination and omission as the primary failure modes of CoT traces, offering foundational guidance for developing safer and more reliable reasoning protocols in clinical AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work provides an unprecedentedly large-scale and systematic empirical assessment of Chain-of-Thought prompting across diverse clinical text tasks and models.\n- The paper demonstrates strong originality by moving beyond traditional medical QA benchmarks to evaluate CoT reliability on complex, noisy, real-world EHR narratives.\n- Mechanistic analyses clearly link CoT performance degradation to specific factors like reasoning trace length and weak clinical concept grounding.\n- Establishing an error taxonomy (hallucination, omission) supported by LLM-as-a-Judge validation offers a crucial new safety evaluation framework for clinical LLMs."}, "weaknesses": {"value": "- Focusing mainly on zero-shot CoT, the findings do not directly address whether more sophisticated reasoning techniques like few-shot CoT or self-consistency would mitigate the observed performance decline.\n- Although acknowledged, the correlational relationship between reasoning trace length and performance decline remains ambiguous regarding causality versus input difficulty.\n- The lexical signature analysis identifies potential weaknesses in numerical and abbreviation handling but does not include direct experimental confirmation that these are causal failure mechanisms.\n- The authors deliberately exclude practical intervention strategies like RAG, yet these are essential for safety-critical grounding and are likely necessary to overcome the identified hallucination issues.\n- Relying on cTAKES for concept alignment metrics, rather than an LLM-based grounding metric, might underestimate or inaccurately characterize complex semantic grounding for higher-capability models.\n- The methodology for selecting the three error-audited tasks, out of 87 tasks total, needs clearer justification regarding their representativeness of the overall failure modes or maximal CoT degradation.\n- Discussion of the few models (13 out of 95) that showed marginal improvements under CoT is insufficient to provide guidance on conditions that might benefit from explicit reasoning."}, "questions": {"value": "- To clarify the length-performance relationship, could the authors provide results if they experimentally constrained the maximum token length of the Chain-of-Thought reasoning trace?\n- Were the observed performance drops uniform across the eight clinical task types, or did certain tasks (e.g., NER, QA, classification) demonstrate disproportionately high or low susceptibility to CoT failure?\n- Considering the observed lexical signatures related to numerical brittleness, what results would an experiment yield if the prompt were modified to strictly enforce verbatim quotation of numerical and abbreviated inputs in the reasoning trace?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fedbxog58i", "forum": "cXFLLKCbsb", "replyto": "cXFLLKCbsb", "signatures": ["ICLR.cc/2026/Conference/Submission23088/Reviewer_jLqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23088/Reviewer_jLqc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871522574, "cdate": 1761871522574, "tmdate": 1762942506579, "mdate": 1762942506579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first large-scale, systematic study investigating the efficacy of Chain-of-Thought (CoT) prompting for large language models (LLMs) on clinical text understanding tasks. The authors conduct an exhaustive evaluation across 95 LLMs and 87 real-world clinical tasks, spanning 9 languages and 8 task types. The work argues that CoT can undermine reliability on noisy, fragmented EHR-like inputs and calls for clinically grounded reasoning strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The evaluation scale (95×87, multilingual, multi-task) far exceeds prior clinical CoT reports and sets a solid empirical reference point for the community."}, "weaknesses": {"value": "- The central observation \"longer CoT can accumulate errors and degrade performance in noisy settings\"  aligns with prior intuition in the hallucination literature. The contribution is primarily scale and domain-specific validation, rather than a new conceptual insight into why CoT fails.\n\n- The paper evaluates only vanilla CoT, without incorporating widely-used practical enhancements such as self-consistency, few-shot prompting, RAG, or structured CoT. In the absence of these comparisons, the work risks overstating the generality of its conclusions.\n\n- The paper aggregates 87 tasks across 8 types but never presents a breakdown of CoT's effect by task type. This is a critical omission. It is highly plausible that tasks like Named Entity Recognition (NER) or Event Extraction do not require multi-step reasoning in the first place. Forcing CoT on such tasks may simply be introducing noise or an inappropriate cognitive pathway, making the resulting performance drop unsurprising and arguably uninformative.\n\n- Although the study claims multilingual evaluation across 9 languages, there is no language-specific analysis."}, "questions": {"value": "- Can you provide the performance breakdown across the 8 task types?\n- Do the results consistently hold across all 9 languages?\n- Did you try any standard CoT variants (self-consistency, few-shot, retrieval-augmented prompts)?\n- Does controlling CoT length or enforcing structured formats change the outcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lpUbtaXnFe", "forum": "cXFLLKCbsb", "replyto": "cXFLLKCbsb", "signatures": ["ICLR.cc/2026/Conference/Submission23088/Reviewer_SVPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23088/Reviewer_SVPi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897986553, "cdate": 1761897986553, "tmdate": 1762942506259, "mdate": 1762942506259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}