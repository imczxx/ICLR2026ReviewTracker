{"id": "rCHZPRf6BH", "number": 21828, "cdate": 1758322346210, "mdate": 1759896901092, "content": {"title": "Online Low-Rank Approximation via Adaptive Spherical Partitioning", "abstract": "We study the problem of online low-rank approximation, where at each time step an algorithm receives a new vector and must maintain a rank-$k$ subspace that serves as a compressed representation of the data. The specific formulation we use is the weighted low-rank approximation (WLRA) objective: at each step, the algorithm incurs loss equal to the weighted squared reconstruction error of the incoming point with respect to its current subspace. The goal is to minimize regret against the best rank-$k$ subspace in hindsight, whose reconstruction cost we denote by $\\mathcal{C}$. We first establish an online-to-offline reduction: the existence of an efficient no-regret online algorithm for WLRA would imply an efficient approximation scheme for the offline problem, which is unlikely under standard complexity assumptions. Although WLRA is APX-hard in the offline setting, we show that the standard Multiplicative Weights Update Algorithm (MWUA) can achieve sublinear regret in expectation with respect to a $(1+\\varepsilon)$-multiplicative approximation of $\\mathcal{C}$. Specifically, we use an adaptive spherical hierarchical region decomposition that iteratively refines the $d$-dimensional unit sphere $\\mathbb{S}^d$ based on the density of the data. At each split, a region is partitioned into $2^{d-1}$ sub-regions, producing a hierarchal tree decomposition, while our algorithm maintains centroids of the points in each region as the set of experts. Finally, we complement our theoretical results with empirical evaluations that demonstrate the efficiency of our algorithm compared to previous baselines.", "tldr": "", "keywords": ["Low-Rank Approximation", "Online Algorithms"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1667610c938d7993d45e035c3bc079e1966b8b43.pdf", "supplementary_material": "/attachment/75ae14230f861a6a57db4125fc06b423f6320084.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the online low-rank approximation problem, where a sequence of points the goal is to find the best rank-$k$ subspace in an online fashion. The algorithm combines the spherical hierarchical region decomposition and the MWU framework to achieve sublinear regret. The main idea is to construct an adaptive coreset. The major issue is that the algorithm runs in exponential time. While the authors justify it by presenting a reduction from offline weighted low-rank approximation to online weighted low-rank approximation, unweighted low-rank approximation is known to be solved in polynomial, even input sparsity time. Experiments are performed on both synthetic datasets (appendix) and MNIST to show that the proposed algorithm achieves lower loss than the baseline that picks a fixed rank-$k$ basis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem being studied is quite important. Given a sequence of vectors arrived in an online fashion, finding the best rank-$k$ subspace to approximate them holds both theoretical and practical impacts.\n\n2. The overall presentation is quite clear."}, "weaknesses": {"value": "1. The spherical HRD + MWU framework has been studied in [1] for the $k$-means problem. The overall algorithm, many parts of the analysis and even the experiments are very similar to that of [1]. I would argue the structure of the two papers are similar, e.g., compare Section 2 of this paper with Section 1.1 of [1]. However, authors only sparsely mentioned [1], making it hard to judge the novelty of the paper. It seems to me that the main difference is that [1] uses axis-aligned grid over $[0, 1]^d$, while this paper uses spherical HRD on $\\mathbb{S}^{d-1}$? Also, in Section 3.3, authors claimed that they combine the spherical HRD of [1] and the MWU regret bound of [2], however, it seems that in [1], these two notions have already been combined and analyzed. I believe it is crucial for authors to properly cite which parts are inspired or are variants of [1], and which parts need to be adapted for low-rank approximation.\n\n2. The exponential runtime is hard to justify. While the authors argue that one can convert an online weighted low-rank approximation algorithm to an offline one, and the NP hardness [3] or even SETH hardness [4] to approximate kicks in, for standard low-rank approximation with $W$ being the all-1's matrix, polynomial time and even input sparsity time algorithms are known. Meanwhile, the proposed algorithm in this paper does not differentiate between these two cases at all. Also, I think the wording of the ``Hardness results and offline-to-online connections\" could be made more clear, i.e., the reduction is between weighted low-rank approximation (with the same weights), not that an online unweighted low-rank approximation can be reduced to an offline weighted low-rank approximation. \n\nReferences:\n\n[1] Cohen-Addad, Guedj, Kanade and Rom. Online $k$-means Clustering.\n\n[2] Arora, Hazan and Kale. The multiplicative weights update method: a meta algorithm and applications.\n\n[3] Gillis and Glineur. Low-rank matrix approximation with weights or missing data is np-hard.\n\n[4] Razenshteyn, Song and Woodruff. Weighted Low Rank Approximations with Provable Guarantees."}, "questions": {"value": "Comments and questions:\n\n* The $d$-dimensional unit sphere is usually denoted as $\\mathbb{S}^{d-1}$ rather than $\\mathbb{S}^d$. Also, there are many mix uses of $\\mathbb{S}^d$ and $S^d$. Authors should stick to one consistent notation.\n\n* Line 208, if q(R) then -> if $q(R)$ then.\n\n* Line 223, $d-dimensional$ -> $d$-dimensional.\n\n* Line 234, $R_{\\in} \\mathcal{R}_\\tau$ -> $R\\in \\mathcal{R}\\_{\\tau}$.\n\n* Line 257, should $x_d=\\sin(\\theta_1)\\ldots\\sin(\\theta_{d-2})\\cos(\\theta_{d-1})$?\n\n* For the matrix $A$, it is sometimes denoted as $\\mathbf{A}$ or $A$. Notation should be consistent. \n\n* What is the baseline algorithm on synthetic datasets? It seems that authors move the synthetic experiments to the appendix, but didn't move the discussion on the baseline algorithm back to the main body of the work. Also, the baseline is a bit too weak.\n\n* The use of $k$ and k is inconsistent.\n\n* Line 687, \"multiplicative weighs\" -> \"multiplicative weights\".\n\n* [4] provides more fine-grained complexity analysis for weighted low-rank approximation. It also gives several scenarios where the problem can be solved in polynomial time. \n\nReferences:\n\n[4] Razenshteyn, Song and Woodruff. Weighted Low Rank Approximations with Provable Guarantees."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TPBYKTWjYr", "forum": "rCHZPRf6BH", "replyto": "rCHZPRf6BH", "signatures": ["ICLR.cc/2026/Conference/Submission21828/Reviewer_oGcz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21828/Reviewer_oGcz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761097935595, "cdate": 1761097935595, "tmdate": 1762941946007, "mdate": 1762941946007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission studies Online Low-Rank Approximation and provides a novel algorithm which achieves an essentially square-root regret bound. It also provides lower bounds and an empirical evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The proofs are non-trivial and the contributions include a nice combination of foundational and empirical results."}, "weaknesses": {"value": "-Has the online formulation of low-rank approximation been studied before? The only citation provided seems to be for\n\n\"Michael Kamp and Mario Boley. Streaming fraud detection: A survey and new research directions. Data Mining and Knowledge Discovery, 33(2):497â€“531, 2019.\"\n\nwhich, however, does not seem to exist. If that is indeed the case then one is left wondering: why does the submission cite a non-existent article as its sole reference for its primary problem of interest (i.e., online low-rank approximation)?\n\nEssentially, the article treats the problem and related notions as well-established, but does not provide any references or explanations substantiating this. This is a critical flaw: without convincing connections of the studied problem to the core topics of the conference, the article would not be of interest to the ICLR community (the employed techniques, while non-trivial, are mostly of interest to TCS researchers). This issue also arises in the empirical evaluation at the end of the paper: the submission claims that its implementation \"consistently outperforms standard baselines\", but never explains what these baselines are, and I could not find any references to these.\n\nFurther, the main contributions of the article are not explained very well. The first two pages of the article does not discuss the contributions at all, and even the \"Our Contributions\" section starts with a lengthy discussion of the generalizability of the supposed results - even though the actual results are not even stated at that point."}, "questions": {"value": "The algorithm listed as the first contribution provides a square-root regret bound. At the same time, the second contribution (the lower bound) essentially excludes achieving sublinear regret in polynomial time. Do I understand correctly that these two results can coexist because the former algorithm does not run in polynomial time?\n\nAside from answering the above question, the authors are of course also welcome to respond to the above-mentioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RAwZangr9B", "forum": "rCHZPRf6BH", "replyto": "rCHZPRf6BH", "signatures": ["ICLR.cc/2026/Conference/Submission21828/Reviewer_rrVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21828/Reviewer_rrVo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336699123, "cdate": 1761336699123, "tmdate": 1762941945796, "mdate": 1762941945796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about the online SVD problem, where the algorithm is required to maintain a low-rank projection $\\Pi_t$ over time $t$ to minimize $\\sum_t ||(I-\\Pi_t) x_t||^2$ for an online sequence of vectors $x_t$. The authors follow a regret minimization approach based on the multiplicative weights update algorithm over unit vectors, and show that sublinear regret can be achieved if we allow an additional (1+eps) multiplicative regret factor (*). The approach and analysis is based on that of Cohen-Addad et al (AISTATS 2021) for the online k-means problem. The authors also show that the extra multiplicative term (*) is necessary for polynomial-time sublinear regret algorithms, under some APX-hardness assumption. Some experimental results are also presented that compare thee uathors' algorithm with a baseline eps-net."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The parts of the analysis that I checked look sound.\n\n* The online SVD problem has a lot of applications."}, "weaknesses": {"value": "* The main result has very high computational complexity, roughly $T^{O(kd)}$, where $T$ is the number of rounds, $k$ is the low-rank dimension, and $d$ is the ambient dimension. In contrast, in the result of Cohen-Addad et al the complexity is much lower, i.e. $(\\log T)^{O(kd)}$. In fact, the $T^{O(kd)}$ bound should be achievable by an arbitrary $\\epsilon$-net (this is also done in Cohen-Addad et al. Theorem 2.1, and the paper is devoted to reducing the polynomial runtime dependency on $T$), without the need to construct adaptive nets. I believe this makes the contribution of hte paper marginal.\n\n* There is extensive previous work on the online svd problem, none of which is cited. See e.g. https://jmlr.org/papers/volume17/15-320/15-320.pdf https://proceedings.mlr.press/v70/allen-zhu17d/allen-zhu17d.pdf https://edoliberty.github.io/papers/opca.pdf. The authors should contextualize their approach in these previous works.\n\n* The authors' approach very closely follows Cohen-Addad et al. However it is not clear which parts are novel and which parts are from Cohen-Addad et al. The authors should more precisely outline their contribution.\n\n* The experimental baseline is quite weak."}, "questions": {"value": "How does the runtime of the authors' approach compare to MWU on a fixed eps-net?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UwIokosczJ", "forum": "rCHZPRf6BH", "replyto": "rCHZPRf6BH", "signatures": ["ICLR.cc/2026/Conference/Submission21828/Reviewer_EyWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21828/Reviewer_EyWZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762457297732, "cdate": 1762457297732, "tmdate": 1762941945509, "mdate": 1762941945509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}