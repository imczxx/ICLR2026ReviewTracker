{"id": "EDCJTaR9bk", "number": 6427, "cdate": 1757983031637, "mdate": 1759897915187, "content": {"title": "VOGUE:  Unified Understanding, Generation, and Editing for Videos", "abstract": "Unified multimodal understanding–generation models have shown promising results in image generation and editing, but remain largely constrained to the image domain. In this work, we present VOGUE, a versatile framework that extends unified modeling to the video domain. VOGUE adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, VOGUE unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that VOGUE matches or surpasses state-of-the-art task-specific baselines in visual understanding, text/image-to-video generation, in-context video editing and generation. Beyond these core capabilities, the unified design allows VOGUE to generalize to unseen free-form editing tasks, such as green-screening characters or novel task composition (e.g., editing + style transfer) in a single instruction. Notably, VOGUE is the first system to support visual-prompt-based video generation in a unified model, where the MLLM interprets visual prompts and guides the MMDiT in synthesis. To foster future research, our model and code will be released.", "tldr": "", "keywords": ["diffusion;multimodal generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c08370560841cae096d72706952384a11dfdb212.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VOGUE, a unified framework for video understanding, generating, and editing from a single multimodal prompt. To achieve this, VOGUE includes a MLLM for complex semantic and instruction understanding, which is followed by a MMDiT for high-fidelity video generation. The authors demonstrate that VOGUE is able to understand complex multimodal instructions and perform diverse tasks such as T2V, I2V, in-context generation, in-context editing, and even zero-shot generalization. The experiments also show that VOGUE outperforms SOTA task-specific methods on multiple benchmarks like VBench, UNICBench, and human evaluations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The most impressive strength of the work is the wide coverage on diverse tasks, including video understanding, T2V, I2V, in-context video generation, and in-context video editing, under a single framework. As far as I understand, VOGUE is the first video generation model that achieves this level of task unification which can substantially enhance the community interest.\n\n- I also like the experiments where the authors show the model can achieve some zero-shot generation tasks which are not explicitly covered during training. For example, it can transfer the editing abilities to the unseen tasks composition (such as combining style transfer with object deletion). This highlights the advantages to leverage the powerful and frozen MLLM.\n\n- The experiments include the comparisons with existing models on eight tasks. It is also impressive that such a general-purposed model can achieve a similar or comparable performance with the tasks-specific models.\n\n- The authors also provide thorough ablation studies, including 1) the necessity of multi-task learning (compared to single-task learning) and 2) dual-stream model architecture, which are insightful.\n\n- The authors promise that they will release the model checkpoint and code, which can significantly increase the reproducibility and also benefit the further research in unified video generation and editing.\n\n- The paper is well written and easy to follow. The figures are well-plotted and informative which can make readers quickly understand the core ideas."}, "weaknesses": {"value": "- My major concern is the incremental technical contribution. The dual-stream architecture that separately processes multimodal instructions and visual inputs has been explored in some existing models such as FLUX, which also employs two streams to process text and image inputs. Also, using MLLM embeddings to condition the diffusion process has also been explored in Qwen-Image. Therefore, while the model design is effective, it feels more like an integration of existing ideas and does not look insightful to me.\n- It is unclear why the dual-stream architecture improves generalization compared to single-stream model design. For example, models like FullDiT [1] and Qwen-Image achieve great performance using full self-attention without separating understanding and generation streams. Could the authors provide the motivation behind this architecture design?\n- The paper provides limited information about the construction of the training dataset. I hypothesize that learning such a general-purpose model requires high-quality, large-scale, and carefully-curated datasets for training. However, the authors only give a brief summary in Appendix F without describing more details such as the data filtering or annotation. It raises concerns about how the model learns to cover complex tasks such as visual prompt understanding.\n- Following the above concerns, while Figure 6 shows several types of visual prompts, it is unclear how the model handles out-of-distribution or ambiguous prompts.\n- There are some missing citations and comparisons with existing multi-subject video personalization models, such as Video Alchemist [2] and Movie Weaver [3], which can also be formulated as in-context generation.\n\n[1] \"FullDiT: Multi-Task Video Generative Foundation Model with Full Attention\", ICCV 2025\n\n[2] \"Multi-subject Open-set Personalization in Video Generation\", CVPR 2025\n\n[3] \"Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts\", CVPR 2025"}, "questions": {"value": "- Could the authors explain the theoretical motivation for the proposed dual-stream architecture and include ablation studies to verify the design choice of separating the understanding and generation streams?\n- Could the authors elaborate on the dataset curation pipeline?\n- For the visual prompting task, could the authors describe whether the model can handle OOD or ambiguous prompts, and provide some failure cases to help check the generality of this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VlARFwpTr1", "forum": "EDCJTaR9bk", "replyto": "EDCJTaR9bk", "signatures": ["ICLR.cc/2026/Conference/Submission6427/Reviewer_dJ5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6427/Reviewer_dJ5L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760843663042, "cdate": 1760843663042, "tmdate": 1762918819919, "mdate": 1762918819919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a method to imbue a pretrained video generator with multimodal input understanding. The work builds on the observation that text embedding can be replaced with embeddings produced by a frozen MLLM, making it possible for the downstream video generator to leverage MLLM capabilities such as thinking and understanding of multimodal inputs. The authors first curate datasets comprising a variety of multimodal tasks such as image editing, image to video, style transfer, object swapping, addition, deletion. Then a pretrained Hunyuan model is adapted to receive QwenVL2.5-7B multimodal inputs in 3 stages, by first training a connector MLP, then fine-tuning the video generator on T2I and T2V, and finally training on the full range of tasks. The resulting model produces convincing results in a variety of tasks and shows ability to generalize outside of the set of training tasks. Quantitative evaluation shows generally better or comparable scores with respect to VACE and commercial models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The task of creating unified video generators capable of tackling multiple visual tasks from T2V to video editing is of high relevance\n- Qualitative results are convincing. The model seems to be able to tackle a range of video generation tasks from object insertion to instruction based editing with convincing quality\n- Ablations are insightful and validate that 1) joint training on all tasks reinforces model performance in all tasks, 2) relying on MLLM visual embeddings is insufficient, requiring visual tokens being passed to the video generation backbone directly\n- The authors provide complete training and dataset details \n- Authors commit to public release of model and code"}, "weaknesses": {"value": "- Ablation on MLLM visual embeddings raises doubts of how much the introduction of the MLLM contributes to model performance with respect to the newly collected data. See questions.\n\nMinor:\n- Minor amount of typos, especially lack of space before citations (LL254, LL258)\n- LL52, LL423-LL427 the claim appears inaccurate. Veo 3 was previously found to support this feature (https://www.reddit.com/r/singularity/comments/1m9b0bq/googles_new_feature_in_veo_3_you_can_now_draw/) (https://www.youtube.com/watch?v=KNGMBRyGcDo). I suggest removing the claim."}, "questions": {"value": "I hope the authors could clarify the following questions\n- Table 5 reports that \"w/o visual for MLLM\" achieves an overall score that is very close to VOGUE. While the exact setting is slightly unclear (are we just removing visual tokens from the MLLM output or is the MLLM not receiving visual tokens at all?) LL440-LL441 suggests that no visual token is processed by the MLLM in this setting. While a small gap is present in the prompt following (PF) metric, this raises the question of whether an MLLM is needed at all. If only text tokens are processed by the MLLM in the performed ablation, wouldn't the original text encoder produce similar performance without the need for an MLLM? This is an important point to clarify as otherwise the role of the MLLM is unclear, and a simpler framework with equal capabilities could be constructed by means of the collected dataset only without any MLLM.\n- Table 2 reports a series of Understanding metrics. Do I understand correctly that these metrics are computed using the frozen MLLM model alone without involvement of the video generator? If so, such metrics are less interesting to show. The claim that the model can perform \"Understanding\" in a unified way with generation and editing in this case would be problematic as the \"Understanding\" part is completely offloaded to a pretrained and frozen external model, which is not a unified approach. In a truly unified approach, the MM-DiT itself would output text tokens for understanding tasks when necessary.\n\nMinor:\n- Could the authors clarify more how the dataset ratios shown in Table 1 were computed? Can the authors offer insights into how optimal ratios should be derived?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gA7RbeJKke", "forum": "EDCJTaR9bk", "replyto": "EDCJTaR9bk", "signatures": ["ICLR.cc/2026/Conference/Submission6427/Reviewer_p7Fb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6427/Reviewer_p7Fb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760910352470, "cdate": 1760910352470, "tmdate": 1762918819285, "mdate": 1762918819285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a system for joint image/video understanding, generation and editing. It consists on two streams: understanding and generation. The understanding stream is the frozen Qwen-VL2.5-7B. The generation stream is initialized from HunyuanVideo-T2V-13B. The training is split into 3 stages: 1) training the connector between MLLM and MMDiT; 2) fine-tuning MMDiT for base tasks (T2I + T2V); 3) multi-task fine-tuning of MMDiT. The results look very good visually, and the model was even shown to zero-shot generalize to novel tasks (e.g. free-form video editing). The authors also perform a thorough quantitative evaluation demonstrating the superiority of the method over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is a quite elegant architecture which i think among the early works for this paradigm shift in generative modeling of concatenating everything into a single sequence and modeling. While it does not serve as a good reference for ablations, but it's a good proof of concept to convince the community that this paradigm works.\n- The results are really good, especially given the little amount of GPUs used to fine-tune the model. Also, the data size is affordable to collect for non-bigtech companies (e.g. startups)\n- The paper reads well and the illustrations are good. The submission also includes many qualitatives which are very easy to view on the website."}, "weaknesses": {"value": "- From my perspective, the main weakness is the lack of rich ablations, that could make the submission to be a good reference for follow up works. For example, is it necessary to do all the 3 stages sequentially or we can train jointly? Is the diffusion schedule the same for all the modalities? How much improvement can we get by making it different? Would it help to do some dropout on some input modalities in a task? Is it possible to replace full attention with cheaper attention variants in-between the modalities? Also some profiling results would be interesting to see, e.g. how much compute is spent on each modality in each component (is MLLM heavy?) And so on.\n- Some important details are missing from the submission, mainly related to data curation. It would be fine to omit them for a technical report, but it is not good practice to omit them for an academic submission. For example, how exactly was stylized video transformed into a real one (appendix F.2)? How was the video inpainting model training (appendix F.1)? Was the dataset post-curated with human annotators? If so, what were the instructions for them?\n\nSmall writing comments:\n- typo: \"generate an video\" => \"generate a video\" (in figures)\n- typo: \"source open source\" on line 907\n- I would suggest re-coloring Figure 3 to display the frozen MLLM as blue, and trainable DiT as red, as common in the prior literature."}, "questions": {"value": "Could you please include more dataset details (as specified in the previous section)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lOxqSLuj4s", "forum": "EDCJTaR9bk", "replyto": "EDCJTaR9bk", "signatures": ["ICLR.cc/2026/Conference/Submission6427/Reviewer_2yjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6427/Reviewer_2yjX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791909732, "cdate": 1761791909732, "tmdate": 1762918818761, "mdate": 1762918818761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is cleanly structured and easy to read. Qualitative results are abundant and illustrative, and the in-context editing/generation comparisons are clearly presented. At a high level, the work’s strengths are system-level integration and breadth of tasks. The level of novelty is average but acceptable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A pragmatic dual-stream unification (frozen MLLM for understanding + MMDiT for generation) that feeds visual inputs to both streams, with ablations showing why both sides need visuals for identity preservation and semantics.\n2. Strong mask-free in-context editing/generation results despite baselines using masks; the qualitative figures and automatic metrics make the claim legible and practically relevant.\n3. Transparent training recipe and task coverage, including staged training, freezing choices, connector design, and explicit mixing ratios. It is useful for reproduction and future baselines."}, "weaknesses": {"value": "1. Thinking Mode is not well and fully discussed(How much does it benefit?).\n2. What is the benefit of making it a single model rather than using a workflow or agent? Qualitative analysis?"}, "questions": {"value": "1. The workflow diagram (Fig. 3) isn’t very clear. Is it correct to interpret that the user feeds an interleaved text–image instruction to the MLLM, and then the MLLM’s output is concatenated with the VAE features of the conditioning images as the input? Also, why is the noise term missing in Fig. 3? It feels like this should be made consistent with Fig. 2.\n2. Maybe compare to more unified understanding & generation model"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FCfcZGPIhO", "forum": "EDCJTaR9bk", "replyto": "EDCJTaR9bk", "signatures": ["ICLR.cc/2026/Conference/Submission6427/Reviewer_zojR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6427/Reviewer_zojR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995047056, "cdate": 1761995047056, "tmdate": 1762918818296, "mdate": 1762918818296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}