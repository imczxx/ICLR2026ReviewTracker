{"id": "ZR3HNkT2cj", "number": 11228, "cdate": 1758193884776, "mdate": 1759897599788, "content": {"title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "abstract": "The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs—including text, reference audio, and reference motion—facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis.", "tldr": "We present JAM-Flow, a unified model that jointly generates speech and facial motion from diverse inputs, enabling expressive and synchronized audio-visual synthesis.", "keywords": ["Talkinghead", "TTS", "AutomatedDubbing", "multi-modal generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f27cedf57a42f64270d0afaa1acc9b0bbfc6ed89.pdf", "supplementary_material": "/attachment/d5d884c00a1c71f33f2dc725852657650cf05493.zip"}, "replies": [{"content": {"summary": {"value": "The paper propose JAM-flow, the first joint training framework for talking head and TTS generation. The proposed inpainting-style joint supervision effectively improves the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is novel. It is the first joint training framework for talking head and TTS generation.\n2. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The performance in demo video is not satisfactory. The teaser video exhibits significant artifacts.\n2. The paper lacks comparisons with recent methods, such as EDTalk.\n3. Since the paper only generate mouth motions, it should compare the method with visual dubbing methods (which also only generate mouth motions), such as wav2vec, stylesync."}, "questions": {"value": "It would be appreciated if the authors can provide comparisons result with recent methods, such as EDTalk, etc.\n\ntypo: 2.2 TALKING HEAD HENERATION -> Generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rd3ZiPjpKs", "forum": "ZR3HNkT2cj", "replyto": "ZR3HNkT2cj", "signatures": ["ICLR.cc/2026/Conference/Submission11228/Reviewer_Qj8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11228/Reviewer_Qj8r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761050681130, "cdate": 1761050681130, "tmdate": 1762922391313, "mdate": 1762922391313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a joint audio–motion flow matching framework that learns both modalities within a single model.\nIt combines Motion-DiT and Audio-DiT through joint attention and asymmetric temporal masking, aiming to improve synchronization between speech and facial motion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear and easy to follow, with well-structured writing and visuals.\n2. It cites relevant work and explains design choices logically, making the reasoning convincing.\n3. The flexible input setting is an interesting and practical aspect of the framework."}, "weaknesses": {"value": "1. The paper does not provide clear quantitative evidence on how joint training improves performance over unimodal setups. Based on the current presentation, the main benefit of joint supervision seems to lie in enabling flexible input configurations rather than delivering measurable quality gains.\n2. The effectiveness of the proposed attention masking is discussed qualitatively, but additional quantitative analysis would help clarify its impact on temporal alignment and overall performance.\n3. The overall novelty feels incremental, as most components (F5-TTS, LivePortrait, MM-DiT) are reused.\nStill, positioning it as the first joint, inpainting-style framework for flexible bidirectional audio–motion generation is reasonable, and specifying what new capabilities or effects arise directly from the joint setup would make the contribution clearer."}, "questions": {"value": "1. Could the authors provide more details about the unimodal baselines—for example, whether the audio-only and motion-only branches were trained independently—and include both quantitative results and qualitative examples comparing these baselines to the joint model?\n2. The paper discusses an asymmetric attention design. It would be helpful to include quantitative evidence showing how this design choice affects temporal alignment or overall performance.\n\nI would be inclined to raise my rating if the authors address the questions and clarifications mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IMyAaUYG0X", "forum": "ZR3HNkT2cj", "replyto": "ZR3HNkT2cj", "signatures": ["ICLR.cc/2026/Conference/Submission11228/Reviewer_cug6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11228/Reviewer_cug6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940777672, "cdate": 1761940777672, "tmdate": 1762922389689, "mdate": 1762922389689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JAM-Flow, a unified framework for joint audio-visual synthesis that simultaneously generates speech and facial motion using flow matching. The method introduces two modality-specific diffusion transformers, Motion-DiT for implicit facial keypoint dynamics and Audio-DiT for speech generation — coupled through selective joint attention layers. Several architectural design choices, such as temporally aligned rotary positional embeddings (RoPE) and localized asymmetric attention masking, are proposed to enable stable and temporally consistent multimodal learning. Experimental results show improvements in synchronization and realism across multiple metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*  joint modeling direction: The idea of training a single flow-matching framework to co-model both speech and facial motion is conceptually appealing and targets an underexplored space between talking-head generation and TTS systems.\n* Architectural clarity: The paper carefully describes how Motion-DiT and Audio-DiT interact through partial joint attention, cross-modal RoPE alignment, and modality-specific attention masking, all of which are reasonable and technically sound design choices.\n* Stability insights: The introduction of asymmetric masking and RoPE alignment are practical contributions that could generalize to other multimodal DiT architectures.\n* Empirical completeness: The paper provides both quantitative results and ablation analyses, showing that the proposed fusion and masking yield modest but consistent performance improvements."}, "weaknesses": {"value": "1. Partial joint attention design not well justified: Only half of the layers are fused via joint attention, but the paper does not explain how this number was selected, whether more or fewer fusion layers were tested, or how fusion depth affects stability and performance.\n2. Unclear practical benefits of joint modeling: While the paper argues that jointly modeling speech and motion reflects the natural coupling of human communication, it is unclear what measurable benefit this joint training provides compared to separately trained TTS and talking-head models. The results do not convincingly demonstrate improvements in either lip synchronization, intelligibility, or overall audiovisual coherence due to the joint setup.\n3. Unclear loss balancing across modalities: The final loss combines motion and audio velocity objectives, but no details are given on how they are balanced. Since these losses have different magnitudes and noise characteristics, simple summation is unlikely to be optimal. Techniques such as dynamic weight averaging or uncertainty weighting could improve stability. Without this clarification, reproducibility and convergence behavior remain ambiguous.\n4. Weak and incomplete ablation study (Table 4):\nThe ablation does not cleanly isolate variables model depth, attention masking, and Audio-DiT finetuning are all changed simultaneously. The absence of a “Full + Mask” baseline makes it difficult to attribute improvements. The gains (e.g., FID 5.75 → 5.66, SIM 0.63 → 0.64) are marginal and lack variance reporting. \n5. Inpainting-style supervision not novel: The “inpainting-style” joint supervision resembles existing masked-prediction training used in F5-TTS and other multimodal diffusion models, and its distinct contribution here is not experimentally validated."}, "questions": {"value": "1.\tWhat concrete advantages does joint modeling provide over independently trained TTS and talking-head models? Please quantify or illustrate how co-training improves synchronization, expressiveness, or audiovisual coherence. What are the scenarios you see this improvement clearly.\n2.\tHow are the audio and motion velocity losses balanced during training? Given their different scales and variances, did you normalize or dynamically weight them to avoid one modality dominating the optimization?\n3.\tWhy are only half of the Transformer layers fused through joint attention, and how sensitive are results to this choice? Was this proportion empirically tuned, or based on observed stability/performance trade-offs?\n4.\tIn Table 4, the ablation mixes multiple design changes (depth, masking, Audio-DiT finetuning).\nCould you provide a controlled ablation isolating each factor. For example, a “Full + Mask” configuration, to clarify where the gains come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zNLn9lRrB1", "forum": "ZR3HNkT2cj", "replyto": "ZR3HNkT2cj", "signatures": ["ICLR.cc/2026/Conference/Submission11228/Reviewer_Zo8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11228/Reviewer_Zo8R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030987265, "cdate": 1762030987265, "tmdate": 1762922388995, "mdate": 1762922388995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified framework that integrates multiple tasks using Flow Matching and MM-DiT, along with an inpainting-style joint supervision strategy to support training across the TTS-to-THD pipeline. While the framework design and experimental coverage are reasonably thorough, the underlying motivation remains unconvincing. The method appears to be driven more by the absence of existing joint models than by a clearly defined problem or application need. Consequently, the contribution reads more as an engineering-level assembly of components than a conceptually motivated advancement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Consolidating multiple functionalities into a unified model enhances flexibility and improves usability for uses.\n\n2. The authors have made commendable efforts in both model functionality and experimental design, which may offer certain insights for future research.\n\n3. The analysis of mouth keypoints in LivePortrait contributes to improved lip-speech consistency."}, "weaknesses": {"value": "1. The authors claim to propose the first unified framework for TTS and THG, yet the motivation and significance of such integration are not clearly articulated. In typical THG pipelines, the driving signals—whether speech, video, or text—are already well-supported by a range of mature generation techniques, which raises questions about the practical necessity of unifying these components.\n\n2. The combination of TTS and THG in this work appears more akin to an engineering optimization effort than a conceptually novel contribution. In practice, many developers have already achieved impressive results by chaining together multiple existing technologies. For example, in the creation of virtual idols.\n\n3. The supplementary video reveals a noticeable issue in image-to-video generation: the background tends to shift along with the subject’s motion. This artifact is non-trivial and may affect perceived realism, yet the authors do not provide analysis or discussion regarding its cause or impact.\n\n4. Weak TTS performance may negatively affect the accuracy of mouth movements, potentially lowering the overall effectiveness of the model.\n\n5. Wav2Lip is a talking face generation method, yet it is incorrectly categorized as a TTS system in the first paragraph of the introduction."}, "questions": {"value": "1. The output of facial expressions relies on mouth keypoints from LivePortrait, which raises the question of how well other expressions such as smiling and eye gaze are represented.\n\n2. How long of a video does the model support generating?\n\n3. The talking head experiments should include comparisons with more representative baselines such as Hallo2, Wav2Lip, and MEMO.\n\n4. I would like to understand the performance gap when using a cascaded model to achieve the same goal. For example, in a text-audio-visual pipeline, the system first generates speech using TTS and then drives facial expressions. How does the performance of this approach compare to the method proposed in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AI0KD54rCd", "forum": "ZR3HNkT2cj", "replyto": "ZR3HNkT2cj", "signatures": ["ICLR.cc/2026/Conference/Submission11228/Reviewer_pYmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11228/Reviewer_pYmU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091989148, "cdate": 1762091989148, "tmdate": 1762922388572, "mdate": 1762922388572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a joint training framework for talking head and TTS generation. Specifically, it combines Motion-DiT and Audio-DiT modules through partial joint attention, RoPE alignment and asymmetric masking. The model handles multiple tasks—talking head generation, TTS, and video dubbing—within one architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea to unify speech and motion synthesis in a single model is interesting. And the architecture design including joint attention and masking is reasonable.\n2. Proposed inpainting-based training enables flexible conditioning under missing modalities."}, "weaknesses": {"value": "1. In the experiment, the improvements over baselines are marginal or inconsistent. In TTS, WER even increases compared with baselines.\n2. The scalability and generalization of proposed framework are not convincingly discussed. How the method performs with longer sequences, different speakers or higher-resolution videos?\n3. The evaluation is limited. Metrics such as LSE-C/LSE-D are known to be unstable and no strong multimodal baselines are provided for fair comparison."}, "questions": {"value": "1. Could the framework generalize beyond face–speech to other modality pairs (for example, gesture–audio)?\n2. How does computational cost compare to independent training of the two unimodal models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PQ16UKyKKr", "forum": "ZR3HNkT2cj", "replyto": "ZR3HNkT2cj", "signatures": ["ICLR.cc/2026/Conference/Submission11228/Reviewer_rF3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11228/Reviewer_rF3g"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762595129051, "cdate": 1762595129051, "tmdate": 1762922388087, "mdate": 1762922388087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}