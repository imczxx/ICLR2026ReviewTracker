{"id": "ll6bxOyKf0", "number": 19708, "cdate": 1758298585434, "mdate": 1759897024072, "content": {"title": "Boosting Adam-like Optimizers with Signal-to-Noise Ratio Guided Updates", "abstract": "The Adam optimizer remains the default choice in deep learning, offering reliable performance across diverse architectures and tasks. \nIn this work, we reinterpret Adam from a signal-processing perspective—viewing its gradient update as a momentum estimate normalized by noise amplitude—and propose a simple modification: replacing the second raw moment with the second central moment (variance). \nWe show that centering provides a more accurate estimate of noise amplitude, allowing the optimizer to normalize the impact of gradient noise uniformly across the loss landscape and to dynamically scale momentum elements according to their signal-to-noise ratio.\nEmpirically, this modification yields consistent performance gains over Adam and its variants across multiple learning paradigms and neural network architectures, including reinforcement learning and sequence modeling. \nNotably, on reinforcement learning benchmarks such as MuJoCo, our centered variant called “Adam+” achieves faster convergence and improved stability compared to Adam, which remains the gold standard in settings characterized by non-stationarity and the absence of reliable learning rate schedules.", "tldr": "", "keywords": ["Optimization", "Deep Learnin"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad4ecf6564ee1a57515e4f1982ddb7a58faf1d46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **Adam+**, an enhancement of the Adam-like optimizers that replaces the second raw moment with the *variance* to better capture gradient noise via the signal-to-noise ratio (SNR). This centering yields more accurate normalization of updates and improves stability. Adam+ also adds mild noise injection to reduce bias between moment estimates. Experiments across reinforcement learning, sequence modeling, and graph regression show faster convergence, higher returns, and greater robustness than Adam and its variants, suggesting that centering second moments is a simple yet broadly effective improvement for adaptive optimizers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments span a diverse range of tasks, including reinforcement learning (MuJoCo, Atari), sequence modeling (LSTM, nanoGPT, BERT), and graph regression, supported by clear plots that demonstrate consistent performance gains of Adam+ over baselines.  \n2. The proposed SNR-guided update is conceptually simple and easy to implement, providing an intuitive link between gradient statistics and learning dynamics while requiring only minimal modification to existing Adam-like optimizers."}, "weaknesses": {"value": "1. The paper lacks theoretical justification for why signal-to-noise ratio–guided updates should improve optimization. It does not provide convergence analysis or formal guarantees showing whether Adam+ achieves faster or more stable convergence than Adam.  \n\n2. Adam+ introduces an additional memory cost proportional to model parameters to store the momentum term $w_t$. The authors should explicitly discuss this overhead and justify its practicality, especially for memory-constrained scenarios such as large-scale language model training.  \n\n3. Figure 6 appears overly dense and visually cluttered. The authors should consider improving its layout or simplifying the presentation to enhance readability and interpretability.  \n\n4. The experimental scale remains limited, focusing mainly on small or toy settings. Larger-scale benchmarks would strengthen the empirical claims and demonstrate practical relevance for real-world training workloads.  \n\n5. The discussion of related work is insufficient and omits several recent optimizers. Moreover, the baselines used in experiments are relatively weak considering current advances in optimization methods. In particular, comparisons with AdaEMAMix [1], SOAP [2], Muon [3], and Scion [4] are missing and would provide a more comprehensive evaluation.  \n\n## References\n[1] Pagliardini, Matteo, Pierre Ablin, and David Grangier. \"The ademamix optimizer: Better, faster, older.\" arXiv preprint arXiv:2409.03137 (2024).  \n[2] Vyas, Nikhil, et al. \"Soap: Improving and stabilizing shampoo using adam.\" arXiv preprint arXiv:2409.11321 (2024).  \n[3] Jordan, Keller et al. \"Muon: An optimizer for hidden layers in neural networks.\" 2024. URL: https://kellerjordan.github.io/posts/muon/.  \n[4] Pethick, Thomas, et al. \"Training deep learning models with norm-constrained lmos.\" arXiv preprint arXiv:2502.07529 (2025)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SlylfUunHw", "forum": "ll6bxOyKf0", "replyto": "ll6bxOyKf0", "signatures": ["ICLR.cc/2026/Conference/Submission19708/Reviewer_KLCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19708/Reviewer_KLCS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416715779, "cdate": 1761416715779, "tmdate": 1762931549459, "mdate": 1762931549459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a variant of Adam called Adam+ by replacing the second raw moment with the second central moment (variance). The central idea is to view its gradient update as a momentum estimate normalized by noise amplitude. A comprehensive empirical result verifies the superiority of Adam+ over Adam in reinforcement learning and sequence modeling tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper provides detailed experimental results to illustrate the superiority of Adam+ over Adam or AdaBelief. The enhancement over training performance seems very clear from the figures and tables.\n\n- The paper is well-written."}, "weaknesses": {"value": "My major concerns mainly lie in the following.\n\n- The motivation of Adam+ is not very clear. In particular, the essential differences between Adam+ and AdaBelief are not very clear. In my view, the only difference is to use $w_t$, which is also the exponential moving average of the gradient controlled by $\\beta_2$, to replace $m_t$ in AdaBelief. I do not see the motivation for this replacement. The authors claim that ``While this formulation shares the use of a central\nmoment, its interpretation differs fundamentally from the SNR perspective presented in this work.\", but without further explanation. \n\n- The author also claims that the correlation between $m_t$ and the noise will hinder the convergence. First, I do not see any convincing experimental results to illustrate this. Perhaps some ablation studies are required. Second, with $m_t$ replaced by $w_t$, the correlation still exists in Adam+. Given this, I think that Adam+ may be just a slightly different version of AdaBelief.\n\n- The author provides some experimental results to show that Adam+ performs better than Adam or AdaBelief. However, I think that the hyper-parameter setups are not very clear. The author only claims that ``Notice that we have used the same set of tuned\nhyperparameters (Haarnoja et al., 2018; Raffin, 2020) for all simulations.\" without any further detailed explanation in the main body."}, "questions": {"value": "- What are the major differences between Adam+ and AdaBelief?\n\n- Which mechanisms lead to the performance enhancement of Adam+? Does this have some mathematical insights or intuition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ytiFAW7X7M", "forum": "ll6bxOyKf0", "replyto": "ll6bxOyKf0", "signatures": ["ICLR.cc/2026/Conference/Submission19708/Reviewer_RYRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19708/Reviewer_RYRY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549395998, "cdate": 1761549395998, "tmdate": 1762931548734, "mdate": 1762931548734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reinterprets Adam through the lens of gradient signal-to-noise ratio, viewing the update as momentum scaled by an estimate of noise amplitude, and argues that Adam’s use of the raw second moment conflates mean and variance. The authors propose Adam+, which replaces the raw second moment with the central second moment by maintaining a slow EMA of gradients $w_t$ and updating $v_t$ with $(g_t - w_t)^2$. They also study a small Gaussian noise injection into gradients to decorrelate $m_t$ and the variance estimate and to avoid overly aggressive scaling when the variance becomes tiny. Across tasks in reinforcement learning, sequence modeling, molecular graph regression, and image classification, Adam+ and centered variants of other Adam-like optimizers show consistent gains or stability improvements over their baselines. In MuJoCo SAC and Atari DQN, Adam+ converges faster and is more robust to non-stationarity and exploration noise than Adam. In sequence modeling, Adam+ improves perplexities for LSTMs on Penn Treebank and reduces instability for nanoGPT relative to AdamW. GLUE fine-tuning with a crammed BERT shows average improvements with AdamW+ over AdamW. The paper supplements results with SNR diagnostics, ablations on the signed non-linear scaling of updates, and detailed hyperparameters to encourage reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Clear SNR-based formulation\n\nThe work offers a clean rethinking of Adam as scaling momentum by noise amplitude and shows that centering the second moment yields an estimator aligned with SNR. The resulting updates are simple, require only an extra EMA for the mean gradient, and are easy to slot into the Adam family. The derivation and notation are concise, and Algorithm 2 makes the change explicit.\n\n### Minimal change, broad compatibility\n\nAdam+ changes only the second-moment estimator and optionally injects tiny Gaussian noise, so it is easy to implement and to port to AdamW, AMSGrad, LAMB, and ADOPT. The appendix lists drop-in “+” variants and non-linear SNR-guided scalings, showing the idea composes with popular Adam-like optimizers."}, "weaknesses": {"value": "### Larger optimizer state\nCompared to Adam, Adam+ must keep $m_t$, $v_t$, and the extra EMA $w_t$, increasing state memory per parameter. While modest, this makes the footprint comparable to AMSGrad-style variants that also track a third buffer.\n\n### Sensitivity in very low-variance regimes\nWhen the centered variance becomes very small, the per-element scaling can explode without safeguards. The method relies on $\\varepsilon$ or the optional noise injection to maintain a noise floor, which introduces additional knobs that may require tuning in some settings.\n\n### Fairness versus per-optimizer retuning\nThe authors hold pipelines and most hyperparameters fixed to isolate the effect of centering, which is methodologically clear but may understate baselines that benefit from per-optimizer retuning. The paper emphasizes identical conditions, yet some optimizers are known to prefer different $\\beta$ schedules or learning-rate rules.\n\n### Possible lag under rapid non-stationarity\nThe mean gradient $w_t$ is a slow EMA using $\\beta_2$. If the gradient distribution shifts abruptly, a slow $w_t$ could lag, briefly misestimating variance before adapting, which the paper partly mitigates with noise injection and shows empirically favorable behavior in RL but does not analyze formally."}, "questions": {"value": "- Adam+ keeps $m_t$, $w_t$, and $v_t$, and injects noise per step. Did the authors measure the per-parameter state memory and step-time overhead versus Adam and AMSGrad on large models and mixed precision training, and can you share numbers for GPU RAM and wall-clock throughput?\n- Algorithm 2 uses the same $\\beta_2$ to form the slow mean $w_t$ and the centered variance $v_t$. Did the authors try decoupling these parameters differently, and does theory suggest an optimal relation between them?\n- How sensitive is the scale of noise injection for the gradient estimate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "v7MgaC7Ghi", "forum": "ll6bxOyKf0", "replyto": "ll6bxOyKf0", "signatures": ["ICLR.cc/2026/Conference/Submission19708/Reviewer_PwsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19708/Reviewer_PwsC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821003512, "cdate": 1761821003512, "tmdate": 1762931547648, "mdate": 1762931547648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adam+, which replaces Adam’s second raw moment with the second central moment (variance), optionally with small noise injection, so updates become SNR-proportional, reducing over-attenuation in high-signal regimes and remaining drop-in compatible with other Adam-like optimizers. Validated across RL, sequence modeling, synthetic optimization, and graph regression, Adam+ shows consistently faster convergence, stronger final performance, and improved training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well written and easy to follow.\n- The proposed method simply replaces Adam’s second raw moment with the second central moment (variance), so that updates become SNR-proportional, mitigating over-attenuation in high-signal gradient regimes; the principle is clear and elegant. Moreover, this change is broadly applicable to various Adam-like optimizers, making it highly useful.\n- Across diverse tasks, including RL and sequence modeling, the method consistently improves performance and stabilizes training, which supports strong confidence in the approach."}, "weaknesses": {"value": "- While the proposed method, Adam+, keeps updates SNR-proportional by replacing the raw second moment with the centered second moment (variance) when estimating vt​ (thus avoiding unnecessary attenuation) the paper does not sufficiently explain how the second design choice, noise injection into the gradient sample, relates to Adam+. Please clarify whether noise injection is theoretically complementary to the SNR-centering (i.e., thanks to centering, adding noise does not break the updates and can be beneficial), or whether it is orthogonal (a regularization that would similarly help other optimizers as well).\n- In experiments, results are not shown for noise injection with standard Adam/AdamW. Please add an ablation comparing Adam/AdamW (+ noise) under the same σ and settings to empirically disentangle whether the effect of noise injection is specific to Adam+ or independent of it.\n- Figure 1 is hard to interpret from the caption alone. From the text, I understand it illustrates that under the proposed method the update is normalized by the noise standard deviation so that, even when noise grows in the late stage, the update becomes small and cautious. Please add a short explanation of this intent to the caption. Also, in the late stage label, shouldn’t γt1​ be γt2​?"}, "questions": {"value": "Please address the above concerns with concrete clarifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfCiM9rtDY", "forum": "ll6bxOyKf0", "replyto": "ll6bxOyKf0", "signatures": ["ICLR.cc/2026/Conference/Submission19708/Reviewer_p2bK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19708/Reviewer_p2bK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762711509045, "cdate": 1762711509045, "tmdate": 1762931547298, "mdate": 1762931547298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}