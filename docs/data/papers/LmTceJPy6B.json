{"id": "LmTceJPy6B", "number": 587, "cdate": 1756750557665, "mdate": 1759898252058, "content": {"title": "Condition Matters in Full-head 3D GANs", "abstract": "Conditioning is crucial for stable training of full-head 3D-aware GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training (\\cref{fig:intro}(a,b)). However, a series of previous full-head 3D-aware GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions (\\cref{fig:intro}(d-i)). In this work, we propose to use \\textit{view-invariant semantic feature} as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training (\\cref{fig:intro}(c)) and enhances the global coherence of the generated 3D heads (\\cref{fig:teaser}). Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.", "tldr": "We propose a semantic-conditional full-head 3D-aware GAN that uses view-invariant semantic features instead of camera views for conditioning, eliminating directional bias and improving generation quality, diversity, and consistency across all views.", "keywords": ["3D Head Synthesis", "3D Avatar", "3D-aware GANs"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13ad781742926bab5bbf495556a7dd7e397e72ed.pdf", "supplementary_material": "/attachment/48986d72428b8dda4da124f953765d30467d1e63.zip"}, "replies": [{"content": {"summary": {"value": "The authors first create a dataset with a range of view angles using FLUX.1 Kontext - 10 multi-view head images. Therefore, they start with existing front-view datasets first, and then use FLUX.1 Kontext to predict the respective other views. While the 2D prior is not 3D consistent but it preserves the identity which is sufficient for training their 3D-aware GAN.\n\nThe paper then proposes a method similar to 3D-aware GAN head generator, in the spirit of existing works such as PI-GAN, EG3D, SphereHead, PanoHead, GGHead, HyPlane etc. The latter approach is the closest method. Here, the authors also propose a semantic-conditioned 3D-GAN head generator. The front view is used to anchor all image vies as a single reference image. Semantic features are then used as a condition – in their current versions the authors use CLIP as a feature embedding.\n\nOverall, the method is essentially HyPlane with a semantic conditioning, trained on a new MVS FLUX-generated multi-view dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Nice results\n- The dataset is quite valuable (is it being released?)"}, "weaknesses": {"value": "- technically quite incremental over previous methods such as SphereHead or HyPlane\n- poor evaluation (see below)"}, "questions": {"value": "Comparison are somewhat unfair as GGHead, PanoHea, and SphereHead are trained on different data.\n- What if the proposed method was trained on the same data as these baselines?\n- What if the baselines were trained on the proposed dataset?\n\nI would’ve also loved to see the comparisons in the video. At the moment, the only comparisons are shown in Figure 5, which is insufficient to properly evaluate the method. \n\nSomewhat optional but a nearest neighbor / novelty analysis with respect to the train images would be nice. I would expect this to work well though.\n\nThe writing in the quantitative results paragraph (end of 5.1) is a bit confusing; it would be great if it could be checked for clarity. For instance, what features are used for the FID evaluation? Are they used from the generated dataset? Could you please clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "203MIIlAVZ", "forum": "LmTceJPy6B", "replyto": "LmTceJPy6B", "signatures": ["ICLR.cc/2026/Conference/Submission587/Reviewer_VqNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission587/Reviewer_VqNs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432064947, "cdate": 1761432064947, "tmdate": 1762915558346, "mdate": 1762915558346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel approach for generating high-fidelity and diverse 3D human heads using Generative Adversarial Networks (GANs). The core contribution is the model, BalanceHead, which addresses the severe mode collapse that typically plagues unconditioned full-head 3D-aware GANs. The authors propose a novel conditioning strategy by integrating view-invariant semantic features into the model's training process. This conditioning is claimed to stabilize training and enable the generation of diverse, multi-view renderings and their corresponding 3D geometries"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work targets a well-known instability issue in 3D-aware GANs, specifically the severe mode collapse that results from a lack of proper conditioning. \n2. The idea of using view-invariant semantic features is a promising direction for decoupling identity/semantics from viewing pose, which should inherently lead to more stable and diverse generation compared to simpler conditioning methods.\n3. The  BalanceHead is shown to generate high-quality results, including random-view, multi-view renderings, and visualizations of the underlying 3D geometries, indicating successful 3D structure synthesis and view consistency."}, "weaknesses": {"value": "1. Experiments do not offer a clear comparison to established State-of-the-Art full-head 3D-aware GANs. Without standard quantitative metrics (e.g., FID, diversity scores, LPIPS), the claimed high-fidelity and diversity are difficult to objectively verify.\n2. While the conditioning is claimed to be novel, the paper needs to demonstrate the necessity of using view-invariant semantic features specifically. Without an ablation comparing this approach to simpler, non-semantic conditioning or existing view-variant conditioning methods, the complexity of the proposed feature extraction is not fully justified."}, "questions": {"value": "1. Please include a comprehensive quantitative comparison against relevant SOTA full-head 3D-aware GAN models. This should include metrics such as FID (Fréchet Inception Distance), Diversity/Recall scores, and potentially LPIPS or KID across multiple common head datasets.\n2. Please compare the proposed view-invariant semantic features against simpler conditioning methods (e.g., using explicit pose/camera parameters alone, or using a simple identity embedding). And please  demonstrate the difference in performance (especially diversity/mode collapse) when using view-invariant versus view-variant features."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "re8xx41J6U", "forum": "LmTceJPy6B", "replyto": "LmTceJPy6B", "signatures": ["ICLR.cc/2026/Conference/Submission587/Reviewer_9wYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission587/Reviewer_9wYw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851303581, "cdate": 1761851303581, "tmdate": 1762915558073, "mdate": 1762915558073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new conditioning method for full-head 3D GANs. Unlike previous work which uses camera view directions as the condition, this paper proposes to use semantic image features extracted from the frontal view images as the condition. To do so, the authors curated a large-scale synthetic portrait image dataset with balanced coverage of the view directions and identities, in which each identity has multi-view images generated with Flux. By training on this curated dataset with the proposed conditioning method, experiments report improved performance compared to existing 3D GAN methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[Originality]\n- How to improve the quality of 3D full head synthesis has been a long problem since the publication of EG3D, and the inherited view conditioning has been a quite annoying part. This paper proposes a new conditioning method under the help of current 2D image generation and editing models. Though the data are synthetic, they maintain a level of realism and help to learn a more geometrically consistent 3D GAN. I personally like this idea.\n\n[Quality]\n- This paper conducts common qualitative experiments regarding 3D GAN, including random sampling, novel view synthesis, GAN inversion, and interpolation. The results look plausible to me, especially regarding the hair details.\n\n[Clarity]\n- The manuscript is easy to follow and understand. If you have basic knowledge of 3D GANs such as EG3D, you should be able to follow the details and solutions proposed in this paper."}, "weaknesses": {"value": "Overall I do not see significant weaknesses of the current manuscript."}, "questions": {"value": "I do not have major questions regarding the manuscript, only a small question:\n1. Will the curated dataset be released? As far as I know, data curation should be the most time consuming and dirty part of this work. I’m glad to see the authors put lots of effort into solving it, and I believe the release of this dataset will facilitate further research in this community."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "As this work involves portrait generation of humans, it should be really careful to ensure no harmful content is generated with this method."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P9YpovF3gK", "forum": "LmTceJPy6B", "replyto": "LmTceJPy6B", "signatures": ["ICLR.cc/2026/Conference/Submission587/Reviewer_t6Lb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission587/Reviewer_t6Lb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945640242, "cdate": 1761945640242, "tmdate": 1762915557902, "mdate": 1762915557902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method produces a training procedure for better training of GANs for generating 3D human heads, which are always 3D-consistent and have a realistic appearance from all sides, including the back of the head, and do not feature artefacts such as Janus artefacts or symmetry breaking at the back of the head. The method is based on an observation that view conditioning is suboptimal for training such GANs, as it introduces instability and 3D-inconsistency. The authors propose semantic conditioning based on a CLIP feature of an image from a frontal view. The results are demonstrated as unconditional sampling visuals and FID-* metrics. The method requires multi-view data to be trained. To obtain such, the authors propose to use FLUX diffusion model to generate such multi-view data and a curation pipeline to filter out unrealistic samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper story sounds generally reasonable. Very clear pitch right from the abstract. \n- The paper is interesting and, despite presenting a simple observation and a fix for it, has value for the community and provides an additional important insight into training GANs. For me, reading the paper was interesting mainly because of the observation about the GAN instability and a bit less about the proposed fix for it (see Weaknesses).\n- Image-flitering head is a smart idea that ensures high quality of the results. \n- Great unconditional sampling results, even for the failure cases in Fig. 12. Generally, the improvement in both metrics and visuals is very substantial."}, "weaknesses": {"value": "- Not very clear how much the size of the dataset + presence of multi-view images in the dataset is actually helping.\n- Writing is often unclear. \n    - Teaser is also a bit unclear: the results there look just very similar to other generators, e.g., SphereHead. I think it would be clearer for the reader if some conditioning is also shown. Also, the geometry of the back of the head is not shown there, even though it's likely one of the strongest improvement points.\n    - My (subjective) feeling from the text is that LLMs might have rephrased it (as authors fairly acknowledge), but in a way that is sometimes hard to understand and with repeated claims/sentences (see Questions). Some things like absent spaces around an em-dash (\"—\") -- e.g. see L313) clearly reveal the lack of post-editing after the use of LLMs. \n- The fact that the frontal view feature is used means that the single-view GAN inversion can only be done for ideally frontal images, right? What happens if the head is at least slightly rotated? And when it's e.g. a profile view? \n    - How exactly was GAN inversion done? Was it latent code optimization, PTI, or anything else? \n- A bit raw experimental section (see Questions)"}, "questions": {"value": "- How much do we benefit from the condition itself, not from switching to a new FLUX-generated dataset?\n    - If multi-view data is necessary for training of this method, one could do an ablation with respect to the size of the dataset and the number of views in it\n- How good are the images generated by FLUX? Probably a visual would help to understand it (except one example in Figure 3)\n- Even though the pitch is pretty clear, not sure why we can't apply it to the existing generator; do we also absolutely necessarily need an (orthogonal) contribution with FLUX data?\n    - There's an explanation for that in the Intro: \"Specifically, for a person’s multi-view images, we align the conditioning of all views to the semantic feature of the frontal view, since it contains the most comprehensive information\" --> multi-view data is rare --> we generate it.  Two questions regarding that:\n        - What does \"we align\" mean? (Or \"we anchor\" later in the text). This remains unclear after reading the whole manuscript too.\n        - Does this sentence mean that the method can only work when there are multi-view images of a person available during inference (to obtain a frontal image feature to condition the GAN)? Or is it a requirement only in training? If only in training, then how is inference performed when the image is not exactly frontal? Perhaps it would make sense to train another network that would try to predict the frontal-view CLIP feature from other views that are close to the frontal? And then no multi-view data would be required in training. \n- In any case, GAN inversion results of not exactly frontal images should be shown. Otherwise, the direct applicability of the method would be highly limited. \n- Do I understand correctly that all the experiments are done for HyPlaneHead only? What happens if the trick is applied to other generators? For instance, does the semantic conditioning fix the PanoHead's issue of symmetry breaking in the back of the head, or is SphereHead's ViCiCo loss still crucial to achieve that? \n- L269: \"Nvidia A10 GPU\" -- just to confirm: was A10 or A100 meant?\n- Will the dataset be released publicly?\n- L315: \"frontal views capture rich facial features but lack hair details, while rear views emphasize hair but miss facial features.\" And then, L320: \"Among all possible views, the front view contains the most comprehensive global information, because it includes not only the facial region, which is most sensitive to human perception, but also reflects overall appearance, clothing, and hairstyle.\" Isn't it illogical? Basically, first, the authors are saying that the back view is essential, and then they are saying that the frontal view contains enough info. I understand that the back view does not contain info (it's evident from the visuals), but the writing seems a bit misleading. \n- Why not stack the semantic info from several (predefined) views then? Is it because we don't have those in inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L1u0Siizj6", "forum": "LmTceJPy6B", "replyto": "LmTceJPy6B", "signatures": ["ICLR.cc/2026/Conference/Submission587/Reviewer_WZgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission587/Reviewer_WZgK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997357678, "cdate": 1761997357678, "tmdate": 1762915555094, "mdate": 1762915555094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}