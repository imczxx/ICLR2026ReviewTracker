{"id": "qepfd7N6W6", "number": 11829, "cdate": 1758204106824, "mdate": 1759897551991, "content": {"title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training", "abstract": "Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis reveals that this stability is not primarily due to explicit mechanisms like KL penalty or chain-of-thought reasoning. Instead, we identify an implicit regularization mechanism inherent to RFT as a key contributing factor. Our theoretical analysis suggests that RFT's gradient updates are naturally scaled by the reward variance, acting as a data-dependent regularizer that inherently protects previously acquired knowledge. Finally, we propose a rollout-based instance filtering algorithm to enhance the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.", "tldr": "", "keywords": ["Continual Learning", "MLLM"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f9be62a2addd3dd8c8f49d09388c66b4d8b606e.pdf", "supplementary_material": "/attachment/ef0be8819f76722ee3559ee8a8dbd8b70d97ed9d.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the catastrophic forgetting problem in the continual post-training (CPT) of multimodal large language models (MLLMs) and conducts a systematic comparison of the knowledge retention capabilities between the two paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). Through extensive experiments, it verifies that RFT has an inherent forgetting mitigation advantage, reveals the implicit regularization mechanism driven by reward variance, and finally proposes the Rollout-based Instance Filtering for RFT (RIF-RFT) algorithm to enhance the stability and efficiency of RFT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper reveals the inherent forgetting mitigation property in the reinforcement fine-tuning (RFT) paradigm, which endows RFT with superiority in post-training.\n2. The mechanism analysis of forgetting mitigation under RFT is in-depth and supported by theories."}, "weaknesses": {"value": "1. Experiments are only conducted based on Qwen2.5-VL-7B-Instruct, making it impossible to confirm whether the forgetting mitigation advantage of RFT is independent of the architecture.\n2. There is a lack of direct comparisons with mainstream continual learning methods in terms of knowledge retention performance, storage overhead, and computational efficiency.\n3. The selection method of the filtering threshold in RIF-RFT is not provided, and relevant ablation experiments are lacking."}, "questions": {"value": "1. There is a correlation between continual learning performance and task order; can the model's forgetting resistance effect be verified under different task orders?\n2. Only models of the 7B scale are verified in the paper; can the impact of model parameter scale on the forgetting mitigation effect of RFT be explored?\n3. The experiment focuses on multimodal models: what is the degree of protection provided by RFT for the knowledge of different modalities, and is there any preference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJ5P5swdxm", "forum": "qepfd7N6W6", "replyto": "qepfd7N6W6", "signatures": ["ICLR.cc/2026/Conference/Submission11829/Reviewer_wgFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11829/Reviewer_wgFr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142385684, "cdate": 1761142385684, "tmdate": 1762922847748, "mdate": 1762922847748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates forgetting in two mainstream post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). The authors report that SFT can induce catastrophic forgetting, whereas RFT tends to preserve -- and sometimes even enhance -- general knowledge. They further propose that RFT benefits from an implicit regularization effect in which gradient updates are scaled by reward variance. Building on this, they introduce a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents an interesting theoretical claim: RFT mitigates catastrophic forgetting via implicit gradient regularization driven by reward variance.\n\n2. It offers solid experiments supporting that RFT reduces forgetting, and argues the effect is not attributable to KL constraints or chain-of-thought (CoT).\n\n3. The paper is clear writing and overall easy to follow."}, "weaknesses": {"value": "1. My primary concern is, the central theorem lacks convincing empirical support. The proposed rollout-based instance filtering method (RIF-RFT), motivated by the theorem, should improve anti-forgetting via lower reward variance under the theory, yet it underperforms the baseline RFT. The authors note that filtering reduces training data, but this raises a fairness concern: why not match the effective data budget across RIF-RFT and baseline RFT to isolate the effect of variance reduction?\n\n2. The efficiency claim for the filtering method needs stronger evidence. While filtering may yield higher-quality data for subsequent RFT, it introduces a nontrivial overhead from sampling answers from the base LLM -- often a bottleneck for methods like GRPO. We need more solid proof of this method’s efficiency.\n\nI am willing to increase my score if the authors can address the concerns mentioned above."}, "questions": {"value": "1. The related-work discussion can be expanded. For example, [1] also studies SFT vs. RFT and suggests that SFT \"directly providing answers to new tasks, without linking them to the model’s existing perceptual abilities through reasoning trajectories, causes the output distribution to shift abruptly\". This seems at odds with the paper’s claim that CoT does not mitigate catastrophic forgetting. Could the authors elaborate on the differences between [1] and their work, and clarify the potential contradiction between them?\n\n2. Typo in Eq. (4): the second \"$a$\" should denote the greedily sampled response and should be a different symbol (e.g., \"$\\hat{a}$\").\n\n\n[1] Zhang, Z., Dong, Q., Zhang, Q., Zhao, J., Zhou, E., Xi, Z., Jin, S., Fan, X., Zhou, Y., Wu, M., Fu, Y., Ji, T., Gui, T., Huang, X., & Chen, K. (2025, June 30). Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cV2YhaJ9G9", "forum": "qepfd7N6W6", "replyto": "qepfd7N6W6", "signatures": ["ICLR.cc/2026/Conference/Submission11829/Reviewer_xz8f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11829/Reviewer_xz8f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683421728, "cdate": 1761683421728, "tmdate": 1762922847248, "mdate": 1762922847248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a compelling study on continual post-training (CPT) of large foundation models, comparing two fine-tuning paradigms: supervised fine-tuning (SFT) versus reinforcement fine-tuning (RFT). The authors conduct extensive experiments on a sequence of seven diverse multimodal tasks using a 7B-parameter vision-language model (Qwen2.5-VL-7B-Instruct) as the base. The key finding is that RFT dramatically outperforms SFT in mitigating catastrophic forgetting: when tasks are learned one after another, SFT suffers severe forgetting of earlier tasks, whereas RFT inherently preserves prior task knowledge, achieving performance on old tasks comparable to an ideal multi-task training baseline. Remarkably, RFT even maintains or improves the model’s general knowledge (evaluated on broad benchmarks like MMMU and MMLU-Pro), whereas SFT significantly degrades general capabilities. The study identifies that this superior knowledge retention of RFT is not primarily due to common tricks like KL-divergence regularization or chain-of-thought prompting, but rather stems from an implicit regularization effect in the RFT paradigm itself. To address the practical challenge that vanilla RFT can be less sample-efficient and stable, the authors propose a simple yet effective addition: a Rollout-based Instance Filtering (RIF) algorithm. RIF prunes “incompetent” training instances (those where the current policy fails to produce any rewarding output) before applying RFT, which significantly improves training stability and efficiency without sacrificing performance. Overall, the paper’s contributions include: (1) the first comprehensive analysis demonstrating RFT’s natural advantage in preserving both task-specific and general knowledge during continual fine-tuning, (2) a theoretical insight that RFT’s use of reward signals induces an implicit, data-dependent regularization that mitigates forgetting better than explicit techniques, and (3) the introduction of the RIF-RFT algorithm to enhance RFT’s efficiency while retaining its robustness to forgetting"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality & Importance: The paper addresses a previously overlooked question – whether reinforcement learning-based fine-tuning could fundamentally improve continual learning – making it a fresh and notable contribution. This exploration of the fine-tuning paradigm itself (as opposed to add-on techniques) is highly original. It targets the important problem of catastrophic forgetting in lifelong learning for foundation models, which has broad relevance for deployed AI systems. Empirical Results – Effectiveness of RFT: The experimental results are impressive and persuasive. RFT nearly eliminates catastrophic forgetting on a diverse set of seven tasks, matching the performance of multi-task training without using any memory or regularization tricks. It also preserves general knowledge and even slightly improves overall capabilities of the base model – a striking outcome since typical fine-tuning often degrades general performance. The magnitude of improvement of RFT over SFT (e.g. retaining old task accuracy ~95% vs dropping to ~70% for SFT, in some cases) demonstrates clear superiority. These strong empirical findings support the paper’s claims and indicate high practical utility. Theoretical Rigor and Insight: The authors provide a sound theoretical explanation for RFT’s advantages. They introduce a forgetting risk metric and prove a bound (Theorem 5.2) showing RFT’s expected forgetting risk is scaled down by reward variance compared to SFT. This insight reveals an implicit regularization effect: RFT’s stochastic policy updates naturally avoid large detrimental shifts in parameters important to past tasks. The theory is novel and aligns with the empirical observations, adding significant credibility and understanding to the results. Comprehensive Analysis and Ablations: The paper is thorough in its analysis. It evaluates multiple RFT algorithms (e.g. GRPO, ReMax, etc.) and ablates key factors: for instance, showing that removing the KL penalty or chain-of-thought does not erase RFT’s forgetting mitigation. This rules out alternative explanations and solidifies that the benefit comes from the core RFT mechanism. The authors also measure both task-specific retention and general knowledge retention, providing a holistic view of the model’s performance. The inclusion of RIF-RFT with results (Table 5) demonstrates the authors’ depth of understanding: they not only identified RFT’s strength but also tackled its weakness (instability on some samples) with a validated solution. Overall, the experimental section leaves little doubt about the conclusions due to its breadth and careful design. Clarity and Presentation: The paper is well-written and organized, making it easy to follow the complex subject matter. The motivation is clearly stated, related work is well-situated, and the contributions are clearly itemized. Figures and tables are effectively used to illustrate key points (e.g. a graph showing SFT vs RFT forgetting dynamics), which aids understanding. The clarity of presentation means the ideas and results are accessible to readers, increasing the impact of the work. Relevance and Impact: The work has strong relevance to multiple areas: continual learning, large-scale model fine-tuning, and reinforcement learning. Its findings are likely to influence future research and practice, as they suggest a paradigm shift (incorporating RL objectives for better retention). The method is also practical – e.g., RIF makes RFT feasible by reducing data needs – which could encourage real-world adoption. The paper thus excels not only academically (with insight and rigor) but also in potential real-world impact, aligning well with ICLR’s interests in advancing learnability of AI systems in dynamic settings."}, "weaknesses": {"value": "Computational Complexity: RFT requires rollouts and policy optimization, making it more computationally intensive and sensitive to hyperparameters than standard SFT. While RIF improves efficiency, real-world deployments may still face engineering challenges. Evaluation Scope: The study focuses on one 7B multimodal model and QA-style tasks. Broader validation across domains, tasks, and model scales would strengthen the generality claims. Reward Dependence: RFT assumes access to clear reward signals. In tasks lacking binary correctness (e.g., subjective or open-ended outputs), reward design could be non-trivial, reducing RFT’s immediate applicability. Initial Model Competence: RFT performs best when the base model has some proficiency on new tasks. In cold-start scenarios with zero initial success, the reward sparsity could limit RFT’s learning. RIF helps mitigate this but may discard too much data in such cases. Limited Baseline Comparisons: While the contrast to vanilla SFT is strong, the paper could benefit from including other continual learning baselines (e.g., SFT with replay or regularization) to further isolate the benefit of the RFT paradigm. These limitations are either practical considerations or natural directions for future research and do not detract from the core strength of the paper."}, "questions": {"value": "Generality to Other Settings: How well do you expect the advantages of RFT to transfer to other model types and domains? For example, have you considered or attempted applying RFT-based continual learning on a pure text large language model, or a smaller model with less prior knowledge? It would be insightful to know if RFT’s implicit regularization still shines in those cases, or if there are any domains where SFT might remain competitive. Reward Function Design: Could you clarify how the reward signals were defined for your tasks, and whether any reward shaping or learned reward models were needed? Since the tasks seem to have ground-truth answers (e.g. classification or QA), was the reward simply 1 for a correct answer and 0 for incorrect, or something more nuanced (partial credit, etc.)? Understanding this would help gauge how easily RFT can be applied to new tasks. Additionally, if a task had a more subjective or continuous evaluation metric, do you foresee any challenges in applying RFT there? RFT in Low-Performance Regimes: How does RFT perform when the model’s initial success on a new task is very low, and how effective is RIF in that scenario? The paper introduces RIF to handle “incompetent” samples – for a new task where the model almost never produces a correct output initially, RIF would filter out many samples. Does this risk filtering out too much data and hindering learning new knowledge? In other words, is there a point at which a task is so difficult that a bit of supervised signal might be needed to kickstart learning before RFT takes over? Any discussion on this or empirical observation (even anecdotal) would be appreciated to understand the limits of RFT’s applicability. Combining RFT with Other CL Techniques: Have you considered if combining RFT with traditional continual learning techniques (like a small experience replay buffer or weight regularization) would further improve performance, or is RFT alone already near-optimal? Your results suggest RFT alone is as good as multi-task training, which is remarkable. But for completeness, it would be interesting to know if adding even minor rehearsal of old tasks (or other CL tricks) on top of RFT yields any marginal gains or if it’s unnecessary. This could help practitioners decide how to allocate resources (e.g., focus purely on RFT or also maintain some memory of past data). (Feel free to address these during the rebuttal if time permits – they are mostly aimed at understanding the breadth of applicability and practical considerations of the proposed approach.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bk52C0DTbE", "forum": "qepfd7N6W6", "replyto": "qepfd7N6W6", "signatures": ["ICLR.cc/2026/Conference/Submission11829/Reviewer_gd9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11829/Reviewer_gd9L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789832898, "cdate": 1761789832898, "tmdate": 1762922846883, "mdate": 1762922846883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}