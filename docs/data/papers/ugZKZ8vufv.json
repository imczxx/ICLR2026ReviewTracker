{"id": "ugZKZ8vufv", "number": 14823, "cdate": 1758244379231, "mdate": 1759897347215, "content": {"title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think", "abstract": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we show that this understanding translates into measurable improvements on both problem-solving and safety benchmarks. We can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we show that training data format (e.g., free-form vs. multiple-choice) impacts reasoning far more than data domain, highlighting the importance of format-aware model design. In short, the CoT Encyclopedia turns reasoning from a black box into a controllable asset, enabling LLMs that think more clearly, perform more reliably, and act more safely.", "tldr": "We propose the CoT Encyclopedia, a bottom-up framework for analyzing, predicting, and controlling reasoning strategies in language models, revealing the impact of training data format and enabling performance gains via optimal strategy steering.", "keywords": ["Reasoning in Language Models", "Chain-of-Thought Interpretability", "Model Behavior Control"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88e30ceb7fb10b8d350ab92ebdb5d2d2b99fb206.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the COT ENCYCLOPEDIA, a novel, bottom-up framework designed to analyze, predict, and control the chain-of-thought (CoT) reasoning in large language models. The method automatically extracts a diverse set of reasoning criteria from model-generated text, uses embedding and clustering to form a coherent taxonomy, and generates interpretable reports. The authors demonstrate the framework's utility through comprehensive human evaluations that show its superiority over predefined analyzers and its ability to measurably improve model performance by steering reasoning toward optimal strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper's primary strength is its data-driven, bottom-up methodology for creating a taxonomy of reasoning strategies. This contrasts sharply with prior work using fixed, human-defined categories, allowing the framework to discover emergent and more nuanced reasoning patterns specific to different models and tasks.\n\n(2) Comprehensive Experimental Validation: The claims are supported by extensive experimentation.\n\n(3) Significant Finding on Data Format vs. Domain: The discovery that training data format has a substantially greater impact on reasoning strategies than the content domain is a crucial and non-obvious contribution."}, "weaknesses": {"value": "(1) All major components of the framework—from the initial ideation of 4,057 criteria (Step 1) to rubric formulation (Step 4) and final classification (Step 5)—are executed through an external proprietary language model (GPT-4o). This design choice embeds a structural reliance on a closed-source system and exposes the analysis to its internal biases. Although the authors acknowledge this limitation and incorporate a multi-evaluator setup to mitigate bias (Appendix B.5), the methodology remains fundamentally shaped by the constraints and opacity of the chosen model.\n\n(2) The paper defines “optimal” reasoning strategies as those correlated with higher frequencies of correct or safe answers within a given dataset (Section 4.1). However, this relationship is correlational rather than causal. It remains ambiguous whether these strategies directly contribute to improved outcomes or merely co-occur with successful reasoning patterns. The experiments stop short of disentangling causation from correlation.\n\n(3) In Section 4.3, classifiers are trained to predict which reasoning strategies perform best for specific question types. These models are developed and evaluated on a fixed set of five benchmarks. Although some cross-domain tests are conducted, the study does not extend to genuinely unseen or out-of-distribution datasets. Consequently, the generalizability and robustness of these classifiers beyond the training domains remain uncertain."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RjkjcJMFOo", "forum": "ugZKZ8vufv", "replyto": "ugZKZ8vufv", "signatures": ["ICLR.cc/2026/Conference/Submission14823/Reviewer_WA1g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14823/Reviewer_WA1g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760703347150, "cdate": 1760703347150, "tmdate": 1762925173178, "mdate": 1762925173178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework named “CoT Encyclopedia” for analyzing, predicting, and controlling model reasoning patterns from CoT outputs. The authors claim that their approach can automatically extract reasoning criteria, cluster them in a semantic space, and derive rubrics to interpret LLM reasoning strategies. They further argue that this framework provides better interpretability, predictive power, and safety control than previous categorization-based techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The goal of making LLM reasoning more interpretable and controllable is important and timely.\n- The paper provides a large-scale qualitative analysis that could inspire follow-up interpretability research.\n- It attempts to link reasoning diversity with data format, a less-explored dimension in CoT studies.\n- Presentation is relatively polished and readable, making the high-level idea easy to follow."}, "weaknesses": {"value": "- The criterion appears to be very important, yet the paper seems to use only a single one. How robust is this criterion to variations from weaker models or the effects of randomness?\n- It seems that a prior for a specific reasoning logic has been incorporated for a certain class of questions, and the performance benefits are evident. I am curious how your framework would be applied when the test set does not include CoT outputs.\n- Figure 6 is blurry and difficult to read.\n- How does the performance of this method compare to instruction optimization techniques such as TextGrad? Additionally, can it lead to the discovery of more effective patterns?\n- The finding that \"data format is more important than domain\" is a valuable insight. However, since these conclusions are drawn primarily from tasks in mathematics and question answering, do these defined \"reasoning strategies\" remain effective for tasks requiring creativity, open-endedness, or emotional understanding, such as poetry generation or legal argumentation?\n- As models continue to evolve, they may develop novel reasoning strategies. This implies that the \"Encyclopedia of CoT\" is not a one-time creation but requires ongoing updates. What are the anticipated costs and frequency of these updates?\n- How are pattern similarity and Q-Q similarity calculated?\n- Missing Reference:\n\n[1] Towards reasoning era: A survey of long chain-of-thought for reasoning large language models\n\n[2] Deconstructing long chain-of-thought: A structured reasoning optimization framework for long cot distillation\n\n[3] When more is less: Understanding chain-of-thought length in llms"}, "questions": {"value": "- Given that criterion is presented as highly important, yet only a single one is used, how robust is it against variations from weaker models or the effects of randomness?\n- Since a prior for a specific reasoning logic appears to be incorporated for certain questions, leading to clear performance gains, how would the proposed framework be applied to a test set that lacks CoT examples?\n- How does the performance of this method compare to instruction optimization techniques like TextGrad, and can it facilitate the discovery of more effective reasoning patterns?\n- How are pattern similarity and Q-Q similarity calculated?\n- The paper's finding that \"data format is more important than domain\" is a valuable insight. However, given that this conclusion is based on math and QA tasks, do the defined \"reasoning strategies\" also apply effectively to tasks that demand creativity, open-endedness, or emotional understanding, such as poetry generation or legal argumentation?\n- As models evolve and potentially develop novel reasoning strategies, the \"Encyclopedia of CoT\" would require continuous updates. What are the anticipated costs and frequency for maintaining and updating this resource?\n- Add reference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HOXqEmsTl5", "forum": "ugZKZ8vufv", "replyto": "ugZKZ8vufv", "signatures": ["ICLR.cc/2026/Conference/Submission14823/Reviewer_u6tC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14823/Reviewer_u6tC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705901121, "cdate": 1761705901121, "tmdate": 1762925172755, "mdate": 1762925172755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoT Encyclopedia, a bottom-up, data-driven framework for analyzing and controlling reasoning strategies in large language models (LLMs) performing long Chain-of-Thought (CoT) reasoning. Unlike top-down approaches that rely on predefined strategy types, CoT Encyclopedia automatically extracts reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters similar dimensions, and builds contrastive rubrics (e.g., top-down vs. bottom-up). It then classifies each CoT response under these rubrics and generates interpretable reasoning reports. ​Experiments across multiple benchmarks (GPQA-Diamond, MMLU-Redux, MATH-500, XSTest, WildGuard, Arena-Hard) show that this framework captures finer-grained reasoning differences across models and tasks, achieving higher interpretability (86% human preference) and consistent performance gains.​ The authors further demonstrate that optimal reasoning strategies can improve both model helpfulness and harmlessness, that question-specific reasoning can be predicted, and that data format (multiple-choice vs. free-form) shapes reasoning behavior more strongly than domain.​ Overall, CoT Encyclopedia provides a scalable taxonomy and control mechanism for reasoning behaviors, contributing to greater interpretability, adaptability, and safety in LLM reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "# 1. Originality and Conceptual Contribution\n\nThe work shifts the paradigm from top-down, predefined reasoning taxonomies to a bottom-up, data-driven discovery of reasoning strategies. This formulation is original and theoretically meaningful: it operationalizes reasoning diversity without relying on human-crafted categories, enabling emergent taxonomies directly grounded in model behavior. The introduction of contrastive rubrics (e.g., “bottom-up vs. top-down,” “inductive vs. deductive”) represents an elegant mechanism for interpretable reasoning dimensions—conceptually parallel to semantic factor disentanglement. The finding that data format (MC vs. FF) shapes reasoning more strongly than domain is novel, empirically grounded, and relevant to both cognitive modeling and training data curation.\n\n# 2. Technical Quality and Empirical Breadth\n\nThe framework is well-engineered: it integrates LLM-assisted criteria generation, embedding-based clustering, and interpretable classification. The experimental coverage is impressive—spanning six benchmarks across both helpfulness (GPQA-Diamond, MMLU-Redux, MATH-500) and harmlessness (XSTest, WildGuard, Arena-Hard). The authors perform rigorous human evaluations and quantitative analyses, reporting consistent improvements in interpretability and measurable performance gains.\nThe inclusion of ablation and robustness studies (e.g., embedding choices, random seeds, model scales) shows commendable attention to reproducibility and methodological soundness."}, "weaknesses": {"value": "# 1. Lack of analysis on classifier choice and sensitivity\n\n​The framework relies on a single LLM (GPT-4o) to perform all classification tasks in the taxonomy pipeline—deciding whether each reasoning trace aligns with one side of a contrastive rubric. Although Appendix B.1 examines benchmark-induced differences (showing that GPQA, MMLU, and MATH benchmarks produce similar criteria while Arena-Hard yields a distinct “User Understanding” dimension), this analysis only reflects task-level variability, not classifier-level robustness. The paper never investigates whether different classifier models (e.g., Claude, Gemini, DeepSeek) or prompting styles would yield consistent categorizations. Consequently, the stability and objectivity of the classification stage remain untested, and taxonomy boundaries may shift under alternate LLMs or small prompt perturbations.\n\n# ​2. Limited human validation of interpretability​\n\nHuman evaluation (250 samples, 10 annotators) focuses on plausibility rather than interpretive alignment or consistency, and no inter-annotator reliability metrics are reported.​\n\n# 3.Benchmark scope limited to English text\n\n​All benchmarks are English and text-based; multimodal and multilingual reasoning remain unexplored."}, "questions": {"value": "# 1. On classifier dependence and taxonomy stability\n\nYour entire classification pipeline—strategy identification, rubric generation, and labeling—relies on GPT-4o. Have you tested whether the same reasoning taxonomy holds when different LLMs (e.g., Claude, Gemini, DeepSeek) are used as classifiers or rubric generators? If not, how do you ensure that the taxonomy reflects general reasoning properties rather than GPT-4o-specific biases?\n\n# 2. On the granularity of reasoning strategy clustering\n\nYour current clustering framework analyzes reasoning strategies at the full CoT–response level, producing high-level dichotomies such as “Inductive vs. Deductive” or “Top-Down vs. Bottom-Up.” However, reasoning trajectories are often compositional: individual reasoning steps may follow distinct micro-strategies that combine to form an overall reasoning pattern. \n\nHave you considered extending the CoT Encyclopedia to identify atomic reasoning step categories—that is, classifying each reasoning step rather than the entire chain—and investigating whether global reasoning strategies emerge as structured combinations of such atomic units? This could reveal a more mechanistic understanding of how complex reasoning behaviors are composed.\n\n# 3. On the granularity and interpretability of clustered criteria\nYou report six high-level reasoning dimensions (Table 5). How were the fine-grained criteria merged into these six? Was k = 6 chosen purely based on silhouette scores or adjusted for interpretability by human judgment?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper conducts a human evaluation with 10 annotators using the Argilla platform but does not specify key ethical details such as recruitment procedures, compensation, informed consent, or IRB approval. Although annotation guidelines and examples are mentioned, the lack of transparency about participants’ treatment and consent constitutes a responsible research practice concern.\n\nAn ethics review is recommended to verify:\n\n- Whether annotators were fairly compensated and gave informed consent;\n\n- Whether the human evaluation adhered to ethical research standards"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3KXus6SxYg", "forum": "ugZKZ8vufv", "replyto": "ugZKZ8vufv", "signatures": ["ICLR.cc/2026/Conference/Submission14823/Reviewer_FM7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14823/Reviewer_FM7J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927277653, "cdate": 1761927277653, "tmdate": 1762925172470, "mdate": 1762925172470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a bottom-up framework to analyze, predict, and steer long chain-of-thought (CoT) reasoning. The pipeline: (i) have an LLM describe the strategies it used, (ii) embed and cluster these criteria, (iii) derive contrastive rubrics (e.g., top-down vs bottom-up), and (iv) classify new CoTs to produce interpretable 'reasoning profiles'. Using the learned rubrics, the authors predict question-specific optimal strategies and inject pattern-based instructions, improving accuracy and safety across multiple benchmarks. They also show that training format (MC vs free-form) shapes reasoning more than domain, and that weight interpolation between MC and FF-trained models smoothly shifts reasoning style. Overall the paper is thorough and the contribution is well articulated. The main limitation of the paper is the lack of a more substantial grounding on the coverage of the underlying categories of reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear narrative. \n\n- Clear empirical insights. Demonstrates that format/domain in shaping reasoning patterns, and that merging model weights interpolates strategies. These provide useful guidance for dataset and model design.\n\n- Sensible empirical analysis. Includes ablations on taxonomy construction (embedding, clustering, … ), human evals of report quality, and analyses of stability across families/sizes."}, "weaknesses": {"value": "- Lack of a rigorous scoping for the problem of reasoning. \n\n- Lack of an argument for the construction of methods which can deliver a comprehensive feature set which describes the CoT reasoning phenomena."}, "questions": {"value": "- The title of the approach (CoT encyclopaedia), points the readers in the direction of an approach which is comprehensive and systematic. Yet, it is unclear how the prompts you use to deliver that induction of a set of CoT characteristics are defined. Could you provide additional details and defend on why these prompts have the required properties to deliver the task? These needs to be mechanism/construction-based (not referring to the empirical analysis).\n\n- The term ‘reasoning’ tends to be underspecified and used as whatever the tasks implement. Can you provide a description of what your task corpus is expressing wrt to reasoning and scope your claims accordingly. \n\n- Wrt validity, how well do rubric-based strategy labels align with expert human coders (definitions, guidelines, IAA)? What are the most common disagreement modes?\n\n- Do results hold when swapping the LLM judge, embedding model, and moderation model? (cross-judge and cross-embed sensitivity analyses)\n\n- How stable are criteria under different k, linkage metrics, and seeds?\n\n- When pattern-based instructions help, how much is due to true strategy change vs prompt priming or longer outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6O1QaWbIaI", "forum": "ugZKZ8vufv", "replyto": "ugZKZ8vufv", "signatures": ["ICLR.cc/2026/Conference/Submission14823/Reviewer_47H3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14823/Reviewer_47H3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160142339, "cdate": 1762160142339, "tmdate": 1762925172032, "mdate": 1762925172032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}