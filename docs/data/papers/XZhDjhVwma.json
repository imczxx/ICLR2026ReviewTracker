{"id": "XZhDjhVwma", "number": 23176, "cdate": 1758340571882, "mdate": 1759896828585, "content": {"title": "Exclusive Unlearning: Forgetting All Except What You Need", "abstract": "Large language models (LLMs) acquire diverse knowledge and abilities through pretraining, but broad and uncontrolled capabilities are not always desirable. \nConventional unlearning aims to erase specific knowledge while preserving general fluency and reasoning. \nHowever, when the target task is clearly defined, we can remove all abilities that are not required to perform that task, rather than attempting to control a wide range of undesirable behaviors. \nFor example, customer-care chatbots should only answer anticipated questions, and in education, a subject-specific model may be preferable to prevent unintended use.\n\nIn this paper, we take the opposite perspective from conventional unlearning and propose a method that preserves only the ability specified by a dataset while forgetting all other knowledge and abilities. \nOur approach is remarkably simple:  we train the model on the target task via standard fine-tuning while simultaneously forcing the probability distribution over the model's generated texts to become uniform. \nThis ensures that the model retains the capability required for the target task, while forgetting all other abilities.\n\nWe demonstrate that our method successfully retains specific abilities (extractive QA and mathematical QA) while forgetting all other knowledge and abilities. Furthermore, we show that our method more effectively removes all abilities except the designated one compared to a standard unlearning approach.", "tldr": "Unlike conventional unlearning, we preserve only the dataset-specified ability and forget all others.", "keywords": ["knowledge unlearning", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1e7759a42f44e88cd87f0668ba66f291121404a.pdf", "supplementary_material": "/attachment/332d9ac545e76950eb3730a3ebfe6c155ae356a9.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Exclusive Unlearning, a novel perspective on model unlearning for large language models (LLMs). Unlike conventional unlearning, which aims to remove specific knowledge while maintaining overall fluency and reasoning ability, this work proposes to retain only the capabilities specified by a target dataset and forget everything else. The proposed method jointly applies a standard fine-tuning objective on the retain dataset (e.g., SQuAD, GSM8K) and a “uniform-loss” objective that drives the output token distribution toward uniformity, which is claimed to erase all other knowledge and abilities. Experiments on various models (OLMo, Pythia, GPT2-XL, GPT-J) show that their method can preserve the target task ability while reducing general knowledge (MMLU) performance to chance level."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides a clear conceptual contrast to traditional unlearning — instead of “forgetting a part,” it attempts to “forget everything except one capability,” which is an interesting inversion of perspective.\n\nThe proposed approach is technically simple (a combination of standard fine-tuning and uniform-distribution regularization) and easy to implement.\n\nThe experimental setup is well-structured, covering multiple model families and tasks (extractive QA and math QA).\n\nThe loss-based evaluation introduces a theoretically interpretable measure (approaching ln|V|) for quantifying the degree of forgetting, which is a neat diagnostic tool."}, "weaknesses": {"value": "**Limited empirical improvement**\nThe claimed benefit of the proposed method is not convincingly demonstrated. On most benchmarks, performance on the retain tasks (SQuAD, GSM8K, MathQA) is similar to or only slightly better than standard supervised fine-tuning, while the model loses almost all other general capabilities. The reported gains (e.g., +1–2%) are marginal and could be within the noise level.\n\n**Severe loss of generalization and transferability**\nBy design, the method intentionally erases general knowledge. However, in many real applications — especially those requiring lifelong learning, domain adaptation, or multi-skill reasoning — prior general knowledge is often essential for solving domain-specific tasks. Erasing all other abilities seems counterproductive and undermines the benefit of large-scale pretraining.\n\n**Questionable motivation and practicality**\nThe motivation for forgetting “everything else” is not fully justified. In most deployment scenarios (e.g., chatbots, education, vertical assistants), controlling responses via task prompts, instruction-tuning, or safety filters is far more efficient and reversible than permanently unlearning. The proposed approach sacrifices flexibility for very narrow gains.\n\n**Lack of qualitative or downstream analysis**\nThe paper does not explore what exactly is being forgotten, nor whether the remaining ability maintains robustness or compositional reasoning. This makes it unclear whether “exclusive unlearning” leads to a stable or brittle model."}, "questions": {"value": "How does the proposed approach behave in lifelong learning or multi-task scenarios?\nIf we later wish to add another ability (e.g., after forgetting everything else), can the model be retrained efficiently, or must it be fully retrained from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8fqcjxdcaF", "forum": "XZhDjhVwma", "replyto": "XZhDjhVwma", "signatures": ["ICLR.cc/2026/Conference/Submission23176/Reviewer_RGD1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23176/Reviewer_RGD1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820754318, "cdate": 1761820754318, "tmdate": 1762942545823, "mdate": 1762942545823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new unlearning paradigm called exclusive unlearning, where the goal is to retain the model’s ability on a specific target task while removing general or unrelated knowledge. To achieve this, the authors propose a method that minimizes a loss combining the target-task objective with an additional loss designed to unlearn the model’s general capabilities. Empirical evaluations show that the proposed approach successfully maintains strong performance on the target task while substantially reducing accuracy on MMLU, indicating effective removal of general knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed setting of unlearning, exclusive unlearning, is novel.\n2. The writing is structured well and overall it is easy to follow."}, "weaknesses": {"value": "1. Motivation of exclusive unlearning is unclear. While the goal of retaining only the target task capability is understandable, it is not evident why removing unrelated abilities—such as those measured by MMLU—would be beneficial. In most practical scenarios, preserving general capabilities seems harmless. A stronger justification for why “exclusive unlearning” is desirable in real-world applications would make the contribution more compelling.\n2. If the paper would like to justify all other abilities can be unlearned, then the set-up of empirical evaluation is insufficient -- only MMLU is evaluated. There are many other benchmarks evaluating model's general ability (PIQA, RACE) as well as the fact datasets such as Wiki that can facilitate to measure how much factual knowledge is in the LLM.\n3. Another baseline can be learning the target dataset from randomly initialized models rather than the pre-trained models."}, "questions": {"value": "Please check the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4RQVNzUPHA", "forum": "XZhDjhVwma", "replyto": "XZhDjhVwma", "signatures": ["ICLR.cc/2026/Conference/Submission23176/Reviewer_GZAH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23176/Reviewer_GZAH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988751370, "cdate": 1761988751370, "tmdate": 1762942544547, "mdate": 1762942544547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a new method for selectively unlearning in LLMs. Unlike prior work which focuses on erasing specific knowledge while preserving other broader abilities, the authors propose exclusive unlearning, i.e., preserving only a designated ability while forgetting all other knowledge and capabilities. The method fine-tunes the model on a target task (retain set) while simultaneously applying a “uniform-loss” objective on self-generated text to drive the model’s output distribution toward uniformity (forget set). They evaluate this on extractive QA (SQuAD) and mathematical QA (GSM8K, MathQA) across several model sizes, showing the model retains the target task performance while performance on a broad benchmark (MMLU) drops to near chance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The inversion of standard unlearning (i.e., keeping everything but forgetting specific knowledge) into only keeping the specific knowledge and forgetting everything else is a novel conceptual shift.\n2. The proposed method using uniform-loss objective is elegant and theoretically sound.\n3. Strong empirical results across model scales and tasks. The author showed the effectiveness of the proposed method on multiple model families and on two different target tasks (extractive QA and math QA) with convincing drops on the broad task benchmark MMLU."}, "weaknesses": {"value": "1. While the proposed method on using Uni CE showed advantages over Neg CE in the experiments, limited insight was revealed and discussed on why Uni CE works better.\n2. While the method fits well for constrained tasks, the trade-offs are under-explored, e.g. what gets lost (e.g. will the language capability also lose in addition to the general knowledge), and what might inadvertently remain.\n3. The retain tasks are both QA (extractive and math). It’s unclear how the method generalizes to other modalities (generation, summarization, dialogue) or less structured tasks."}, "questions": {"value": "1. What makes Uni CE works so much better than Neg CE in this unlearning setting?\n2. How sensitive is the method to the quality/diversity of the self-generated “forget” dataset, what happens if key domains are under-sampled?\n3. Figure 2: Why a large lambda (weight for the retain loss) lead to a drop in performance of the retain task and improvement on forgetting task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zqlByyyUpF", "forum": "XZhDjhVwma", "replyto": "XZhDjhVwma", "signatures": ["ICLR.cc/2026/Conference/Submission23176/Reviewer_55Xo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23176/Reviewer_55Xo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124098073, "cdate": 1762124098073, "tmdate": 1762942543757, "mdate": 1762942543757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unlearning method that aims to maintain only necessary knowledge and forget everything else. It achieves this by generating a forget dataset consisting of texts generated by the model itself and pushes the model’s output probability to be as uniform as possible. Meanwhile, it assumes a target task dataset exists to retain information necessary for the task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is very straightforward.\n\nThe writing is easy to follow.\n\nThe motivation of preventing misuse is generally valid."}, "weaknesses": {"value": "The technical approach proposed is not very novel, and too naive to be meaningful. The evaluations are too weak to support the usefulness of the approach.\n\n1. The proposed method that tries to uniformalize the output distribution is not novel, as it has been proven to work for other unlearning and/or debiasing tasks. However, directly using this technique in the vocabulary space is potentially catastrophic, because it not only erases knowledge, but also the general ability of articulation the model obtains through trains.\n\n2. How much the model forgets depends on the size of the forget dataset. If the dataset is too small, the information it forgets is too little; if the dataset is too large, then the model loses the general ability to talk. The paper does not discuss a principled approach to find the correct size of the forget dataset.\n\n3. There is a risk of catastrophically breaking the talking ability of the model and the method does not provide any measure to prevent this from happening. The model only retains the ability to speak on the target dataset. So what would happen if the user inputs any text outside the distribution of the retain/target dataset? - assuming the forget dataset is sufficiently large, the model would essentially output random tokens, instead of rationally rejecting the question, which significantly reduces the usefulness of the model.\n\n4. The paper provides no evidence using a forget dataset and a retain dataset is better than simply training the LLM only on the retain dataset from scratch - what is the benefit of using a pretrained LLM at all if the goal is to literally forget everything not in the retain dataset? If the objective proposed by this paper is to make any output distribution other than \n\n5. The paper provides no evaluations on data outside of the forget dataset and the retain dataset. It is possible the model still remembers the information as long as prompted in a different way from the ones in the forget dataset. It is also possible it becomes very easy to make it output complete random tokens."}, "questions": {"value": "Please see Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eEd4UcDoXT", "forum": "XZhDjhVwma", "replyto": "XZhDjhVwma", "signatures": ["ICLR.cc/2026/Conference/Submission23176/Reviewer_Qsxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23176/Reviewer_Qsxo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762598618288, "cdate": 1762598618288, "tmdate": 1762942543521, "mdate": 1762942543521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}