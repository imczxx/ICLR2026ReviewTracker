{"id": "GTpf2NuwtR", "number": 669, "cdate": 1756765188967, "mdate": 1759898248100, "content": {"title": "3D Aware Region Prompted Vision Language Model", "abstract": "We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements. We show more qualitative results anonymously at https://sr3d-iclr.github.io/.", "tldr": "", "keywords": ["Vision Language Models", "Spatial Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/675b9dd5e8a0f218162a3beea79f823c1a41ec79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a method to unify single-view and multi-view tasks within a single vision-language model. To achieve this, the authors propose a novel **dynamic tiling-based region extractor** that produces generalizable embeddings across single- and multi-view settings. With the proposed approach, the **SR-3D** model achieves state-of-the-art (SOTA) performance on general multimodal and 3D understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of extracting canonical positional features shared across single- and multi-view inputs is simple yet effective.\n2. The proposed method demonstrates strong generalization from 2D multimodal to 3D multimodal scenarios. Its impressive zero-shot performance on VSI-Bench highlights this capability.\n3. The method leverages diverse 3D representations and can generalize to pseudo point maps derived from 3D foundation models, showing its ability to integrate SOTA VLMs with 3D foundation models."}, "weaknesses": {"value": "1. For multi-view inputs, the SR-3D model requires 32 frames, whereas other 3D-input VLMs such as LEO use only around 60 object-centric tokens. This substantially increases sequence length and may affect training and inference efficiency, which should be discussed.\n2. Related works such as **LLaVA-3D** also evaluate video understanding on benchmarks like **MVBench [1]** and **VideoMME [2]**. Although the proposed method exhibits strong spatial understanding in multi-view settings, it remains unclear whether the enhanced 3D understanding compromises its general video understanding ability.\n3. The 3D evaluation is not comprehensive enough. It seems that the 2D and 3D evaluations were conducted using different models. The proposed **SR-3D-Bench** focuses primarily on region-level spatial scene understanding, and the baselines are limited to video-based VLMs. Recent benchmarks such as **MMScan [3]** offer more comprehensive 3D evaluations from object- to room-level, and results on them are recommended for inclusion.\n\n[1] [[2311.17005\\] MVBench: A Comprehensive Multi-modal Video Understanding Benchmark](https://arxiv.org/abs/2311.17005)\n\n[2] [[2405.21075\\] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/abs/2405.21075)\n\n[3] [[2406.09401\\] MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations](https://arxiv.org/abs/2406.09401)"}, "questions": {"value": "1. **Use of masks or bounding boxes during evaluation:** In real-world scenarios, obtaining ground-truth object masks or bounding boxes is challenging. Does the method rely on GT instance masks or boxes during evaluation, or are these extracted using 3D foundation models such as **Mask3D [4]**? Additionally, how sensitive are the results to mask quality?\n2. **Integration of Paligemma (L1126):** How is **Paligemma** used as the visual backbone, given that it is a VLM? Furthermore, why was **Qwen-2-7B** selected as the LLM backbone? Would more recent models such as **Qwen-2.5-7B** yield better results?\n3. **Checkpoint consistency:** Are the checkpoints used for 2D and 3D evaluations identical? If not, does the 3D model still retain its 2D understanding ability?\n4. **Base model selection:** Why was **NVILA-Lite-8B** chosen as the base model? Does the 2D model use similar pretraining data or a comparable data recipe?\n\n[4] [[2210.03105\\] Mask3D: Mask Transformer for 3D Semantic Instance Segmentation](https://arxiv.org/abs/2210.03105)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Weos38aqQH", "forum": "GTpf2NuwtR", "replyto": "GTpf2NuwtR", "signatures": ["ICLR.cc/2026/Conference/Submission669/Reviewer_TQXC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission669/Reviewer_TQXC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541653689, "cdate": 1761541653689, "tmdate": 1762915578227, "mdate": 1762915578227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a 3D multi-view spatial reasoning framework based on 2D VLMs with region-level prompts. The core insight is that spatial understanding fundamentally relies on cross-frame consistency of the same object. To this end, the authors introduce a unified 3D positional encoding to enrich the visual features of powerful 2D VLMs, thereby leveraging their strong priors while mitigating the scarcity of 3D data. In addition, they adopt a tile-and-stitch strategy for both visual features and masks to ensure fine-grained region-level feature extraction. Experiments demonstrate that the proposed method achieves notable improvements on various spatial reasoning and scene understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear. The paper identifies that the key to spatial understanding and reasoning lies in maintaining cross-frame consistency for the same object, and accordingly designs a 3D positional encoding module, complemented by a tile-and-stitch strategy.\n2. The training pipeline is well-designed, effectively preserving the strong priors of the underlying VLM.\n3. The framework can operate with estimated depth in the absence of ground-truth depth and supports flexible region prompting, showing promising practical potential.\n4. Extensive experiments are conducted, and the figures and visualizations are well-presented and visually appealing."}, "weaknesses": {"value": "1. The idea of 3D positional encoding has been widely explored in prior 3D modeling and vision literature, and the proposed design does not appear to introduce substantial contribution compared to existing approaches.\n2. As shown in Table 8 (ablation study), the 3D positional encoding contributes only marginal improvement, while most performance gains come from single-view pretraining. However, the paper’s main claim centers on the role of 3D positional encoding in spatial understanding and reasoning.\n3. While the experimental results are solid overall, the core idea lacks strong conceptual contribution, and the experiments do not convincingly demonstrate the effectiveness of 3D positional encoding in enhancing spatial understanding and reasoning."}, "questions": {"value": "1. The zero-shot results in Table 7 and the visualizations in Table 5 suggest that SR-3D exhibits strong sensitivity to 3D distances. Is this metric-scale capability primarily attributable to the 3D positional encoding design? Which specific components contribute to it? Please provide theoretical justification and supporting experiments.\n2. In the ablation study (Table 8), the gains from 3D positional encoding are limited, while most improvements come from single-view pretraining. Since 3D PE is a key claimed contribution, please explain this discrepancy.\n3. The tile-and-stitch component lacks detailed ablation studies. Please include analyses isolating its effect.\n4. When depth ground truth is unavailable during training and only estimated depth is used, how much does performance degrade? In particular, could depth bias be amplified after transforming to the unified 3D coordinate system for PE, thereby harming cross-frame alignment and metric measurement?\n5. Beyond 3D PE, have you explored alternative 3D representations (e.g., explicit depth features, VGGT or Depth Anything features)?\n6. I’m open to revising my score after the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rn1HL2hDbL", "forum": "GTpf2NuwtR", "replyto": "GTpf2NuwtR", "signatures": ["ICLR.cc/2026/Conference/Submission669/Reviewer_2EZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission669/Reviewer_2EZD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744362266, "cdate": 1761744362266, "tmdate": 1762915578082, "mdate": 1762915578082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SR-3D, a 3D-aware VLM that unifies 2D and 3D spatial understanding through a shared visual token space. It bridges single-view (2D) and multi-view (3D) representations by enriching 2D features with 3D positional embeddings derived from depth maps, aiming to enable spatial reasoning across frames even when objects do not co-occur.\nThe model is trained first on large-scale 2D datasets for strong visual-language priors, then fine-tuned on multi-view 3D datasets (e.g., ScanQA, SQA3D, Scan2Cap). The authors also evaluate the performance on 3D spatial reasoning, region-level understanding, and video-based spatial question answering benchmarks such as BLINKDepth, ScanQA, and VSI-Bench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-designed figures: The figures are clear, well-organized, and effectively illustrate both the overall pipeline and qualitative examples.\n2. Comprehensive evaluation: The paper presents extensive experiments across 2D, 3D, and video-based spatial reasoning benchmarks.\n3. Practical and flexible design: The proposed SR-3D framework supports flexible region prompting (e.g., bounding boxes, masks, or 3D annotations)"}, "weaknesses": {"value": "1. The claim in the contribution \"We introduce SR-3D, the first 3D-aware vision-language model that unifies representations for both single-view and multi-view tasks.\" might be a bit overstated, while not exactly the same but existing works have already explore similar idea such as LLaVA-3D, 3D-LLM etc\n\n2. The authors emphasize the unified representation between single- and multi-view images. While this idea is conceptually sound, it may be difficult to directly demonstrate that the model indeed develops such a capability, or that the improvements reported in the tables stem from this specific design. It would be valuable if the authors could provide additional evidence or analysis to better support this claim."}, "questions": {"value": "Please address the weakness mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WCgvYW80xy", "forum": "GTpf2NuwtR", "replyto": "GTpf2NuwtR", "signatures": ["ICLR.cc/2026/Conference/Submission669/Reviewer_jRLg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission669/Reviewer_jRLg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919526276, "cdate": 1761919526276, "tmdate": 1762915577948, "mdate": 1762915577948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SR-3D, a vision-language model designed to unify spatial reasoning for both single-view 2D images and multi-view 3D data. The core mechanism involves enriching 2D visual features with 3D positional embeddings. These embeddings are derived from estimated (for 2D) or ground-truth (for 3D) depth and canonicalized into a shared 3D coordinate system, allowing a base 2D VLM to leverage its priors for complex 3D tasks. The model also introduces a \"dynamic tiling-based region extractor\" to efficiently process high-resolution region prompts. The authors demonstrate state-of-the-art performance on several 3D scene understanding and spatial reasoning benchmarks. They also introduce a new benchmark, SR-3D-Bench, to evaluate region-level spatial question answering in 3D environments."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The model's architecture effectively unifies single-view and multi-view spatial understanding. By mapping both to a canonical 3D positional space, the model successfully leverages powerful 2D pre-trained priors for 3D reasoning tasks.\n2. The paper proposes a dynamic tiling-based region extractor. This seems to be an effective solution for handling high-resolution inputs and flexible region prompts (boxes, masks) without the distortion common in other methods, which is a practical and important problem.\n3. The empirical evaluation is thorough and very strong. The authors achieve state-of-the-art results across multiple challenging benchmarks (ScanQA, SQA3D, VSI-Bench), outperforming numerous existing 2D and 3D models.\n4. The introduction of the SR-3D-Bench benchmark is a good contribution. It fills a clear gap by providing a testbed for region-level spatial reasoning, which is more complex and ambiguous than global-level QA."}, "weaknesses": {"value": "1. The model's capability is currently restricted to static scenes. The authors acknowledge this in the limitations, but it is a significant point, as many target applications for 3D spatial reasoning (like robotics or AR/VR) involve dynamic environments with moving objects. The proposed positional encoding scheme does not inherently account for temporal dynamics.\n2. The ablation study in Table 8, while justifying the full model, also shows that the 3D positional embeddings (\"3D PE\") alone (without 2D pre-training) offer limited gains. The authors' conclusion that this is due to scale and that \"larger-scale settings\" would better exploit them is speculative. A deeper analysis of why the 3D PE isn't more impactful on its own would strengthen the paper.\n3. The paper notes a slight but consistent performance degradation on OCR-related tasks. This suggests that the 3D-aware pre-training stage, while boosting spatial skills, may mildly compromise some of the base VLM's other established capabilities."}, "questions": {"value": "1. The authors mention that a single, unified checkpoint for both 2D and 3D tasks is left for future work. What are the primary difficulties in co-training the model on both the 2D pre-training and 3D fine-tuning datasets? Does it introduce optimization challenges or data balancing issues?\n2. What is the computational and memory overhead of the dynamic tiling-based region extractor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vWBq4pbVnA", "forum": "GTpf2NuwtR", "replyto": "GTpf2NuwtR", "signatures": ["ICLR.cc/2026/Conference/Submission669/Reviewer_VFqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission669/Reviewer_VFqG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971483457, "cdate": 1761971483457, "tmdate": 1762915577796, "mdate": 1762915577796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}