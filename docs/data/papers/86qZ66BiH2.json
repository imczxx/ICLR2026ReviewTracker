{"id": "86qZ66BiH2", "number": 6179, "cdate": 1757956819263, "mdate": 1759897931546, "content": {"title": "Learning to Compose Degradations: A Codebook of Primitives for All-in-One Image Restoration", "abstract": "All-in-one image restoration aims to develop a single model for diverse degradations, a challenge whose success critically hinges on the precise representation of the underlying degradation process. Existing methods simplify this challenge by mapping each degradation to a coarse-grained, monolithic representation---effectively treating them as discrete categories (e.g., \"haze,\" \"noise\"). This paradigm, even in prompt-learning variants, fundamentally fails to capture the continuous and fine-grained nature of real-world corruptions, such as varying intensities, leading to suboptimal performance. \nTo address this, we argue that degradations are better represented as a composition of a finite set of learnable, elementary degradation primitives. We introduce DACode, a novel framework built upon a global, learnable codebook embodying these primitives. The core of DACode is a two-stage, dual cross-attention mechanism. First, in the Context-Aware Code Adaptation stage, the codebook primitives act as queries to attend to the input image features, generating a contextually-adapted codebook. Subsequently, in the Code-based Feature Modulation stage, the image features query this adapted codebook, aggregating relevant primitive information to perform targeted feature restoration. This dynamic process allows DACode to construct highly specific restorative features for each input. Notably, our analysis reveals that DACode learns to activate distinct code combinations in response to both varying degradation types (e.g., haze vs. rain) and severities (e.g., light vs. heavy haze), providing direct evidence for its fine-grained modeling capability and interpretability. Extensive experiments show that DACode significantly outperforms state-of-the-art methods across all-in-one restoration benchmarks.", "tldr": "", "keywords": ["All-in-One Image Restoration", "Codebook"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a44ba49008942897e9ea13f2604dadf6a59ba24b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of all-in-one image restoration, arguing that existing categorical or prompt-based methods (e.g., treating degradations as discrete “haze” or “rain” types) fail to capture the continuous and compositional nature of real-world corruptions. It proposes DACode, a framework that represents degradations as mixtures of learnable primitives within a global codebook, with a dual cross-attention mechanism that adapts these primitives to input features and modulates restoration accordingly. The method reports state-of-the-art results on standard 3-/5-task and composite (CDD11) benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method replaces fixed task labels with a learnable codebook of primitives, allowing adaptive blending across degradation types and severities without separate prompts or heads.\n2. A two-step interaction first adapts the codebook to the input and then uses it to guide restoration, promoting context-aware specialization.\n3. The method shows solid results across multiple benchmarks."}, "weaknesses": {"value": "1. The idea and motivation are not particularly novel. Using a codebook to represent and distinguish degradations has been explored in prior work, such as “Neural Degradation Representation Learning for All-In-One Image Restoration,” which is also an open-source method. As a result, the contribution of this paper appears to be incremental rather than fundamentally innovative. \n\n2. The paper primarily presents average activation differences (e.g., Fig. 4), but the observed gaps are small and lack variance reporting, significance testing, or basic discriminative analyses. Without such probes or robustness checks, it remains unclear whether the learned primitives genuinely distinguish degradation types and severities or merely capture noisy correlations.\n\n3. DACode is inserted only in the decoder, yet the encoder also shapes how degradations are captured and compressed. There’s no systematic study of alternatives (encoder-only, encoder+decoder, different layers/counts), so we don’t know if the current placement is optimal."}, "questions": {"value": "1. In Fig. 1, it is difficult to visually distinguish the difference between the two images labeled [PSNR: 27.3 dB] and [PSNR: 35.6 dB]; they appear almost identical, as if one were a copy of the other."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NRLHAmWBNA", "forum": "86qZ66BiH2", "replyto": "86qZ66BiH2", "signatures": ["ICLR.cc/2026/Conference/Submission6179/Reviewer_khZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6179/Reviewer_khZo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186932113, "cdate": 1761186932113, "tmdate": 1762918519621, "mdate": 1762918519621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DACode, a new framework for all-in-one image restoration. It challenges the conventional approach of representing degradations as discrete categories and instead posits that degradations are compositional, arising from a finite set of learnable \"primitives\" stored in a global codebook. The core mechanism is a two-stage, dual cross-attention process where the codebook primitives first query the input image features to become context-adapted , and subsequently, the image features query this adapted codebook to perform feature modulation and restoration. The method achieves strong state-of-the-art results on several benchmarks, including those for composite degradations"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The conceptual shift towards a compositional, fine-grained degradation model is well-motivated and intuitive , and the model achieves impressive state-of-the-art empirical performance, particularly on complex composite degradation benchmarks."}, "weaknesses": {"value": "My primary concern is the methodological novelty. The core technical contribution, the two-stage dual cross-attention module, appears nearly identical to the mechanism in \"Prompt-In-Prompt Learning for Universal Image Restoration\" (PIP) and shares significant overlap with \"Boosting All-in-One Image Restoration via Self-Improved Privilege Learning.\" Specifically, the \"Context-Aware Code Adaptation\" stage  (where codebook queries image features) and the \"Code-based Feature Modulation\" stage  (where image features query the adapted codebook) directly mirror the prompt-in-prompt design. The paper frames this as a 'codebook' of 'primitives,' but the implementation is essentially a re-branding of an existing prompt-learning architecture. This significantly diminishes the paper's contribution, and the authors must explicitly discuss and differentiate their work from these prior arts. \n\nSecondly, the paper does not adequately address the computational and training efficiency. Inserting this dual-attention module into each decoder stage adds non-trivial overhead, but the analysis is limited to parameter counts  without FLOPs or latency comparisons. \n\nFinally, the paper's core claim of a fine-grained, compositional model that should generalize better is not fully substantiated. The evaluation is limited to synthetic benchmarks . There is no validation on real-world, in-the-wild degradation datasets. It is unclear if these 64 'primitives'  have learned fundamental degradation properties or have simply overfit to the specific synthetic degradation types in the training mix. A cross-domain or zero-shot experiment on a completely unseen, real-world dataset is necessary to validate the ambitious claims of learning a \"rich, internal language of degradation\"."}, "questions": {"value": "Can the authors precisely articulate the architectural differences between the proposed DACode module  and the core mechanism in PIP? If the architectures are indeed identical, the justification for this work's methodological contribution must be clarified. \n\nFurthermore, given the visualizations in Figures 4 and 5 , what happens when the model is presented with a completely unseen degradation type (e.g., JPEG compression artifacts) or a novel combination of tasks not seen in the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EcSxMVPjVQ", "forum": "86qZ66BiH2", "replyto": "86qZ66BiH2", "signatures": ["ICLR.cc/2026/Conference/Submission6179/Reviewer_A2a8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6179/Reviewer_A2a8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011937151, "cdate": 1762011937151, "tmdate": 1762918519227, "mdate": 1762918519227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new CNN-based method to tackle all-in-one image restoration problem. In this paper, the all-in-one context focuses on bad weather conditions including dehazing, deraining, de-snowing, as well as denoising tasks. The proposes method introduces a code book vector which is a learnable vector. The code book vector is embedded into attention structure twice to categorically encode and split the degradation types. Extensive experiments show that the proposed method outperforms baseline methods on the suggested benchmarking datasets in dehazing, deraining, deblurring, low-light enhancement and denoising tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method outperforms the baseline methods in each of the image enhancement task shown in the quantitative benchmarks."}, "weaknesses": {"value": "1. This paper is not well written and revised. There is even a typo in the title. \"Restoratio -> Restoration\" \n2.  It is not stated clearly that if the evaluated model is simultaneously trained on all the datasets of different task types in Sect 4.2 implementation details. The main all-in-one framework's goal is to conduct training process only once and apply the model for multiple tasks. \n3. For deraining tasks, the main paper and supplementary materials are both showing synthetic rain images. The authors are suggested to demonstrate real-world deraining results to verify the effectiveness of the proposed method. \n4. Figure 3, dehazing task does not show obvious superiority of the proposed method. \n5. One of the most concerning points for the proposed method is about the theoretical relationship between the proposed new codebook and image restoration task types. It is not explicitly described why the codebook vector may modularize the input image features based on the degradation types automatically. \n6. The novelty of the proposed method is quite limited as there are a number of codebook search algorithms for image restoration problems."}, "questions": {"value": "The authors are suggested to re-polish the paper carefully and address the main issue from the weaknesses section during rebuttal period. The  main concern is regarding the theoretical reasoning of the codebook and the degradation types."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WptedNTTeA", "forum": "86qZ66BiH2", "replyto": "86qZ66BiH2", "signatures": ["ICLR.cc/2026/Conference/Submission6179/Reviewer_5a1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6179/Reviewer_5a1R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031081951, "cdate": 1762031081951, "tmdate": 1762918518511, "mdate": 1762918518511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}