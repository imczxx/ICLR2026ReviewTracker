{"id": "2Q0U2rV2Jz", "number": 6516, "cdate": 1757987589734, "mdate": 1763353331385, "content": {"title": "Neural Networks Learn Multi-Index Models Near the Information-Theoretic Limit", "abstract": "In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\\boldsymbol{x})=g(\\boldsymbol{U}\\boldsymbol{x})$ with hidden subspace $\\boldsymbol{U}\\in \\mathbb{R}^{r\\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\\widetilde{\\mathcal{O}}(d)$ samples and $\\widetilde{\\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.", "tldr": "We prove that neural networks trained via GD can efficiently learn Multi-Index models near the information-theoretic limit.", "keywords": ["Representation Learning", "Multi-Index Models", "Two-Layer Network", "Gradient Descent", "Sample Complexity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d78468d8eca7a7abdf994ff7320e01405cba86e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of learning multi-index models with a two layer network using gradient descent. In particular the authors show that first training the first layer for a few step, and then freezing it an only training the second one it's possible to achieve vanishing test error with linearly many samples in the input dimension and quadratically many iterations. The key insight is that with a carefully chosen initialization (first layer with infinitesimally small weights, second layer either +1 or -1) GD is essentially performing a fixed point iteration on the empirical covariance of the data, which means that after $O(log(d))$ iterations the learned features are in the subspace of the multi-index model directions. One can then train the second layer to have good generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is extremely well written and does an incredibly good job at explaining the general idea of the paper and its proof directly in the main text. I believe this work to be a significant contribution to the theoretical understanding of machine learning, extending the literature on multi-index models significantly."}, "weaknesses": {"value": "Both the data model and the training procedure is extremely idealized and ad-hoc, and it's unclear to me how these results would generalize in more realistic settings. Even within the theory landscape, the target multi-index model is just a polynomial, in contrast with what stated in the abstract. In particular training the layers one at a time is not standard practice, and this is made even worse by the need to control the number of iterations when training the first layer. Additionally the paper lacks any experimental validation or numerical check of the results."}, "questions": {"value": "1. I am puzzled by the need to train the first layer for no more than $O(log(d))$ iterations. I find this very surprisingly low, could you comment on this? \n2. More on 1: is that something that is a direct consequence of the layer-wise training? Beyond what you can prove, how much of a role do you expect layer-wise training to play in your results?\n3. In Troiani et al. '25 the authors show that AMP undergoes a saddle to saddle dynamic. Do you expect GD to undergo similar dynamics? Would something like the \"grand staircase\" in chapter 5 of their paper happen for GD?\n4. Would the results change if $g$ would be a generic leap exponent 2 function instead of a polynomial?\n5. I find the total absence of numerical experiments slightly disappointing. I would greatly appreciate having a set of experiments, especially investigating theeffect of changing $T_1$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1F5dHRRvAR", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Reviewer_5Tcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Reviewer_5Tcn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590976025, "cdate": 1761590976025, "tmdate": 1762918878405, "mdate": 1762918878405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate the problem of learning a polynomial Gaussian multi-index model using a two-layer neural network trained via gradient descent. Through a layer-wise training procedure, and under mild assumptions on the loss function and activation, they prove that there exists a suitable choice of hyperparameters for which the network efficiently learns the hidden subspace and achieves a small generalization error, close to the information-theoretic sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel theoretical analysis of the ability of shallow neural networks to learn multi-index models. The assumptions on the loss function and activation are fairly general and their necessity is discussed transparently. The connection between the early-stage training dynamics and power iteration provides insight into the trade-off between training time and full subspace recovery. The bound in Theorem 1 improves upon previous results, showing that, in principle, two-layer neural networks can generalize polynomial multi-index models with $n = \\widetilde{O}(d)$, approaching the information-theoretic limit. The manuscript is well structured, with Section 4 (and B.1) offering clear intuition behind the main steps of the proof."}, "weaknesses": {"value": "1. The analysis strongly depends on specific initialization assumptions and on the particular layer-wise training scheme. The manuscript lacks a discussion of their necessity or the validity of the claims beyond these settings.\n\n2. Several references are missing or misplaced:\n\n- Line 95: [1] and [2] are two other relevant references on learning thresholds of single-index models.\n\n- Line 116: spectral methods for multi-index models (with generative exponent 2) achieving the optimal learning threshold have been studied in [3], concurrent with [Kovacevic et al. (2025)]. Both works should also be cited on line 45, together with [Damian et al. (2025)].\n\n- [Troiani et al. (2025)]: this work shows that many multi-index models of interest can be learned with $\\Theta(d)$ samples; therefore, it should be included in the discussions on lines 110 and 437. It could also be mentioned on line 72.\n\n- [Lee et al. (2024)], [Arnaboldi et al. (2025)]: both works cite each other as concurrent and show equivalent results. Moreover, could the authors clarify the meaning of the statement “the recovered features are learned only in a quite weak sense” (lines 52 and 139)?\n\n3. The paper is purely theoretical and lacks numerical validation. While not a weakness per se, a few illustrative examples would strengthen the claims and improve the overall presentation.\n\n4. The bibliography should be harmonized, and missing journal or conference names should be added when necessary.\n\n\n\n[1] Barbier et al. \"Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models\"\n\n[2] Lu, Li \"Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation\"\n\n[3] Defilippis et al. \" Optimal Spectral Transitions in High-Dimensional Multi-Index Models\""}, "questions": {"value": "1. Following point 1 in Weaknesses, could you offer some insights on which assumptions regarding initialization and the training scheme (e.g., sample splitting, bias reinitialization) could be relaxed, under a more careful analysis, without changing the qualitative results? Which ones do you believe are necessary for the theoretical guarantees to hold?\n\n2. At the end of Sections 1.1 and 4, you mention that choosing $T_1 = o(\\log d)$ is necessary to prevent the weights from collapsing onto the top eigenvector of $\\hat{\\Sigma}_\\ell$. However, Appendix A also states that the power-iteration approximation remains accurate only up to $o(\\log d)$ steps. Could you clarify the relationship between these two statements? In particular:\n - does the breakdown of the power-iteration approximation beyond $o(\\log d)$ still imply that part of the signal is suppressed, or the analysis no longer applies?\n - more generally, could you provide some intuition on what happens to subspace recovery if the first-layer training continues for longer times, when corrections to the power-iteration dynamics become non-negligible?\n\n3. The discussion of Assumption 5 could be expanded. In particular, which polynomial target tasks would violate it for standard choices of the loss function ($\\ell_1$, Huber, squared loss, etc.)? Conversely, could you give examples of reasonably natural losses for which the assumption would be satisfied for most polynomial targets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YkK1KQOa24", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Reviewer_jpVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Reviewer_jpVz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924668718, "cdate": 1761924668718, "tmdate": 1762918878050, "mdate": 1762918878050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies learning multi-index models with two-layer neural nets using gradient descent (GD). The authors prove that for polynomials with leap exponent at most 2, $\\Theta(\\sqrt{\\log d})$ steps of GD on the first layer with $\\tilde{O}(d)$ samples learns the latent representation, which is almost information-theoretically optimal, with a computational complexity of $\\tilde{O}(nd)$ which is also nearly optimal. The main intuition is that with a super-constant number of steps, GD mimics power iteration on a certain matrix and act as a spectral method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The finding that a super-constant number of steps can improve the sample complexity of feature learning under the multi-index model is interesting, and the proof sketch seems intuitive and can be used beyond this work."}, "weaknesses": {"value": "1. In comparison with the recent literature, Assumption 5 is a bit restrictive. It would be interesting if one could show that gradient descent would automatically reduce leap complexity down to at most 2, similar to Lee et al., 2024 for the single-index case. I agree however that it would be technically challenging to achieve such result, and it's perhaps an interesting direction for future research in this area.\n\n2. The Lipschitz assumption on loss (Assumption 3) rules out squared loss, and I'm not sure if it's really necessary.\n\n3. Similar to other papers in this area, the authors have to consider a layer-wise training dynamics. Having an analysis of training both layers at the same time however is highly non-trivial."}, "questions": {"value": "4. The authors mention the possibility of extending their analysis to non-Gaussian inputs. One relevant work here is [1], where the authors prove that GD can learn generic functions with near information-theoretically optimal sample complexity, at the expense of a potentially super-polynomial computational complexity. One interesting aspect of such result is that it can also accommodate non-isotropy in the input. In fact, I'm curious if a sample complexity better than $\\tilde{O}(d)$ could be achieved if we assume a structured covariance matrix for the input that reveals some information about the target direction.\n\n5. Wouldn't you need some assumption on the loss that enforces the prediction and ground-truth label to be close (e.g. by enforcing that the loss grows as $t$ and $y$ diverge)? Currently the loss seems too general, but for the network to learn the correct representation (which is how the proof works) I suspect that such an assumption might be necessary?\n\n6. I might've missed this but I couldn't find the assumption $r=\\mathcal{O}(1)$ written in the paper. I think one either needs this, or explicit dependence on $r$ in the bounds.\n\n\nReferences:\n\n[1] A. Mousavi-Hosseini, D. Wu, M. A. Erdogdu. \"Learning Multi-Index Models with Neural Networks via Mean-Field Langevin Dynamics.\" ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YkX25oiNow", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Reviewer_t3JK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Reviewer_t3JK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124712240, "cdate": 1762124712240, "tmdate": 1762918877494, "mdate": 1762918877494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the theoretical ability of two-layer neural networks trained with gradient descent to learn so-called multi-index models of the form f(x) = g(Ux). The authors assume that the link function g is a polynomial function of a few linear projections of the input. They analyze a specific layer-wise training algorithm: first, the inner weights are updated by a few small gradient-descent steps (the “feature-learning” phase), and then the outer weights are optimized on a fresh dataset (the “readout” phase). The main theoretical result (Theorem 1) shows that, under certain regularity and non-degeneracy assumptions on the loss and activation, the resulting network achieves vanishing test error using O(d log d) samples—thus matching the information-theoretic limit of O(d) up to logarithmic factors.\n\nThe central idea is to replace the usual spectral initialization by a short sequence of small gradient-descent steps followed by one large step, and to prove that with sufficiently many samples this procedure achieves perfect learning. In other words, the first phase acts as a substitute for spectral initialization, while the final large gradient step ensures convergence to the correct subspace. This approach avoids relying on an explicit spectral start, which is the standard technique in related problems.\n\nWhile this is an interesting and elegant way to re-establish an expected result, the paper in its current form contains several overstated claims, imprecise statements made to artificially broaden its generality, and misleading references to prior work. The contribution is promising and potentially valuable, but the presentation requires revision and clarification (see detailed comments below). \n\nThis issue is the main reason for my low overall score: the technical results are solid and interesting, but the framing, scope, and literature discussion are overstated. Importantly, these problems should be relatively easy for the authors to correct through a careful revision that accurately reflects what is proved and properly situates the work within the existing literature."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It was already known from Chen & Meka (2020) and related works that low-dimensional polynomial functions (i.e., generative-exponent-2 multi-index models) can be learned efficiently, near the information-theoretic limit. The present paper’s contribution is to re-establish this result within a neural-network training framework, showing that a standard two-layer network trained by gradient descent can implicitly reproduce the behavior of these earlier polynomial-learning algorithms. Replacing an explicit spectral initialization by a short sequence of small gradient steps is an elegant idea that bridges classical spectral methods with early gradient-descent dynamics.\n\nOverall, this is a good and solid piece of work. It addresses a well-motivated and long-standing open question—whether standard gradient-descent training of neural networks can, on its own, achieve near–information-theoretic learning of structured high-dimensional targets. The main statements are interesting and relevant to the current theoretical understanding of representation learning, and the approach provides a clear and rigorous connection between classical spectral algorithms and neural-network optimization dynamics. Even though the setting is somewhat restricted, the paper gives a precise and constructive answer to this important question, and the results are likely to be of interest to researchers working on the theory of learning dynamics and high-dimensional inference."}, "weaknesses": {"value": "I think the main result is genuinely interesting—essentially showing that, in a specific regime, the network dynamics are equivalent to a power iteration on a certain matrix. But the framing is off: the paper presents this as a broad statement about neural networks learning generic multi-index models, whereas in reality it addresses a much narrower, well-understood setting. A more honest and focused framing would make the contribution stronger and easier to appreciate. As of now, the paper currently overstates its contributions, blurs the limits of its applicability, and misrepresents parts of the existing literature.\n\n1. Statements and Overclaimed Results\n\n• Overstated main claim.\nThe paper does not fully align with either its title or its main motivating question. It is therefore misleading to describe the work as proving that “neural networks learn multi-index models near the information-theoretic limit.” In practice, the analysis concerns only non-staircase, generative-exponent-2 polynomial functions, a regime already known to be learnable efficiently since the work of Chen & Meka (2020). The authors simply replace the spectral start used in those earlier algorithms by a short gradient-descent phase with a (largely unspecified) modified loss that mimics the spectral initialization. This is a nice idea and could justify publication, but it is essential to correctly reflect the actual scope and contribution of the work.\n\nFrom the title and framing, readers are led to believe that the authors have proved that “neural networks learn multi-index models near the information-theoretic limit.” In reality, it is not even clear which specific class of models this applies to (see Assumption 5). The claim is clearly overstated; even for single-index models, such a general statement would not be justified.\n\nIndeed, the spectral-start strategy (or its gradient-descent surrogate) can only access functions that are not staircase, or more precisely, not generative-staircase in the terminology of Troiani et al. Using the rigorous interpolation method of Aubin et al., those authors showed that learning general multi-index models requires an iterative staircase mechanism—learning one direction at a time and restarting the spectral step. All these regimes are excluded by construction here. While such an extension might be possible, it is far from trivial and should not be implicitly claimed.\n\n• On Assumptions 4 and 5.\nThese assumptions effectively linearize the loss and amount to introducing a spectral start. The text states:\n\n“Recent studies on generative exponents Damian et al. (2024; 2025) show that for almost all common tasks … there exists a pre-processing function that ensures the second-order information is non-degenerate.”\n\nFor generative exponent 2, however, this result is in fact due to Yue Lu (2017) or Mondelli & Montanari (2018). Reading Damian et al. (2024, 2025) reveals no genuinely new results for exponent 2—they mainly restate existing work and extend it to higher exponents. It is therefore misleading to credit them for ideas that are much older. The fact that the paper never references the original works on spectral methods—single-index (Lu 2017; Mondelli & Montanari 2018; Maillard et al. 2020) or multi-index (Troiani et al. 2025; Defilippis et al. 2025; Kovačević et al. 2025)—is surprising. The key matrix Σ already appears in these earlier analyses. (See, for instance, Lu 2017, arXiv:1702.06435; Maillard et al., arXiv:2012.04524; Aubin et al., arXiv:2010.03460, none of which are cited despite predating most of the works referenced here.)\n\n\n2. Imprecise Statements\n\nThe paper relies on a modified loss, special activation functions, and other restrictive assumptions, which make it unclear what it truly means for these models to be “learned by neural networks.” For instance, using a standard ℓ₂ loss would not work, nor would a ReLU activation. This is not a problem in itself, but given the ambitious title and claims, it is misleading.\n\n• Choice of loss function.\nThe authors do not use the square loss but rather a specially chosen loss that ensures non-degeneracy. Ass.4 and 5 are so vague that it is unclear which functions are actually covered. Given the limitations above, the authors should clearly specify the excluded cases (e.g., all staircase structures). Even if such a loss can be generic, this remains a limitation that should be clearly acknowledged. Saying that “neural networks learn” is somewhat misleading, since the success of the algorithm depends on a modified loss. Similar connections have already been discussed in the literature—for instance arXiv:2403.02418, arXiv:2510.18435, and, in the DMFT context, arXiv:2509.23527 and arXiv:2103.04902—none of which are cited. (Notably, Yue Lu’s 2017 paper was itself titled “Spectral Start.”)\n\n\n3. Discussion of Related Work\n\nThe discussion of related work is incomplete and historically inaccurate, which substantially weakens the scholarly framing of the paper. In fact, reading it one might believe that the relevant literature began in 2024. Several key theoretical developments that underpin the results are misrepresented or omitted. More generally, the section over-emphasizes extremely recent U.S.-based works while under-citing the earlier theoretical literature that established (i) Bayes-optimal errors and phase diagrams for GLMs, (ii) spectral thresholds (BBP-type) for pre-processed covariance methods, and (iii) AMP algorithmic limits—all directly relevant to the present theorems.\n\n(1) Information-theoretic foundations.\nThe paper attributes the identification of the information-theoretic scaling n = Θ(d) for single- and multi-index models to Dudeja & Hsu (2024) and Damian et al. (2024). In fact, these results were rigorously established much earlier by Barbier, Krzakala, Macris, Miolane & Zdeborová (2017, PNAS), who derived the exact Bayes-optimal risk and phase transitions in high-dimensional GLMs using the Guerra/Talagrand interpolation method. The multi-index generalization appeared the following year in Aubin et al. (NeurIPS 2018, “The Committee Machine: Computational to Statistical Gaps in Learning a Two-Layer Neural Network”)—not to mention the non-rigorous statistical-physics literature from the 1980s that already predicted these results.\n\n(2) Spectral and AMP results for single-index models.\nThe manuscript credits Mondelli & Montanari (2018) for “generic single-index models and sharply deriving the learning threshold.” Yet this threshold was obtained earlier in Barbier et al. (2017, PNAS), where Eq. (11) gives the celebrated “generative-exponent 2” transition derived via AMP. The contribution of Mondelli & Montanari was to employ pre-processing within a spectral method—but even this approach was introduced earlier by Yue M. Lu and collaborators (arXiv:1702.06435; arXiv:1811.04420), who analyzed spectral initialization and its BBP-type phase transitions for a wide class of models.\n\nThe quoted papers by Damian et al. on generative-exponent 2 for single-index models do not introduce new results; they merely restate those earlier works. Their genuine (and important) contribution was the extension to higher exponents, which is not relevant to the present paper, as it remains confined to exponent 2.\n\n(3) Causality and scope mismatches: The following sentence is incorrect: “Subsequently, Troiani et al. (2025) … recovers the results of Mondelli & Montanari (2018).” In fact:\n- Causality is reversed: The Bayes-AMP theory (Barbier et al., 2017–2019) came first; Mondelli & Montanari (2018) recovered its single-index specialization.\n- Scope mismatch: Mondelli & Montanari treated single-index models only. The generalization to multi-index models followed Troiani et al. (2025), which inspired two independent recent spectral studies:\n(i) Kovačević, Zhang & Mondelli (2025, COLT, arXiv:2502.01583)\n(ii) Defilippis, Dandi, Mergny, Krzakala & Loureiro (2025, NeurIPS, arXiv:2502.02545)\n\nBoth derived equivalent results, and the latter appeared slightly earlier (as acknowledged in the former’s conclusion). Only the first is cited here. Because these spectral starts are central to the present paper, such omissions and reversed chronology substantially weaken the discussion.\n\n(4) On staircase and generative-exponent structures.\nThe authors emphasize the staircase or leap-complexity results of Abbe et al. but ignore the more relevant generative-exponent framework. Unfortunately, no gradient-descent results are known for that case, and the strategy used here cannot address it. These limitations are never discussed, even though the paper is restricted to polynomial targets (see Assumption 1). This omission is problematic given the claimed generality of the title (see Section 1 of this review). \n\n(5) DMFT and dynamical analyses.\nNo discussion of DMFT-based results appears apart from the recent Montanari & Urbani (2025). Rigorous DMFT studies have long shown that, with an appropriate “hot start,” two-layer networks can be learned efficiently (see e.g. arXiv:1710.04894; arXiv:2006.06098; arxiv:2112.07572 arXiv:2210.06591, rXiv:2303.00055). Instead, the authors cite an unpublished and identifiable work, “Andrea Montanari and Zihao Wang, Spiked Model for the Hessian of Two-Layer Neural Networks, In preparation, 2025.”\n\nWe note that this citation violates the ICLR anonymity policy: it (i) names specific, non-anonymous researchers; (ii) refers to unverifiable, unpublished work; and (iii) could reveal the submitting authors’ identities through known collaborations. Even if the reference is genuinely independent, such a citation breaks the spirit of the double-blind process and should be removed."}, "questions": {"value": "To strengthen the paper and make its contribution both accurate and impactful, I suggest the authors consider the following revisions:\n\n* Reframe the scope and claims: The current title and framing promise a general solution to “learning multi-index models near the information-theoretic limit,” while the actual analysis applies only to non-staircase, generative-exponent-2 polynomial functions. The paper would benefit from a more precise and honest statement of scope. A few possible titles that would better reflect the contribution: “Learning Low-Dimensional Polynomial Features via Layer-Wise Gradient Descent”\n\n* Clarify conceptual novelty: The key insight is that gradient descent can mimic spectral initialization—an elegant technical observation. The authors should emphasize this as the main conceptual contribution, rather than claiming a fundamentally new learning phenomenon. It would be useful to explicitly contrast their method with the spectral algorithms, clarifying what is gained by expressing them in the language of neural-network training.\n\n* Specify assumptions and limitations clearly. The paper should explicitly list the restrictions implied by Assumptions 1–5, particularly that (i) the link function must be a low-degree polynomial, (ii) the activation is smooth and quadratic near zero, and (iii) the loss must be specially chosen to ensure non-degeneracy. The authors could also discuss whether these conditions are essential or merely technical conveniences.\n\n* Correct and balance the related-work section. The discussion should be revised to include and properly attribute earlier foundational results. Correcting the chronology and expanding the citations will make the contribution clearer and situate it fairly within the literature. It would be useful to comment explicitly on how this analysis relates to the established dynamical mean-field (DMFT) or “state evolution” approaches. Are the authors’ results a finite-time counterpart of those dynamical equations? A short discussion would make the connection to the broader learning-dynamics literature much clearer.\n\n* Address anonymity issues: The reference to “Andrea Montanari and Zihao Wang, in preparation (2025)” should be removed or replaced by an anonymized placeholder consistent with the ICLR double-blind policy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F21LPLICDX", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Reviewer_Quxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Reviewer_Quxi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155554241, "cdate": 1762155554241, "tmdate": 1762918876949, "mdate": 1762918876949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank the reviewers for their detailed and constructive feedback.\n\nIn this round of response, we have made the following revisions.\n\n(1) **Relaxed loss assumptions.**\nBeyond the original Lipschitz setting (Assumption 3), we now introduce Assumption 3', which allows loss functions whose derivative has at most linear growth in the prediction. This covers square loss, (\\ell_1) loss, Huber loss, and pseudo-Huber loss. Theorem 1 is now stated and proved under this more general assumption (see Sections 2–3 and Appendix B.4).\n\nMore generally, our techniques extend to any loss whose first and second derivatives satisfy a polynomial growth condition. This extension is technically straightforward but notationally heavy, and does not introduce new ideas, so we do not include it in full detail. The main motivation for the current revision is to explicitly include quadratic loss. With this change, our results strictly strengthen those of [arXiv:2206.15144], which to our knowledge is (at least, one of) the state-of-the-art theoretical works on learning multi-index models with neural networks up to vanishing error under generic assumptions (and achieving strong, not just weak, recovery).\n\n(2) **Expanded related work.**\nWe substantially expanded the related-work discussion, especially on DMFT / state evolution and spectral methods or spectral initialization for multi-index models and feature learning. We would be happy to incorporate further references if there are important references we are still missing.\n\n(3) **Refined framing and exposition.**\nWe polished the main text and appendix, and adjusted the framing to make the statements more technically precise. In particular, we plan to slightly change the title to “Neural networks learn generic multi-index models near the information-theoretic limit” or \"Neural networks learn almost all multi-index models near the information-theoretic limit\" in the final version and in the arxiv version (after the rebuttal) (the title of the submitted version cannot be updated at this stage).\nIn the abstract, we now clearly state that we require a non-degeneracy assumption on the link function. In Section 1.1 (“Our result”) we spell out this non-degeneracy condition, explain its role, and compare it carefully to the assumptions in previous works. We also emphasize this point in Sections 3 and 4 to address the reviewers’ concerns, and to make clear in what sense our results still represent a substantial improvement over the existing theory of neural network learning.\n\n(4) **Added numerical experiments.**\nWe added two numerical studies (Appendix A). The first shows that the sample-size exponent (\\alpha(d)) required for the test error to vanish drifts towards (1), consistent with our master theorem predicting (n = \\widetilde O(d)). The second study considers a Hermite-4 target and compares different loss functions: with square loss, training fails to recover the correct representation and does not generalize, while with Huber loss (which satisfies Assumption 5) the network succeeds. This experiment, detailed in Appendix A, illustrates the importance of choosing an appropriate loss and is in line with our theoretical predictions.\n\nWe also addressed other minor issues. We will detail them in the individual response.\n\nPlease let us know any further suggestions and concerns, and we are willing to revise accordingly!"}}, "id": "aesaxtvIa1", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763336943097, "cdate": 1763336943097, "tmdate": 1763337767132, "mdate": 1763337767132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank the reviewers for their detailed and constructive feedback.\n\nIn this round of response, we have made the following revisions.\n\n(1) **Relaxed loss assumptions.**\nBeyond the original Lipschitz setting (Assumption 3), we now introduce Assumption 3', which allows loss functions whose derivative has at most linear growth in the prediction. This covers square loss, (\\ell_1) loss, Huber loss, and pseudo-Huber loss. Theorem 1 is now stated and proved under this more general assumption (see Sections 2–3 and Appendix B.4). We cost an additional polylog(d) factor for this.\n\nMore generally, our techniques extend to any loss whose first and second derivatives satisfy a polynomial growth condition. This extension is technically straightforward but notationally heavy, and does not introduce new ideas, so we do not include it in full detail. The main motivation for the current revision is to explicitly include quadratic loss. With this change, our results strictly strengthen those of [arXiv:2206.15144], which to our knowledge is (at least, one of) the state-of-the-art theoretical works on learning multi-index models with neural networks up to vanishing error under generic assumptions (and achieving strong, not just weak, recovery).\n\n(2) **Expanded related work.**\nWe substantially expanded the related-work discussion, especially on DMFT / state evolution and spectral methods or spectral initialization for multi-index models and feature learning. We would be happy to incorporate further references if there are important references we are still missing.\n\n(3) **Refined framing and exposition.**\nWe polished the main text and appendix, and adjusted the framing to make the statements more technically precise. In particular, we plan to slightly change the title to “Neural networks learn generic multi-index models near the information-theoretic limit” or \"Neural networks learn almost all multi-index models near the information-theoretic limit\" in the final version and in the arxiv version (after the rebuttal) (the title of the submitted version cannot be updated at this stage).\nIn the abstract, we now clearly state that we require a non-degeneracy assumption on the link function. In Section 1.1 (“Our result”) we spell out this non-degeneracy condition, explain its role, and compare it carefully to the assumptions in previous works. We also emphasize this point in Sections 3 and 4 to address the reviewers’ concerns, and to make clear in what sense our results still represent a substantial improvement over the existing theory of neural network learning.\n\n(4) **Added numerical experiments.**\nWe added two numerical studies (Appendix A). The first shows that the sample-size exponent (\\alpha(d)) required for the test error to vanish drifts towards (1), consistent with our master theorem predicting (n = \\widetilde O(d)). The second study considers a Hermite-4 target and compares different loss functions: with square loss, training fails to recover the correct representation and does not generalize, while with Huber loss (which satisfies Assumption 5) the network succeeds. This experiment, detailed in Appendix A, illustrates the importance of choosing an appropriate loss and is in line with our theoretical predictions.\n\nWe also addressed other minor issues. We will detail them in the individual response.\n\nWe believe our revision should address the concerns. However, please let us know any further suggestions and concerns, and we are willing to revise accordingly!"}}, "id": "aesaxtvIa1", "forum": "2Q0U2rV2Jz", "replyto": "2Q0U2rV2Jz", "signatures": ["ICLR.cc/2026/Conference/Submission6516/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6516/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission6516/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763336943097, "cdate": 1763336943097, "tmdate": 1763355661303, "mdate": 1763355661303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}