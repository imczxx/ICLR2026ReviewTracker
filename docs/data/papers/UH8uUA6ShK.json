{"id": "UH8uUA6ShK", "number": 2868, "cdate": 1757293862261, "mdate": 1759898122004, "content": {"title": "DreamExplorations: Leveraging Suboptimal Noisy Robot Trajectories in Offline RL", "abstract": "Exploration is a desirable characteristic in online reinforcement learning, where the online agent can interact with the environment, explore the diverse states, and update the policy. However, since the datasets of offline reinforcement learning are static and the traditional offline RL algorithms always rely on the relatively good quality of demo agents, it is very hard to explore the diversity of state space. In this paper, we have found out that in offline goal-conditioned reinforcement learning (OGCRL), we can theoretically leverage suboptimal/high noisy datasets for state exploration and we have designed a pipeline to use them. In this case, the highly noisy datasets which are always discarded and regarded as useless datasets in previous researches are used as exploration experts to keep improving the performances of offline reinforcement learning as we scale the sizes of suboptimal datasets. Experimental results demonstrate that our method consistently outperforms baselines and significantly improves models trained solely on high-quality data, especially in environments with large state spaces. This work highlights the untapped potential of imperfect data in enhancing the robustness and generalization of offline RL. We will open-source our code after publication.", "tldr": "", "keywords": ["Robot Learning", "Reinforcement Learning", "Embodied AI"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1da535439cea604ed83c8954e4a02ccb6be57018.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "**Summary.** This work studies how do we leverage sub-optimal trajectories in offline RL settings. The authors propose an algorithm for merging value information from both high- and low-quality datasets. They claim that suboptimal data can enhance state space coverage and improve value estimation. Their pipeline involves three key properties: \n- decoupled value learning for exploitation and exploration\n- a learned novelty ratio network to weight the contributions of each value function\n- mixture-based policy training.\n\n---\n**Review summary.** The paper tries to tackle a popular problem in offline RL, but its technical and empirical depth is insufficient for acceptance. The approach is conceptually intuitive yet incremental, the theory and practice linkage is weak, and the writing quality undermines the clarity of otherwise reasonable ideas. Strengthening the mathematical rigor, providing richer experimental analyses, and improving exposition could elevate the work for future submission. Therefore, the reviewer assigns an initial score of 2 and plan to revisit this rating after the authors address the concerns and questions raised in this review."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Writing**\n- Figures and tables are numerous and relevant to the claims.\n\n---\n**Methodology**\n- The proposed decoupled value learning and novelty-weighted value fusion present a clean and modular extension to HIQL.\n- The idea of using suboptimal data to improve state coverage and generalization is practical. \n\n---\n**Theory**\n- The bias-variance decomposition and the closed-form derivation of the optimal weight demonstrate an attempt to ground the intuition.\n\n---\n**Experiments**\n- The didactic simulation is helpfuul to understand how combining suboptimal and expert trajectories might reduce estimation error.\n-  Ablation studies test warm-up, data ratio, and novelty parameters, giving some view into design sensitivity.\n- The scaling analysis is useful for highlighting diminishing returns from excessive suboptimal data."}, "weaknesses": {"value": "**Writing**\n- The reviewer thinks that writing quality is usbpar for ICLR submission.\n    - Several sentences are tautological, e.g., *We have proposed extensive experiments to thoroughly evaluate the effectiveness and robustness of our proposed algorithms.* \n    - Several sentences are verbose and unconvincing, e.g., *This insight is inspired by the development of large-scale language models. The remarkable performance of GPT-4 and DeepSeek-R1 stems not from carefully filtered, perfect corpora, but from massive and heterogeneous datasets.*\n    - Logical flow between theory (Sec. 4.5–4.6) and method (Sec. 5) is abrupt.\n    - Related works section seems dated. Please discuss and survey recent heterogeneous/offline RL methods.\n- Some figures lack clear legends, scales, or quantitative interpretation, reducing clarity.\n\n---\n**Methodology**\n- The reviewer thinks that the proposed solution is incremental relative to HIQL. Essentially, introduces an extra scalar gating function over two separately trained value networks.\n- There is no empirical or theoretical validation that the learned novelty network $R_\\psi$ approximates the analytic optimal weighting derived earlier.\n- The loss definition in Eq. (15) with fixed hyperparameters $A^+, A^-$ is heuristic and lacks adaptive justification.\n\n---\n**Theory**\n- The derivations are shallow and partially inconsistent with the later implementation. Constants $c_1, c_2, d_1, d_2$ are assumed environment-invariant without evidence, and their empirical interpretability is unexamined.\n- The reviewer thinks that the theory to practice transition (from Eq. 11 to Eq. 16) is abrupt. There is no quantitative connection between analytic and learned weights.\n\n---\n**Experiments**\n- No analysis of alternative fusion methods (nonlinear, uniform, hard assignment) is provided, so the benefit of the proposed linear weighting remains unsubstantiated.\n- All results lack standard deviations, error bars, or significance tests.\n- The novelty network’s learned behavior is never visualized or quantitatively analyzed; we do not see evidence that $R_\\psi(s)$ correlates with novelty or visitation frequency."}, "questions": {"value": "- How is the novelty network $R_\\psi(s)$ validated in practice? Can the authors show visualizations of its predictions over the state space and their correlation with visitation frequencies?\n- Are constants $c_1,c_2,d_1,d_2$ from the analytic derivation estimated or tuned empirically, and how sensitive is the model to their assumptions?\n- Would nonlinear or uniform mixture strategies yield similar improvements?\n- How does the algorithm behave when scaling suboptimal data in large-scale or realistic domains, does performance degrade predictably?\n- Can the approach handle other forms of suboptimality, such as sensor corruption or occlusion, rather than stochastic action noise?\n- How are $A^+, A^-$ selected across tasks, and do they require domain-specific tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CMOiBkCdxN", "forum": "UH8uUA6ShK", "replyto": "UH8uUA6ShK", "signatures": ["ICLR.cc/2026/Conference/Submission2868/Reviewer_eGZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2868/Reviewer_eGZf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629263377, "cdate": 1761629263377, "tmdate": 1762916419598, "mdate": 1762916419598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for leveraging suboptimal trajectories as a useful learning signal in offline reinforcement learning. While it is widely recognized that suboptimal data can contribute valuable information when integrated appropriately, the challenge lies in how to effectively combine it with high-quality behavior data.\n\nTo address this, the authors introduce an approach that trains separate value functions over two datasets: one representing high-quality behavior and another derived from exploratory, suboptimal trajectories. A novelty score estimation network is employed to estimate whether a given state originates from the high- or low-quality dataset. Based on this estimation, the method constructs a novelty ratio–weighted mixture of the two value functions. This weighted integration allows for good results on the DM control suite."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the paper is very clear\n- the paper is easy to follow\n- the presentation and the toy example help understanding"}, "weaknesses": {"value": "It is unclear why the paper focuses exclusively on offline goal-conditioned RL, rather than situating the work within the broader offline RL literature. Expanding the discussion to include related methods in general offline RL would help clarify the broader relevance of the proposed approach.\n\nThe claim of achieving a new state-of-the-art algorithm is not sufficiently supported by the current set of experiments. To substantiate this claim, evaluation on established benchmarks—such as AntMaze, Kitchen, CALVIN, Procgen Maze, Visual AntMaze, and Roboverse—following HIQL evaluation protocols would be necessary.\n\nIn particular, Figure 6, subfigures (c–f) do not show a clear relationship between data mix and performance, suggesting that optimal data mixing may not be the key factor influencing results (for most tasks). This point warrants further clarification and possibly additional analysis.\n\nSeveral of the claims may depend heavily on the choice and quality of the non-expert data used. A more rigorous discussion or ablation study examining the sensitivity of results to different non-expert data sources would strengthen the manuscript"}, "questions": {"value": "Please address the weaknesses 1 - 4 above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yGhVofLiOe", "forum": "UH8uUA6ShK", "replyto": "UH8uUA6ShK", "signatures": ["ICLR.cc/2026/Conference/Submission2868/Reviewer_FNgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2868/Reviewer_FNgD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966977738, "cdate": 1761966977738, "tmdate": 1762916419265, "mdate": 1762916419265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DREAMEXPLORATIONS, a method for offline goal-conditioned reinforcement learning (OGCRL) that addresses policy overfitting by leveraging suboptimal and noisy trajectories, which are typically discarded by traditional methods, to achieve better state exploration and generalization. The core mechanism involves decoupled value learning, where separate value functions are trained for exploitation (using high-quality data) and exploration (using low-quality data). These separate functions are then combined using a novelty estimation network that predicts a blending ratio, dictating the optimal linear mixing of the two value signals to guide policy learning. This strategy successfully utilizes imperfect data as an asset, demonstrating consistent performance improvements over baselines, particularly in environments requiring extensive state coverage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A key strength of the paper lies in its innovative reframing of suboptimal data as an asset. The work addresses a critical limitation in Offline Goal-Conditioned Reinforcement Learning (OGCRL) by demonstrating how highly noisy or suboptimal datasets, which prior research often disregards as “useless,” can instead be effectively leveraged. By harnessing this imperfect data, the approach promotes more diverse state exploration and contributes to improved robustness and generalization of the learned policy. This perspective not only challenges conventional assumptions in offline RL but also opens new avenues for utilizing previously overlooked data sources."}, "weaknesses": {"value": "W1 \n\nThe assumption that access to data from the optimal policy is available appears quite strong and may limit the practical applicability of the proposed approach. It would be helpful if the authors could discuss how the method performs when only suboptimal or noisy data is available.\n\nW2\n\nThe idea of leveraging lower-quality data closely resembles that in [1] CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning. It would strengthen the paper to include experimental comparisons with this work to clarify the relative advantages of the proposed approach."}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ycFehhjoKY", "forum": "UH8uUA6ShK", "replyto": "UH8uUA6ShK", "signatures": ["ICLR.cc/2026/Conference/Submission2868/Reviewer_VjWX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2868/Reviewer_VjWX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982840646, "cdate": 1761982840646, "tmdate": 1762916418958, "mdate": 1762916418958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}