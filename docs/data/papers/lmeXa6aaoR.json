{"id": "lmeXa6aaoR", "number": 21049, "cdate": 1758313209811, "mdate": 1759896945239, "content": {"title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks", "abstract": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about captcha resolution. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.", "tldr": "We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes.", "keywords": ["web agents", "agent benchmark", "agent evaluations", "human preference modeling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d8c0fb8c82e64d1f453a6ac80e5f7e8f94a5dd9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work builds a toolkit for collecting web-based tasks and annotating agent playing trajectories in an open web environment. The authors also conduct several experiments to analyze the performance of current large models on the open web, and identify their main failure patterns."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This is a toolkit that allows web agents to execute tasks in the open web environment, making it easier to crowdsource more tasks and annotations from users. Compared with contributions focusing on datasets or benchmarks, this work is more suitable to be evaluated under a demo track."}, "weaknesses": {"value": "First, this data collection toolkit should ideally address at least some of the failure patterns it identifies, such as handling CAPTCHA and closing pop-ups. These are not really the intended goals of web-agent research — these are trivial, procedural problems that can be solved with simple dedicated pipelines. For example, we can’t really say that solving CAPTCHA is a core capability which web agents are developed for. If that’s the case, why doesn’t the toolkit itself handle these issues to avoid their interference with the main conclusions?\n(It feels somewhat ironic if the biggest challenge revealed by this platform for current web agents turns out to be “solving CAPTCHAs and closing banners.”)\n\nSecond, before submission, the authors could benchmark their platform against existing “arena” frameworks such as Chatbot Arena, and evaluate which features an arena should have—for example, a more fine-grained ranking system and anomalous user detection. These design elements are crucial for ensuring the accuracy and robustness of evaluations in an open arena setting.\n\nThird, has this arena attracted a large user base? If not, what is the plan to engage more users? Designing and releasing an open-source toolkit is primarily an engineering task, but to demonstrate that the toolkit truly works, it needs to be instantiated with a sufficient number of real data points, showcasing its usage and analysis at scale."}, "questions": {"value": "* Why doesn’t this toolkit help mitigate issues like CAPTCHA and pop-ups to prevent them from distorting the main evaluation conclusions?\n* How does the arena ensure evaluation accuracy and robustness in an open setting?\n* Has it attracted a large number of users, and if not, what strategies are in place to do so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fkm1S9cS9c", "forum": "lmeXa6aaoR", "replyto": "lmeXa6aaoR", "signatures": ["ICLR.cc/2026/Conference/Submission21049/Reviewer_G5Uu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21049/Reviewer_G5Uu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859952194, "cdate": 1761859952194, "tmdate": 1762940624516, "mdate": 1762940624516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrowserArena, a live, open-web platform for evaluating LLM web agents using user-submitted tasks and Chatbot Arena-style pairwise comparisons. To address limitations of final-output metrics, the core contribution is a methodology utilizing granular step-level human feedback collected on agent traces. Analyzing 109 user-submitted tasks, the authors identify three consistent agent failure modes: captcha resolution, pop-up banner removal, and unintended direct navigation. Subsequent targeted experiments demonstrate notable behavioral differences and brittleness across contemporary LLM agents in handling these real-world obstacles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The most significant contribution is the diagnostic evaluation methodology rooted in collecting step-level human annotations, which effectively surfaces intermediate performance issues that traditional final-output benchmarks overlook. The use of live, user-submitted tasks enhances the ecological validity, avoiding the highly specific or artificial constraints of self-hosted environments. Furthermore, the empirical rigor shown by using the collected feedback to identify and construct specialized datasets focused on persistent failure modes (e.g., Captcha, Pop-up) provides quantifiable insights into agent deficiencies."}, "weaknesses": {"value": "1. The participants' motivation must be considered when building this arena. The motivation of participants in a chatbot arena is clearly stronger, as the inherent instability of LLMs makes people need to see the outputs of different LLMs, and the overall process is fast and efficient. However, for GUI Agents, participants seem to lack sufficient motivation to interact by watching two GUI Agents output different reasoning. Without enough motivation, this system will not be scalable, and the value will not be significantly different from offline labeled evaluation data. I believe this is the core problem of this work. To justify this motivation, it should be demonstrated either through the actual number of user-generated interactions or by conducting interaction experiments in the field of HCI.\n2. Some insights arise from the limitations of the evaluation setup: for instance, the lack of methods and evaluation for multimodal observation and grounding.\n3. Relying on the browseruse framework is also unreasonable because there are multiple implementation paradigms for web agents, which significantly limits the value generated by the evaluation."}, "questions": {"value": "1. How to evaluate the validity of step-level feedback? This is because many tasks for GUI Agents cannot be judged as correct or incorrect based solely on the current step (as a single semantic unit/subtask often requires multiple steps), and some GUI Agents may be skilled at recovering from errors.\n2. What specific distributional insights are there regarding the user-submitted queries, and what potential biases exist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "djeZGvmn7d", "forum": "lmeXa6aaoR", "replyto": "lmeXa6aaoR", "signatures": ["ICLR.cc/2026/Conference/Submission21049/Reviewer_jXW9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21049/Reviewer_jXW9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997731327, "cdate": 1761997731327, "tmdate": 1762940623945, "mdate": 1762940623945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an evaluation platform, BrowserArena, that collects user preference data on 109 user-submitted tasks to construct a language model leaderboard.\nThey proposed a new method for evaluating LLM performance in web browsing by collecting step-level user annotations on agent traces and analyzing them to identify failure modes."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset is well-motivated and moves toward challenges that are better representative of real-world tasks instead of sandbox tasks.\n- This work focuses on failures in CAPTCHA, pop-ups, and navigation, which are overlooked in many other benchmarks."}, "weaknesses": {"value": "- Adding more information on data quality and distribution would be helpful.\n\n- Authors need to add more details and information to ensure reproducibility."}, "questions": {"value": "- What happens if both responses from agents are bad? \n\n- Most of the figures are not readable and need larger font/better color selection."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Live browsing and solving captchas raises ToS, privacy, and safety and legal issues, which have not been discussed properly in the paper."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ze88fg4lwe", "forum": "lmeXa6aaoR", "replyto": "lmeXa6aaoR", "signatures": ["ICLR.cc/2026/Conference/Submission21049/Reviewer_igzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21049/Reviewer_igzk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180646773, "cdate": 1762180646773, "tmdate": 1762940623199, "mdate": 1762940623199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an evaluation platform, BrowserArena, that collects user preference data on 109 user-submitted tasks to construct a language model leaderboard.\nThey proposed a new method for evaluating LLM performance in web browsing by collecting step-level user annotations on agent traces and analyzing them to identify failure modes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset is well-motivated: BrowserArena collects real user-submitted tasks rather than synthetic or template-based ones, making it more representative of real-world browsing behavior, unlike benchmarks built on self-hosted replicas (e.g., WebArena) or heavily templated QA-style tasks (e.g., Mind2Web, MMInA).\n\n- This work focuses on failures in CAPTCHA, pop-ups, and navigation, which are overlooked in many other benchmarks."}, "weaknesses": {"value": "- While the dataset is promising, it would benefit from more detail on task distribution and data quality: e.g., domain breakdown, difficulty variability, types of interaction required, proportion of tasks discarded, and quality-control measures for user-submitted tasks. Similarly, more information on the consistency and reliability of step-level human annotations would be helpful.\n\n- The paper could include additional details to ensure reproducibility, such as a more transparent description of the annotation interface, user instructions, and post-processing pipelines used to extract failure modes."}, "questions": {"value": "- What happens if both responses from agents are bad? \n\nSuggestion: \n- Most of the figures, especially figure 3, are not readable and need a larger font/better color selection (contrast). It's extremely hard to see the number in the yellow squares in Figure 3."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Live browsing and solving captchas raises ToS, privacy, and safety and legal issues, which have not been discussed properly in the paper."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ze88fg4lwe", "forum": "lmeXa6aaoR", "replyto": "lmeXa6aaoR", "signatures": ["ICLR.cc/2026/Conference/Submission21049/Reviewer_igzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21049/Reviewer_igzk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21049/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180646773, "cdate": 1762180646773, "tmdate": 1763022388438, "mdate": 1763022388438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}