{"id": "FLyvCS6eJf", "number": 13929, "cdate": 1758225359049, "mdate": 1759897402861, "content": {"title": "AN ONTOLOGY ENRICHMENT FRAMEWORK USING RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS", "abstract": "Ontology enrichment, understood as the process of extending and refining existing ontologies with new concepts, relations, and instances, has become a critical task for building robust and up-to-date knowledge bases. The exponential growth of scientific publications, datasets, and multimodal resources makes manual enrichment highly impractical, creating the need for automated or semi-automated approaches. In this work, we propose a framework that leverages multimodal large language models and retrieval-augmented generation to support ontology enrichment. Our method systematically extracts semantic knowledge units, aligns them with existing ontological structures, and generates interlinked triples, thereby enhancing both the coverage and the expressivity of the ontology. This framework addresses the knowledge acquisition bottleneck by enabling scalable integration of heterogeneous resources and fostering cross-domain semantic interoperability. To illustrate its effectiveness, we apply the framework to the domain of 4D printing, a rapidly evolving field at the intersection of materials science, manufacturing, and design. By incorporating knowledge about materials, properties, stimuli interactions, process parameters, and design strategies, the framework enriches a domain-specific ontology and supports innovation in the development of programmable and multifunctional structures.", "tldr": "Scalable Ontology Enrichment via Multimodal Large Language Models and Symbolic Reasoning Integrating Scientific Articles, Datasets, and Knowledge Graphs", "keywords": ["Ontology enrichment", "Multimodal large language model", "Retrieval-augmented generation", "Knowledge graph", "4D printing"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80f8c56e81db622709c1ab6dfa6259fdc9debb3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper employs LLM and RAG to populate an ontology from a given set of keywords describing a domain, 4D prinitng in this case."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "S1. The paper is easy to read."}, "weaknesses": {"value": "W1. No technical contribution. The paper is basically a technical report describing a standard implementation of a LLM-based ontology population approach.\n\nW2. Missing important technical details. For example, how does your symbolic reasoner work? How did you describe and cataloge a column? How did you extract object properties to link individuals? What are your relationship mapping strategies?How did you normalize and clean entity names?\n\nW3. Lack of evaluation. How did you evaluate the quality of the generated ontology? How would you compare your approach with existing methods?"}, "questions": {"value": "Q1. What is the technical contribution of this paper?\n\nQ2. See the questions in W2 and W3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bpR5ts9R76", "forum": "FLyvCS6eJf", "replyto": "FLyvCS6eJf", "signatures": ["ICLR.cc/2026/Conference/Submission13929/Reviewer_yZLL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13929/Reviewer_yZLL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637959145, "cdate": 1761637959145, "tmdate": 1762924434061, "mdate": 1762924434061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for automatically enriching ontologies using multimodal large language models combined with retrieval-augmented generation. The authors extract semantic knowledge from scientific papers, datasets, and knowledge graphs to expand existing ontologies. They demonstrate their approach on the 4D printing domain, growing the HERMES ontology from 170 classes to 5849 classes and over 12.5 million instances."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting problem - automatically updating knowledge bases.\n2. Combining text and figures from papers makes sense for materials science, where diagrams and microscope images often contain information you can't get from text alone."}, "weaknesses": {"value": "1. The paper evaluates the proposed framework in isolation without comparing against existing ontology learning methods. The paper cites Phrase2Onto, OntoGPT, and other LLM-based ontology extension approaches but provides no quantitative comparison. Without baselines, it is impossible to assess whether the performance (Graph BERTScore F1 = 0.7) represents an improvement over simpler alternatives or state-of-the-art methods.\n2. The proposed framework consists of multiple components RAG retrieval, LLaVA fine-tuning, multimodal fusion, and multi-stage validation. However, the paper provides no ablation study showing the contribution of each component. The appendix only compares \"fine-tuning alone\" versus \"fine-tuning + one-shot,\" which doesn't isolate the contributions of RAG, multimodal inputs, or individual validation criteria.\n3. The paper claims the framework is \"fully domain-agnostic\", yet validation is performed exclusively on 4D printing. This contradicts the generalization claim and is insufficient to demonstrate transferability. \n4. The paper relies on a single metric (Graph BERTScore F1 = 0.7) mentioned briefly in the main text with details relegated to the appendix. This is insufficient for validating the core contribution.\n5. Essential technical details are missing: the size of the fine-tuning dataset for LLaVA, threshold values for domain relevance, semantic coherence, and similarity-based filtering, and computational requirements for the system.\n6. All figures are labeled \"adapted from (Bougzime et al., 2025b)\", a paper that is also under review. The paper should clarify what is novel here versus the cited work and whether this represents appropriate academic practice.\n7. No code, no data availability statement, insufficient implementation details."}, "questions": {"value": "1. Can you provide quantitative comparisons with at least 2-3 existing ontology learning methods (Phrase2Onto, OntoGPT, etc.) on standardized test sets?\n2. Can you demonstrate generalization by applying the framework to at least two additional domains beyond 4D printing?\n3. What's the size of your fine-tuning dataset?\n4. What's the computational cost in GPU hours?\n5. Can you provide ablation studies showing the contribution of RAG, multimodal inputs, and each validation criterion separately?\n6. What are the most common errors? Can you show examples of false positives and false negatives with analysis?\n7. Can you provide systematic ablation studies demonstrating the individual contribution of each framework component?\n8. What proportion of initially extracted triplets are filtered at each validation stage? Can you provide concrete examples of hallucinated assertions that were successfully detected and removed by your validation pipeline?\n9. What constitutes the core technical or scientific contribution beyond systems integration? Can you justify why this integrated pipeline yields superior results compared to direct prompting of frontier models like GPT-4 without the additional complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G91RmfAizW", "forum": "FLyvCS6eJf", "replyto": "FLyvCS6eJf", "signatures": ["ICLR.cc/2026/Conference/Submission13929/Reviewer_yygp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13929/Reviewer_yygp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916794383, "cdate": 1761916794383, "tmdate": 1762924433545, "mdate": 1762924433545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an automated framework for ontology enrichment that combines multimodal large-language-models with retrieval-augmented generation and symbolic reasoning. The pipeline ingests scholarly text and figures, retrieves relevant content, extracts candidate triples, filters and aligns them to a target ontology, then integrates results into the ontology store. The case study on a 4D-printing ontology reports large growth in classes, properties and instances, and explores metric-based validation and sensitivity to temperature and fine-tuning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Coherent end-to-end framework that unifies retrieval, multimodal extraction and symbolic checks for ontology population.\n2. Clear pipeline description with stages for ingestion, retrieval, extraction, validation, alignment and integration, which improves readability and reproducibility potential.\n3. Multimodal design leverages figures alongside text, which is valuable for technical domains with schematics and tables.\n4. Practical scale demonstrated: expansion from about 170 seed classes to thousands of classes and many millions of instances, showing the approach can run at scale.\n5. Reasoning-based consistency checks provide structure and constraint validation beyond pure language-model extraction.\n6. Initial parameter study comparing fine-tuned versus one-shot prompting and temperature settings shows attention to stability."}, "weaknesses": {"value": "1. Evaluation is not adequate for the core claims. There is no expert-curated gold set and no triple-level precision/recall or ontology-level error taxonomy. Automatic similarity scores alone do not establish correctness.\n2. No competitive baselines are included against established ontology enrichment tools or strong text-only extractors plus alignment. This leaves the value of multimodality, retrieval and fine-tuning unquantified.\n3. Missing ablations for each component. The paper claims value from retrieval, images and fine-tuning, but does not quantify each componentâ€™s marginal contribution.\n4. Domain generality is asserted but not shown. Results are limited to one domain. \n5. Reported scale for instances and classes lacks quality controls. No sampling audits, curator checks or constraint violation summaries are provided, so growth may include duplicates or noise.\n6. The role of images is not sufficiently analyzed. The share of triples that need visual evidence and which relation types benefit remain unclear.\n7. Reproducibility gaps. No released code, prompts, model checkpoints or ontology dumps, and limited reporting on compute cost and runtime make it hard to verify claims or adopt the method.\n8. Novelty is partly diluted by overlap with prior lines of work and by visible self-referencing patterns that may risk double-blind compliance.\n9. Metric choices are weakly justified. The mapping from graph-similarity or n-gram style metrics to human-judged accuracy is not established, and there are no confidence intervals."}, "questions": {"value": "1. What is the measured gain from retrieval, images and fine-tuning relative to a strong text-only baseline? Please report triple-level precision, recall and F1 on an expert gold set for each configuration.\n2. Did domain experts audit a stratified sample of triples, classes and relations? Please share sample size, agreement and an error taxonomy.\n3. What fraction of correct triples requires visual evidence? Which relation types benefit most? Please provide examples where images change the decision.\n4. Could you add baselines against competitive ontology-engineering pipelines and text-only triple extractors with ontology matching? Iw would like to see a report on extraction quality and end-to-end coherence.\n5. Could you provide evidence to substantiate the domain-agnostic claims?\n6. For the reported scale, please provide sampling-based precision estimates, duplicate rates and constraint violation counts and show distributional diagnostics for classes and properties.\n7. Could you complement similarity-based metrics with triple precision/recall, relation-typing accuracy, constraint violation counts? The study would also benefit from a downstream task such as SPARQL-query success or retrieval over the enriched ontology.\n8. Do you even intend to release code, prompts and at least a redacted subset of the enriched ontology? Which document model versions, licences and compute were used?\n9. Could you provide one concrete downstream use-case that improves due to enrichment and quantify that improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ms4wkgtMDt", "forum": "FLyvCS6eJf", "replyto": "FLyvCS6eJf", "signatures": ["ICLR.cc/2026/Conference/Submission13929/Reviewer_oqR1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13929/Reviewer_oqR1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944535924, "cdate": 1761944535924, "tmdate": 1762924432930, "mdate": 1762924432930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework on enriching ontologies using LLMs with a use-case on 4D printing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- the ontology enrichment framework based on textual and multimodal information from research articles which seems interesting."}, "weaknesses": {"value": "- The details about the framework could be given in the abstract to strengthen the interest of the reader.\n- There is a whole introduction on LLMs in the second paragraph which is not necessary. \n- Paragraph 3 talks about interpretability, are you focusing on interpretability.\n- The authors talk about fine-tuning on page 4 which doesn't need an introduction here, there could be a separate section on this.\n- Page 5 could use an example prompt from the dataset.\n- Why exactly the authors chose the 4D printing as a scenario, if the authors mean to generalize the framework for any use-case, 2-3 related use-cases could be introduced.\n- There is no evaluation of the generated ontology or the downstream task on the use of the ontology generated.\n- The paper is quite verbose at times with unnecessary details. \n- Many acronyms were repeatedly introduced since LLM was used for grammar correction possibly."}, "questions": {"value": "- Why exactly the authors chose the 4D printing as a scenario, if the authors mean to generalize the framework for any use-case, 2-3 related use-cases could be introduced.\n- See other comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QerEdURFHO", "forum": "FLyvCS6eJf", "replyto": "FLyvCS6eJf", "signatures": ["ICLR.cc/2026/Conference/Submission13929/Reviewer_J8DD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13929/Reviewer_J8DD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092059102, "cdate": 1762092059102, "tmdate": 1762924432375, "mdate": 1762924432375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}