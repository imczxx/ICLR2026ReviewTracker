{"id": "DxAq2F0Sv9", "number": 16196, "cdate": 1758261385464, "mdate": 1759897255189, "content": {"title": "Provably Convergent and Private Distributed Optimization via Smoothed Normalization", "abstract": "Federated learning enables training machine learning models while preserving the privacy of participants. Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems with convergence guarantees. The reason is that standard privacy techniques require bounding the participants' contributions, usually enforced via clipping of the updates. Existing literature typically ignores the effect of clipping by assuming the boundedness of gradient norms or analyzes distributed algorithms with clipping, but ignores DP constraints. In this work, we study an alternative approach via *smoothed normalization* of the updates, motivated by its favorable performance in the single-node setting. By integrating smoothed normalization with an Error Compensation mechanism, we design a new distributed algorithm $\\alpha$-NormEC. We prove that our method achieves a superior convergence rate over prior works. By extending $\\alpha$-NormEC to the DP setting, we obtain the first differentially private distributed optimization algorithm with provable convergence guarantees. Finally, our empirical results from neural network training indicate robust convergence of $\\alpha$-NormEC across different parameter settings.", "tldr": "We design and analyze the first differentially private distributed optimization method with provable convergence guarantees.", "keywords": ["distributed optimization", "private learning", "smoothed normalization", "clipping", "error feedback", "differential privacy"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ef500c7ce86df026403d443cc3099cb049b3fa0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes $\\alpha$-NormEC, which integrates smoothed normalization and error compensation for advanced federated learning (FL), and its private variant DP-$\\alpha$-NormEC. While the algorithmic components are combinations of known techniques, the paper organizes several theoretical contributions. Under only the L-smooth assumption (i.e., without using bounded gradient norms or bounded data heterogeneity assumptions), it establishes for non-convex, smooth problems: (i) convergence of $\\alpha$-NormEC, and (ii) provable convergence of DP-$\\alpha$-NormEC in the private setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**(S1) Clear presentation**\nThe major contributions are clearly summarized in Section 1.1 and Table 1, and the exposition remains consistent throughout.\n\n**(S2) Sufficient theoretical contributions**\n\n(S2a) Convergence of gradient norms for L-smooth non-convex objectives using $\\alpha$-NormEC (Theorem 1). The key appears to be Lemma 2 and an appropriate choice of parameters $(\\alpha,\\beta,\\gamma)$, which together bound the norm of the difference between the full gradient and the stochastic gradient.\n\n(S2b) Convergence of gradient norms for L-smooth non-convex objectives in the private setting using DP-$\\alpha$-NormEC (Theorem 2). Given the private setting and the minimal assumptions, this is likely among the first provable convergence results of this setting.\n\n**(S3) Sufficient discussion against baselines**This paper discusses how the obtained results (Theorems 1 and 2) compare with baselines (e.g., Clip21, EF21), which helps clarify the contribution of this paper."}, "weaknesses": {"value": "**(W1) Are the convergence rates tight? (Theorem 1)**\nDeriving convergence rates without any bounded heterogeneity assumption seems elegant; however, empirical FL behavior often depends strongly on data heterogeneity. Hence, the presented rates may not be tight in practice. A heterogeneity-aware convergence rate could plausibly be tighter.\n\n**(W2) Concerns about initialization (Theorem 2)**\nMy understanding is that the key point in Theorem 2 is in the initialization (e.g., $g_i^0$ and parameters $\\beta,\\gamma$) to reduce the additive term introduced by the privacy setting. Since I believe this is important, it would be better to include a concise recipe in the main paper. I skimmed C.1.1, but it remained somewhat opaque. Could you provide an additional explanation?\n\n**(W3) Limited experiments (Sec. 5)**\nWhile I recognize this paper’s main contributions are on the theoretical side, the experimental section feels limited.\n\n(W3a) Few benchmarks. The evaluation uses only CIFAR-10 with ResNet-20; can you validate on additional datasets/models?\n\n(W3b) Baselines. It seems comparisons to strong SOTA methods are missing in both non-private and private settings (e.g., SCAFFOLD and DP-SCAFFOLD).\n\n(W3c) Validation of data heterogeneity. Related to W1: I recommend that you demonstrate performance dependence on heterogeneity by controlling non-IIDness through the Dirichlet concentration parameter, which is a standard approach in FL."}, "questions": {"value": "(Comment 1) This is just a suggestion: since the proposed method (Sec. 4) appears from p.6 onward, Sections 1–2 could be made more concise, freeing space to (a) elaborate key proof ideas and initialization guidance, and (b) expand the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Q2HiLEkRj", "forum": "DxAq2F0Sv9", "replyto": "DxAq2F0Sv9", "signatures": ["ICLR.cc/2026/Conference/Submission16196/Reviewer_Qzmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16196/Reviewer_Qzmf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270843501, "cdate": 1761270843501, "tmdate": 1762926357329, "mdate": 1762926357329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a differentially private mechanism for server-assisted nonconvex optimization. The authors point out that existing approaches typically require gradient clipping to ensure differential privacy. To mitigate the effect of clipping-induced errors on convergence accuracy, they propose an approach that combines smoothed normalization and error compensation techniques, and achieves convergence to a neighborhood of a stable solution. In my opinion, the main contribution of this paper lies in removing the bounded gradient assumption commonly used in existing results.\n\nMy main concerns are as follows: \n\n(i) Overstated and misleading statements (the algorithm is only applicable to server-assisted optimization (e.g., federated learning) rather than distributed optimization; see Weakness 1 for details); \n\n(ii) Weak convergence results (the algorithm does not achieve accurate convergence, which has already been achieved in existing works; see Weaknesses 1 and 2 for deatils); \n\n(iii) Lack of differential-privacy analysis (see Weakness 3 for deatils);"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper removes the bounded gradient assumption used in existing differential-privacy results. The experimental evaluation is relatively thorough."}, "weaknesses": {"value": "1. **Overstated and misleading statements:** In the Abstract, the authors state: \"Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems with convergence guarantees.\" This statement (made without qualification) is overly exaggerated, as several approaches have already been proposed (see, e.g., [r1], [r2], [r3]) for fully distributed nonconvex optimization or fully distributed bilevel nonconvex optimization. Moreover, these works have achieved both accurate convergence and differential privacy.\n\n[r1] Chen J, Wang J, Zhang J F. Differentially private distributed nonconvex stochastic optimization with quantized communication. IEEE Transactions on Automatic Control, 2025 (arXiv version in 2024).\n\n[r2] Chen Z, Wang Y. Locally differentially private decentralized stochastic bilevel optimization with guaranteed convergence accuracy. Forty-first International Conference on Machine Learning. 2024.\n\n[r3] Yue X Y, Xiao J W, Liu X K, et al. Differentially private linearized ADMM algorithm for decentralized nonconvex optimization. IEEE Transactions on Information Forensics and Security, March, 2025.\n\nIn addition, the statements in the paper are quite misleading. For example, in the first paragraph of the Introduction and throughout the paper, terms such as \"distributed optimization\", \"distributed setting\", and \"distributed gradient methods\" are repeatedly used. From these descriptions, I was under the impression—up to page 6—that the authors were considering a fully distributed setting without any centralized server or aggregator. However, in Algorithm 1 on page 6, a centralized server is still required for computation and coordination, which contradicts the earlier claims as well as the title of the paper. \n\n2. **Weak convergence results:** Theorem 1 only proves that Algorithm 1 (without considering privacy constraint) converges to a neighborhood $O(2R+\\frac{L}{2}\\gamma)$ of a stable point to problem (1), rather than achieving accurate convergence. Following Theorem 1, the authors explain that, by choosing $g_{i}^{0}$, e.g., $g_{i}^{0}=\\nabla f_{i}(x^{0})+e$\nwith $e=(D/\\sqrt{K+1},0,\\ldots,0)$, they can ensure $R=\\max_{i\\in[1,n]}||\\nabla f_{i}(x^{0})-g_{i}^{0}||=D(K+1)^{-\\frac{1}{2}}$, and hence, obtain a convergence guarantee. However, it should be noted that $g_{i}^{0}$ is an initialization in Algorithm 1, which is typically a pre-selected constant. In this case, $K$ is also a pre-determined constant and cannot tend to infinity, thereby failing to ensure accurate convergence. Similar issues also arise in Theorem 2, which analyzes the convergence of Algorithm 1 under privacy constraints. What's worse, the optimization error established in Theorem 2 grows to infinity as the number of iterations $K$ tends to infinity (with an increasing rate of $O(K)$).\nTherefore, the authors' claims of  \"convergence guarantee\" and \"$\\alpha$-NormEC achieves $(\\varepsilon,\\delta)$-DP and comes with convergence guarantees\" are unconvincing.\n\n3. **Lack of differential-privacy analysis:** Although the authors cite relevant literature (e.g., Abadi et al. (2016)), differential privacy (which is one of the main focuses of this paper) is not analyzed anywhere in the manuscript, including the Appendix. This omission makes the paper not self-contained."}, "questions": {"value": "See the weaknesses above. In addition, I have the following questions:\n\n1. In Algorithm 1, the authors also require the injected Gaussian noise to have bounded variance, which renders the statement on page 2 self-contradictory. (See the statement \"While the method has been studied in the single-node setting, the convergence results rely on unrealistic and/or restrictive assumptions, such as symmetric gradient noise (Bu et al., 2024) and almost sure bounds on the gradient noise variance\" on page 2). \n\n2. Theorem 1 only proves convergence to a neighborhood of a stable solution to problem (1). Following Theorem 1, the authors state that \"By proper choices of parameters\", Algorithm 1 can achieve accurate convergence. However, it is unclear which parameters are being referred to and how they should be tuned?\n\n3. Theorem 2 holds only when $\\beta=\\frac{1}{K+1}$ (see Corollary 2), which is important and should be explicitly clarified in the theorem statement.\n\n4. In experimental setups (in Appendix on page 29), the authors simply state that \"The train samples were randomly shuffled and distributed across 10 workers.\" However, random shuffling typically yields (near) IID data distribution. How is heterogeneity of the data distribution across workers/agents ensured?\n\n5. Since Assumptions 1–2 already ensure the Lipschitz continuity of each objective function $f_{i}$, the latter part of Assumption 1–1) is redundant.\n\nIf the authors could avoid the inaccurate statements and address Weaknesses 2 and 3, I would consider raising the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ms090PLoTj", "forum": "DxAq2F0Sv9", "replyto": "DxAq2F0Sv9", "signatures": ["ICLR.cc/2026/Conference/Submission16196/Reviewer_7Nk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16196/Reviewer_7Nk8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517371747, "cdate": 1761517371747, "tmdate": 1762926356967, "mdate": 1762926356967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes α-NormEC, a distributed optimization algorithm that integrates smoothed normalization with the EF21 error-feedback mechanism. The method addresses convergence challenges in non-convex, distributed, and differentially private (DP) optimization without assuming bounded gradients. Theoretical results show sublinear convergence in both non-private and private settings, including the provable utility bound for DP distributed optimization. Some experiments on CIFAR-10 with ResNet-20 and comparison between DP-SGD and DP-Clip21 are also included."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an integration of smoothed normalization and EF21 that enables convergence without bounded-gradient assumptions. Theoretical analysis is sound and avoids restrictive conditions. The induction-based proof for state-dependent contractive operators is technically elegant. α-NormEC achieves provable convergence in the DP distributed setting."}, "weaknesses": {"value": "1. There are already several existing works on generalizing clip21 into a DP version, for example \"Double Momentum and Error Feedback for Clipping with Fast Rates and Differential Privacy\". The proposed structures are almost identical to that shown in this paper except the final normalization step performed by the server. And the theoretical role of server normalization (SN) is not fully analyzed. While SN sometimes stabilizes training empirically, there is no clear theoretical justification or guidance on when it should be applied. From the provided the experiments, the normalization seems to compromise the performance. \n\n2. There is a minor inconsistency in Assumption 1: if each local function fi is Li-smooth, then their average f is also L-smooth with L = (1/n)∑Li, so it is not necessary to restate the Lipschitz condition for f separately—only boundedness of f needs to be assumed.\nLine 187-188, the sentence “Notice that the DP Gaussian noise variance (3) is scaled with the sensitivity Φ” is inaccurate. According to the standard DP-SGD formulation, the standard deviation, not the variance, scales linearly with Φ. Clarifying this would improve precision.\n\n3. The paper also lacks comparison with SOTA DP-SGD benchmark on CIFAR10, e.g., \"unlocking high-accuracy differentially private image classification through scale\", \"differentially private learning needs better features (or much more data)\", \"a theory to instruct differentially-private learning via clipping bias reduction\". I cannot see clear privacy-utility tradeoff from the experiments. For example, I cannot find the test accuracy achieved by $(\\epsilon=8, \\delta=10^{-5})$.  \n\n4. In addition, the visualization quality could be improved. In several figures, the dashed and solid curves (e.g., Figure 2) are difficult to distinguish, which affects readability."}, "questions": {"value": "1. There are already several existing works on generalizing clip21 into a DP version, for example \"Double Momentum and Error Feedback for Clipping with Fast Rates and Differential Privacy\". The proposed structures are almost identical to that shown in this paper except the final normalization step. Can the authors compare these results and explain how much the normalization can help, .e.g., under what precise conditions? \n\n2. The convergence rate in Theorem 3 seems independent of α once normalization saturates. How should we intuitively understand this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N2pUU1WsX4", "forum": "DxAq2F0Sv9", "replyto": "DxAq2F0Sv9", "signatures": ["ICLR.cc/2026/Conference/Submission16196/Reviewer_DZsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16196/Reviewer_DZsE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897159423, "cdate": 1761897159423, "tmdate": 1762926356631, "mdate": 1762926356631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses smoothed normalization with an error compensation mechanism and proposes a new algorithm, DP–NormEC, for the differentially private setting. The authors aim to mitigate the negative effects of clipping in DP-SGD by replacing the clipping operation with a smoothed normalization step, which they claim provides better convergence behavior. They derive convergence guarantees for both the non-private and private versions of the algorithm and provide empirical results on CIFAR-10 to support their claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is relevant, as gradient clipping remains a key challenge for differential privacy in optimization and federated learning.\n- The idea of combining smoothed normalization with error compensation is reasonable, and the theoretical analysis is presented clearly."}, "weaknesses": {"value": "- The novelty is limited. In the non-private case, DP–NormEC appears to follow from EC21 once smooth normalization is assumed contractive, so I am unsure that it matches the level required for ICLR paper.\n- The paper lacks a formal privacy analysis, despite presenting itself as a differentially private algorithm. The seven in appendix only provide big O guarantees which is not enough for the different privacy results\n- The federated learning setting is not convincingly addressed: only one-step updates per agent are considered, which limits both the technical contribution and the generality of the approach.\n- Both the clipping threshold ($\\tau$) and smoothing parameter ($\\alpha$) are sensitive. Figures 3 and 15 show that performance depends strongly on $\\alpha$, yet the paper suggests that tuning normalization is easier than clipping without clear justification, whereas it is a key point to motivate the method\n- The experimental validation is weak: results are limited to CIFAR-10, seemingly from a single run, with no error bars or statistical analysis.\n- The server normalization step can degrade utility when not needed, which is not discussed."}, "questions": {"value": "- Can you compare your work to *\"Double Momentum and Error Feedback for Clipping with Fast Rates and Differential Privacy\", Rustem Islamov, Samuel Horvath, Aurelien Lucchi, Peter Richtarik, Eduard Gorbunov*\n- Can you provide explicit privacy guarantees and clarify how the noise is calibrated?\n- Why is tuning $\\alpha$ expected to be easier than tuning $\\tau$, given its sensitivity in experiments?\n- How would the approach extend to more realistic federated settings with multiple local updates or partial participation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JLSJUyHjJz", "forum": "DxAq2F0Sv9", "replyto": "DxAq2F0Sv9", "signatures": ["ICLR.cc/2026/Conference/Submission16196/Reviewer_Dc5T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16196/Reviewer_Dc5T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16196/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901534313, "cdate": 1761901534313, "tmdate": 1762926356165, "mdate": 1762926356165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}