{"id": "N0d6tG377V", "number": 21613, "cdate": 1758319645008, "mdate": 1759896912444, "content": {"title": "Improving and Evaluating Open Deep Research Agents", "abstract": "Deep Research Agents (DRAs) are systems that can take a natural language prompt from a user and autonomously search for and utilize internet-based content to address it. Recent DRAs have demonstrated impressive capabilities on public benchmarks, but most research has focused on proprietary, closed-source systems. At the time of this work, we identified only one open-source DRA, Open Deep Research (ODR). To enable systematic comparison, we adapt the challenging BrowseComp benchmark and introduce BrowseComp-Small (BC-Small), a computationally tractable subset designed for academic labs. We benchmark ODR and two proprietary systems from Anthropic and Google on BC-Small, finding that all three achieve 0% accuracy on the 60-question test set. We then propose ODR+, an enhanced version of ODR with sub-question decomposition, iterative planning, and structured synthesis. ODR+ achieves 10% accuracy on BC-Small—state-of-the-art among both open-source and closed-source systems under evaluation. Ablation studies confirm that all three improvements contributed to ODR+’s performance.", "tldr": "We present ODR+, an open‑source deep research agent that autonomously answers complex web questions using LLMs, subquestion decomposition, iterative search, and structured answer synthesis.", "keywords": ["Deep Research Agents", "Autonomous Web Search", "Multi-hop Question Answering", "Large Language Models", "Open-source Benchmarks", "BrowseComp", "ODR+", "Information Retrieval and Synthesis"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d33b04f0d7880298fd90aafc2c36a12a577c2e5d.pdf", "supplementary_material": "/attachment/391b20afe087810a25edd26f96200e718721c26d.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a BC-Small benchmark and an ODR+ system. BC-Small is the sampling of the existing BrowseComp benchmark, and ODR+ extends the existing Open Deep Research agent with question decomposition, sub-solution search, and response synthesis steps."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed ODR+ system improves the original ODR agent on the proposed small-scale benchmark BC-Small.\n2. The ablation studies on a subset of 20 questions show the effectiveness of each ODR+ component."}, "weaknesses": {"value": "1. The proposed BC-Small benchmark is merely a sub-sampling of the existing BrowseComp benchmark, and the sampling criteria are mainly (1) preserving the original topic distribution, and (2) reducing to a smaller scale. Without carefully studying the chosen samples, the contribution appears weak.\n2. The key ideas of the proposed ODR+ system are question decomposition and sub-problem solving, but this line of research has been studied a lot. To name a few: [1,2,3,4].\n3. The effectiveness of the proposed ODR+ system is validated on BC-Small, which is also proposed in this paper, and is a small-scale sampling of an existing benchmark. Thus, the empirical evidence appears not sound and generalizable to me. The same goes for the ablation study.\n4. The paper presentation has room to improve.\n    - Typos such as Line 045 \", however recent\" $\\to$ \". However, recent\" and Line 049 \"Therefore DRA\" $\\to$ \"Therefore, DRA\"\n    - The first half of Section 3 and the whole Section 4 are not informative, as they only describe the existing work.\n    - Figure 1 does not look informative or illustrative. Also, Figure 1 and Figure 2 can be combined to illustrate the proposed ODR+ system.\n    - Do not use `\\citet{}` if the authors names are not meant to be part of the text.\n\n- [1] Min et al. 2019. Multi-hop Reading Comprehension through Question Decomposition and Rescoring\n- [2] Perez et al. 2020. Unsupervised Question Decomposition for Question Answering\n- [3] Huang et al. 2023. Question Decomposition Tree for Answering Complex Questions over Knowledge Bases\n- [4] Press et al. 2023. Measuring and Narrowing the Compositionality Gap in Language Models"}, "questions": {"value": "1. How exactly is the sampling conducted in constructing BC-Small?\n2. See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DmU8FZTCIP", "forum": "N0d6tG377V", "replyto": "N0d6tG377V", "signatures": ["ICLR.cc/2026/Conference/Submission21613/Reviewer_agiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21613/Reviewer_agiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543748127, "cdate": 1761543748127, "tmdate": 1762941855727, "mdate": 1762941855727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes two core contributions:\n\n1.Propose the BrowseComp-Small (BC-Small) benchmark: This is a more computationally-tractable DRA benchmark comprising a subset of 120 questions from the challenging BrowseComp benchmark. Importantly, the authors split BC-Small into a training set (60 questions) and a testing set (60 questions), enabling open-source development and performance evaluation.\n\n2.Propose the ODR+ system: After finding that the existing (and at the time, only) open-source DRA—ODR—was unable to answer any of the questions (achieving 0% accuracy) on the BC-Small test set , the authors proposed ODR+. ODR+ addresses the limitations of ODR by introducing three strategic improvements : (1) Question Decomposition , (2) Iterative Sub-solution Search , and (3) Response Synthesis.\n\nOn the BC-Small test set, ODR+ achieves a 10% success rate , greatly outperforming the original ODR baseline (0%). Surprisingly, ODR+ also outperformed the two proprietary DRAs tested: Claude-DR and Gemini-DR, both of which achieved 0% accuracy. Finally, the paper reports ablation studies indicating that all three of the proposed improvements contributed to the success of ODR+."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper correctly identifies and addresses the core problem hindering open-source DRA development: the lack of an available baseline and benchmark .\n\n2.The paper does an excellent job in reproducibility , providing pseudocode (Algorithm 1) , prompt summaries (Table 1) , evaluation methodology , and hyperparameters , which is critical for subsequent research.\n\n3.The paper is well-organized and clearly written. The authors clearly articulate a complex problem and present a logically sound solution. The figures in the paper are particularly excellent."}, "weaknesses": {"value": "1.The paper claims that ODR+ \"achieves the current state-of-the-art (SOTA) performance on the BrowseComp benchmark among open-source models\". This is a very misleading claim because the only open-source model it was experimentally compared against was the original ODR. ODR's accuracy was 0%, which is an extremely low baseline; beating this alone is not sufficient to claim \"SOTA\". The authors mentioned other contemporary open-source DRAs in their related work section, such as DeepResearcher and WebThinker , but explicitly admitted that ODR+ was not compared against them, stating, \"Given our limited compute budget... we were unable to obtain reliable runs suitable for benchmarking. We therefore restricted our comparisons to ODR, ODR+...\"\n\n2.The author states that all experiments (for ODR and ODR+) rely on a specific model, GPT-4o-mini. This introduces a potential confounding variable: could it be that the model's own reasoning ability is insufficient? A more robust experimental design should include testing the performance of different LLMs on the ODR+ architecture to separate the \"architecture's contribution\" from the \"underlying model's contribution\"."}, "questions": {"value": "1.In the 90% (54/60) of cases where ODR+ failed, what was the primary failure mode? Your ablation study shows all modules are necessary (disabling one drops accuracy to ~0%), but this doesn't identify the primary bottleneck in the complete system.Could the authors provide a failure attribution analysis for the 54 failed cases? Specifically, what percentage of failures were due to: (a) Module 1 (Decomposition) errors? (b) Module 2 (Search) failing to retrieve info? (c) Module 3 (Synthesis) failing to synthesize the answer from correct info?This analysis is critical for guiding future research.\n\n2.Is the system's high 90% failure rate a flaw in the ODR+ architecture, or is it caused by the insufficient reasoning ability of the underlying model (GPT-4o-mini)? Did the authors run experiments on the ODR+ architecture using more powerful models (e.g., GPT-5 or Gemini 2.5 Pro) as the backbone LLM? If so, what was the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QXaJqR8lya", "forum": "N0d6tG377V", "replyto": "N0d6tG377V", "signatures": ["ICLR.cc/2026/Conference/Submission21613/Reviewer_W8Yb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21613/Reviewer_W8Yb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729720483, "cdate": 1761729720483, "tmdate": 1762941855386, "mdate": 1762941855386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies “Deep Research Agents” (DRAs)—LLM-driven systems that browse the web to answer multi-hop questions—and proposes ODR+, an improved open-source agent built on Open Deep Research (ODR). The authors also curate BrowseComp-Small (BC-Small), a 120-question subset of BrowseComp split into 60 train and 60 test items to make evaluation more accessible. On BC-Small, baseline ODR and two proprietary agents (Anthropic “Claude-DR” and Google “Gemini-DR”) score 0%, while ODR+ achieves 10% exact-match accuracy on the 60-question test set. Ablations suggest three additions—question decomposition, iterative sub-solution search, and structured response synthesis—each matter; removing any one collapses performance on a 20-question subset. \n\nMethodologically, ODR+ enforces a fixed budget (max depth of 6, time limit of 210s), re-queries search 3 times to stabilize rankings, selects top 3 URLs by frequency, extracts constrained facts per sub-question, and synthesizes answers in the official BrowseComp format (Explanation / Exact Answer / Confidence). Evaluation follows the BrowseComp protocol using the released GPT-4o-based judge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical contribution: introduces a smaller, more accessible subset (BC-Small) for BrowseComp-style evaluation.\n\n2. Ablation sanity checks: removing any major component degrades performance, demonstrating each stage matters.\n\n3. Focused study on an important emerging topic (LLM-based deep research agents) that the community cares about."}, "weaknesses": {"value": "1. Statistical rigor is weak — 6/60 correct (10%) with no confidence intervals, variance estimates, or bootstrap analysis makes the result fragile.\n\n2. Baseline comparison fairness concerns — proprietary systems evaluated without structured-output wrappers and without similar tuning; zero-shot vs trained split mismatch. I also did not understand what \"tuning\" meant for their ODR+ system. \n\n3. Ablations too limited — only on a 20-question subset; no partial ablations or sensitivity studies (e.g., varying query count, top-k URLs).\n\n4. LLM judge dependence — results rely entirely on GPT-4o judgments with no human spot-check or inter-rater agreement measurement.\n\n5. Limited external validation — no testing beyond BC-Small (e.g., Mind2Web2 / Deep Research Bench), so generalization remains unclear.\n\n6. Minimal qualitative insight — few concrete traces of success and failure; unclear where the system fails (bad sub-questioning? retrieval drift? synthesis error?).\n\n7. Absolute performance still extremely low — 10% accuracy leaves open whether method truly “works” beyond cherry-picked improvements."}, "questions": {"value": "1. Can you report 95% confidence intervals and/or bootstrap CIs for the 6/60 result? Is the improvement statistically meaningful?\n\n2. How sensitive is performance to judge randomness? Have you tested multiple GPT-4o seeds or a smaller human-audited subset?\n\n3. Can you provide a format-coercion wrapper for Claude-DR/Gemini-DR and rerun baselines to rule out formatting disadvantage?\n\n4. How was BC-Small sampled? Was there stratification by task type or difficulty? Will you release IDs and reconstruction instructions?\n\n5. Could you run partial ablations (e.g., keep decomposition but remove re-querying) across the full test set?\n\n6. Any results on other datasets (e.g., Mind2Web2) to verify generalization beyond BrowseComp?\n\n7. Please share more qualitative traces showing where failures arise (retrieval? decomposition? assertion selection?).\n\n8. Did you measure judge agreement rates between GPT-4o and humans (or GPT-4o at different prompts/seeds)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGoZEyMI3l", "forum": "N0d6tG377V", "replyto": "N0d6tG377V", "signatures": ["ICLR.cc/2026/Conference/Submission21613/Reviewer_FDp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21613/Reviewer_FDp2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885531615, "cdate": 1761885531615, "tmdate": 1762941854980, "mdate": 1762941854980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrowseComp-Small - a subset of the challenging BrowseComp dataset. Specifically, benchmarking Open Deep Research (ODR) alongside proprietary deep research agents from Anthropic and Google shows that all achieve 0% accuracy initially on BroseComp-Small. The paper then introduces three key improvements, creating ODR+, which reaches a 10% success rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes three key strategies to improve the Open Deep Research (ODR) system.\n2. The paper will release the code for ODR+ to support continued progress."}, "weaknesses": {"value": "1. Details are not provided on how this smaller subset of BrowseComp-Small is selected. The original BrowseComp contains information such as distribution of topics of the test set, human performance on questions etc. Such details are missing for BrowseComp-Small  which makes it hard to understand which skills are being tested by BrowseComp-Small and what it means to have 0% accuracy on this set.\n2. The paper argues that BrowseComp-Small is a smaller and more practical subset of BrowseComp but comparison with the whole subset in terms of time, compute and cost needed is missing. How much less compute / time is needed by BrowseComp-Small?\n3. Comparison is also missing with other benchmarks such as Deep Research Bench and Mind2Web2 in terms of benchmark size, difficulty, time and compute needed, skills tested etc."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UfiITpO1pI", "forum": "N0d6tG377V", "replyto": "N0d6tG377V", "signatures": ["ICLR.cc/2026/Conference/Submission21613/Reviewer_FxaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21613/Reviewer_FxaA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21613/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132578638, "cdate": 1762132578638, "tmdate": 1762941854291, "mdate": 1762941854291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}