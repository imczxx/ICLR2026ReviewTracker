{"id": "hNejGT1d6d", "number": 9933, "cdate": 1758150102241, "mdate": 1759897684619, "content": {"title": "Mixup for Survival Analysis", "abstract": "Survival analysis with censored outcomes underpins risk prediction in domains such as medicine and engineering. While deep survival models capture complex input-output relationships, they may benefit from vicinal risk minimization techniques such as mixup, which have proven effective as simple, model-agnostic data augmentation methods. We introduce a mixup for survival analysis, called H-Mixup, a principled framework that adapts mixup to censored time-to-event data by defining augmentation strategies that yield interpolated outcomes consistent with valid survival trajectories, encouraging local linearity in hazard functions. Theoretically, we show that H-Mixup contracts empirical Rademacher complexity and can tighten generalization bounds, while noting that the overall bound still depends on vicinal bias, which varies with alignment between mixing assumptions and the underlying risk structure. Empirical results on semi-synthetic and real-world datasets suggest that H-Mixup improves predictive performance for deep survival models. Overall, this study addresses the gap between mixup and survival analysis, providing a general recipe for vicinal regularization via simple data augmentation in time-to-event modeling.", "tldr": "We propose a mixup strategy for survival analysis.", "keywords": ["mixup", "survival analysis", "time-to-event data", "vicinal risk minimization", "vrm"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9148308c246d482c827c1209080cb75e5abe171f.pdf", "supplementary_material": "/attachment/5627a482957fa95cb92f3138fe9b49ff7221a9f7.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims at solving the overfitting and poor generalization of the deep learning survival analysis, using mixup -- a type of vicinal risk minimization. The paper proposes H-Mixup, for survival analysis that interpolates covariates and hazard function through data augmentation.  Theoretically, the paper claims the local linearity in the hazard function and tighter generalization bounds than empirical risk minimization (ERM). Empirically, the method demonstrates superior performance on semi-synthetic datasets, and similar performance with ERM on real datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is the first work that aims to mitigate the sharp decision boundary issues in survival analysis."}, "weaknesses": {"value": "I find the paper fundamentally flawed in three aspects: (1) an unconvincing and even misleading motivating example, (2) incorrect theorem and proof, and (3) weak empirical evidence. I elaborate on these points below.\n\n---\n## Flawed Motivation and Misleading Example\n\nThe motivating example in Figure 1 is not convincing. While the figure visually shows H-MixUp producing a smooth decision boundary between high- and low-risk samples, this effect is largely an artifact of an unrealistically extreme setup. If one carefully reads Appendix A, the two groups’ hazard functions -- both constant -- differ by several orders of magnitude:\n- Low-risk group: $h(t) = 0.25*\\exp(3)\\approx 5$\n- High-risk group: $h(t) = 0.25*\\exp(10)\\approx 5507$\n\nThat is more than a **thousand-fold difference**. Meanwhile, the censoring hazard is a negligible 0.18, meaning virtually no censoring occurs. Under such a large signal gap, a sharp decision boundary is expected and not problematic -- indeed, this is what ERM yields.\n\nPlotting the survival functions makes the issue clearer: for the high-risk group, the survival probability essentially collapses to zero immediately after $t>0$ (e.g., $S(t=0.001) = \\exp(-5507* 0.001)\\approx 0.004$). The ERM prediction correctly reflects this -- darkest regions align with high-risk samples. In contrast, the H-MixUp panel misplaces the darkest regions (and colors the high-risk region as not-so-dark), indicating potential **miscalibration**. In fact, the naive interpolation seems to outperform H-MixUp here.\n\nThis raises a deeper concern: if a model is already well-calibrated, why should we prefer a smooth hazard or survival boundary over a sharp one? Since the counterfactual outcomes in the interpolated regions are **unobservable**, there is no principled reason to favor smoothness. If the true boundary is sharp, enforcing smoothness is actively harmful. This undermines the entire motivational basis of the paper.\n\n---\n## Incorrect and Non-Substantive Theoretical Results\n\nTheorem 1, the core theoretical result, is both **incorrect in conclusion** and **incorrectly derived**.\n\nFirst, the stated conclusion (Eq. 8) is not what one would want:\n\n$h(t \\mid \\tilde{x}) = \\lambda h(\\underline{\\lambda} t \\mid x) + (1 - \\lambda) h(\\underline{(1-\\lambda)} t \\mid x')$ \n\ni.e., a convex combination of hazards *at earlier times* (the two underlined terms). But the intended property should be\n\n$h(t \\mid \\tilde{x}) = \\lambda h(t \\mid x) + (1 - \\lambda) h(t \\mid x')$\n\na convex combination **at the same time point**. The proven statement is thus irrelevant to the algorithm’s desired behavior.\n\nSecond, the proof itself is incorrect. In line 795 of the Appendix, the mixup time is represented as $\\min(\\frac{T}{\\lambda}, \\frac{T'}{1-\\lambda})$ -- using event times. However, in the definition (Eq. 8), the mixup involves $\\min(\\frac{O}{\\lambda}, \\frac{O'}{1-\\lambda})$  -- **observation times**. This is not a subtle difference: plugging in the correct definition breaks the derivation since $P(O > \\lambda t) \\not= S(\\lambda t \\mid x)$ and $P(O' > (1 - \\lambda) t) \\not= S((1-\\lambda) t \\mid x')$ in line 801. The subsequent steps simply do not hold.\n\nMoreover, Theorem 2 is stated with minimal justification. The authors essentially port existing Mixup theory to the survival setting without showing why the log-likelihood or partial likelihood is L-Lipschitz, which is not trivial. I think it requires some extra assumption for this property to hold. \n\n---\n## Weak and Selective Empirical Evidence\n\nWhile the method performs adequately on synthetic image-based datasets, its real-world performance tells a different story. Table 8 (buried at the end of the Appendix) shows that across 36 comparisons, standard ERM actually outperforms H-MixUp 18 times and ties once. These results contradict the strong claims in the main text about “consistent improvements” (line 410).\n\nIt seems that the proposed methods (and the other Mixup methods) mostly work on image datasets, yet the authors provide no evidence on real image-based survival datasets, or any synthetic datasets which is not derived from MNIST. \n\nMore importantly, the comparison with a naive interpolation baseline is missing. Without it, it is unclear whether the gains (if any) are due to the proposed mixup strategy or simply from data (feature) augmentation."}, "questions": {"value": "Beyond the three major weaknesses discussed above, I also have several additional concerns:\n1. The paper claims (line 193) that the mixed observation times “tend to skew a bit later, ..., but some will still be early.” However, since $0<\\lambda<1$, both rescaled observation times $\\frac{o}{\\lambda}$ and $\\frac{o'}{1-\\lambda}$ must be strictly larger than the original observed times. It is therefore unclear how the resulting mixup time could ever be earlier than either of the original observation times. \n2. It is difficult to grasp the key implications of Theorem 2 by reading Section 3 alone. If the authors choose to present the formal statement of the theorem in the main text, they should at minimum explain all the terms, assumptions, and their significance directly in that section rather than deferring entirely to the appendix. As written, it is unnecessarily opaque.\n3. The ECE metrics for evaluating calibration: how does it handle censoring?\n4. In Section 2, non-informative censoring is defined as $T \\perp C \\mid X$, while in lines 1304-1305, it is defined as censoring time is independent of both progrnostic factors and true failure times. Leads to contradictory definitions.\n5. Section 5 introduces two alternative mixup strategies, but they are neither theoretically justified nor empirically evaluated. If these variants are intended to strengthen the paper’s contribution, the lack of corresponding experiments undermines that goal; if not, they only add unnecessary noise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WNGUUV6VpB", "forum": "hNejGT1d6d", "replyto": "hNejGT1d6d", "signatures": ["ICLR.cc/2026/Conference/Submission9933/Reviewer_TFSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9933/Reviewer_TFSk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761182785588, "cdate": 1761182785588, "tmdate": 1762921383756, "mdate": 1762921383756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces H-Mixup, an adaptation of the Mixup data augmentation strategy for  censored survival outcomes. Standard Mixup interpolates inputs and labels to improve generalization, but this approach breaks down when applied to time-to-event data due to censoring constraints. H-Mixup resolves this by shifting the focus from observed censored outcomes to the latent event-time distribution by interpolates covariates and hazards through data augmentation. The method provides both theoretical guarantees (showing reductions in Rademacher complexity and tighter generalization bounds) and practical improvements across multiple benchmarks.\n\nOverall, this is a strong paper that effectively extends Mixup to censored survival data, bridging a theoretical and practical gap in deep time-to-event modeling. H-Mixup demonstrates clear performance improvements, sound theory, and broad applicability. The paper is well-executed and accessible, with only minor limitations related to hyperparameter sensitivity and robustness analysis"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  This is the first rigorous formulation of Mixup for censored survival data.\n\n2.  The paper establishes generalization bounds specific to survival modeling.\n\n3. Through comprehensive experiments, H-Mixup demonstrates consistent gains across synthetic and real-world datasets."}, "weaknesses": {"value": "1.  The method’s dependence on the alignment between mixing assumptions and true hazard structure.\n\n2.  It Requires tuning of the Beta distribution parameter α for stability.\n\n3.  Limited analysis of domain shifts."}, "questions": {"value": "1. Could the approach be extended to informative censoring scenarios, where censoring depends on covariates or risk, and how would it work? \n\n2. Could adaptive or learned mixing coefficients improve alignment between interpolated and true hazards?\n\n3. How sensitive is performance to the event/censor ratio correction scheme in highly imbalanced data?\n\n4. Another potential competitor to H-Mixup is generative modeling. How would its performance compare with that of H-Mixup?\n\nNote:  The reference for Zhong et al. (2021) is incorrect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IMQARZvR6f", "forum": "hNejGT1d6d", "replyto": "hNejGT1d6d", "signatures": ["ICLR.cc/2026/Conference/Submission9933/Reviewer_RLto"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9933/Reviewer_RLto"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519029585, "cdate": 1761519029585, "tmdate": 1762921383505, "mdate": 1762921383505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel, model-agnostic regularization approach for survival analysis by extending the mix-up technique to handle right-censored data—a challenge unique to survival settings. The authors further provide a theoretical analysis of the proposed strategy, deriving its generalization bound to justify its regularization effect. Experimental evaluations are conducted on semi-synthetic image-based survival datasets and real-world tabular datasets, where the proposed mix-up method is applied across various deep survival models with different architectures. The paper also compares the proposed approach against several existing mix-up variants adapted for survival analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Provides theoretical justification for the effect of applying mix-up on generalization performance.\n- Includes other variants, such as S-Mixup, for a more comprehensive comparison of the proposed mix-up strategy in survival analysis.\n- Applies the proposed mix-up strategy to a range of deep survival models with different architectures and output types, including those that predict conditional hazard functions or probability mass functions."}, "weaknesses": {"value": "Major comments:\n\n- The linearity derived from applying mix-up via Eq. (7) does not imply linearity in the time-to-event outcomes or the corresponding survival risks. Demonstrating the linearity of the conditional hazard function across different time steps is not a valid justification. For instance, proof of Vicinal Bias of H-Mixup is done assuming the linearity of hazard functions at the same time points, which misaligns with the proof given in Theorem 1. Moreover, Eq. (7) inflates the time-to-event outcomes, which would inevitably bias the estimator toward underestimating the risk for given samples. Please refer to Question 1 for further details. \n- While provided in Thoerem 2 and Proposition 1, experiment results on how the proposed methods behave with different censoring rates is not fully observed. \n- While the analysis on Vicinal Bias of H-Mixup is thoroughly done, the analysis focuses on the hazard function not the cumulative effect of survival outcomes (e.g., time-to-event outcomes or survival functions), which limits the understanding of the vicinal bias on the time-to-event outrcomes.\n- The current experiments primarily compare against the authors' own variants of Mixup. While this internal comparison is thorough, this comparison must be extended to include relevant state-of-the-art Mixup techniques from the regression domain, such as C-Mixup [A]. (A straightforward implementation treating the observed time $o$ as a standard regression target would be sufficient for this comparison.)\n\nReferences:\n[A] H. Yao et al., \"C-Mixup: Improving Generalization in Regression,\" NeurIPS 2022.\n\n\n\nMinor comments:\n- The application of the traditional mix-up to the survival target (Lines 152-162) is not appropriate. The event indicator $\\delta$ is a binary variable (event vs. censoring) and cannot be meaningfully interpolated. Instead, applying a naive interpolation to the observed time $o$ and how it would likely introduce bias and an unwanted smoothing effect on the risk predictions will be more motivating and sound.\n- In (7), how to draw \\lambda should be specified. \n- Showing the abbreviation for H-Mixup (presumably, hazards) could be helpful."}, "questions": {"value": "- (Validity of Theorem 1) The theorem claims that the H-Mixup data generation process inherently guarantees hazard linearity. However, Eq. (8) does not demonstrate the linearity of $h(t|\\tilde{x})$ with respect to the conditional hazards at the same time step, i.e., $h(t|x')$ and $h(t|x)$. It is unclear why the authors instead show linearity across different time steps, as this does not directly translate to linearity in the time-to-event outcomes. Furthermore, Eq. (7) appears to inflate the time-to-event outcome. For instance, when $\\lambda = 0.5$ and $\\delta = \\delta' = 0$, the equation yields $\\tilde{o} = 2o$ or $\\tilde{o} = 2o'$. It is unclear how this formulation contributes to learning a correct time-to-event predictor, as it would likely cause the model to underestimate the risk.\n- How does the analysis of the vicinal bias of H-Mixup extend to different types of survival models and outcome formulations? For instance, certain methods (e.g., DeepHit) do not explicitly estimate the hazard function, making the implications of the vicinal bias less straightforward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1AsLHCqrYr", "forum": "hNejGT1d6d", "replyto": "hNejGT1d6d", "signatures": ["ICLR.cc/2026/Conference/Submission9933/Reviewer_1sqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9933/Reviewer_1sqc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549858110, "cdate": 1761549858110, "tmdate": 1762921383275, "mdate": 1762921383275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an extension of the mixup strategy for data augmentation to the setting of survival analysis, where some of the outcomes are censored (with the assumption of non-informative censoring). They provide theoretical evidence of the linear behavior of the proposed approach, as well as a very thorough experimental evaluation on semi-synthetic image datasets, and on real-world tabular data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This work addresses a clear gap in the literature, that seems to prove really useful in practical applications, both for images and tabular data. The article is very clear and well-written. The experiments are very thorough, with several metrics covering all aspects of the evaluation."}, "weaknesses": {"value": "The experiments on images are solely for semi-synthetic data. There exists datasets with survival outcomes and images, like the TCGA. It would be interesting with respect to the censoring distribution."}, "questions": {"value": "1. How relevant is the expected calibration error for survival outcomes? There are several metrics of calibration for survival analysis, for example those implemented in the SurvivalEVAL package (Qi, Shi-ang, Weijie Sun, and Russell Greiner. \"SurvivalEVAL: A comprehensive open-source python package for evaluating individual survival distributions.\" Proceedings of the AAAI Symposium Series. Vol. 2. No. 1. 2023.). Would you add them to the results?\n2. For figures 2 and 3, are the points average over several experiments? it could be insightful to add error bars to assess the significativity of variations.\n3. Why is MNIST retina not in figures 2 and 3?\n4. reference \"Haiyong Cui, Fan Zhang, Hui Fan, and Na Xu. Mulitdeepsurv: survival analysis of gastric cancer\nbased on deep learning and multimodal data. PMC, 2024.\" seems to be with the wrong authors and year and journal (or wrong title). Can you correct that and check carefully the other references?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oL7paz3iPh", "forum": "hNejGT1d6d", "replyto": "hNejGT1d6d", "signatures": ["ICLR.cc/2026/Conference/Submission9933/Reviewer_488W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9933/Reviewer_488W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948315204, "cdate": 1761948315204, "tmdate": 1762921382776, "mdate": 1762921382776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}