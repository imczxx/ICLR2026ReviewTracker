{"id": "kefUcn22mk", "number": 20373, "cdate": 1758305262387, "mdate": 1759896981293, "content": {"title": "Evaluating Language Models' Evaluations of Games", "abstract": "Reasoning is not just about solving problems---it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over 100 novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more \"jaggedness\" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.", "tldr": "Reasoning is not just about solving new problems, but deciding what problems to solve in the first place. We should evaluate LMs on evaluation.", "keywords": ["game AI; meta-reasoning; cognitive science; problem evaluation"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65bda83b42471998346f4cf188c24992822feb13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel AI evaluation paradigm that shifts from assessing problem-solving to evaluating problem evaluation capabilities. The study uses 121 novel board games (Tic-Tac-Toe variants) to assess language and reasoning models across two dimensions: expected payoff (fairness) and funness. The authors compare multiple models (GPT-4, DeepSeek-v3/R1, Gemini 2.5, o1, o3, GPT-5) against baselines including human judgments and game-theoretic optimal solutions. Key findings include: (1) reasoning models align better with human judgments than non-reasoning models; (2) a non-monotonic relationship exists between game-theoretic optimality and human alignment; (3) funness evaluations show higher cross-model variability; (4) reasoning token usage is highly unpredictable and weakly correlated with task complexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly distinguishes between problem-solving and problem evaluation capabilities, a meta-cognitive perspective with theoretical and practical value. The proposed two-dimensional framework of \"difficulty to compute\" and \"difficulty to quantify\" provides a generalizable structure for future research.\n2. The experiments compare 7+ models against multiple meaningful baselines: human judgments, game-theoretic optimal solutions, and computational cognitive models (Intuitive Gamer, Expert Gamer, MCTS). The comparison of reasoning versus non-reasoning models enables clear assessment of reasoning's value."}, "weaknesses": {"value": "1. All 121 games are Tic-Tac-Toe variants differing only in board size and rules, representing an extremely narrow subset of the game space. The exclusive focus on two-player, zero-sum, perfect-information competitive board games undermines claims about general \"problem evaluation\" capabilities.\n2. The paper treats funness as a \"hard to quantify\" query but provides no clear evaluation framework, instead instructing participants \"you can define fun however you wish.\" This lack of standardization means observed human-model misalignment may reflect disagreement about what funness means rather than evaluation capability differences.\n3. Game-theoretic optimal values are only computable for 64% of games (78/121), with the remainder relying on MCTS convergence without reported convergence criteria, runtime, or accuracy validation. The key finding of \"non-monotonic relationship between optimality and human alignment\" depends critically on the correctness of these values.\n4. Results are based on a single prompt design with heterogeneous temperature settings (1.0 vs 0.7) and only 20 rollouts per model, despite the authors acknowledging that \"model performance is sensitive to factors like exact prompt.\" Appendix Figure 18 shows reasoning amount significantly impacts results, yet this is not thoroughly analyzed."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GZ9pHDrYlI", "forum": "kefUcn22mk", "replyto": "kefUcn22mk", "signatures": ["ICLR.cc/2026/Conference/Submission20373/Reviewer_Ra3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20373/Reviewer_Ra3f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544903501, "cdate": 1761544903501, "tmdate": 1762933826324, "mdate": 1762933826324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new axis for evaluating language models (LMs): instead of only measuring whether models can solve problems, it evaluates how models assess them in a manner similar to humans. Using more than 100 two-player games, the study compares human judgments with model predictions on two quantities: fairness (expected payoff) and funness.\n1) Non-reasoning LMs produce similar judgments to each other but differ from humans and gameplay-based baselines on fairness.\n2) Reasoning LMs (with chain-of-thought(CoT)) show closer alignment with human judgments than non-reasoning models.\n3) The alignment with human judgment is not monotonic: as models move closer to optimal play, similarity to human judgments can decrease.\n4) For funness, models often reference similar factors in their reasoning traces, yet their final scores still differ.\n5) Reasoning-token usage varies widely across models and shows weak association with game novelty and with distance from optimal or human predictions."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The game collection size, the number of human responses per game, and the number of model samples per condition are all within a reasonable range. This allows estimation of meaningful statistical signals and enables analysis of variance across games and models.\n2) The evaluation covers a broad set of models, including non-reasoning and reasoning variants. The consistent observation that reasoning models show closer alignment to humans than non-reasoning models suggests that explicit reasoning traces have a measurable effect on judgment behavior.\n3) Two evaluation targets, “fairness” and “funness,” which provide additional structure beyond standard goal-oriented problem solving. This creates a concrete way to compare models and humans on perceptual or judgment-driven properties, relevant to studying whether models can serve as proxies for human judgments in cognitive science settings."}, "weaknesses": {"value": "1. Human and game selection reporting are incomplete. The main conclusions depend on the properties of both the game set and the human participants. However, the paper only states that it uses 121 games from Zhang et al. (2024a) and Collins et al. (2025) and that each game received approximately 20 human responses. There is no information about participant recruitment, demographics, incentives, exclusion criteria, or screening. Because judgments of fairness and funness are subjective, missing details about the human population make it difficult to assess how general the findings are. A similar concern applies to game selection; no justification is given for why these games are representative of broader gameplay settings.\n2. Ambiguous use of the term “novel.” The text refers to the 121 games as “novel,” but they are sourced from prior work. If “novel” means new to participants or models, this should be specified. Without clarification, readers may incorrectly infer that the authors introduce a new game set.\n3. Payoff scale design lacks justification. The payoff query uses two separate probability questions on a 0 – 100 scale (win given no draw, and draw) and later maps these to an expected payoff in [−1, 1]. The paper does not explain why this continuous format is preferred(especially when it will give larger variance) over a simpler ternary label {−1, 0, 1} or why no explicit uncertainty option is included. \n4. Non-monotonic model-human alignment is described but not explained. The paper reports that similarity to human judgments can decrease as models approach game-theoretic optimal play. However, no mechanism is proposed. Without additional analysis, this pattern is descriptive only. It remains unclear whether the divergence stems from structural differences between human and optimal strategies, prompt effects, or sampling artifacts.\n5. Positioning relative to prior LM-as-a-judge work is limited. The study contrasts non-reasoning models with models using CoT and notes that reasoning improves alignment. Similar patterns have been discussed in other LM-as-a-judge settings. The paper would benefit from situating its contribution more precisely within that literature rather than relying mainly on internal comparisons.\n6. Funness construct lacks shared operational definition. Participants/models are explicitly told they may define funness however they wish. Without a shared definition or anchoring examples, different participants may use incompatible internal criteria. This increases variance in the target signal and makes it unclear what construct models are attempting to approximate. While the paper analyzes which factors appear in reasoning traces, there is no attempt to identify a stable latent structure of funness or to evaluate whether model factors correlate with participant factors.\n7. Typo. There are many typos and I will just list some of them \n- “evaluate the the expected payoff and funness” -> remove the duplication (Section 3.2)\n- \"Afer you feel like you understand the game, you can provide your response.\" -> After (A3.2 SYSTEM PROMPT)"}, "questions": {"value": "see Weaknesses section"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This paper mentioned they have \"450 participants\" no information about how they recruit them and how they use this resource. Just mentioned \"20 people per query\"."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ixoawHCoeq", "forum": "kefUcn22mk", "replyto": "kefUcn22mk", "signatures": ["ICLR.cc/2026/Conference/Submission20373/Reviewer_cvbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20373/Reviewer_cvbH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875402508, "cdate": 1761875402508, "tmdate": 1762933825519, "mdate": 1762933825519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors explore and evaluate how frontier language models evaluate games. The bigger picture is that, as a field, we demonstrate that models can play and learn in games, but can they also choose which game to play, and if so, based on what? Do their tastes in games match human preference, or do they qualitatively differ? The paper collects human annotations for game evaluation for two specific questions. The frontier models are asked the same questions. The paper analyses the (dis)agreement between humans and frontier models, as well as the intra-frontier model disagreement. The main contribution is probably the extensive statistical analysis of human and model preference. The most interesting takeaway seems that reasoning in models leads to stronger correlation with human notions of fun, at least for these specific games, in a specific annotation setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Clarity\n- The paper presents its results transparently and thoroughly.\n- The paper is clear about hyperparameters and which prompts were used.\n- The authors are transparent with the limitations.\n- The paper is mostly easy to follow.\n- The figures are easy to understand and mostly well formatted.\n- The authors compare and contrast with relevant related work.\n\n## Quality\n- The statistical analysis is thorough, and meaningful measures of variance are provided. The details of how they were computed were also documented.\n- The experiments are thorough, cover a lot of frontier models, and investigate the role of reasoning (both through CoT and trained reasoning models).\n- The evaluation goes the extra mile and also looks at token usage, which was interesting!\n\n## Originality and Significance\n- The broader topic is definitely interesting and goes into capturing human subjectivity and open-ended agentic problems. The actual study setup can also be interesting to game developers."}, "weaknesses": {"value": "## Clarity\n1. The reviewer struggles to understand the motivation for the two dimensions of evaluations as outlined in the Intro. It's unclear how the second paragraph \"highlights\" (line 48) two dimensions of evaluations. For example, Hunicke (2004) appears to discuss the MDA framework, rather than the two dimensions of game evaluation. Isn't something hard to quantify necessarily hard to compute? What is an example of something hard to quantify but easy to compute? For example, in Figure 1, the example question is \"What kind of game is this?\" While being more of a classification task than a quantification task, why is that easier to compute? Currently, it's the reviewer's opinion that \"Hard to compute\" is too hard to quantify and \"Hard to quantify\" is too hard to compute to provide a useful framework for game evaluation by models.\n2. For the question about expected payoff, it was not clear from the main text that the prompt that humans and models were asked was:\n\n> Then, for each game, your task is to answer: assuming both players play *reasonably*\n\nThe Introduction describes the question as if it's focused *only* on the hard-to-compute part, i.e., all humans and models have to do is attempt to compute the game-theoretically optimal outcome for two rational players. \n\n> For each game, we test a series of language and reasoning\nmodels using two reasoning queries that engage both dimensions of evaluation: one that is difficult\nto compute and another that is difficult to compute and quantify.\n\nand also in Section 3.2:\n\n> We prompted a series of language and reasoning models to evaluate the the expected payoff and funness of each of the 121 games\n\n\nHowever, the question asks explicitly for *reasonable*, which, at least to the best of the reviewer's understanding, is not a well-defined term. For example, when playing against a child, where you might let them win on purpose, reasonable gameplay looks different from rational gameplay. In the reviewer's opinion, the expected payoff question then also becomes a question that is both hard to quantify *and* hard to compute. This muddies the comparison to the game-theoretic optimal performance. The results do seem to suggest that humans and frontier models tend to infer \"rational\", but for example, the reasoning trace from DeepSeek on line 1538 explicitly considers the agents to be reasonable, not rational.\n\n3.  For the annotation study, it's unclear in what order the human annotators were provided the questions. Would the same humans always answer both question 1 and question 2, and in the same order? If so, answering question 1 might bias a human to think about payoffs in the funness estimates. Ideally, annotators would be asked either question 1 or question 2 about a game to avoid a bias that wouldn't be present in the models' context.\n\n## Originality and Significance\n1. In the Conclusion, the authors write\n\n> We laid out a framework for thinking about evaluation of AI systems’ capacity for problem\nevaluation\n\nand the Abstract states:\n> In this paper, we advocate for a new paradigm that\nassesses AI systems’ evaluation of games. First, we introduce a formalism for\nevaluating such evaluations. We then leverage a large-scale dataset of over 100 novel board games and over 450 human judgments\n\n\nHowever, it's unclear to the reviewer what exactly the framework or new paradigm is. Section 2 explains that games may have factors that you can use to judge a game by. However, there is nothing specific to games in the framework described in Section 2. Anything that might be evaluated may have multiple factors (dimensions) by which to judge it. Is the framework the two specific questions, i.e., should an agent always evaluate expected payoff and fun? Is it the two dimensions? At the moment, to the best of the reviewer's understanding, the framework is simply gathering human annotations on a subjective measure, asking LLMs the same question and measuring correlation for a very specific domain. There is no evidence that any of these insights would generalise or that these dimensions (or questions) prove useful in \"evaluating language models' evaluations of games\".\n\nTo summarise, the paper's correlation analysis is well-executed; however, it's challenging to see how this paper contributes anything more general than a correlation analysis with human notions of fun for a specific family of games."}, "questions": {"value": "1. Figure 2 caption says \"modle-predicted\"\n2. There's a typo in the example prompt, i.e., see line 981 \"For each quetsion, provide your a single number\"\n3. The y-axes ticks in Figure 4b are very small.\n\nThe reviewer has no problem with the technical execution of the paper. If the authors could address the concerns raised in Weakness, the reviewer would be very grateful and would look forward to the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ex4yoCeIs", "forum": "kefUcn22mk", "replyto": "kefUcn22mk", "signatures": ["ICLR.cc/2026/Conference/Submission20373/Reviewer_AE1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20373/Reviewer_AE1w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017688074, "cdate": 1762017688074, "tmdate": 1762933824913, "mdate": 1762933824913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose to evaluate whether models can evaluate games (as benchmarks) themselves. Using 121Tic-Tac-Toe variants of board games and ~450 human judgments, the authors compare non-reasoning LMs, reasoning LMs (with traces), and simulation agents, and report a non-monotonic trade-off: models closer to game-theoretic optimality, their capacity as (game) evaluation judges align less with human judges. They also observe higher variance for funness evaluation and highlight the need for resource-rational meta-reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluating the games (as evaluation benchmarks) themselves (for design properties like payoff, funness) is a novel and valuable lens. \n\n2. Includes non-reasoning LMs, reasoning models, and rule-based (MCTS) agents, contextualizing model judgments against humans and game-theoretic references. \n\n3. Systematically contrasts AI vs human evaluations, revealing the non-monotonic alignment for game-theoretic optimal LLM players."}, "weaknesses": {"value": "1. There lacks evaluation metrics beyond R^2. Would be useful and informative to add Spearman’s rho and Kendall’s tau to capture rank alignment.​\n\n2. There lacks a strong justification on why choosing MDA-style constructs for formalizing \"funness\". Could the authors provide more discussion on their choice?\n\n3. The 121 games are largely tic-tac-toe variants.\n\n4. Lacks a prompt sensitivity study and analysis."}, "questions": {"value": "1. Would make the paper stronger if the authors could report preliminary results for expert-refined vs auto-tuned prompts (e.g. with DSPy), since prompting is a major driver of judgments. \n\n2. Could the authors include other rank-based metrics (Spearman/Kendall) or motivate why it suffices to sticking with R^2 as evaluation metrics? \n\n3. Does the human-vs-optimal non-monotonicity persist on cooperative or asymmetric games? Any early evidence? \narXiv\n\n4. Do the authors have comments on how do these results transfer to other evaluation settings (beyond game-as-an-eval, for other benchmark/test construction)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7IKOM473rf", "forum": "kefUcn22mk", "replyto": "kefUcn22mk", "signatures": ["ICLR.cc/2026/Conference/Submission20373/Reviewer_Tg2R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20373/Reviewer_Tg2R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087826991, "cdate": 1762087826991, "tmdate": 1762933824024, "mdate": 1762933824024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}