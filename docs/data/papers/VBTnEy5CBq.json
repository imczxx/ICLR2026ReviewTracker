{"id": "VBTnEy5CBq", "number": 12248, "cdate": 1758206577982, "mdate": 1759897522712, "content": {"title": "PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Langauge Models", "abstract": "Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by transferring the knowledge from teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we first introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs—i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects probabilistically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Extensive evaluations on 11 datasets show that ActiveKD consistently improves performance across selection methods (e.g., +29.07% on ImageNet, averaged over methods). Under ActiveKD, PCoreSet ranks first in 64/73 settings (≈87.7%) across 5 student and 3 teacher networks, always achieving the best performance except for the first 2 AL rounds.", "tldr": "", "keywords": ["active learning", "vision-language model", "knowledge distillation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55d2e4a90035eeb9c48bd4052756028e582a495f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ActiveKD, an active-learning framework that distills knowledge from a zero/few-shot vision–language model (VLM) into a compact student during each AL round, and introduces PCoreSet, a probability-space coreset selection rule that greedily maximizes the minimum l2 distance between a candidate's student prediction and those of labeled points. The authors argue VLM predictions exhibit structured clustering in the probability simplex and provide a simple analysis that such structure propagates to the student under distillation. Extensive experiments on 11 datasets show large gains from ActiveKD over \"No Distill\", and PCoreSet often ranks best among baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of employing VLM is advanced. The paper offers a theoretical argument for how the VLM's knowledge structure is passed to the student. It nicely connects this idea directly to the PCoreSet algorithm's goal of covering the probability space.\n2. The PCoreSet selection rule is simple and computationally cheap. This makes it practical and should be easy to integrate into existing active learning pipelines.\n3.  Strong empirical improvements from enabling KD with a VLM across many datasets."}, "weaknesses": {"value": "1. The core concept of combining active learning with knowledge distillation from a powerful teacher, is a natural combination and not entirely new. The main algorithmic piece, using a VLM's outputs to select diverse data, is intuitive but also quite simple. It follows a classic active learning strategy, coreset, with the main novelty being the use of a VLM teacher and the focus on probability outputs rather than feature representations. The theory, while a nice addition, mainly formalizes an existing intuition rather than breaking new ground. Therefore, the novelty is limited.\n\n2. When a powerful VLM is available, the empirical gains are substantial, so the framework could be practically valuable. However, these improvements are largely attributable to adding a strong teacher rather than to the selection rule itself; the marginal benefit of PCoreSet over other selectors is smaller and sometimes inconsistent in early rounds.\n\n3. The method's simplicity is a double-edged sword: it's easy to implement, but the core idea of using a VLM to pseudo-label and select diverse outputs is fairly expected; the results match what one would anticipate when injecting a much stronger teacher into an AL loop."}, "questions": {"value": "Can you provide computational analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gWCEqG91vk", "forum": "VBTnEy5CBq", "replyto": "VBTnEy5CBq", "signatures": ["ICLR.cc/2026/Conference/Submission12248/Reviewer_YbDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12248/Reviewer_YbDT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753519427, "cdate": 1761753519427, "tmdate": 1762923190560, "mdate": 1762923190560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ActiveKD, a novel active-learning framework that distills knowledge from large vision–language models (VLMs) into compact task-specific networks. The key technical contribution is PCoreSet, a selection criterion that queries unlabeled samples lying in under-represented regions of the teacher’s probability simplex rather than in feature space. Extensive experiments on 11 datasets, 5 students and 3 teachers show consistent gains (+29 % on ImageNet over baselines) and top rank in 64/73 settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work is the first to tightly integrate knowledge distillation with pool-based active learning, addressing a realistic scenario where labeled data are extremely scarce. \n\n- The insight that VLM predictions exhibit structured bias transferable to students is both novel and well-supported theoretically (bias-propagation proposition) and empirically.\n\n- PCoreSet is simple, fast (O(CMN)) and complementary to existing acquisition functions;"}, "weaknesses": {"value": "- The motivation is not very clear. The initial motivation of this paper is more like a combination of two existing modules, more than a well-defined problem. For AL part, one powerful model like teacher model (e.g. VLM) is what we need so why we conduct KD? For KD part, ActiveKD seems more reasonable and may benefit efficient KD while the paper is more prone to AL settings.\n\n- The problem settings are highly similar to semi-supervised learning while the SOTA SSL methods are not compared in the experimental parts."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "piTFVY03WQ", "forum": "VBTnEy5CBq", "replyto": "VBTnEy5CBq", "signatures": ["ICLR.cc/2026/Conference/Submission12248/Reviewer_NApS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12248/Reviewer_NApS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921334295, "cdate": 1761921334295, "tmdate": 1762923189482, "mdate": 1762923189482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ActiveKD, a new framework that integrates Active Learning (AL) with Knowledge Distillation (KD) to train compact, task-specific models in data-scarce environments. It addresses a key challenge: AL typically lacks the labeled data required to train a powerful \"teacher\" model for KD. ActiveKD solves this by using a large Vision-Language Model (VLM) as a zero-shot or few-shot teacher. In each AL round, the \"student\" model is trained using both the few human-labeled samples and soft labels (knowledge) distilled from the VLM teacher on the entire unlabeled data pool.\n\nThe authors observe that VLM teachers exhibit a \"structured prediction bias,\" meaning their output probabilities form distinct clusters. They propose to leverage this as a useful inductive bias. To do this, they introduce Probabilistic CoreSet (PCoreSet), a novel AL selection strategy. Unlike traditional coreset methods that select samples for diversity in the feature space, PCoreSet selects samples that are diverse in the probability space, effectively covering the teacher's output clusters.\n\nExtensive experiments on 11 datasets show that the ActiveKD framework itself provides a large performance boost to all AL methods. Within this new framework, PCoreSet is the most effective selection strategy, outperforming baselines in 64 out of 73 tested settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces ActiveKD, a novel framework that successfully integrates Active Learning (AL) with Knowledge Distillation (KD). This is significant because it solves a key problem: AL operates in data-scarce settings where powerful task-specific \"teacher\" models are normally unavailable, but ActiveKD overcomes this by leveraging the zero- and few-shot capabilities of large Vision-Language Models (VLMs) as teachers.\n2. The paper identifies that VLM teachers exhibit a \"structured prediction bias,\" where their outputs cluster in the probability space, and reframes this not as a limitation but as a useful inductive bias. Based on this insight, it proposes the Probabilistic CoreSet (PCoreSet), a novel selection strategy that maximizes coverage in the probability space rather than the feature space, which is a key differentiator from standard coreset methods."}, "weaknesses": {"value": "1. The paper reframes the VLM's \"structured prediction bias\" as a positive inductive bias to be exploited. However, it fails to investigate the negative case: what if this \"structured bias\" is just the VLM being confidently and systematically wrong about a whole cluster of data? The proposed PCoreSet method, by design, would actively sample from this erroneous cluster, potentially leading the student to efficiently learn the teacher's mistakes.\n2.  The ActiveKD framework (Algorithm 1) applies knowledge distillation to both the small labeled set $\\mathcal{D}^{(l)}$ and the large unlabeled pool $\\mathcal{D}^{(u)}$. The large performance gains attributed to ActiveKD (e.g., +29.07% on ImageNet) are never ablated to separate the effect of distillation on the unlabeled pool (a form of semi-supervised learning) from the effect of distillation on the actively selected labeled set. It is unclear how much of the gain is simply from using the VLM teacher in a semi-supervised fashion versus the active learning component itself."}, "questions": {"value": "1. You reframe the VLM's \"structured prediction bias\" as a positive inductive bias to be exploited. However, what if this bias simply represents a \"blind spot\" where the VLM teacher is confidently and systematically wrong about a whole cluster of data? Since PCoreSet is designed to find and sample from these clusters, how does your method prevent the student model from efficiently learning the teacher's errors (i.e., negative knowledge transfer)?\n2. The ActiveKD framework applies knowledge distillation to both the small labeled set $\\mathcal{D}^{(l)}$ and the entire unlabeled pool $\\mathcal{D}^{(u)}$. Your results show that this framework provides a massive performance boost (e.g., +29.07% on ImageNet) over the \"No Distill\" baseline, even for Random selection. This suggests a large gain comes from the semi-supervised learning (SSL) component on $\\mathcal{D}^{(u)}$. How much of the performance improvement is actually attributable to the active selection strategy versus this powerful SSL effect? Could you provide an ablation that applies KD only to the labeled samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Owo8CfmtVc", "forum": "VBTnEy5CBq", "replyto": "VBTnEy5CBq", "signatures": ["ICLR.cc/2026/Conference/Submission12248/Reviewer_hCWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12248/Reviewer_hCWn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970930593, "cdate": 1761970930593, "tmdate": 1762923188818, "mdate": 1762923188818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to construct a set of unlabeled data for labeling, aimed at improving model performance incrementally. The approach combines active learning and knowledge distillation, leveraging probability-based diversity coverage to guide selection. Performance gains are demonstrated through experiments using VLM models on selected benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is carefully developed and supported by both theoretical analysis and experimental results.\n2. The method is presented clearly, with evidence that illustrates its performance gain over selected benchmarks."}, "weaknesses": {"value": "1. The method requires additional training and repeated querying to construct the core set. In practical applications, this overhead may limit scalability and reduce its real-world applicability, weakening the overall contribution.\n2. The baseline comparisons are not fully fair, as they rely on different training and query budgets. In particular, some baselines require additional networks that must be trained separately, resulting in higher computational cost and an unequal evaluation setting."}, "questions": {"value": "1. If the query and training costs are constrained, the benefits of the proposed method become less clear. In fact, some baseline approaches, such as uncertainty-based scoring, offer much more efficient computation for selecting core sets to be labeled, without requiring additional training overhead. This raises the question of whether the added complexity of the proposed method is justified when simpler, low-cost alternatives can achieve comparable results under realistic resource constraints.\n2. The large-scale benchmark datasets used in the study exhibit limited diversity, raising concerns about the method’s ability to generalize in broader large-scale training scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "flj2CJ1fqE", "forum": "VBTnEy5CBq", "replyto": "VBTnEy5CBq", "signatures": ["ICLR.cc/2026/Conference/Submission12248/Reviewer_mf9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12248/Reviewer_mf9U"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975775304, "cdate": 1761975775304, "tmdate": 1762923188365, "mdate": 1762923188365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}