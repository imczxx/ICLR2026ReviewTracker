{"id": "Y6idSGOwfK", "number": 23023, "cdate": 1758338325234, "mdate": 1759896835514, "content": {"title": "Cross-Modal Factor Reasoning with LLMs: Toward Semantic-Structured Generalization for Recommendation", "abstract": "Multimodal recommendation aims to enhance personalization by leveraging content signals such as text and images. However, existing methods often treat modalities as shallow auxiliary inputs, fusing raw embeddings without reasoning about what semantics are useful or how they influence user preference. Content-based graphs typically rely on low-level similarity, lacking structured semantic relations such as functionality or style. Moreover, collaborative signals are used solely for ranking, without grounding content semantics. To address these limitations, we present MARS, a framework for Cross-Modal FActor Reasoning with LLMs, enabling Semantic-Structured Generalization in recommendation. MARS introduces a cognitively guided paradigm that prompts large language models (LLMs) to extract human-interpretable semantic factors (e.g., functionality, material and usage scenario) from raw visual and textual descriptions. These structured factors are used to build heterogeneous graphs that capture multi-aspect semantic relations among items. To integrate semantics into representation learning, we propose an auxiliary semantic prediction task that aligns collaborative embeddings with LLM-inferred factor knowledge. In addition, a cross-modal consistency loss encourages agreement across semantic views from different modalities. Extensive experiments show that MARS achieves superior accuracy and generalization compared to state-of-the-art multimodal baselines and LLM-based methods.", "tldr": "", "keywords": ["Multimodal Recommendation", "Large Language Model", "Semantic Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/408da9f70134782119f524f108cca7dec26ef192.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MARS, a novel multimodal recommendation framework that leverages Large Language Models (LLMs) for cross-modal factor reasoning. The key idea is to extract human-interpretable semantic factors (e.g., functionality, material) from raw multimodal content using LLMs, construct heterogeneous graphs based on these factors, and integrate semantics into representation learning via an auxiliary semantic prediction task and cross-modal consistency loss. Extensive experiments on Amazon datasets demonstrate superior performance over state-of-the-art baselines in both accuracy and cold-start scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Effectively integrates LLM-derived factors into graph-based learning.\n\n- Relevant experiments for the cold start scenario are provided."}, "weaknesses": {"value": "- [Mandatory] How are the item complementarity, stylistic coherence, or functional substitutability mentioned in the Introduction reflected? In fact, the selected Amazon datasets (Baby, Sports, Clothing) do not exhibit such phenomena. Please carefully review the meta-data of these datasets.\n\n- [Mandatory] In the Introduction, the authors claim that no work considers leveraging collaborative signals to support content. In reality, some works [1-2] have used collaborative signals to purify content. Other works [3-4] have also considered using collaborative signals to guide modality alignment.\n\n- [Mandatory] The factor extraction in Eq. 1 requires different adjustments for different scenarios. The current experiments focus on the e-commerce domain but lack experiments in other domains to verify generalizability. Additionally, its quality heavily depends on the pre-defined factors in the prompt.\n\n- [Mandatory] While the factor extraction strategy captures key features to some extent, it undoubtedly loses much fine-grained information about items, reducing the personalization of the recommendation system.\n\n- [Mandatory] This design significantly hinders the extension to multiple modalities. For example, in scenarios involving audio modalities (e.g., TikTok datasets), it may fail to extract effective factors from spectrograms.\n\n- [Mandatory] Admittedly, current multimodal recommendation systems cannot fully utilize visual information. However, converting visual modalities into text modalities undoubtedly results in the loss of valuable semantic information, limiting the theoretical upper bound of multimodal recommendation performance.\n\n- [Mandatory] This model is heavily dependent on the capabilities of LLMs.\n\n- [Mandatory] The configuration of $\\mathcal{A}_{f_k}$ determines the quality of the heterogeneous semantic graph construction and introduces higher hyperparameter tuning costs compared to semantic homogeneous graphs.\n\n- [Mandatory] The training of the Semantic Factor Prediction Auxiliary Task is supposed to focus on the semantic encoder. However, it fails to retain semantic-relevant information while filtering out modality-specific noise.\n\n- [Mandatory] Some works published in 2024–2025 should be included as baselines, such as [2–5].\n\n- [Mandatory] Splitting into multiple factors increases the amount of graph message passing, so the model's efficiency needs to be empirically and theoretically validated and discussed.\n\n\n\nRefs:\n\n[1] Multi-View Graph Convolutional Network for Multimedia Recommendation, ACM MM 2023.\n\n[2] Cohesion: Composite graph convolutional network with dual-stage fusion for multimodal recommendation, SIGIR 2025.\n\n[3] Mentor: multi-level self-supervised learning for multimodal recommendation, AAAI 2025.\n\n[4] GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal Recommendation, CIKM 2024.\n\n[5] Mind Individual Information! Principal Graph Learning for Multimedia Recommendation, AAAI 2025."}, "questions": {"value": "Please refer to Weaknesses. Btw, I have some optional questions:\n\n- [Optional] Can this work handles modalities like audio without losing critical information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yPzcIh3Dgz", "forum": "Y6idSGOwfK", "replyto": "Y6idSGOwfK", "signatures": ["ICLR.cc/2026/Conference/Submission23023/Reviewer_MvG2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23023/Reviewer_MvG2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760809031417, "cdate": 1760809031417, "tmdate": 1762942482128, "mdate": 1762942482128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARS, a framework for cross-modal factor reasoning with LLMs, to solve existing multimodal recommendation limitations (shallow fusion, no structured semantics, disconnected collaborative-content signals) . It uses LLMs to extract interpretable semantic factors (e.g., functionality) from visual/textual content, builds heterogeneous graphs for item semantic relations, and adds an auxiliary semantic prediction task plus cross-modal consistency loss ."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike traditional multimodal recommendation approaches that rely on low-level similarity (e.g., visual/textual embedding proximity) and lack high-level relations (e.g., functionality, style coherence), MARS uses LLMs (e.g., GPT-4o) to extract interpretable semantic factors from raw visual/textual content and builds heterogeneous graphs based on these factors, enabling fine-grained semantic relation modeling .\n\n2. MARS introduces an auxiliary semantic prediction task to align collaborative embeddings with LLM-derived factors, and a cross-modal consistency loss to unify ID/visual/textual representations. This resolves the one-way supervision gap (collaborative signals only for ranking) in existing frameworks, making embeddings semantically grounded ."}, "weaknesses": {"value": "1. MARS relies on powerful LLMs (e.g., GPT-4o) for semantic factor extraction, which may incur high computational costs and depend on external LLM services. No lightweight alternatives for factor reasoning are proposed, limiting deployment in resource-constrained scenarios.\n\n2. Experiments are only conducted on three Amazon Review subsets (Sports, Clothing, Baby), all belonging to e-commerce. No validation on other multimodal recommendation domains (e.g., social media micro-videos, music) means its adaptability to diverse scenarios remains unproven.\n\n3. Some of the text in Figure 1 is too small and difficult to read."}, "questions": {"value": "1. Does the method require powerful LLMs (e.g., GPT‑4o) during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5lxvdxv5tk", "forum": "Y6idSGOwfK", "replyto": "Y6idSGOwfK", "signatures": ["ICLR.cc/2026/Conference/Submission23023/Reviewer_tVZn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23023/Reviewer_tVZn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581915134, "cdate": 1761581915134, "tmdate": 1762942481795, "mdate": 1762942481795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARS, a semantic-structured multimodal recommendation framework that aims to move beyond shallow feature fusion. The authors use LLMs to extract human-interpretable semantic factors (e.g., functionality, material, scene) from item content, construct a factor-aware heterogeneous graph to model structured semantic relations across items, and leverage collaborative filtering signals to supervise semantic factor learning. This enables the model to align content semantics with preference semantics. Experiments on multiple Amazon datasets show improved performance, cold-start gains, and enhanced interpretability through more meaningful semantic clustering."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper highlights an important gap in multimodal recommendation: content semantics (what an item is) does not necessarily align with preference semantics (why users choose it). Positioning collaborative filtering as a semantic value filter to ground LLM-extracted semantics in actual user preference is insightful and intellectually valuable.\n\n2. Using LLMs to extract explicit semantic factors and building a factor-aware heterogeneous graph brings human-readable structure into item representation learning. This improves transparency and provides more intuitive explanations than typical embedding-based multimodal fusion.\n\n3. The proposed factor-aware semantic graph offers a more principled way to capture high-level item relations that go beyond surface-level visual/textual similarity. This design aligns with how users mentally categorize items, enabling more meaningful reasoning compared to pure embedding-similarity graphs."}, "weaknesses": {"value": "1. The paper assumes multimodal encoders mainly learn surface-level similarity (e.g., appearance), but recent VLMs such as CLIP/BLIP-2/LLaVA already capture functional and conceptual semantics. The authors should clarify what semantic gaps remain unsolved by modern multimodal encoders and why factor-graph modeling is necessary beyond them.\n\n2. While results show improvements, there is no head-to-head comparison against strong VLM-based baselines (e.g., CLIP-enhanced or BLIP-enhanced recommenders). Without such comparisons, it is difficult to isolate whether gains come from the factor graph itself or from general architectural complexity.\n\n3. The ablation (w/o FP) shows that semantic factor prediction benefits ranking performance, which indirectly supports the usefulness of CF-supervised semantics. However, the paper does not provide direct evidence that collaborative filtering supervision refines the semantic relevance of the extracted factors or filters out non-preference-relevant semantics. The claim that CF acts as a “semantic value filter” remains conceptual. Additional analyses—such as semantic factor quality evaluation, preference relevance studies, visualization of factor distributions before/after CF supervision, or semantic pruning impact—would strengthen this key contribution."}, "questions": {"value": "1. Could authors specify which aspects of semantics current VLMs fail to capture that your factor-graph modeling resolves? A comparison against a CLIP/BLIP-augmented baseline would make the necessity of your approach more convincing.\n\n2. Can authors provide evidence that CF supervision improves semantic relevance of extracted factors rather than merely boosting ranking performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xJ4vr8Q0YH", "forum": "Y6idSGOwfK", "replyto": "Y6idSGOwfK", "signatures": ["ICLR.cc/2026/Conference/Submission23023/Reviewer_TA2u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23023/Reviewer_TA2u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966077847, "cdate": 1761966077847, "tmdate": 1762942481509, "mdate": 1762942481509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the MARS, which extracts interpretable semantic factors from multimodal content through LLM, builds heterogeneous graphs, and introduces semantic prediction and cross-modal consistency tasks, which improve the performance of recommendation systems in cold start and generalization scenarios. This method sounds good, but lacks codes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper was well written and clearly structured.\n+ Sufficient comparisons on many types of baseline models.\n+ The design of the semantic prediction task is interesting."}, "weaknesses": {"value": "+ GPT-4o is used to extract semantic factors, the adaptability of different LLMs (such as open source models, small language models) is not evaluated, and the impact of LLM reasoning costs on large-scale scenario deployment is not discussed.\n+ The selection of semantic factors relies on the Top-10 high-frequency factors output by LLM. There is a lack of quantitative analysis on the rationality of factor screening, and the impact of different factor combinations on model performance has not been verified.\n+ The experimental dataset is limited to subsets of Amazon, and the scenario is quite single.\n+ Source code is missing."}, "questions": {"value": "+ The current method relies on the quality of semantic factors output by LLM, does not consider possible deviations or errors in LLM, and lacks an automatic evaluation or error correction mechanism for the quality of factors.\n+ What is the motivation of the current factor selection method? Are there cases where some low-frequency semantic factors that are critical to a specific user group/item category are missed?\n+ For different scenarios, the definition of semantic factors may need to be adjusted. How to extend it to other fields, such as video recommendation?\n+ The same factor may be defined differently in different types of goods. Will this affect the accuracy of semantic relationship modeling in heterogeneous graphs? Such as the functionality factor “outdoor” in clothing and tools."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "81fg31wdlS", "forum": "Y6idSGOwfK", "replyto": "Y6idSGOwfK", "signatures": ["ICLR.cc/2026/Conference/Submission23023/Reviewer_HYsP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23023/Reviewer_HYsP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180750803, "cdate": 1762180750803, "tmdate": 1762942480572, "mdate": 1762942480572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}