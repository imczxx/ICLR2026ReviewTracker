{"id": "sZIkXA6pMh", "number": 10409, "cdate": 1758170539862, "mdate": 1759897652600, "content": {"title": "LLM-REVal: Can We Trust LLM Reviewers Yet?", "abstract": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. \nWhile previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. \nIn this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation.\nThis simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. \nBased on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments:\n(1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones;\n(2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions.\nOur analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements.\nThese results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. \nOn the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.", "tldr": "We focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation.", "keywords": ["AI in research", "AI in review", "systematic bias", "fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6c7d151df939c9c6e292e7258612380e7086079.pdf", "supplementary_material": "/attachment/8ad6f5386836fe958316a8019de0716c46de8366.zip"}, "replies": [{"content": {"summary": {"value": "The paper examines whether LLMs can function as reliable reviewers in academic peer review. To explore this, the authors develop a multi-round simulation framework called LLM-REVal, which models the interaction between a Research Agent—responsible for generating both human-like and LLM-authored papers—and a Review Agent that evaluates those submissions. The simulation reproduces the full review process, including initial assessment, rebuttals, revisions, and resubmission cycles, with a focus on fairness and bias in LLM-based reviewing.\n\nThe results reveal that LLM reviewers systematically favor the writing style characteristic of LLM-generated papers, leading to inflated scores for LLM-authored work. In contrast, some human-written papers—particularly those that discuss risks, fairness, or critical perspectives on AI—consistently receive lower scores, even after multiple revisions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Novel Problem Formulation: Addresses a timely and underexplored issue—what happens when LLMs act as both researchers and reviewers, creating feedback loops in scientific workflows.\n- Comprehensive Simulation Framework: The paper develops a realistic multi-agent system encompassing literature search, paper creation, feedback, rebuttal, revision, and meta-review. This end-to-end simulation is technically impressive.\n- Clear Empirical Evidence of Bias: The study rigorously demonstrates two forms of bias in LLM reviewers:\n  - Linguistic bias toward LLM-style writing.\n  - Topic/framing bias against papers emphasizing risks or fairness."}, "weaknesses": {"value": "No significant weaknesses from my sight. Some minor comments below:\n\n- The section title font is slightly different from other papers.\n- Potential Missing Citations\n  - [A Sentiment Consolidation Framework for Meta-Review Generation](https://aclanthology.org/2024.acl-long.547/)\n  - [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)\n  - [Position Paper: How Should We Responsibly Adopt LLMs in the Peer Review Process?](https://openreview.net/forum?id=KZ3NspcpLN)\n  - [Position: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process](https://openreview.net/forum?id=gnyqRarPzW&noteId=1Y3P0jqL5z)\n  * [Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards](https://openreview.net/forum?id=l8QemUZaIA)"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YYRICzEIHC", "forum": "sZIkXA6pMh", "replyto": "sZIkXA6pMh", "signatures": ["ICLR.cc/2026/Conference/Submission10409/Reviewer_TMen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10409/Reviewer_TMen"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288844933, "cdate": 1761288844933, "tmdate": 1762921721105, "mdate": 1762921721105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper simulates an academic workflow to see if LLMs can be trusted as peer reviewers. The authors built a system with an LLM agent that writes papers and another that reviews them, comparing the results for both LLM-authored and human-authored papers. The study finds that LLM reviewers are significantly biased: they systematically give higher scores to other LLM-generated papers and lower scores to human papers that discuss critical topics like \"risk\" or \"fairness\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written and easy to follow.\n- The research identifies specific biases in LLM reviewers, notably a \"linguistic feature bias\" favoring LLM-generated text and an aversion toward critical statements.\n- The findings from the simulation are contrasted with human evaluations, which reveal a clear misalignment in judgment; human reviewers, for instance, did not share the LLM reviewers' preference for LLM-authored papers."}, "weaknesses": {"value": "- The comparison between LLM-generated and human-authored papers is somehow not rigorous. The authors extract keywords from real human-authored papers and use these keywords to guide the LLM in generating a new paper. However, (1) the LLM may likely generate a paper with a distinct idea; (2) even with a similar idea, the LLM-generated paper uses \"predicted\" the results while the results in human-authored papers are real, ... That is, there are many variables that may lead to different review scores, making the comparison unfair and the corresponding conclusions less convincing.\n- The research agent's process is simplified, as it \"predicts\" experimental results rather than actually executing experiments, which may not capture the full complexity of manuscript quality. \n- The \"Irreducible Rejection\" finding is interesting, but it's not entirely clear why these specific human papers were persistently underrated, even after multiple revisions guided by the LLM's own feedback."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBTYVVU0SZ", "forum": "sZIkXA6pMh", "replyto": "sZIkXA6pMh", "signatures": ["ICLR.cc/2026/Conference/Submission10409/Reviewer_EYY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10409/Reviewer_EYY5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921627244, "cdate": 1761921627244, "tmdate": 1762921720732, "mdate": 1762921720732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at the risk of LLMs as per reviewers by simulating multi round paper generation and the review process. The paper constructs a Research Agent that generates papers and a Review Agent that evaluates submissions, comparing 100 human-authored ICLR papers against 100 LLM-generated papers on identical topics across multiple review-revision cycles. The aim to then see the misalignment between LLM reviewers and human reviewers. They obseve two main biases: LLMs have linguistic feature bias favoring LLM-generated writing styles and aversion toward critical discussions. Such a study is important to discuss the course of using LLMs in peer review or not."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very timely and helpful in the discussions of allowing AI tools for paper writing or/and paper reviewing. \n\n- The authors validate their review agent first using real ICLR 2025 data (100 papers). The 73.7% acceptance prediction accuracy and significant correlation with human scores establish credibility and trust for downstream evaluations.  \n\n- Human in the loop validation and testing.\n\n- While the results that LLMs prefer their own similar generations has been established in the literature previously, nine-metric linguistic analysis is thorough for review purpose."}, "weaknesses": {"value": "- LLM papers use \"predicted results\" rather than actual experimental execution. This could lead to unrealistic results or bumped up values whereas human papers might have realistic results where the proposed method doesnt always outperform.\n\n- Building upon the prev one, my biggest concern is that this study has different confounders for human paper and LLM paper, making it difficult to find causal relationship in the results.\n\n- A bit of circular evaluation. LLMs prefer their own outputs is already known - this study essentially re-discovers it in a new context making the novelty low, especially since the reviewer agent used is also been proposed already."}, "questions": {"value": "- With human-LLM reviewer correlation at only r=0.50 how do you establish which evaluation is \"correct\"? Why frame disagreement as \"LLM bias\" rather than \"human unreliability\" or \"both are noisy\"?\n\n- How do you account for the confounding bias between the two LLM and human papers(paper length, overclaiming novelty)?\n\n- If we over-sampled rejected papers, this biases results toward lower-quality human papers (as per the dataset it has 50% acceptance rate?)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1dw1W4IC2s", "forum": "sZIkXA6pMh", "replyto": "sZIkXA6pMh", "signatures": ["ICLR.cc/2026/Conference/Submission10409/Reviewer_soLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10409/Reviewer_soLf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001075455, "cdate": 1762001075455, "tmdate": 1762921720330, "mdate": 1762921720330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies how using LLMs for both research and review influences AI reviewing.\nAn AI research (agent) and an AI reviewer (agent) generate papers and review them in a loop.\nThe work finds that LLM reviews inflate the scores of LLM-generated papers \nand penalize human-written papers that are self-critical."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A key strength of this work is that it studies an AI researcher (agent) together with an AI reviewer (agent).\nA builder-reviewer loop is often used in the emerging field of automated scientific discovery (ASD).\n\n2. The work finds that LLMs tend to favor LLM-written papers over human-written papers, reward revisions, and penalize human self-criticism in human-written papers."}, "weaknesses": {"value": "1. The key issue with this work is that the review process only includes papers and is missing a review of their data and code.\nWithout the AI reviewing the entire submission: data, code, and paper, the AI reviewer cannot distinguish between real and fabricated papers, which may hallucinate experiments, results, etc.\nWithout reviewing the entire submission, it is unclear if improvements are hallucinated research or real research.\n\n2. The architecture figure 1 (on page 2) is AI-generated with gross spelling errors, “Guild by reviews”, “Reversion”.\n\n3. The \"research agent\" may be simplified by using an agent such as Claude Code (with a flat fee without incurring any API token costs).\nThe \"review agent\" could be improved by using a strong model such as GPT-5 Pro.\n\n4. This perspective on LLM reviewing of both human and AI researchers is missing the issue of detecting problems with automated scientific discovery (ASD) systems.\nSee for example:\n@article{jiang2025badscientist,\n  title={BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?},\n  author={Jiang, Fengqing and Feng, Yichen and Li, Yuetai and Niu, Luyao and Alomair, Basel and Poovendran, Radha},\n  journal={arXiv preprint arXiv:2510.18003},\n  year={2025}\n}"}, "questions": {"value": "Can the work be used to evaluate issues with automated scientific discovery (ASD) systems instead of just LLM reviewing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eEFjcv0Q4l", "forum": "sZIkXA6pMh", "replyto": "sZIkXA6pMh", "signatures": ["ICLR.cc/2026/Conference/Submission10409/Reviewer_9d5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10409/Reviewer_9d5H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762484443738, "cdate": 1762484443738, "tmdate": 1762921719902, "mdate": 1762921719902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}