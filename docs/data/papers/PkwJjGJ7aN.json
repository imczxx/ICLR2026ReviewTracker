{"id": "PkwJjGJ7aN", "number": 13931, "cdate": 1758225409617, "mdate": 1763731241265, "content": {"title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality. Our code is available.", "tldr": "Graph-R1 presents an agentic GraphRAG framework with end-to-end reinforcement learning that outperforms previous GraphRAG and RAG methods in accuracy, retrieval efficiency, and generation quality.", "keywords": ["Knowledge Graph", "Large Language Model", "GraphRAG", "Reinforcement Learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f572762a46994ff49cc9187d8dc589d02a7948.pdf", "supplementary_material": "/attachment/9a8b3a983541fcd909f4ba1b909e0300d444a06e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Graph-R1, an optimized agent for retrieval-augmented generation using GraphRAG. Graph-R1 first constructs a HyperGraph that serves as its backbone retrieval system, enabling structured knowledge retrieval. The agent then employs a multi-turn approach to answer queries by iteratively refining search queries and hypergraph retrieval operations (entity/hyperedge retrieval), allowing for progressive improvement in response quality. Graph-R1 is trained end-to-end using GRPO reinforcement learning, with rewards designed to encourage both correct answer generation and proper response formatting. Experimental evaluation on six datasets demonstrates Graph-R1's effectiveness, significantly outperforming competing GraphRAG approaches and search agents across metrics."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- S1) Graph-R1 demonstrates strong performance, outperforming existing GraphRAG approaches and search agents across multiple benchmarks.\n\n- S2) Ablation studies show that combining enhanced knowledge representations (GraphRAG) with reinforcement learning yields greater performance gains than traditional chunk-based retrieval methods.\n\n- S3) The paper provides comprehensive experimental analysis across multiple dimensions, such as performance, efficiency, response length, and turn count, demonstrating Graph-R1's advantages."}, "weaknesses": {"value": "- W1) Graph-R1 appears to rely on existing approaches, Search-R1 (Jin et al., 2025) and HyperGraphRAG (Luo et al., 2025), i.e., on Search-R1's reinforcement learning framework and HyperGraphRAG's retrieval operations and graph construction. In addition, HyperGraphRAG seems to outperform Graph-R1 on some benchmarks when using GPT-4o-mini (Table 2), while Graph-R1 requires >15 steps to surpass HyperGraphRAG's performance (Figure 5b).\n\n- W2) Graph-R1's reliance on entity-based retrieval (Section 2.2) may limit its generalizability to queries lacking explicit entities, such as \"Summarize the trends in the last 5 financial reports.\""}, "questions": {"value": "- Q1) Do Graph-R1 and Search-R1 use identical training data, or are you reporting results from the released Search-R1 version? Additionally, what is the number of training examples per method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xbCXNBZNRI", "forum": "PkwJjGJ7aN", "replyto": "PkwJjGJ7aN", "signatures": ["ICLR.cc/2026/Conference/Submission13931/Reviewer_g7wu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13931/Reviewer_g7wu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496570692, "cdate": 1761496570692, "tmdate": 1762924435792, "mdate": 1762924435792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work applied RL with verified outcome reward to enhance the GraphRAG framework. Specifically, it construct a knowledge hypergraph from a given knowledge source and then let an LLM to perform multi-turn graph retrieval on the constructed hypergraph followed up by reasoning / thinking. These process is improved via GRPO with outcome-directed reward function."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well writing.\n\n- The paper did detailed empirical experiments across multiple datasets and also provide insightful case studies."}, "weaknesses": {"value": "- The research novelty is quite limited. This work applies GRPO to GraphRAG scenario. The most techniques (GRPO, GraphRAG etc.) utilized in this work is already extensively studied. Basically this paper only proves that GRPO can work in the GraphRAG scenario. \n\n- The proposed OOD setting is not enough to evaluate the generalization ability of the finetuned LLMs. Many datasets such as 2wiki and hotpotqa are all wiki-based datasets. The paper need to evaluate on other domain of data to show the generalization ability."}, "questions": {"value": "- Some claims are too strong. For example, the paper claims that existing GraphRAG all aim to gather sufficient knowledge in a single fixed retrieval. However, there are already some GraphRAG work such as HybGRAG [1] that leverages multi-retrieval strategy. The paper need to do a more comprehensive literature review.\n\n\n[1] HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases](https://aclanthology.org/2025.acl-long.43/) (Lee et al., ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g9wVqi7eY6", "forum": "PkwJjGJ7aN", "replyto": "PkwJjGJ7aN", "signatures": ["ICLR.cc/2026/Conference/Submission13931/Reviewer_ZyZx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13931/Reviewer_ZyZx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781707187, "cdate": 1761781707187, "tmdate": 1762924435265, "mdate": 1762924435265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Graph-R1, a GraphRAG method that leverage LLMs as agent to interact with constructed knowledge hypergraph. The proposed method is trained via end-to-end reinforcement learning. Experiments on multiple dtasets demonstrate the proposed Graph-R1 outperforms bases while also achieve strong generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Graph-R1 leverage LLMs as agent to interact with graph, which solve the issues of finxed retrieval process with only one-time interaction.\n\n2. The experiments are conducted on multiple datasets and include recent baselines.\n\n3. The authors also conduct detaield ablation studies and generalization comparision."}, "weaknesses": {"value": "1. There is another work GraphRAG-R1 [1], which also use RL to solve the GraphRAG issues. The authors may compare the difference between these two methods.\n\n2. What is the motivation to use Knowledge HyperGraph intead of KG?\n\n\n[1] Yu, Chuanyue, et al. \"GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning.\" arXiv preprint arXiv:2507.23581 (2025)."}, "questions": {"value": "1. Can the propsoed method be applied to KGQA, which contains the explicit KG.\n2. Is the <think> steps necessary for RL?\n3. Why does GRPO significantly outperform PPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "adZtgM68sY", "forum": "PkwJjGJ7aN", "replyto": "PkwJjGJ7aN", "signatures": ["ICLR.cc/2026/Conference/Submission13931/Reviewer_ru5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13931/Reviewer_ru5v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958889138, "cdate": 1761958889138, "tmdate": 1762924434735, "mdate": 1762924434735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Graph-R1, an RL-post-trained agentic GraphRAG framework. It operates on a constructed knowledge hypergraph and answers queries in a multi-turn fashion, i.e., it repeatedly performs a \"think-retrieve-rethink-generate\" loop within the knowledge hypergraph environment. Graph-R1 is trained using GRPO, with rewards based on format correctness and answer accuracy. Comprehensive empirical studies are conducted over multiple QA datasets, demonstrating the effectiveness of Graph-R1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. RL post-training with a multi-turn query and retrieval design makes sense.\n2. The empirical studies are comprehensive.\n3. The paper is well written and nicely illustrated."}, "weaknesses": {"value": "1. The technical contributions seem limited: multi-turn action loop + GRPO + downstream task accuracy reward + format correctness reward are not particularly new. The authors could further clarify the core contributions.\n2. The propositions appear decorative rather than informative.\n3. I am curious about how the knowledge hypergraphs are built. It is not entirely clear to me from Sec. 2.1. Could the authors further clarify how this step is performed?\n4. I am also interested in the generalization behavior of the trained policy. What would happen if the model were trained on all datasets: could it then achieve a universally effective (and maybe better) policy across all test sets?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WjF34YYse6", "forum": "PkwJjGJ7aN", "replyto": "PkwJjGJ7aN", "signatures": ["ICLR.cc/2026/Conference/Submission13931/Reviewer_SR8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13931/Reviewer_SR8i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976150460, "cdate": 1761976150460, "tmdate": 1762924434210, "mdate": 1762924434210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}