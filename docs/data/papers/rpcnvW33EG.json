{"id": "rpcnvW33EG", "number": 22557, "cdate": 1758332817074, "mdate": 1759896859712, "content": {"title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting", "abstract": "The rapid deployment of large language models (LLMs) in consumer applications has led to frequent exchanges of personal information. To obtain useful responses, users often share more than necessary, increasing privacy risks via memorization, context-based personalization, or security breaches. We present a framework to formally define and operationalize data minimization: for a given user prompt and a response model, quantifying the least privacy-revealing disclosure that maintain utility, and propose a priority-queue tree search to locate this optimal point within a privacy-ordered transformation space. We evaluated the framework on four datasets spanning open-ended conversations (ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth answers (CaseHold, MedQA), quantifying the achievable data minimization with nine LLMs as the response model. Our results demonstrate that, for the same user prompts, larger frontier LLMs can tolerate stronger levels of data minimization while maintaining task quality. In contrast, smaller open-source models are less robust to aggressive minimization. In addition, by comparing with our oracles, we show that LLMs are poor predictors of data minimization, exhibiting a consistent bias toward abstraction that leads to significant oversharing. By providing an oracle for data minimization, our framework establishes a principled and empirically validated way to balance privacy preservation with task utility in LLM applications.", "tldr": "We introduce a framework that operationalizes data minimization of LLM prompting as the least privacy-revealing prompt that preserves task utility, showing more capable LLMs tolerate greater minimization and establishing a predictive baseline.", "keywords": ["NLP", "privacy", "LLM", "data minimization", "data sanitization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e50528d4dfbee02d7bceaa7515bfacdbdf3c65b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework to formally define and operationalize data minimization, in the context of users sharing personal information to LLMs. For a given user prompt and response model, they quantify the least privacy-revealing disclosure that maintains utility, and propose a priority-queue tree search to locate this. They evaluated this on a diverse set of four datasets and nine LLMs, and show that for the same user prompts, larger frontier LLMs can have stronger data minimization while maintaining task quality. They also show that LLMs are poor predictors of data minimization and have a bias towards oversharing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper formally operationalizes data minimization in the context of privacy-preserving prompts for LLMs and presents an algorithm for doing this, which is different from existing approaches that mainly focus on detecting personal information and redacting or abstracting it.\n- The authors ran a systematic evaluation on a good range of datasets and LLMs and were able to show how more powerful frontier models can be better for data minimization."}, "weaknesses": {"value": "- It is not clear how to directly apply these findings to real applications, since the data minimization algorithm involves querying and checking the utility for multiple variants of the original prompt that will reveal the personal information, and might also take significant cost and latency.\n- It would be good to include more details about the PII annotation, such as how the annotators were chosen, and more information about the amount of consensus. For instance, in table 1, how many examples are there with human consensus at least 0.8?"}, "questions": {"value": "- How could the results be applied to real applications?\n- Could more details about the PII annotation be provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dTUDjDNt5p", "forum": "rpcnvW33EG", "replyto": "rpcnvW33EG", "signatures": ["ICLR.cc/2026/Conference/Submission22557/Reviewer_NzwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22557/Reviewer_NzwN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418161418, "cdate": 1761418161418, "tmdate": 1762942278031, "mdate": 1762942278031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for reducing privacy leakage in LLM prompting. It first identifies the sensitive spans in the prompt and introduces a priority-queue tree search algorithm that explores combinations of RETAIN / ABSTRACT / REDACT operations on the spans."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper addresses a important problem as how to minimize the privacy leakage of LLM prompt.\nThe paper addresses an important and timely problem — practical data minimization for LLM usage — with clear motivation.\n2.\tThe paper includes comprehensive experimental evaluation with  multiple datasets, models, and attack evaluations.\n3.\tAlthough without strict privacy leakage guarantee, the proposed ranking-based evaluation method offers an insightful and practical perspective for quantifying privacy leakage."}, "weaknesses": {"value": "1. The evaluation of privacy relies on pairwise comparison, lacking of theoretical guarantees.\n2. The paper evaluates utility by comparing the generated output with the target response, treating it as a binary classification problem. This oversimplification fails to reflect the continuous performance characteristics of modern LLMs.\n3. The practicability of the proposed method remains questionable, as it assumes there is a local deployed LLM while the aim of the paper is to protect privacy of  prompt  when using LLM via API."}, "questions": {"value": "1. Could you please present how sensitive are the utility threshold gamma? From my point of view, a little gamma may lead to significant performance degradation in LLMs.\n2. I am curious about  whether the “utility predictor” can be generalized across tasks or must be retrained for each task?\n\n3.During the search process, are all intermediate prompts exposed to the main LLM? If so, how is privacy leakage mitigated? If using different LLMs, assuming a smaller LlM local deployed, can the obtained privacy-preserved still transfere effectively to the larger main LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9pa8iu5P0z", "forum": "rpcnvW33EG", "replyto": "rpcnvW33EG", "signatures": ["ICLR.cc/2026/Conference/Submission22557/Reviewer_Djhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22557/Reviewer_Djhr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724415382, "cdate": 1761724415382, "tmdate": 1762942277744, "mdate": 1762942277744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work addresses the problem of \"oversharing\" of personal data in LLM prompts. The authors study the data minimization problem as a constrained optimization problem where they want to maximize privacy with a strict constraint on utility degradation. The work aims to find minimal prompts for each model that does not degrade utility. The paper proposes a \"Freeze-then-Search\" algorithm to find this optimal prompt. The search space consists of span-level transformations: {RETAIN, ABSTRACT, REDACT} where RETAIN is the least private and REDACT is the most private. To obtain an full ordering on the space, the authors utility a privacy comparator model that is a fine-tuned Qwen2.5-7B-Instruct model.  The search is conduced using a priority-queue tree search and terminates once a prompt is found that satisfies the utility constraint. To measure that utility constraint, the authors use a judge model (GPT-4o) to measure whether the answer from the minimized prompt is compared to a reference response.\n\nThe work finds these minimized prompts for 9 LLMs (from small models like Qwen-0.5B to large ones like GPT-5) on 4 datasets. The authors find that large models are tolerant of far more minimization than small models. \n\nThe work also tries to understand if LLMs are good predictors of these minimized prompts and find that all LLMs are poor predictors of this. They also find that the predictors exhibit a strong bias towards ABSTRACT rather than using REDACT as in the minimized prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper does a good job of formulating the data minimization problem as a constrained optimization problem including defining a well ordered search space.\n- The generated minimized prompts seem useful to make progress on data minimization predictors.\n- The insight that models are poor predictors of predicting the minimal prompt required is interesting."}, "weaknesses": {"value": "- The minimized prompts are model specific.\n-"}, "questions": {"value": "- Is it the case that large models require less information is probably because they are able to infer the missing pieces of information better? \n- The \"abstract\" bias seems interesting. Could it be related to how the model is instructed to generate its minimal prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BEUdXvHJfM", "forum": "rpcnvW33EG", "replyto": "rpcnvW33EG", "signatures": ["ICLR.cc/2026/Conference/Submission22557/Reviewer_nGjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22557/Reviewer_nGjY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953082912, "cdate": 1761953082912, "tmdate": 1762942277368, "mdate": 1762942277368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework to define and operationalize the principle of data minimization (PII redaction) for LLM prompting. The authors formulate data minimization as an optimization problem: finding the most privacy-preserving version of a user prompt (by applying a series of actions to sensitive spans) that still maintains a minimum level of task utility for a given response LLM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, with motivated and formalized problem settings. The introduction and related work sections do a good job of comparing the data minimization formulation within existing privacy-preserving techniques like DP-training or simple sanitization."}, "weaknesses": {"value": "1. The most significant weakness is the lack of comparison against simple, non-LLM baselines. The paper compares its computationally expensive oracle to single-pass LLM predictors. However, it fails to include a much simpler and more practical baseline: using a standard PII identification or NER tool to identify and redact PIIs. \n2.  The methodology relies on an LLM-as-a-Judge (GPT-4o) to evaluate utility for open-ended tasks. While the authors commendably validate their privacy comparator LLM against human consensus, the utility judge is not validated at all. The authors provide no data on the judge's accuracy, its agreement with human assessments of utility, or the robustness of its rubric."}, "questions": {"value": "1. What PII is commonly present in MedQA, a curated medical expert QA dataset? How representative is this of PII that users would actually share, as opposed to PII that is simply embedded in the original exam questions?\n2. Could using test-time compute (Gemini Pro Thinking or Claude Extended Thinking) help close the gap in PII removal between single-pass and expensive search-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VppmywKtWX", "forum": "rpcnvW33EG", "replyto": "rpcnvW33EG", "signatures": ["ICLR.cc/2026/Conference/Submission22557/Reviewer_ofax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22557/Reviewer_ofax"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964449847, "cdate": 1761964449847, "tmdate": 1762942277071, "mdate": 1762942277071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}