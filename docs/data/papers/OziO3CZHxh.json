{"id": "OziO3CZHxh", "number": 2185, "cdate": 1757014893181, "mdate": 1763585168919, "content": {"title": "Beyond Rational Illusion: Behaviorally Realistic Strategic Classification", "abstract": "Strategic classification (SC) studies the interaction between decision models and agents who strategically manipulate their features for favorable outcomes. \nExisting SC frameworks typically rely on the idealized assumption that agents are perfectly rational. \nHowever, evidence from behavioral economics and psychology consistently shows that real-world decision-making is often shaped by cognitive biases, deviating from pure rationality. \nOur work challenges the rational-agent paradigm and formally characterizes its limitations.\nWe introduce the behaviorally realistic strategic classification problem, where manipulations arise from realistic psychological and behavioral biases.\nTo address this problem, we extend prospect theory and behavioral economics into the strategic classification settings and propose a Prospect-Guided Strategic Framework (Pro-SF). \nSpecifically, to reflect agents' actual manipulation in practice, our framework incorporates asymmetry between benefits and costs, different subjective reference points, and irrational probability distortion into the Stackelberg game formulation of SC. This paradigm shift redefines both the agent’s strategic manipulation dynamics and the classifier’s optimization objective. Experiments on synthetic and real-world datasets highlight Pro-SF as a behaviorally grounded approach to strategic classification, bridging machine learning and behavioral economics for more reliable deployment in practice.", "tldr": "", "keywords": ["machine learning; prospect theory"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a61b9d9bed1d70c3584696ddc29c53a5e1bca4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends strategic classification (SC) by incorporating behavioral biases into agents’ decision processes. It introduces the Behaviorally Realistic Strategic Classification (BR-SC) problem and a Prospect-Guided Strategic Framework (Pro-SF) based on prospect theory. The paper models 3 types of behaviors: loss aversion, reference bias, and probability distortion. Experiments on synthetic and real datasets show that Pro-SF remains robust across rational, non-rational, and mixed behavioral patterns."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is solid and well-justified. The three behavioral deviations are defined clearly in behavioral economics and represent a realistic extension of strategic classification beyond the rational-agent assumption.\n\n2. The writing is clear and well-structured, with careful explanations of both the theoretical setup and the proposed framework.\n\n3. The experiments are conducted on multiple real and synthetic datasets, showing consistent performance and reasonable robustness under different behavioral settings."}, "weaknesses": {"value": "1. Though I agree with the behavioral deviations from classic SC, I am not sure your modeling of utility is the cleanest way to model irrational SC behaviors. One question is, if we permit different agents to have different utility functions, even the classic norm-based SC cost functions can account for different behavior patterns. For example, if we model the utility as $u(x,x') = f(x') - \\beta c(x,x')$ for some agents and $\\beta > 1$, it seems that we can also model loss aversion. This makes me feel the settings in this paper are a bit artificial and the contribution might be marginal. Are there any convincing reasons we need to use the **Prospect-Based Utility** Eq. (11)? \n\n2. On lines 358-362, it seems that all parameters for the **Prospect-Based Utility** are set by hand, and there is no \"learning\" regading these coefficients. Although the experiments include varying the \"true\" parameters to test robustness, I still feel the authors should discuss choosing parameters from the perspective of the decision-maker.\n\nTo conclude, my main concern is that the utility function is too artificial, but it seems that the key contribution lies in this modeling of utility."}, "questions": {"value": "Could you provide more context justifying your modeling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iHaBkJFqnG", "forum": "OziO3CZHxh", "replyto": "OziO3CZHxh", "signatures": ["ICLR.cc/2026/Conference/Submission2185/Reviewer_ZKzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2185/Reviewer_ZKzn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760465578725, "cdate": 1760465578725, "tmdate": 1762916111090, "mdate": 1762916111090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends strategic classification (SC) by incorporating behavioral biases into agents’ decision processes. It introduces the Behaviorally Realistic Strategic Classification (BR-SC) problem and a Prospect-Guided Strategic Framework (Pro-SF) based on prospect theory. The paper models 3 types of behaviors: loss aversion, reference bias, and probability distortion. Experiments on synthetic and real datasets show that Pro-SF remains robust across rational, non-rational, and mixed behavioral patterns."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is solid and well-justified. The three behavioral deviations are defined clearly in behavioral economics and represent a realistic extension of strategic classification beyond the rational-agent assumption.\n\n2. The writing is clear and well-structured, with careful explanations of both the theoretical setup and the proposed framework.\n\n3. The experiments are conducted on multiple real and synthetic datasets, showing consistent performance and reasonable robustness under different behavioral settings."}, "weaknesses": {"value": "1. Though I agree with the behavioral deviations from classic SC, I am not sure your modeling of utility is the cleanest way to model irrational SC behaviors. One question is, if we permit different agents to have different utility functions, even the classic norm-based SC cost functions can account for different behavior patterns. For example, if we model the utility as $u(x,x') = f(x') - \\beta c(x,x')$ for some agents and $\\beta > 1$, it seems that we can also model loss aversion. This makes me feel the settings in this paper are a bit artificial and the contribution might be marginal. Are there any convincing reasons we need to use the **Prospect-Based Utility** Eq. (11)? \n\n2. On lines 358-362, it seems that all parameters for the **Prospect-Based Utility** are set by hand, and there is no \"learning\" regading these coefficients. Although the experiments include varying the \"true\" parameters to test robustness, I still feel the authors should discuss choosing parameters from the perspective of the decision-maker.\n\nTo conclude, my main concern is that the utility function is too artificial, but it seems that the key contribution lies in this modeling of utility."}, "questions": {"value": "Could you provide more context justifying your modeling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iHaBkJFqnG", "forum": "OziO3CZHxh", "replyto": "OziO3CZHxh", "signatures": ["ICLR.cc/2026/Conference/Submission2185/Reviewer_ZKzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2185/Reviewer_ZKzn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760465578725, "cdate": 1760465578725, "tmdate": 1763647491955, "mdate": 1763647491955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the strategic classification setting, where a principal reveals a classifier and the user can then manipulate the data they submit to the classifier. The classic Stackelberg model considers rational users who perfectly best responds to the classifier - the principal based their strategic optimum based on such an assumption of rationality about the users. Inspired by behavioural econ, this paper looks to challenge that by introducing 3 behavioral tendencies: loss aversion, reference bias, probability distortion. They propose a new behavioural model of user decision making and empirically consider a Stackelberg optimum against such a model.\n\nI think the problem is very well motivated and the three behavioral tendencies have long justification in economics and psycology. Beyond an elaboration of the model, however, I struggle to find the major contribution of the work. The theoretical results on under and over defense are immediate. Given the proposed user behvaioural model, no theoretical results (on learnability or optimizability of the stackelberg equilibrium) are given. I do think it's a major shortcoming, if compelling empirical work was provided. But this falls short on several grounds. First, behavioral models should be backed with some actual human evaluations - they should be communicated the classifier and assessed how they actually manipulate here. Otherwise, one naturally expects the behaviour aware classifier to outperform the control when all agents are assumed to behave exactly as the former expects. Second, are the principals supposed to know the behavioural parameters like $\\kappa, r$? Why would we expect this to be the same for all users. These are tricky questions, but one where we could have more results on if real field deployed experiments were conducted.\n\nAs such, my rating is based on the inconclusive nature of both the theoretical and empirical results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See above."}, "weaknesses": {"value": "See above"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F1rC9MqzDS", "forum": "OziO3CZHxh", "replyto": "OziO3CZHxh", "signatures": ["ICLR.cc/2026/Conference/Submission2185/Reviewer_Y5cB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2185/Reviewer_Y5cB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761436067881, "cdate": 1761436067881, "tmdate": 1762916109691, "mdate": 1762916109691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied the case where full rationality assumption is broken in strategic classification (SC)  and proposed the Behaviorally Realistic Strategic Classification (BR-SC) framework. It also introduced the Prospect-Guided Strategic Framework (Pro-SF) to deal with the problem. Pro-SF explicitly models loss aversion, reference bias, and probability distortion, extending the standard Stackelberg game formulation of SC."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Formulate the BR-SC problem, model three key deviations from rationality, focusing on loss aversion, probability distortion, and reference bias. The over-defense and under-defense case and the corresponding accuracy degradation is introduced.\n- The paper is well-organized, and related literature is discussed.\n- Experiments on synthetic and real-world datasets show that Pro-SF outperforms rational-based baselines under mixed or non-rational agent behaviors."}, "weaknesses": {"value": "- The strategic behaviour and limited rationality is not new in the literature, and the combination of two is also studied a lot.\n- For all three cases: loss aversion, probability distortion, and reference bias, the psychological parameters $(\\alpha, \\beta, \\kappa, \\gamma, K)$ are selected heuristically. There is no empirical evidence that they correspond to actual human behavior.\n- In the experiment, the sensitivity test only provides a small deviation from the ground-truth value, and without prior, it's hard to fine-tune those parameters."}, "questions": {"value": "- How does the Pro-SF framwork scale with high-dimensional problem? Can it be extended to multi-agent or game-theoretic interaction settings beyond independent agents?\n- Is it possible to have some theoretical guarantees about the convergence result under Pro-SF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k8srJGc2qX", "forum": "OziO3CZHxh", "replyto": "OziO3CZHxh", "signatures": ["ICLR.cc/2026/Conference/Submission2185/Reviewer_kBEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2185/Reviewer_kBEE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974268429, "cdate": 1761974268429, "tmdate": 1762916108263, "mdate": 1762916108263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of irrationality in strategic classification. The core contribution is the proposal of an integral utility formulation for agents within the strategic classification framework \"Pro-SC\". The study demonstrates the accuracy degradation in the case of irrational agent behavior under traditional strategic classification methodologies. Furthermore, through experiments and ablation tests, the authors confirm the influence of the various modeled aspects of irrationality and showcase the robustness of the proposed Pro-SC classifier against fluctuations in these irrationality parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The originality of this work is good, and the simulation analysis is complete with high quality.  This work analytically explains and characterizes the negative impact of the overlooked irrationality. This work innovatively delineates and explains well the negative impacts brought by different aspects of irrationality and formulates the Pro-SC problem, which incorporates various aspects of irrationality into the agent's utility function."}, "weaknesses": {"value": "- This paper may need a slight improvement in clarity (see Questions for details).\n- The paper lacks a theoretical analysis of the Stackelberg equilibrium of the Pro-SC game, which could be a drawback in technical depth."}, "questions": {"value": "- Perhaps a missing reference that is relevant: This might be a relevant reference: Raman Ebrahimi, Kristen Vaccaro, and Parinaz Naghizadeh. 2025. The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25). Association for Computing Machinery, New York, NY, USA, 868–886. https://doi.org/10.1145/3715275.3732056\n- I couldn't see where the classifier $f$ enters the agent utility in the Pr-SC formulation of Equation (11). Equation (11) uses the agent's perceived acceptance rate p(x), but it is missing how the classifier $f$ is related to this perceived acceptance. \n- Is there any reference or further explanation of Equation (9)? This function doesn't seem symmetric around p=0.5, and is there a reason for that?\n- Some typos: \n    - Line 313 and in Equation (12): Is D a distribution or a feature space? If the latter, this notation conflicts with the distribution notation in Equation (13) (same for Equations (1) and (2))"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5oVzEFIwcQ", "forum": "OziO3CZHxh", "replyto": "OziO3CZHxh", "signatures": ["ICLR.cc/2026/Conference/Submission2185/Reviewer_E4B5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2185/Reviewer_E4B5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230375259, "cdate": 1762230375259, "tmdate": 1762916105978, "mdate": 1762916105978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}