{"id": "l85ODqN0sc", "number": 8969, "cdate": 1758104927458, "mdate": 1763753891420, "content": {"title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction", "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in the field of video understanding in recent times. Nevertheless, existing video benchmarks predominantly rely on text prompts for evaluation, which often require complex referential language and diminish both the accuracy and efficiency of human–model interaction in turn. To address this limitation, we propose V2P-Bench, a robust and comprehensive benchmark for evaluating the ability of LVLMs to understand Video Visual Prompts in human–model interaction scenarios. V2P-Bench consists of 980 videos and 1172 well-structured high-quality QA pairs, each paired with manually annotated visual prompt frames. The benchmark spans three main tasks and twelve categories, thereby enabling fine-grained, instance-level evaluation. Through an in-depth analysis of current LVLMs, we identify several key findings: 1) Visual prompts are both more model-friendly and user-friendly in interactive scenarios than text prompts, leading to significantly improved model performance and enhanced user experience. 2) Models are reasonably capable of zero-shot understanding of visual prompts, but struggle with spatiotemporal understanding. Even o1 achieves only 71.8%, far below the human expert score of 88.3%, while most open-source models perform below 60%. 3) LVLMs exhibit pervasive Hack Phenomena in video question answering tasks, which become more pronounced as video length increases and frame sampling density decreases, thereby inflating performance scores artificially. We anticipate that V2P-Bench will not only shed light on these challenges but also serve as a foundational tool for advancing human–model interaction and improving the evaluation of video understanding.", "tldr": "A Video-Language Understanding Benchmark for Evaluating LVLMs on Human-Model Interaction through Visual Prompts", "keywords": ["LVLMs", "Video Understanding", "Visual Prompt"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b8dd6665828a0bc6f5a698315f2a829d9ef64fc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new video benchmark that tests models’ abilities at visual question answering while allowing for visual prompting in addition to text.  With visual prompting, users are allowed to annotate the video and ask questions that refer to the annotation (eg., what is the person with the box around them doing?).  The benchmark is built on existing video collections and includes a range of different types of questions, ranging from low level questions about, for example, object attributes, to higher level questions, about plot or causal relationships.  It seems that all questions are designed to require identification of specific people or objects that might be difficult to otherwise describe.  A large number of open and closed models are identified on the benchmark, which still demonstrate performance that is below human level."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The evaluations seem thorough.  Visual prompting does not seem to have been studied adequately in video.  The work seems very comprehensive."}, "weaknesses": {"value": "First, it is not clear how important this problem is.  The questions in the dataset seem designed to benefit from visual prompting.  But how often do users really want to ask questions about people or objects that are difficult to describe?  This might happen often, but there is no evidence given that this is the case.\n\nSecond, it is not clear that the results that visual prompting is more effective and efficient than text prompting are truly valid, since the questions developed seem designed to favor visual prompting.  This would be more convincing if these questions arose organically from users attempting to solve real tasks.  \n\nThird, the fact that the benchmark contains high and low level questions does not seem very significant.  Visual prompting is used entirely to make it easier for a user to specify a person or object.  The only real question is how much more effective this is than specifying people or objects with text.  If the model can interpret the visual prompting, failures to answer higher level questions have nothing to do with the prompt, but more to do with higher level reasoning. \n\nFour, one of the key questions in video understanding is the extent to which models can integrate information from different parts of the video.  Video prompting seems to be primarily used here with questions that are temporally localized, and could perhaps be answered using a single frame of the video."}, "questions": {"value": "Please provide more details on how the questions were chosen.  How do you ensure that you are not selecting questions that will bias performance in favor of visual prompting?\n\nExperiments in Table 3 are not clearly explained.  How are the text and visual prompts generated?  In Table 3b, what interface is provided for visual prompts?  Is training required?  The supplementary material addresses some of this, but I still found it unclear.  This should be clearly explained in the body of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wsCsPNvT10", "forum": "l85ODqN0sc", "replyto": "l85ODqN0sc", "signatures": ["ICLR.cc/2026/Conference/Submission8969/Reviewer_VvkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8969/Reviewer_VvkZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761429630856, "cdate": 1761429630856, "tmdate": 1762920703353, "mdate": 1762920703353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes V2P-Bench, a new benchmark addressing the inefficiency of text-based evaluation in video-language understanding. By using visual prompts directly on video frames, it assesses model performance across perception, temporal, and reasoning tasks. Results show that visual prompts improve model accuracy while exposing weaknesses in long video and spatiotemporal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces the novel idea of using visual prompts for video-language evaluation, providing a more intuitive and human-aligned alternative to text-based prompts. The dataset and tasks are well designed, with careful annotation and broad experimental coverage across major LVLMs. The presentation is clear and well-structured, supported by effective figures and analyses."}, "weaknesses": {"value": "The main weaknesses of this paper lie in its theoretical and methodological depth. While the idea of visual prompts is novel, the work lacks a stronger theoretical or cognitive grounding to explain why this approach better reflects human interaction. The evaluation remains limited to offline video QA with a relatively small dataset and does not include audio or multimodal signals, which limits its realism and scalability. In addition, the analysis is mostly descriptive, without deeper investigation into why models fail in spatiotemporal reasoning or specific error types."}, "questions": {"value": "I have several critical questions that the authors should address to clarify the scientific validity of the work.\n\nFirst, the central claim, that visual prompts are a more human-aligned form of interaction, remains unsubstantiated. Could the authors provide theoretical grounding or empirical evidence (e.g., from cognitive studies or user experiments) rather than relying on intuitive justification?\n\nSecond, the benchmark design is still based on offline video QA, which seems inconsistent with the paper’s framing around “human–model interaction.” How do the authors ensure that this static setup meaningfully reflects real interactive understanding?\n\nThird, the error analysis is largely descriptive and lacks mechanism-level insight. Why do models consistently fail on spatiotemporal reasoning tasks, and how can the authors be sure that the evaluation protocol effectively eliminates “hack” behaviors rather than concealing them? \n\nAddressing these questions is essential to assessing the scientific rigor and actual contribution of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xG47oj0bA1", "forum": "l85ODqN0sc", "replyto": "l85ODqN0sc", "signatures": ["ICLR.cc/2026/Conference/Submission8969/Reviewer_8395"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8969/Reviewer_8395"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715227285, "cdate": 1761715227285, "tmdate": 1762920703023, "mdate": 1762920703023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes V2P-Bench, a robust and comprehensive benchmark for evaluating the ability of LVLMs to understand video visual prompts in human–model interaction scenarios. The authors argue that existing video benchmarks primarily rely on text prompts, which often involve complex referential language and, in turn, reduce the accuracy and efficiency of human–model interaction. To address this, they introduce more user-friendly visual prompts. The authors conduct a comprehensive analysis and highlight several key findings, such as performance on spatiotemporal understanding and the prevalence of the hack phenomenon."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated and studies a timely problem of visual prompt-based evaluation instead of unnecessarily overcomplicated textual prompts. I particularly liked that the authors clearly established this with a human study as well.\n2. The benchmark is carefully curated with multiple automated and human checks.\n3. The authors conduct comprehensive experiments to study different models, and go beyond standard evaluation to study interesting hack phenomenon and other ablations."}, "weaknesses": {"value": "While I don't foresee any major weaknesses, I have some questions about experiment design and further experiments that I would like to see before I raise my score further."}, "questions": {"value": "1. The hack phenomenon is concerning, especially as it increases rates as the video length grows. I am curious if the authors convert their benchmark to an open-ended generation one instead of MCQ-based, would that help in reducing some of this? \n\n2. I am curious how does the structuring of the visual query itself impact the performance? For instance, what if you have scribbled unstructured bounding boxes instead of perfect rectangles or other more natural markers that are more user-friendly, how does that impact overall performance? \n\n3. From many of the query examples provided in the paper, it appears that most could be resolved using only a few frames, with the main bottleneck being the grounding of the visual query frame. Could the authors elaborate on this point, perhaps by quantifying it using temporal certificates [1] or frame rate ablations? Specifically, do the models on their benchmark perform better as more visual information is provided, or can the benchmark generally be answered using just a few frames?\n\n[1] EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zn9KPBA4KB", "forum": "l85ODqN0sc", "replyto": "l85ODqN0sc", "signatures": ["ICLR.cc/2026/Conference/Submission8969/Reviewer_9ULh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8969/Reviewer_9ULh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887351438, "cdate": 1761887351438, "tmdate": 1762920702644, "mdate": 1762920702644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces V2P-Bench, a benchmark designed to evaluate large vision-language models (LVLMs) using visual prompts instead of text prompts for video understanding, enabling fine-grained assessment of spatial, temporal, and reasoning abilities. Results show that visual prompts improve both model accuracy and user experience, but current LVLMs still lag behind humans, particularly in spatiotemporal comprehension and robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a visual prompt–based benchmark that improves realism in human–model interaction evaluation.\n- Provides a comprehensive dataset with multi-level tasks and rigorous annotation ensuring high quality and diversity.\n- Offers clear empirical insights into LVLM weaknesses, including hack phenomena and spatiotemporal reasoning gaps."}, "weaknesses": {"value": "1. The benchmark currently focuses on synthetic QA tasks rather than natural, conversational interactions, which limits its generalization to real-world human–model dialogues. How would you integrate free-form conversational or multimodal dialogue settings to better simulate authentic human–AI interaction? If not, it is really not geberalisable. \n\n2. The dataset construction process is highly manual and time-consuming, making it difficult to scale to larger domains or real-time applications. How can we incorproate scalability?\n\n3. Several analyses, such as the user experience evaluation and hack behavior detection, rely on qualitative interpretation rather than standardized quantitative frameworks. Cou.d the authors design more rigorous, quantitative evaluation protocols and reproducible metrics to strengthen the reliability and comparability of findings?\n\n4. The benchmark primarily evaluates static performance outcomes but does not assess model adaptability or learning progression over time."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lc0mTOHcfe", "forum": "l85ODqN0sc", "replyto": "l85ODqN0sc", "signatures": ["ICLR.cc/2026/Conference/Submission8969/Reviewer_rfAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8969/Reviewer_rfAK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889136416, "cdate": 1761889136416, "tmdate": 1762920701955, "mdate": 1762920701955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}