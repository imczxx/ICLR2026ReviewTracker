{"id": "5wqTal0EuC", "number": 803, "cdate": 1756818765595, "mdate": 1763676442846, "content": {"title": "Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity", "abstract": "Asynchronous stochastic gradient methods are central to scalable distributed optimization, particularly when devices differ in computational capabilities. Such settings arise naturally in federated learning, where training takes place on smartphones and other heterogeneous edge devices. In addition to varying computation speeds, these devices often hold data from different distributions. However, existing asynchronous SGD methods struggle in such heterogeneous settings and face two key limitations. First, many rely on unrealistic assumptions of similarity across workers' data distributions. Second, methods that relax this assumption still fail to achieve theoretically optimal performance under heterogeneous computation times. We introduce Ringleader ASGD, the first asynchronous SGD algorithm that attains the theoretical lower bounds for parallel first-order stochastic methods in the smooth nonconvex regime, thereby achieving optimal time complexity under data heterogeneity and without restrictive similarity assumptions. Our analysis further establishes that Ringleader ASGD remains optimal under arbitrary and even time-varying worker computation speeds, closing a fundamental gap in the theory of asynchronous optimization.", "tldr": "We show that asynchronous SGD can be provably optimal under heterogeneous data by buffering gradients and updating the model at the right time.", "keywords": ["asynchronous SGD", "data heterogeneity", "optimal time complexity", "nonconvex optimization", "parallel methods", "stochastic optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39eed73d3aa0f991945d0fe258bce5ca178d2235.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies asynchronous optimization methods for smooth nonconvex functions while relaxing the similarity across workers’ data and considering heterogeneous computational times. The authors proposed a new asynchronous algorithm called Ringleader SGD, which they show to achieve the theoretically optimal rate in the regimes considered in the paper."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposed an asynchronous optimization method called Ringleader SGD. This method extends the previous method of Ringmaster SGD to heterogeneous settings. When $L = O(L_f)$, the method of Ringleader SGD is proven to match the algorithmic independent lower bound proven in (Tyurin & Richtarik, 2024). Despite the minimax optimal rate is already achieved by a synchronous method called Malenia SGD, Ringleader SGD is a truly asynchronous variant and does not discard any gradient works. Also, the new algorithm does not need to know the gradient variances and target accuracies. \n\nI check the proofs of the main theoretical results in section 5. The proofs seem correct to me. However, I did not check the proofs of the additional results on time-varying delays."}, "weaknesses": {"value": "The Malenia SGD is already optimal in the regime considered in this paper. Therefore, despite the arguments of “truly asynchronous method” and “no discarding work”, it is impossible to show any theoretical improvement over Malenia SGD in the same regime. I totally agree on this one, and this is not the weakness. *However,* I was wondering whether it is possible to show any improvement over Malenia SGD under certain cases. On the theory side, under additional restriction of data similarity, whether Ringleader SGD might be provably faster? \n\nSince there isn’t any clear theoretical advantage and the scale of the numerical experiments provided in this paper is also not convincing enough, the truly non-incremental part of contribution seems to be a parameter-free method without knowing gradient variances and target accuracies in advance. Hence, I think this contribution alone is a bit limited."}, "questions": {"value": "My only question is about the presentation of the results. Why didn’t the authors write down the algorithmic-independent lower bound from (Tyurin & Richtarik, 2024) explicitly in the paper? They claimed their algorithm is optimal throughout the paper, but the readers will have to download (Tyurin & Richtarik, 2024) and check the lower bound therein to confirm it. It would be great to cite the lower bound of $\\Omega \\left( \\frac {L_f \\Delta} {\\epsilon} \\left( \\tau_n + \\tau _{avg} \\frac {\\sigma^2} {n \\epsilon} \\right)  \\right)$ as a separate theorem statement, or at least include it explicitly as a row in Table 1. \n\nOne suggestion is to present the results more modestly. While the authors keep claiming their methods are “optimal” (with the word “optimal” appearing for about 20 times in the paper), it should be noted that the optimality is only achieved when $L = O(L_f)$, and their method is not optimal in the general regime. The authors have mentioned this but only in the footnotes. I would recommend this restriction on “optimality” to be stated explicitly at least in the contribution and in the remarks below their main theorem. From the current presentation, it is very misleading in general, since their method might be even worse than Minibatch SGD in certain regimes. (Just to clarify, I think stating the exact restrictions helps to avoid vagueness, thus making the true contribution of this paper clearer.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jPFeN4U9Fl", "forum": "5wqTal0EuC", "replyto": "5wqTal0EuC", "signatures": ["ICLR.cc/2026/Conference/Submission803/Reviewer_nJYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission803/Reviewer_nJYq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760624353206, "cdate": 1760624353206, "tmdate": 1762915608381, "mdate": 1762915608381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Ringleader ASGD, an asynchronous stochastic gradient descent (SGD) algorithm that claims to achieve optimal time complexity under both data and computation heterogeneity. The method integrates mechanisms from several prior works: it uses a gradient table similar to IA2SGD, employs delay control ideas from Ringmaster ASGD, and adopts a two-phase update structure inspired by Malenia SGD. Theoretical results show that, under a fixed computation-time model, the algorithm matches the known lower bound for parallel first-order stochastic methods, thereby closing a theoretical gap regarding the optimality of asynchronous SGD in heterogeneous settings.\n﻿\n\nI should note that I am **not** an expert in asynchronous or distributed optimization theory, so my ability to judge the novelty and technical rigor of the proofs is limited. From my reading, the paper appears clearly written and the analysis internally consistent, but I would encourage the area chair to give greater weight to the opinions of reviewers with deeper expertise in this area, particularly regarding the theoretical contributions and their significance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong theoretical contribution.**\nThe paper provides a rigorous theoretical analysis establishing that Ringleader ASGD achieves the *optimal time complexity* under heterogeneous data and computation speeds. This closes an open gap left by prior work which achieved optimality only in synchronous or homogeneous settings.\n﻿\n2. **Clear connection to prior literature.**\nThe authors carefully position their method relative to previous parallel SGD variants and articulate how each component contributes to achieving both asynchrony and optimality. The theoretical results are clearly stated and supported by well-organized proofs in the appendix.\n﻿\n3. **Conceptual clarity of the design.**\nThe proposed two-phase structure (gradient collection and sequential asynchronous updates) provides a clear mechanism to control gradient delays while avoiding idle workers or wasted computation. The algorithmic description is logically consistent and easy to follow."}, "weaknesses": {"value": "The experimental section is limited and does not convincingly demonstrate the practical relevance of the proposed algorithm. Experiments are restricted to small datasets (MNIST, Fashion-MNIST) and a simple two-layer MLP, which do not reflect realistic distributed or federated learning scenarios."}, "questions": {"value": "The experimental section is rather limited, focusing only on small-scale simulations with simple MLP models on MNIST and Fashion-MNIST. Could the authors provide more comprehensive empirical evaluations to better demonstrate the practical impact of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Dc4IoMZw6k", "forum": "5wqTal0EuC", "replyto": "5wqTal0EuC", "signatures": ["ICLR.cc/2026/Conference/Submission803/Reviewer_M5AX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission803/Reviewer_M5AX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475827204, "cdate": 1761475827204, "tmdate": 1762915608259, "mdate": 1762915608259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Ringleader ASGD, a stochastic gradient descent method designed to achieve optimal time complexity in distributed nonconvex optimization under data heterogeneity. The work addresses a fundamental gap in asynchronous optimization theory by proposing a method that avoids restrictive similarity assumptions across workers' data distributions while matching established lower bounds."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a rigorous analysis of Ringleader ASGD, proving optimal time complexity."}, "weaknesses": {"value": "1. The presence of the initial gradient collection phase (Phase 1) introduces a dependency whereby the failure of any single worker node halts the entire algorithmic process. Consequently, Ringleader ASGD cannot be classified as purely asynchronous. Its core design represents a hybrid methodology that integrates elements of both synchronous and asynchronous paradigms. \n\n2. Each iteration of Ringleader ASGD may require numerous gradient computations. The paper only provides both iteration complexity and time complexity. Could you analyze its computational complexity and compare it with other methods?\n\n3. In my opinion, academic writing should adhere to fundamental structural conventions. The experimental section and conclusions should be placed within the main body of the text rather than in an appendix.\n\n4. The experimental scope could be strengthened by evaluating the method on more challenging benchmarks. Expanding the experiments to include larger-scale datasets (e.g., ImageNet, CIFAR-100) and more complex model architectures would help better validate the robustness and scalability of the proposed approach."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aUIttdbkpv", "forum": "5wqTal0EuC", "replyto": "5wqTal0EuC", "signatures": ["ICLR.cc/2026/Conference/Submission803/Reviewer_kQKe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission803/Reviewer_kQKe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983756071, "cdate": 1761983756071, "tmdate": 1762915608143, "mdate": 1762915608143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the task of finding an approximate stationary point for the average of smooth and non-convex objectives in a distributed optimization framework. It proposes and analyzes a new asynchronous optimization algorithm, Ringleader ASGD. The primary setting the paper discusses is that of fixed but heterogeneous computation speeds on different clients. In the regime where communication times to and from the server are negligible, the algorithm is shown to have optimal time complexity, up to a difference in the second-order smoothness constants across the machines. \n\nIn the appendix, the analysis is also extended to other, more general asynchronous models, along with experiments that demonstrate Ringleader SGD offers performance competetive to other baseline algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, the setting is rigorously defined, and the baseline algorithms and related works are discussed clearly and sufficiently. The key theoretical result of the paper is stated clearly, and both the build-up to the algorithms and their proof sketch provide helpful intuition for the problem. Overall, it is straightforward to follow the paper's contribution. The experiments offered in the appendix also provide insight into the algorithm's actual performance."}, "weaknesses": {"value": "My main criticism of the paper and the related literature is that it seems unreasonable to ignore communication costs, given that they are well known to often comprise one of the most significant chunks of overall time complexity in practice, in both synchronous and asynchronous workloads. Within the scope of the assumptions made in the paper, all the results and conclusions are valid; however, I believe that emphasizing optimality in this arguably unrealistic computation-communication model is potentially misleading. I list some explicit questions about this below. \n\nI do not expect the authors to consider communication costs in this work, but I want to see a thorough discussion of the limitations of this setting before I am convinced to raise my score.  \n\nSimilarly, I am also a little concerned about the claim that the work does not consider any heterogeneity assumptions, because Assumption 2 appears to be one. Specifically, the assumption appears to be related to the second-order heterogeneity assumption, which has been considered in the [non-convex](https://openreview.net/forum?id=SNElc7QmMDe) and [convex](https://arxiv.org/abs/2405.11667) settings."}, "questions": {"value": "1. The authors discuss the issue of non-negligible communication costs (arguably the most realistic setting) only once while mentioning Tyurin et al. (2024b). On the other hand, multiple places in the paper, including the title, discuss optimal time complexity. This is a bit misleading. I urge the authors to dedicate a whole section to communication costs, discussing any known results in the setting with communication costs, their own algorithm and its guarantees in that setting, and potential sources of sub-optimality. In the experiments, it would be beneficial to simulate communication costs and demonstrate how the proposed algorithm performs compared to other baselines, as communication forms a larger portion of the overall physical time. Given that the improvements with respect to IA-SGD are not drastic, I am curious how these different algorithms compare when communication costs increase. Intuitively, since IA-SGD has lower overhead per communication (because buffers and other states do not need to be maintained/updated on the server), one would expect it to improve upon ringleader SGD when communication costs increase/overhead per communication increases. This can be simulated by changing model size or by increasing parallelization, which would force the server to \"wait out\" any clients trying to communicate. \n\n2. It would also be good to discuss as well as implement a variant of minibatch SGD with different mini-batch sizes on each machine, which is another natural synchronous baseline in the face of different compute speeds. In a sense, this is the simplest synchronous algorithm, not the naive mini-batch discussed in Section 3. When communication costs are high, I would expect this algorithm to outperform both IA-SGD and ringleader SGD, as it requires fewer locks (a single lock per communication round between all devices). In the setup discussed in the previous bullet point, as the authors vary the communication cost, this should intuitively beat all baselines at some point. \n\nThe last two points highlight the age-old heuristic in distributed optimization: \"Is the precious compute on the server better spent in compressing/implementing asynchrony/de-biasing or running more synchronous updates for a fixed time budget?\" Naturally, this is an empirical question, and the role of the theory should be to help weed out different regimes where different choices make sense. \n\n3. The discussion around Assumption 2 and the need for data similarity assumptions in this paper is a bit unclear to me. To me, it appears that in the non-trivial regime where $L^2$ is smaller than the average smoothness-squared across the machines, it essentially amounts to a data heterogeneity assumption. For instance, in the synchronous setting where Assumption 2 is only required on $x = \\frac{i}{n}\\sum_{i\\in[n]}y_i$, then it is akin to a second-order data heterogeneity assumption, which several works have considered for analysing local update algorithms ([1](https://openreview.net/forum?id=SNElc7QmMDe), [2](https://arxiv.org/abs/2008.03606), [3](https://arxiv.org/abs/2102.03198), [4](https://arxiv.org/abs/2405.11667), [5](https://arxiv.org/abs/1910.06378)). I understand that if the authors assume all clients are smooth, it is in some sense not restrictive to think of $L$ as just a smoothness constant; however, I would like to see a discussion of the second-order heterogeneity literature. More specifically, can the analysis in this paper be extended to the setting with a bounded second-order heterogeneity constant? Hence, the time-complexity decouples between a heterogeneity term, and another that depends on $L_f$? As a motivation, in the quadratic setting, instead of the right-hand side of Assumption 2 with just L one can write an upper bound of, $\\frac{2}{n}\\sum_{i\\in[n]}\\left(L_f^2\\|\\|x-y_i\\|\\|_2^2 + \\tau^2 \\|\\| y_i - \\bar y\\|\\|_2^2\\right),$ where $\\bar y$ is the average of $y_i$'s and $\\tau$ is the second-order heterogeneity parameter. It is unclear whether this alone will lead to an analysis in the asynchronous setting, but it at least highlights that the constant $L$ encapsulates both $L_f$ and a data-heterogeneity effect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qCyZvKp3XT", "forum": "5wqTal0EuC", "replyto": "5wqTal0EuC", "signatures": ["ICLR.cc/2026/Conference/Submission803/Reviewer_rk1S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission803/Reviewer_rk1S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031009860, "cdate": 1762031009860, "tmdate": 1762915608037, "mdate": 1762915608037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}