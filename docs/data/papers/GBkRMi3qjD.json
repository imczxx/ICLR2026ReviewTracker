{"id": "GBkRMi3qjD", "number": 19637, "cdate": 1758297889190, "mdate": 1759897028899, "content": {"title": "Understanding and Relaxing the Limitations of Transformers for Linear Algebra", "abstract": "Matrix operations, such as linear solves, eigendecompositions, and log determinants, are foundational building blocks for any number of downstream applications. Therefore, any broadly capable learning system should be able to effectively approximate these operations in its internal representation. Accordingly, there is great motivation to study transformers for linear algebra --- for if transformers cannot even semi-competently perform matrix operations, then we cannot expect them to form a basis for a generally intelligent system. We demonstrate that current techniques developing transformers for linear algebra have striking failure modes, prohibitive scaling, and particularly poor out-of-distribution generalization to other matrix distributions, and matrices of different sizes. Investigating further, we find that current transformer approaches operate as statistical interpolators, rather than discovering algorithms that will generalize to matrices from other distributions. Based on our understanding of these limitations, we develop a sequence of interventions that substantially improve scaling and performance, including matrix embeddings through a learnable projection, linear attention, looping, and a data pre-training distribution of structured matrices. We term the resulting method the \\emph{RangeFormer}, which we show has significantly improved scaling and performance on challenging OOD matrices from the \\emph{matrix market}. Moreover, with RangeFormer we show for the first time that transformers can be successfully applied to downstream tasks that involve iterative matrix operations, including Gaussian process learning, and improving the sampling distribution of randomized methods.", "tldr": "We show that current transformer methods for linear algebra are simply learning statistics about the data", "keywords": ["Numerical Linear Algebra", "Transformers", "Out-of-distribution", "Generalization"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37e77c45db6e6030356cfa5d97999d9581520879.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The goal of the paper is to study the capability of the transformer architecture to perform numerical linear algebra tasks, such as linear system solving, computing SVD and EVD and so on. The first contribution of the paper is that the existing architecture approaches is unable to learn generalizable algorithms and show OOD generalization. Their main contribution is RangeFormer, which has various features such as learnable matrix embeddings, linear attention, and a diversifying the training distribution. These improvements first  reduce the make the transformer efficient for linear algebra tasks and also improves OOD generalization.\nThey show their new architecture can scale well compared to the classical transformer in performing various tasks such as Gaussian process learning and randomized numerical linear algebra. Furthermore the paper also explores using transformers to improve the performance of classical methods such as Krylov subspace methods and randomized SVD by providing better initialization and sampling distributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is in using linear algebraic tasks as a benchmark for understanding the limitations of transformer architecture. Various works have shown such limitations for different algorithmic tasks, and this paper delves deeper into tasks pertaining to implementing algorithms for numerical linear algebra."}, "weaknesses": {"value": "Perhaps a minor weakness of the paper is in terms of the writing, and I would suggest the authors to include a main summary of their key findings in the beginning of the paper."}, "questions": {"value": "I found the paper quite interesting and I have a few questions regarding it. Firstly there has been quite a lot of work on linear algebraic tasks such as linear regression and the ability of transformers to solve them in context ( for eg. refer to this work https://arxiv.org/abs/2211.15661). As far as I understand your current work pretrains the architecture for linear algebraic tasks, however it would be interesting to know if claims along the lines that the transformer has enough expressivity to perform tasks such as SVD or linear system solving in context would be quite interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WI6ku24bqP", "forum": "GBkRMi3qjD", "replyto": "GBkRMi3qjD", "signatures": ["ICLR.cc/2026/Conference/Submission19637/Reviewer_gjAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19637/Reviewer_gjAT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511160085, "cdate": 1761511160085, "tmdate": 1762931491062, "mdate": 1762931491062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper performs an empirical analysis of transformers trained in a supervised manner to perform linear algebra tasks. The authors design detailed experiments to identify the bottlenecks of transformers in performing such operations. Specifically, they find that transformers tend to learn properties of the input data distribution rather than the underlying matrix operation algorithms.\nTo address this, the authors propose a new training recipe and a novel combination of architectural components to mitigate the identified limitations. Finally, they evaluate the improved architecture on three downstream applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper conducts a detailed and comprehensive analysis of the limitations of transformer-based architectures in performing matrix operations. It further proposes a new model architecture and training recipe, and evaluates them on a wide range of tasks to demonstrate their effectiveness.\n\n* The paper provides thorough details of all experimental settings, which is good for future reproducibility.\n\n* The work uses simple linear algebra tasks as a testbed to reveal that transformers may primarily learn the statistical properties of the data distribution rather than the underlying algorithmic procedures."}, "weaknesses": {"value": "* Overall, the paper is a solid empirical study. However, my main concern is in the motivation for investigating the transformer’s ability to perform such simple linear algebra tasks.\nAs the authors repeatedly emphasize, (e.g., line 86: “We note that the goal of our work is in understanding the potential capabilities and limitations of transformers for linear algebra…”; line 472: “This research area is in its early stages… our goal is not to compete with purpose-built classical algorithms…”),  the stated goal is primarily exploratory. However, it remains unclear why understanding transformers’ capabilities on these tasks is important, and what future directions this understanding enables. Does this line of research aim to eventually develop a single transformer that outperforms classical linear algebra solvers? Why is a new transformer architecture needed to solve problems that classical solvers already handle perfectly? It is also not evident whether the insights obtained from these experiments will translate to a better understanding of transformer usefulness in real-world analytical settings.\n\n* Many experiments in the main text lack multi-run results with different random seeds. While the appendix provides details about whether hyperparameter optimization was performed, the plots in the main text appear to come from single runs. Including multiple seeds would strengthen the statistical reliability of the results. Since the models are relatively small and trained for fewer than 10,000 iterations, adding multi-run averages and std should be feasible.\n\n\n* Some aspects of the writing logic could be improved. For example, in line 297 -line 300, the authors state \"We now construct, step-by-step, our RangeFormer architecture, and introduce our synthetic data procedure....\", but the following paragraph immediately shifts to discussing experimental results instead of the mentioned architecture."}, "questions": {"value": "*  I appreciate the detailed and carefully designed experiments examining the transformer’s limitations in solving linear algebra tasks. However, as mentioned in the weakness section, I would appreciate a more in-depth explanation of the importance and broader motivation behind this problem. Specifically, what kind of future research directions or practical applications does understanding transformers’ ability to perform linear algebra enable?\n\n*  Could the authors clarify why they selected the three specific downstream tasks in Section 5? Are these tasks directly related to the failure cases identified in Section 3, or do they represent scenarios where classical solvers perform poorly or are computationally expensive, thus providing opportunities where RangeFormer offers advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fH6gYhPlQt", "forum": "GBkRMi3qjD", "replyto": "GBkRMi3qjD", "signatures": ["ICLR.cc/2026/Conference/Submission19637/Reviewer_w4zs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19637/Reviewer_w4zs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672738243, "cdate": 1761672738243, "tmdate": 1762931490398, "mdate": 1762931490398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive empirical investigation into the capability of transformer architectures to execute fundamental linear algebraic operations. Through their simulations, the authors assess whether transformers can accurately perform tasks such as vector addition, matrix multiplication, and eigenvalue estimation. The results provide empirical evidence that, despite their expressive power, transformers—particularly those constructed following existing theoretical proofs—struggle to reliably perform even simple linear algebra operations. \nFinally, the paper proposes a new version of transformer that is capable of performing those tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The simulation section is comprehensive.\n* The research question this paper tries to address is indeed new to the ML community."}, "weaknesses": {"value": "My main concern of this paper is that I do not see how the results from this paper would provide insights to either empirical or theoretical studies of transformer.\nI believe a main statement this paper tries to make is that the transformer constructed in [Garg et al., 2023, Von Oswald et a. 2023], does not appears to be the same as the learned transformer.\nHowever, this does not seem to be a significant insight to the theoretical community.\nIt is obvious that in a series of works on \"transformers simulate algorithms\", their proofs are mainly constructive, meaning they only show the existence of a solution instead of the uniqueness of it.\nTherefore, while analyzing transformers block-by-block seems to be a new approach to my knowledge, I think it is obvious that trained transformers will not necessarily converge the the exact construction from previous studies.\n\n\nMy second concern is that even from the empirical perspective, I am unsure what does the insight or the proposed model from this paper contribute to empirical ML.\nI understand that this paper is not trying to push SOTA performance. However, if ones goal is to utilize transformers for linear algebra operations, why not simply follow the constructions in previous studies?"}, "questions": {"value": "1. Can the authors explain why would adding an extra matrix embedding layer would help the performance? At least in the linear regression task [Garg et al., 2023, Von Oswald et a. 2023], multiplying the data matrix with a random matrix results in another linear regression problem. Therefore, intuitively I do not understand how would it work.\n\n2. Can the authors address my two concerns? It is possible that I misunderstand some of the goals of this paper.\n\n3. One observation from the paper is that trained transformers do not have good OOD generalization on linear algebra tasks, which seems to be an universal issue other than for this specific task. Would training transformers on a more diverse dataset gain performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4MbdRJDt4H", "forum": "GBkRMi3qjD", "replyto": "GBkRMi3qjD", "signatures": ["ICLR.cc/2026/Conference/Submission19637/Reviewer_XjAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19637/Reviewer_XjAK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679885269, "cdate": 1761679885269, "tmdate": 1762931489741, "mdate": 1762931489741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper begins by identifying the inherent limitations and failure modes of Transformer-based approaches to matrix computation. The authors’ analysis reveals that standard Transformers primarily act as statistical interpolators rather than genuine algorithm discoverers, which leads to particularly poor performance on out-of-distribution (OOD) data. Building upon these insights, the paper introduces a series of architectural improvements aimed at enhancing both scalability and accuracy, culminating in a new model termed RangeFormer. The proposed architecture demonstrates substantially improved scalability and performance on OOD matrices from the Matrix Market collection. Moreover, the paper provides, to the best of the authors’ knowledge, the first empirical evidence that Transformers can be effectively applied to downstream tasks involving iterative matrix operations, such as Gaussian process learning and improving sampling distributions in randomized numerical methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces a learnable projection mechanism that embeds the input matrix into a higher-dimensional latent space. This design substantially improves computational efficiency and scalability, while enhancing the Transformer’s performance on matrix-multiplication–type tasks.\n\n* By adopting a looped Transformer structure inspired by iterative methods in numerical linear algebra, the model can repeatedly process the same matrix multiple times. This enables adaptive refinement and better handling of tasks with varying difficulty and condition numbers.\n\n* The authors construct a novel structured data mixture for training, consisting of matrices with diverse structures and eigenspectrum decay patterns. Compared to conventional Gaussian-random training data, this approach significantly improves out-of-distribution (OOD) generalization.\n\n* The paper provides the first empirical demonstration that Transformers can be successfully applied to downstream tasks involving iterative matrix operations, such as Gaussian process learning and improving sampling distributions in randomized numerical methods."}, "weaknesses": {"value": "I will admit I am not very familiar with this paper, so I am basing my review more on the clarity of the writing. \n\n\n1. The theoretical explanation of the RangeFormer design remains largely intuitive. The paper proposes four key modifications, range embedding, looping, linear attention, and structured data mixture, but their effectiveness is demonstrated only empirically, without rigorous theoretical justification or detailed analysis. In particular:\n* Why can the projection operator $X = A\\Gamma$ preserve the matrix spectrum or operator properties?\n* Must the projection be Gaussian? In practice, Gaussian projections are dense and computationally expensive, could sparse alternatives such as CountSketch or other structured embeddings achieve similar effects?\n* Why does *linear attention* improve the approximation of matrix multiplications?\n* What are the convergence and stability guarantees of the *looping* mechanism?\n* Does the structured data mixture genuinely improve algorithmic generalization, or does it merely enlarge the training support domain?\n\n2. RangeFormer is derived from the baseline NumFormer through several architectural modifications. While the paper describes these individual changes, it does not provide a complete framework diagram of RangeFormer that would help readers clearly understand the overall workflow and methodological pipeline of the proposed model."}, "questions": {"value": "* The notation in the figures (e.g., “HP”) is insufficiently explained, making them difficult to interpret.\n* It is recommended that the main text include additional references to the literature on RandNLA and RSVD.\n* In the comparison between RSVD and RFSVD, the paper only reports the error metrics but does not provide a comparison of the running time. Although RFSVD improves accuracy, it remains unclear whether this improvement comes at the cost of increased computational time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NQ51cR40lc", "forum": "GBkRMi3qjD", "replyto": "GBkRMi3qjD", "signatures": ["ICLR.cc/2026/Conference/Submission19637/Reviewer_Qhzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19637/Reviewer_Qhzi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818908142, "cdate": 1761818908142, "tmdate": 1762931488837, "mdate": 1762931488837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Starting with the argument that a \"generally intelligent system\" should be perfectly competent at performing matrix operations, this work studies Transformers' ability to perform primitives relevant to linear algebra (e.g., eigenvalue computations, trace computation, etc.). Results show the Transformers architecture explored in this work is unable to perform well, but interventions on the architecture based on seen failures leads to a new architecture---called rangeformer---which can fix these issues. In essence, rangeformer is a series of interventions involving changes from positional encodings to weight tying (aka looping) to data level interventions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper's experiments are well written and the sequential structure made it fun to follow."}, "weaknesses": {"value": "- My core apprehension is that I do not currently see sufficient novelty or utility to the paper. The paper is not necessarily a science of deep / machine learning project that tries to elicit new phenomena or explain known ones (i.e., does not ask question of the sorts \"why does a Transformer fail to perform a task\"), nor is it an explicit methodological improvement (i.e., it does not try to introduce a new architecture for practical tasks and verify the efficacy of proposal). This, personally, makes it hard for me to contextualize this paper with respect to existing literature.\n\n- Related to above, if I focus on the precise narrative the authors took for motivating their study, i.e., generally intelligent systems should be able to perform linear algebra, then I find myself not convinced at all by this narrative. For several problems that Transformers were benchmarked on in this paper, I argue I would personally fail to solve several of these tasks with a sufficiently high performance. That said, I can easily write a program or do a library call to perform these deterministic calculations, and so can Transformers. In this sense, I don't really see the motivation to wanting to replace a deterministic calculator with a fuzzy one.\n\n  - To be clear, I'd generally deem a point like this to be minor and not flag it in the review. However, given that this statement was made multiple times by the authors and because I do not see a sufficient motivation for the paper, I'm leaning into authors' proposed motivation.\n\n- Prior work: Authors state (L46--48) that there is little prior work exploring how Transformers can perform linear algebra tasks. I strongly disagree with this. There's several papers exploring, e.g., the ability of a model to perform tasks like matrix completion [1], broader math operations [2], identifying the benefits of looping for this [3], effects of data properties [4], and so on. \n\n[1] https://arxiv.org/abs/2410.22244\n\n[2] https://arxiv.org/abs/2506.13688\n\n[3] https://arxiv.org/abs/2502.17416\n\n[4] https://arxiv.org/abs/2307.03381"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VYYQxIarBN", "forum": "GBkRMi3qjD", "replyto": "GBkRMi3qjD", "signatures": ["ICLR.cc/2026/Conference/Submission19637/Reviewer_LWQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19637/Reviewer_LWQ3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762404920258, "cdate": 1762404920258, "tmdate": 1762931488139, "mdate": 1762931488139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}