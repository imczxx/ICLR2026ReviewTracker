{"id": "nrinbZTy26", "number": 8626, "cdate": 1758092895092, "mdate": 1759897772687, "content": {"title": "Eliciting Secret Knowledge from Language Models", "abstract": "We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.", "tldr": "Study of eliciting secret knowledge from LLMs using black-box and white-box techniques.", "keywords": ["auditing", "AI safety", "mechanistic interpretability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b17ba27d9ac2468c8ec7cbdc52ce59a3ba403e29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work is largely an empirical study providing a new benchmark in the form models fine-tuned to hide secret knowledge.\n\n1. The study trained Large Language Models (LLMs) to possess secret knowledge (a keyword, an encoded instruction, or user gender) while denying awareness of it, creating a testbed for eliciting hidden information.\n2. Various black-box (e.g., prefill attacks, user persona sampling) and white-box techniques (e.g., logit lens, Sparse Autoencoders) were evaluated for their ability to help an auditor LLM uncover these secrets.\n3. Prefill attacks proved most effective overall, significantly improving the auditor's success rate in identifying the hidden knowledge across most settings, while white-box methods also demonstrated utility by extracting signals from the models' internal states.\n\nWhile the benchmarks and model organisms offered are valuable to the community, the work itself lacks sufficient novelty to warrant acceptance. Given the effort involved, I recommend submission to a different venue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. New fine-tuned models are valuable to the community.\n2. Well-constructed, albeit limited, empirical evaluation.\n3. The work establishes a public benchmark, including open-sourced models and code, to facilitate future research on secret elicitation methods."}, "weaknesses": {"value": "1.  No Theoretical Contribution: The study is empirical, establishing a benchmark and evaluating techniques without proposing new theories on LLM knowledge representation or concealment mechanisms.\n2.  Limited Novel Insights: Key findings primarily confirm the high efficacy of prefill attacks and the utility of specific white-box methods (logit lens, SAEs), while noting the ineffectiveness of fuzzing, high-temperature sampling, and generic jailbreaking in this context.\n3.  Complex Pipeline, Limited Ablation: The setup relies heavily on custom elements (synthetic data generation, specific fine-tuning, crafted prompts, multi-stage auditing) without systematic ablations to isolate the impact of each component, hindering generalizability."}, "questions": {"value": "1.  Given the empirical nature of the study, were there any unexpected results or specific observations about elicitation techniques that might suggest avenues for future theoretical investigation into LLM knowledge representation or concealment?\n2.  Beyond confirming the effectiveness of certain methods (like prefilling) and the ineffectiveness of others (like fuzzing), did the evaluations reveal any deeper or surprising insights into how these models internally represent or attempt to suppress the secret knowledge during elicitation attempts?\n3.  Considering the complex pipeline involving custom synthetic data, prompts, and multi-stage auditing, how confident can we be in the specific effectiveness rankings of the elicitation techniques themselves, absent ablation studies isolating the impact of each pipeline component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqZ71V8pDC", "forum": "nrinbZTy26", "replyto": "nrinbZTy26", "signatures": ["ICLR.cc/2026/Conference/Submission8626/Reviewer_UK1Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8626/Reviewer_UK1Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582676058, "cdate": 1761582676058, "tmdate": 1762920459568, "mdate": 1762920459568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and systematically investigates the problem of \"secret elicitation\" — extracting knowledge that a large language model (LLM) has been trained to apply implicitly but not to state explicitly. The authors create a controlled testbed by training three families of LLMs to hold specific secrets (e.g., a user's gender) that influence their generations in a downstream task while being explicitly denied in direct questioning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is written in an accessible and easy-to-understand manner.\n\n2. The figures in the paper are presented with exceptional clarity."}, "weaknesses": {"value": "I have several concerns regarding the experimental section of this paper:\n\n1. The baseline comparisons in the paper are not sufficiently clear. While an \"in-context baseline\" is mentioned, its specific implementation is not detailed. It would be beneficial to include other advanced methods discussed in the related work as baselines to provide a more comprehensive evaluation of the proposed approach's performance.\n\n2. The paper appears to lack ablation studies. Conducting such experiments is crucial to validate the contribution and necessity of each component within the proposed method."}, "questions": {"value": "The current experiments are conducted solely on 4B and 8B parameter LLMs. Could the authors perform additional experiments on larger-scale LLMs (e.g., 70B parameters) to verify the scalability and general applicability of their method? Furthermore, the selected LLM architectures are somewhat outdated; it is recommended to consider using more recent foundation models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T5lgJJTRVp", "forum": "nrinbZTy26", "replyto": "nrinbZTy26", "signatures": ["ICLR.cc/2026/Conference/Submission8626/Reviewer_EhiA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8626/Reviewer_EhiA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936451459, "cdate": 1761936451459, "tmdate": 1762920459103, "mdate": 1762920459103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors train three models that possess specific knowledge but are trained not to disclose it when asked directly. Three types of secret knowledge are evaluated: (i) **Taboo**, the model knowing a secret keyword, (ii) **Secret Side Constraint (SSC)**, the model learns to obey to a Base64-encoded instruction embedded in the prompt that must never be revealed, and (iii) **User Gender**, where model outputs align with a hidden user gender while refusing to disclose it. The authors then benchmark several white-box and black-box elicitation methods conducted with a smaller **auditor** LLM tasked with guessing the secret. This work present a testbed for evaluating knowledge elicitation approaches and the presented methods yield strong performance in their settings.\nThe paper releases models, prompts, and code to support reproducibility and future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Relevance, Clarity.**\nThe contributions are well motivated and clearly presented and substantiated. Knowledge elicitation of secret behaviours is an important question for the security of LLMs.\n\n**Well-defined testbed with known ground truth.**\nThe use of controlled models with *known* secrets allows evaluation of elicitation tools and removes ambiguity on the model's knowledge. The three settings (Taboo, SSC, User Gender) cover different mechanisms, explicit in-context triggers vs. out-of-context reasoning, and are described with good clarity in concrete examples (Fig. 2).  \n\n**Simulating practical weak-to-strong supervision.**\nThe auditor models are intentionally *smaller* than the audited models which demonstrates practical aspect of the methods.\n\n**Breadth across black-box and white-box tools.**\nThe paper thoroughly compares several knowledge elicitation methods: prefilling, persona sampling, fuzzing, logit-lens-adjacent probes, SAEs, etc. Their effectiveness is well supported, and the methods presented can inspire future research for secret knowledge elicitation.\n\n**Openness and reproducibility.**\nModels, prompts, and evaluation scripts are released (anonymous links), which meaningfully increases the value of the benchmark."}, "weaknesses": {"value": "**Some design choices are under-motivated or asymmetrically justified.**\n\n* **Why Base64 for SSC?** The choice is plausible (benign-looking trigger), but the paper should discuss alternatives (random tokens, other encodings). Comparing at least one alternative trigger would be highly appreciated.\n* **Model selection.** The rationale in Section 3.1 is partly functional (“SSC requires Base64 decoding, so we use Llama 3.3 70B”), but the **breadth** of model choices is limited (Gemma 2 9B for two settings). The justification for choosing Llama 3.3 70B because of its alleged ability to decode Base64 should be sourced, but also users report Gemma 2 9b to be able to decode Base64 as well (https://www.reddit.com/r/LocalLLaMA/comments/1ecz5is/gemma_9b_can_decode_base64_natively_the_next/) .\n\n**Prior knowledge given to auditor models undermine practicality.**\nThe auditor is told the **type** of secret (e.g., “there exists an encoded instruction” or “the secret is a gender”), which makes single-turn guessing feasible but weakens the **external validity** for real-world auditing where the space of secrets is unknown and potentially infinite. An auxiliary experiment with **broader or no priors** (or a retrieval step over a larger hypothesis set) would make conclusions more compelling.  \n\n**Temperature and sampling policy differences merit justification.**\nThe evaluation samples at **T=1** except **prefill at T=0**. This asymmetry can favor prefilling, and the appendix shows temperature can influence elicitation success. Please justify, and add a controlled comparison (same T across methods). \n\n**Some evaluation choices raise concerns.**\n\n* **Relying on judge-based metrics** makes the results less convincing in certain cases (Appendix C). Likelihood-based complements (e.g., token-level or constrained decoding likelihoods) would be a useful cross-check.\n* **Potential overestimation of in-context SSC,** for which, the judge sees the target concept, potentially inflating similarity scores.\n\n**Minor clarity/typos and figure conventions.**\n\n* Typo in Section 5.1, line 351: “response,s” → “responses”. \n* Fig. 6 color legend: “I/O + Prefill + Fuzzing” is styled like a black-box method though it’s **not** purely black-box. Please fix this legend convention.\n* **“Base” vs. “Instruction-tuned.”** In Section 3.3 and figures, the initial model is chat- / instruction-tuned, not a raw base model. Please **rename** to “Instruction-tuned” to avoid confusion."}, "questions": {"value": "1. Why Base64 specifically for SSC? Could your findings change with a different trigger (e.g., different encoding or simply random token)? Could you please add such an ablation.\n\n2. Why use **T=0** for prefill but **T=1** elsewhere? Please provide results at matched temperatures for the main table to show conclusions not relying on this choice.\n\n3. Could you report a small **cross-model** check (e.g., another 7–9B open chat model per setting) to test whether prefill dominance and SSC white-box superiority **replicate**?\n\n4. In Appendix C, why prioritize judge-based scoring? (Please also clarify the GPT-4 vs. GPT-4.1 judge choice)\n\n5. Can we expect model trainers will defend against your knowledge elicitation methods by training their models on them?\n\n6. For the **User Gender** scenario, could you comment on how your benchmark might extend to **age**/**occupation**/**arbitrary preference** settings? A brief discussion would broaden relevance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SbjftFYIzt", "forum": "nrinbZTy26", "replyto": "nrinbZTy26", "signatures": ["ICLR.cc/2026/Conference/Submission8626/Reviewer_hhmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8626/Reviewer_hhmc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963712587, "cdate": 1761963712587, "tmdate": 1762920458358, "mdate": 1762920458358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study a secret elicitation scenario where an adversary aims to elicit information that another model is finetuned to hide via verbalisation. The authors audit both blackbox and whitebox setups and release model organisms and a public benchmark for future works in the area."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well written, easy to understand and motivates the problem well. An important problem within AI Safety is addressed here\n\nS2. The three model organisms cover different secret elicitation scenarios and I appreciate the diversity of settings tested with respect to how the LLMs utilise the secret information. This is a well thought out benchmark and can benefit the community.\n\nS3. Both blackbox and whitebox methods are tested for the secret elicitation experiments. The methods covered here are diverse and cover a broad spectrum of approaches to elicit knowledge from LLMs via different informational pathways. The realistic auditing setup is useful and realistic."}, "weaknesses": {"value": "W1. How are the secret words chosen? Authors should ideally conduct a study evaluating how the secret word impacts the success of the audit and which words are better to use in the audit\n\nW2. The authors should benchmark more advanced methods like steering model activations (both in the auditor and the secret keeper model) to fully benchmark current methods.\n\nW3. The adversarial training appears to teach \"refusal\" (e.g., \"I don't have any way to determine that\" ) rather than \"deception\" (e.g., lying and stating the wrong gender). The high success of prefill attacks  might be an artifact of this \"shallow\" hiding, which is more easily broken by \"jailbreaking\" techniques.\n\nW4. The white box evaluations for SSC and User Gender models have a multi stage auditing pipeline. Is the comparison with blackbox methods really fair? since the lower performance of white box methods could be due to a weaker signal from the method itself or the auditor's inability to parse the noisy data.\n\nW5. The authors only perform SFT to study the model organisms, but a more realistic setup could also include RL post training."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FgORSukPty", "forum": "nrinbZTy26", "replyto": "nrinbZTy26", "signatures": ["ICLR.cc/2026/Conference/Submission8626/Reviewer_JtGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8626/Reviewer_JtGD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144901186, "cdate": 1762144901186, "tmdate": 1762920457719, "mdate": 1762920457719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}