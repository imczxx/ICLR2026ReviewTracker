{"id": "bIS0BbYjjd", "number": 7897, "cdate": 1758041850387, "mdate": 1759897823711, "content": {"title": "Is In-Context Learning Learning?", "abstract": "In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. \nThis has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's _ad-hoc_ encoding is not a robust mechanism, and suggests limited all-purpose generalisability.", "tldr": "A large-scale evaluation to empirically characterise in-context learning as a learning paradigm, ablating out common drawbacks of LLM evaluation, and finding results contradicting or aligning with conventional wisdom", "keywords": ["LLMs", "in-context learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b307591faa3ccc6f260debe8119f520032cf11b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that ICL constitutes a form of learning. It first defines ICL in the PAC-style definition of learning and then conducts an extensive empirical study to support the claim. The work contributes towards a foundational understanding of ICL, characterizing ICL as a learning paradigm rather than just prompt-based problem solving. It shows interesting results that performance gaps between different LLMs and prompts reduce as the number of exemplars grow, suggesting that ICL emerges due to autoregression paradigm. Experiments further show that ICL relies on  ad-hoc pattern extraction from exemplars, which causes brittleness for shifted distributions and inconsistency across closely-related tasks. Another interesting finding show that lexical features has limited impact on the ICL performance given enough exemplars."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work presents a clean and well-controlled evaluation of ICL, first setting up in the PAC-style learning definition and then empirically testing the theory w.r.t. different prompting strategies and task designs. \n2. The use of established tasks and prompt designs makes it easy to comprehend the setup. Further, the choice of formal-language based tasks provide a more principled probe to find if ICL learns underlying rules or statistical pattern matching.\n3. The experimental ablations are thorough and cover a good range of controlled conditions including prompting strategies, exemplar order and distribution shifts. \n4. It is an interesting insight that the exemplar ordering has limited effect, especially as the number of exemplars grow. This challenges the prior claims of success and brittleness in a few-shot setting. \n5. The inclusion of traditional baselines like kNN is important, which it often overlooked by many ICL studies.\n6. The paper is well written and easy to follow. The study contains all details in order to reproduce the results including prompt designs, call parameters and code."}, "weaknesses": {"value": "The study claims that ICL emerges from a combination of prompt input and model’s intrinsic knowledge encoded in the learned weights. However, the experimental setup uses synthetic symbols with no semantic grounding. This restricts the model from leveraging its pretrained knowledge, which is a key component of how ICL functions in real-world natural language LLMs. Since the tasks in this study rely on artificial alphabets and rule-based structures, the findings may underestimate the ability of LLMs in semantically meaningful contexts, where stored knowledge plays a role during inference. Please discuss the implications of this design choice for generalizing the results to language-based ICL scenarios.\n\nMore questions and weaknesses are included in the Questions sections."}, "questions": {"value": "1. In the introduction, the authors state that the study performs empirical studies on sensitivity to pretraining and memorization.  However, no experiments explicitly evaluate these factors. Memorization is partially controlled for through label noise, but its effects are not examined, and no experiments investigate the impact of pretraining sensitivity. Can authors please discuss these implications on ICL performance. \n\n2. Stack and Reversal tasks require saving state representation. In the studied inference-only setup, autoregressive transformers have no explicit memory. Poor performance on PDA tasks may therefore be due to architectural constraints rather than inherent limitations of ICL. It would strengthen the paper by including an analysis on how much of the difficulty is due to transformer architecture versus the properties of ICL.\n\n3. The study enforces strict parsing of model outputs, which I acknowledge is needed for consistent measurement. However, formatting failures are penalized the same as incorrect predictions, potentially mixing output style with task understanding. It would add transparency to the work, if the authors could clarify how much of the observed error is attributed to parsing issues rather than reasoning failures.\n\n4. What does 1.89M datapoints/predictions comprise? Provide more details about these datapoints. Specifically, does it count only the test predictions, or are the exemplars included in each prompt also considered datapoints? A breakdown of the total by task, number of shots, and OOD settings would help readers better understand the evaluation schema."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h3n4eWkR1P", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Reviewer_ACYp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Reviewer_ACYp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759183389, "cdate": 1761759183389, "tmdate": 1762919928422, "mdate": 1762919928422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a formal view of in-context learning (ICL) under a probably approximately correct(PAC)-style framework, where the prediction rule is treated as a learner conditioned on exemplars and prompt. The authors define an explicit ICL objective (Eq. 4) and argue that although ICL can be interpreted as a form of learning under this framework, its empirical behavior is largely driven by pattern matching associated with autoregressive inference rather than robust rule acquisition.\n\nThe empirical study evaluates four LLMs (two open source; two proprietary) on nine synthetic formal-language-style tasks, including FSA-type and PDA-type languages and simple arithmetic. Several prompting strategies are tested: few-shot with and without task descriptions, chain-of-thought, APO, direct token encoding, and randomized “word-salad’’ prompts.\n\nMain empirical observations:\n- Accuracy improves with additional exemplars across different model families during ICL, which shows model learn to fit exemplars; performance across models and prompt types becomes more similar at higher shot counts. Peak performance often appears around 50-100 shots, instead of at typical few-shot settings.\n- Performance is stable when exemplar distributions and ordering vary within a task, but accuracy degrades under test-time distribution shifts. Chain-of-thought(CoT) and APO prompts show the strongest degradation under these shifts.\n- Cross-task generalization is inconsistent. Tasks with similar underlying structure can differ by up to approximately 31 percentage points in peak accuracy. On roughly half of the tasks, simple supervised baselines (DT, kNN, MLP) achieve performance comparable to or better than ICL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Provides a clear PAC-style formulation of ICL, giving a precise mathematical objective (Eq. 4).\n- Evaluates multiple model families (open-source and proprietary), multiple style tasks and multiple prompting strategies, reducing model-specific/task-specific bias and show conclusion can be applied generally.\n- Quantifies how performance changes with the number of exemplars, showing that gains often appear at higher shot counts (50-100) and use this to decouple the influence of prompt and examplars.\n- Distribution shift analysis to show the limitation of ICL"}, "weaknesses": {"value": "- The evaluated model set does not include recent models(all models were released last year). This limits the scope of the empirical claims, especially for open-source models, and it remains unclear whether the observed patterns (e.g., OOD brittleness) would hold for newer models especially there are many \"reasoning\" models. Although the authors mentioned \"running synchronously a single task for an LLM could take over four months\" but all model providers or routers support async call, which makes the claim of \"synchronously\" not a blocker.\n- Prior work has already discussed the role of data distribution in driving ICL behavior (e.g., [1]). The paper revisits related themes, although in a different synthetic-task setting.\n- The empirical coverage is restricted to single-call autoregressive prompting. The formal discussion gestures toward broader claims about ICL, but the results may not extend to multi-step or multi-call inference settings.\n- Parts of the presentation could be improved: legends in Figures 2-3 are difficult to read and Figure 2 should be before Figure 3. Also there are some typos like \"noting noting\" in Section 3.\n- The task suite focuses on formal-language classification and simple arithmetic. While this yields controlled evaluation, it is unclear how the reported effects transfer to more realistic natural-language tasks with richer structure.\n- Path/graph-style tasks are evaluated without strong algorithmic baselines. This makes it harder to determine whether the model is failing due to true reasoning difficulty or due to prompt/format limitations.\n\n[1] Chan et al., Data distributional properties drive emergent in-context learning in transformers, 2022"}, "questions": {"value": "see weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SFyGtUUu0k", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Reviewer_CLXz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Reviewer_CLXz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959088832, "cdate": 1761959088832, "tmdate": 1762919927963, "mdate": 1762919927963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper characterizes learning as generalization to OOD examples. It evaluates the ability of ICL to perform such generalization on a range of tasks. It finds that generalization performance decreases with test examples that are increasingly OOD. It also evaluates this gap under specific conditions, e.g. with and without chain-of-thought reasoning or automated prompt optimization, or replacing natural language with nonsense words."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper poses an interesting question (\"is in-context learning learning\"), and makes an admirable attempt at trying to resolve a philosophical/theoretical question as an empirical exercise. It poses a particular formalism for the question. It does a good job at evaluating on a wide range of tasks and models."}, "weaknesses": {"value": "I think that the main weaknesses of the paper are wrt to the framing:\n* the paper assumes a specific interpretation of learning. But there are many others. E.g. a more statistical view that evaluates learning as performance on a single distribution (from which we sample both training data and test data). Or e.g. an optimization view  that sees it as error minimization or loss minimization. Some interpretations of learning (and criticisms of the term \"in-context learning\" even require a permanent modification to the model, e.g. via weights). It should be acknowledged that this is just one particular interpretation of what learning is.\n* it is difficult to say what exactly is OOD. (a) it would be great if the paper could more explicitly describe the concrete choices for selecting the OOD distributions Q (if this is in the appendix, there should be a pointer. apologies if I missed it!). (b) I think the decision to use delta as a measure of distance actually really well supports a framing that OOD-ness is on a spectrum. This could be emphasized more. And I would recommend it not be definitely stated that \"ICL performs learning\"\nwhy is the analysis limited to binary classification? this should be raised as a limitation, or otherwise it should be explained why this is more general\n\nThe clarity of the exposition can also be improved. For example:\n* I think it will be especially helpful to clearly distinguish for the reader two things: generalization from pretraining data, vs generalization from the data in context (which is what's studied here). I didn't really understand what the paper was evaluating until section 4. It would be helpful if you said this earlier in the paper, and made that quite clear! E.g. something like \"We evaluate the generalization of ICL by evaluating k-shot learning from context, where the query (which follows the k-shot examples) is either from the same distribution or a different distribution.\" (please refine/correct as needed).\n* I'm confused about this statement: \"That is, it modifies its states at run-time to–ideally–generalise to new observations for any task and any Q\". Why should it also generalize to any task (the Valiant 1984 definition only seems to concern Q)?\n* I believe that `delta` is overloaded\n* section 3.1 \"features are labelled with an unknown function c\" -> perhaps \"examples are labelled…\" would be clearer. I initially misunderstood the setup as each x being just a feature of a larger example\n* in the contributions section, it's unclear what \"accuracy gap\" you are referring to\n\n\nSuggestions for missing citations:\n* citations: many-shot ICL. this paper looked at many of the same questions: increasing k, and also order sensitivity\n* the paper states that it takes \"LLMs as recognisers of unknown expressive power\" – but there's been a lot of formal work on this question. e.g. \"Constant Bit-size Transformers Are Turing Complete\", \"\"Chain of Thought Empowers Transformers to Solve Inherently Serial Problems\" and many more"}, "questions": {"value": "see above for questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q2n4U6KOBB", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Reviewer_pfQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Reviewer_pfQS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965402370, "cdate": 1761965402370, "tmdate": 1762919927489, "mdate": 1762919927489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We would like to thank all the reviewers for their comments. They are constructive and we appreciate how evident it is that all reviewers took the time to read the work. \nAs we work on improvements suggested by all (in particular, clarifying the definitions of learning and readability), we might comment directly to request clarifications in some points."}}, "id": "WO1mrMohH2", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763398975607, "cdate": 1763398975607, "tmdate": 1763398975607, "mdate": 1763398975607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seeks to answer whether in-context learning (ICL) in large language models (LLMs) constitutes genuine learning rather than memorization or pattern recognition.\n\nThe authors start by reframing PAC-learning to define ICL formally as a process that can, in principle, satisfy generalization criteria. They then perform a large-scale empirical study (1.89 million model predictions) across four LLMs (GPT-4 Turbo, GPT-4o, Mixtral-8×7B, Phi-3.5 MoE), nine formal-language tasks, and multiple prompting strategies (description, CoT, APO, direct encoding, word-salad, etc.).\n\nThey report that:\n\n1. Increasing the number of exemplars (shots) consistently improves accuracy, reducing model and prompt variance.\n\n2. ICL is robust to training-distribution perturbations but brittle under test-distribution shifts, especially for CoT and APO prompts.\n\n3. Closely related tasks show inconsistent performance gaps (up to 31%).\n\nThe authors conclude that ICL mathematically qualifies as learning but is limited by the autoregressive paradigm’s ad-hoc encoding and its sensitivity to distributional properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The study covers a large number of models, prompts, and ablations—possibly one of the most comprehensive empirical ICL analyses on synthetic tasks.\n\n2. The paper tackles an important and widely discussed conceptual question—whether ICL counts as “learning”—and attempts to formalize this using PAC-style reasoning.\n\n3. The inclusion of open-sourced data, prompts, and methodology is commendable and aligns with ICLR’s reproducibility standards."}, "weaknesses": {"value": "1. The paper repeatedly asserts strong conclusions (“ICL is learning in the limit”) without solid theoretical or causal evidence linking empirical findings to the formal definition. Long sentences and vague phrasing (“it leverages statistical features from the prompt”) obscure the actual mechanism being tested. \n\n2. The results section is dense and under-interpreted. While the experiments are numerous, the findings are mostly confirmatory (more shots → better accuracy; CoT → less robust OOD). \n\n3. Tables 1–2 and Figures 2–4 are difficult to interpret, filled with numeric slopes and variances but lacking qualitative takeaways or error analysis. There is no principled explanation of why related tasks diverge in performance or how exemplar count interacts with task complexity.\n\n4. The related-work section is unstructured and uncomprehensive. For example, it largely omits the extensive empirical literature on understanding how ICL operates (e.g., feature induction, model circuit analyses)."}, "questions": {"value": "1. Can you clarify what empirical behavior would falsify your claim that ICL “is learning”?\n\n2. Could you provide simpler visualizations (accuracy vs. shots, grouped by task type) and eliminate the overly detailed slope tables?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hVzr6RWlPp", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Reviewer_8VB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Reviewer_8VB7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161653440, "cdate": 1762161653440, "tmdate": 1762919927131, "mdate": 1762919927131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether in-context learning in autoregressive models constitutes \"learning\" in the formal sense, using a variation of the PAC framework to define robustness in learning to ground discussion. They run an empirical study with 4 LLMs on nine formal, synthetic tasks to test this. The paper concludes that mathematically, ICL is a form of learning but limited in being able to learn the observed distribution in the prompt, rather than being able to generalize."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles a fundamental question and provides an evidence-based answer based on formal definitions\n- The result that best-of-average accuracies are achieved with 50-100 examples in the limit challenges the current few-shot understanding\n- The study's scale and rigor is a strength"}, "weaknesses": {"value": "- The scope of the tasks studied in the paper is rather narrow. I understand the choice to use synthetic, formal tasks to enable a rigorous analysis and minimize reliance on pretrained natural language knowledge, it would be helpful to further sign post the scope of the findings throughout the paper (though the authors do acknowledge this), and add a discussion on which/how the findings may generalize to natural language tasks\n- I'd also love to see further motivation on why the paper adopted its definition of \"learning\", explore alternative definitions, and what that entails for the findings of the paper"}, "questions": {"value": "- In equation 4, is it necessary to rename the datapoints to e_i from x_i? \n- I'm personally curious why there are such large performance gaps between the seemingly closely related tasks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sb1C6OtZL3", "forum": "bIS0BbYjjd", "replyto": "bIS0BbYjjd", "signatures": ["ICLR.cc/2026/Conference/Submission7897/Reviewer_X2GS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7897/Reviewer_X2GS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762307793618, "cdate": 1762307793618, "tmdate": 1762919926871, "mdate": 1762919926871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}