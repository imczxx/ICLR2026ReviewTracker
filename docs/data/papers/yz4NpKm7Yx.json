{"id": "yz4NpKm7Yx", "number": 24845, "cdate": 1758361026841, "mdate": 1759896745728, "content": {"title": "Attention Smoothing: Correcting Causal Bias in Autoregressive Language Models", "abstract": "Autoregressive large language models (LLMs) suffer from causal bias: once attention states are cached under the causal mask, they cannot be revised, leading to information solidification and path-dependent errors. This structural limitation undermines contextual fidelity and amplifies hallucinations. We introduce Attention Smoothing, a decoding-time framework that revises attention after the entire context is observed. Our method models token-to-token information flow as an absorbing Markov chain, computes token-level surprisal scores, and derives a smoothed posterior attention distribution that corrects the causal bias. The framework is model-agnostic, training-free, and can be seamlessly integrated into existing inference pipelines. Experiments on multiple hallucination and factuality benchmarks show that Attention Smoothing consistently improves contextual faithfulness across model scales, highlighting the importance of managing information flow for more reliable LLM generation.", "tldr": "", "keywords": ["Large Language Models", "Hallucination Mitigation", "Attention Smoothing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6da0e96f5e7389a315ff429ce9759b35fbefe6f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "RAS models a sequence’s evolving “semantics” as an absorbing Markov chain (AMC) to quantify “semantic surprise” in prefixes, then performs a training-free two-pass adjustment that (i) computes an information score from the AMC and (ii) in a second pass removes the causal mask on selected layers/heads and blends this unmasked attention with the original masked attention to “retrofit” earlier queries with future context. The method reports consistent gains on TruthfulQA (MC), FACTOR (Wiki/News), and HaluEval using LLaMA-2/3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*  Clear mechanism without retraining: uses absorbing Markov chain signals and a two-pass attention smoothing; the backbone stays frozen with only a few small hyperparameters like alpha, beta, and layer choice.\n\n*  Principled though heuristic: defines cover rate and log surprise, uses a fundamental matrix from the transition matrix, and builds an information score to reweight attention.\n\n*  Empirical gains across tasks and models: improves over DOLA and Activation Decoding on multiple datasets, with component and hyperparameter ablations."}, "weaknesses": {"value": "* Semantic meaning in LLMs depends on the entire prefix, i.e., \\(P(x_{t+1}\\mid x_{1:t})\\), so modeling “semantic pathways” as a first-order absorbing Markov chain over tokens is an approximation. The single-step transition \\(\\tilde{P}_z(z_i, z_{i+1})\\) by itself does not encode full history, but the method aggregates multi-step effects via path products and the fundamental matrix \\(N=(I-Q_z)^{-1}\\); thus it can capture some nonlocal influence, even if higher-order, history-dependent interactions may still be missed.\n\n* The paper provides no evidence that semantic trajectories satisfy the Markov property\n\n* Lacks comparison to recent well-known baselines like Chen, Chao, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. \"INSIDE: LLMs' internal states retain the power of hallucination detection.\" arXiv preprint arXiv:2402.03744 . I request the authors to find the contemporary research works and compare their method to the recent works."}, "questions": {"value": "Same as above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3mbzzpNwFy", "forum": "yz4NpKm7Yx", "replyto": "yz4NpKm7Yx", "signatures": ["ICLR.cc/2026/Conference/Submission24845/Reviewer_Zqof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24845/Reviewer_Zqof"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761614642, "cdate": 1761761614642, "tmdate": 1762943218670, "mdate": 1762943218670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper models a sequence’s evolving “semantics” as an absorbing Markov chain (AMC) and computes an information/surprise score from the chain’s fundamental matrix to detect where prefix-based interpretations diverge from full-context meaning. It then runs a training-free, two-pass adjustment that (i) computes AMC signals from the full input and (ii) in a second pass reweights attention (removing the causal mask on selected layers) and fuses it back to reduce prefix dominance. Experiments on TruthfulQA (MC), FACTOR (Wiki/News), and HaluEval show consistent but modest gains over DOLA and Activation Decoding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Casting semantic evolution as an absorbing Markov chain yields a principled handle (fundamental matrix, absorption flow) to quantify prefix vs. full-context divergence. \n\nTraining-free, plug-in smoothing. The two-pass, query-only attention adjustment is easy to bolt onto frozen LLMs. \n\nConsistent multi-task gains. Improvements over DOLA/AD across TruthfulQA (MC), FACTOR, and HaluEval; ablations indicate both AMC signals and reweighting matter."}, "weaknesses": {"value": "* The paper models semantic pathways as an absorbing Markov chain with tokens as states and uses causal masking to make Q upper triangular. However, this violates the Markov property because in LLMs, the transition from token i to token j depends on all previous tokens (x₁...xᵢ), not just state xᵢ. The paper conflates token positions with semantic states without justification.\n\n* The paper defines r(z) = E[T/τ] where τ is cover time, then claims r(z) = ∏P̃z(zᵢ,zᵢ₊₁). This is mathematically incoherent: the left side is an expected ratio, while the right side is a product of transition probabilities with no expectation operator. The equation mixes discrete path probability with expected value without proof.\n\n* Please compare the method to recent studies like INSIDE, Loopback Lens for robustness of the method."}, "questions": {"value": "Please refer the weakness for questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zvciwKqnLB", "forum": "yz4NpKm7Yx", "replyto": "yz4NpKm7Yx", "signatures": ["ICLR.cc/2026/Conference/Submission24845/Reviewer_KeaF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24845/Reviewer_KeaF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795284408, "cdate": 1761795284408, "tmdate": 1762943217897, "mdate": 1762943217897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes a formal framework, based on absorbing Markov chains, to quantify the semantic bias introduced by the causal (autoregressive) design of LLMs.\n2. The authors also introduce a complementary mitigation framework to address hallucinations arising from these biases and report strong empirical results supporting its effectiveness.\n3. Unlike methods that operate only on the outputs of LLMs, this research intervenes inside the model, proposing modifications to the attention mechanism at each layer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong empirical results, with clear experimental support demonstrating consistent improvements.\n2. A new approach that operates at the attention level in each layer. This is likely more powerful than methods that only modify the output, since the adjustment is propagated through the network and effectively allocates more FLOPs to the change.\n3. Notably, the method works without any training (zero-training / parameter-free), which is both practical and impressive."}, "weaknesses": {"value": "1. While the Markov chain framework is compelling, the current method appears to rely on a heuristic. How far does this heuristic drift from the original theoretical formulation? It would help if the authors clarified the mapping from theory to implementation, including what assumptions are introduced, which components are approximated, and what, if anything, is sacrificed from the original framework in terms of guarantees, scope, or fidelity.\n\n2. The idea of modifying attention is strong, but it would be useful to characterise the runtime tradeoffs. Specifically, how much slower is the proposed method relative to approaches that operate only on the output of the LLM? Please provide wall-clock timings or throughput comparisons across model sizes to quantify the latency and efficiency impact of intervening at every layer."}, "questions": {"value": "See weaknesses + a few more minor questions:     \n1.  Does the modified attention still normalise to 1? If not, is this a potential problem?    \n2. Why is the second adjustment on the output needed (equation 11)? Any insights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X1199ukEEa", "forum": "yz4NpKm7Yx", "replyto": "yz4NpKm7Yx", "signatures": ["ICLR.cc/2026/Conference/Submission24845/Reviewer_RXCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24845/Reviewer_RXCJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853454858, "cdate": 1761853454858, "tmdate": 1762943217563, "mdate": 1762943217563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for post-hoc correction to attention scores in causal language models. The method reframes the attention score matrix as an absorbing Markov chain (AMC), and computes various quantities from the theory on AMCs, which are used in their method. The final design of their corrector is a heuristic approach to adjusting the scores which makes use of the Information Score, and Utilization of a node. The adjustment itself does not come from the theory, but is of the authors own design. The method introduces a variety of hyperparameters. It is shown to improve language model performance detecting hallucinated content."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of semantically aligning the token representations to agree with the complete input is interesting, and intuitively makes sense.\n* Drawing the connection from the attention score matrix to AMC theory is also interesting.\n* Some of the diagrams are helpful aids to the writing."}, "weaknesses": {"value": "* Very limited experimental results. Hallucination detection is their only comparison against other methods, and it uses all similar scale instruct models: 7B, 8B, 13B. They provide some analysis of their method on a different dataset HotPotQA but do not provide an benchmarking on that dataset. This method needs much more thorough empirical validation than what is presently offered in the paper.\n* Detection results in Table 1 show the proposed method only offers very slight improvements most of the time.\n* The method introduces a number of hyperparameters but discussion of setting and/or tuning these hyperparameters is only mentioned briefly in the appendix, however, this is an important detail for those wishing to use your method.\n* The paragraph starting on line 421 claims that they demonstrate their method helps regardless of\"dataset type, reasoning style, or model capacity\" but they only test the method with a very narrow range of model sizes (7-13B) on one task (hallucination detection). The results presented in the paper do not justify such strong claims.\n* The writing would benefit from clearer language -- there is a lot of jargon (e.g. line 079 \"semantic pathways in semantic space\", line 022 \"guide a smoother\", line 246 \"intermediate semantics along the prefix pathway\"). It would help the reader to use more concrete language instead of vague terminology.\n* As the heuristic adjustment, introduced in section 4.2, does not have rigorous theoretical backing it would be helpful to provide the intuition behind its design.\n* The case study visualization in section 5.4 is difficult to parse. The section would benefit from more thorough discussion of what the reader should take away from this figure.\n* There is a typo in Figure 1 \"Liver replies on sunlight\" should be \"Liver relies on sunlight\".\n* The matrix $N_{dg}$ is not defined in the paper.\n* The matrix $V$ is not defined in the paper.\n* The text in Figure 4 is much too small. The text in Figure 5 is better but still on the small side."}, "questions": {"value": "* In Table 1, baselines Dola and AD both underperform the base model. Dola consistently underperforms by significant margin, and AD occasionally underperforms but again by a noticeable margin. Why is this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9Qndydd8AT", "forum": "yz4NpKm7Yx", "replyto": "yz4NpKm7Yx", "signatures": ["ICLR.cc/2026/Conference/Submission24845/Reviewer_nByb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24845/Reviewer_nByb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949259689, "cdate": 1761949259689, "tmdate": 1762943217327, "mdate": 1762943217327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}