{"id": "0IFqBfX7Ak", "number": 15897, "cdate": 1758256779840, "mdate": 1759897274656, "content": {"title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient", "abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms that support these behaviors remain opaque, raising concerns regarding truthfulness, safety, and controllability in practical applications.\nExisting interpretability approaches either rely on human-annotated contrastive pairs to derive control vectors, which limits reliability and generalization, or identify neurons correlated with superficial textual concepts, failing to capture the complexity of reasoning processes. \nConsequently, current methods struggle to precisely localize complex reasoning mechanisms or capture causal effects from model internal workings to the reasoning outputs.\nIn this paper, we build on causality-aware and outcome-oriented principles that focus on identifying components that have causal contributions to reasoning behavior where outcomes are cumulated by long-range effects.\nWe propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model inner workings like neurons, by propagating compound outcome-based signals (e.g., post reasoning accuracy) backward through model inference trajectories.\nIPG is efficient requiring only a few calls to the standard gradient operator, which uncovers causal structures governing complex reasoning and avoids large manual supervision.\nEmpirical evaluations demonstrate that our approach achieves more precise mechanistic interpretability and enables reliable modulation of reasoning behaviors across diverse reasoning models.", "tldr": "A method to causally control and interpret LLM reasoning behaviors by identifying and intevening internal reasoning-critical components.", "keywords": ["Large Language Models", "Reasoning", "Mechanistic Interpretability", "Policy Gradient", "Sparse Autoencoder"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37cc955be40917d01dca561f285b20133300783f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Integrated Policy Gradient (IPG), a method intended to attribute and modulate reasoning components in large language models by applying a policy‐gradient–like formulation on hidden activations, followed by scaling of the identified components. The aim is to locate “reasoning circuits” and then control them via a scalar $\\gamma$. Experiments on math‐reasoning datasets (GSM8K, MATH500, AIME2024, GPQA‐Diamond) show modest gains.\n\nWhile the aim is timely, the paper falls short of clearly distinguishing itself from recent work in RL‐based reasoning and lacks methodological and empirical depth to justify its claims of mechanistic interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses an important goal — locating reasoning features inside LLMs and offering intervention methods rather than purely black‐box fine-tuning.\n2. Builds on long‐horizon rewards rather than purely prediction loss, which is underexplored in hidden‐state attribution.\n3. The empirical gains shown (some improvement when scaling selected neurons) at least suggest there is signal in the method."}, "weaknesses": {"value": "1. IPG’s experiments manipulate hidden states (Eq.~(4) scaling), which is more akin to an optimization/steering method rather than a genuine causal attribution. Results resemble those from EM‐PG (arXiv:2504.18587), which focus on optimizing reasoning behaviours rather than tracing hidden “circuits”.\n\n2.  Eq.~(3)’s selection statistic \\(S_i\\) aggregates across samples without variance control or normalization. Considering the off‐policy nature of PG in RPG, how is sample bias handled? The more rigorous treatment in RPG (stop‐gradient, importance weights) suggests IPG is under‐specified.\n\n3. IPG’s transfer improvements are minor (< +1.2%), whereas RPG achieved up to +6 pp in comparable settings. Without clear generalisation, the claimed “general reasoning circuit” is weak.\n\n4. No mediation analysis, no circuit dissection across heads/layers, no intervention experiments that isolate a sub‐module and show step‐wise effect. By contrast, interpretability‐driven works emphasise causality and modularity. IPG’s approach remains correlation‐based.\n\n5. Given EM‐PG and RPG already apply policy‐gradient frameworks to LLM reasoning, the novelty of applying PG to hidden states (rather than parameters) needs stronger justification."}, "questions": {"value": "1. In Eq.~(1), you write \\(\\frac{\\partial J(h)}{\\partial h} = \\mathbb{E}_{\\tau\\sim\\pi_\\theta} \\left[ \\frac{\\partial \\log \\pi_\\theta(\\tau; h)}{\\partial h} (R(\\tau)-b) \\right]\\). But since \\(\\pi_\\theta\\) is parameterised by \\(\\theta\\), and \\(h\\) is a deterministic hidden activation rather than stochastic policy variable, can you provide a rigorous derivation from the log‐derivative trick?  \n\n2. Regarding Eq.~(2)’s path integral, how do you select the baseline \\(h'\\)? Have you tested sensitivity to different baselines (zero vector, mean hidden state, random noise)?  \n\n3.For the selection score \\(S_i\\) in Eq.~(3), why is simple averaging used? Why not adopt variance‐based or Shapley‐value–style attribution for hidden features?\n\n4. In Eq.~(4), if you set \\(\\gamma\\to\\infty\\), does performance keep improving? If so, then you are simply amplifying hidden signals globally, which undermines the claim of localised reasoning features.\n\n5. How does your method handle off‐policy bias and importance sampling? In RPG (arXiv:2505.17508) the weighting of importance samples is a central issue; what safeguards does IPG include?\n\n6. You compare to baseline LLMs and “vanilla” fine‐tuning, but did you evaluate against RPG or EM‐PG methods directly? If not, why?\n\n7. Please provide trial‐to‐trial variance (across seeds) and statistical significance of intervention effects, especially in the suppression experiments where accuracy drops to zero—how generalisable are these results?\n\n8. Can you show a case where your intervention corrects an incorrect reasoning chain (intermediate step), not just the final answer? This is crucial for interpretability claims.\n\n9. Since I am very interested in your work, but I have seen some other interesting methods on Google Scholar (not referring to similarity), I think if you could give me a concise comparative explanation, I think it would be very convincing. (arXiv:2505.17508)(arXiv:2504.18587)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LdLNBdLfcA", "forum": "0IFqBfX7Ak", "replyto": "0IFqBfX7Ak", "signatures": ["ICLR.cc/2026/Conference/Submission15897/Reviewer_2ZEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15897/Reviewer_2ZEq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557834460, "cdate": 1761557834460, "tmdate": 1762926111595, "mdate": 1762926111595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a framework for attributing behaviors to internal components and then steering those components to control behavior. The paper is pretty well written and the methods are clear. However, the main problem is that this paper is not properly situated in the existing literature and fails to compare many baseline methods for model control. It also makes some very wrong claims about the state of existing literature. \n\nThe framing that mechanistic interpretability lacks causality is simply wrong. In fact, you cite Stolfo et al. (2023), a causal mediation analysis paper, in the sentence that makes that claim!\n\nThere is loads of work in the frameworks of causal mediation and abstraction analysis of LLMs. For example, https://arxiv.org/abs/2004.12265 and https://arxiv.org/abs/2004.12265; see https://arxiv.org/abs/2301.04709 or https://arxiv.org/pdf/2408.01416 for surveys with citations. \n\nSteering literature. There really is a huge amount of steering literature and this is all about causality, i.e., causing the behavior to change. For example, https://arxiv.org/abs/2205.05124, or https://arxiv.org/pdf/2310.06824 or https://arxiv.org/abs/2306.0334 or https://arxiv.org/abs/2308.10248\n\nYou also should be aware of representation fine-tuning: https://arxiv.org/abs/2404.03592. This is should be included as a baseline in your paper. Also, axbench https://arxiv.org/abs/2501.17148 evaluates a bunch of methods for control, and you could evaluate your method on this benchmark to get standardized comparisons against a lot of different methods.\n\n\nWithout further experiments that demonstrate the performance of the method in context of existing literature, its hard to know whether it marks a novel improvement. \n\nI think the focus on long term dependencies is a good part of the paper, but you would need to compare against something like ReFT with a language modeling loss to show you are better at handling long term dependencies. Overall, this method might be an improvement, but we can't know with the given set of experiments."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See summary"}, "weaknesses": {"value": "See summary"}, "questions": {"value": "See summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6DCiAbxsC", "forum": "0IFqBfX7Ak", "replyto": "0IFqBfX7Ak", "signatures": ["ICLR.cc/2026/Conference/Submission15897/Reviewer_JNjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15897/Reviewer_JNjR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953798509, "cdate": 1761953798509, "tmdate": 1762926111209, "mdate": 1762926111209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces integrated policy gradient (IPG), a gradient-based method that attributes intermediate activations to some final outcome. This is done as follows:\n1. Take the gradient $\\nabla_h E_\\pi[J(h)]$  where $J$ is the outcome (reward). Do so using the score function / policy gradient trick to get unbiased estimates.\n2. Estimate this individually for each component of some hidden state, and average over all vectors from the latent vector $h_i$ to some baseline $h_i’$ (path gradient to reduce noise)\n3. Pick the top p components by the average magnitude of their IPG over some dataset \n\nAfter identifying the top components, the behaviour of the LLM can be controlled via scaling each component by a scalar to be larger / smaller. The authors suggest it is better to do this in k-sparse autoencoder latent space, rather than over raw neurons. The authors find that IPG can steer vectors to drive LM behaviour in the form of improved output accuracy, and do so better than other baseline interpretability methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of the policy gradient / score function trick to estimate gradient-based attribution is very clever. This was thought provoking for me to think about how PG can be applied generally to interpretability. I think this is both original and potentially significant. \n\nThe writing is generally easy to follow. The motivation to move beyond correlation-based methods is justified, and the main claims seem supported."}, "weaknesses": {"value": "It may be due to my lack of expertise in interpretability, but there are some problem set-up and design choices I do not fully understand. I discuss them in the questions below.\n\nIt would also be nice to have not only math and reasoning performance, but steering for harmfulness, for example.\n\nFormatting issues: L246-247, figure is blocking main text (likely due to negative vspace)"}, "questions": {"value": "**Q1:** How is the advantage estimated in Eq 1? L293 seems to suggest it is the GRPO advantage, but L164/165 mentions “accuracy reward”? Appendix A.3 also mentions IPG variants and GRPO advantage. \n\nFor what it’s worth, the GRPO advantage is a biased estimate of the true policy gradient (particularly the part where it divides the advantage by its standard deviation). I mention this because it feels like while having low-variance, biased gradients may work well in the training case (where GRPO is applied), it feels very important to have un-biased gradients in the interpretability case so you can accurately estimate contribution. You can always reduce variance by sampling and averaging over larger batches. It would be nice to see a discussion / ablation on this.\n\n**Q2:** On controlling behaviour via scaling activations (Eq 4), does this assume some kind of linearity in the representation? I.e. increasing activation magnitude always increases some kind of output behaviour? Is the idea that in order to do good interpretability we must find components that obey this relationship?\n\n**Q3** Architecture assumptions. Section 4.5 mentions steering DeepSeek-R1-\nDistilled-Qwen-1.5B using neurons identified in Qwen2.5-Math-1.5B-Instruct. Generally speaking, this can only be done for models with identical architecture, vocabulary, and tokenizer, right?\n\n**Q4** It is not clear to me what argument Figure 4 is making. For instance in Fig 4b, what should we read from the Jaccard similarity numbers? IPG’s _lower_ Jaccard similarity is argued as “more meaningful”, which I am not sure I follow. Wouldn’t higher Act similarity across the math datasets indicate better generalization of discovered components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P1UhPxUr9m", "forum": "0IFqBfX7Ak", "replyto": "0IFqBfX7Ak", "signatures": ["ICLR.cc/2026/Conference/Submission15897/Reviewer_dx8d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15897/Reviewer_dx8d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15897/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995274200, "cdate": 1761995274200, "tmdate": 1762926110862, "mdate": 1762926110862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}