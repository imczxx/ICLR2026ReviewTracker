{"id": "FJhtHBphCt", "number": 24330, "cdate": 1758355522759, "mdate": 1759896771106, "content": {"title": "Online Decision-Focused Learning", "abstract": "Decision-focused learning (DFL) is an increasingly popular paradigm for training predictive models whose outputs are used in decision-making tasks. Instead of merely optimizing for predictive accuracy, DFL trains models to directly minimize the loss associated with downstream decisions. However, existing studies focus solely on scenarios where a fixed batch of data is available and the objective function does not change over time. We instead investigate DFL in dynamic environments where the objective function and data distribution evolve over time. This setting is challenging for online learning because the objective function  has zero or undefined gradients---which prevents the use of standard first-order optimization methods---and is generally non-convex. To address these difficulties, we (i) regularize the objective to make it differentiable and (ii) use perturbation techniques along with a near-optimal oracle to overcome non-convexity. Combining those techniques yields two original online algorithms tailored for DFL, for which we establish respectively static and dynamic regret bounds. These are the first provable guarantees for the online decision-focused problem. Finally, we showcase the effectiveness of our algorithms on a knapsack experiment, where they outperform two standard benchmarks.", "tldr": "", "keywords": ["decision-focused learning", "integrated estimaton optimization", "predict-then-optimize", "online learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/450fdd08fe12c043f59975471ba7cf70dbb021fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the predict-then-optimize framework in the online setting. To handle the non-stationarity of online data streams, the authors regularize the objective and present two new algorithms, based on FTPL and OGD, to handle the non-convexity of the overall objective function. The authors prove both static and dynamic regret bounds, complementing their theoretical results with strong empirical evidence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow\n- The problem is well-motivated and of general interest to the ML community\n- In my opinion, the contributions by the authors are substantial as they provide both strong theoretical and empirical results"}, "weaknesses": {"value": "My biggest gripe with this paper is its lack of clarity in the problem setup. Below, I summarize a few points of confusion for me. \n\n-  In line 136, the authors write $\\bar{g}_t(X_t) = E[Z_t|X_t]$ for some hidden state $Z_t \\n R^d$. What does this mean? Does $X_t$ index a hidden distribution $D_t$ and $\\bar{g}_t(X_t)$ is the expected value of this distribution? \n\n- In line 176, $F_t$ is defined in terms of an expectation. What is the expectation taken with respect to? Just the learner's randomness?\n\n- In Equation 4, why is there an expectation over the player's actions in dynamic regret but not static regret? \n\n- In Line 196, what is the probability taken with respect to? Line 164-165 claims that you place no assumption on $(X_t \\bar{g}_t(X_t)), so I'm not sure where the randomness is coming from...\n\n-  Likewise, why is there an expectation on the data in Theorem 1 if, according to Lines 164-165 you place no assumptions on the data-generating process?  \n\nOverall, I think the authors need to clarify how the adversary is selecting the data stream. This lack of clarity is the main reason for my rating. I am happy to increase my score, given that the authors address this."}, "questions": {"value": "See Weaknesses. Also, can the authors comment on practical settings where having access to an offline optimization oracle is reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EMs8gqSscn", "forum": "FJhtHBphCt", "replyto": "FJhtHBphCt", "signatures": ["ICLR.cc/2026/Conference/Submission24330/Reviewer_riB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24330/Reviewer_riB6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396850477, "cdate": 1761396850477, "tmdate": 1762943044994, "mdate": 1762943044994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies an online version of decision-focused learning, where the learner is evaluated not by the accuracy of the predictions but by the quality of the decisions made. Unlike in the batch case, in the online version, the problem may evolve over time in a fully non-stationary way. As the problem is non-differentiable and non-convex, the authors leverage a regularized version of the bi-level optimization problem and rely on near-optimal oracles to derive regret guarantees by instantiating follow-the-leader and online gradient descent algorithmic schemes. Finally a synthetic experiment illustrates the effectiveness of the proposed algorithms in minimizing the average cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper extends the decision-focused framework to online learning, whereas most of the existing literature on the topic is limited to the batch case, where iid samples and a fixed estimation/optimization problem is available.\n* The authors provide a complete analysis of the problem leveraging state-of-the-art technical algorithms and tools from online learning literature."}, "weaknesses": {"value": "* The technical novelty of the paper is limited. While the authors stress the challenges posed by the online decision-focused learning setting (non-differentiable and non-convex functions), once formulated as in eq.(3) the problem is amenable to any \"standard\" online learning treatment. Indeed most of the results in the paper are obtained by carefully instantiating known assumptions, algorithms, and theoretical results, such as assumptions H1/H2, FTL and OGD, regularization and approximate oracle for non-convex objectives. As such, the results in the paper should be mostly assessed based on the interest of the setting.\n* While I'm not very familiar with the decision-focused learning paradigm, I can see a strong resemblance with other decision-making settings in machine learning, where accuracy in prediction problems do not directly translate into performance wrt to a target objective function. These includes reinforcement learning (e.g., accurately estimating an MDP does not translate into computing an optimal policy for a given reward) or bandit (e.g., accurately estimating the mean of each arm does not translate into accurately returning a high-reward arm). While the decision-focused learning paradigm may have emerged in a different literature, it would be helpful to have a more extensive justification of how the proposed online version differs from adversarial bandit/RL settings and provide more practical examples supporting its relevance."}, "questions": {"value": "* Please refer to weaknesses.\n* Please clarify if any technical contribution in the paper is novel or it required specific treatment due to the nature of the problem.\n* In Fig.1 the average cost of PFL, SPO, and FTPL see to converge towards the same value. Is this the case? If the overall process is stationary and ergodic, maybe we should indeed expect all methods to coverage to the same performance. If not, what is the explanation behind the difference in performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oe4WKS3LcK", "forum": "FJhtHBphCt", "replyto": "FJhtHBphCt", "signatures": ["ICLR.cc/2026/Conference/Submission24330/Reviewer_FEZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24330/Reviewer_FEZs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557293536, "cdate": 1761557293536, "tmdate": 1762943044572, "mdate": 1762943044572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies decision-focused online learning and utilize regularization for differentiability and perturbation for non-convexity. They succeed in generating static and dynamic regret results with respect to loss predictor parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe originality arises from a proper problem formulation for the regret analysis of decision-focused online optimization, incorporating static and dynamic regret analysis into the said problem and removing limitations of non-differentiability and non-convexity.\n\nQuality:\nThe submission seems technically correct, experimentally rigorous and reproducible (except minor caveats in the algorithms).\n\nClarity:\nThe submission is mostly clear.\n\nSignificance:\nThe submission presents theoretical (and possibly algorithmic) novel findings to achieve the static and dynamic regret results for online decision-focused learning."}, "weaknesses": {"value": "I did not notice any substantial weakness, so I am leaning towards acceptance.\n\nOne thing to note is that the exact challenge in achieving the said results could be emphasized. Possibly, after proper formulation, everything follows from the existing regret analysis techniques."}, "questions": {"value": "Questions:\n\nPage 2 Line 93: how does the linear structure cause zero or undefined gradient?\n\nPage 4 Line 195: should it be $i\\in[K]$?\n\nPage 4 Line 204: why does $\\epsilon=0$ imply anything? Doesn't the inequality hold for all $\\epsilon$?\n\nPage 6 Algorithm 1 Line 3: shouldn't the objective for $w$ be regularized?\n\n\nSuggestions:\n\nPage 2 Line 85: correct grammar.\n\nPage 3 Line 117: there seems to be a typo between $g$ and $\\phi$.\n\nPage 5 Line 275: $\\Theta$ was the parameter space in (8). Please correct.\n\nPage 7 Line 352: check grammar."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eA8aRTWf25", "forum": "FJhtHBphCt", "replyto": "FJhtHBphCt", "signatures": ["ICLR.cc/2026/Conference/Submission24330/Reviewer_X6pL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24330/Reviewer_X6pL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995431051, "cdate": 1761995431051, "tmdate": 1762943043815, "mdate": 1762943043815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of training predictive models that are used not downstream decision-making tasks. The authors frame this as a bi-level online optimization problem. They overcome challenges of non-differentiability and non-convexity by using regularization and perturbation, and develop variants of follow the perturbed leader and online gradient descent that get sublinear static and dynamic regret."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is an excellent paper!\n\n* The paper is very well written and includes intuitive explanations along with precise mathematical statements.\n\n* The paper tackles a fundamental problem: online decision-focused learning, where the goal is to train predictive models not just for prediction but for using those predictions in a downstream decision-making task. While prior work looked at this problem in the offline setting, this paper studies this problem in the online setting, provides provable regret guarantees - static and dynamic - and shows strong experimental results, where the proposed algorithms have a worse prediction accuracy than baselines but superior downstream performance.\n\n* The formulated problem is challenging: bi-level optimization, non-differentiable, and non-convex. The authors overcome these challenges using a combination of smoothing/regularization, perturbation and offline optimization oracles - all somewhat standard tools. Then, they develop variants of standard online learning algorithms, follow the perturbed leader and online gradient descent, for the online decision-focused setting. However, the overall problem formulation and solution methodology is very elegant and this is a plus in my opinion, especially when combined with the theoretical and empirical results and the importance of the problem."}, "weaknesses": {"value": "Please see the questions section some weaknesses/questions."}, "questions": {"value": "* The oracle assumption is understandable but it seems quite expensive to invoke the oracle in each round, especially for large non-convex problems. I understand that there are a lot of analyses on favorable loss landscapes for neural networks and how gradient descent-based methods can find \"good optima\". However, running such methods to convergence in each round seems quite slow in practice. Did you measure this in your experiments / are your experiments large-scale enough for this to be a problem? Do you have thoughts on how you could use lazy updating to only invoke the oracle once every few rounds while maintaining or suffering from a tolerable degradation in the regret bound?\n\n* Is my understanding correct that the choice of the regularizer does not affect the final regret bound as long as the hyperparameter $\\alpha_t$ is chosen correctly? This hyperparameter is chosen a function of $T, m$ and $n$. I understand that the time horizon is sometimes unknown. Are there cases where $m$ and $n$ might be unknown? In case of mis-specified $T, m$ or $n$, how does the regret bound degrade with the degree of mis-specification?\n\n* The paper assumes that the set $\\mathcal{W}$ is a convex polytope. How restrictive is the polytope assumption? If we had general convex sets, how would that impact your analyses and where would the problems arise?\n\n* The (average) regret bound of $ T^{-\\frac14} $ is quite slow compared to the usual $ T^{-\\frac12} $ regret bounds. I understand that the problem in this paper is harder (bi-level optimization, non-differentiable, non-convex). Do you have thoughts on whether this is tight and a lower bound? If not, what do you think could lead to improved regret bounds? For example, using an alternative to perturbation, an alternative to offline optimization oracles, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v4eAIx1auG", "forum": "FJhtHBphCt", "replyto": "FJhtHBphCt", "signatures": ["ICLR.cc/2026/Conference/Submission24330/Reviewer_JCMo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24330/Reviewer_JCMo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028596508, "cdate": 1762028596508, "tmdate": 1762943043638, "mdate": 1762943043638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}