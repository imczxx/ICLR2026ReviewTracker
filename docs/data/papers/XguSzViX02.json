{"id": "XguSzViX02", "number": 22764, "cdate": 1758335190183, "mdate": 1763117439645, "content": {"title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "abstract": "Existing text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and cross-domain transformation error. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing technique that produces visually appealing and temporally coherent videos through latent optimization guided by our novel STR score. The proposed score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal attention maps in text-to-video~(T2V) diffusion models, without the overhead of computationally expensive full 3D attention. Integrated into a latent optimization framework with a latent mask, STR-Match generates high-fidelity videos with strong spatiotemporal consistency, preserving key visual attributes of the source video while remaining robust under significant domain shifts. Our extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.", "tldr": "", "keywords": ["Generative Model", "Video Diffusion", "Training-Free Video Editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bdcb513afd5bf94595ede6d3a46cbf044e367c99.pdf", "supplementary_material": "/attachment/3430460d2269f67fa776da7bd5b917072e2392b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a STR-Match, a training-free method for text-guided video editing. The core of the method is a SpatioTemporal Relevance (STR) score, which aims to model pixel relationships across frames to maintain temporal consistency and visual fidelity. The STR score is computed by combining self-attention and temporal-attention maps from a pre-trained text-to-video (T2V) diffusion model, avoiding the need for computationally expensive full 3D attention. The authors demonstrate results on both U-Net and DiT-based T2V models (LaVie and CogVideoX)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A training-free method for text-guided video editing.\n2. A SpatioTemporal Relevance (STR) score is proposed to model pixel relationships across frames."}, "weaknesses": {"value": "1. The quality and writing of this paper are inferior. In fig. 1, fig.2, and fig.4, the square video samples from the TGVE dataset are forced to be distorted into rectangles. The appendix link is also broken. There is no appendix in the paper at all. Why did the author add it? Therefore, I think this paper is very hasty and cannot meet the submission standards.\n2. The core idea of using attention maps from a pre-trained model to guide editing is well-established in image editing (e.g., Prompt-to-Prompt) and has been extended to video (e.g., FateZero, MotionFlow). The paper does not sufficiently articulate why previous attention-manipulation techniques are Inadequate and why this specific, complex score was necessary to solve the problem.\n3. The mathematical formulation of the STR score (Equations 2-4) lacks a clear, intuitive explanation. The authors provide a procedural definition but fail to justify why this specific multiplicative and additive combination of attention maps is the optimal way to capture spatiotemporal relevance. The design feels arbitrary and overly engineered. \n4. The paper repeatedly emphasizes avoiding \"computationally expensive full 3D attention\" as a key advantage. However, calculating the STR score involves combining and optimizing multiple attention maps, and its actual computational overhead and memory usage have not been compared with existing methods. Without training, its practicality and scalability remain questionable."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SIrh6S46pF", "forum": "XguSzViX02", "replyto": "XguSzViX02", "signatures": ["ICLR.cc/2026/Conference/Submission22764/Reviewer_ZKRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22764/Reviewer_ZKRM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666641579, "cdate": 1761666641579, "tmdate": 1762942376473, "mdate": 1762942376473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "iy9rx1HsYj", "forum": "XguSzViX02", "replyto": "XguSzViX02", "signatures": ["ICLR.cc/2026/Conference/Submission22764/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22764/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763117437564, "cdate": 1763117437564, "tmdate": 1763117437564, "mdate": 1763117437564, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents STR-Match, a training-free method for consistent text-driven video editing.\nThe key idea is to compute spatiotemporal pixel relevance score and integrate it into a latent optimization (like guidance).\nThey show that their results are temporal coherence, maintaining semantic alignment with the edited prompt and preserving source appearance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong results – the qualitative examples (in the supplementary HTML and ZIP files) are impressive. Even for non-trivial edits involving large domain shifts, STR-Match maintains high fidelity and coherence.\n\n2. Clarity and presentation – the paper is well organized and easy to follow, with intuitive figures (2,3) which make the method readable and clearer."}, "weaknesses": {"value": "1. Dependence on the number of temporal neighbors - the method appears to rely heavily on the number of frames (neighbors) used when computing the spatiotemporal relevance. This parameter directly affects qualitative quality, quantitative scores, and runtime. However, it is not reported or ablated (as far as I could find). Clarifying how many neighbors are used, and analyzing the method’s sensitivity to this choice is important.\n\nDataset curation - the paper states that the authors collected their evaluation videos. Most of the shown examples involve slow or smooth motion, which may favor methods based on feature similarity. It is unclear whether STR-Match would perform as well on complex or fast motions (e.g., parkour, running, or hand-held camera movement). Please comment on the diversity of your dataset and whether results generalize to such settings.\n\nScalability to longer videos - the presented results seem limited to short clips (~16–17 frames).\nHow does the method scale to longer sequences, where relevance estimation and temporal accumulation could drift over time?\n\nMinor presentation issue - (1) the teaser figure (cat example) appears stretched — the aspect ratio seems incorrect, which slightly reduces the professional appearance of the paper. (2) Figure 2: Should z_1​ actually be z_t​? The figure seems to illustrate an intermediate denoising step rather than the first latent."}, "questions": {"value": "My questions are already mentioned in the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uDPoDFnays", "forum": "XguSzViX02", "replyto": "XguSzViX02", "signatures": ["ICLR.cc/2026/Conference/Submission22764/Reviewer_eFQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22764/Reviewer_eFQs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823787348, "cdate": 1761823787348, "tmdate": 1762942376106, "mdate": 1762942376106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents score distillation-based, training free, text-guided video editing framework, STR-Match. In addition to existing sds based editing framewokr, STR-Match proposes to extract spatial and temporal attention maps, and applies pixel-relevance guidance in the latent optimization stage. The paper experiments with both U-Net based and DiT based T2V models, outperforming existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a v2v editing method that is model-training free.\n- The paper conducts extensive comparison with sota baselines and show superiority against them.\n- The paper conducts comprehensive ablation studies."}, "weaknesses": {"value": "- The paper is based on DDS (Delta denoising Score, ICCV 2023) framework, and extends the score-based text-guided editing to t2v setup. Thus, naive extension of DDS on t2v models should be ablated.\n- The paper has very close philosophy as DreamMotion (ECCV 2024), which is also a training-free, score distillation-based video editing method. The work also exploits decomposed spatial and temporal attention maps for their guidance term. There should be a discussion on how STR-match is different from DreamMotion and also compared with the method.\n- The paper lacks a computational overhead comparison with baselines, which is an important analysis for training-free video editing methods.\n- Important ablation studies should be included in the main paper."}, "questions": {"value": "- What is the reference for the CogVideoX-V2V method?\n- Although the method is model training free, it requires latent optimization. How much more computational overhead is needed for the latent optimization stage, in both terms of time and memory?\n- Is it necessary to apply the loss $L_{cos}$ in every denoising step?\n- What is the reason/ground for choosing the first two blocks of CogVideoX DiT model for extracting attention maps?\n- Is Eq.(5) applied single time (single time solver) for each denoising step? Or does it require multiple optimizations within a denoising step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VVXxCUqK0q", "forum": "XguSzViX02", "replyto": "XguSzViX02", "signatures": ["ICLR.cc/2026/Conference/Submission22764/Reviewer_T6dk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22764/Reviewer_T6dk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106457238, "cdate": 1762106457238, "tmdate": 1762942375822, "mdate": 1762942375822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes training-free video editing approach by matching spatiotemporal attention feature between source and target latents.\nTo obtain spatiotemporal information, we propose the STR score, a spatiotemporal pixel relevance score that combines self- and temporal-attention maps. The effectiveness of proposed approach has been verfied with several base T2V diffusion models in multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Belows are strong points that this paper has:\n\n1. The paper is clear and enjoyable to read, providing a straightforward explanation of the motivation and methodology.\n\n2. The proposed STR score matching between source and target is a novel yet simple approach that can be easily applied to any pre-trained text-to-video (T2V) model.\n\n3. The experiments are thoughtfully designed and effectively demonstrate the strength and validity of the proposed method."}, "weaknesses": {"value": "1. Although the authors present computational complexity in Appendix B, details regarding inference time and additional FLOPs should be included to enable a more comprehensive comparison.\n\n2. The current STR score combines spatial and temporal attention components. To better demonstrate the complementary effects between these two aspects, it would be helpful for the authors to include an ablation study—such as using the full STR score, spatial-only, and temporal-only variants."}, "questions": {"value": "Please check above listed in Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4gHUAjTGhZ", "forum": "XguSzViX02", "replyto": "XguSzViX02", "signatures": ["ICLR.cc/2026/Conference/Submission22764/Reviewer_NTVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22764/Reviewer_NTVJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762447246515, "cdate": 1762447246515, "tmdate": 1762942375521, "mdate": 1762942375521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}