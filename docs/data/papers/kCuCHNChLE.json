{"id": "kCuCHNChLE", "number": 3321, "cdate": 1757401448465, "mdate": 1763517656614, "content": {"title": "DR-GGAD: Dual Residual Centering for Mitigating Anomaly Nonâ€‘Discriminativity in Generalist Graph Anomaly Detection", "abstract": "Generalist Graph Anomaly Detection (GGAD) seeks a unified representation learning model to detect anomalies in unseen graphs, but cross-domain transfer often entangles the learned anomalous and normal representations. We formalize this degradation as Anomaly non-Discriminativity (AnD) and define a normalized score to quantify it. We present DR-GGAD, which avoids direct comparison between anomalous and normal nodes via two residual modules: 1) a multi-scale Hyper Residual (HR) Center measuring node-to-center distances, yielding a compact normal residual structure with margin-pushed anomalies; 2) an Affinity-Residual (AR) module enforcing local residual directional consistency to recover structural separability. With frozen parameters (no target fine-tuning), DR-GGAD fuses both signals into a unified score. On 8 benchmark target graphs, it achieves new SOTA: mean AUROC +5.14% over the best prior GGAD, with large gains on high-AnD datasets (Facebook +12.59%, ACM +9.96%, Amazon +7.48%) and strong AUPRC boosts (Amazon +17.12%, CiteSeer +17.77%). Ablations confirm complementary roles of the two modules. DR-GGAD thus establishes AnD as a measurable bottleneck and delivers robust cross-domain anomaly detection.", "tldr": "This paper proposes DR-GGAD, a framework that addresses Anomaly non-Discriminativity in generalist graph anomaly detection.", "keywords": ["Graph Mining", "Social Network Analysis", "Graph Anomaly Detection"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04635f791e3e3de68d073d09086737ab5b552016.pdf", "supplementary_material": "/attachment/b5cb946fac6ecd65065e48fd045c857623f5f529.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new evaluation metric, AnD, which analyzes anomaly scores by defining two types of residuals: the hyper-residual center and the affinity residual, corresponding to feature and structural aspects, respectively. The difference between these two residuals is used to compute the anomaly score. The proposed method is evaluated on eight datasets and achieves the best performance under a zero-shot setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) This dual-center strategy effectively addresses the non-discriminative nature of anomalies by reducing residual overlap in both feature and structural spaces, thereby enabling robust zero-shot detection on unseen graphs.\n\n(2) They propose AnD to quantitatively measure representation overlap and visualize the dual residuals, making the paper well-motivated and conceptually clear."}, "weaknesses": {"value": "(1) The hyperparameters in the clustering module have not been thoroughly investigated. How does the performance of DR-GGAD vary with different values of $\\tau$ and the number of clusters? The results seem to become stable once the number of prototypes exceeds 20. Please provide an explanation for why around 20 prototypes are appropriate for a two-class anomaly-detection setting across various datasets.\n\n(2) Since multiple graphs are used for training, the paper should clearly explain how the pre-trained model is optimized across these graphs. Please describe the detailed training pipeline, including how samples from different graphs are organized and how parameter updates are coordinated across them.\n\n(3) ARC, UNprompt, and AnomalyGFM employ supervised pre-training, whereas the proposed method appears to follow an unsupervised paradigm. To ensure a fair and transparent comparison, the main results table should explicitly indicate the differences in supervision settings among the compared methods."}, "questions": {"value": "See above **Weakness**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tQc28xMZyP", "forum": "kCuCHNChLE", "replyto": "kCuCHNChLE", "signatures": ["ICLR.cc/2026/Conference/Submission3321/Reviewer_XcC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3321/Reviewer_XcC9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870431018, "cdate": 1761870431018, "tmdate": 1762916669308, "mdate": 1762916669308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the generalist graph anomaly detection (GGAD) problem. The authors quantify the anomaly-normal separability of different datasets by a new metric termed anomaly non-discriminativity. Based on this, a new GGAD method is introduced, with hyper residual and affinity residual modules for anomaly scoring. Experiments are conducted to show the effectiveness of the proposed method DR-GGAD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper (i.e., quantifying the discriminatively ability of cross-dataset GAD models) is interesting. It would be useful to measure the separability of different datasets in terms of graph anomaly detection.\n\n2. The performance of the proposed method is competitive compared to other baselines. \n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The motivation of designing HR and AR should be better clarified. It is not clear why these designs can help address the AnD bottleneck.\n\n2. In the \"node attribute alignment\" part, only a linear/random mapping is used here. In this case, how to ensure the semantic alignment of features across different datasets?\n\n3. No discussion is given about the running efficiency of the proposed method. More evidences, including complexity analysis and empirical efficiency comparison, are expected.\n\n4. In table 1, ARC is described as \"needs Few fine-tuning\". However, according to the paper I think ARC doesn't need fine-tuning (but requiring few-shot samples). It's better to fix this."}, "questions": {"value": "Please refer to the issues discussed in the Cons section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ragAeOC71", "forum": "kCuCHNChLE", "replyto": "kCuCHNChLE", "signatures": ["ICLR.cc/2026/Conference/Submission3321/Reviewer_cnZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3321/Reviewer_cnZP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954204635, "cdate": 1761954204635, "tmdate": 1762916668750, "mdate": 1762916668750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents DR-GGAD, a generalist anomaly detection framework that introduces two residual modules for centre-based scoring. Specifically, the two residual modules focus on attribute and structural normality separately. A GAD statistic, Anomaly non-Discriminativity (AnD), is proposed to quantify the overlap between normal and anomalous representations, which serves as a measurable indicator of cross-domain separability. Experiments on eight benchmark datasets demonstrate the competitive performance of the proposed method for generalist graph anomaly detection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. Clear visualisations are provided to enhance understanding.\n3. Theoretical analysis is presented to support the proposed Anomaly non-Discriminativity (AnD) score."}, "weaknesses": {"value": "1. Centre-based scoring is used in this paper. However, deep hypersphere-based AD methods Deep SVDD (Ruff et al., 2018) and Deep SAD (Ruff et al., 2020) are not discussed. A discussion comparing the proposed scoring approach with these methods would be essential to highlight the differences and advantages.\n2. The paper mentions that cross-domain transfer often entangles anomalous and normal representations. Previous works such as COMMANDER (Ding et al., 2021) and ACT (Wang et al., 2023) have explored cross-domain evaluation, and discussing their relevance would help clarify this limitation. \n3. In Figure 3, the curve for BlogCatalog appears almost flat, suggesting that the synergy between the two residual terms may be limited in some cases. It would be good to provide more details on this.\n4. It would be helpful to elaborate on how the dominant factor for abnormality can be determined in practice for unseen graphs.\n5. Some datasets are from GAD Benchmark (Tang et al. 2023). Could the author provide some justifications on why they are selected over the rest?"}, "questions": {"value": "Please refer to my weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WbyNvqnOWZ", "forum": "kCuCHNChLE", "replyto": "kCuCHNChLE", "signatures": ["ICLR.cc/2026/Conference/Submission3321/Reviewer_CXFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3321/Reviewer_CXFF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960493346, "cdate": 1761960493346, "tmdate": 1762916668491, "mdate": 1762916668491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces $\\mathcal{A}n\\mathcal{D}$, a graph anomaly detection framework that leverages multi-scale residual representations derived from successive graph neural network layers to detect anomalous nodes and substructures. It constructs domain-invariant reference centers and measures affinity residuals between node embeddings and these references to identify deviations indicative of anomalies. The method aims to capture both feature-level and topological irregularities in graphs without relying on labeled anomalies or reconstruction losses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is easy to follow.\n2.\tThe performance seems improved a lot.\n3.\tThe statistical results are comprehensive."}, "weaknesses": {"value": "1.\tIs the dimensionality of the visualized embeddings in Figure 1 directly set to 2? Since t-SNE projects data into a 2D space, the resulting distribution differs from the actual data distribution.\n2.\tThe authors should provide a discussion comparing $\\mathcal{A}n\\mathcal{D}$ with existing distribution-level metrics such as MMD. While MMD measures distributional distances, what advantages does $\\mathcal{A}n\\mathcal{D}$ offer?\n3.\tThe authors should clarify the technical differences between the proposed method and clustering-based approaches.\n4.\tThe paper claims that multi-scale residuals from successive GNN layers form a domain-invariant reference, but no theoretical justification or citation is provided to support this statement.\n5.\tThe authors state that the Affinity Residual component can reveal anomalies primarily hidden in the topology. However, no experimental evidence supports this claim. It is also recommended to compare with subgraph anomaly detection methods to substantiate this advantage.\n6.\tThe authors mention that no existing methods explicitly address representation overlap. However, representation overlap appears related to clustering operations that aim to increase inter-class separation, such as works [1], [2]. Please discuss how the proposed method differs from or improves upon those clustering-related works.\n7.\tThe paper lacks one-class classification baselines such as OCSVM. The authors claim their method remains effective across various unseen graphs. If the decision boundary tightly encloses the normal data, one-class models may also generalize well. Please include a comparison or discussion on this point.\n8.\tAn ablation study comparing the concatenation of residuals from all layers versus using only the final-layer residual is missing. The authors should explain why concatenation was chosen instead of alternative fusion strategies (e.g., addition or averaging).\n9.\tIt remains unclear how the proposed method extracts semantically meaningful embeddings without the guidance of a reconstruction or logits-based loss.\n10.\tWhy was K-Means chosen for center initialization instead of Spectral Clustering? Please include comparisons and also provide results using K-Means directly for anomaly detection.\n11.\tThe problem definition is unclear. Does the center construction step rely solely on normal data?\n12.\tWhat is the meaning of $\\epsilon$, and what value is used in the experiments?\n13.\tMore visual comparisons for the ablation studies should be presented to illustrate their effects intuitively.\n14.\tHow does the proposed method handle source data with inconsistent feature dimensions?\n15.\tThe dataset name in Figure 3 is inconsistent with that in the experimental table; please correct it.\n16.\tPlease analyze why the proposed method achieves lower AUPRC on the Reddit and Weibo datasets.\n17.\tThe manuscript contains multiple formatting issues. Please proofread carefully for consistency and correctness.\n\n**Reference:**\n\n [1] \"discDC: Unsupervised discriminative deep image clustering via confidence-driven self-labeling.\" Pattern Recognition (2025): 112382.\n\n[2] \"Towards feature distribution alignment and diversity enhancement for data-free quantization.\" 2022 IEEE International Conference on Data Mining (ICDM). IEEE, 2022."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rz3a9RrjSj", "forum": "kCuCHNChLE", "replyto": "kCuCHNChLE", "signatures": ["ICLR.cc/2026/Conference/Submission3321/Reviewer_ozsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3321/Reviewer_ozsE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993050398, "cdate": 1761993050398, "tmdate": 1762916668244, "mdate": 1762916668244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}