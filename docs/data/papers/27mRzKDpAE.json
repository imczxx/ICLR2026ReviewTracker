{"id": "27mRzKDpAE", "number": 5104, "cdate": 1757848438299, "mdate": 1759897994300, "content": {"title": "Building Scalable Real-World Robot Data Generation via Compositional Simulation", "abstract": "Recent advancements in foundational models, such as large language models and world models, have greatly enhanced the capabilities of robotics, enabling robots to autonomously perform complex tasks. However, acquiring large-scale, high-quality training data for robotics remains a challenge, as it often requires substantial manual effort and is limited in its coverage of diverse real-world environments. To address this, we propose a novel hybrid approach called Compositional Simulation, which combines classical simulation and neural simulation to generate accurate action-video pairs while maintaining real-world consistency. Our approach utilizes a closed-loop real-sim-real data augmentation pipeline, leveraging a small amount of real-world data to generate diverse, large-scale training datasets that cover a broader spectrum of real-world scenarios. We train a neural simulator to transform classical simulation videos into real-world representations, improving the accuracy of policy models trained in real-world environments. Through extensive experiments, we demonstrate that our method significantly reduces the sim2real domain gap, resulting in higher success rates in real-world policy model training. Our approach offers a scalable solution for generating robust training data and bridging the gap between simulated and real-world robotics.", "tldr": "Building Scalable Real-World Robot Data Generation via Compositional Simulation", "keywords": ["sim2real", "world model", "robotic manipulation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5a78437564237bc7f54eef88e7d84e3c0cb2614.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This submission introduces ComSim, a hybrid framework that integrates classical and neural simulation through a real–sim–real pipeline to generate robot data. Experiments show that the proposed method improves real-robot success rates, and demonstrate generalization capability. However, several concerns exist."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The submission is well-written and easy to follow."}, "weaknesses": {"value": "- The overall contribution of this submission is weak. While authors claim a new real-sim-real pipeline, there exists many real-sim-real works that achieve better performance, such as Chen et al., 2024; Torne et al., 2024; Dai et al., 2024. Using diffusion models for augmentation is also not new. People have already explored this for robotics, such as Yu et al., 2023. Furthermore, the conclusion drawn from this submission---real-robot performance can benefit from synthetic data---is also not new, as researchers have derived from, e.g., Maddukuri et al., 2025.\n\n- The claim about poor sim-real joint training performance (L47) is factually wrong. In fact, researchers have shown that sim-real joint training can significantly help real-robot performance (Maddukuri et al., 2025).\n\n- While authors claim the proposed method addresses issues such as scaling, appearance, physics, and action consistency (Fig. 1), there is no experimental results that support each of this claim.\n\n- For robots to work in complex real-world environments, only visual observations are insufficient. The proposed method is incapable of handling other important modalities, such as proprioception, force, etc.\n\n- While authors claim the proposed method can generate diverse data. Upon a closer look, all data is merely on a tabletop setting with single or only a few objects. The object diversity is also extremely limited.\n\n- The experimented robotic tasks are too simple and trivial. It's merely a tabletop setting with stationary robot arms. Objects are few and quite limited in categories. \n\n- If the proposed method replays the same robot trajectories from the real-world data, given the same amount of seeding real-world data, it's unlikely the trained policies can exhibit better spatial generalization.\n\n- Comparisons to other robot data synthesis methods are necessary, such as MimicGen (Mandlekar et al., 2023), DemoGen (Xue et al., 2025), etc.\n\n## References\n- Chen et al., URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images, RSS 2024.\n- Torne et al., Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation, RSS 2024.\n- Dai et al., Automated Creation of Digital Cousins for Robust Policy Learning, CoRL 2024.\n- Yu et al., Scaling Robot Learning with Semantically Imagined Experience, arXiv 2023.\n- Maddukuri et al., Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation, RSS 2025.\n- Mandlekar et al., Mimicgen: A data generation system for scalable robot learning using human demonstrations, CoRL 2023.\n- Xue et al., DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning, RSS 2025."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xNxRhVhLre", "forum": "27mRzKDpAE", "replyto": "27mRzKDpAE", "signatures": ["ICLR.cc/2026/Conference/Submission5104/Reviewer_U711"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5104/Reviewer_U711"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540708149, "cdate": 1761540708149, "tmdate": 1762917876688, "mdate": 1762917876688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes learning a neural renderer that maps simulator video–action pairs into realistic videos, thereby augmenting robot manipulation data and reducing the sim-to-real gap. To achieve this, the authors build a simulation environment that closely mirrors the real-world setup, enabling the construction of sim-video / real-video / action triplets. Experimental results demonstrate that the proposed method effectively transforms unseen simulated videos into realistic ones, leading to improved policy performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of leveraging generative AI techniques to mitigate the sim-to-real gap is novel and insightful. It provides a fresh perspective on how modern generative models can be applied to robotics and data augmentation.\n\n2. The paper conducts extensive experiments, offering strong empirical validation for this approach and demonstrating its effectiveness across various scenarios."}, "weaknesses": {"value": "1. The proposed method relies on paired sim-to-real data, which limits scalability. In many real-world applications, it may not be feasible to build a simulator that perfectly matches each real system. The authors should consider exploring unsupervised or unpaired approaches to enable broader generalization and application.\n\n2. The experimental section lacks comprehensive baselines. In particular, since the method assumes the availability of a simulator, it would be beneficial to compare with other simulation-based data augmentation techniques, such as MimicGen, to convincingly establish performance advantages."}, "questions": {"value": "1. Can this method generalize more broadly — for example, would it still work effectively in scenarios where no simulator is available?\n\n2. Could the authors provide stronger baselines to verify the effectiveness of this method compared to other native sim-to-real or data-augmentation approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XQ5K52oq35", "forum": "27mRzKDpAE", "replyto": "27mRzKDpAE", "signatures": ["ICLR.cc/2026/Conference/Submission5104/Reviewer_7ZeK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5104/Reviewer_7ZeK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924901481, "cdate": 1761924901481, "tmdate": 1762917876439, "mdate": 1762917876439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Compositional Simulation, a real–sim–real data generation pipeline that combines classical simulation with a fine-tuned diffusion-based video-to-video model to generate pseudo-real robot demonstrations. A small set of real trajectories are replicated in simulation to train a sim-to-real neural renderer, which is then used to convert large amounts of simulated rollouts into realistic data. Experiments on tabletop tasks show improved real-world performance and stronger spatial and object generalization under low-data regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear motivation and useful problem scope:** Addresses the practical challenge of scaling robot training with minimal real-world data.\n- **Simple yet effective hybrid approach:** Real–sim–real pipeline is intuitive and avoids hallucination issues common in video-generation action simulators.\n- **Empirical gains:** Demonstrates improvement in both in-domain and OOD settings for real robot manipulation tasks.\n- **Data efficiency:** Achieves strong results using only 10 real demos + pseudo-real rollouts, showing value for low-resource robotics.\n- **Reproducibility:** Provides training details, simulation setup, and evaluation protocols, facilitating replication."}, "weaknesses": {"value": "1. **Citation formatting inconsistencies** — Please adopt proper LaTeX citation conventions (e.g., `\\citep{}`) for clarity and consistency with ICLR standards.\n2. **Unaddressed dynamics gap** — The method primarily tackles appearance transfer; however, real-world dynamics mismatch remains unstudied, limiting applicability to more complex manipulation domains.\n3. **Over-simplified visual settings** — Experiments rely on black backgrounds and isolated tabletop objects, which reduces visual difficulty; simpler sim-to-real techniques or classical augmentation pipelines may already perform well under such conditions.\n4. **Limited task complexity and diversity** — OOD generalization is demonstrated only on basic tabletop tasks. Evaluating more realistic, cluttered, or multi-stage manipulation scenarios would substantially strengthen the evidence.\n5. **Lack of real robot execution videos** — Real-world rollout videos are essential to verify qualitative behavior, demonstrate consistent control performance, and support robustness claims."}, "questions": {"value": "1. **Why not directly edit policy inputs?**  \n   How does compositional simulation compare to applying image editing or video style-transfer directly on policy observation streams, potentially leveraging large human video corpora? Given your test-time environment setup, one might expect that direct image editing approaches could yield similar benefits — a clarification on this comparison would be valuable.\n\n2. **Include real-image references in qualitative results**  \n   For clarity and to better assess target realism, please include paired real images alongside the pseudo-real outputs in qualitative figures.\n\n3. **Clarify Figure 2 embedding dimensions**  \n   Why is the projected latent feature dimension larger than the final reconstructed video representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0HofeZfttj", "forum": "27mRzKDpAE", "replyto": "27mRzKDpAE", "signatures": ["ICLR.cc/2026/Conference/Submission5104/Reviewer_AVmY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5104/Reviewer_AVmY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951992374, "cdate": 1761951992374, "tmdate": 1762917876075, "mdate": 1762917876075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a method to leverage simulation data directly for learning real world tasks. The approach is primarily based on closing the visual gap between simulation and real world environments. They use a classical simulator (with classical rendering), and train a video model to translate image sequences from simulation to their counterparts in the real world. To train the video model, they carefully collect aligned data in simulation and real world. Experiments show that co-training with a small amount of real world data and simulation data translated to look like real world data, yields superior performance to using real-only, or raw simulation data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, easy to follow, and the motivation is clear.\n- Experiments in the real world are extensive, including both in domain and out of domain analyses."}, "weaknesses": {"value": "An overall limitation of this work is in its scalability:\n- First, building simulations for arbitrary real-world tasks is challenging, due to issues with modeling replicas of complex scenes, and deformable manipulation.\n- Another limitation is the requirement of collecting aligned data between simulation and the real world, which can be very time-consuming and error-prone.\n\nFinally, this work focuses primarily on the visual gap, rather than other potential gaps between simulation and the real world, such as physics and the content modeling gap. The overall impact of this approach may be limited, compared to full neural world models that explicitly model all aspects of simulation (physics and visuals) by learning from real-world data."}, "questions": {"value": "- Are there ways to train the video model without aligning simulation and real-world trajectories?\n- How can this approach be extended to support existing large-scale simulation datasets, to close the visual gap for them and make them useful for learning real-world tasks?\n- It would be interesting to see a comparison to methods that train on raw real-world and simulation trajectories, trained with heavy domain randomization.\n- Would the benefits of co-training with \"translated\" simulation data be present if training on larger real-world datasets, eg. with 50 real-world demonstrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qjvxx2vbul", "forum": "27mRzKDpAE", "replyto": "27mRzKDpAE", "signatures": ["ICLR.cc/2026/Conference/Submission5104/Reviewer_Hqoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5104/Reviewer_Hqoy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5104/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981764772, "cdate": 1761981764772, "tmdate": 1762917875855, "mdate": 1762917875855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}