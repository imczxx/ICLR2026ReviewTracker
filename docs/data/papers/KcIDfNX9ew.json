{"id": "KcIDfNX9ew", "number": 5964, "cdate": 1757948843863, "mdate": 1763121099159, "content": {"title": "All You Need Are Random Visual Tokens?  Demystifying Token Pruning in VLLMs", "abstract": "Vision Large Language Models (VLLMs) usually incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (_e.g._, beyond the 20th), existing training-free pruning methods _perform no better than random pruning_. We hypothesize that this degradation is caused by **\"vanishing token information''**, where visual tokens progressively lose their salience with increasing network depth. \nTo validate this hypothesis, we formally quantify a token's information content by measuring the perturbation to the model's output probability upon its removal. \nUsing this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon'', beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (_e.g._, Qwen2.5-VL) make more effective use of deeper visual tokens compared with weaker models (_e.g._, LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods across various models and benchmarks, with improvements up to 6.7% on LLaVA-1.5-7B. Using DART with random pruning achieves state-of-the-art results, maintaining 93.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens.", "tldr": "", "keywords": ["Token Pruning", "Vision-Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/14c4e508bf4be46736eead41b02786b05f4ebf4f.pdf", "supplementary_material": "/attachment/1035040dae9fe53cf7a7aba4308b58137cd36750.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates token pruning in Vision-Language Large Models (VLLMs) and introduces the concept of an \"information horizon\"—a layer beyond which visual tokens contribute little to model output. The authors propose an information score based on output perturbation to quantify token importance, and use it to analyze the salience of visual tokens across layers. They find that simple random pruning in deep layers can balance efficiency and performance, and further enhance existing pruning strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a compelling empirical finding—the existence of an “information horizon” in deeper layers of VLLMs—where visual tokens become increasingly redundant.\n\n2. The proposed method for quantifying token information via output perturbation is intuitive and well-validated."}, "weaknesses": {"value": "1. The paper proposes to use information scores to identify the “information horizon,” beyond which pruning can be safely applied. However, calculating this score for every visual token requires measuring perturbations in output probability, which can be computationally expensive.\n\n2. The finding that pruning visual tokens in deeper layers does not significantly degrade performance raises the question of whether essential information has already been transferred to the text tokens. However, the paper does not investigate the information content of text tokens. Analyzing their information scores might uncover further insights into modality interactions or model reliance on textual representations in later stages.\n\n3. Since pruning is applied only in the final few layers, the efficiency improvements may be limited. The paper would benefit from a more detailed analysis of actual computational savings, such as FLOPs reduction or latency measurements. Furthermore, the overhead of computing the optimal pruning layer could negate some of these efficiency gains, making the approach more observational than practically deployable.\n\n4. While the paper empirically observes the existence of an information horizon, it lacks an in-depth explanation of why visual token information diminishes in deeper layers. A more detailed exploration could enhance the scientific contribution of the work."}, "questions": {"value": "1. While the paper demonstrates that random pruning in deeper layers is effective, it remains unclear how the specific layers for pruning are selected in the experimental setup or tables. Is the pruning depth fixed across all downstream tasks, or is it adjusted for each task individually?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTPY8gEy3m", "forum": "KcIDfNX9ew", "replyto": "KcIDfNX9ew", "signatures": ["ICLR.cc/2026/Conference/Submission5964/Reviewer_yk93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5964/Reviewer_yk93"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798045404, "cdate": 1761798045404, "tmdate": 1762918381158, "mdate": 1762918381158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Hq6nm2frrO", "forum": "KcIDfNX9ew", "replyto": "KcIDfNX9ew", "signatures": ["ICLR.cc/2026/Conference/Submission5964/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5964/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121081499, "cdate": 1763121081499, "tmdate": 1763121081499, "mdate": 1763121081499, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies token pruning in Vision-Language Large Models (VLLMs) and discovers that, in deeper layers, existing pruning methods perform no better than random pruning. The authors propose measuring each visual token’s “information” by the change in output probability upon its removal, revealing that token information gradually vanishes at an “information horizon.” This horizon varies with task complexity and model capability. It demonstrates that simply integrating random pruning with existing pruning methods can more effectively balance inference efficiency and accuracy across various datasets"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall clearly written and easy to follow.\n2. It thoroughly investigates the behavior of visual information in the deeper layers of multimodal large models."}, "weaknesses": {"value": "1. The results shown in the teaser are not very convincing. The three benchmarks used are either too simple or yield unstable evaluations. It would be more appropriate to choose datasets such as TextVQA, MMBench, or DocVQA instead.\n\n2. The overall observation seems quite similar to that in PDrop. A discussion and comparison with the observations in PDrop should be added to the paper.\n\n3. When estimating a visual token’s information in shallow layers, multiple forward passes are required, which incurs very high computational cost."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cusTZIUtAw", "forum": "KcIDfNX9ew", "replyto": "KcIDfNX9ew", "signatures": ["ICLR.cc/2026/Conference/Submission5964/Reviewer_YVyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5964/Reviewer_YVyB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934772417, "cdate": 1761934772417, "tmdate": 1762918378999, "mdate": 1762918378999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the phenomenon of vanishing token information, observing that visual tokens become increasingly redundant in intermediate and deeper layers. Building on this insight, the authors propose a simple random pruning strategy applied to deeper layers to balance the trade-off between performance and efficiency. Extensive experiments across various models and benchmarks demonstrate that integrating existing methods with random pruning yields state-of-the-art results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper presents an analysis showing that visual token information becomes increasingly trivial in deeper layers.\n2. Experimental results demonstrate that integrating existing approaches with random token pruning leads to performance improvements."}, "weaknesses": {"value": "1. The novelty of this paper is limited. Prior studies have already observed that visual tokens become less important in deeper layers [1], and another work [2] has examined the effects of random token pruning. VScan [3] has also shown that model predictions tend to converge at varying intermediate layers; therefore, performing token pruning in deeper layers can effectively balance efficiency and performance.\n2. The performance gains are relatively minor, with improvements of less than 0.5%.\n3. The experimental results are not comprehensive. For example, the paper does not report performance on video question answering tasks. In addition, comparisons with more recent baselines are missing.\n4. This work reads more like a technical report rather than a research paper. It primarily presents empirical observations showing that applying random pruning can yield slight improvements, but lacks deeper theoretical analysis or novel methodological insights to substantiate or generalize the findings.\n5. It is unclear how the efficiency of the proposed approach compares with existing methods. A detailed comparison in terms of computational cost, inference speed would strengthen the paper.\n\n[1] PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction. https://arxiv.org/abs/2410.17247.\n\n[2] Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem? https://www.arxiv.org/abs/2502.11501.\n\n[3] VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models. https://arxiv.org/abs/2505.22654."}, "questions": {"value": "See the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cwQ6FWp1ZC", "forum": "KcIDfNX9ew", "replyto": "KcIDfNX9ew", "signatures": ["ICLR.cc/2026/Conference/Submission5964/Reviewer_nD3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5964/Reviewer_nD3h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941859997, "cdate": 1761941859997, "tmdate": 1762918378488, "mdate": 1762918378488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presented a study of visual token pruning mechanism in VLMs. The authors first proposed a metric called visual token information (layer specific), which is defined as the contribution of a specific visual token to the ground truth output logits, and it is measured by passing only that visual token after a certain layer and compare against no visual token baseline. From this new metric definition, the analysis of several visual token pruning methods shows that their selection of tokens exhibit no difference than random selection at deeper layers, i.e. in deeper layers, all visual tokens contain very similar information. Based on this finding, they proposed to simply drop a random subset of visual tokens in deep layers to improve efficiency while keeping performance. The proposed random dropping can also be combined with existing token pruning methods to achieve better performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presented some interesting analysis of how different visual token pruning methods’s token selection differs in early layers but exhibit much larger similarity in deeper layers. \n\nThe proposed random token dropping in deeper layers, when combined with existing visual token pruning methods, yield further improvements on 7 visual understanding benchmarks using two VLMs."}, "weaknesses": {"value": "The overall novelty and contribution of this paper feels quite limited. The proposed visual token information measurement is actually a commonly used technique to measure the important of individual tokens (Jain and Wallace 2019), the insights about how shallow layers contains much more interaction between visual and text tokens but deeper layers require much less interaction have been discussed in previous works such as VTW (Lin et al. 2024) and VScan (Zhang et al. 2025). In fact, the proposed random token dropping is very similar to VTW conceptually except that a small random subset of the tokens are still kept in this paper. \n\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nLin, Zhihang, Mingbao Lin, Luxi Lin and Rongrong Ji. “Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference.” ArXiv abs/2405.05803 (2024): n. pag.\n\nZhang, Ce, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia P. Sycara, Haitao Mi and Dong Yu. “VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models.” ArXiv abs/2505.22654 (2025): n. pag."}, "questions": {"value": "Have you tried the same analysis on larger models? e.g. 72B scale where the number of layers are much larger. Will the same conclusion still hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQO1mxAQLC", "forum": "KcIDfNX9ew", "replyto": "KcIDfNX9ew", "signatures": ["ICLR.cc/2026/Conference/Submission5964/Reviewer_6ECg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5964/Reviewer_6ECg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976883369, "cdate": 1761976883369, "tmdate": 1762918377670, "mdate": 1762918377670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}