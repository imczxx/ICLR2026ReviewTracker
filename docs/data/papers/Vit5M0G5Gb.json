{"id": "Vit5M0G5Gb", "number": 9640, "cdate": 1758132157432, "mdate": 1759897707424, "content": {"title": "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Architectures", "abstract": "Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.", "tldr": "We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures.", "keywords": ["learning dynamics", "gradient flow", "simplicity bias"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24c1831d77c6255a9d7f617f61a02b59c60e46ec.pdf", "supplementary_material": "/attachment/f9cecc8ff16b8b9ef34fbad2022fbec1828bd63b.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a dynamical explanation for the simplicity bias observed in neural networks trained with SGD. The authors argue that when a neural network is initialized with small weights (i.e., near the origin of the loss landscape), it evolves closely along an invariant manifold corresponding to a width-$h$ network. It can then transition to an invariant manifold corresponding to a network of width $h+1$, which results in a steep decrease in loss.\n\n- Their central contribution is that, for this initialization, the saddle-to-saddle dynamics corresponds to a one-by-one activation of neurons. The network starts by fitting a single neuron, evolving closely along the corresponding invariant manifold until the optimal one-neuron solution is found. However, this minimum from a one-neuron perspective is a saddle point for the full-width network. The network then transitions to activate another dormant neuron and evolves closely along a two-neuron invariant manifold. This process repeats.\n\n- While the embedding of width-$(h-1)$ networks within width-$h$ networks is well-known, the authors demonstrate this for CNNs and attention layers and connect the phenomenon to the dynamics of SGD. They also provide novel propositions regarding timescale separation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work connects saddle-to-saddle dynamics to the stage-wise learning of neurons along invariant manifolds, proving a specific version of simplicity bias. While the embedding of width-$h$ neural networks in the loss landscape of width-$(h+1)$ NNs is well-known, the authors extend these results to CNNs and attention layers.\n\n- They prove a timescale separation for the saddle-to-saddle dynamics, which is driven by the target function for linear neural networks but by the weight magnitude in the quadratic case.\n\n- One of the most interesting points is the new understanding of the lazy vs. rich regime. The dynamics within these regimes (i.e., saddle-to-saddle dynamics along invariant manifolds) can be distinct from strong feature learning, such as learning a low-rank solution. This has the potential to provide a much more detailed account of SGD dynamics."}, "weaknesses": {"value": "- A more formal introduction to dynamical systems, perhaps in a short paragraph or an appendix, would be helpful. For instance, the term \"invariant manifold\" is used but never formally defined.\n\n- There is a significant body of literature on spectral bias in kernels (e.g. https://arxiv.org/pdf/1912.01198), such as the Neural Tangent Kernel (NTK). The paper is missing a discussion on how its results connect to spectral bias in function space. Although the mechanism in the infinite-width limit is different, it would be valuable to explain these differences and explore how the two perspectives could be combined.\n\n- A clearer study, even if only empirical, on how the structure of the target function influences learning and saddle-to-saddle dynamics would be beneficial. For example, what happens with single-index models or k-sparse parity? How does the data distribution $p(x)$ (e.g., normal, uniform, or spiked) affect the results? It's unclear to what extent the observed dynamics are solely a result of how neural networks are initialized and trained. Since the gradient is taken with respect to the loss function, it would be highly interesting to understand how data-induced invariant manifolds (see questions) might arise and interact with the model-induced ones.\n\n- A clearer and more detailed phase diagram of the lazy and rich regimes would be nice, incorporating the new insight that the distinction is more about saddles and invariant manifolds than just the initialization scale and learing rate.\n\n-  There is a typo on Line 424."}, "questions": {"value": "- What happens to the dynamics if there are loss-induced invariant manifolds? For instance, in a teacher-student setup where the target function (the teacher) has the same symmetries as the student network, how would this affect the dynamics? In general, how does the data set and e.g. the data set size influence the dynamics and generalization.\n\n- There is a long history of research on simplicity bias (especially: https://www.jmlr.org/papers/volume22/20-676/20-676.pdf, https://www.nature.com/articles/s41467-024-54813-x). How does the notion of simplicity bias presented here connect to these prior findings?\n\n- Based on these results, what are potential new definitions for the lazy vs. rich regimes?\n\n- How can this framework be connected to the kernel perspective on spectral bias?\n\n- How do the results change when we remove the parameter symmetries? (https://neurips.cc/virtual/2024/poster/93573)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7k3TL9GRXN", "forum": "Vit5M0G5Gb", "replyto": "Vit5M0G5Gb", "signatures": ["ICLR.cc/2026/Conference/Submission9640/Reviewer_8EAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9640/Reviewer_8EAS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761045026269, "cdate": 1761045026269, "tmdate": 1762921171356, "mdate": 1762921171356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted work analyzes saddle to saddle dynamics in neural networks. Specifically, the authors introduce a unifying framework for analyzing various architectures, including FCNs, CNNs and Attention. \n\nThe key results are:\n1. Fixed points of narrow networks are embedded inside wider networks.\n2. Mechanism behind saddle to saddle dynamics\n3. Definition of 'simple' in different networks: for instance, low rank in linear networks\n\nThese results have interesting implications:\n1. While network width does not affect the time scale separation in linear networks, adding more attention heads in shortens the plateaus\n2. The power law exponent of the data results in longer plateaus\n3. Shorter plateaus result from increases initialization scale"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has several interesting contributions:\n1. Different architectures learn differently: linear networks are more data driven where quadratic models (such as attention) are sensitive to initialization scale.\n2. Conditions for Saddle-to-saddle dynamics\n\nOverall, the paper is well written has significant contributions towards our understanding of neural network dynamics and its dependence on architecture and dataset properties."}, "weaknesses": {"value": "* The work is limited to simple setups: feedforward networks, gradient flow and synthetic datasets"}, "questions": {"value": "* Do the authors have an intuition about how saddle to saddle dynamics changes on adding residual connections? At least empirically?\n* Can the authors clarify how their results on time scale separation differs from works like [1]\n\n[1] Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3kppX92IeH", "forum": "Vit5M0G5Gb", "replyto": "Vit5M0G5Gb", "signatures": ["ICLR.cc/2026/Conference/Submission9640/Reviewer_axwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9640/Reviewer_axwH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761427415178, "cdate": 1761427415178, "tmdate": 1762921171045, "mdate": 1762921171045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work attempts to explain the emergence of saddle-to-saddle dynamics in\nthe training of neural networks by gradient descent via invariant manifolds. The\ninvariant manifolds in question correspond to the number of effective units of the\nnetwork. In order for saddle-to-saddle dynamics to emerge, the authors argue\nthat initialization must be close to an invariant manifold on which zero loss is\nunattainable and that the escape path from saddles closely follows invariant\nmanifolds.\n\nConcretely, a result on the embedding of local minima in smaller networks\nas saddles within larger networks is proven, extending [7]. Theorem 3 provides\na group of properties invariant under gradient flow. The theoretical results\non saddle-to-saddle dynamics focus on two-layer linear and quadratic (in the\nweights) networks. Timescale separation is argued to occur in linear networks\ndue to the distribution of data and in quadratic networks (analysis restricted\nto scalar targets) due to the initialization. Based on this characterization of\nthe invariant manifolds visited during the training, the authors provide some\nconjectures on learning dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The structure of the paper is good and related literature is thoroughly documented. Equations (6) and (7) in Theorem 1 provide new examples of embedded fixed points which may be of independent interest. Theorem 3 shows\nthat the properties of units being zero, have equal weights, have proportional\nweights in the case of $\\phi$ homogeneous in $u$, or have linearly dependent weights\nin the case of $\\phi$ linear in $u$ are preserved under gradient flow. I found these\nresults to be interesting. In Appendix F.1 it is shown that on such invariant\nmanifolds the network may be represented with one with fewer units. Therefore saddle-to-saddle dynamics between such manifolds could indicate learning\nsequentially more complex networks. The proofs of these results are clear and I\ncan find no issue with them. Theorem 4 and Proposition 5 are presented clearly\nand their proofs (if one accepts the setup) seem to be correct and concise, with\nobservations on different drivers of saddle-to-saddle dynamics in the two considered settings. Proposition 5 seems to be both novel and interesting. The\nmotivation for scaling up width in transformers is a good application and experiments align with the theory. More generally, claims are supported with well\npresented and relevant experiments. The authors provide conjectures on when\nsaddle-to-saddle dynamics fail to emerge and broader settings exhibiting permutation symmetry where saddle-to-saddle dynamics may emerge. That some\nfuture research directions are motivated by this work is positive."}, "weaknesses": {"value": "I am concerned that Theorem 4 is not novel. As the authors note in Appendix\nA.1, the incremental learning dynamics of linear networks are quite well understood. [12] and [8] seem to cover the same ground as Theorem 4, and closely related work includes [1, 9, 10, 6, 13]. Could the authors be more explicit in\nexplaining the novelty of this theorem?\n\nIt is noted that one of the conditions for saddle-to-saddle dynamics is that\nescape paths closely follow invariant manifolds. The authors do not discuss at\nlength when this may be satisfied or violated. As this is posited as key for\nthe emergence of such dynamics the paper would be improved with some more\nexamples and heuristic commentary, even if a general theoretical treatment of\nthis condition is infeasible.\n\nThe procedure of linearizing about fixed points and studying this dynamical\nsystem is not valid once some of the weights have begun to move away from the\ncritical point. Questions of rigour notwithstanding, the theoretical setting is\nquite restrictive in that it only considers networks of depth 2 and with no non-linear activation. This setting is arguably more restrictive than, for example, [2], although I appreciate that a diagonal parametrisation is used here (as an\naside the authors might like to consider including this reference?). The authors\nsuggest why tanh networks fail to exhibit saddle-to-saddle dynamics, but a\nwider discussion of how the results apply to commonly used activations (e.g.\nleaky ReLU) would strengthen the paper. Similarly, a discussion of the role\nof depth would make the paper more relevant. It seems to me that Appendix\nB illustrates that saddle-to-saddle dynamics can still occur in deeper networks\nwith non-linear activations, but without much comment on differences with the\nlinear two-layer case (or justification if there is no difference)."}, "questions": {"value": "The claim that the Theorems 1 and 3 together imply saddle-to-saddle dynamics\nof increasing network complexity seems quite strong. On this point I have the\nfollowing four questions:\n\n1) Can the authors justify that there do not exist many other invariant\nmanifolds under gradient flow? If others do exist, why could the dynamics not\ntake us between these manifolds?\n\n2) Can the authors more clearly justify the statement ”invariant manifolds\nindicate that there exist gradient flow paths connecting pairs of embedded fixed\npoints defined in Theorem 1”? Unless I’m missing something simple this may\nneed a reference/proof/discussion.\n\n3) Can the authors justify that the learning between saddles is incremental?\nIt is not apparent to me that one could not ’skip’ several stages of complexity,\nor that (noting the numerical errors and stochastic effects in practice) in\ngeneral some perturbations may not push the flow away from following saddle-to-saddle dynamics.\n\n4) Could the authors explain how their theory interacts with times when\none does not escape a simple representation, sometimes at the cost of failing to\nminimize the loss function. Papers I have in mind here are [4, 3].\n\nI then have three more points:\n\n5) In 300 ”Subsequent iterations of saddle-to-saddle dynamics operate similarly.” Providing a full account of visited saddles in the case of a diagonal\nnetwork [11] shows that the saddles one visits in this setting follow iterates\nof AISSM [5], and it is non-trivial to prove. Is this setting simpler owing to\nthe comments in paragraph 4 of A.1 noting ”Linear networks with scalar input\nor scalar output do not have the embedded fixed points described by Theorem\n1 except the zero fixed point”? Otherwise it seems like there might still be more to say.\n\n6) Could the authors briefly explain how [14] differs in setup to allow saddle-to-saddle dynamics, as compared with comments in Sect 7?\n\n7) Typo: 311: follow\n\nReferences:\n\n[1] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. NeurIPS 2019.\n\n[2] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and\nJoshua Susskind. Transformers learn through gradual rank increase, NeurIPS 2023.\n\n[3] Etienne Boursier and Nicolas Flammarion. Early alignment in two-layer\nnetworks training is a two-edged sword. Journal of Machine Learning Research, 26(183):1-75, 2025.\n\n[4] Etienne Boursier and Nicolas Flammarion. Simplicity bias and optimization\nthreshold in two-layer ReLU networks. ICML 2025.\n\n[5] Martin Burger, Michael Moller, Martin Benning, and Stanley Osher. An\nadaptive inverse scale space method for compressed sensing. Mathematics\nof Computation, 82(281):269-299, 2013.\n\n[6] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic\nStephan. How two-layer neural networks learn, one (giant) step at a time. Journal of Machine Learning Research 25:1-165, 2024.\n\n[7] K Fukumizu and S Amari. Local minima and plateaus in hierarchical\nstructures of multilayer perceptrons. Neural networks, 13(3):317-327, 2000.\n\n[8] Gauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in deep linear neural networks. NeurIPS 2019.\n\n[9] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit\nbias of gradient descent on linear convolutional networks. NeurIPS 2018.\n\n[10] Michael Kleinman, Alessandro Achille, and Stefano Soatto. Critical\nlearning periods emerge even in deep linear networks. ICLR 2024.\n\n[11] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. NeurIPS 2023.\n\n[12] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions\nto the nonlinear dynamics of learning in deep linear neural networks. ICLR 2014.\n\n[13] Zhenfeng Tu, Santiago Tomas Aranguri Diaz, and Arthur Jacot. Mixed dynamics in linear networks: Unifying the lazy and active regimes. NeurIPS 2024.\n\n[14] Yaoyu Zhang, Zhongwang Zhang, Tao Luo, and Zhiqin J Xu. Embedding principle of loss landscape of deep neural networks. NeurIPS 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b9d8uFGbDZ", "forum": "Vit5M0G5Gb", "replyto": "Vit5M0G5Gb", "signatures": ["ICLR.cc/2026/Conference/Submission9640/Reviewer_S6X7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9640/Reviewer_S6X7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505862113, "cdate": 1761505862113, "tmdate": 1762921170631, "mdate": 1762921170631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that saddle-to-saddle dynamics can induce an implicit bias toward simpler solutions across diverse architectures. Leveraging permutation symmetry, the authors show that a saddle point in a narrower architecture remains a saddle point when extended to a wider one, and they identify new conditions under which such saddles can exist in larger networks. Furthermore, the paper demonstrates that, under these conditions, the dynamics evolve within an invariant manifold, suggesting that saddle-to-saddle dynamics correspond to transitions between invariant manifolds via perturbations. \n\nMore specifically, the authors unify multiple architectures within a single framework and exploit the fact that permutation symmetry is shared across them. Their analysis of saddle-to-saddle dynamics also reveals a benefit of increasing the hidden dimension HHH in self-attention architectures—a contrast to the fully connected case—which they further validate empirically."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a unifying perspective on the widely assumed simplicity bias underlying saddle-to-saddle dynamics across various architectures and, importantly, elucidates its connection to permutation symmetry. This formalization and unification constitute the core contribution of the work.\n\nAlthough the analysis is based on a heuristic scenario, it is consistent with both experimental observations and existing understanding of saddle-to-saddle dynamics. The analysis further reveals an unexpected benefit of increasing $H$ in self-attention architectures.\n\nThe paper is clearly written, and the illustrative examples effectively convey the key ideas."}, "weaknesses": {"value": "While I fully agree with the reasoning presented in Section 5 and consider it a sufficient contribution, several steps remain heuristic. \nFor instance, the transition between invariant manifolds is not thoroughly explored: while the paper studies the existence of invariant manifolds and the dynamics within them, it offers limited explanation for why the dynamics should adhere to these manifolds in the first place. Proposition 5 states that if one variable takes a larger value, the others must be small, yet it does not clarify why one should dominate in the first place. The argument is intuitively consistent with the rich-get-richer dynamics of layerwise structures, but making this connection more explicit—especially in settings where many neurons can fit the target — has been a central focus of works such as Kunin et al. (2025), which the paper also cites. \n\nWhile I acknowledge that the analysis in Section 5 is restricted to two-layer networks, I believe the current analysis can be potentially misleading, particularly regarding Figures 2A and 2B and the conclusion that the saddle-to-saddle mechanism differ for different architectures. As the authors conjecture in line 354, I believe that the degree or effective depth — the number of times the learnable parameters are multiplied to achieve $f(x)$ — is the main cause of the sensitivity to $H$ and initialization: a 3-layer network with linear dependence on $\\phi$ would likely exhibit similar sensitivity to $H$ and initialization as the 2-layer self-attention networks.\n\nThe intuitive description of Proposition 5 (line 331) and the conjecture on depth (line 354) seem to suggest that the number of times learnable parameters are multiplied — the effective depth — is the key factor, and that the self-attention and quadratic networks simply obtain an additional effective depth from $\\phi \\propto u^2$. While I believe the analysis is correct for two-layer networks, the authors could improve clarity on what specifically causes the sensitivity to $H$ and initialization, especially as the authors already discuss the limitations of Section 5 regarding depth. Please see my question 2 for a verifiable experiment. \n\nThe empirical section could also be strengthened by providing result for all architectures (convolutional, attention, quadratic) in Figures 4, to make it consistent to Figure 1. This is important as the theory claims to unify dynamics across diverse deep architectures.  Additionally, one or two experiments on real dataset would significantly strengthen the paper."}, "questions": {"value": "## First Question\nDo the authors have corresponding results for all deep architectures in Figure 4?\n\n ## Second Question\nFrom the analysis related to Figures 2A and 2B, authors argue that the difference between linear attention and fully connected networks arise from the quadratic dependence of $\\phi(x, u)$ on $u$. However, I believe the observation of Figures 2A and 2B arise from the depth and initialization. \n\nCan the authors repeat the experiments of Fig 2 A and B for 3-layer diagonal-ish linear network \n\n$f(x) =  \\sum_j \\sum_{i=1}^Ha_{ij}b_{ij}c_{ij}x_j$ \n\nusing the same initialization as the self-attention network in the paper? Here, $a_{ij},b_{ij}, c_{ij} \\in \\mathbb{R}$ and $x_j$ are the entries of the input. The model, while similar to a quadratic network, does not have $u^2$ dependence. It only shares that there are three products of parameters ($abc$ instead of $vu^2$) to express $f$.\n\nGreater effective depth—i.e., more multiplicative parameters—typically increases sensitivity to initialization, as suggested by the more Heaviside-like saddles observed with larger parameter products (e.g., your Figure 4). Notably, the self-attention and quadratic networks use a larger initialization $\\epsilon=0.005$, compared to $\\epsilon=10^{-6}$ for fully connected and convolutional networks; I interpret this as a practical choice to escape a longer initial saddle plateau caused by more products of small initial weights (three for quadratic/attention vs. two for fully connected/convolution).\n\nWhen depth-induced sensitivity is combined with a larger (yet still “rich-regime”) initialization, increasing $H$ may simply allow the model to sample slightly larger initial effective parameters, thereby shortening the transient around the saddle. If a 3-layer diagonal-like network exhibits similar sensitivity to $H$ and initialization as observed in the self-attention networks, the analysis should emphasize the role of multiplicative parameterization rather than attributing the phenomenon primarily to the activation form only (i.e. a special case). If no such effect is observed, the current interpretation is convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9fbfp4taiB", "forum": "Vit5M0G5Gb", "replyto": "Vit5M0G5Gb", "signatures": ["ICLR.cc/2026/Conference/Submission9640/Reviewer_Peza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9640/Reviewer_Peza"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717989140, "cdate": 1761717989140, "tmdate": 1762921169821, "mdate": 1762921169821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unifying theoretical framework explaining simplicity bias in gradient descent dynamics through what the authors call saddle-to-saddle dynamics. The authors argue that across diverse architectures—fully connected, convolutional, ReLU, quadratic, and self-attention networks—training often progresses by traversing a sequence of saddles, each corresponding to increasingly complex solutions. The work generalizes earlier analyses of linear networks, introducing formal results on embedded fixed points and invariant manifolds to explain why networks successively recruit additional “effective units.” The paper also provides clear implications for how initialization, network width, and data distribution affect the number and duration of learning plateau."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a broad, architecture-agnostic framework that connects dynamical simplicity bias phenomena seen across many model types. The idea of interpreting learning stages as saddle-to-saddle transitions is elegant and theoretically grounded.  \n2. The analysis of fixed points, invariant manifolds, and timescale separation (linear vs. quadratic cases) is mathematically sound, and the simulations (Figure 1 & 2) convincingly demonstrate the predicted dynamics and validate the theory’s implications for initialization and data statistics.  \n3. Despite the depth of the mathematics, the exposition is clean, well-structured, and includes intuitive visualizations that aid understanding."}, "weaknesses": {"value": "1. The simulations, while well-chosen, are limited to small synthetic examples. It would strengthen the paper to show whether the predicted stage transitions are visible in real-world or larger-scale training runs.  \n2. Some proofs depend on idealized gradient flow dynamics and homogeneity assumptions that may not hold under practical stochastic optimization with large step size."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a6ol76OYsN", "forum": "Vit5M0G5Gb", "replyto": "Vit5M0G5Gb", "signatures": ["ICLR.cc/2026/Conference/Submission9640/Reviewer_9Lv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9640/Reviewer_9Lv2"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784989833, "cdate": 1761784989833, "tmdate": 1762921169421, "mdate": 1762921169421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}