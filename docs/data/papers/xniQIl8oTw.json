{"id": "xniQIl8oTw", "number": 11119, "cdate": 1758189711207, "mdate": 1763358703987, "content": {"title": "CORectifier: Hierarchical Trajectory Rectifications Boost Reinforcement Learning for Combinatorial Optimization", "abstract": "In the rapidly advancing field of Neural Combinatorial Optimization (NCO), Reinforcement Learning (RL) stands out for its ability to naturally enforce complex constraints. However, mainstream RL-based methods suffer from reward sparsity and sample inefficiency in the vast combinatorial action space, leading to ineffective training, subpar performance, and poor scalability. To address these challenges, we propose CORectifier, a novel NCO solver learned with hierarchical supervision incorporated to regularize the arbitrary RL exploration. Technically, we design a \"rectification\" mechanism for training: partial policy-predicted actions in a trajectory are probabilistically replaced with high-quality segments from reference solutions. Distinct from heatmap-guided Supervised Learning (SL) or vanilla Imitation Learning (IL) methods, CORectifier prompts the model with tri-level optimal signals, operating at the batch, instance, and sub-instance levels with fragments of random lengths injected at random decision steps. This Rectified RL (RRL) paradigm helps develop optimality-aware and sample-efficient RL learners while maintaining their sequential-decision manner for constraint satisfaction, delivering a new perspective to hybridize RL and SL/IL for solving CO problems with improved flexibility and utility of limited supervisory data. Empirical results on **TSP, ATSP, PCTSP, CVRP, and KP, across synthetic and real-world benchmarks**, validate the superior training efficacy, generalization and applicability of CORectifier. With a simplest greedy decoder, CORectifier achieves up to 59.7% and 26.5% performance gains over RL- and SL-based baselines, respectively, and reduces the performance gap on TSP-500 (sufficiently large for RL solvers) by up to 89.8%.", "tldr": "", "keywords": ["Neural Combinatorial Optimization", "Reinforcement Learning", "Imitation Learning", "Hierarchical Rectification"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/518452369cac4105c2c7a69d227b0e84250ee572.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CORectifier, a reinforcement-learning-based framework for Neural Combinatorial Optimization (NCO) that introduces hierarchical trajectory rectifications.\n\nThe key idea is to probabilistically replace partial segments of policy-generated trajectories with locally optimal fragments from expert solutions during training.\n\nThis “Rectified Reinforcement Learning” (RRL) paradigm aims to alleviate reward sparsity and poor sample efficiency in standard RL-based solvers while retaining the flexibility of sequential decision making.\nEmpirical results on TSP, ATSP, and PCTSP show significant improvements over prior RL-based baselines and competitive performance compared to supervised and unsupervised methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality** \n\n-\tThe paper introduces an interesting and nontrivial attempt to integrate expert information into RL via partial trajectory replacement, bridging imitation and RL in a structured way. Extensive results with ablations, comparison with recent neural solvers\n-\tThe three-level (batch, instance, sub-instance) rectification mechanism is well-motivated and could be a general recipe for hybrid learning in NCO.\n\n**Quality**\n\n-  common and consistent notation, good writing\n\n**Significance**\n\n-\tExtensive results with ablations, comparison with recent neural solvers (not only RL-based ones) which show substantial gains in solution quality\n-\tThe idea of reusing expert solutions as decomposable local fragments is promising, especially under limited supervision."}, "weaknesses": {"value": "**Limited applicability beyond simple TSP-like problems**\n\nThe proposed rectification mechanism fundamentally relies on being able to replace local segments of a trajectory while maintaining feasibility.  \nThis assumption breaks down for problems with richer constraints (e.g., scheduling with precedence relations, capacitated VRPs, PDPs), where feasibility depends on multi-dimensional states (capacity, time, precedence).  \nIn such settings, ```Feas(a*, s)``` would likely fail for most rectifications, making the method ineffective.\nExperiments are restricted to TSP variants, which all share the same single-tour structure.\n\n**Lacking clarity**\n\nThe paper would benefit from a more detailed figure describing the rectification process or a small toy example. All in all, The definition of the rectifier $\\mathcal{R}(\\tau, \\tau^\\ast, \\mathcal{M})$ seems underspecified (see questions). \n\n**Dependence on expert-labeled data**\n\nThe approach requires high-quality reference solutions for supervision. For larger or more complex problems, such data are difficult and expensive to obtain.\n\n**Scaling behavior**\n\nThe method underperforms on larger instances compared to recent heatmap-based approaches. The authors should explicitly discuss why this happens (e.g., policy entropy collapse, reduced rectification success) and suggest possible remedies.\n\n**Lack of detailed analysis of the rectifier’s behavior**\n\nThe paper would benefit from more in-depth analysis on the rectifier operation. E.g., showing the impact of rectification on the reward; analysing how often a rectification fails (possibly grouped by step t, as I reckon rectification in later stages is more difficult as most nodes are already visited)"}, "questions": {"value": "- How could CORectifier be applied to problems with more complex constraints, such as job-shop scheduling, capacitated VRPs, or pickup-and-delivery problems?  \nGiven that feasibility in these domains depends on non-local state variables, how would rectification be defined or enforced?\n\n- In Algorithm 1 and Eq. (10), the rectification operator replaces certain actions in a sampled trajectory $\\tau$ with successors of corresponding nodes in the expert trajectory $\\tau^\\ast$. However, after such a replacement, the subsequent actions in $\\tau$ were originally generated under a different state (i.e., before rectification)- Could the authors clarify, how exactly the remainder of the trajectory, i.e., $\\tau_{t+k+1:M}$ is constructed / restored to ensure feasibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n4N05UJUcX", "forum": "xniQIl8oTw", "replyto": "xniQIl8oTw", "signatures": ["ICLR.cc/2026/Conference/Submission11119/Reviewer_sQuT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11119/Reviewer_sQuT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760608366258, "cdate": 1760608366258, "tmdate": 1762922293205, "mdate": 1762922293205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CORectifier as a method to address the challenge in Neural Combinatorial Optimization (NCO), where it becomes increasingly difficult to explore better solutions as training progresses. The authors present an approach that refines the exploration results of reinforcement learning (RL) using optimal solutions, and then utilizes the refined trajectories for further model training.\n\nSpecifically, the proposed training process operates as follows:\nFirst, a batch is constructed by mixing problems with and without known optimal solutions. Then, among the multiple trajectories explored in parallel, a subset is selected. The selected trajectories are subsequently improved using the optimal solutions and masked rectifiers, and the improved trajectories are used to train the model.\n\nExperiments are conducted on TSP, ATSP, and PCTSP, and the proposed method is compared with various heatmap-based and sequential decision methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a novel training approach that refines parallelly generated trajectories using optimal solutions.\n\n- The idea of leveraging optimal solution information to improve the quality of RL-based exploration is innovative and holds potential to enhance both training stability and convergence speed."}, "weaknesses": {"value": "- The proposed method can only be applied when an oracle solver is available, since it relies on optimal solutions during training. However, if such a solver exists, the need for Neural Combinatorial Optimization (NCO) diminishes; conversely, when no oracle solver is available, the proposed approach cannot be applied.\n\n- The method appears to be limited to TSP-type problems (i.e., problems involving Hamiltonian path finding) and may not generalize well to other types of combinatorial optimization tasks.\n\n- In the experimental results, the method shows inferior performance compared to heatmap-guided methods, particularly as the problem size increases. Moreover, in the comparison with sequential decision methods, there are considerable discrepancies between the reported results in this paper (Table 1 and Table 2) and those presented in the original papers (e.g., TSP-POMO, ATSP-GOAL)."}, "questions": {"value": "- Is the RRL Loss presented in Equations (11) and (12) different from the Loss function used in POMO [28]? If so, what are the key differences?\n\n- In Section 3.2.2 (2), is there a specific strategy for selecting the subset $\\mathcal{\\widehat{T_i}}$? Is it chosen randomly, or by some defined criterion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lQSOCScZas", "forum": "xniQIl8oTw", "replyto": "xniQIl8oTw", "signatures": ["ICLR.cc/2026/Conference/Submission11119/Reviewer_1exv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11119/Reviewer_1exv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461174535, "cdate": 1761461174535, "tmdate": 1762922292669, "mdate": 1762922292669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CORectifier, an extension of reinforcement learning (RL) methods for solving combinatorial optimization problems (COPs). The key idea is to incorporate expert solutions (reference trajectories from oracle solvers) into the RL training process in a systematic way. Specifically, the proposed Rectified Reinforcement Learning (RRL) framework probabilistically replaces parts of the model’s predicted trajectories with expert segments, enabling the training procedure to benefit from both reinforcement learning (exploration-based optimization) and supervised/imitation learning (expert guidance). The method introduces a hierarchical rectification mechanism operating at batch, instance, and sub-instance levels, effectively balancing exploration and guidance. Empirical results on several COP benchmarks (TSP, ATSP, and PCTSP) show that CORectifier consistently outperforms prior RL- and SL-based baselines, achieving improved sample efficiency, stability, and scalability."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is mathematically well-structured and takes care to clearly define all variables and formulations, which improves readability and rigor.\n\nThe proposed idea of combining supervised (imitation) and reinforcement learning is conceptually sound and aligns with recent efforts to improve sample efficiency and stability in neural combinatorial optimization.\n\nThe method has practical appeal, as many real-world optimization settings can provide partial or full expert solutions that could be leveraged in a similar rectification manner."}, "weaknesses": {"value": "1)\nThe applicability of the proposed approach appears fundamentally limited to relatively simple combinatorial optimization problems such as TSP and ATSP, which involve minimal or well-structured constraints. The method’s core mechanism—replacing segments of a policy-generated trajectory with expert subsequences—implicitly assumes that any inserted segment remains feasible within the overall solution. However, in more realistic or constraint-heavy problems (e.g., CVRP, VRPTW, scheduling or assignment tasks with capacity or time-window constraints), such partial replacements can easily violate global feasibility, leading to invalid solutions or an extremely low feasible-sampling rate during training. This structural fragility makes the method difficult to extend beyond toy-like routing benchmarks. The paper does not discuss strategies for preserving feasibility under complex constraints, nor does it include experiments on problems with non-trivial feasibility conditions. As a result, the proposed approach seems best suited for simplified academic benchmarks rather than real-world CO applications.\n\n2)\nThe paper would benefit from a stronger empirical validation of its claimed ability to leverage expert data. For instance, an ablation experiment varying the number of expert trajectories (e.g., 1K, 10K, 100K) could help demonstrate whether the model genuinely improves as more expert supervision becomes available. Such an analysis would clarify whether the reported gains stem from the proposed rectified learning principle or from engineering-heavy interventions such as dynamic scheduling, hyperparameter tuning, and imitation-based warm-up. In its current form, it remains unclear how much of the observed performance improvement can be attributed to the central idea of combining RL and SL, rather than to these auxiliary mechanisms."}, "questions": {"value": "Line 080: in analogy to the language models when the next token grow increasingly distant from the initial state, leading to unstable and inaccurate predictions.\nThe sentence could be rephrased for clarity. I cannot understand what it means."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ghPXjilxGp", "forum": "xniQIl8oTw", "replyto": "xniQIl8oTw", "signatures": ["ICLR.cc/2026/Conference/Submission11119/Reviewer_xVoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11119/Reviewer_xVoR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840945606, "cdate": 1761840945606, "tmdate": 1762922292206, "mdate": 1762922292206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CORectifier, a rectified reinforcement learning (RRL) framework that integrates hierarchical supervision into reinforcement learning to improve sample efficiency and reward sparsity in neural combinatorial optimization (NCO). The proposed method regularizes the exploration process by probabilistically replacing partial policy-generated trajectories with high-quality segments from expert solutions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method has good generality and can be adapted to multiple NCO approaches.\n2. The core idea is simple and easy to understand, integrating expert-guided rectification into RL in a straightforward way.\n3. The experimental section is rich, covering multiple tasks and baselines."}, "weaknesses": {"value": "1. The motivation is not entirely clear. Although the paper points out three limitations of existing RL-based NCO methods, it does not clearly explain how the proposed method effectively addresses each of them.\n2. The advantage of the proposed way to combine RL and imitation learning (IL) is not deeply analyzed. It is intuitive that integrating IL can improve RL, but it is unclear whether the proposed RRL provides substantial benefits over simple integration schemes such as first-IL-then-RL two-stage training or approaches like [1] that use local search to refine sampled rollouts as expert demonstrations during training.\n3. During trajectory rectification, feasibility checks are applied, so the actual number of replaced segments is unknown. An analysis of this aspect would help understand the mechanism behind the improvement.\n4. The backbone models used for each problem type are not clearly stated in the main text.\n5. The proposed framework introduces many hyperparameters, raising concerns about potential tuning difficulty and task-specific parameter dependency.\n6. The hyperparameter study lacks convincing justification. For instance, the choice of 0.1 for $p_{batch}$, $p_{inst}$, $\\alpha$ and $\\beta$ seems arbitrary, and it is unclear how performance changes with smaller values.\n7. The technical presentation of the trajectory rectification process (involving masks and mathematical notation) is somewhat difficult to follow, which may hinder understanding of the implementation details.\n8. The reference formatting does not fully conform to the ICLR citation style.\n\n[1] Preference optimization for combinatorial optimization problems, ICML 2025."}, "questions": {"value": "1. Are the three hierarchical levels (batch, instance, intra-instance) truly complementary? Could the authors provide ablation results for enabling/disabling each level independently?\n2. Is the proposed approach applicable to non-routing combinatorial optimization problems?\n3. The definition of $M$ as problem size seems inconsistent. In the Intra-instance Level section, $M$ sometimes appears to represent the length of the decision sequence. Could the authors clarify this definition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8UUW2Ag7QQ", "forum": "xniQIl8oTw", "replyto": "xniQIl8oTw", "signatures": ["ICLR.cc/2026/Conference/Submission11119/Reviewer_UfUe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11119/Reviewer_UfUe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841808943, "cdate": 1761841808943, "tmdate": 1762922291752, "mdate": 1762922291752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}