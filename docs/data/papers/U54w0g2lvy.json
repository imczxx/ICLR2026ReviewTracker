{"id": "U54w0g2lvy", "number": 15972, "cdate": 1758257916104, "mdate": 1763749929809, "content": {"title": "Large EEG-U-Transformer for Time-Step-Level Detection Without Pre-Training", "abstract": "Electroencephalography (EEG) reflects the brain's functional state, making it a crucial tool for diverse detection applications, including event-centric analysis like seizure detection and status-centric analysis like pathological detection. While deep learning-based approaches have recently shown promise for automated detection, traditional models are often constrained by limited learnable parameters and only achieve modest performance. In contrast, large foundation models showed improved capabilities by scaling up the model size, but required extensive time-consuming pre-training. Moreover, both types of existing methods focus on window-level classification, which requires redundant post-processing pipelines for event-centric tasks. In this work, based on the multi-scale nature of EEG events, we propose a simple U-shaped model to efficiently learn representations by capturing both local and global features using convolution and self-attentive modules for sequence-to-sequence modeling. Compared to other window-level classification models, our method directly outputs predictions at the time-step level, eliminating redundant overlapping inferences. Beyond sequence-to-sequence modeling, the architecture naturally extends to window-level classification by incorporating an attention-pooling layer. Such a paradigm shift and model design demonstrated promising efficiency improvement, cross-subject generalization, and state-of-the-art performance in various time-step and window-level classification tasks in the experiment. More impressively, our model showed the capability to be scaled up to the same level as existing large foundation models that have been extensively pre-trained over diverse datasets and outperforms them by solely using the downstream fine-tuning dataset.", "tldr": "", "keywords": ["EEG", "AI for Science", "Deep Learning", "Seizure Detection"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/013c46b60586dc67f863b9c75ef5b72e2a7449f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EEG-U-Transformer, a U-shaped architecture that combines convolutional encoders, ResCNN stacks, and transformer modules for time-step-level EEG analysis. The model aims to eliminate sliding window-based post-processing and pre-training requirements, achieving competitive results on seizure detection, sleep staging, and pathological detection tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The sequence-to-sequence design effectively bypasses redundant window-level inference, simplifying pipelines for event-centric tasks like seizure detection.\n\nThe model scales competitively with large pre-trained foundation models while relying solely on downstream data, reducing computational costs."}, "weaknesses": {"value": "The emphasis on \"no pre-training\" as a virtue seems misleading; large-scale pre-training learns generalizable representations that often enhance performance and robustness when fine-tuned to specific tasks. By forgoing this, the model may underutilize broad EEG patterns, limiting its adaptability to diverse domains or low-data scenarios. Please note that the data output by different EEG devices can vary significantly, and strong general knowledge is a necessary tool to bridge this gap in clinical practice.\n\nCross-dataset results (Table 6) reveal significant performance drops (e.g., ~34% F1-score decrease on SeizeIT1), indicating sensitivity to domain shifts in electrodes, demographics, or devices.\n\nThe post-processing pipeline (thresholding, morphological operations) remains heuristic and dataset-dependent, undermining the end-to-end promise.\n\nHyperparameter sensitivity (e.g., kernel sizes, transformer layers) is underexplored, raising reproducibility concerns without meticulous tuning.\n\nComputational analysis focuses on inference time but omits training costs and memory footprint, especially for long sequences (e.g., T=15360).\n\nGlobal-local interaction and fusion have been extensively explored in numerous previous works, encompassing both the general visual recognition and EEG analysis domains, e.g, [1-4]. Consequently, this cannot be considered a significant contribution of this paper. Furthermore, I did not observe any relevant discussions about these works.\n\nRefs:   \n[1] On the integration of self-attention and convolution. Arxiv '23.  \n[2] TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition. Arxiv '23.  \n[3] Multiscale global prompt transformer for EEG-based driver fatigue recognition. TASE'24.  \n[4] Learning robust global-local representation from EEG for neural epilepsy detection. TAI '24."}, "questions": {"value": "My questions have been listed in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TaeU5NW8xz", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Reviewer_Zb9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Reviewer_Zb9o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729881723, "cdate": 1761729881723, "tmdate": 1762926184027, "mdate": 1762926184027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new architecture for an EEG-based application. This model uses a U-Net architecture with Transformer layers. This allows to have more generalizable networks. The model is then tested on three different modalities: seizure detection, sleep staging, and abnormality detection."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a new architecture that leverages the power of U-Net with the generalization power of the transformer between the encoder and the decoder.\n- This model offers versatility in the application for both Windows-based event and segmentation tasks.\n- This new model is applied to three different modalities for which the model outperforms other models, except for the TUAB dataset, where the results are worse with their model.\n- In addition, the model is faster to run compared to other models."}, "weaknesses": {"value": "If the paper is proposing a new model that works well on several modalities, I think that the paper lacks clarity in some points:\n- In Figure 1, it is not so clear that the arrows represent layers, especially when scaling embeddings, and Figure 3 represents layers by blocks.\n- In the same Figure, it will be clearer to use a letter instead of a number (example: 15360 -> T and then T/2 ...)\n- In Figure 2, the windowing of the signal is exactly the same between the two examples. Since the strength of the method is to be able to cut the windows as we want, it could be interesting to see the possible difference.\n- The organization of the paper is hard to follow sometimes. Part 2.3 gives information on the pre-processing of the dataset, but Part 2 focuses on the architecture of the model. I would suggest moving it to part 3.1 or at least to the end of part 2.\n- This claim: \"Such experimental settings and\nevaluation metrics do not fit with real-world requirements and often limit the model design, as\ndifferent model architectures might benefit from different sequence lengths,\" is not true for all tasks. In sleep staging, for example, all the datasets are annotated every 30 seconds. Adding a reference can give more strength to the claim that is central to the motivation of the paper.\n\nMinor: \n- add number of subjects in Table 1\n- For the TUAB dataset, the claim that you are in the top tier of the AUROC score and \"marginally\" lower than BIOT is too strong. Your method is losing 3% compared to Biot, which is the improvement that you have on sleed-EDFx.\n- Several typos were seen in the paper.\n\nIn my opinion, this paper has good propositions and results, but the lack of clarity makes it hard to follow."}, "questions": {"value": "- You are categorizing the datasets into no-activity, full-activity, and partial-activity. Is it something usual for one of the modalities? Did you do that on every dataset? \n- On which dataset was the ablation study done?\n- In sleep staging, the models are usually using sequences of windows. For example, in the U-Sleep paper, they are using 35 windows as input. This is giving more context to the models. Is it something doable with your method? For sleep, for example, could we pass a longer time length than 30 seconds to get multiple outputs at the end?\n- For the time comparison (Table 3), why are only 3 competitors given? Knowing that you run several models, it would be easy to give every running time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ePLzh9pcUu", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Reviewer_bx19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Reviewer_bx19"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735488130, "cdate": 1761735488130, "tmdate": 1762926183331, "mdate": 1762926183331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Large EEG-U-Transformer (EEG-U-T), a sequence-level model designed to perform end-to-end EEG time-step detection without pre-training. The authors argue that existing EEG foundation models (e.g., BIOT, LaBraM, EEGPT) rely on fixed window segmentation and heavy pre-training, leading to redundant processing and computational overhead. To address this, they introduce a U-shaped CNN–Transformer encoder–decoder that directly models long EEG sequences in a sequence-to-sequence manner, aiming to capture both local and global temporal dependencies. The model is evaluated on three datasets (TUSZ, TUAB, Sleep-EDF) across seizure detection, pathological EEG classification, and sleep staging tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and timely problem: improving EEG temporal modeling beyond single-epoch representations. The paper is easy to follow with a clearly described architecture."}, "weaknesses": {"value": "IMO the paper has a mispositioned motivation and runs the risk of overclaiming. It mischaracterizes the limitations of existing foundation models such as BIOT, LaBraM, and EEGPT. Those models are not designed for sequence-to-sequence detection but focus on learning single-epoch representations and channel adaptation, which are foundational rather than temporal tasks. The proposed model instead targets continuous sequence labeling—a fundamentally different objective. Moreover, the claim that existing foundation models suffer from “window segmentation limitations” is inaccurate and overstates their weaknesses. Windowing is a task-dependent design choice, not a methodological flaw.\n\nMoreover, I am afraid the novelty is limited in that the proposed method is essentially a sequence wrapper over existing epoch-based models. The proposed EEG-U-Transformer can be viewed as stacking a CNN–Transformer encoder-decoder to model long-range temporal dependencies. Conceptually, this is a sequence-level wrapper built upon the same features that existing single-epoch models already extract effectively. It would be straightforward to use features from published foundation models (BIOT, LaBraM, EEGPT) and train a small seq-to-seq model for fine-tuning. Hence, the contribution lies more in architectural repackaging than in a novel modeling principle or learning paradigm.\n\n\nThe authors claim that existing foundation models require “diverse datasets and tremendous computation resources.” However, the proposed EEG-U-T introduces much larger model size (Table 5) and a heavier training process, contradicting the stated motivation of efficiency. While parameter counts increase significantly, the AUROC actually drops below BIOT on TUAB. Table 3 compares the proposed model’s runtime against task-specific baselines. However, the proposed method is presented as a foundation-style model, which in realistic usage would employ only the final-layer embeddings for inference. Thus, comparing full training runtime across architectures with different usage paradigms is not meaningful. A fair comparison would include inference-only latency and pre-training vs. fine-tuning cost under identical settings.\n\nRegarding experiments, the setup lacks uniformity and rigor:\n- Different baselines are used across datasets (TUSZ vs. TUAB vs. Sleep-EDF), making comparisons non-standardized.\n- EDF is a very small dataset—training a large model on such a dataset raises questions about overfitting and necessity.\n- TUAB results differ from reported metrics in prior papers, yet no explanation or setting alignment is provided.\n- No parameter sensitivity, ablation, or standard deviation analysis is presented.\nOverall, the empirical evidence is not strong enough to support the claimed generalization or efficiency improvements.\n\nI should also point out that the contribution is incremental: recent research on EEG foundation models have already explored representation learning, cross-channel dynamics, and scaling. This paper does not introduce a fundamentally new modeling principle—it mainly combines known CNN–Transformer and U-Net design patterns for sequence labeling. The contribution feels incremental, and the conceptual advancement over existing frameworks remains limited."}, "questions": {"value": "please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J1VocwgqR6", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Reviewer_qfhj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Reviewer_qfhj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800633455, "cdate": 1761800633455, "tmdate": 1762926182627, "mdate": 1762926182627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"Large EEG-U-Transformer,\" a hybrid U-Net and Transformer architecture. It is designed for efficient, time-step level  event detection. The paper's central and most important claim is that this model, trained only on downstream datasets, can outperform large, pre-trained foundation models (LFMs) like EEGPT and BIOT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1/ The core result—that a 6.1M parameter model trained from scratch can beat a 25M pre-trained LFM (EEGPT) on Sleep-EDFx.\nStrong Empirical Performance: The model achieves convincing SOTA results on the TUSZ seizure task (beating DeepSOZ-HEM)。and the Sleep-EDFx task. And the algorithm is of high efficiency compared with prior arts."}, "weaknesses": {"value": "The claim in Appendix A that combining U-Nets and Transformers is novel for biomedical signals is false. This concept is foundational in medical imaging (e.g., UNETR, Swin-Unet)  and exists in time-series (Yformer). The authors also failed to cite highly relevant prior work using attention-gated U-Nets for the exact same task (Chatzichristos et al. 2020).\n\nThe paper's scaling study (Tables 9 & 10) is a weakness, not a strength. Performance peaks at 59.9M parameters and then drops significantly at 80.9M . This contradicts the scaling laws that underpin LFMs and is poorly explained."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4fG6Dh8Szg", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Reviewer_btKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Reviewer_btKw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965627634, "cdate": 1761965627634, "tmdate": 1762926181897, "mdate": 1762926181897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper couples a U-Net style temporal convolutional backbone with a Transformer encoder to build a sequence-to-sequence model for per-time-step EEG labeling, and then uses attention pooling to convert those features into window-level predictions for tasks like seizure detection and sleep/abnormal screening. On TUSZ it reports solid event-level F1 and fast inference, but the method and evaluation largely combine standard components, so originality is limited.\n\nThe major contributions include: \n1. A unified EEG framework that performs time-step segmentation and window-level classification in one model, using U-Net temporal features, Transformer encoding, and attention pooling to bridge granular and aggregate predictions.\n2 An efficient inference and post-processing pipeline that achieves competitive event-level performance on TUSZ while keeping runtime low, demonstrating practical viability for long recordings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: One backbone handles both per-time-step labeling and window classification via attention pooling.\nQuality: Clinically aligned time-step and event-level metrics plus simple post-processing reduce false alarms.\nClarity: Figures and equations clearly explain the architecture and attention pooling.\nSignificance: Competitive TUSZ event-level F1 with fast inference shows practical value on long recordings."}, "weaknesses": {"value": "1. Limitted novelty: U-Net–style temporal conv plus a Transformer encoder is already common in time-series/biomed (see recent EEG/ECG segmentation hybrids); the paper does not show why this variant is fundamentally better than a strong pure-Transformer or pure-UNet baseline under identical settings. \n\n2 I am worry that authors made several Unfair or under-specified comparisons. Several baselines use different window lengths, channels, or preprocessing, so current tables cannot isolate gains from the proposed model rather than from setup differences; a “same data, same window, same channels, same hardware” table is missing. \n\n3 Mathematical/formulation glitches. The time-step loss mixes indices and does not clearly sum over dataset/time; positional encoding uses a nonstandard denominator likely to be a typo, which hurts clarity and reproducibility. \n\n4 Event-level evaluation is too forgiving. Results rely on tolerant matching and post-processing (morphological ops, min-duration) without reporting FP/h, onset/offset error, or sensitivity to threshold, so it is unclear whether the method is robust in stricter clinical regimes. \n\nAdditionally, figures/tables not presentation-ready. Table 1 mixes dataset stats with model configs, and several figures lack legend/abbreviation expansion, making it hard to verify the pipeline or reproduce it."}, "questions": {"value": "1 You use 10000^(2i/Td) rather than the standard 10000^(2i/dmodel). Is this intentional? Please justify the choice and provide an ablation/sensitivity study versus the standard form.\n2 Can you re-train and re-evaluate all baselines and your model under identical settings (same channels, window length, preprocessing, hardware, and batch size), reporting mean ± 95% CI over multiple seeds?\n3 Current results rely on tolerant matching and post-processing. Please report FP/h, onset/offset error distributions, and FROC, include tolerance/threshold sweeps, and ablate the morphological filtering and minimum-duration pruning to quantify their contribution.\n4 could you provide Leakage control with overlapping windows? It would be fair if authors could precisely document train/val/test split policy (file/patient level) and show that overlapping windows do not cross splits. Provide a sensitivity analysis to different overlap ratios to rule out temporal leakage."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Privacy and safety: Provide IRB/DUA IDs for each dataset, confirm complete PHI removal and redistribution rights, and add a clear “not for clinical use” disclaimer in the release.\n\nResponsible research practice: List dataset versions and approvals, state whether new labels were created with annotator qualifications and compensation, and prove patient-level isolation across train/val/test splits.\n\nFairness: Report subgroup performance by age, sex, site/device, and comorbidities with confidence intervals to detect and quantify disparate error rates."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSioAbDSv3", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Reviewer_UH5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Reviewer_UH5h"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985848876, "cdate": 1761985848876, "tmdate": 1762926180546, "mdate": 1762926180546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response - Novelty Clarification"}, "comment": {"value": "We want to humbly acknowledge that the modules that we used, such as UNet and Transformer, are well-established techniques, and we have listed some related work. **However,  the proposed model, EEG-U-Transformer, is merely a side contribution of this paper. Our main novelty lies in the time-step level representation learning framework and its adaptivity to maintain strong performance in window-level classification tasks.** \n\n**By proposing such a framework, we further propose the EEG-U-Transformer to be integrated into this framework. We show the design philosophy of this model in the general response of `Model Design Justification` to show that the EEG-U-Transformer is suitable for our framework.**\n\n## Unified EEG Analysis Framework\nFirstly, in this paper, we pointed out that most existing work focuses on window-level representation learning, which, when it comes to downstream tasks, requires redundant overlapping inference for event-centric tasks. Starting from this motivation, **we go beyond window-level representation and propose a training/inference framework to do sequence-to-sequence modeling**. We show that such a method eliminates the overlapping requirement and achieves:\n\n* a smaller time complexity(Appendix F)   \n* significant(10-fold) inference efficiency improvement in the simulated experiments,   \n* where our model can process a 1-hour EEG recording in just 3 seconds.\n\nMoreover, beyond handling time-step level classification for event-centric tasks, we integrate an attention-pooling layer to maintain the model’s ability to do window-level classification for status-centric tasks. **Such a combination makes our method a unified solution for EEG analysis that can handle both event-centric(time-step level) and status-centric(window level) tasks**. And our experimental results showed that our model achieves the state-of-the-art in both types of tasks.\n\n## Model\nLast but not least, we propose a simple model architecture that is fit with this unified framework and integrate it into our framework. Although the building blocks are well-established techniques, we justify our design choice in the general response of `Model Design Justification` to show the intuition of employing each module. **As a result of such a design, our method achieves the state-of-the-art performance, which is impactful to the community**. \n\n## Experiments Insights\nBeyond justifying that our methodology solved the aforementioned challenges and demonstrating the strongest performance, as discussed in the paper’s `Section 4`, **we argue that our experimental results revealed a meaningful insight that a strong performance can be achieved through a well-designed, simple architecture without reliance on complex pre-training or massive data resources in the EEG analysis area**. Such results lead to an under-explored question: **whether the pre-training process can exert its power in EEG analysis, especially considering the current constrained data collections**. \n\nUnderstanding this is important because it challenges a common assumption, derived from the success of other fields like NLP that are significantly different from EEG analysis, that “pre-training is always beneficial” and instead invites the community to re-examine when and how large-scale representation learning should be deployed in biosignal domains. If pre-training cannot exert its full power in the EEG area, then overly complex pre-training pipelines may provide diminishing returns relative to carefully engineered architectures directly optimized for accurate and robust inference.\n\nWe summarize our clarified novelty at the end of the introduction section of the paper."}}, "id": "fdOIdJYvKf", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763731462846, "cdate": 1763731462846, "tmdate": 1763731683961, "mdate": 1763731683961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response - Model Design Justification"}, "comment": {"value": "We list the following points to show that **it is a natural idea to design such architecture that is perfectly adaptive to our framework** and to answer the reviewer UH5h’s question: “**Why is this variant fundamentally better than pure-Transformer or pure-UNet?**”:\n\n* We first report the results of our pre-empirical analysis in `Table 1`, where we attempted to deploy a pure Transformer model in our framework. **The pure-Transformer model has achieved only modest performance in every sequence length we tested(10s, 30s, 60s).**   \n* This is intuitively reasonable as the semantic information of time series is mainly hidden in the temporal variation, for which the self-attention model cannot effectively extract. There is work that has empirically shown that convolution achieves better classification performance than Transformer \\[3\\].   \n* Moreover, as shown in `Table 1`, **the amount of memory grows with the sequence length in self-attentive models**. This is because in a self-attention model, for a sequence with length T, each token attends to every other token, which leads to an $O(T^2)$ attention matrix. **This is fatal as lots of target EEG events, such as seizure/epilepsy, span several minutes, which requires a large window size to do effective time-step level representation learning**.  \n* Because of these two factors, we choose to use convolution layers before the Transformer module to exploit local structures and to down-sample the given long sequence. After the transformer module, as we aim to do time-step level representation learning, we use transpose convolutions to reconstruct to original sequence length. Essentially, this gives a U-shaped neural network.\n\n* We also have intuitively discussed the drawbacks of a pure-UNet architecture in our paper’s `Section 1`: “**U-Net primarily operates within local receptive fields, making it difficult for U-Net to capture global features effectively. Beyond that, building up a U-Net requires stacking deeper layers, often leading to vanishing gradients and overfitting**”.  \n* We justified our intuitive assumption by reporting its performance in our paper’s ablation study in `Section 3.2`. As shown in `Figure 4`, **the vanilla U-Net has an underwhelming performance with a low AUROC mean**. **Integrating both the ResCNN and Transformer stacks produces not only a higher mean AUROC but also a reduced variance with fewer extreme false cases, indicating that these components complement each other effectively**.\n\n**Reviewer qfhj** also suggests directly using existing foundation models’ features. Here, **we tried integrating a pre-trained EEGPT into our framework to be a new baseline and report performance in `Table 2`, where our model outperformed EEGPT**. More information can be found in the response to Reviewer qfhj. \n\n| Sequence Length | Transformer |  | EEG-U-Transformer |  |\n| :---- | ----- | :---- | ----- | :---- |\n|  | Memory Usage(batch=4) | F1-score | Memory Usag(batch=4) | F1-score |\n| 10s | 1596.1 MB | 0.3503 | 367.0 MB | 0.5839 |\n| 30s | 4752.0 MB | 0.4375 | 653.2 MB | 0.6444 |\n| 60s | 9482.8 MB | 0.3738 | 1156.2 MB | 0.6713 |\n| 120s | 18945.9 MB | Out of Memory | 2077.6 MB | 0.6701 |\n\n**Table 1: Performance comparison between pure-transformer and EEG-U-Transformer in seizure detection task. The memory usage of pure-Transformer significantly increases as the sequence length increases, while the performance drops in longer sequence lengths.** \n\n| Model | Threshold | Event F1 | Sensitivity | Precision |\n| :---: | ----- | ----- | ----- | ----- |\n| EEGPT | 0.9 | 0.5324 | 0.7640 | 0.4085 |\n|  | 0.8 | 0.5014 | 0.8053 | 0.364 |\n|  | 0.7 | 0.4740 | 0.8201 | 0.3333 |\n| EEG-U-Transformer | 0.8 | 0.6713 | 0.7168 | 0.6312 |\n\n**Table 2: Performance comparison between pre-trained EEGPT and EEG-U-Transformer in the seizure detection task, where EEGPT shows a lower F1-score with low precision even under a 0.9 threshold.**"}}, "id": "H3dQVThdgx", "forum": "U54w0g2lvy", "replyto": "U54w0g2lvy", "signatures": ["ICLR.cc/2026/Conference/Submission15972/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15972/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission15972/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763731586436, "cdate": 1763731586436, "tmdate": 1763731586436, "mdate": 1763731586436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}