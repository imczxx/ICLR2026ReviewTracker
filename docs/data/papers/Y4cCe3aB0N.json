{"id": "Y4cCe3aB0N", "number": 15653, "cdate": 1758253616678, "mdate": 1763610196532, "content": {"title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables", "abstract": "With the widespread application of multimodal large language models in scientific intelligence, there is an urgent need for more challenging evaluation benchmarks to assess their ability to understand complex scientific data. Scientific tables, as core carriers of knowledge representation, combine text, symbols, and graphics, forming a typical multimodal reasoning scenario. However, existing benchmarks are mostly focused on general domains, failing to reflect the unique structural complexity and domain-specific semantics inherent in scientific research. Chemical tables are particularly representative: they intertwine structured variables such as reagents, conditions, and yields with visual symbols like molecular structures and chemical formulas, posing significant challenges to models in cross-modal alignment and semantic parsing.\nTo address this, we propose ChemTable—a large-scale benchmark of chemical tables constructed from real-world literature, containing expert-annotated cell layouts, logical structures, and domain-specific labels. It supports two core tasks: (1) table recognition (structure and content extraction); and (2) table understanding (descriptive and reasoning-based question answering). Evaluation on ChemTable shows that while mainstream multimodal models perform reasonably well in layout parsing, they still face significant limitations when handling critical elements such as molecular structures and symbolic conventions. Closed-source models lead overall but still fall short of human-level performance.\nThis work provides a realistic testing platform for evaluating scientific multimodal understanding, revealing the current bottlenecks in domain-specific reasoning and advancing the development of intelligent systems for scientific research.", "tldr": "", "keywords": ["Table Question Answering", "Table Recognition", "Scientific Document Understanding", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87208edeaffebe38bbad04458dac80200acffcd3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ChemTable, a large-scale benchmark for evaluating multimodal large language models (MLLMs) on chemical table recognition and understanding. The dataset includes 1,382 real-world chemical tables extracted from top-tier chemistry journals, annotated with structure, content, and over 9,000 QA pairs. The authors evaluate both open-source and proprietary MLLMs on tasks such as table structure extraction, molecular recognition, and reasoning over chemical data. The key claim is that current MLLMs fall short of human-level performance, especially in domain-specific reasoning and molecular structure understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents ChemTable, a carefully constructed and richly annotated benchmark focused on chemical table recognition and understanding, which fills a notable gap in the evaluation of multimodal large language models (MLLMs) on domain-specific scientific content. The dataset is derived from real-world chemistry literature and includes over 1,300 tables and nearly 10,000 question-answer pairs, supporting both structural extraction and reasoning tasks. The authors provide a comprehensive evaluation pipeline and assess a wide range of open-source and proprietary models, offering a useful reference for the community. The work is clearly written, well-organized."}, "weaknesses": {"value": "- The experimental conclusions lack depth and novelty. The finding that proprietary models outperform open-source ones is widely acknowledged and not specific to this domain. The paper does not provide detailed error analysis or insights into why models fail, making the conclusions too generic to guide future model development.\n\n- The paper does not sufficiently justify the necessity of a chemistry-specific benchmark in light of existing general-purpose benchmarks like MMMU or HLE, which also include chemistry-related content. There is no comparative analysis showing that ChemTable introduces uniquely challenging or uncovered tasks, weakening the motivation for a new dataset.\n\n- The evaluation metrics, while standard, are not diagnostic. High-level accuracy scores do not reveal whether models truly understand chemical content or are relying on superficial cues. There is no attempt to evaluate intermediate reasoning steps or semantic correctness, especially in molecular recognition and multi-hop reasoning tasks.\n\n- The paper lacks exploration of training strategies or model behavior. All models are evaluated in a zero-shot or frozen setting, with no investigation into whether domain-specific pretraining or fine-tuning improves performance. This limits the benchmark’s utility as a tool for driving model development rather than just evaluation.\n\n- The generalizability of the findings is not discussed. While the benchmark is chemistry-specific, there is no attempt to assess whether insights or improvements from ChemTable transfer to other scientific domains or table types, limiting its broader impact."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1bsA89vgKS", "forum": "Y4cCe3aB0N", "replyto": "Y4cCe3aB0N", "signatures": ["ICLR.cc/2026/Conference/Submission15653/Reviewer_zKp5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15653/Reviewer_zKp5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817766890, "cdate": 1761817766890, "tmdate": 1762925911549, "mdate": 1762925911549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ChemTable. It is a large-scale benchmark designed to evaluate the recognition and understanding abilities of MLLMs when interacting with real-world chemical tables. \nChemTable includes more than 1300 tables collected from top chemistry journals.\nEach table is enriched with pixel-level annotations, logical layout structures, and domain-specific labels.\nThe benchmark supports two major tasks: table recognition (structure parsing and content extraction) and table understanding (descriptive and reasoning QA).\nThe authors conduct extensive experiments on both proprietary and open-source MLLMs. They benchmark these models across a diverse set of tasks and compare their ability with human performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper makes a valuable contribution by introducing a benchmark on chemical tables, an area previously lacking multimodal datasets.\nUnlike prior table benchmarks (e.g., FinQA, SciTab, MMTab), ChemTable focuses on chemistry-specific content that includes molecular structures, chemical symbols, and experimental conditions.\n- The dataset contains over 1300 real-world chemistry tables and 9000 question–answer pairs, covering a wide range of tasks such as table recognition, structure parsing, and reasoning-based question answering. \nThe annotation process is comprehensive, including layout, text, molecular graphics, and metadata. \n- The authors conduct an extensive comparison across multiple leading open-source (e.g., Qwen-VL, Llama, InternVL) and proprietary models (e.g., GPT-4.1, Gemini-2.5-Flash). \nThis benchmarking provides a clear and balanced view of current model capabilities and limitations in chemical table understanding."}, "weaknesses": {"value": "- The core evaluation framework of ChemTable mainly adopts existing standard metrics such as TEDS, Edit Distance, and Accuracy. While these metrics are reliable, they are not tailored to capture the unique characteristics of chemical tables (e.g., molecular structures, symbolic notation, multimodal relationships). The evaluation approach appears to be a direct transfer from general table recognition tasks, lacking methodological innovation specific to the chemistry domain.\n- As a benchmark, ChemTable focuses almost exclusively on general-purpose multimodal large language models (MLLMs), while lacking systematic evaluation of domain-specific models for chemistry or broader scientific applications. This omission limits its value as a professional reference benchmark for “chemical table” understanding and recognition.\n- Many tasks within the benchmark (such as Table Recognition, Title Description, Annotation Description, Yield and Conditions, Value Comparison, Find Min/Max, and Multi-hop Retrieval) already achieve high performance across mainstream MLLMs, with most models scoring above 80. This raises a key concern: if general models already perform well on these tasks, do these benchmarks still provide sufficient discriminative power or research value?\n- Notably, Gemini-2.5-Flash, a small-sized model, achieves the best or near-best performance in most tasks, which raises an important question: if a “smaller” model performs this well, would flagship models such as Gemini-2.5-Pro, GPT-5, or Claude-4 easily surpass the current results? If so, does the benchmark still provide meaningful evaluation or differentiation between stronger models?"}, "questions": {"value": "Please refer to the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oYNMfTXPYt", "forum": "Y4cCe3aB0N", "replyto": "Y4cCe3aB0N", "signatures": ["ICLR.cc/2026/Conference/Submission15653/Reviewer_v8z9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15653/Reviewer_v8z9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978730389, "cdate": 1761978730389, "tmdate": 1762925911052, "mdate": 1762925911052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark for evaluating multimodal LLMs on chemical table recognition and understanding. The dataset spans ~1,300 annotated chemical tables from literature with 9000 QA instances to measure table understanding capabilities. The work evaluates multiple MLLMs on the benchmark, revealing performance gaps in molecular structure recognition and domain reasoning compared to human performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel benchmark focused on a challenging domain of chemical table recognition and reasoning\n\nRigorous annotational protocol incorporating both manual and synthetic data generation\n\nComprehensive evaluation across multiple open-source and proprietary MLLMs"}, "weaknesses": {"value": "Limited dataset scale. Although the benchmark incorporates multiple table types, the overall number of samples remains modest, with 41.4% of the tables related to “Condition Optimization”. These limitations can constrain the generalizability of the findings. It would be good if the authors can provide some discussion of sampling bias and coverage across chemistry subfields.\n\nQuestions distribution. The paper does not report the distribution of QA instances across tables. Given that multiple filtering steps are applied, how did the authors ensure that evaluation metrics are not biased by skewed QA density? It would be helpful to include per-table or per-category QA statistics after filtering.\n\nQualitative error analysis. Although the paper provides metrics for different subdomains of the benchmark (e.g. by question type, molecular complexity), it would be helpful to include a few concrete failure case studies with visualizations to clarify where the models break."}, "questions": {"value": "I'm not so sure about the contribution—the authors propose a dataset for OCR in chemistry; overall, it seems useful, but not for the broader community. Looks like a paper for Chemoinformaics journals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mo0XcJLhDl", "forum": "Y4cCe3aB0N", "replyto": "Y4cCe3aB0N", "signatures": ["ICLR.cc/2026/Conference/Submission15653/Reviewer_K6DN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15653/Reviewer_K6DN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982997626, "cdate": 1761982997626, "tmdate": 1762925910511, "mdate": 1762925910511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChemTable, a benchmark consists of chemical tables with various contents. The core tasks include table recognition and table understanding. Evaluations are carried out for several closed-source or open-source multi-modal models, showing that they consistently fall short in complex tasks such as handling molecular structures and symbolic conventions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors construct a real-world chemical table benchmark with expert annotations. The evaluated tasks are diverse, and the paper presents adequate number of experimental observations that seems reasonable."}, "weaknesses": {"value": "Major points:\n\n* The size of benchmark seems to be inadequately large. \n* The evaluated models are not state-of-the-art. What are the performance of more powerful models, such as Claude-4.0, GPT-5, Gemini-2.5-Pro? Also, the paper should include MLLMs specifically finetuned for chemistry tasks, such as ChemVLM.\n* The presentation form of tables is always image. Did you try out tabular data form and study the performance of relational foundation models? Or using text descriptions for symbolic elements / graph representations for molecular structures when applicable?\n* This is a purely benchmarking paper, and the authors fail to provide theoretical justifications or insights in depth. In particular, for those open-sourced models where one can observe the reasoning patterns inside the models, can you provide any analysis?\n\nMinor points: \n\n* Typo: \"Claude-3-7-Sonnet\" should be \"Claude-3.7-Sonnet\" in Table 3."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BmKNN4024o", "forum": "Y4cCe3aB0N", "replyto": "Y4cCe3aB0N", "signatures": ["ICLR.cc/2026/Conference/Submission15653/Reviewer_84uM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15653/Reviewer_84uM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032758203, "cdate": 1762032758203, "tmdate": 1762925910017, "mdate": 1762925910017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Revision of the Submitted Manuscript"}, "comment": {"value": "We would like to thank all the reviewers for their efforts in reviewing our manuscript. Your valuable suggestions inspire us to improve our study and gracefully revise our manuscript. Specifically, we have resubmitted a newly revised manuscript following your suggestions. The main revision parts can be summarized as below.\n\nWe have updated the main comparison table to include three recent flagship MLLMs—GPT-5, Gemini-2.5-Pro, and Claude-4.5-Sonnet—and slightly refined our conclusions to reflect their improved performance while still highlighting the non-trivial gap to human experts on chemistry-intensive and compositional tasks. In addition, we have added a new Appendix M that presents qualitative case studies (with table images, ground-truth answers, and model predictions) to illustrate typical failure modes in localization, visual-style grounding, symbol/footnote binding, and multi-step reasoning. Finally, we have improved the overall exposition and organization of the paper, clarified several descriptions, and corrected minor typographical issues."}}, "id": "rwc5wP75bC", "forum": "Y4cCe3aB0N", "replyto": "Y4cCe3aB0N", "signatures": ["ICLR.cc/2026/Conference/Submission15653/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15653/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission15653/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763610365640, "cdate": 1763610365640, "tmdate": 1763610365640, "mdate": 1763610365640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}