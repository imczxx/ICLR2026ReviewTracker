{"id": "T985gm4sDA", "number": 4442, "cdate": 1757681446403, "mdate": 1759898032125, "content": {"title": "Scaling Laws for Diffusion Transformers", "abstract": "Diffusion transformers (DiT) have already achieved appealing synthesis and scaling properties in content recreation, \\emph{e.g.,} image and video generation.\n\nHowever, scaling laws of DiT are less explored, which usually offer precise predictions regarding optimal model size and data requirements given a specific compute budget.\n\nTherefore, experiments across a broad range of compute budgets, from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling laws in DiT \\emph{for the first time}. Concretely, the loss of pretraining DiT also follows a power-law relationship with the involved compute.\n\nBased on the scaling law, we can not only determine the optimal model size and required data but also accurately predict the text-to-image generation loss given a model with 1B parameters and a compute budget of 1.5e21 FLOPs.\n\nAdditionally, we also demonstrate that the trend of pretraining loss matches the generation performances (e.g., FID), even across various datasets, which complements the mapping from compute to synthesis quality and thus provides a predictable benchmark that assesses model performance and data quality at a reduced cost.", "tldr": "", "keywords": ["Scaling laws", "diffusion models", "transformers"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a71d595064278e6a457d6bee172882110a9579ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the first systematic investigation of the scaling laws of Diffusion Transformers (DiT) for text-to-image synthesis. The authors conduct extensive expriements accross compute budgets $C$ from $1e17$ to $6e18$, and predict the performance for a larger 1B-model with compute budgets $1.5e21$.\n\nThe key contributions are:\n1. **First Establishment of Scaling Laws of DiT**: This paper is the first to investigate and confirm the power-law relationship with the compute budget.\n1. **Fit the Power Law Equations**: This papers fits the power-law equations for model/data size: $N_{opt}=0.0009\\cdot C^{0.5681}$ and $N_{opt}=186.8535\\cdot C^{0.4319}$, as well as for the loss $L=3.3943\\cdot C^{-0.0273}$.\n1. **Validation on Larger Compute Budget**: The derived laws are validated by extrapolating to a significantly larger compute budget ( $1.5e21$), a 1B parameter model, and demonstrating that its loss and performance metrics match the predictions.\n1. **Evalution Metrics Justification**: The authors also verify generative quality metrics like FID also follows the scaling laws ($FID = 2.2566 \\times 10^6 \\cdot C^{-0.234}$), as well as other evaluation metrics like VLB, exact likelihood, GenEval, and human preference scores. This also justifys that these evaluation metrics are suitable for text-to-image synthesis evaluation.\n1. **Generative Robustness**: The paper demonstrates the robustness of these laws by showing they hold for out-of-domain (OOD) data (e.g., COCO) and across different model architectures.\n1. **Guidance on Model/Data design**: This paper shows difference model architecture (In-Context *v.s.* Cross-Attention) has different coefficients on the power-law equations, which helps evalutate future model designs using small compute budgets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper for the first time provide a systematic investigation and verification of predictive, quantitative scaling laws of Diffusion Transformer (DiT).\n1. The expriments conducted to investigate the scaling laws of DiT is reasonable and thorough, which include large-scale validation, evaluation of various metrics and coefficents comparision of different models.\n1. The proposal to use scaling exponents as a predictable benchmark is practically usable and provides the community with a powerful and low-cost tool for architectural and data-quality comparisons."}, "weaknesses": {"value": "The paper is generally good, but there's some mirror \"weaknesses\" or improvement sugesstion. I am happy to increase the score if they are addressed.\n1. **Contradiction in Section 4**: There is a small contradiction between the text and Table 1 when comparing model architectures. The text states, \"The Cross-Attention Transformer exhibits a *larger* model exponent\". However, Table 1 shows the Cross-Attention model exponent (0.54) is smaller than the Vanilla In-Context model's exponent (0.56). The text's conclusion—that \"more resources should be allocated toward scaling the dataset\" —is correct and does align with the table (data exponent 0.46 > 0.43).\n1. **Some writtings are confusing**:\n    1. Line 241, $C$ is the symbol of \"Compute\", but later it's referenced as compute budget in line 252. First-time reader might not know \"Compute\" and \"Compute budget\" are the same.\n    1. There are three sub-figures in Figure 1, but they are refered as a whole, e.g., in Line 265, it should refer to the first sub-figure of Figure 1.\n    1. The logic from Line 243 to 251 is not clear to me. It says $C=6ND$, a linear relationship between $C$ and $N$ or $D$, but later it ypothesizes $N_{opt} \\propto C^{a}$ and $D_{opt} \\propto C^{b}$. It's unclear to me why, untill I review Figure 1 that the x-axis is in log-scale and the fitting curve is a strage line. But still the y-axis of Figure 1 does not say they are $N_{opt}$ and $D_{opt}$ so it's still unclear to me how the hypothesis comes from.\n    1. Figure 3 shows legend the second and the third sub-fogures of Figure 1 do not. And the second sub-figure of Figure 3's legend is *fitter* curve."}, "questions": {"value": "1. As shown in Figure 3, as the $C$ increases, FID and Loss decrases, and the fitted curve is strage line (as y-axis is log-scale). However we cannot increase compute budget forever to obtain a minus loss, there should be saturation or even overfit. How the scaling curves show staturation and overfit?\n1. For a fixed compute budge $C$, the relationship between it and $N$, $C$ is defined as $C=6ND$, but how about $N_{opt}$ and $D_{opt}$?  Say we have a designed a model, and fit the coefficents of $N_{opt}$ and $D_{opt}$, and we have limited compute budget, how can we balance the between $N_{opt}$ and $D_{opt}$?\n1. The second and the third sub-figures of Figure 1, the y-axis should be *optimal* parameter $N_{opt}$ and *optimal* token $D_{opt}$?\n1. Any experimental FID result of Vanilla In-Context vs Cross-Attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HtkWVC0qGA", "forum": "T985gm4sDA", "replyto": "T985gm4sDA", "signatures": ["ICLR.cc/2026/Conference/Submission4442/Reviewer_fsKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4442/Reviewer_fsKy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658229507, "cdate": 1761658229507, "tmdate": 1762917369046, "mdate": 1762917369046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the scaling laws of text-to-image diffusion transformers (DiTs). It shows that these models follow a scaling behavior similar to that of large language models: the training loss and other performance metrics exhibit a power-law relationship with compute when the model size and token count are optimally balanced. Furthermore, the paper demonstrates that this trend generalizes across out-of-domain datasets and different model architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is very well written.\n\n* The findings enable practitioners to tune the hyperparameters of DiTs more efficiently.\n\n* The paper demonstrates scaling laws not only with respect to the loss, but also for other useful metrics such as FID and human preference reward."}, "weaknesses": {"value": "Beyond text-to-image generation, diffusion models have also been applied to tasks such as class-conditioned image generation and text-to-video generation. However, the paper only experiments on text-to-image generation, so it remains unclear whether the same scaling laws extend to these other tasks."}, "questions": {"value": "Overall, I enjoyed reading this paper. While the techniques are not new and the results are not particularly surprising given the established scaling laws of large language models, confirming that similar laws hold for DiTs is nevertheless a valuable and solid contribution. I have a few questions listed below and hope the authors can address them in the rebuttal.\n\n* I would like to confirm how Figure 2 is derived. Is it obtained using the same procedure as Figure 1? That is, for each metric, (1) train models of varying sizes under different compute budgets, (2) fit a parabola between the metric and model size for each compute budget, (3) identify the optimal model size for each compute budget, and (4) plot the metric versus compute budget using these optimal model sizes?\n\n* Eq. 4: This equation is somewhat confusing. First, N is not defined. Second, the summation index i does not appear in the summand, making it unclear what is being summed over.\n\n* In Section 3.1, \"Variational Lower Bound\" and \"Exact Likelihood\" are subpoints under \"Likelihood.\" However, in the current formatting, \"Likelihood,\" \"Variational Lower Bound,\" and \"Exact Likelihood\" appear at the same heading level, which may mislead readers into thinking they are parallel sections, causing potential confusion.\n\n* Figure 5: How many data points are included in each line?\n\n* Section 4 states that the experiments show cross-attention transformers exhibit a superior scaling trend compared to in-context transformers. However, as discussed in Section 4, recent works suggest that in-context conditioning performs better. Could the authors clarify where this discrepancy might come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2tCJGJswzU", "forum": "T985gm4sDA", "replyto": "T985gm4sDA", "signatures": ["ICLR.cc/2026/Conference/Submission4442/Reviewer_y2Z9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4442/Reviewer_y2Z9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809374135, "cdate": 1761809374135, "tmdate": 1762917368720, "mdate": 1762917368720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the scaling laws of diffusion models for text-to-image generation. The authors conduct experiments across a wide range of compute budgets, spanning from 1e17 to 6e18 FLOPs. The results reveal empirical relationships between training loss, evaluation metrics, and compute expenditure. Based on these relationships, the paper claims that the performance of larger diffusion models can be accurately predicted."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses an important problem: establishing scaling laws for text-to-image generation with diffusion models. The findings have the potential to offer valuable insights to the community.\n- The authors conduct extensive experiments and dedicate substantial effort to derive and validate the proposed scaling laws."}, "weaknesses": {"value": "- In Figure 1, the assumption underlying the use of parabolic fitting for the performance curve is not clearly stated. If the curve is assumed to be unimodal, then a ternary search strategy could directly identify the optimal loss without requiring curve fitting.\n- In Figure 20 (GenEval results), only the value \"10\" appears on the y-axis, making it difficult to determine the specific GenEval scores associated with each data point. Providing a complete y-axis scale would significantly improve readability.\n- The scaling analyses use log scale for FID (Figure 3) and GenEval (Figure 20), but a linear scale for Human Preference Rewards (Figure 21). It would strengthen the consistency and interpretation of the results to explain why different scales are used and why each metric is expected to exhibit a linear or log-linear relationship with model performance.\n- The analysis would be further strengthened by comparing the derived scaling laws to existing text-to-image diffusion models. Including these models in figures or table would provide a more comprehensive empirical grounding and reinforce the conclusions about scaling behavior."}, "questions": {"value": "Please refer to the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sdkd6P4dRc", "forum": "T985gm4sDA", "replyto": "T985gm4sDA", "signatures": ["ICLR.cc/2026/Conference/Submission4442/Reviewer_KekD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4442/Reviewer_KekD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811514428, "cdate": 1761811514428, "tmdate": 1762917368005, "mdate": 1762917368005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is an empirical study on how pretraining loss and downstream generation quality of DiTs scale with compute, model size and data. It also shows that the pretraining loss follows a power-law relationship with compute. It proposes rules to to pick optimal model / data and to predict downstream metrics from pretraining loss. Additionally authors also identify the relationship between pretraining loss and generation performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Paper is very well-written and easy to follow. The method section has been particularly well-described.\n1. Authors explore a very timely topic in diffusion based transformer models. \n2. Authors conduct experiments with DiT across various compute budgets and provide empirical proofs.\n3. The paper also reports a correlation between pretraining loss and downstream FID, GenEval and human preference  metric which can be potentially beneficial for practitioners."}, "weaknesses": {"value": "1. This paper fixes  a particular training dataset derived from Laion-5B. \nHowever, with generative models, we are observing that a careful data curation pipeline can affect the training and model scaling options. Authors can consider also studying how noisy / clean data can affect scaling properties.\n\n2. The power-law relationship between training budget and generation performance provides a sign that the scaling law can predict generation performance. However, it is a bit unknown if this law will continue to hold under different hyper-parameter / diffusion sampling settings including varying classifier guidance strength for example."}, "questions": {"value": "Same as weeknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6TcyqmjROW", "forum": "T985gm4sDA", "replyto": "T985gm4sDA", "signatures": ["ICLR.cc/2026/Conference/Submission4442/Reviewer_knY9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4442/Reviewer_knY9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951772574, "cdate": 1761951772574, "tmdate": 1762917367463, "mdate": 1762917367463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}