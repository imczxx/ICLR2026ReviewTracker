{"id": "UA7Bfy8QaW", "number": 1654, "cdate": 1756901382896, "mdate": 1763747125741, "content": {"title": "ML Estimation from Bits", "abstract": "Estimating statistical parameters from quantized signals has received significant attention in recent years, as recovering information from quantized measurements has numerous applications across signal processing, communications, and data analysis. In this work, we focus on maximum likelihood (ML) estimation of statistical parameters from quantized samples. Directly solving the ML problem is challenging, as the likelihood function involves multiple integrals that are difficult to evaluate.\nTo address this challenge, we propose an expectation-conditional-maximization (ECM) algorithm under a general distributional framework. Our approach generalizes the quantization model to multi-bit settings and allows the underlying signal to follow any distribution within the normal mean-variance mixture family. By designing suitable surrogate functions, the ECM algorithm ensures that all model parameters can be updated in closed form at each iteration. Leveraging the ECM framework, we provide convergence guarantees, and under specific distributional assumptions, we further derive bounds on the convergence rate and the statistical error. Extensive experiments demonstrate the effectiveness of our method in recovering statistical parameters from quantized data.", "tldr": "", "keywords": ["MLE", "quantization", "expectation-conditional-maximization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/808a71b2d13d82284f5f08e3eaca68c029f62121.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors consider maximum likelihood estimation of the parameters $(\\mu,\\xi,\\Sigma)$ of a random vector $x \\in \\mathbb{R}^d$ following a normal variance-mean mixture model from quantized observations $y_i = Q(x_i)$ where $Q$ is a scalar quantizer and $x_1,…,x_n$ are iid samples of $x$. According to this model\n\n$$x = \\mu + z\\xi + (z \\Sigma)^{1\t/2} \\epsilon,$$\n\nwhere $\\mu$ is the location parameter, $\\xi$ is the skewness parameter, $\\Sigma$ is the scattering parameter, $\\varepsilon$ is a standard normal random vector, and $z$ is a non-negative random variable with density $p(z)$.\n\nSince evaluating the likelihood function $L$ involves double integration and is not possible analytically, the authors use Jensen’s inequality to construct a surrogate functional that lower bounds $L$ and can be maximized more efficiently by alternating steps of taking expectations and maximizing the current surrogate.\n\nIn Theorem 2 they show global convergence of the alternating method to stationary points of L at a linear rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Paper is easy to read and tackles a relevant problem"}, "weaknesses": {"value": "- Convergence result is not clearly stated\n- Numerical evaluation is hard to interpret\n- Efficiency of the method for covariance estimation is not analyzed (neither theoretically nor experimentally)"}, "questions": {"value": "Since the paper does not convince me that the ML approach outperforms existing methods for quantized covariance/correlation estimation, I do not recommend it to be accepted yet. In my opinion, the numerical comparison with existing approaches must be clarified to clearly prove gains in computational or sample efficiency. Moreover, the convergence result needs to be revised.\n\nHere a list of questions/issues that should be addressed:\n\n- l. 61: Q is quite loosely defined at the moment. In particular, the statement that Q becomes sign for e=2 is not correct since the present definition would also allow Q taking values -a and b for values in (-\\infty,c) and [c,\\infty), respectively, where $a,b > 0$ and $c$ can be chosen arbitrarily.\n\n- Some relevant more recent literature has not been mentioned:\n\n[1] Chen, Junren, and Michael K. Ng. \"A parameter-free two-bit covariance estimator with improved operator norm error rate.\" Applied and Computational Harmonic Analysis (2025): 101774.\n\n[2] Dirksen, Sjoerd, and Johannes Maly. \"Tuning-free one-bit covariance estimation using data-driven dithering.\" IEEE Transactions on Information Theory 70.7 (2024): 5228-5247\n\n- l. 109: The works of Chen et al. and of Dirksen et al. are not restricted to Gaussian distributions\n\n- ll. 169+177: irrelevant -> independent ?\n\n- l. 188: independent with -> independent of\n\n- Proposition 1: For which stationary point does the result hold? It cannot hold for all stationary points simultaneously. From the proof, I cannot see that a particular stationary point is picked, so I doubt the correctness of the argument. I did not have time to check the proof in detail though.\n\n- Section 5 is suddenly discussing matrix completion and compressed sensing as applications of the work without specifying how the results can be applied. For matrix completion this is then done in Section 6, for compressed sensing it’s completely missing. I find this structure strongly confusing.\n\n- Section 6: I find the labelling of the plots highly unclear and I cannot really connect the lines to the methods discussed in the text. To make this comparison rigorous and interpretable, please clearly name the methods you compare and use the same labelling in text and plots. The present presentation does not convince me of the value of the method. Furthermore, efficiency in approximation is something that should definitely be evaluated and compared in the covariance estimation setting since existing approaches are quite cheap to compute. \n\n- Finally, is there any hope to analyze the estimation error of the ML approach in dependence of the number of samples?  Chen et al. and of Dirksen et al. provide rigorous and non-asymptotic error guarantees for their respective estimators, which are essential for reliability of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pcP9mlK2cY", "forum": "UA7Bfy8QaW", "replyto": "UA7Bfy8QaW", "signatures": ["ICLR.cc/2026/Conference/Submission1654/Reviewer_Z3VM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1654/Reviewer_Z3VM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718919930, "cdate": 1761718919930, "tmdate": 1762915846803, "mdate": 1762915846803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers parameter estimation with quantized samples. This builds theoretically using on an ECM algorithm approach developed in a line of other works, now generalizing to the normal mean-variance mixture case.  An approximate ML function is used to approach ML estimation. An ECM algorithm is developed and convergence proofs are derived.  Numerical examples compare to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theory is clearly developed and the ECM is shown to linearly converge to a stationary point, similar to prior analysis for the less general cases.  The method adds some generalization over past Gaussian, now with normal mean-variance mixture, adding some additional distribution parameters. The work is shown to reduce to previously developed Gaussian methods.  The authors present examples with matrix completion for a recommendation system which is an interesting real-life application. \n\nThe method might be useful for robustness against Gaussian mismatch, and the paper makes statements about heavy tailed distributions."}, "weaknesses": {"value": "Overall the paper is strong on theory, and less so on the examples and testing. The examples are relatively benign and limited.  It isn’t clear if a local minimum is possible in the optimization, and what is lost between max-likelihood and using the bounded surrogate function approach. The overall complexity is not well characterized.  The quantization loss is also not clearly considered in the examples. \n\nThe robustness question is interesting, e.g., heavy tailed cases.  It would be useful to explore this."}, "questions": {"value": "What happens with more than one bit quantization?  Complexity, for example.\n\nHow to characterize complexity and compare against other algorithms?\n\nShow also a \"full\" quantized case to see what is lost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RPnAzlTe6j", "forum": "UA7Bfy8QaW", "replyto": "UA7Bfy8QaW", "signatures": ["ICLR.cc/2026/Conference/Submission1654/Reviewer_D8DK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1654/Reviewer_D8DK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837351726, "cdate": 1761837351726, "tmdate": 1762915846535, "mdate": 1762915846535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an expectation/conditonal maximization (ECM) algorithm for estimation of signals following a mean variance mixture model from quantized measurements. Following the common premise for EM algorithms, the E step estimates latent variables based on observations and existing model parameter estimates and the CM step updates the parameters to best match the latent variables and observations. The paper includes convergence results for the model parameters and shows examples in covariance estimation, matrix completion, and compressed sensing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper generalizes existing frameworks by letting the quantization scheme be arbitrary rather than just binary (e.g., sign preservation) and using a broader model for the signals (vs. the usual multivariate Gaussian) and shows agreement between new and existing results in these cases."}, "weaknesses": {"value": "Since the paper is focused on broadening the model applied to solve existing problems currently addressed with narrow models, it would be good to get a discussion of the benefits afforded by this contribution. The experiments do not seem to need to leverage the broader model, and a comparison with existing approaches is lacking as detailed below.\n\nThe presentation is not always fully clear; there are multiple instances of notation used without introduction (detailed at the end of this response). \n\nNoise does not appear to be considered in the acquisition process.\n\nThe numerical comparisons are for the most part focusing on the role of mismatch in the modeling on the performance of estimation, but there is a lack of comparison with previously proposed approaches. Although some competitors are mentioned in page 8, it is not clear if the comparison is against an ECM approach that uses a particular narrower data model or against the approaches of the cited references, which are not EM-based. There is no consideration of the computational cost of the EM approach to contrast against accuracy metrics (e.g., Table 2).\n\nIt would be good to have a discussion of applications for the quantized covariance estimation problem.\n\nLine 115: Parameters mu, xi, Sigma have not been defined\nLine 131: the Sigma^{-1} norm has not been defined (but one can guess)\nLine 168: \"theta underlined\" is not previously defined\nLine 169: “irrelevant of” does not seem to be appropriate wording. Perhaps independent or \"does not depend\"?\nTitle: the PDF title does not match the title in the database record."}, "questions": {"value": "Is there a source for the statement about Netflix movie recommendations in line 275? It may hark back to \"The Netflix Prize\" from the decade of the 2000's, but it's not clear this is still true; a more generic statement could be made about recommendation systems though. See for example https://dl.acm.org/doi/10.1145/2891406\n\nCan you elaborate on \"the constraints in certain prior works\" mentioned in line 312? It would better inform the choices made in Section 6.1, particularly because it's not clear that a comparison to existing approaches is being provided.\n\nGiven the closeness of the lines in Figure 1, would a wider zoom or a table be more appropriate to convey the results? Or alternatively, is it important to show which algorithm is better when their performances are very similar to one another?\n\nCan you provide motivation or a citation for the use of the surrogate (16)? It is not clear how it connects to the mean variance mixture model being assumed. Perhaps it would be clearer to explicitly write an instantiation of the ECM algorithm of page 4 for each application?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PH62hoLaRD", "forum": "UA7Bfy8QaW", "replyto": "UA7Bfy8QaW", "signatures": ["ICLR.cc/2026/Conference/Submission1654/Reviewer_F6qK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1654/Reviewer_F6qK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851265221, "cdate": 1761851265221, "tmdate": 1762915845947, "mdate": 1762915845947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the ECM (expectation conditional maximization) algorithm for quantized maximum-likelihood estimation. The authors prove convergence guarantees for the algorithm and test the algorithm on covariance estimation and matrix completion tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper extends previous works on quantized statistical estimation to multiple bit and a more general normal mean-varaince mixture model.\n2. The proposed method demonstrates generality to some extent, which can be applied to covariance estimation and matrix completion, the latter task is very important in recommendation systems."}, "weaknesses": {"value": "1. It is not clear whether to what extent the normal mean-variance mixture model is useful in machine learning applications. The experiment is only performed on MovieLens 100k data, and the performance is not compared with other machine learning approaches to demonstrate the effectiveness of the maximmum-likelihood estimator.\n2. The scalability of the approach is not clear. The MovieLens 1M and 20M benchmark may be more relevant for modern machine learning applications."}, "questions": {"value": "What are other potential applications of the normal mean-variance mixture model in other machine learning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ox4WEXHXbv", "forum": "UA7Bfy8QaW", "replyto": "UA7Bfy8QaW", "signatures": ["ICLR.cc/2026/Conference/Submission1654/Reviewer_Cium"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1654/Reviewer_Cium"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission1654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762499512048, "cdate": 1762499512048, "tmdate": 1762915844364, "mdate": 1762915844364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}