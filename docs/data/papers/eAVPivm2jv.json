{"id": "eAVPivm2jv", "number": 9920, "cdate": 1758149069117, "mdate": 1759897686045, "content": {"title": "Is Memorization Actually Necessary for Generalization", "abstract": "\\begin{abstract}\nDeep learning models are known for their ability to memorize training data. While memorization is often linked to risks such as privacy leakage and poor robustness, a highly influential claim by~\\citet{feldman2020longtail} argues that memorization is actually \\textit{necessary} for generalization. Their conclusion is based on the observation that removing points with high memorization scores reduces test accuracy. Upon closer inspection of their work, we uncover four critical flaws in the underlying methodology: \\textbf{(1) sampling bias} in their approximation algorithm inflates memorization scores; \\textbf{(2) high false positive rate} in their definition of memorization, leads to misclassification of non-memorized points as memorized; \\textbf{(3) unprincipled thresholding} that resulting in an ill-posed problem; and \\textbf{(4) data leakage} skews the test accuracy results. To address these limitations, we introduce a modifications for correctly identifying and evaluating memorization, including higher sampling rates, modifying the original memorization definition to reduce the false positive rates, proposing a method to identify a principled score threshold, and employing test datasets especially designed to avoid data leakage. Having accounted for these errors, our results show that, in contradiction to the original work, removing truly memorized points does not cause a drop in accuracy, and in most cases, improves test performance. These findings call into question the necessity of memorization in deep learning and highlight the importance of mitigating its risks.\n\n\n\\end{abstract}", "tldr": "We find that memorization decreases generalization, unlike the findings of the popular work from Feldmen et al", "keywords": ["Memorization", "Generalization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5149b1e5da56003ee4b779000c0e4aa6431943e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper revisits and challenges the claim by Feldman & Zhang (2020) that memorization is essential for neural network generalization. It  argues that the original conclusions were based on four critical methodological flaws: (1) sampling bias from using a low sampling rate that inflated memorization scores, (2) a high false positive rate due to an imprecise memorization definition, (3) unprincipled thresholding that lacked a statistically grounded criterion for distinguishing memorized from non-memorized samples, and (4) data leakage between training and test sets that artificially increased measured accuracy. To address these issues, the paper introduces methodological corrections, including higher sampling rates approaching leave-one-out, a new fractional difference definition to reduce false positives, a null-distribution-based threshold for principled identification of memorized points, and leakage-free test datasets. Through experiments on CIFAR-10, CIFAR-100, and ImageNet across multiple architectures (MobileNet, VGG19, ResNet18, and ResNet50), the paper shows that removing truly memorized samples never harms and often improves test accuracy, particularly for more complex datasets. These results overturn the notion that memorization is necessary for generalization, suggesting instead that it can degrade performance, and the paper advocates viewing memorization as a behavior to mitigate rather than a property to preserve in deep learning models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper tackles an influential and widely cited claim about the necessity of memorization for generalization, offering a fresh and critical perspective.\n\n2. It systematically identifies four core methodological flaws—sampling bias, high false positive rate, unprincipled thresholding, and data leakage—in the original Feldman & Zhang (2020) study.\n\n3. The paper proposes concrete fixes, including higher sampling rates, a fractional-difference definition of memorization, statistically grounded thresholding via null distributions, and de-duplicated test datasets.\n\n4. The experimental evaluation spans multiple datasets (CIFAR-10, CIFAR-100, ImageNet) and architectures (MobileNet, VGG19, ResNet18, ResNet50), providing empirical evidence.\n\n5. The paper clearly articulates the logical flow from identifying issues to proposing corrections and presenting re-evaluated results."}, "weaknesses": {"value": "1. In section 3.2, the paper treats randomly relabeled samples as the ground truth for memorized points when comparing definitions. While convenient, this synthetic setup may not reflect real-world memorization behaviors (e.g., rare or atypical but semantically consistent samples), limiting the validity of the conclusions about false positive rates.\n\n2. The paper does not release code for reproducing key experiments, limiting transparency and reproducibility.\n\n3. In Section 3.4, the paper identifies train–test overlaps as a potential source of inflated accuracy and supports this claim with qualitative examples (Figure 4). However, it does not quantify the extent of this leakage — e.g., how many images overlap in CIFAR or ImageNet, or what proportion of the test set is affected. Without a concrete measurement, it is difficult to assess whether data leakage is a widespread issue or limited to a few instances, weakening the strength of the argument.\n\n4. Figure 6 shows that the approximation error for CIFAR-100 is higher than for CIFAR-10, even though the two datasets are structurally similar and differ mainly in the number of classes. The paper does not explain why this happnes. A deeper analysis of how dataset complexity affects sampling bias would strengthen the paper’s claims.\n\n5. In Figure 6, the approximation error for CIFAR-10 appears nearly unchanged between sampling rates of 0.5 and 0.9, suggesting that increasing the sampling rate does not substantially reduce the error for this dataset. The paper does not explain why this occurs. Additionally, the results for ImageNet are not reported, leaving it uncertain whether similar trends hold for larger and more complex datasets.\n\n6. While Feldman & Zhang (2020) trained on the order of thousands of models to approximate the memorization score, this paper uses 50 models (section 4.1). Although computationally more feasible, this smaller number of models may not fully capture the variability needed for precise memorization-score estimation.\n\nMinor weaknesses:\n1. In section 3.4 there are some \"?\" that indicate some references might not be cited properly."}, "questions": {"value": "1. Ground truth for memorization: In Section 3.2, you use randomly relabeled samples as a proxy for “truly memorized” points. Could you elaborate on why this choice is representative of real-world memorization? Have you considered alternative or complementary definitions, such as using rare or low-density samples as ground-truth memorized points?\n\n2. Extent of data leakage: In Section 3.4, Figure 4 presents qualitative examples of train–test overlaps, but the paper does not quantify the extent of this issue. Could you provide statistics on how many overlapping or near-duplicate samples were detected in CIFAR-10, CIFAR-100, and ImageNet? Knowing whether the leakage affects a small subset or a substantial portion of the test set would greatly strengthen your argument.\n\n3. Sampling rate behavior across datasets: Figure 6 shows that CIFAR-100 exhibits higher approximation error than CIFAR-10, even though the datasets are structurally similar. Could you clarify what drives this discrepancy? \n\n4. Unclear trend for CIFAR-10 and missing ImageNet results: In Figure 6, the approximation error for CIFAR-10 appears almost constant between sampling rates of 0.5 and 0.9, which seems inconsistent with the claim that higher sampling rates reduce bias. Can you explain why this occurs? Also, could you include or discuss corresponding results for ImageNet to assess whether this trend generalizes to larger datasets?\n\n5. Model number: In Section 4.1, you mention training approximately 50 models. Could you clarify how you determined that this number was sufficient for stable memorization estimates? Have you evaluated whether increasing the number of models meaningfully changes the observed trends or variance of the results?\n\n6. Reproducibility: Since no code is released, would you consider sharing implementation details, hyperparameters, and scripts? This would help readers verify the findings and reproduce key experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tgXzNNykq7", "forum": "eAVPivm2jv", "replyto": "eAVPivm2jv", "signatures": ["ICLR.cc/2026/Conference/Submission9920/Reviewer_XWnb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9920/Reviewer_XWnb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664741135, "cdate": 1761664741135, "tmdate": 1762921375266, "mdate": 1762921375266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conclusions of Feldman and Zhang (2020) [2] concerning the definition and\nempirical role of memorization in generalization. The authors argue that several methodological factors\nsuch as sampling bias, arbitrary threshold selection, lack of model convergence, and data leakage may have\nexaggerated the reported connection between memorization and test accuracy. By introducing a cleaner\nexperimental pipeline together with a null distribution calibration, the paper re-examines whether\nremoving “memorized” samples genuinely harms model performance.\n\n[1] Feldman, Vitaly. \"Does learning require memorization? a short tale about a long tail.\"\n[2] Feldman, Vitaly, and Chiyuan Zhang. \"What neural networks memorize and why: Discovering the long\ntail via influence estimation.\""}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and clearly structured, making it easy to follow the motivation, methodology, and\nresults. The introduction of the random-label null distribution is an interesting and original idea that\nprovides a fresh statistical perspective on memorization. The experimental analysis is broad and\nsystematic, covering multiple datasets and models."}, "weaknesses": {"value": "**1. On the meaning of “evaluated against a baseline.”**\n\nThe memorization score introduced in [2] was designed as an empirical instantiation of the theoretical\ndefinition proposed in [1]. In that framework, the score is not a heuristic metric but a property intrinsic to\neach sample, derived directly from theory. The visualizations in [2] (see https://pluskid.github.io/influence-\nmemorization/) clearly show that samples with high memorization scores tend to be atypical or rare from a\nhuman perspective. Given this context, the statement on line 61 that “the memorization scoring definition is\nnever evaluated against a baseline” may not be conceptually meaningful, since the quantity itself serves as its\nown theoretical ground truth. If the authors intend to reinterpret memorization as a detectability or\ninference problem, a more suitable comparison might come from membership inference analysis (MIA),\nwhich explicitly measures how much a model’s output depends on individual training samples. I could not\nfind a clear indication that the current paper engages with the MIA literature, so it would help to clarify\nwhether this connection was considered or deliberately excluded.\n\n**2. On the null-distribution analysis.**\n\nThe introduction of random labels to construct a null distribution is a valuable methodological addition.\nHowever, it would be insightful to visualize which samples receive high memorization scores under the null\ndistribution and whether they remain visually atypical to human observers. In addition, reporting the\ncorrelation between memorization scores computed under the null and natural distributions would help\nreveal whether the score primarily captures intrinsic sample rarity or genuine label–feature relationships.\n\n**3. On the long-tail assumption after data cleaning.**\n\nBoth [1] and [2] derive their conclusions under the assumption that the data distribution is long-tailed\nacross sub-populations. Since this paper removes near-duplicates and mislabeled samples, it should verify\nwhether the cleaned CIFAR-10/100 datasets still exhibit such a long-tail structure. Providing the post-\ncleaning memorization-score histogram or sub-population frequency-rank plot is necessary to demonstrate\nthat the theoretical premise of the original argument remains valid in the new experimental setting.\n\n**4. On data-leakage detection.**\n\nSection 3.4 briefly mentions the removal of near-duplicates but does not specify the exact detection\nmethod. Was cosine similarity in feature space used, or pixel-level distance? Providing per-class statistics\nfor the number of removed examples, the similarity threshold, and total removal counts in the appendix\nwould improve transparency and help assess whether the cleaning process introduced class-specific bias.\nAlso, please check lines 330–332, where double question marks appear in the citations.\n\n**5. On experimental comparison in Section 4.1.**\n\nIt necessary to include a direct comparison between the original [2] scoring method and the revised\nmethod proposed here, evaluated under identical sampling rates and all other training setups. Reporting\nthe resulting accuracy changes after removing top-scored versus random samples for both definitions\nwould more clearly isolate the effect of the new metric itself."}, "questions": {"value": "1. Could the authors clarify whether they intend to reinterpret memorization as a detectability/inference task, and if so, why membership inference analysis (MIA) was not included as a baseline comparison?\n\n2. Can the authors report visualizations and correlation statistics comparing null-distribution scores to natural scores to assess whether the metric reflects sample rarity or label–feature alignment?\n\n3. Could the authors provide post-cleaning memorization-score histograms or sub-population frequency-rank plots to verify that the long-tail structure still holds?\n\n4. What similarity metric and threshold were used to remove near-duplicates, and can the authors provide per-class removal statistics?\n\n5. Can the authors include a controlled comparison showing accuracy changes when removing top-scored versus random samples under both the original and revised scoring methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pPxAnbsW3D", "forum": "eAVPivm2jv", "replyto": "eAVPivm2jv", "signatures": ["ICLR.cc/2026/Conference/Submission9920/Reviewer_AsrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9920/Reviewer_AsrV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834888413, "cdate": 1761834888413, "tmdate": 1762921374598, "mdate": 1762921374598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits Feldman & Zhang (FZ, 2020) and argues that their core claim “memorization is necessary for generalization” is an artifact of methodology. The authors identify four issues in FZ’s pipeline: (1) sampling bias from training on small subsets, (2) a high false-positive definition of “memorization,” (3) unprincipled thresholding, and (4) train-test leakage. They propose fixes: train at higher sampling rates (near LOO), replace the absolute difference score with a fractional difference score, adopt a null-distribution threshold, and evaluate on deduplicated test sets. Under these changes, they report that removing “memorized” points never hurts and often improves test accuracy, especially on CIFAR-100 and ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear diagnosis of pitfalls. The four highlighted issues (sampling bias, high FPR, arbitrary thresholding, leakage) are well-motivated and relevant to the broader literature. \n2. Better detection criterion. The fractional-difference score is a sensible refinement; the paper evaluates detection quality using FPR@95% TPR, showing reductions in false positives relative to the original definition.\n3. Leakage awareness. The paper documents near-duplicates and evaluates on non-leaky test sets (CIFAR-no-dups, ImageNet-V2), which strengthens claims about generalization."}, "weaknesses": {"value": "1. The paper challenges a seminal result (Feldman–Zhang, “FZ”) but doesn’t bring the mountain of evidence needed to overturn it; the current experiments and analysis aren’t enough for a contrary claim.\n\n2. Fact-level imprecision: the manuscript says FZ used only a 0.7 subset. In reality, FZ released CIFAR-100 models spanning 0.1–0.9, susbet for CIFAR100 with 0.7 only for ImageNet. \n\n3. Sampling-bias critique undercut by scale: authors claim sample bias yet train only ~50 models and provide no ablation on number of modes. FZ trained about 2,000 ImageNet and 10,000 CIFAR-100 models (for each subsampling ratio on cifar100) without matching that scale, tail effects and variance aren’t convincingly addressed.\n\n4. Underuse of existing resources: instead of training models, the paper could directly use FZ’s released models to replicate and stress-test claims, removing implementation confounds. Using FZ models to prove the opposite would be the STRONGEST case for the claims of the paper, this I feel like this is a missed opportunity given the scale of model trained by Feldman–Zhang which is also publicly available.\n\n5. Evaluation too narrow: the new metric is interesting, but the paper mostly reports FPR@95% TPR. The paper should potentially also include full ROC/PR curves, AUROC, and AUPR, with confidence intervals and class-wise breakdowns.\n\n6. Weak theoretical engagement: the rebuttal to FZ’s long-tail theory is basically two statements of intuition (see lines 277–280) with no rigorous counter-argument or alternative theory. The experimental results are solid, but purely empirical; they don’t actually refute the theoretical result.\n\nMinor:\nCitation/presentation problems: at least one missing reference (around line 330 and 332)"}, "questions": {"value": "See weaknesses, but my general stance is that I do believe the authors are on to something but when making contrary claims to well established results the mountain of evidence to prove the contrary is missing from the paper. The use of FZ models to prove the contrary to FZ results would be the best argument for the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IHiRH3spTR", "forum": "eAVPivm2jv", "replyto": "eAVPivm2jv", "signatures": ["ICLR.cc/2026/Conference/Submission9920/Reviewer_LVvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9920/Reviewer_LVvf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973056589, "cdate": 1761973056589, "tmdate": 1762921374332, "mdate": 1762921374332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the influential claim by Feldman & Zhang (2020) that memorization is necessary for generalization. The authors posit that the conclusion of the original work is founded upon four flaws: (1) sampling bias in the memorization score approximation algorithm, (2) a high false positive rate in the original definition of memorization, (3) an unprincipled and arbitrary threshold for identifying memorized points, and (4) data leakage between the training and test sets. The paper introduces four corresponding modifications: (1) using a high sampling rate to approximate the leave-one-out (LOO) setting, (2) a new \"fractional difference\" definition for the memorization score, (3) a statistically principled threshold based on a null distribution, and (4) the use of curated test sets designed to prevent train-test overlap. Upon applying these corrections, the authors claim the original conclusion by Feldman & Zhang (2020) must be reversed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* I appreciate the critical view adopted by this paper. The paper identifies several legitimate methodological weaknesses in the work of Feldman & Zhang (2020), and as the authors state potentially in many other ML papers. The proposed solutions, particularly the use of a null distribution for thresholding and the identification of data leakage, are theoretically sound."}, "weaknesses": {"value": "* It is important to distinguish between truly hard samples that must be memorized and those that are memorized due to their unfavorable properties (e.g., being noisy/outlier or duplicate). It seems the example used in section 3.2 looks at the memorization phenomenon from a purely numerical perspective. $x_o$ could be a noisy sample hence having a high memorization score, indicating the model is sensitive to its presence while $x_i$ could be a truly hard example. If one agrees with this premise, the proposed score in eq 2 inflates the score of noisy samples and mark them as memorized while suppress that of truly hard samples. This could be misleading and in fact could be the underlying factor behind the conclusion of the paper: if truly hard samples are not marked as memorized and only noisy ones are marked as such, given that the latter have nothing to do with the test data, are not needed for generalization. Building upon this, I suggest the authors to use their proposed score in eq 2 for dataset cleaning, i.e. measure its utility for identifying noisy and duplicate ones. \n\n* The experiments lack some important details. Please see the Question section."}, "questions": {"value": "* The paper claims the denominator $Pr_{in} + Pr_{out}$ in eq 2 was chosen for \"numerical stability\" and cites \"Wu & Baleanu (2018)\". But it seems this is a paper on dynamical systems and fractional calculus, which has no discernible relevance to the statistical properties of estimator of the memorization score proposed in eq 2. Can you comment which specific result from Wu & Baleanu (2018) substantiate your claim of numerical stability?\n\n* Section 4.1 states, \"To evaluate whether memorization is necessary for generalization, we trained 50 models for each of the two conditions\". However, Appendix A states, \"During marginal utility, we train 10 models in for each threshold\". Is this a typo?\n\n* The caption for Table 2 states, \"CIFAR values are scaled by 100, while ImageNet values are scaled by 200 for readability\". Do the authors mean accuracies by \"value\"? If so, this note is incoherent. If the \"Mem Remove Acc (%)\" for ImageNet (e.g., 12.26) were scaled by 200, the true accuracy would be $12.26 / 200 = 0.0613\\%$, which is nonsensical.\n\n* The p-values reported in Table 2 are strange. For ImageNet/VGG19, the p-value is $6.06 \\times 10^{-45}$. For ImageNet/ResNet 18, it is $1.35 \\times 10^{-37}$. Achieving such astronomical levels from only 50 runs of a deep learning experiments implies a variance that is effectively zero. Is this due to using a high sampling rate of 0.95? \n\n* I'd like to see the results reported in Figure 5 and Table 2 using the original FZ score with sampling rate 0.95 as opposed to 0.7 used originally by Feldman and Zhang and see how it compares with the proposed approach. Overall, the ablation study needs to be improved to see whether all of the four components are essential or just one, e.g. high sampling rate is adequate. I am willing to increase my score depending on the authors' response to this particular question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jye1brdm47", "forum": "eAVPivm2jv", "replyto": "eAVPivm2jv", "signatures": ["ICLR.cc/2026/Conference/Submission9920/Reviewer_KoMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9920/Reviewer_KoMs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103110618, "cdate": 1762103110618, "tmdate": 1762921373879, "mdate": 1762921373879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}