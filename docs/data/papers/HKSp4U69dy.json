{"id": "HKSp4U69dy", "number": 13441, "cdate": 1758217956844, "mdate": 1759897437318, "content": {"title": "Adaptive Hopfield Network: Rethinking Similarities in Associative Memory", "abstract": "Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability.\nHowever, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness.\nWe reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process.\nConsequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin.\nThis perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories.\nTo this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval.\nWe theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased.\nWe integrate this mechanism into a novel adaptive Hopfield network (`A-Hop`), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.\nOur code is publicly available at https://anonymous.4open.science/r/Adaptive-Hopfield-Network-C137/.", "tldr": "Propose a adaptive similarity that captures context subtleness and strives for correct retrieval defined by variant distribution.", "keywords": ["similarity measure", "associative memory", "Hopfield network"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97583d92e546f68add854bee0ad9cacc36e2fc09.pdf", "supplementary_material": "/attachment/5ae38b813b7b220ea2b62e72f56d36aebd8835f4.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces an Adaptive Hopfield network (A-Hop) that learns a context-dependent similarity measure for content-addressable memory. \nBy reframing retrieval as a maximum a posteriori problem under a variant distribution, the authors derive an adaptive similarity function that approximates the likelihood of a stored pattern generating the query. \nThey prove that, with appropriate learned weights, \nA-Hop achieves optimal correct retrieval for three common variant types (noisy, masked, biased). \nEmpirically, integrating this adaptive similarity into Hopfield networks yields state-of-the-art performance on synthetic memory retrieval tasks and on downstream benchmarks (tabular data, image classification, multiple-instance learning), outperforming prior Hopfield variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles a fundamental issue (retrieval correctness vs. proximity) with a clear probabilistic framework.\n\n- Offers theoretical guarantees for optimal retrieval under well-defined generative scenarios (noise, masking, bias).  I skimmed through the proofs. Looks reasonable to me, but I didn't check them line-by-line.\n\n- Extensive experiments across diverse tasks, showing consistent improvements over strong baselines. Code provided. I skimmed through them. Looks reasonable to me. I didn't check line-by-line nor run them at my end."}, "weaknesses": {"value": "1. [major] Incremental novelty: Core idea is essentially learning a weighted distance (metric learning) for Hopfield retrieval similar to Wu & Hu et al 2024. It is a relatively straightforward extension of known concepts.\n\n2. [minor] Theory vs. implementation gap: Proofs assume per-dimension weighting (unsorted footprint) for optimal retrieval, but the actual method uses sorted feature footprints without a formal optimality guarantee for that sorting. Please correct me if I am wrong.\n\n3. [minor] Sorting overhead: Computing the similarity footprint requires sorting features ($O(d \\log d)$ or $O(n \\log n)$). This adds computational cost. The paper does not discuss runtime impact or scalability to very high-dimensional data.\n\n4. [very minor] Reliance on supervision: The adaptive similarity weights are learned using ground-truth associations (either known memory-query pairs or class labels). The method may not apply directly in fully unsupervised retrieval settings without a way to obtain such training signals.\n\n5. [very very minor, almost personal opinions] I kind of like this paper. It shows some solid efforts on theory and numerical validations. This might already be a good paper, but I really hope it is better, even great. But the clarity is not good enough in general. I feel it can be more precise and concise in many places. It will be great if the authors can polish more."}, "questions": {"value": "1. where did you validate the design choices in the adaptive similarity? if there is no, please add\n\n2. any runtime comparison (or complexity analysis)? please Report the computational overhead of A-Hop. should be at least provided to show how the $O(d \\log d)$ footprint computation scales with the dimensionality and number of memories. \n\nLLM disclaimer: I used LLM to polish my language."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q78ujCMiev", "forum": "HKSp4U69dy", "replyto": "HKSp4U69dy", "signatures": ["ICLR.cc/2026/Conference/Submission13441/Reviewer_ohC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13441/Reviewer_ohC5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777573918, "cdate": 1761777573918, "tmdate": 1762924065648, "mdate": 1762924065648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adaptive similarity metric to augment Hopfield Network-based storage and retrieval. It is a common observation that since the task is memory retrieval, if the query deviates from the stored patterns, then the retrieved pattern may not necessarily be 'semantically' close to the query. The authors analyze three particular query variants, namely, noisy variants of patterns, masked variants of patterns, and biased variants of patterns. The basic problem is to predict the most likely stored pattern given a query. The approach taken is to consider the query as a generative variant of the stored patterns and ask the question,  what is the maximum a posteriori probability that a given patttern is likely given the query. To estimate this, they propose an approach in which the similarity between query and stored pattern vectors are computed at the individual dimension level and the vectors are sorted. A combination of Euclidean and cosine distance are used and the softmax operator is applied to select the final stored pattern."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The beginning of the paper was on the right track in terms of addressing a problem since the current limitation of Hopfield networks was their ability to handle conceptual variants of the stored patterns. Modeling the query as a generative variant of the store patterns is also reasonable. Experimental results are indicating that the technique works under the modeled transformations of stored patterns. The proofs of the theorems are provided in the appendix which adds to some of the clarifications."}, "weaknesses": {"value": "While the approach was well-motivated, the method presented was a bit under-whelming. Why would sorting along each dimensions help in finding similarity. It is also known that such operations can bring dis-similar vectors close together as well and create distractions. The tabular results are too abstract to interpret and not enough time is devoted in the paper. Having an illustration of the method through visual examples in case of image retrieval would strengthen the understanding of the  method. \n\nThe definitions and theorems are stated but not proved in the text which is fine but no reference to the appendix section is made."}, "questions": {"value": "It would also be useful to address this problem in the context of cross-modal Hopfield retrieval, particularly using textual queries where it will be easier to show performance against truly conceptual variants. For example, if the stored model is a visual description is that of inhalator, queries such as inhaler should still be able to find them"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oAci16WKUR", "forum": "HKSp4U69dy", "replyto": "HKSp4U69dy", "signatures": ["ICLR.cc/2026/Conference/Submission13441/Reviewer_L7xN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13441/Reviewer_L7xN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855411435, "cdate": 1761855411435, "tmdate": 1762924064686, "mdate": 1762924064686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the fixed, proximity-based similarity measures in traditional Hopfield networks, arguing they fail to capture context-dependent associations and cannot guarantee \"correct retrieval\" of a query's true origin. To solve this, the authors introduce the Adaptive Hopfield Network (A-Hop), which reframes retrieval as a probabilistic problem governed by a \"variant distribution\". A-Hop replaces the fixed metric with a learnable \"adaptive similarity\" that is built from a similarity footprint of sorted, dimension-wise similarities. The key advantage is that A-Hop's similarity measure is not fixed. It learns from samples to align with the task's specific context, such as noise or masking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's first strength lies in its novel reframing of the retrieval problem in associative memories through a probabilistic framework, and go beyond traditional epsilon-retrieval to a new concept term as \"correct retrieval\". The goal becomes finding the memory pattern that is the most probable origin of the query, based on a clear mathematical definition: maximizing the a posteriori probability\n\n2. The authors demonstrate that A-Hop achieves state-of-the-art performance across a diverse set of four distinct tasks.\n\n3. The paper is in general well-written."}, "weaknesses": {"value": "1. My main concern is whether the proposed similarity footprint and adaptive similarity, $s(\\xi,x) = w^{\\top}U\\tilde{q}$, can approximate the posterior distribution, about which the author spends a whole subsection (3.1) discussing the novelty of framing the retrieval problem in this probabilistic way. Maybe I miss some part of the content, but I feel like it lacks a strong theoretical justification for why this specific basis (a linear combination of cumulative sums of sorted dimension-wise similarities) is a universal approximator for an arbitrary likelihood function $p_{\\nu}(x|\\xi)$.\n\n2. The authors write a detailed and insightful discussion section in the appendix (A.3), but this crucial context is never linked to or integrated into the main text. This discussion addresses fundamental questions (e.g., the trade-off between the provably optimal and the learnable formulation) that should be visible to the reader. It would be great if the authors could move the most critical parts of the discussion section into the main text, likely condensing and replacing some of the dense definition-heavy material in section 3.1."}, "questions": {"value": "1. Does the author aim to release their code? The current link provided in the paper leads to a folder containing only a readme file.\n\n2. Want to clarify the complexity of the proposed method compared to the modern Hopfield network. The proposed method requires a sort operation ($\\mathcal{O}(d \\log d)$) for each of the $N$ memory patterns during the similarity calculation. A modern Hopfield network (M-Hop) performs this in $\\mathcal{O}(Nd)$ ($d$ for dot product and need to repeat for N memory patterns), but A-Hop increases this to at least $\\mathcal{O}(Nd \\log d)$, is my understanding correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Web5qrlNc6", "forum": "HKSp4U69dy", "replyto": "HKSp4U69dy", "signatures": ["ICLR.cc/2026/Conference/Submission13441/Reviewer_67xP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13441/Reviewer_67xP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910461003, "cdate": 1761910461003, "tmdate": 1762924064138, "mdate": 1762924064138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}