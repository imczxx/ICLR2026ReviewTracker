{"id": "cTQ72zSohs", "number": 14629, "cdate": 1758240518704, "mdate": 1759897358562, "content": {"title": "DISCO: Dynamic Scheduling for CPU Offload in ML Workloads", "abstract": "An obvious way to alleviate memory difficulties in GPU-based ML workloads is via CPU offload, where data are moved between GPU and CPU RAM. While CPU offload is useful, it can greatly slow down a computation due to the relatively slow transfer rate between CPU RAM and GPU RAM. To address this, overlapping memory transfer and compute is a necessity. In this paper, we present a unique approach to CPU offload in ML workloads, called DISCO (**D**ynam**I**c **S**cheduling for **C**pu **O**ffload). DISCO views an ML workload as a fine-grained dataflow graph. Operations in the graph are individual kernel calls to be run on a specific GPU, CPU-to-GPU transfers, GPU-to-CPU transfers, and GPU-to-GPU transfers. DISCO makes use of a work-conserving, dynamic scheduler to asynchronously execute the operations in the graph, whenever the underlying resource is available and the system can be sure that executing the operation cannot violate the correctness of the computation. In this way, DISCO ensures that all resources—GPUs, CPU-to-GPU bus—are fully utilized.", "tldr": "", "keywords": ["CPU offload", "Memory Management", "Dynamic Scheduler"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cee4b8dcd932afd8eede5396ef26deac56c26bb2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, DISCO targets memory-constrained ML workloads that rely on CPU offloading. It unifies computation and data movement through a fine-grained dataflow abstraction called TASKGRAPH, which models both computation and transfer dependencies. During compilation, DISCO constructs a MEMGRAPH that encodes data and memory dependencies and inserts offload or reload operations as needed. At runtime, a work-conserving dynamic scheduler executes ready tasks asynchronously once dependencies are satisfied and resources are available, covering GPU kernels as well as C2G and G2G transfers to maximize compute–communication overlap and prevent GPU idle time caused by batch-level synchronization. The system performs simulated execution to determine tensor memory placement in advance and eliminate overwrite conflicts, avoiding dynamic cudaMalloc and cudaFree during execution. Results show that DISCO achieves lower first-token latency than ZeRO-Inference and FlexGen in most settings, supports single-sequence contexts up to 32K tokens without batch parallelism, and outperforms ZeRO-Infinity in several training configurations, demonstrating strong usability and low-latency advantages under constrained GPU memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The fine-grained unified abstraction combined with a work-conserving scheduler significantly improves compute–communication overlap and resource utilization. Both theoretical examples and empirical results (e.g., first-token latency) show clear advantages over layer-wise batch-synchronized execution.\n\n2. The compile-time construction of MEMGRAPH, which inserts memory dependencies and offload/reload operations, ensures correctness and parallelism. During execution, the system avoids dynamic GPU memory allocation, and its event-driven design with multiple CUDA streams minimizes scheduling and memory management jitter.\n\n3. The system demonstrates strong practicality under extremely constrained GPU memory and diverse parallel paradigms. It supports single-sequence contexts of up to 32K tokens, remains insensitive to batch size, and outperforms ZeRO-Infinity in multiple LoRA training settings."}, "weaknesses": {"value": "1. The evaluation primarily focuses on average latency and offline batch experiments, lacking a systematic analysis under realistic online workloads featuring bursty arrivals and mixed sequence lengths. In particular, there are no measurements of tail latency, continuous batching, stateful execution, or mechanisms such as rate limiting, prioritization, and preemption. Moreover, the study does not include end-to-end comparisons with recent mainstream inference stacks—such as vLLM and PagedAttention and FastGen under equivalent precision settings and operator stacks.\n\n2. During compile-time MEMGRAPH construction, the paper illustrates the process mainly with equal-sized slot examples, without addressing how the system handles heterogeneous tensor size distributions, bandwidth and latency asymmetries, or how it mitigates victim selection imbalance, fragmentation, and reload-induced jitter under such conditions.\n\n3. The paper also lacks systematic ablations that disable key components such as the work-conserving scheduler, fixed topological execution, or offload/reload insertion. As a result, it remains difficult to determine the primary sources of performance gains, whether the scheduler itself becomes a bottleneck at larger scales or higher concurrency, and where the system’s stable operating regime lies for larger batch sizes and model scales."}, "questions": {"value": "1. How does the system preserve an acyclic dependency graph and safe‑overwrite invariants under variable-size tensors or multiple LoRA instances, while remaining compatible with continuous batching and paged KV caching mechanisms as in vLLM?\n\n2. Please provide formal sufficient and necessary conditions for contention-free execution, along with upper bounds on compilation and allocation complexity. Clarify how victim selection mitigates jitter and thrashing under variable tensor sizes and heterogeneous interconnects, and specify the worst-case guarantees.\n\n3. Under identical precision and operator/stack configurations, compare throughput and tail latency (e.g., P95/P99) against related work,  such as FastGen or others. Include ablation studies that disable work-conserving scheduling and offload/reload insertion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fHkNf5EnNn", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_bT7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_bT7C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965630769, "cdate": 1761965630769, "tmdate": 1762925006183, "mdate": 1762925006183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DISCO, a system that enables efficient overlap between communication and computation through asynchronous execution using a work-conserving dynamic scheduler. DISCO specifically targets CPU offloading scenarios where communication overhead is significant. The work-conserving dynamic scheduler is based on MEMGRAPH, which captures both data and memory dependences. MEMGRAPH is built upon a TASKGRAPH, which represents the dataflow of multi-GPU computation and can be generated by existing frameworks like FlexFlow or Alpa. By simulating the execution of the TASKGRAPH, MEMGRAPH is generated by adding offload/reload vertices when needed and inserting memory dependences to avoid race conditions in shared memory locations. Experiments on language models such as LLaMA-7B and LLaMA-65B, conducted on A100 and P100 GPU servers, show that DISCO outperforms FlexGen and ZeRO-Inference in first token inference, as well as ZeRO-Infinity in LoRA training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tries to address a significant challenge that arises when training models under limited memory conditions. Also, experiment results show that DISCO outperforms existing baseline systems in LLM inference and LoRA training."}, "weaknesses": {"value": "The paper lacks a detailed explanation of how variable-sized tensors are handled. It merely states that the proposed algorithm, BUILDMEMGRAPH, does not change significantly for the “real-life” scenarios. However, without a detailed explanation, it remains unclear how the system addresses potential challenges such as fragmentation and simulation overhead that may arise in practical deployments.\n\n Also, the experiments are limited to a small set of workloads – only Llama is evaluated, the decode stage is not dealt with, and training is performed exclusively with LoRA. For the baseline systems, it would have been better to compare the proposed method with other general approaches mentioned in the Related Work section, such as pofo, AutoTM, and Checkmate. Finally, including an ablation study showing the time required to build MEMGRAPH through simulation would help demonstrate the practical usability of the system."}, "questions": {"value": "-\tThe paper says that DISCO was proposed because it is difficult to engineer solutions for ML workloads that are not as simple. However, the evaluation uses transformer models, which generally exhibit relatively regular workloads. This raises a question of what the paper considers to be a “simple workload,” and further clarification on this criterion would be helpful.\n\n-\tDISCO’s runtime value for sequence length 16K is not seen clearly in Figures 12 and 13.\n\n-\tA more detailed analysis of the evaluation results would be desirable. For example, why was the performance difference between ZeRO Infinity and DISCO much less pronounced on the P100 server?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HacsZ1L2ZY", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_gRdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_gRdB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979735400, "cdate": 1761979735400, "tmdate": 1762925005711, "mdate": 1762925005711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DISCO, a dynamic schedulng tool for CPU offloading of ML algorithms. DISCO helps address the issue of OOM errors, by better utilizing and pipelining information across the CPU and GPU, by constructing a memgraph and using for scheduling. The results are tested on both modern (A100) and older (P100) GPUs, which showcase its generality and how it can even resurrect older GPUs with limited memory to be practical for running LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ Design of the Memgraph and maintaining consistency is a challenge, and the authors did a good job to simplifiy and maintain dependencies while addressing the core memory challenge.\n+ I really liked the experiment of running a 7B param model on P100. It actually helps highlight the benefits of DISCO."}, "weaknesses": {"value": "- The technique isn't novel per se. But that is less important given the benefits provided and the memory wall we are hitting with AI these days."}, "questions": {"value": "- Will this be open source?\n- How is this related to CPU pipelining? Could a scoreboard-like technique be used for DISCO's scheduling management?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pty7njznYU", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_j77t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_j77t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996272884, "cdate": 1761996272884, "tmdate": 1762925005302, "mdate": 1762925005302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DISCO, a system to address performance bottlenecks from CPU offloading in ML workloads. The core contribution is a \"work-conserving\" dynamic scheduler that operates on a pre-compiled, fine-grained dependency graph MEMGRAPH. By scheduling tasks asynchronously whenever resources are free, DISCO aims to maximize the overlap between computation and I/O, thus improving overall resource utilization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The fine-grained, dynamic, \"work-conserving\" scheduler is an advancement over the static, coarse-grained pipelines used by current SOTA systems.\n\n- The MEMGRAPH-based approach is model-agnostic. It can handle any workload that can be expressed as a dataflow graph, making it general.\n\n- The paper provides empirical evidence that DISCO achieves significantly lower latency than SOTA systems in key tasks like LoRA training and first-token inference."}, "weaknesses": {"value": "- DISCO \"failed\" during the 65B model training, whereas the baseline (ZeRO Infinity) was \"more robust\". This raises significant questions about the practical feasibility and stability of this complex scheduling approach at scale.\n\n- The MEMGRAPH is generated statically before execution. This design seems ill-suited for dynamic workloads, particularly inference, where runtime bottlenecks like the dynamically growing KV Cache(as identified by work like TightLLM) change with every iteration. The paper does not adequately address how this static plan would adapt."}, "questions": {"value": "- Can you elaborate on the root cause of the 65B model \"failure\"? Does it stem from an inherent scalability bottleneck such as memory management in the dynamic scheduler itself?\n\n- How does your static MEMGRAPH design handle runtime-dynamic bottlenecks, such as the growing KV Cache in inference, where the I/O load changes with every iteration?\n\n- The paper's optimizations are focused entirely on the GPU-CPU RAM (Tier-2) bottleneck. However, for extreme-scale models, the true bottleneck may lie at Tier-3 (CPU-SSD). It would be interesting to know if DISCO's PCIe scheduling optimizations would provide any meaningful benefit when the entire system is bottlenecked by much slower storage I/O. What benefit would you expect your system to provide in a Tier-3 offloading scenario, where the primary bottleneck is orders of magnitude slower than the one you are optimizing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nanJO7HCPI", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_KCDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_KCDr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031910690, "cdate": 1762031910690, "tmdate": 1762925004777, "mdate": 1762925004777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DISCO introduces a runtime and scheduling approach for memory-constrained ML workloads that rely on CPU offloading. Instead of processing models layer-by-layer, DISCO represents computations as a fine-grained dataflow graph and uses a work-conserving dynamic scheduler to overlap CPU–GPU transfers and GPU compute, to ensure that resources are not idle."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Efficient CPU offloading is an important challenge"}, "weaknesses": {"value": "* Difference between standard double buffering unclear. The need for overlapping between compute and communication is a very well-known systems optimization. Figure 3 is misleading because it is a highly unoptimized version of how CPU offloading should work. It is unclear what the novelty of the proposed system is from the paper. \n* The Related work simply lists a set of prior works but does not clearly articulate what shortcomings these works all have that the proposed paper addresses. \n* There is no motivation section that demonstrates in state-of-art ML frameworks that this underutilization of resources actually occurs. \n* There is only one model evaluated (LLAMA). This is insufficient for a work that aims to address a system-level bottleneck.\n* It is unclear what the baseline serving framework is and the need for CPU-offloading is also not clear in the baseline system. Does it have insufficient memory resources for inference and fine-tuning? \n* It is not clear what a \"level\" of a neural network is. Does this mean a layer?"}, "questions": {"value": "Please see under weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f4VZvDWmTr", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_TxhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_TxhG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113855068, "cdate": 1762113855068, "tmdate": 1762925004177, "mdate": 1762925004177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DISCO, a novel system for dynamic scheduling of CPU offload in machine learning workloads. Unlike traditional bulk-synchronous approaches, DISCO models ML workloads as fine-grained dataflow graphs and uses a work-conserving dynamic scheduler to overlap memory transfers and computation. The key contribution of the paper is introducing MEMGRAPH and the corresponding algorithm for constructing MEMGRAPH, a DAG that includes data dependency and memory dependency to allow dynamic scheduling along with correctness. The proposed approach demonstrates a significant speedup over existing approaches for inference in memory-constrained systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on an important problem: improving resource utilization for machine learning workflows by improving the overlap of CPU-GPU communication and GPU operations. \n2. The proposed approach of using a DAG with all dependencies captured is a generalized approach and can be used for different types of models.   \n3. The paper is well written, and the explanation of MEMGRAPH construction is easy to follow\n4. The algorithm proposed for MEMGRAPH construction is lightweight."}, "weaknesses": {"value": "1. The interaction of proposed dynamic scheduling with different GPU parallelism strategies (e.g., model parallelism), which might require synchronization after each block, is not clear.\n2. The paper proposes to use a dynamic work-conserving scheduler, but does not provide much detail on it. Overheads of graph construction and scheduling are unclear as well\n3. DISCO faces more OOM compared to existing work, hence its usefulness for training is unclear"}, "questions": {"value": "I have the following questions for the authors: \n1. Impact of model parallelism: Several model parallelism strategies require GPU synchronization at the end of the layer, which might limit the scope and impact of the proposed asynchronous execution. Furthermore, I could not find details on what parallelism strategy is used for the evaluation section, which makes it hard to evaluate the interaction with parallelism strategies. \n2. Example given in Figure 3 does not seem to account for the required synchronization before executing layer 2 for the tensor parallelism strategy.  \n3. Can you provide more details on the dynamic work-conserving scheduler? What is the overhead of this dynamic scheduler? Does it make scheduling decisions at runtime (*dynamic* work might be confusing here) or at compile time?\n4. How does the size of this MEMGRAPH and the overhead of constructing/scheduling increase with the model size and the cluster size (e.g., training with 1024 GPUs)?\n5.  Can you provide more details on the training results? The paper mentions that ZeRO Infinity was more robust to larger batch sizes and DISCO fails. It is unclear why DISCO results in OOM more often than the prior work, and how DISCO can be improved for training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e07FcAcT9N", "forum": "cTQ72zSohs", "replyto": "cTQ72zSohs", "signatures": ["ICLR.cc/2026/Conference/Submission14629/Reviewer_Hcji"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14629/Reviewer_Hcji"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission14629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142901777, "cdate": 1762142901777, "tmdate": 1762925003845, "mdate": 1762925003845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}