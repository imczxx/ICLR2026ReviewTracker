{"id": "5qd7V5TNGV", "number": 2320, "cdate": 1757058264070, "mdate": 1759898156121, "content": {"title": "CP4D: Compositional physics-aware 4D scene generation", "abstract": "4D generation ($\\textit{i.e.}$, dynamic 3D generation) has recently emerged as a rapidly growing research frontier due to its powerful spatiotemporal modeling capabilities. However, despite notable advances, existing approaches typically fail to capture the underlying physical principles, producing results that are both physically inconsistent and visually implausible. To overcome this limitation, we present CP4D, a novel paradigm for photorealistic 4D scene synthesis with faithful adherence to complex physical dynamics. Drawing inspiration from the compositional nature of real-world scenes, where immutable static backgrounds coexist with dynamic, physically plausible foregrounds, CP4D reformulates 4D generation as the integration of a static 3D environment with physically grounded dynamic objects. On this basis, our framework follows a three-stage pipeline: $\\textbf{1)}$ Firstly, we leverage pre-trained expert models to generate high-fidelity 3D representations of the environment and foreground objects respectively. $\\textbf{2)}$ Subsequently, to produce physically plausible trajectories and realistic interactions for these objects, we propose a hybrid motion synthesis strategy that integrates priors from physical simulators with the common sense embedded in video diffusion models. $\\textbf{3)}$ Finally, we develop an automated composition mechanism that seamlessly fuses the static environment and dynamic objects into coherent, physically consistent 4D scenes. Extensive experiments demonstrate that CP4D can generate explorable and interactive 4D scenes with high visual fidelity, strong physical plausibility, and fine-grained controllability, significantly outperforming existing methods.", "tldr": "", "keywords": ["4D Generation; Physica-aware; Compositional"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b2e44a542109e543d1cd0f404a52a5f25ed0c9f.pdf", "supplementary_material": "/attachment/574bc52d61bfdbecb5f8abcdf4a2f7d613f905ea.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CP4D, a compositional, physics-aware framework for 4D scene generation. The core idea is to decouple a static 3D background from physically grounded dynamic foreground objects, following a three-stage pipeline: (1) generate high-fidelity 3D representations for background and foreground with pre-trained expert models; (2) synthesize motion via heterogeneous physics simulators (MPM for elastic/flexible, rigid-body, and PBD for fluids) and then refine with video-diffusion SDS; (3) automatically compose foreground into background using monocular-depth–based position initialization and a camera-frustum–based scale heuristic, followed by optimization.\n\nExperiments compare CP4D with physics-driven simulators, conditional video generators, and text-to-4D baselines, using VBench/WorldScore and an LLM-assisted evaluation; ablations indicate material and position optimization both contribute."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The pipeline of CP4D is clear and easy to follow. And for method, combining VLM-assisted physical initialization with SDS refinement is an interesting hybridization."}, "weaknesses": {"value": "1. The motivation of CP4D is dividing static background and dynamic foreground, which is not novel in 4D generation area, such as [1,2,3].\n2. The biggest concern of the pipeline is robustness. The pipeline depends on many off-the-shell models, especially the monocular depth  and a frustum heuristic part. The authors should give more examples to support the robustness of CP4D, while now only 17 simple prompts are listed.\n3. The video results in supplementary material are not convincing, all cases just include very shot clip, and multiview examples do not show big camera motion exchange.\n4. There is a typo in line75, where foreground is spelled as \"foareground\".\n\n[1]. Comp4d: Llm-guided compositional 4d scene generation. Arxiv 24.03. Xu et al.\n[2]. Compositional 3d-aware video generation with llm director. Nips2025. Zhu et al.\n[3]. DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing. CVPR2024. Liu et al."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ziLAR1jNAE", "forum": "5qd7V5TNGV", "replyto": "5qd7V5TNGV", "signatures": ["ICLR.cc/2026/Conference/Submission2320/Reviewer_826b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2320/Reviewer_826b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557425663, "cdate": 1761557425663, "tmdate": 1762916192750, "mdate": 1762916192750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a physical grounded 4D generation pipeline. Given a text prompt, it can synthesize dynamic scenes composed of static background and foreground object with physically plausible motion. The proposed pipeline comprises three stages: In the first stage, it generates separate foreground and background 3D representation using existing image generation/editing/segmentation, and 3D reconstruction/generation models. In the second stage, it initializes the physical parameters and external force via VLM, then simulate motions for diverse object with heterogeneous physical solvers and refine the estimated parameters through a SDS loss. In the last stage, the relative scale and translation of foreground and background objects are determined via depth-aware heuristics and photometric optimization for adequate composition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is clearly written and easy to follow.\n2.\tEvaluation on 17 representative prompts demonstrates that the proposed framework can generate physically plausible 4D scenes containing foreground objects with diverse materials."}, "weaknesses": {"value": "1.\tThe main concern is about the limited technical novelty. The proposed framework is basically the direct combination of existing components without touching their own limitations. While it attempts to integrate multiple solvers for different materials, the integration is largely naïve and lacks rigor evaluation for specific materials. For example, the simulation for fluid objects is only shown in the fourth video of the anonymous webpage with two raindrops, while they exhibit strange elastic behavior (bounce off the ground) due to the naïve boundary constraint handling.\n2.\tDespite being able to generate plausible motion, the proposed framework does not consider complex lighting interaction in the composition, so describing it photorealistic or visually realistic is somewhat inappropriate. Actually, this has already explored in previous physically grounded generation works such as PhysGen3D. The proposed “automated composition” mainly consists of some engineering heuristics—given $I_{b,f}$ and trusting their monocular depth, this task seems relatively trivial.\n3.\tThis framework still relies on simplified assumptions of uniform material, limiting its scalability beyond simple objects and motions, and making the comparison with general-purpose video generation models somewhat unfair.\n4.\tThe input of noise estimator in Equation (1) and (4) should be related to $\\epsilon$。\n5.\tL093 claims that the proposed framework can avoids “realistic environments juxtaposed with cartoon-like objects” compared to text-to-3D alternatives. But the adopted text-to-image-to-3D approach can only mildly constrain the style of single input view. The cartoon style of generated assets largely stems from the training data distribution of 3D generation models. \n6.\tNo substantial novel-view rendering is provided, making it difficult to assess the 3D consistency."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SOcTwRYuJ3", "forum": "5qd7V5TNGV", "replyto": "5qd7V5TNGV", "signatures": ["ICLR.cc/2026/Conference/Submission2320/Reviewer_9Hsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2320/Reviewer_9Hsn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582304866, "cdate": 1761582304866, "tmdate": 1762916192223, "mdate": 1762916192223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a compositional framework for 4D scene generation focused on physical plausibility by decoupling scenes into static 3D backgrounds and dynamic, physically-grounded 3D foreground objects. The three-stage pipeline first synthesizes 3D assets using a cascade of pre-trained models (LLM, T2I, Image-to-3D). Next, a hybrid motion strategy generates coarse trajectories using physical simulators (MPM, PBD) and then refines them using priors from video diffusion models (SDS loss). Finally, the framework automatically composes the scene, using monocular depth estimation and optimization to set the scale and position of foreground objects."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The task is worse investigating."}, "weaknesses": {"value": "1. The dynamic view demos contains only zoom-in zoom-out motions, no camera pose/view angle change, the Static novel-view generation results seems just a crop from the original view. And yet the task is called 4D scene generation.\n\n2. The foreground and background are almost completely irrelavent in the final results, the authors only did a ground/plane estimation to put the foreground to the corresponding postions but no interactions with the background.\n\n3. Comparisons are not convincing, the sora and wan results are too bad compare to my experiences, looks like a reverse cherry picking. And other physic aware 3D/video generation methods provides much better visual qualities in their demos, I highly doubt the fidelity of the Vbench and worldscore results in Table1 and Table 2.\n\n4. The proposed method is a highly complex cascade of numerous expert models (LLM, T2I, Image Edit, SAM, I23D (Trellis, Viewcrafter), VLM, Depth Estimator, and Video Diffusion). This pipeline-of-pipelines has many potential points of failure. \n\n5.  In section 4.2, the author admitted that the VLM predicted parameteres are not accurate and need refinements from video diffusion model, which is counter-intuitive to the core idea of the framework the author proposed in the first place."}, "questions": {"value": "1. Since the task is text guided and the foreground objects are generated, why bother use a VLM to determine parameters like Young’s modulus, Poisson’s ratio µ, and density from your rendered generated 3D objects multi-view? The poor textures in the examples might introduce more noises."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hk8GvRVCpE", "forum": "5qd7V5TNGV", "replyto": "5qd7V5TNGV", "signatures": ["ICLR.cc/2026/Conference/Submission2320/Reviewer_1sfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2320/Reviewer_1sfj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990563684, "cdate": 1761990563684, "tmdate": 1762916191816, "mdate": 1762916191816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The CP4D framework addresses limitations in existing 4D scene generation by ensuring faithful adherence to complex physical dynamics. It uses a compositional paradigm that integrates static 3D environments with physically grounded dynamic objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "CP4D presents a novel compositional framework for photorealistic 4D scene generation, emphasizing faithful adherence to complex physical dynamics. The method integrates static 3D environments with physically grounded dynamic objects. Contributions include a hybrid motion synthesis strategy combining physical simulators and video diffusion priors for plausible trajectories and realistic interactions, and an automated composition mechanism that seamlessly fuses scene elements."}, "weaknesses": {"value": "The primary limitation of the CP4D framework is the relatively long runtimes required to generate a complete physically realistic 4D scene. This inefficiency is due to the adoption of a stage-wise optimization strategy. Furthermore, the complexity arises because initial physical parameters estimated by Vision-Language Models (VLMs) often lack numerical accuracy. The approach must also address physics solvers' reliance on coarse grid approximations, which can lead to perceptually implausible outcomes such as \"spurious collisions\" or \"phantom contacts\""}, "questions": {"value": "Could the authors elaborate on the composition and key features of this planned dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2xTxmqO40d", "forum": "5qd7V5TNGV", "replyto": "5qd7V5TNGV", "signatures": ["ICLR.cc/2026/Conference/Submission2320/Reviewer_DzxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2320/Reviewer_DzxA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045394450, "cdate": 1762045394450, "tmdate": 1762916191213, "mdate": 1762916191213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}