{"id": "Ric2If6Xur", "number": 6643, "cdate": 1757991147184, "mdate": 1763736095913, "content": {"title": "Difficulty-Aware Reasoning for Mobile GUI Automation via Reinforcement Fine-Tuning", "abstract": "Automating GUI tasks remains challenging due to layout complexity, element density, and intent ambiguity, which requires effective and efficient reasoning to facilitate each operation. Existing agents typically employ a uniform chain-of-thought (CoT) reasoning process for all actions, a one-size-fits-all approach that incurs unnecessary computational overhead and even performance degradation on trivial steps.\nTo address this, we introduce \\textbf{AdaGUI-R1}, a GUI agent that pioneers a difficulty-aware reasoning paradigm by dynamically modulating its reasoning depth based on action complexity. Our methodology consists of reasoning inducing and reasoning enhancing.\nDuring reasoning inducing, we introduce a self-supervised mechanism to generate high-quality, difficulty-aware reasoning trajectories. Fine-tuning on this curated data endows the agent with the fundamental capability to adjust its reasoning depth according to action complexity. Subsequently, Group Adaptive Policy Optimization (GAPO) algorithm is implemented to enhance reasoning performance. It leverages an adaptive thought reward to encourage thinking on challenging steps, and a novel exploration reward with a difficulty-aware Gaussian bandwidth to improve action accuracy.Extensive experiments demonstrate that AdaGUI-R1 sets a new state-of-the-art. It concurrently reduces unnecessary reasoning tokens by 40% while improving action accuracy by 5%, underscoring the power of adaptive reasoning in GUI automation.", "tldr": "", "keywords": ["GUI agent", "reinforcement fine-tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de272b9175d65147aa1502cb85f6129be16115bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper *“Difficulty-Aware Reasoning for Mobile GUI Automation via Reinforcement Fine-Tuning (AdaGUI-R1)”* introduces a new framework for mobile GUI agents that can dynamically adjust how much reasoning they perform based on the difficulty of each step. Traditional GUI automation models apply uniform reasoning chains to all tasks, which leads to inefficiency—simple steps are overanalyzed while complex ones lack adequate reasoning. AdaGUI-R1 addresses this by integrating **difficulty-aware reasoning** into both supervised and reinforcement fine-tuning stages.\n\nIn the first stage, the model learns when to think by generating “Think–Action” pairs only for difficult steps, while easy ones receive a placeholder “None” thought. A self-supervised consistency mechanism ensures that generated reasoning aligns with the correct actions. In the second stage, the authors propose Group Adaptive Policy Optimization (GAPO), which introduces two key rewards: an adaptive thought reward that encourages longer reasoning for hard steps and shorter for easy ones, and a Gaussian exploration reward that provides smoother, distance-based feedback for click actions with difficulty-sensitive variance.\n\nExperimental results on multiple mobile GUI benchmarks show that AdaGUI-R1 outperforms prior state-of-the-art methods, achieving around **5% higher action accuracy** while reducing reasoning token usage by **about 40%**. The study demonstrates that allocating reasoning effort adaptively—thinking deeply only when needed—can improve both efficiency and robustness in GUI automation agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper contributes a new difficulty-aware reasoning paradigm for GUI automation. Instead of applying a uniform Chain-of-Thought across all tasks, it introduces a principled way to adjust reasoning depth based on estimated step difficulty. This rethinking of how reasoning effort should be distributed marks a clear conceptual advancement over prior “one-size-fits-all” reasoning frameworks. The work provides several concrete and novel algorithmic components:  \n   - A self-supervised CoT generation mechanism that ensures consistency between thought and action.  \n   - The Group Adaptive Policy Optimization (GAPO) algorithm, integrating adaptive thought rewards and Gaussian exploration rewards.  \n\n   Together, these elements enhance stability, exploration efficiency, and reasoning adaptability, forming a cohesive and technically sound framework.\n\nAdaGUI-R1 achieves substantial performance improvements on multiple GUI automation benchmarks, increasing success rates while reducing reasoning token usage by about 40%. These empirical gains demonstrate that adaptive reasoning not only improves efficiency but also sets a foundation for broader applications in multimodal and interactive AI systems."}, "weaknesses": {"value": "1. The paper primarily evaluates the model on multiple offline benchmarks, where the metrics focus on step-level accuracy. However, these benchmarks differ from the more commonly used online interactive benchmarks (e.g., AndroidWorld, AndroidLab) that measure full-task success rates (SR). It is recommended to include a discussion on how the proposed method relates to these online benchmarks, and to report additional results of AdaGUI-R1-7B and its ablation models on such interactive benchmarks. Demonstrating effectiveness on SR metrics would significantly strengthen the paper’s empirical validity.\n\n2. This paper emphasizes step-level difficulty awareness, with extensive design innovations in the “think” component compared to prior work. It would be beneficial to provide several case analyses, including examples of how steps are categorized by difficulty, and how the thought content changes before and after training with the Thought Reward mechanism. Such qualitative insights would clarify the behavioral impact of the proposed difficulty-aware design.\n\n3. The Action Exploration Reward section improves upon the conventional binary (0/1) feedback by introducing a Gaussian Exploration Reward, which smooths the reward function for individual actions. However, computing Gaussian functions introduces additional computational cost compared to previous approaches. It is recommended to provide comparative experiments—such as evaluating against baselines that use bounding-box inclusion or distance-based penalty smoothing—to demonstrate that this extra computational overhead yields meaningful performance benefits."}, "questions": {"value": "The questions are already included within the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1nlqhakIRW", "forum": "Ric2If6Xur", "replyto": "Ric2If6Xur", "signatures": ["ICLR.cc/2026/Conference/Submission6643/Reviewer_issC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6643/Reviewer_issC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197709395, "cdate": 1761197709395, "tmdate": 1762918959859, "mdate": 1762918959859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AdaGUI-R1, a mobile GUI agent that introduces difficulty-aware reasoning, dynamically adjusting its reasoning depth based on task complexity. The method integrates a self-supervised CoT generation process to produce consistent reasoning-action pairs and a Group Adaptive Policy Optimization (GAPO) algorithm with adaptive thought and exploration rewards. Experiments on multiple GUI automation benchmarks demonstrate significant improvements in both accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Innovative reward design: The paper introduces a difficulty-aware reward mechanism that assigns different reward functions to actions of varying difficulty, effectively aligning reasoning depth with task complexity.\n\n- Strong empirical results: The proposed AdaGUI-R1 achieves strong performance on three benchmarks, surpassing prior models in both accuracy and efficiency."}, "weaknesses": {"value": "- More qualitative examples (e.g., reasoning traces comparing easy vs. hard steps) are required.\n- Lack of novelty in \"Self-Supervised CoT Generation\": The proposed \"self-supervised CoT generation\" closely mirrors the STaR [1] approach and does not introduce a fundamentally new mechanism. The pipeline of generating initial CoT, validating actions, and revising reasoning is nearly identical to prior methods.\n- Soft reward on coordinates is not new: Using spatially smoothed rewards for click or grounding actions has been explored in previous RL-based GUI grounding works.\n\n[1] STaR: Bootstrapping Reasoning With Reasoning."}, "questions": {"value": "- What is the performance of model after SFT.\n- According to the experimental results, the average reasoning token length of AdaGUI-R1 is less than 20 tokens, which suggests that the model performs almost no explicit reasoning. So does it mean that it is enough to train the model to predict actions? And have you observed that the decrease in thought length is mainly due to the omission of content?\n- What content or reasoning elements are being omitted in the reduction of cot length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quLVpW8om0", "forum": "Ric2If6Xur", "replyto": "Ric2If6Xur", "signatures": ["ICLR.cc/2026/Conference/Submission6643/Reviewer_HLM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6643/Reviewer_HLM6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761458344510, "cdate": 1761458344510, "tmdate": 1762918959464, "mdate": 1762918959464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Automating GUI tasks is challenging because of the task complexity. State-of-the-art employs chain-of-thought reasoning in order to deal with such complexity; however, they equally apply the reasoning protocol without considering the difficulty or complexity of the sub-tasks. This results in computational inefficiency because of applying unnecessary reasoning steps for trivial sub-tasks. Also, such a one-fit-for-all approach results in performance degradation, especially in complex subtasks, since a fixed number of reasoning steps may not be enough for some complex sub-tasks, while being more than enough for trivial ones. This paper introduces a difficulty-aware reasoning, which adapts the depth of reasoning to the action complexity. The core idea is determining the difficulty of subtasks via a pre-trained VLM for GUI tasks. Then, a reward component is employed to reward the model for thinking longer for harder tasks and shorter for easier ones. The proposed method shows performance improvements over baselines for GUI tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Difficulty-aware Reasoning**: Determining the difficulty and encouraging the agent to think longer for harder sub-tasks for GUIs looks promising for GUI agents. \n- **Improvements over Baselines**: Results are promising, showing significant improvements over the base model they compared.\n- **Reward Components**: Reward components are ablated nicely, and it is shown that each component helps the agent to reach a better performance.\n- **Difficulty-threshold Analysis**: The effects of the difficulty threshold, which takes a key role in $R_{thought}$, on the model performance is analyzed nicely."}, "weaknesses": {"value": "**Major**:\n- **Novelty**: The paper's claims and coverage are scoped entirely to GUI automation, and the key contribution described as the difficulty-aware CoT. However, there are already works in literature where CoT is adapted based on the task difficulty (see below). These works must be discussed in detail, and the proposed approach should be compared with them, since the novelty of the proposed method is questionable beyond the experimental setting.  \n\n[a]: Waheed, Abdul, et al. \"Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation.\" arXiv preprint arXiv:2509.05226 (2025).\n\n[b]: Yu, Zishun, et al. \"Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization.\" Forty-second International Conference on Machine Learning.\n\n[c]: Wang, Xinglin, et al. \"Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning.\" Findings of the Association for Computational Linguistics: NAACL 2025. 2025.\n\n[d]: Han, Tingxu, et al. \"Token-budget-aware llm reasoning.\" arXiv preprint arXiv:2412.18547 (2024).\n\n[e]: Aggarwal, Pranjal, Aman Madaan, and Yiming Yang. \"Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs.\" Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.\n\n[f]: Damani, Mehul, et al. \"Learning How Hard to Think: Input-Adaptive Allocation of LM Computation.\" The Thirteenth International Conference on Learning Representations.\n\n- **Computational Overhead**: The proposed approach requires difficulty estimation and CoT pre-training. However, the computational overhead over the baselines are not discussed. This must be clearly elaborated, and equal-compute comparisons and a quality–cost curve (tokens/FLOPs/wall-clock) should be presented.\n- **Discrete Difficulty Levels**: It is unclear why only five discrete difficulty levels are selected. This design choice is not explained.\n- **CoT Pre-training**: It is mentioned that they first teach how to generate CoT to their agent, using a curated, annotated dataset. The details of such fine-tuning and the data size are unclear. \n- **Reward Design**: It is unclear how R_{thought} is designed.  It would also be great to show the impact of this reward design on the model's performance by comparing it with simpler designs.\n- **Experimental Results**: In the experimental results in Table 1, for AITZ, UI-TARS-7B works better than the proposed approach. However, these results are not written in bold; instead, the results of the proposed approach are written in bold, which is misleading. This must be corrected. Also, please elaborate on why UI-TARS-7B performs better than the proposed approach.\n- **Confidence Intervals**: No confidence intervals are presented for the results in Table 1. Please report the confidence intervals either in the main table or in the appendix. This is important since it looks like UI-TARS-7B performs closely to the proposed approach.\n\n**Minor**: \n- **Presentation**: Figure 1-right is very unclear. Please either explain what this figure shows in the caption in detail or remove it."}, "questions": {"value": "- In figure 1-right, why do we have multiple bars for different levels? What are these levels, the difficulty levels according to Eq. 4?\n- Why are there five discrete difficulty levels? Have you ever considered continuous difficulty levels, or are there any drawbacks to using them?\n- Please explain how you teach \"how\" and \"when\" to generate CoT to your agent. Can you also elaborate on the data curation and the size of the data used for such pre-training?\n- How is R_{thought} designed? It is mentioned that the function is smooth and strictly monotonic, but is this the only design consideration? Have you ever considered different functions?\n- Why does UI-TARS-7B perform better than the proposed approach in AITZ? This must be clearly elaborated.\n- Can you please report confidence intervals for the results in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xuWrphun7F", "forum": "Ric2If6Xur", "replyto": "Ric2If6Xur", "signatures": ["ICLR.cc/2026/Conference/Submission6643/Reviewer_GsMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6643/Reviewer_GsMa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898752008, "cdate": 1761898752008, "tmdate": 1762918959046, "mdate": 1762918959046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces reinforcement learning fine-tuning algorithm that induces difficulty-aware reasoning. The intuition is that the model should \"reason\" only in difficult states.\n\nThe proposed method works in two stages:\n1. Inducing reasoning using supervised fine-tuning.\n2. Refining reasoning to be difficulty-aware through a novel reinforcement learning algorithm: GAPO that defines rewards based on the difficulty of the decision in that state. Precisely, during fine-tuning an agent is penalised when providing an answer to a difficult step without reasoning, or for too much \"thinking\" in low-difficulty states.\n\nIn the first stage the difficulty is measured by another VLLM, while in the second stage the difficulty is computed on the fly based on the group of generated samples.\n\nThe suggested method outperforms other recent grounding models with or without reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive set of ablations for all components of the algorithm.\n- Cost analysis demonstrating efficient use of tokens (a performance / tokens 2D plot would be useful for visualisation here)\n- Strong empirical results."}, "weaknesses": {"value": "1. No direct comparison with other adaptive reasoning methods.\n2. Reduced novelty: adaptive thinking has been introduced before."}, "questions": {"value": "1. Why isn't the current solution compared to AdaptThink, AdaCoT, ThinkSwitcher... works that were correctly mentioned in Section 2?\n2. When computing the difficulty level $l$ during \"enhancement\", rather than sampling multiple times and counting how many generations exactly match the target, wouldn't be easier to measure the log-probs of the correct answer against a threshold?\n3. Do all the models in Table 1 follow the same protocol of using half of the data for training and half for testing. Is the split the same?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e1KQWTK1H8", "forum": "Ric2If6Xur", "replyto": "Ric2If6Xur", "signatures": ["ICLR.cc/2026/Conference/Submission6643/Reviewer_GfcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6643/Reviewer_GfcT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948295159, "cdate": 1761948295159, "tmdate": 1762918958498, "mdate": 1762918958498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to create a difficulty aware reasoning paradigm that aims to reduce token budget and adaptively reason longer on hard steps and avoid reasoning longer on easy ones. The paper overall manages to reduce the token budget and not reduce performance at the same time. It is a timely and interesting work  that aims to increase the metacognitive abilities of the agent by helping in its resource allocation of thinking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the problem of thinking budget of the agent thereby increasing the metacognitive aspects of decision making in GUI environments. The paper also increases the efficiency of exploration of agent in a high dimensional setting as GUIs by including an action exploration reward for the hard steps where reward is sparse and making it more dense with a Gaussian exploration reward\nAblations and experiments seem robust."}, "weaknesses": {"value": "Even though the agent is advertised as self supervised, it does require ground truth labels from existing datasets that are human annotated and in that the approach isn’t general or scalable. \n\nThere's a possibility that there is a lot of redundancy in these datasets, so I am not sure the degree of generalization that this approach promises. In general, on exploration task fine-tuning on similar trajectories improve performance. Hence results could also be explained by data leak, as the model get finetuned  on half the trajectories randomly selected. So some sort of stratified split up of the tasks in these datasets showing some analysis would be helpful to understand as in where exactly the performance is coming from. \n\nSome figures are not referenced in the text."}, "questions": {"value": "In line 399, the paper claims and I quote “not only improves the accuracy of hard steps, but also avoids the hallucination triggered by introducing over analysis in easy steps”, this is a serious claim but I would like to see some evidence from some analysis which you could do to show it.\n\nThere are eight actions: key, click, swipe, long press, type, system button, terminate, and wait. Is the gaussian exploration function helpful for all these actions or is it limited to some?\n\nAny reason why this algorithm was tested only on mobile GUI environments only (not a demerit of the paper)?\n\nAny reason you went for an easy/hard dissociation instead of a graded difficulty thinking budget. What I mean is it could have easily been a continuous function ( like agreement of high accuracy outputs) of difficulty adaptation instead of just these two distinct categories.\n\nHow do you ensure that the agent doesn't reward hack the length bonus and increases the token budget unnecessarily for hard problems which such a long length might not be needed. It could be that you are managing to reduce the token budget for easy steps but increasing it for harder ones unnecessarily with your design choices.\n\nI would be willing to revise my scores if you could answer these questions satisfactorily and address weakness.Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R5cxK2MBRU", "forum": "Ric2If6Xur", "replyto": "Ric2If6Xur", "signatures": ["ICLR.cc/2026/Conference/Submission6643/Reviewer_1NDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6643/Reviewer_1NDW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994362283, "cdate": 1761994362283, "tmdate": 1762918957781, "mdate": 1762918957781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}