{"id": "82XCBdf8G2", "number": 16297, "cdate": 1758262871912, "mdate": 1759897249423, "content": {"title": "DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Foundation Models", "abstract": "The content-agnostic, fixed-grid tokenizers used by standard large-scale vision models like Vision Transformer (ViT) and Vision Mamba (Vim) represent a fundamental performance bottleneck, creating a trade-off between capturing fine-grained detail and suffering from redundant computation. To resolve this dilemma, we introduce DART, a fully differentiable Dynamic Adaptive Region Tokenizer. DART employs learnable region scores and quantile-based partitioning to create content-aware patches of varying sizes, intelligently allocating a higher token density to information-rich regions. The impact of this approach is profound: it unlocks a more intelligent scaling paradigm, where a DART-equipped DeiT-Small (22M parameters) matches the performance of a DeiT-Base (86M) with nearly double the inference speed by efficiently capturing high-resolution details in key regions. Furthermore, the principle of adaptive tokenization proves its generality with clear benefits in dense prediction and spatiotemporal video tasks. We argue that by resolving the tokenizer bottleneck at its source, adaptive tokenization is a key component for building the next generation of more efficient and capable foundation models for multimodal AI, robotics, and content generation.", "tldr": "We introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART) module that adaptively partitions the image by combining learnable region scores with piecewise differentiable quantile operations.", "keywords": ["Tokenization", "Vision Fundation Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4197a8812ade493380b1021bd24265bace6eee44.pdf", "supplementary_material": "/attachment/d3e6930114b5e27286e70a2270e09360c04495ce.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a content-aware tokenizer that addresses the inherent inefficiencies of traditional fixed-grid tokenizers used in vision models. DART improves computational efficiency by concentrating resources on high-information regions, while reducing redundancy in low-information areas. A lightweight CNN network is adopted to predict the scores to evaluate the importance of the image. The proposed method is versatile, demonstrating substantial improvements across tasks such as image classification, dense prediction, and spatiotemporal video classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The figures are visually clear and aesthetically pleasing, and the narrative is easy to follow, with ideas presented in a straightforward manner.\n\n2. The experimental details are comprehensive and well-documented.\n\n3. The overall method is technically sound. The motivation is reasonable."}, "weaknesses": {"value": "1. The paper lacks a formalized mathematical description of the proposed method, and it would benefit from a detailed architectural diagram of the entire module. Additionally, the implementation relies heavily on existing, well-established components, which could be seen as limiting its originality.\n\n2. The number of baselines used to validate the effectiveness of the proposed module is insufficient, making it difficult to demonstrate its superiority conclusively.\n\n3. In Table 4, the introduction of the module clearly increases the model's parameters. Experiments compensating for the added parameters are needed to show that the performance improvement is not merely due to the increased parameter count.\n\n4. The datasets used for validation are not widely recognized or commonly used in the field. It would strengthen the paper to include more general-purpose datasets familiar to the broader research community.\n\n5. A similar idea for adaptive token sampling is seen in other ViT papers, such as progressive sampling, token merging in ViT, and deformable ViT. Thus, it weakens the paper's contribution.\n\n6. The effectiveness is strongly encouraged to be validated in other areas, such as the LLM-related domain."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zFCSB2P3zZ", "forum": "82XCBdf8G2", "replyto": "82XCBdf8G2", "signatures": ["ICLR.cc/2026/Conference/Submission16297/Reviewer_jyH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16297/Reviewer_jyH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787663641, "cdate": 1761787663641, "tmdate": 1762926439142, "mdate": 1762926439142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a strong, novel, adaptive tokenization framework that is fully differentiable and serves as an in-place replacement for ViT like models in-place of the uniform grid based tokenizer. The authors use a score network to assign region wise importance and create partitions such that the areas are inversely proportional to information density. The authors present several compelling results on a variety of metrics, demonstrating the strong performance of their approach on a wide variety of applicable scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and contains many clear examples, results and ablations to support their design considerations.\n- The experiments are conducted on a variety of scales, datasets, scenarios, and model types, suggesting the strong performance of their proposed approach.\n- The proposed method is largely novel, adaptive, and scalable, and serves as a significant future direction when working with visual understanding and generation methods"}, "weaknesses": {"value": "- The authors suggest that their adaptive partitioning and scaling of sequence length can lead to smaller models matching the performance of their larger variants at cheaper resource allocations. Does this hold for denser tasks like object detection and semantic segmentation as well -- or is this limited to simpler classification problems as presented?\n- Figure 6 suggests a good handling on increasing image resolution -- can this be extended further for tasks like superresolution with constrained sequence lengths. For instance, a very high resoluton image cannot be computationally patchified using a standard 16x16 grid, but can DART process such inputs while maintaining a fixed tractable sequence length and still produce outputs of good quality,\n- I believe the related work section, while relevant, can be expanded further to discuss more tokenization and efficiency directions for vision transformer models, perhaps in the appendix."}, "questions": {"value": "Please see the weaknesses section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xHJgAvK33l", "forum": "82XCBdf8G2", "replyto": "82XCBdf8G2", "signatures": ["ICLR.cc/2026/Conference/Submission16297/Reviewer_tN6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16297/Reviewer_tN6A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997052795, "cdate": 1761997052795, "tmdate": 1762926438664, "mdate": 1762926438664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes DART, a differentiable, content-adaptive tokenizer that replaces the fixed patch grid in uniform backbones with an importance-weighted partition. A lightweight scorer predicts a per-pixel density map; a differentiable quantile inversion then allocates small patches to salient areas and large patches to background, followed by differentiable resampling into fixed-size tokens [1]. The authors report that equipping small ViT / SSM models with DART matches or exceeds larger baselines at substantially lower FLOPs, with additional gains on ADE20K and video benchmarks. \n\nThe central idea of pre-tokenization adaptivity is clear and practically attractive. However, the manuscript’s placement within prior work is exceedingly thin. Closely related adaptive tokenization and perceptual-grouping lines are conspicuously absent from the current related work, including:\n- Existing adaptive tokenization methods for efficiency [1,2] have already been proposed. Omitting these seem like a significant oversight.\n- Superpixel Tokenization [3,4,5,6] has been successfully applied to adaptive pre-tokenisation. While not explicitly applied uniquely for efficiency, these works represent recent important progress in adaptive tokenization for segmentation, and should be included in the related work.\n\nAs the paper currently reads, the central contribution is best described as an elegant quantile-based continuous relaxation of adaptive region partitioning that integrates smoothly with standard ViTs/SSMs, but not the first differentiable adaptive tokenizer for images. With this in mind, the paper is well placed to make a contribution to the existing field, if central concerns are addressed.\n\n[1] [Havtorn et al. 2023 - Dynamic Mixed-Scale Tokenization for Vision Transformers](https://arxiv.org/abs/2307.02321)\n\n[2] [Ronen et al. 2023 - Vision Transformers With Mixed-Resolution Tokenization](https://arxiv.org/abs/2304.00287)\n\n[3] [Aasan et al. 2025 - A Spitting Image: Modular Superpixel Tokenization in Vision Transformers](https://arxiv.org/abs/2408.07680)\n\n[4] [Chen et al. 2025 - Subobject-level Image Tokenization](https://arxiv.org/abs/2402.14327)\n\n[5] [Mei et al. 2024 - SPFormer: Enhancing Vision Transformer with Superpixel Representation](https://arxiv.org/abs/2401.02931)\n\n[6] [Aasan et al. 2025 - Differentiable Hierarchical Visual Tokenization](https://openreview.net/forum?id=y8VWYf5cVI)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed quantile inversion provides continuous, differentiable boundaries. Additionally, DART-Flow lets tokens “flow” globally to salient zones while preserving a fixed token budget, so batches and token counts remain consistent. This provides computational benefits compared to fully adaptive methods, notably with some loss in flexibility.\n2. Similarly to recent work [6], DART can serve as a drop-in for uniform backbones without altering the backbone. Reminiscent to kernelled positional encoding, the authors propose a transform to align positional embeddings via a warp transform over a 2D field to align existing learnable positional embedding to a new adaptively deformed partition. This allows the method to work out-of-the box with existing pre-trained models.\n3. The paper shows small models (DeiT-S, Vim-S) matching or beating Base counterparts at far lower FLOPs/latency, reducing overall token count to a fixed number. Performance gains are demonstrated over different of downstream tasks. While moderate, the results clearly demonstrate benefits.\n4. The novel CDF based mechanism proposed by the authors is technically distinct, and has potential to synergise well with alternative approaches. The core ideas are insightful and interesting.\n5. The authors include discussion of a failure mode (Fig. 8a) which indicates a case where the method may have difficulty tokenising an image. This reviewer appreciates the openness to disclosing limitations. The \"adaptive degeneration\" behavior on uniform textures (Fig 8b) is particularly insightful."}, "weaknesses": {"value": "1. A central concern of this reviewer is that the related work section is quite underdeveloped. There has been much work done in this field, and the omission of many concurrent works in adaptive tokenization [1,2,3,4,5,6] overstates the conceptual novelty of the approach.\n2. The claimed “adaptive pre-tokenization” is functionally limited. The adaptivity of regions is limited, the token count is fixed and the global token budget predetermined. While this is beneficial for efficiency (which is the current scope of the paper), it restricts adaptivity to spatial redistribution within a constant sequence length, rather than true structural adaptivity as in superpixel- or graph-based approaches. As the paper currently reads, this is not the intended scope of the work, so this should be taken as a minor weakness. Addressing this via positing the work accordingly will paint a clearer picture of the work in relation to existing research.\n3. As this reviewer understands the method, the scoring network is frozen and externally pretrained, which undermines the notion of fully learned adaptivity. The backbone and tokenizer are not co-optimized, and therefore DART cannot genuinely adapt its token allocation to the downstream objective. In contrast, the scoring network in MS-ViT [1] is end-to-end optimisable. This is a clear limitation that should be addressed in the paper.\n4. The empirical results are somewhat narrow. No additional datasets are evaluated for classification. No evaluation on fine-grained recognition tasks (CUB, Stanford Cars) where detail preservation claims would be most tested. Dense tasks are included, but to a limited extent. This downplays the generalizability of the proposed method. \n5. There are no significant ablations reported in the work towards determining the central drivers of performance and efficiency gains in the method:\n\t- Missing analysis of scoring network design.\n\t- No ablation on number of rows or tokens.\n\t- No analysis of resolution. It is not clear that the method (448) is compared with the baseline resolution (originally 224).\n6. Missing key comparisons and baselines: \n\t- No direct comparison with the cited adaptive tokenization methods [1,2] .\n\t- Missing comparison with recent token reduction methods (ToMe, TokenLearner) \n\t- No comparison with superpixel tokenization approaches [3,4,5,6]. \n7. Several claims in the introduction and conclusion are overstated:\n\t- Line 086: \"resolves core representational dilemma\" - only partially; doesn't address the fixed token budget limitation.\n\t- Line 088: \"more intelligent scaling paradigm\" - the paradigm isn't new, just the specific method.\n\t- Line 478: “resolves the inherent limitations of rigid tokenization in uniform backbones like ViT and Mamba”. This overstates the contribution. Arguably, existing methods solve this much better and while the proposed method is innovative, the global resolution of all ailments in existing tokenizer is a very grand, overstated claim."}, "questions": {"value": "1. Will the authors commit to revamp the related work section to include existing works on adaptive tokenization? How do the authors consider the contribution in terms of the existing work in the field?  2. What is the effect of varying resolution? Why was 448 x 448 selected? How does the method work with smaller resolution? Is the baseline also computed with 448 resolution, or does the baseline use the more standard 224 resolution? \n3. How does the method fare on alternative benchmarks for classification? Including small additional tasks is necessary to show that the method generalises beyond ImageNet.\n4. The authors diligently include a failure mode in Fig. 8a. Do the authors have other examples of failure or limitations of the approach? Can the authors characterize which types of images (e.g., texture-rich vs object-centric) benefit most from DART?\n5. Are the authors able to provide ablations for design of the scoring network, or the effect of adjusting the number of tokens? \n6. Can the authors provide direct quantitative comparison with MS-ViT [1] Quadformer [2] and other differentiable adaptive tokenizers?\n7. What is the performance when the scoring network is trained end-to-end rather than frozen?\n\n**Requested Changes**\n\n1. Substantial expansion of related work to include [1-6] with honest positioning of contribution.\n2. Direct experimental comparison with MS-ViT [1], Quadformer [2], and (optionally) comparable superpixel methods [5,6].\n3. Additional ablation studies on resolution, token count, and scoring network design.\n4. Evaluation on at least one additional classification dataset and one additional dense prediction task.\n5. Reframe claims about novelty in light of existing methods on the topic of adaptive tokenization. \n6. (Optional) There is no need to relabel tokenization---which is the act of partitioning an input into different atomic components---as “pre-tokenization”. The proposed method is well defined as a tokenizer, which aligns with existing terminology in the field of adaptive tokenization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BUBxg5Mq19", "forum": "82XCBdf8G2", "replyto": "82XCBdf8G2", "signatures": ["ICLR.cc/2026/Conference/Submission16297/Reviewer_5vGt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16297/Reviewer_5vGt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007983460, "cdate": 1762007983460, "tmdate": 1762926438244, "mdate": 1762926438244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}