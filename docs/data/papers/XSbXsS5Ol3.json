{"id": "XSbXsS5Ol3", "number": 13848, "cdate": 1758223602420, "mdate": 1759897408804, "content": {"title": "MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph", "abstract": "Medical deep learning models depend heavily on domain-specific knowledge to perform well on knowledge-intensive clinical tasks. Prior work has primarily leveraged unimodal knowledge graphs, such as the Unified Medical Language System (UMLS), to enhance model performance. However, integrating multimodal medical knowledge graphs remains largely underexplored, mainly due to the lack of resources linking imaging data with clinical concepts. To address this gap, we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and textual medical information through a multi-stage construction pipeline. MEDMKG fuses the rich multimodal data from MIMIC-CXR with the structured clinical knowledge from UMLS, utilizing both rule-based tools and large language models for accurate concept extraction and relationship modeling. To ensure graph quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel filtering algorithm tailored for multimodal knowledge graphs. We evaluate MEDMKG across five tasks under two experimental settings, benchmarking twenty-four baseline methods and four state-of-the-art vision-language backbones on six datasets. Results show that MEDMKG not only improves performance in downstream medical tasks but also offers a strong foundation for developing adaptive and robust strategies for multimodal knowledge integration in medical artificial intelligence.", "tldr": "A medical multimodal knowledge graph benefiting various downstream tasks.", "keywords": ["medical knowledge graph", "multimodal knowledge graph"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13bce96779c99f79dab20ee8c6c3a1dc7b890a32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of existing medical deep learning models relying on single-modal knowledge graphs and lacking resources linking visual and clinical concepts. It proposes a medical multimodal knowledge graph, MEDMKG, through a multi-stage construction process that fuses multimodal data from MIMIC-CXR with structured clinical knowledge from UMLS. It introduces a Neighbor-aware Filtering (NaF) algorithm to ensure graph quality and simplicity. The paper benchmarks 24 baseline methods and four mainstream vision-language backbone models across five tasks (link prediction, text-image retrieval, and visual question answering) under two experimental settings. Results demonstrate that MEDMKG not only improves the performance of downstream medical tasks but also provides a foundation for the development of adaptive and robust strategies for integrating multimodal knowledge in medical AI. The paper also identifies key insights, such as the need for model selection to match graph structure and the potential for external knowledge to improve downstream task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It breaks the limitations of traditional unimodal medical knowledge graphs by innovatively integrating MIMIC-CXR imaging data with UMLS clinical knowledge to construct MEDMKG, the first multimodal knowledge graph tailored for medical scenarios. The proposed NaF algorithm offers a new approach to redundancy filtering in multimodal knowledge graphs by balancing image connectivity and uniqueness.\n\n2.The construction process is rigorous, combining MetaMap and GPT-4o to achieve high-precision concept extraction and relationship modeling. The graph's quality is validated through expert evaluations and quantitative analyses. The experimental design is comprehensive, covering 5 tasks, 2 experimental settings, and 24 baseline methods, ensuring reliable and statistically meaningful results.\n\n3.The paper follows a standardized structure with a logical flow from problem formulation to solution development and experimental validation. Key technologies and experimental details are described in detail, facilitating reproduction and reference by other researchers.\n\n4.It fills the gap in resources for multimodal medical knowledge graphs. Experimental results confirm that MEDMKG can effectively improve the performance of downstream medical tasks, providing a new paradigm for multimodal knowledge integration in medical AI and making a significant contribution to advancing the field of medical intelligence."}, "weaknesses": {"value": "1.The core parameters of the NaF algorithm (e.g., the determination method of M in the formula) are not elaborated, and no parameter sensitivity analysis is conducted to verify the impact of different parameter settings on graph quality and downstream task performance. Furthermore, there is no comparison with other mainstream knowledge graph filtering algorithms, making it difficult to highlight the advantages of NaF.\n\n2.Experimental data mainly rely on MIMIC-CXR (chest X-ray images) and UMLS, without extension to other medical imaging types (e.g., CT, MRI) or clinical datasets (e.g., electronic health records, EHR). This fails to fully validate the generalization ability of MEDMKG across multiple medical scenarios. In some tasks, the performance of head entity prediction is significantly lower than that of tail entity prediction, but the fundamental cause of this difference is not analyzed in depth.\n\n3.The paper does not discuss the feasibility of deploying MEDMKG in real clinical scenarios, such as the graph's update mechanism (how to integrate new medical knowledge and imaging data), compatibility with existing clinical systems, and specific strategies to address potential ethical risks (e.g., diagnostic bias) in clinical decision support."}, "questions": {"value": "1.In the NaF algorithm, is M (the total number of medical images in the knowledge graph) a fixed value or dynamically adjusted based on the dataset? If dynamically adjusted, what is the basis for adjustment? Can you supplement comparative experiments on algorithm performance under different M values to verify the rationality of parameter selection?\n\n2.In the experiment, the performance of head entity prediction is lower than that of tail entity prediction. The authors attribute this to modal heterogeneity. Can you further analyze the specific differences between image and text modalities in the embedding space and propose targeted optimization directions?\n\n3.Currently, MEDMKG only integrates chest X-ray images and UMLS knowledge. How do you plan to extend it to other medical imaging types (e.g., CT, ultrasound) or clinical data (e.g., laboratory test results) in the future? During the extension process, how will you resolve semantic inconsistency issues among different modal data?\n\n4.The paper mentions that MEDMKG can be used for clinical decision support. Can you elaborate on its deployment process in real clinical scenarios? For example, how to interface with existing hospital systems, how to ensure the real-time and accuracy of graph updates, and how to avoid potential diagnostic bias risks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tj6A9moM5Q", "forum": "XSbXsS5Ol3", "replyto": "XSbXsS5Ol3", "signatures": ["ICLR.cc/2026/Conference/Submission13848/Reviewer_qVVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13848/Reviewer_qVVF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621597146, "cdate": 1761621597146, "tmdate": 1762924372019, "mdate": 1762924372019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEDMKG, a Medical Multimodal Knowledge Graph that integrates textual concepts from UMLS with radiological images from MIMIC-CXR. The authors design a multi-stage construction pipeline combining rule-based extraction (MetaMap) and LLM-based disambiguation (GPT-4o), and propose a simple Neighbor-aware Filtering (NaF) heuristic to select informative images. They benchmark the resulting graph across two settings: (1) link prediction and (2) downstream multimodal tasks (text-image retrieval and VQA) using four vision-language backbones. The results show moderate but consistent performance gains when external knowledge from MEDMKG is integrated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive baseline coverage for link prediction tasks.\n2. Integrating structured clinical knowledge with medical imaging is quite novel in the clinical ML domain."}, "weaknesses": {"value": "W1. The paper claims that multimodal KGs outperform text-only KGs, but no direct comparison is provided. Without a UMLS-only baseline, the improvement cannot be attributed to multimodality itself, leaving the core hypothesis untested.\n\nW2. The proposed Neighbor-aware Filtering (NaF) is a handcrafted scoring rule combining neighbor count and distinctiveness. It is not compared to simpler alternatives such as random sampling, degree-based filtering, or embedding-based diversity selection. Without such comparisons, it is unclear whether NaF truly improves graph quality.\n\nW3. The observed improvements in retrieval and VQA could stem from larger data volume or overlap between MIMIC-CXR and evaluation datasets, rather than genuine multimodal reasoning. An ablation that isolates the contribution of image-concept links versus pure text-based knowledge would be essential to establish causal validity\n\nW4. Experiments are reported with single runs and no variance estimates. Results should be averaged across 3-5 random seeds with significance testing to confirm the robustness of claimed improvements.\n\nW5. A case study or visualization showing how MEDMKG influences predictions (e.g., specific examples of correctly or incorrectly augmented VQA answers) would greatly improve transparency."}, "questions": {"value": "If the author could help clarify the following questions, I am happy to raise the score.\n1. How does MEDMKG compare to using UMLS alone as a knowledge graph?\n2. Can you provide quantitative ablation showing the contribution of the NaF step versus simple baselines?\n3. I don't understand the description of dataset leakage prevention. (L374-376) \"To prevent any potential data leakage regarding MIMIC-CXR, we only select text-image pairs that were not used during the curation of MEDMKG, and we randomly sample a fixed set of 10,000 pairs from these remaining examples.\" -- Why were some text-image pairs not used during the curation of MEDMKG? Does that mean MEDMKG includes only a representative subset of text-image pairs?\n4. How sensitive are the results of Table 3 and Table 4 for different random seeds?\n5. Could you include error analysis to show which question types or retrieval categories benefit most from the multimodal graph?\n6. When I click on the anonymized GitHub link in the appendix, the code and dataset are not found. Is there are technical issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2glT6mOnfI", "forum": "XSbXsS5Ol3", "replyto": "XSbXsS5Ol3", "signatures": ["ICLR.cc/2026/Conference/Submission13848/Reviewer_GHE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13848/Reviewer_GHE9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684521138, "cdate": 1761684521138, "tmdate": 1762924371499, "mdate": 1762924371499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEDMKG, a medical multimodal knowledge graph built by linking chest X-ray images, their paired radiology reports, and UMLS clinical concepts. The authors describe a pipeline to extract clinical findings from reports, align them to ontology terms, associate them with images, filter the resulting graph, and then use this graph in downstream settings such as link prediction, image–text retrieval, and medical visual question answering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work makes a clear attempt to build a structured resource that connects imaging data with clinical concepts and to organize that resource in a way that can be consumed by standard models. The paper also provides benchmark tasks (link prediction, retrieval, VQA) and reports baseline performance numbers on them, which makes it easier for future work to compare under similar settings."}, "weaknesses": {"value": "Overall, while the paper presents MEDMKG as a broadly useful medical multimodal knowledge graph and reports promising downstream results, there are several issues in the scope of the resource, the reliability of how it is constructed, and how its impact is evaluated.\nW1: The paper presents MEDMKG as a Medical Multimodal Knowledge Graph that can support knowledge intensive clinical tasks and unify visual and textual medical knowledge. In practice, the resource is almost entirely limited to chest radiology: it is built from chest X rays and their paired radiology reports, plus UMLS terms and relations. This is much closer to a focused chest imaging subgraph than a general clinical multimodal knowledge graph. The paper also frames prior work as mostly single modality, but multimodal knowledge graphs with image nodes and cross modal links already exist in other domains, and the related work section acknowledges work of that kind.\nW2: The graph is constructed with a two stage pipeline: MetaMap is first used to propose UMLS concepts from reports, then ChatGPT 4o is used to disambiguate concepts and to decide whether each finding is present, absent, or uncertain for the image. This relies on a closed large language model to make fine grained clinical judgments, but the paper does not report quantitative accuracy, agreement with human annotators, or any human reference study. It is not clear how reliable this automatic alignment is in a radiology setting.\nW3: The paper describes this alignment process as high quality, but the only human validation shown is a figure with subjective scores, without key details such as number of raters, sampling protocol, blinding, or inter rater agreement. This is weak as evidence for clinical correctness.\nW4: The Neighbor aware Filtering method (NaF) is presented as a contribution, but it is essentially a heuristic that keeps images which connect to many and relatively rare relation–concept pairs and filters out others. It is not learned, not optimized end to end, and there is no ablation showing downstream performance with and without NaF or how different thresholds affect results. It is difficult to judge NaF as more than manual pruning.\nW5: For link prediction, MEDMKG is treated as a standard knowledge graph and off the shelf knowledge graph embedding models such as TransE are applied. Images are effectively treated as entity ids with learned embeddings, and evaluation is done as missing edge completion under a random triple split. This mainly shows the model can fit the constructed graph, not that the graph enables multimodal reasoning or encodes visual semantics in a meaningful way.\nw6: For retrieval and medical visual question answering, the paper reports that models augmented with MEDMKG perform better. However, there is no controlled comparison where the same model is trained under identical conditions with and without MEDMKG, or compared to another knowledge source. Because the contribution of MEDMKG is not isolated, it is hard to attribute the reported improvements directly to this resource."}, "questions": {"value": "All of my concerns are already reflected in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y4uxhtpzXe", "forum": "XSbXsS5Ol3", "replyto": "XSbXsS5Ol3", "signatures": ["ICLR.cc/2026/Conference/Submission13848/Reviewer_2Vrj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13848/Reviewer_2Vrj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706198121, "cdate": 1761706198121, "tmdate": 1762924371114, "mdate": 1762924371114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper constructs a multimodal medical knowledge graph (MMKG) by linking UMLS concepts to chest X‑ray (CXR) images (MIMIC‑CXR) and associated text. It introduces Neighbor‑Aware Filtering (NaF), a rarity‑weighted, greedy‑coverage heuristic, to down‑select image nodes that are distinctively valuable yet concept‑covering. Evaluation spans intrinsic graph tasks (link prediction and entity‑type prediction) and extrinsic multimodal tasks: image–text retrieval (Table 3; OpenI and MIMIC‑CXR) using adapter variants (FashionKLIP, KnowledgeCLIP) on top of pretrained vision-language backbone base models, and VQA. The submission argues that the graph and its construction pipeline will be a useful community resource; the exact release scope of the graph should be further clarified by the authors. \n\nOverall: I recommend that the paper be rejected in its current form. \nWhile the proposed approach is technically elegant and NaF is a clever, interpretable heuristic for multimodal graph pruning, the current evaluation scope limits the broader medical or multimodal impact. The work’s contribution would be convincing if it demonstrated value beyond chest X-rays, clarified NaF impact and ablations and extended to translational medicine, beyond VQA/retrieval tasks.\n\nReviewer LLM Usage:\nI have read the paper in full and written the review myself. Large Language Models (LLMs) were used only for writing polish, clarity improvements, and to refresh memory of (public) related work or references. The analysis and conclusions are entirely my own."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Elegant filtering method: NaF combines a TF–IDF‑like log rarity term (\\log(M / |N(r,c)|)) elegantly balances redundancy reduction and rarity based high-value selection. The combination with greedy concept coverage ensures the resulting graph remains both compact and concept-rich. This is well suited for redundancy-heavy medical imaging corpora. It is also agnostic to type of images or domain. \n2. The link prediction table (Table 2) compares 17 KGE models across head / relation / tail prediction. Multiple model families (e.g., TransD, TransE, TuckER, AttH, ConvE) achieve non‑trivial Hits@K and reasonable MR, indicating the graph encodes coherent, learnable relations rather than noise.\n3. In Table 3, KnowledgeCLIP consistently improves retrieval over bare backbones (e.g., with CLIP: OpenI Recall@100 53.48 → 76.16; MIMIC‑CXR 58.26 → 74.37). This supports the premise that KG information can help downstream multimodal retrieval.\n4. The small‑scale human evaluation (Sec 3.5) of the constructed knowledge quality adds qualitative credibility."}, "weaknesses": {"value": "1. Limited Multimodality (CXR-Only):\nDespite claims of general multimodal capability, all experiments use only chest X-rays. This confines multimodality to “text + chest images + ontology,” not multiple image types. Consequently, the claim of extending UMLS to multimodal space is overstated. The graph does not yet demonstrate coverage for other imaging modalities (CT, MRI, ultrasounds, etc.) though base UMLS does.\n2. Overstated Benchmark Diversity:\nThe claim of “six diverse datasets” and “five tasks” is somewhat misleading. The datasets all correspond to two (extrinsic) downstream tasks — image/text retrieval and VQA. The “five tasks” count includes intrinsic graph-quality measures. The 24 baselines stem from combinations of four VL backbones (CLIP, BioMedCLIP, GLoRIA, ALBEF) and several integration variants much like hyperparameter choice exploration, rather than 24 distinct competing methods.\n3. Missing ablation: No explicit NaF ablation in experiments of Table 3 and Table 4 is called out. So, while NAF is an elegant graph construction proposal, the incremental impact on quality from NAF is not clear in the experiments. \n4. Missing clinically impactful tasks: The downstream benchmarks focus on sub-component (retrieval) or research-oriented (VQA) tasks rather than end-to-end clinically impactful ones such as diagnostic reasoning, report generation, or longitudinal temporal sequencing. As a result, the paper doesn’t yet demonstrate cross-cutting medical impact.\n5. Bias and Generalization testing: Since this is CXRs from a single dataset, and test datasets don't state population diversity or device diversity, it is unclear if the results would extend to demographic or device based CXR differences. See suggestion (1a-v). below."}, "questions": {"value": "1. Is it possible to add actual benchmarking diversity and include some translational medicine tasks?\nTo enhance generality and community value, consider these as some ideas (any 2-3, not even all):  \n a). Integrate at least one additional imaging modality (e.g., CT, ultrasound, or pathology) to show extensibility beyond CXRs. _Your method is powerfully agnostic to the image type, so using at least one other image type will validate this robustness_.    \nb). Expand benchmarks beyond VQA/retrieval (atleast a couple of these will be sufficient to prove the work’s value):    \n  i) Diagnostic reasoning: e.g., CheXpert or PadChest, evaluating if the KG improves disease prediction or explanation.   \n  ii).\tReport generation: e.g., MIMIC-CXR report-generation task to test factual grounding and fluency.    \n  iii).\tPhenotype clustering / cohorting: e.g., UK Biobank imaging subset to show improved patient grouping with chest multimodal KG.    \n  iv).\tTemporal reasoning: e.g., MIMIC-IV linkage to test prediction of sequential imaging related findings.\n  v).\tCross-population generalization: e.g., on geographically (VinDr-CXR) or modality-diverse datasets (Shenzen TB). \nThis would also show that the specific dataset used for CXRs keeps findings generalizable to other within-CXR tasks without overfitting.   \n  c)\tAdd Intrinsic Image–Image Similarity Prediction:\nExtend the intrinsic evaluations with an image–image similarity task. This would test relational reasoning and consistency on the newly added modality (images) by itself e.g., finding past CXRs most similar to a query based on shared concept neighborhoods.\n\n2. Calibrate Claims:\nAdjust the narrative to reflect the current scope: “image-text-ontology multimodality in CXRs” rather than full “medical multimodality.” Likewise, rephrase the “diverse benchmarks” claim as “multiple datasets across two extrinsic tasks.”\n\n3. Can you clarify how the splits are done from graph construction to testing with MIMIC-CXR? Are these patient level or report level? And how do you avoid leakage? Even if this is patient level, the concepts would still be similar across the reports - is this an accurate understanding? \n\n4. How do you handle reports that have temporal findings (comparison of previous and new findings in text, once indicating no pneumonia and another indicating pneumonia) or ambiguous phrasing (e.g. \"cannot exclude pneumonia\")? A presentation of the edge (positive, negative, uncertain) analysis would be useful to understand false-positives and confusion from temporally conflicting edge relations. \n\n5. Can you clarify from Appendix E.2, how multiple annotators agreement scores were on the same subset they evaluated? Or did just one human evaluate per subset?\n\n6. Can you ablate NAF and no-NAF methods for experiments like Table 3 and Table 4? \n\n\nAdditional Minor Writing/Presentation Issues.  \n1.\tWould be useful to summarize big tables like Table 4 with summarized quantities in narrative too.    \n2.\tClearly specify what is released - the code, pipeline, and whether the MedMKG graph (with or without images) will be released.    \n3.\tL409-411 could be clearer since pre-training KG augmentation is not from scratch. Backbones are already pre-trained so this seems like post-training for this task."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "details_of_ethics_concerns": {"value": "Since the CXRs used to extend the graph, which would be used for all downstream CXR related tasks, are from a single dataset and also tested on similar domain, it is unclear how the graph generalizes to the tasks for other demographics and/or CXRs captured on lower resolution devices (such as phones or, in low economic health regions, on older devices). Suggestion 1/b/v could be one way to test for generalization."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BSKcPg7XZf", "forum": "XSbXsS5Ol3", "replyto": "XSbXsS5Ol3", "signatures": ["ICLR.cc/2026/Conference/Submission13848/Reviewer_bzhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13848/Reviewer_bzhb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974620331, "cdate": 1761974620331, "tmdate": 1762924370522, "mdate": 1762924370522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors construct MEDMKG, a chest X-ray–centric multimodal medical knowledge graph that links radiographs to UMLS concepts through both cross-modal and intra-modal edges. A two-stage extraction pipeline combines MetaMap with GPT-4o for concept disambiguation and polarity labeling, while a novel neighbor-aware filtering score prunes redundant images to maintain graph quality and diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses an important gap between unimodal medical knowledge graphs and vision–language models in healthcare. Radio graph KG is relatively rare, so I feel this work represents a notable contribution to the field.\n\n2. The proposed NaF method is a simple but effective heuristic that balances connectivity and distinctiveness when selecting representative images.\n\n3. The dataset is comprehensively evaluated, and the authors provide a thoughtful discussion of performance trends across tasks and models."}, "weaknesses": {"value": "1. Results in Table 2, 3 and 4 do not have an error bound, yet lacking statistical power. I would suggest author to add the error bound as sometimes two numbers are quite close in the table, and we cannot tell if they are significantly different.\n\n2. The paper uses gpt-4o for cross-modality relation extraction. I'm unsure if there is any bias. Maybe the author could add some small-scale ablation studies to check the alignment between gpt-4o and other LLMs.\n\n3. In qualitative analysis, the paper claims that MEDMKG achieves an average of approximately 80% across all three metrics. But from Figure 2, it seems that the results have a huge variance. Also, I have no idea if 80% if good or not. It might be better to add a baseline comparison."}, "questions": {"value": "Please refer to my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Do0J8KdT99", "forum": "XSbXsS5Ol3", "replyto": "XSbXsS5Ol3", "signatures": ["ICLR.cc/2026/Conference/Submission13848/Reviewer_ewCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13848/Reviewer_ewCn"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976301013, "cdate": 1761976301013, "tmdate": 1762924370020, "mdate": 1762924370020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}