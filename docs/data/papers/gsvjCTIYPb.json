{"id": "gsvjCTIYPb", "number": 2976, "cdate": 1757310404058, "mdate": 1759898116168, "content": {"title": "Vidarc: Low Latency Embodied Video Diffusion Model with Closed-loop Control", "abstract": "Robotic arm manipulation in data-scarce settings remains challenging due to complex embodiment dynamics and diverse contexts. Recent video-based approaches, though adept at capturing and transferring temporal and physical interactions, are typically not optimized for the embodiment-specific closed-loop control, suffering from high latency and insufficient grounding. We present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), an autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, our method achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc outperforms state-of-the-art baselines in both task success rates (at least 15% higher in real-world deployment) and latency, demonstrating robust generalization and error correction on unseen robotic platforms.", "tldr": "", "keywords": ["Robotics", "Video Diffusion Model", "Computer Vision"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7855d6ae67acb1b1a24332bc0b4c2535a43750c2.pdf", "supplementary_material": "/attachment/8ce7790fc4e592a31bf85c4f6cc642b29e9534e9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VidARC, a framework for low-latency action reconstruction in embodied vision-language-action (VLA) models. The authors address the common inefficiency of current VLA agents, which process perception and action sequentially and suffer from high inference latency during video-conditioned control. VidARC introduces an Action Reconstruction Transformer that incrementally predicts future action embeddings conditioned on video context and recent actions, enabling chunk-wise autoregressive decoding. The system can thus execute actions continuously while maintaining visual grounding. The framework is evaluated in both simulated and real-world environments, showing significant improvements in latency and control stability compared to existing diffusion-based and autoregressive policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly organized with informative figures and step-by-step reasoning that make the approach easy to follow.\n\nThe focus on low-latency action reconstruction fills a clear gap between pure video understanding and real-time embodied control.\n\nThe chunk-wise autoregressive inference mechanism is elegant and practically motivated. It effectively balances context utilization and temporal efficiency."}, "weaknesses": {"value": "The paradigm used in this paper has already been extensively explored in prior works such as UniPi[1], VLP[2], RoboDreamer[3], UniSim[4], MPI[5], WorldSimBench[6], TesserAct[7], and Unified Video Action Model[8]. These studies have discussed the potential and applications of the video policy framework from different perspectives, including planning, execution, and benchmarking. The lack of discussion and citation of these related works makes it difficult to fully assess how this paper differs from and contributes beyond them.\n\nI acknowledge that accelerating video policies is an important research direction. However, the introduction of the KV-cache mechanism appears to be more of an engineering implementation rather than a conceptual or methodological innovation. Moreover, the video autoregressive generation component seems closely related to teacher forcing and diffusion forcing, which may further limit the paper’s technical novelty.\n\nRegarding the mask mechanism, I have some concerns. If the mask generation performs poorly in out-of-distribution (OOD) scenarios, would it significantly affect the performance of the video policy compared to approaches without a mask? In addition, since mask training requires additional data annotation, does this process restrict the scalability of the framework? Alternatively, if automated labeling is used, could it introduce error accumulation over time (for example, when the ground-truth masks are inaccurate)?\n\nI would also like the authors to clarify how the mask converges to focus on the robotic arm regions, as shown in the visualizations provided in the paper.\n\nAs for the experiments, I believe that using only RobotWin as the simulation benchmark is not sufficiently comprehensive. Many existing video policy studies mentioned above typically evaluate on CALVIN[9] or LIBERO[10]. Since the authors did not reproduce multiple baselines on RobotWin, adopting more general benchmarks or testing against additional video policy or VLA baselines could improve the credibility and completeness of the experimental results.\n\n\n[1] UniPi: Learning universal policies via text-guided video generation (NeurIPS 2023)\n\n[2] Video Language Planning (NeurIPS 2023)\n\n[3] RoboDreamer: Learning Compositional World Models for Robot Imagination (ICML2024)\n\n[4] Learning Interactive Real-World Simulators (ICLR2024)\n\n[5] Learning Manipulation by Predicting Interaction (RSS2024)\n\n[6] WorldSimBench: Towards Video Generation Models as World Simulators (ICML2025)\n\n[7] TesserAct: Learning 4D Embodied World Models (ICCV2025)\n\n[8] Unified Video Action Model (RSS2025)\n\n[9] CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks (RAL)\n\n[10] LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RVFVFeY5N3", "forum": "gsvjCTIYPb", "replyto": "gsvjCTIYPb", "signatures": ["ICLR.cc/2026/Conference/Submission2976/Reviewer_tgFC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2976/Reviewer_tgFC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900623325, "cdate": 1761900623325, "tmdate": 1762916474152, "mdate": 1762916474152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Vidarc, an autoregressive embodied video diffusion framework integrated with a masked inverse dynamics model, designed to address the challenges of low-latency closed-loop control and generalization in data-scarce robotic manipulation scenarios. By introducing an embodiment-aware diffusion loss guided by action-relevant masks and incorporating KV caching for real-time environmental feedback, Vidarc aims to enhance the alignment between video generation and robotic embodiment dynamics. Pre-trained on one million cross-embodiment episodes and fine-tuned on unseen platforms, the model demonstrates higher task success rates (15-17% higher than baselines like Vidar and Pi0.5) and lower latency in both simulated and real-world experiments, along with error correction capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The integration of autoregressive video diffusion with closed-loop control via KV caching and environmental feedback re-prefilling addresses the high-latency issue of traditional non-autoregressive video-based methods, enabling more responsive robotic manipulation.\nThe embodiment-aware loss, weighted by masks from the inverse dynamics model, effectively prioritizes action-relevant regions, mitigating the problem of irrelevant visual distractions and improving the actionable quality of generated videos.\nExtensive pre-training on large-scale cross-embodiment datasets ensures strong generalization to unseen robotic platforms and tasks, with experimental results validating superior performance over state-of-the-art baselines in both simulation and real-world settings."}, "weaknesses": {"value": "Unclear Visualization and Ambiguous Definitions: The error correction example in Figure 1 lacks sufficient explanation, making it difficult to understand the model’s error correction mechanism. Additionally, the definition of the observation space O is ambiguous—if O is a single image, predicting actions from a single frame contradicts the fundamental definition of inverse dynamics models and poses an ill-posed problem.\nIncomplete Literature Review: The paper claims that existing video-based methods are inefficient but overlooks prominent efficient approaches such as Vidman and VPP, leading to an incomplete assessment of the current research landscape.\nPractical Efficiency Limitations: Despite adopting CausVid for optimization, Vidarc’s reliance on the Wan2.2 backbone results in excessive memory overhead during training and inference. With a per-step latency of 3 seconds, the model fails to meet the real-time requirements of embodied intelligence applications.\nThin Simulation Experiments: The simulation evaluations are only conducted on the RoboTwin benchmark, lacking validation on other widely used benchmarks like Libero or RLBench. This limits the generalizability of the reported performance.\nAmbiguous Contribution of Core Components: The performance advantage of Vidarc over baselines is primarily attributed to closed-loop control, but this component is not inherently tied to video generation—any autoregressive method could integrate such feedback. Moreover, removing closed-loop control leads to a significant performance drop (66.8% vs. 80.7% average success rate on RoboTwin), casting doubt on the necessity of the embodiment-aware loss. The weighted loss may also neglect background information critical for diffusion denoising, yet no theoretical justification or quantitative analysis of the loss’s feasibility is provided.\nInsufficient Analysis of Key Mechanisms: The masked inverse dynamics model uses different input sources during training and inference, raising concerns about the stability and controllability of its performance, especially with observations generated via embodiment-aware training. Additionally, there is a lack of case studies and quantitative analysis on how closed-loop control improves action accuracy over iterations, leaving the mechanism’s effectiveness underexplored."}, "questions": {"value": "Overall, the concept of Vidarc is relatively straightforward, and many implementation details lack sufficient empirical or theoretical support. The insufficient experimental design raises concerns about whether the reported results are robust or merely coincidental, reducing the work’s ability to provide meaningful guidance for future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VTialeA6ti", "forum": "gsvjCTIYPb", "replyto": "gsvjCTIYPb", "signatures": ["ICLR.cc/2026/Conference/Submission2976/Reviewer_k5Kh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2976/Reviewer_k5Kh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904485431, "cdate": 1761904485431, "tmdate": 1762916473851, "mdate": 1762916473851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an autoregressive video diffusion model that augmented with a masked inverse dynamics model.  The authors show that by grounding the video prediction of the video diffusion model with action-relevant masks, they obtain fast and accurate closed-loop control.  The authors claim the learned masks prioritizes the video generation quality of the action-relevant regions. The masked inverse dynamics model is from an earlier paper called Vidar.  The authors show that the model when pre-trained on one million cross-embodiment episodes outperforms baselines in success rates and latency, even on unseen robotic platforms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The video generation model incorporates an embodiment-aware loss to make the model focus generation on action-relevant regions.\n\n- Paper produces a method for low-latency video generation for closed-loop control.\n\n- Results work on unseen robotic platforms."}, "weaknesses": {"value": "- The key strength of this paper was to enable the video generation to focus generation on action-relevant regions.  However, this is possible due to the prior work on masked inverse dynamics model (from Vidar?).  This makes the novelty of this work weaker."}, "questions": {"value": "- Figure 1, the left hand side should be drawn in a classical feedback control diagram that is popular in robotics rather than having an execution and environment feedback at the top.  A diagram that shows observation o_t leading to o_t^' producing a_t which then results in o_{t+1} that is feedback is better than the current diagram - which is confusing as the bottom o_1, o_2, o_3 feedback is not clear.  (This is my opinion.)  The figure on the right is disconnected from the figure on the left - this can be revised and the caption can be modified to better connect the two subfigures.\n\n- Figure 2 is missing the action a_1.  Typically, o_{t+1} is produced based on a_t that is computed as a function of o_t.  Here, o_1 -> o_2^' -> a_2, which misses a_1.\n\n- Sec. 2.1: v_\\theta : V x R X C -> V, here the spaces V and C are not defined.  Is V the space of the video x and C the space of the condition c?\n\n- The diffusion loss in Eq. (1) looks at the difference of the vector field v_\\theta and the mean vector field (x_0 - x_1).  v_theta is the vector field at x_t at t, where as (x_0 - x_1) is just the mean vector field.  How does getting these two to match up be sufficient for diffusion?  Can you double check this loss.\n\n- In Eq. (5), why not have a_t = I(o_t) instead of I(\\hat o_t) ?  (I understand that ideally o_t and \\hat o_t should be the same, but they typically are not due to training loss not being zero.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fb2eUTuQhP", "forum": "gsvjCTIYPb", "replyto": "gsvjCTIYPb", "signatures": ["ICLR.cc/2026/Conference/Submission2976/Reviewer_fS78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2976/Reviewer_fS78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098180913, "cdate": 1762098180913, "tmdate": 1762916473093, "mdate": 1762916473093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}