{"id": "8Jylj3K02W", "number": 3227, "cdate": 1757381903061, "mdate": 1763746047846, "content": {"title": "Decomposing Scientific Paper Queries with Draft-and-Follow Policy Optimization to Narrow Knowing-Doing Gap", "abstract": "The rapid growth in the volume of scientific papers presents a significant challenge for researchers to keep up with the latest advances in their field by relying solely on manual reading. Given recent advances in Large Language Models (LLMs), there is a growing trend of employing autonomous agents to extract key information from scientific papers. Although promising, existing approaches generally rely on either meticulously engineered prompts or a standard SFT-RL pipeline, methodologies that are often prone to inducing excessive and ineffective exploration. Inspired by cognitive science, we introduce \\textbf{PaperCompass}, a novel framework designed to address these limitations. Specifically, PaperCompass first generates a draft outlining the sequence of planned execution steps and subsequently engages in fine-grained reasoning to determine parameters for the corresponding function calls. Furthermore, to support this process, we develop a bespoke RL method named \\textbf{D}raft-\\textbf{F}ollow \\textbf{P}olicy \\textbf{O}ptimization, which concurrently optimizes both the draft plan and the final solution. \\textbf{DFPO} can be viewed as a streamlined implementation of Hierarchical RL, designed to bridge the `knowing-doing' gap observed in LLMs. We provide a theoretical analysis of DFPO, demonstrating its desirable properties and thereby ensuring a reliable optimization process. Experiments on paper-based question-answering (Paper-QA) benchmarks demonstrate that PaperCompass's superior efficiency over existing baselines without compromising performance, achieving results comparable to those of much larger models.", "tldr": "We significantly enhance small LLM's interaction efficiency without compromising performance by introducing a special hierarchical reinforcement learning architecture.", "keywords": ["Agentic Reinforcement Learning", "LLM", "Scientific Paper QA"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/02406f2f787a3b5e80fea8a0685bd31860c4c707.pdf", "supplementary_material": "/attachment/dc89f22af7c00996d7340dfe95acf70ff2012a6e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework for improving scientific paper question answering (Paper-QA) by bridging the “knowing-doing gap” in large language models (LLMs). Although LLMs may “know” the correct reasoning steps, they often fail to execute them effectively. To address this, the authors propose a Draft-and-Follow Policy Optimization (DFPO) approach that separates high-level planning (“draft”) from concrete actions (“follow”).  \nPaperCompass integrates hierarchical reinforcement learning, optimizing both knowledge-level and action-level reasoning. It also introduces techniques such as negative sample masking and reward routing to stabilize training. Experiments on two benchmarks (AirQA-Real and SciDQA) show that DFPO significantly improves efficiency, achieving similar accuracy with fewer tool calls and enabling a 3B model to match the performance of much larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The paper clearly defines the “knowing–doing gap” and provides a structured way to address it. The draft-and-follow approach neatly separates planning and execution, inspired by hierarchical reinforcement learning.  \n-  DFPO boosts efficiency without losing accuracy and allows smaller models to perform competitively on research-level QA tasks. The use of negative sample masking and reward routing improves training stability and prevents wasted updates.  \n-  The approach shows that small, structured models can handle complex reasoning tasks efficiently, which is valuable for research applications."}, "weaknesses": {"value": "-  The experiments are limited to paper-QA tasks, so it’s uncertain whether the method generalizes to other reasoning or decision-making domains.  \n-  The multi-stage pipeline (fine-tuning + RL optimization) adds overhead and could be difficult for others to reproduce.  \n-  The framework relies on reliable tool-use environments, so its benefits may shrink in open-ended or less-structured settings."}, "questions": {"value": "- The proposed DFPO framework is evaluated mainly on scientific paper QA tasks. Could the authors discuss how this method might generalize to other domains that also involve reasoning and action?\n- The multi-stage training pipeline (DTFT + DFPO) seems computationally expensive. Can the authors provide an estimate of training cost compared to standard RL fine-tuning methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BPSDrpM1Xr", "forum": "8Jylj3K02W", "replyto": "8Jylj3K02W", "signatures": ["ICLR.cc/2026/Conference/Submission3227/Reviewer_UDhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3227/Reviewer_UDhF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890920470, "cdate": 1761890920470, "tmdate": 1762916610262, "mdate": 1762916610262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called papercompass for extracting information from papers by agents. Specifically, it generates a draft first, then lets the agent work with it. To enhance this process, they also introduced an RL method called DFPO, which could enhance model performance for larger models on PaperQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well structured and comprehensive\n\n2. This paper introduces a novel framework called Papercompass, a fine-tuning method called DTFT and a specific RL method called Draft-Follow Policy Optimization. \n\n3. Papercompass can enhance the capabilities of small models, resulting in lower deployment costs compared to using larger models."}, "weaknesses": {"value": "1. Most results in this paper are lacking human feedback. How does a researcher evaluate the performance of your model?\n\n2. The abbreviation “DTFT” is introduced abruptly in the text, which makes the reading confusing."}, "questions": {"value": "Good and comprehensible work. Can your method (e.g., Papercompass 7B) extract key information effectively from this paper based on your judgment? Compared to a result using the prompt generated from papercompass by a full model, which one do you think is better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dauDPNcd39", "forum": "8Jylj3K02W", "replyto": "8Jylj3K02W", "signatures": ["ICLR.cc/2026/Conference/Submission3227/Reviewer_cwPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3227/Reviewer_cwPa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924229504, "cdate": 1761924229504, "tmdate": 1762916610080, "mdate": 1762916610080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study addresses efficiency issues in existing LLM agents when processing scientific papers, particularly the tendency of small-scale models to fall into repetitive retrieval loops. The core innovation lies in introducing a \"Draft-and-Follow\" architecture inspired by cognitive science, which requires the agent to first generate a high-level planning draft before executing fine-grained tool calls based on that draft. To support this architecture, the researchers developed a specialized reinforcement learning algorithm called DFPO (Draft-and-Follow Policy Optimization), which can simultaneously optimize draft quality and final solutions. Experimental results demonstrate that PaperCompass based on Qwen2.5-3B achieves performance comparable to 32B parameter models on the AirQA-Real benchmark, while significantly improving the interaction efficiency metric I-Avg. Across two major benchmarks (AirQA-Real and SciDQA), the method reduces tool calls by 31.3% without sacrificing accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "**1. Comprehensive Experimental Design**\n\nThe research includes extensive ablation studies and analyses, encompassing entropy dynamics analysis, repetition behavior analysis, and independent contribution evaluation of each component. The paper provides detailed efficiency statistics across multiple dimensions, including correct rate, valid answers, and interaction turns, offering thorough empirical validation of the proposed approach.\n\n**2. Strong Reproducibility**\n\nThe paper provides complete experimental settings and implementation details. The authors have made their code publicly available and documented detailed hyperparameters, including maximum interaction turns, temperature settings, and training configurations. This level of transparency facilitates reproduction and verification of the results by the research community.\n\n**3. Thoughtful innovation**\n\nThe paper cleverly applies the draft-solution paradigm and designs the DFPO algorithm to assign different advantages to draft and solution components. The framework represents a novel streamlined implementation of RL designed to bridge the 'knowing-doing' gap observed in LLMs. The integration of negative sample masking and reward router mechanisms demonstrates thoughtful engineering to address practical training challenges."}, "weaknesses": {"value": "**1. Excessive Theoretical Derivations with Limited Substance**\n\nThe authors attempt to justify their method design through theoretical proofs, but most of these proofs merely restate algorithmic definitions without providing genuine theoretical contributions.\n\n**(1) Misleading \"Regularization\" Claim:** The claim that DFPO's policy gradient is a \"regularized form\" of M-GRPO is imprecise and the derivation lacks theoretical contribution. The authors first define J_DFPO to contain both draft and solution advantage terms, then \"prove\" it can be written as M-GRPO plus an additional term. However, this additional term has no independent theoretical origin—it is purely a product of algebraic rearrangement. The derivation employs only trivial mathematical manipulations: (a) distributive property: decomposing Â^draft into Â^solution + (Â^draft - Â^solution), and (b) symbolic reorganization: defining ΔÂ = (Â^draft - Â^solution). \n\nThe \"regularization term\" in DFPO neither prevents overfitting nor represents an independently added constraint—it is simply a relabeled portion of the original objective function. The paper's use of the term \"regularization\" is highly misleading, as this decomposition is a direct consequence of the objective function's definition. This represents an engineering observation rather than a theoretical contribution.\n\n**(2) Circular Reasoning in Negative Sample Masking:** The actual operation of negative sample masking is straightforward—when the solution is incorrect, directly set the draft advantage to the solution advantage (typically negative). However, the authors construct an elaborate mathematical framework through Theorem 2, Proposition 1, and Lemma 3 to \"prove\" the rationality of this heuristic rule, which essentially constitutes tautological reasoning.\n\nThe logical structure forms a circle: **Premise A**: Negative sample masking sets draft advantage to negative values for wrong solutions; **Theorem 2**: Proves this setting prevents erroneous reinforcement; **Proof method**: Assumes negative sample masking is already in effect, then demonstrates advantages are negative; **Conclusion**: Negative sample masking is justified (because Theorem 2 says it is justified).\n\nThis is a case of \"using rules to prove rules\"—circular argumentation. The paper does not derive the necessity of negative sample masking from independent theoretical foundations; instead, it first defines this rule, then \"proves\" it conforms to its own definition. The core contribution can be stated in one sentence: \"When the solution is wrong, we do not update the draft policy to avoid reinforcing erroneous planning paths\". This is far clearer than the current Theorem 2-Lemma 2-Proposition 1 system.\n\nI do not believe that top-tier ML conferences necessarily require theoretical derivations. The authors seem to feel that simple, effective methods are \"insufficiently academic,\" and thus compensate through mathematical packaging—but this approach undermines the paper's credibility and readability.\n\n**2. Marginal and Sometimes Negative Performance Gains**\n\nOn the AirQA-Real benchmark, PaperCompass demonstrates quite limited accuracy improvements. Specifically, for the Qwen2.5-3B model-based version, accuracy only increased from 22.2% (SFT baseline) to 23.7% (final), an improvement of merely 1.5 percentage points. For the 7B model, the situation is slightly better but still not significant: from 23.9% (SFT) to 25.3% (DFPO), an increase of approximately 1.4 percentage points. In certain subcategories, the 7B model even exhibits negative growth. These results warrant further evaluation of whether the method is genuinely effective.\n\n**3. Lack of Statistical Significance Validation**\n\nThe paper lacks multiple repeated experiments and significance testing. Given that some improvements are marginal (as pointed out in Weakness 2), without significance testing it is difficult to determine whether these represent genuine improvements or merely noise in the experimental results."}, "questions": {"value": "**Regarding Draft Length and Training Signal Strength:**\n\nWhat is the average token length of the draft in the current implementation? What are the average token  lengths of think and answer components per turn? If the draft length constitutes a low proportion across multi-turn interactions, it may be drowned out in batch loss computation. Can the draft be sufficiently learned under such conditions? This concern is particularly relevant given that the draft is crucial to the proposed framework's effectiveness, and insufficient training signal could undermine the core contribution of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UOkSmHW9jt", "forum": "8Jylj3K02W", "replyto": "8Jylj3K02W", "signatures": ["ICLR.cc/2026/Conference/Submission3227/Reviewer_YEmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3227/Reviewer_YEmS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924389749, "cdate": 1761924389749, "tmdate": 1762916609676, "mdate": 1762916609676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a multi-turn RL framework called PaperCompass for training agents for scientific paper querying. Authors take inspiration from cognitive science to bridge the knowing-doing gap by constructing high-level plans, and then getting into more fine-grained actions. They propose a novel DFPO method to train for this method, imposing a hierarchical decision-making process: A high-level\ndraft leverages the LLM’s instruction-following capabilities to guide the low-level follow execution, thereby preventing deviations into greedy action sequences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. Novel hierarchical decision-making approach\n2. Theoretical results add more clarity and justification to the proposed approach."}, "weaknesses": {"value": "Weaknesses:\n1. The authors should explain the knowing-doing gap concept more at the introduction of the paper.\n2. Related work is not present in the main paper.\n3. The details of the synthetic data generation are also deferred to the Appendix. I think the authors should add one paragraph to show how synthetic data is being generated.\n4. Only the Qwen family is considered for finetuning the case."}, "questions": {"value": "Questions:\n1. Why is only one LLM agent used? What happens if two LLM agents are used to generate the draft and to execute the plan, respectively?\n2. On line 408 the authors say that the result is interpreted as the narrowing of knowing doing gap, but the rationale behind this interpretation is not clear. Any metrics to show that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zDj8NKbO90", "forum": "8Jylj3K02W", "replyto": "8Jylj3K02W", "signatures": ["ICLR.cc/2026/Conference/Submission3227/Reviewer_pyt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3227/Reviewer_pyt2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204567868, "cdate": 1762204567868, "tmdate": 1762916609413, "mdate": 1762916609413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}