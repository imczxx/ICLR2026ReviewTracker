{"id": "RAEJAW5Gfi", "number": 18653, "cdate": 1758289748959, "mdate": 1761308259736, "content": {"title": "DPLoRA: A Dual-Pruning Framework based on ILP Optimization and Progressive Pruning for Parameter-Efficient LoRA Fine-Tuning", "abstract": "We propose DPLoRA (Dual-Pruning Low-Rank Adaptation), an optimized Low-Rank Adaptation (LoRA) method for parameter-efficient fine-tuning of large language models. Our approach introduces a two-stage compression framework: (1) an initial pruning stage, OPLoRA, that formulates a first ILP problem to automatically discover the optimal layer-wise LoRA rank ($r$) configuration before training; (2) a progressive pruning stage that formulates a second ILP problem during training, incorporating Exponential Moving Average (EMA) of layer-wise importance scores to further reduce rank ($r$) adaptively. On the GLUE benchmark, our first stage, OPLoRA, achieves a new state-of-the-art (SOTA) performance, surpassing all baselines. Furthermore, the full DPLoRA framework also demonstrates superior capabilities, outperforming strong PEFTs like AdaLoRA and SoRA while achieving up to an 80\\% reduction in trainable parameters and a 50\\% reduction in training time. This study offers a new direction for efficiently deploying large-scale language models in resource-constrained environments.", "tldr": "Dual Pruning Framework based on ILP", "keywords": ["Parameter-Efficient Fine-Tuning", "Integer Linear Programming", "Dual Pruning Framework", "Low-Rank Adaptation", "Large Language Models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/22e1b2647945e8308468f26eedb485d1ef0150eb.pdf", "supplementary_material": "/attachment/67ad7b0d698666403dd44ffe700cb5bd2c4f333c.zip"}, "replies": [{"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "PBzQCvbRzo", "forum": "RAEJAW5Gfi", "replyto": "RAEJAW5Gfi", "signatures": ["ICLR.cc/2026/Conference/Submission18653/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18653/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308259013, "cdate": 1761308259013, "tmdate": 1761308259013, "mdate": 1761308259013, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}