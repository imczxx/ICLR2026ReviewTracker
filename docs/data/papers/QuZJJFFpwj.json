{"id": "QuZJJFFpwj", "number": 17782, "cdate": 1758280461692, "mdate": 1759897154203, "content": {"title": "Robust Backdoor Removal by Reconstructing Trigger-Activated Changes in Latent Representation", "abstract": "Backdoor attacks pose a critical threat to machine learning models, causing them to behave normally on clean data but misclassify poisoned data into a poisoned class. \nExisting defenses often attempt to identify and remove backdoor neurons based on Trigger-Activated Changes (TAC) which is the activation differences between clean and poisoned data. \nThese methods suffer from low precision in identifying true backdoor neurons due to inaccurate estimation of TAC values.\nIn this work, we propose a novel backdoor removal method by accurately reconstructing TAC values in the latent representation. Specifically, we formulate the minimal perturbation that forces clean data to be classified into a specific class as a convex quadratic optimization problem, whose optimal solution serves as a surrogate for TAC. We then identify the poisoned class by detecting statistically small $L^2$ norms of perturbations and leverage the perturbation of the poisoned class in fine-tuning to remove backdoors. \nExperiments on CIFAR-10, GTSRB, and TinyImageNet demonstrated that our approach consistently achieves superior backdoor suppression with high clean accuracy across different attack types, datasets, and architectures, outperforming existing defense methods.", "tldr": "", "keywords": ["Trigger-Activated Changes", "Backdoor Removal", "Backdoor Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1880fe85fa60bdaa21a67a1263f6af4a60d5f2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a backdoor defense method that reconstructs Trigger-Activated Changes (TAC) in the latent representation of a poisoned model. The method formulates the reconstruction of TAC as a convex quadratic optimization problem that finds the minimal perturbation forcing all clean samples to be classified into a specific class. The poisoned class is identified by comparing perturbation norms, and then fine-tuned the model by using the corresponding perturbation to neutralize backdoor effects. Thereby achieving effective defense against backdoor attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formulates the reconstruction of Trigger-Activated Changes (TAC) as a quadratic convex optimization problem, offering a systematic approach to analyze backdoor effects.\n2. This paper provide detailed theoretical modeling and derivation, demonstrating that the proposed defense method admits stable solutions.\n3. The method is empirically compared with several recent defense techniques, showing its effectiveness and robustness in practice."}, "weaknesses": {"value": "1.\tThe method requires solving one convex QP per class, which may become impractical for large-scale models.\n2.\tThe approach is limited to single-target scenarios and does not address multi-target or multi-trigger backdoors.\n3.\tThe method’s performance depends heavily on thresholds α and β, but no adaptive or learning-based tuning mechanism.\n4.\tExperiments are conducted only on ResNet models and image datasets, which may limit the generalizability of the results.\n5.\tLow poisoning rates may reduce the accuracy of detecting poisoned classes, thereby affecting the overall defense performance."}, "questions": {"value": "1.\tA low poisoning rate may increase the minimum perturbation required to misclassify clean samples into poisoned classes, which could affect the selection of poisoned categories and ultimately influence the overall defense results. Experiments with varying poisoning rates could be added to demonstrate that the proposed method remains effective even in low poisoning rates.\n2.\tManually tuning α and β, where α controls poisoned class identification and β balances backdoor defense with task accuracy, is time-consuming and often leads to unstable or suboptimal performance. I wonder if it is possible to use an adaptive strategy to make the process more efficient and reliable"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V8KxEjKF14", "forum": "QuZJJFFpwj", "replyto": "QuZJJFFpwj", "signatures": ["ICLR.cc/2026/Conference/Submission17782/Reviewer_SXdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17782/Reviewer_SXdY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396574519, "cdate": 1761396574519, "tmdate": 1762927624262, "mdate": 1762927624262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel backdoor removal framework that reconstructs Trigger-Activated Changes (TAC) in the latent representation of neural networks to achieve robust backdoor defense. The method computes minimal perturbations that force a model to misclassify clean data into each class, identifies the poisoned class through statistical outlier detection in the L2-norm of these perturbations, and then fine-tunes the model using the perturbation corresponding to the identified poisoned class. Experiments on CIFAR-10, GTSRB, and TinyImageNet demonstrate improved defense performance compared to state-of-the-art methods while maintaining high clean accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of reconstructing TAC in the latent representation through convex quadratic optimization offers a neat and interpretable surrogate approach that does not rely on poisoned data. This reformulation is novel and mathematically well-grounded.\n- The mathematical explanation is solid and convincing, although it is also not easy to understand.\n- The empirical evidence of using the smallest-perturbed class is clear and convincing.\n- The experiments are solid with a comprehensive comparison with the baselines. And leave nearly no improvement for future research."}, "weaknesses": {"value": "- There is a lack of clear outlines for the appendix content, making it hard to find the remaining experiments and the desired explanations.\n- Solving multiple convex programs per class may be nontrivial for large-scale models (e.g., high-dimensional latent spaces or hundreds of classes). No analysis of time or resource overhead is given.\n- The extensive experiments related to the scalability are needed to further verify the effectiveness of the proposed method. For example, the experiments on a larger model (e.g., ViT) and a more complex dataset (e.g., ImageNet). The current results (e.g., Table 2) show that the SOTA baseline (e.g., SAU) already performs good enough, weakening your contribution in this field."}, "questions": {"value": "Can you provide more evidence from a bigger scale (e.g., weakness 3 above) to show the superiority of your method? Or can you provide some intuitive explanations to show that we need your contribution for the community? It can be either insights (e.g., how reconstructed TAC contributes to future research) or empirical results (e.g., how your method solves the corner cases that are previously unsolved)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "24steBx9mT", "forum": "QuZJJFFpwj", "replyto": "QuZJJFFpwj", "signatures": ["ICLR.cc/2026/Conference/Submission17782/Reviewer_sDFY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17782/Reviewer_sDFY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639183352, "cdate": 1761639183352, "tmdate": 1762927623646, "mdate": 1762927623646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel backdoor defense framework that removes backdoors from neural networks by reconstructing Trigger-Activated Changes (TAC), the differences in neuron activations between clean and poisoned data, without needing poisoned samples. The TAC reconstruction is performed by computing a minimal perturbation for each class."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Extensive experiments on multiple datasets and attacks demonstrate better or comparable performance over prior methods.\n\n- The presentation of the paper is easy to follow.\n\n- The motivation is clear, and the proposed method addresses an important problem."}, "weaknesses": {"value": "- The experiments are primarily on ResNet-18.\n\n- The method assumes one poisoned class, which may limit performance in multi-target or all-to-all attacks.\n\n- Experiments do not include large datasets, such as ImageNet-1K.\n\n- The performance is not significantly better than all baselines, such as FT-SAM."}, "questions": {"value": "Thanks for the interesting work. I have a few questions and suggestions.\n\n- Computational overhead. As the proposed method requires computing \"minimal perturbation\" for every class. What is the computational cost of this method?\n\n- Why not directly remove the high-TAC neurons? If the proposed TAC reconstructing method is effective, removing the high-TAC neurons should also work. In addition, the authors could also provide some figures to demonstrate the reconstructed TAC values, like Figure 2 in [A].\n\n- Transformer-based architectures, such as ViT. I suggest the authors include experiments on more architectures, such as ViT.\n\n[A] Towards Backdoor Stealthiness in Model Parameter Space"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pInuo1td84", "forum": "QuZJJFFpwj", "replyto": "QuZJJFFpwj", "signatures": ["ICLR.cc/2026/Conference/Submission17782/Reviewer_vvLe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17782/Reviewer_vvLe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936808093, "cdate": 1761936808093, "tmdate": 1762927623203, "mdate": 1762927623203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approximate TAC-based method for backdoor removal and defense. In particualr, in the feature space, perturbations are optimized to force clean data to be classified into a specific class. Generated perturbations are used to distinguish between benign and backdoor samples and then utilized in fine-tuning to remove backdoors. This defense method is inspired by TAC, while it is also closely related to feature space backdoor defenses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This method uses clean data to generate the perturbations, making it suitable for realistic defender settings where poisoned data are unavailable. Later, perturbations can be used for both detection and removal."}, "weaknesses": {"value": "Comparison with feature-space defenses. While the paper is inspired by Trigger-Activated Changes (TAC), its practical implementation closely resembles feature-space backdoor defenses[a]. However, the paper provides limited comparative analysis with these prior methods. A deeper comparison would strengthen the contribution and clarify the novelty.\n\nAdaptive evaluation. The work does not evaluate the defense under adaptive or defense-aware backdoor attacks. Since the proposed method depends on the assumption that poisoned-class perturbations exhibit smaller L2 norms, an attacker aware of this could manipulate with this regard. Testing against attacks that minimize perturbation norms would provide more substantial evidence of robustness.\n\nHyperparameters. The defense relies on several dataset-specific hyperparameters, such as the outlier threshold. The paper gives limited guidance on how these parameters generalize across datasets or model architectures. In addition, reproducibility could be improved by reporting computational cost and sensitivity analyses.\n\n[a]Towards Stable Backdoor Purification through Feature Shift Tuning. NeurIPS 2023."}, "questions": {"value": "Compare with feature space defenses, discuss adaptive attacks, discuss the generalization w.r.t hyperparameters"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SNGBn04lLd", "forum": "QuZJJFFpwj", "replyto": "QuZJJFFpwj", "signatures": ["ICLR.cc/2026/Conference/Submission17782/Reviewer_norE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17782/Reviewer_norE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762682763526, "cdate": 1762682763526, "tmdate": 1762927622809, "mdate": 1762927622809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}