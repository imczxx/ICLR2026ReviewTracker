{"id": "SjHaqpXDVW", "number": 16791, "cdate": 1758268721296, "mdate": 1759897219287, "content": {"title": "PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis", "abstract": "Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.", "tldr": "We address the motion token granularity issue of LLM in text-to-motion tasks, and propose progressive planning method and flow-enhanced motion tokenizer.", "keywords": ["Human 3D Motion Generation; Text-to-Motion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a90b3da82fdb10ed0f56020faaa3c997d27d985.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents PlanMoGPT, an LLM-based framework that addresses the trade-off between global coherence and motion detail in text-to-motion generation. It introduces a progressive planning mechanism that uses the LLM’s autoregressive abilities to generate motion tokens hierarchically—starting from sparse global plans and refining to full sequences—and a flow-enhanced fine-grained tokenizer that doubles temporal resolution and expands the codebook eightfold to reduce discretization loss. A flow-enhanced decoder further restores motion nuances. Experiments on standard benchmarks show state-of-the-art performance, with a 63.8% FID improvement on long-sequence generation (0.380 → 0.141) and a 49.9% boost in motion diversity, effectively resolving the diversity–quality trade-off that limits current non-LLM methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper points the granularity bottleneck in motion tokenization and tackles it with a coherent “plan-then-detail” pipeline—progressive planning for global-to-local consistency and a flow-enhanced fine-grained tokenizer to retain details, plus a flow-matching decoder to restore nuances.\n2. Strong long-sequence performance: On newly built long-motion benchmarks, PlanMoGPT delivers large gains (FID 0.380→0.141, +49.9% diversity), effectively breaking the diversity–quality trade-off that hampers non-LLM approaches and showing excellent long-range semantic alignment.\n3. Comprehensive and careful experimentation: The authors evaluate across standard datasets (HumanML3D, KIT-ML) and introduce two extended long-motion datasets (HumanML3D++, KIT-ML++) constructed via motion concatenation with GPT-4 text merging and human QC. They report extensive baselines (diffusion and token-based), ablations (codebook, flow vs residual VQ-VAE, plan intervals, text encoders), diversity–quality analysis, inference cost, and user studies.\n4. Generality and robustness: The flow-enhanced tokenizer improves other frameworks (e.g., MoMask), indicating that the proposed tokenization/decoding scheme is transferable beyond their own LLM planner."}, "weaknesses": {"value": "The paper primarily reports results with a single decoder-only LLM (TinyLLaMA-1.1B), but it lacks a systematic study across multiple LLM sizes and families (e.g. Qwen, Gemma)."}, "questions": {"value": "1. What is the length and the corresponding motion length of the motion interval?\n2. What's the speed of your model? how many frames can you generate in one second on average?\n3. Why do you choose TINY-LLaMA 1.1B as your base model?\n4. Does your model support reasoning-driven motion generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x6YJvUaFNp", "forum": "SjHaqpXDVW", "replyto": "SjHaqpXDVW", "signatures": ["ICLR.cc/2026/Conference/Submission16791/Reviewer_zHZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16791/Reviewer_zHZd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760867089936, "cdate": 1760867089936, "tmdate": 1762926830011, "mdate": 1762926830011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the issue of the granularity of motion tokenization to improve the performance of text-to-motion. To relieve this issue, this paper introduces PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. Extensive experiments on HumanML3D, HumanML3D++, and KIT-ML++ demonstrate the effectiveness ofthe  proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper focuses on the issue of the granularity of motion tokenization and introduces flow-matching into motion tokenization to propose flow-enhanced fine-grained motion tokenization. This paper also introduces progressive generation for an LLM-based motion generation model. Comprehensive ablation experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "There are two experimental results in this paper that cannot support the contribution of the paper：\n1. PlanMoGPT achieves suboptimal results on the KIT-ML dataset.\n2. Introducing time interval 8 does not improve the text-to-motion performance, and time interval 6 leads to higher FID."}, "questions": {"value": "1. Reducing the time sampling rate and introducing a multi-granularity time interval will cause the sequence to become longer. Do authors consider the issue of reduced generation efficiency due to longer sequences?\n2. Since the author mentioned that PlanMoGPT's poor performance on KIT-ML is due to the small size of the dataset, have the authors tried training on a larger dataset, such as SnapMoGen[1] or Motion-X[2]?\n[1] Guo C, Hwang I, Wang J, et al. SnapMoGen: Human Motion Generation from Expressive Texts[J]. arXiv preprint arXiv:2507.09122, 2025.\n[2] Lin J, Zeng A, Lu S, et al. Motion-x: A large-scale 3d expressive whole-body human motion dataset[J]. Advances in Neural Information Processing Systems, 2023, 36: 25268-25280."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUI8aAk0Hm", "forum": "SjHaqpXDVW", "replyto": "SjHaqpXDVW", "signatures": ["ICLR.cc/2026/Conference/Submission16791/Reviewer_PRLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16791/Reviewer_PRLB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385039185, "cdate": 1761385039185, "tmdate": 1762926829498, "mdate": 1762926829498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PlanMoGPT, a LLM–based framework for text-to-motion generation. It identifies the local dependency problem in fine-grained motion tokenization as a key limitation of existing approaches and addresses it through a progressive planning strategy, where motion is generated from coarse global plans to fine-grained details, and a flow-enhanced motion tokenizer that improves motion representation and reconstruction. Experiments on their proposed datasets suggesting that PlanMoGPT achieves superior motion quality and diversity compared to prior methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes PlanMoGPT, which demonstrates notable performance improvements on the authors’ customized benchmarks"}, "weaknesses": {"value": "1. Lacks novelty:\n    - the paper appears to be an incremental improvement, and the scientific contribution is not clearly articulated. Much of the work seems engineering-oriented (e.g., “doubles the downsampling resolution and expands the codebook size by eight times” as stated in the abstract).\n2. Writing and presentation issues.\n    1. The overall narrative lacks clarity. The introduction discusses problems of LLMs, but the method actually targets issues inherent to Transformers in general, not specifically LLMs.\n    2. Additionally, the first paragraph attributes the issue to LLMs, while the second paragraph shifts focus to tokenization as the core challenge, implying the problem lies in the tokenizer rather than the LLM. This weakens the logical coherence of the argument.\n    3. Missing results in Table 2. Table 2 includes KIT++ results but omits KIT, although the implementation details (lines #306–312) suggest that experiments on KIT-ML were conducted.\n    4. Ambiguity in Table 3(b). Table 3 states that “base” refers to not using the residual Transformer or flow-enhanced method. However, the table includes rows labeled with both “Flow” and “Base,” which creates confusion about whether the flow-enhanced method was used.\n    5. In lines #74–89, “first” and “firstly” are used, but there is no corresponding “secondly”\n3. Experiments:\n    1. Limited comparison on proposed datasets. In Table 2, results on HumanML3D++ and KIT-ML++ are compared with only two other models. It lacks comprehensive comparison and thus weakens the empirical support. It would be helpful to include results of BAMM and other LLM-based approaches.\n    2. Limited performance gains on commonly used dataset HumanML-3D. The method shows only marginal improvement on HumanML-3D.\n4. Insufficient explanation of flow-enhanced method: The paper does not clearly explain how the proposed flow-enhanced method addresses the issue of overemphasizing short-term performance, which is highlighted as a key motivation in the abstract."}, "questions": {"value": "1. What are the results on the KIT-ML dataset?\n2. Could you provide further clarification on the issues described in Weakness 2(d)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gD1KABS5fY", "forum": "SjHaqpXDVW", "replyto": "SjHaqpXDVW", "signatures": ["ICLR.cc/2026/Conference/Submission16791/Reviewer_C3L6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16791/Reviewer_C3L6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925094186, "cdate": 1761925094186, "tmdate": 1762926828901, "mdate": 1762926828901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PlanMoGPT is an LLM-based framework for text-to-motion generation that tackles fine-grained tokenization bottlenecks by integrating progressive planning and flow-enhanced tokenization. It leverages LLMs' autoregressive capabilities to refine sparse global plans into full sequences and expands the tokenizer's codebook while minimizing discretization loss, achieving state-of-the-art results on benchmarks with a 63.8% FID improvement and 49.9% diversity boost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The writing and structure of the paper are clear and easy to follow.\n- The authors conducted comprehensive experiments on multiple public datasets, demonstrating improvements in numerical metrics for the proposed method."}, "weaknesses": {"value": "- The paper lacks video samples. For a 3D motion generation model, providing diverse generated video samples is crucial, as it intuitively showcases the model's generation capabilities and quality. Without video samples, it is difficult for me to assess the model's actual performance, and as a reviewer, I cannot accept a 3D motion generation paper without any video samples.\n- The baseline methods compared are outdated. The authors should include comparisons with the latest state-of-the-art approaches, such as works [1-5], in 3D human motion generation. The current comparisons fail to convincingly show the proposed method's superiority.\n- Additionally, this paper is an LLM-based 3D motion generation model, yet numerous LLM-based related works are not cited or compared, such as [6-8]. The absence of comparisons with these relevant works makes the paper's contributions unclear and hinders the evaluation of its novelty and effectiveness.\n- Similarly, while the paper focuses on long-sequence motion generation, it lacks comparisons with many related works in long-sequence motion generation, such as [9-10].\n\n\n[1]: Guo C, Hwang I, Wang J, et al. SnapMoGen: Human Motion Generation from Expressive Texts[J]. arXiv preprint arXiv:2507.09122, 2025.\n\n[2]: Meng Z, Xie Y, Peng X, et al. Rethinking diffusion for text-driven human motion generation[J]. arXiv preprint arXiv:2411.16575, 2024.\n\n[3]: Zhang J, Fan H, Yang Y. Energymogen: Compositional human motion generation with energy-based diffusion model in latent space[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 17592-17602.\n\n[4]: Yuan W, He Y, Shen W, et al. Mogents: Motion generation based on spatial-temporal joint modeling[J]. Advances in Neural Information Processing Systems, 2024, 37: 130739-130763.\n\n[5]: Zhang Z, Kong B, Liu Q, et al. Towards robust and controllable text-to-motion via masked autoregressive diffusion[C]//Proceedings of the 33rd ACM International Conference on Multimedia. 2025: 9326-9335.\n\n[6]: Wang Y, Huang D, Zhang Y, et al. Motiongpt-2: A general-purpose motion-language model for motion generation and understanding[J]. arXiv preprint arXiv:2410.21747, 2024.\n\n[7]: Xu H, Xu G, Zheng Z, et al. VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models[J]. arXiv preprint arXiv:2508.12081, 2025.\n\n[8]: Wu B, Xie J, Shen K, et al. MG-MotionLLM: A unified framework for motion comprehension and generation across multiple granularities[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 27849-27858.\n\n[9]: Zhang Z, Liu A, Reid I, et al. Motion mamba: Efficient and long sequence motion generation[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 265-282.\n\n[10]: Lee T, Baradel F, Lucas T, et al. T2lm: Long-term 3d human motion generation from multiple sentences[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 1867-1876."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lo2ApRiTxS", "forum": "SjHaqpXDVW", "replyto": "SjHaqpXDVW", "signatures": ["ICLR.cc/2026/Conference/Submission16791/Reviewer_K8SB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16791/Reviewer_K8SB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762786643137, "cdate": 1762786643137, "tmdate": 1762926828458, "mdate": 1762926828458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}