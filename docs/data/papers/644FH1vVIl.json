{"id": "644FH1vVIl", "number": 10245, "cdate": 1758165003443, "mdate": 1759897663713, "content": {"title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference", "abstract": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. We conduct a systematic analysis across models and datasets and discover a U-shaped entropy pattern: high entropy on simple problems despite high accuracy, low entropy on medium difficulty, and high entropy on hard problems reflecting uncertainty. The 22--25\\% entropy reduction from simple to optimal regions reveals a fundamental inefficiency—an \\emph{overthinking} phenomenon on easy instances. Building on these insights, we introduce \\textbf{DiffAdapt}, a lightweight, deployment-ready framework that predicts problem difficulty from hidden states and selects among Easy/Normal/Hard reasoning strategies to allocate computation adaptively. DiffAdapt requires no retraining of the base LLM and is compatible with common inference optimizations. Across five models and eight benchmarks, DiffAdapt achieves comparable or improved accuracy while reducing token usage by up to 22.4\\%, establishing a practical path toward compute-efficient reasoning.", "tldr": "", "keywords": ["LLM reasoning", "Adaptive Inference", "Entropy Analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d96d2aef736a597bc572c3f1ea5d5e539135bff9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the inefficiency of reasoning LLM, which often generate excessively long reasoning traces regardless of task difficulty. Through a systematic empirical analysis across multiple models and datasets, the authors discover a U-shaped entropy pattern on simple problems, low entropy on medium difficulty, and again high entropy on hard problems. This reveals a phenomenon termed overthinking, where models over-allocate computational resources to easy problems. Building on this observation, the paper proposes DiffAdapt, a lightweight, training-free framework that predicts problem difficulty from the model’s hidden states and dynamically selects among three reasoning strategies to adapt computational budget during inference. DiffAdapt requires no retraining of the base LLM and is compatible with existing inference systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed DiffAdapt is a simple yet effective solution that does not require retraining or finetuning the base LLM. By attaching a small hidden state probe to predict difficulty, the method enables difficulty-aware computation allocation at inference time, making it highly practical for real world deployment. The paper conducts extensive experiments across five reasoning models and eight benchmarks, providing convincing evidence for the generality and robustness of DiffAdapt across architectures, scales, and domains. It can be seamlessly combined with reinforcement-learning-based length control methods such as ThinkPrune and LC-R1, as well as deployed under common inference frameworks, which underscores its engineering value and scalability."}, "weaknesses": {"value": "1. Limited evaluation of reasoning-chain integrity under tight computational budgets. The paper primarily reports accuracy, token reduction, and speed-up metrics but does not evaluate the integrity or consistency of reasoning traces (e.g., whether truncated reasoning affects logical completeness). This omission makes it difficult to judge whether DiffAdapt preserves coherent chain-of-thought reasoning when aggressive token reduction is applied."}, "questions": {"value": "1. Appendix D.2 states that thresholds for distinguishing Easy/Normal/Hard are heuristically chosen per model based on entropy, with only a small sanity check. Since these thresholds depend on model family, temperature, and domain, does this design undermine generalization and zero-shot ability? Would each new deployment require manual calibration?\n2. Could the authors show how often misclassification (e.g., Hard -> Easy) causes early truncation or logical failure, and evaluate the real effectiveness of this fallback mechanism during deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QCD0qqCqX4", "forum": "644FH1vVIl", "replyto": "644FH1vVIl", "signatures": ["ICLR.cc/2026/Conference/Submission10245/Reviewer_7RDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10245/Reviewer_7RDd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921216307, "cdate": 1761921216307, "tmdate": 1762921604493, "mdate": 1762921604493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DiffAdapt, a lightweight, training-free framework for difficulty-adaptive reasoning in large language models (LLMs). The authors identify a U-shaped entropy pattern across problem difficulty levels, indicating an “overthinking” phenomenon where models exhibit high uncertainty on easy problems despite high accuracy. To address this, DiffAdapt uses a small probe trained on the model’s hidden states to predict problem difficulty and select among Easy/Normal/Hard inference strategies dynamically, without retraining the base model. Experiments on five LLMs and eight benchmarks show that DiffAdapt maintains or improves accuracy while reducing token usage by up to 22.4%, and decreases end-to-end inference time by up to 6×. The framework is compatible with existing inference systems and complements length-control RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work identifies and systematically analyzes a previously underexplored phenomenon—overthinking in reasoning LLMs—via a novel U-shaped entropy pattern analysis. This empirical finding is insightful and establishes a theoretical foundation for adaptive reasoning.\n \n2. The proposed DiffAdapt framework is conceptually elegant: predicting difficulty from hidden states and dynamically adjusting reasoning strategy without any retraining of the base model.\n\n3. The experimental section is comprehensive, spanning five models and eight benchmarks (both in-domain and out-of-domain). The Oracle analysis provides solid upper bounds that justify the design choices.\n\n4. The contribution is practically important: it offers a lightweight, deployment-ready framework. DiffAdapt requires no retraining of the base LLM and is compatible with common inference optimizations."}, "weaknesses": {"value": "1. The evaluation is heavily focused on mathematical and scientific reasoning. It remains unclear whether the proposed three-regime (Easy/Normal/Hard) framework generalizes to other domains, such as commonsense reasoning, dialogue, or creative writing, where difficulty is harder to quantify.\n\n2. The difficulty predictor leverages only prefill hidden states, which simplifies deployment but may overlook valuable cues from generation dynamics. Discussion or experimentation on this trade-off would be helpful.\n\n3. The paper does not provide ablations on the probe’s architecture or its sensitivity to training data size. Such results would clarify how much of DiffAdapt’s performance depends on probe complexity or data scale."}, "questions": {"value": "1. How sensitive is DiffAdapt to the chosen thresholds (α, β, γ)? \n\n2. Could DiffAdapt extend beyond reasoning tasks—for example, to summarization or dialogue where difficulty varies dynamically?\n\n3. How does probe misclassification (wrong strategy selection) affect performance or stability?\n\n4. In the Hard strategy, generating only a \"method outline\" may fail on problems requiring detailed computation. Could you discuss potential limitations of this approach—particularly for problems that may still benefit from detailed computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O9vTRbhNgQ", "forum": "644FH1vVIl", "replyto": "644FH1vVIl", "signatures": ["ICLR.cc/2026/Conference/Submission10245/Reviewer_S9HU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10245/Reviewer_S9HU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953856017, "cdate": 1761953856017, "tmdate": 1762921604083, "mdate": 1762921604083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper observes a U‑shaped entropy–difficulty pattern (high entropy for easy/hard, low for medium) and posits “overthinking” on easy items. DiffAdapt trains a light probe on prefill hidden states to predict Easy/Normal/Hard and selects a matching reasoning strategy at inference. Across five model families and eight benchmarks (including OOD), DiffAdapt maintains or improves accuracy while reducing tokens (up to 22.4%). The approach is orthogonal to Length‑Control RL and integrates with mainstream serving stacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strategy selection (not just length/temperature) based on a learned difficulty proxy. Notably, the paper’s identification of a U-shaped entropy–difficulty curve is an interesting empirical finding. Previous work mainly reported monotonic increases of entropy with problem difficulty, but not this symmetric “overthinking” pattern on easy items. This observation provides a concrete diagnostic for inefficiencies in reasoning-token allocation.\n- Consistent savings across models/benchmarks; OOD and LC-RL results broaden applicability.\n- Oracle and ablation studies (vs. fixed strategies, DEER) clarify where gains come from."}, "weaknesses": {"value": "- Heuristic thresholds (α,β,γ) are set per model from scatterplots; it’s unclear how stable they are across multi‑task mixtures and domain shifts, or how often the probe/thresholds need re‑tuning.\n- Stage‑1 data generation (multiple long samples per item) is expensive for new domains; low‑budget or few‑shot variants are not discussed.\n- Missing head‑to‑head with “when‑to‑think” switchers (e.g., Thinkless/AdaCoT); this would position the magnitude of gains."}, "questions": {"value": "- Multi‑task generalization: In a realistic mixed‑task setting, how robust are the probe and (α,β,γ) without re‑tuning? Can you show cross‑domain transfer or per‑task calibration drift?\n- Sensitivity of results to (α,β,γ); can you show heatmaps or robust ranges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xNS4FB5Vd", "forum": "644FH1vVIl", "replyto": "644FH1vVIl", "signatures": ["ICLR.cc/2026/Conference/Submission10245/Reviewer_PqhK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10245/Reviewer_PqhK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992894395, "cdate": 1761992894395, "tmdate": 1762921603626, "mdate": 1762921603626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the overthinking phenomenon in reasoning LLMs, where models exhibit high uncertainty and unnecessarily long reasoning traces even on simple problems. Through systematic entropy analysis across models and datasets, the authors reveal a consistent U-shaped entropy pattern across difficulty levels, indicating computational inefficiency. Building on this finding, the authors propose DiffAdapt, a difficulty-adaptive inference framework that predicts problem difficulty using a lightweight probe on hidden states and dynamically selects from Easy/Normal/Hard reasoning strategies. The approach requires no retraining of the base LLM and can be deployed with existing inference frameworks. Empirical results on five models and eight benchmarks show that DiffAdapt can reduce token usage by up to 22.4% while maintaining or improving accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed DiffAdapt framework only requires training a small external probe, yet it effectively improves reasoning efficiency and model performance without modifying or fine-tuning the LLM itself.\n- The identification of a consistent U-shaped entropy–difficulty pattern is novel and insightful. It sheds light on the overthinking behavior of reasoning models and provides valuable guidance for future “long-to-short” reasoning research.\n- The framework is complementary to reinforcement learning–based length control methods (e.g., ThinkPrune). This suggests DiffAdapt can be combined with those training-based long-to-short approaches for even greater efficiency gains."}, "weaknesses": {"value": "- Model-specific probe requirement: The main weakness lies in the need to train a separate probe for each LLM. This introduces (1) additional training overhead and (2) potential generalization issues — the probe’s limited transferability could restrict DiffAdapt’s applicability across different models or domains.\n- Lack of probe analysis experiments: The paper would be stronger if it included an analysis of the probe’s generalization ability. For example: (a) How well does a probe trained on the DeepMath dataset transfer to other reasoning tasks or domains? (b) Can probes be transferred between models of similar architecture or scale (e.g., between Qwen-3/4B and LLama-3B)?\n- Ad-hoc reasoning strategy design: The selection of parameters for the Easy/Normal/Hard strategies (e.g., temperature = 0.5/0.8/0.4, token ratio = 0.4×/1.0×/0.5×) feels somewhat heuristic. The paper lacks experimental justification or ablation to support why these specific hyperparameters are optimal."}, "questions": {"value": "- Have the authors tested whether a probe trained on DeepMath generalizes to other domains or reasoning benchmarks?\n- How transferable is the probe across models of similar size or architecture?\n- What motivates the specific parameter choices for the three reasoning strategies, and have alternative configurations been compared experimentally?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "klAD1LrhQc", "forum": "644FH1vVIl", "replyto": "644FH1vVIl", "signatures": ["ICLR.cc/2026/Conference/Submission10245/Reviewer_zvSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10245/Reviewer_zvSP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998776074, "cdate": 1761998776074, "tmdate": 1762921603258, "mdate": 1762921603258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}