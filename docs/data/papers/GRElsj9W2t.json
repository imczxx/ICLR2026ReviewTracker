{"id": "GRElsj9W2t", "number": 5510, "cdate": 1757917008488, "mdate": 1759897970281, "content": {"title": "Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening", "abstract": "Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesired visual artifacts due to the misalignment between two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. We thus propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. MPD alleviates the misalignment by reconstructing the denoised estimate of the backward path from distilled forward motion residual. With our method, we can deliberately avoid denoising end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. Our method can be applied to off-the-shelf inbetweening works without any modification of model parameters. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.", "tldr": "", "keywords": ["Diffusion", "Generative Inbetweening", "Video Interpolation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c56f2419ea9619b7236982784fae08383601808.pdf", "supplementary_material": "/attachment/6c9e91be33a14120cb6e04482977a6e503c956a4.zip"}, "replies": [{"content": {"summary": {"value": "Inference-time sampling strategies for video interpolation, which leverage the generative priors of large-scale pretrained image-to-video (I2V) models without additional training (e.g., TRF and ViBiDSampler), often suffer from temporal discontinuities. These methods typically fuse or alternate between temporally forward paths (starting from latent $z_{start}$) and backward paths (starting from latent $z_{end}$). However, misalignment between the two paths, arising from differently conditioned motion priors, frequently leads to inconsistent temporal coherence.\n\nThis work proposes an inference-time distillation technique that transfers motion priors from the forward path (starting from $z_{start}$) to the backward path during the early sampling stage ($T \\rightarrow (1 - \\gamma)T$). Specifically, the method computes frame-wise residuals from the forward path and recursively injects these residuals into the end-frame latent $z_{end}$ to generate the backward path. As a result, the backward path is effectively constrained with $z_{end}$, while the diffusion prior is applied only to the forward path—thereby reducing misalignment and improving temporal coherence.\n\nAfter applying inference-time distillation in the early sampling stage ($T \\rightarrow (1 - \\gamma)T$), the method transitions to existing sampling frameworks such as TRF and ViBiDSampler, demonstrating that the proposed technique effectively enhances their performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) Good motivation and novel framework\n\nMotivated by the goal of resolving the bidirectional path misalignment problem, the authors propose a novel framework called Motion Prior Distillation. This approach injects motion information from the forward path into the end-frame latent $z_{end}$ to guide the generation of the backward path. As a result, the method constrains to terminate at the end-frame latent $z_{end}$ while utilizing only the forward-path motion prior, effectively mitigating bidirectional path misalignment. Using only the forward-path motion prior in this context is both novel and insightful.\n\n2) Comprehensive experiments\n\nThe authors clearly demonstrate that applying Motion Prior Distillation during the early sampling stage enhances the performance of existing inference-time frameworks such as TRF and ViBiDSampler. In addition, they conduct extensive ablation studies to explore the underlying principles of the proposed approach, which are appropriately designed and well-analyzed in terms of quantitative metrics.\n\n3) Zero-shot improvement\n\nIt is worth highlighting that the proposed method improves upon prior works while maintaining a zero-shot setting. This makes the contribution particularly relevant and appealing to researchers working on video diffusion and zero-shot video interpolation."}, "weaknesses": {"value": "1) Limited explanation of early-stage application\n\nAfter carefully reviewing the submission, it remains unclear why the proposed method should be applied only during the early stage of the sampling process. The authors briefly attribute this choice to the “coarse-to-fine property of diffusion sampling,” but a more detailed explanation is needed. In particular, the paper should clarify why the proposed method is especially effective for correcting global or low-frequency structures, thereby helping readers understand the underlying principle of why the approach works well in the early stage.\n\n2) Additional ablation study on distillation step ratio\n\nBuilding on the previous point, an ablation study investigating different distillation step ratios, especially values larger than 0.3, would help substantiate the authors’ claims regarding the optimal range and behavior of the proposed method.\n\n3) Limited demonstration of computational efficiency\n\nWhile prior works typically report quantitative efficiency metrics such as runtime and VRAM usage, this paper lacks such evaluations. Including these measurements would make the efficiency claims more convincing and allow for fair comparison with existing approaches."}, "questions": {"value": "1) Clarification on the necessity of switching methods\n\nAs mentioned in Weakness 1, it is unclear why the proposed method cannot perform the full sampling process independently. The authors should clarify why it is necessary to switch to another sampling framework midway, rather than continuing the proposed method throughout the entire process.\n\n2) Additional ablation on distillation step ratio\n\nIt would be helpful to include additional ablation studies that extend the distillation step ratio up to 1.0. This would make it clearer how the proposed method behaves when applied across the full sampling range and help justify the chosen configuration.\n\n3) Quantitative efficiency metrics\n\nPlease report quantitative efficiency metrics such as runtime (s/frame) and VRAM usage (GB) to support the claimed efficiency and enable fair comparison with prior methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "srXmyn0Pje", "forum": "GRElsj9W2t", "replyto": "GRElsj9W2t", "signatures": ["ICLR.cc/2026/Conference/Submission5510/Reviewer_6htp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5510/Reviewer_6htp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541266179, "cdate": 1761541266179, "tmdate": 1762918102608, "mdate": 1762918102608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the task of generative inbetweening, using the well-established image-to-video diffusion models. This paper tries to solve a problem of motion conflict when using time reversal sampling, which is the misalignment of motion paths estimated from two different directions of predictions - forward and backward. Existing methods try to fuse the estimations of both directions, and did not focus much on fixing possible misalignment issues. The authors propose Motion Prior Distillation, which use the residual noise predictions estimated from the forward path and distill them to the backward path in reverse, to match the predicted paths of both directions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem definition is clear.\n- The results of the proposed method seem to be strong, outperforming existing work."}, "weaknesses": {"value": "1. According to my understanding, in short, the proposed method aims to model the noise residual between frames, especially from the forward path, and distill it to the backward path for alignment. However, I feel quite unsure how this could well-align the motions. Figure 1 (c) seems to be a good description, and this also display my concern. In Fig.1(c), the position of the red car in the forward path and the backward path differ. To be more specific, in the backward path, starting from the end (green car), using the residual distilled from the forward path ends up in a different location of the red car, which is different from the reference start frame (red car). To my understanding, I believe that it is possible to cause a misalignment, and I wonder how this could rather resolve misalignment and motion conflict. Fusing the estimates of two paths may be a part which handles this part, but in that case, I am not sure of how the proposed method could theoretically contribute to the problem of resolving motion conflicts. Could the authors provide further explanation and clarify on this?\n2. Theoretically speaking, I understood the problem definition described by the authors in Sections 3 and 4.1. However, I am not following whether this is a “motion” “conflict”. For instance, Fig.1(d) is a good example to support my point. To my perspective, the forward path and the backward path both seem to locate the cars at similar positions, aligned quite well, despite blurry and unsatisfactory quality. Despite being blurry, the position of the visual artifacts, which is presumably the location where the model tried to draw the car, seems to be aligned well. Thus I find this problem to be more of a failure of inserting the object (car), rather than failure of modeling motions. I think it is not a problem with “motion”, but a failure of object injection / disappearance; and think the motions of forward and backward paths are not in “conflict”, with good alignment. It could be a matter of how we define things, but currently, I think the word choice of definition is a little misleading. Could the authors provide some discussions on this?\n\nThe weaknesses mentioned above are more of a question / discussion, rather than flaws. With these questions above clearly resolved, I am open to raise the rating."}, "questions": {"value": "- In lines 73-74, could the authors provide a little more explanation on how connecting the forward and backward paths differ from aligning them? Connection of forward and backward paths would inevitably require alignment, and I feel unsure of what this precisely means."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AFUjA31GzN", "forum": "GRElsj9W2t", "replyto": "GRElsj9W2t", "signatures": ["ICLR.cc/2026/Conference/Submission5510/Reviewer_mj1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5510/Reviewer_mj1L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738187260, "cdate": 1761738187260, "tmdate": 1762918102364, "mdate": 1762918102364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of motion priors misalignment when adapting image-to-video diffusion models for generative in-betweening. Unlike existing adaptation methods, the proposed approach distills the forward motion prior conditioned on the first input frame into backward motion prior, eliminating the need for conditioning on the second end frame. Experiments demonstrate that this simple yet effective motion prior distillation achieves better performance compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper tackles a well-defined problem: the mismatch of generated motion priors that occurs when conditioning the image-to-video denoiser on each end frame during the adaptation of image-to-video models for generative in-betweening. The novel aspect of the proposed approach lies in converting the forward motion noise estimate into a backward motion noise estimate using per-frame motion residuals. This design allows the backward motion to be learned without relying on denoiser estimates conditioned on the second end frame, which could otherwise generate a complete different motion path that is difficult to align with the forward trajectory."}, "weaknesses": {"value": "When the end point of the forward motion prior path is far from the  second input end frame,  Eq (15) would not be a good initialization. The paper should discuss this limitation and include examples to to illustrate the scenarios in which the proposed method is most effective and scenarios in which is not effective."}, "questions": {"value": "1. GI (Wang et al., 2025b) also addresses the motion prior misalignment issue by sharing temporal self-attention maps in a time-reversed manner. Based on the video results, both GI and the proposed method appear to perform similarly for rigid motions, such as those involving cars or boats. I am curious about the specific advantages of the proposed approach over GI in practical application scenarios.\n\n2. I also wonder how the proposed method performs when the two end frames are farther apart—for example, 50 frames. In such cases, using an image-to-video model capable of generating longer sequences (e.g., 50 frames), would the proposed method remain effective? Such a setting might further amplify the limitations discussed in the weaknesses section, as the motion priors will be more ambiguous over longer sequence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "R1FskRut1E", "forum": "GRElsj9W2t", "replyto": "GRElsj9W2t", "signatures": ["ICLR.cc/2026/Conference/Submission5510/Reviewer_CGhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5510/Reviewer_CGhH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798175805, "cdate": 1761798175805, "tmdate": 1762918101729, "mdate": 1762918101729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}