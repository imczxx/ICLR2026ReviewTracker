{"id": "1HjzhdTEC7", "number": 18241, "cdate": 1758285511903, "mdate": 1759897117216, "content": {"title": "Theory-Grounded Evaluation of Human-Like Fallacy Patterns in LLM Reasoning", "abstract": "We study logical reasoning in language models by asking whether their errors follow established human fallacy patterns. Using the Erotetic Theory of Reasoning (ETR) and its open‑source implementation, PyETR, we programmatically generate 383 formally specified reasoning problems and evaluate 38 models. For each response, we judge logical correctness and, when incorrect, whether it matches an ETR‑predicted fallacy. Two results stand out: (i) as a capability proxy (Chatbot Arena Elo) increases, a larger share of a model’s incorrect answers are ETR‑predicted fallacies ($\\rho=0.360, p=0.0265$), while overall correctness on this dataset shows no correlation with capability; (ii) reversing premise order significantly reduces fallacy production for many models, mirroring human order effects. Methodologically, PyETR provides an open‑source pipeline for unbounded, synthetic, contamination‑resistant reasoning tests linked to a cognitive theory, enabling analyses that focus on error composition rather than error rate.", "tldr": "Language models of different strengths show shifting patterns in how they make mistakes, with stronger models’ errors more often resembling predictable human reasoning slips.", "keywords": ["Language models", "reasoning", "synthetic data", "contamination-proof", "human-like errors", "cognitive fallacies", "Erotetic Theory of Reasoning", "PyETR", "logical fallacies", "human-like reasoning patterns", "reasoning evaluation", "question-driven inference", "inverse scaling laws", "human cognition", "rationality vs fallibility", "cognitive biases", "order effects", "reasoning benchmarks", "cognitive science alignment", "AI alignment", "systematic deviations from logic", "normative vs descriptive reasoning", "reasoning tasks", "disjunction fallacy", "modus ponens", "modus tollens", "syllogistic inference", "logical validity", "data contamination", "natural language reasoning tasks", "formal semantics", "mental models", "evaluation harness", "Chatbot Arena", "medical diagnosis", "legal reasoning", "high-stakes decision-making", "alignment benchmarks", "robust reasoning systems", "AI evaluation frameworks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e37f601183ec190dee03b6409bcda1ae91482f0a.pdf", "supplementary_material": "/attachment/35f1d993b6b6eba7d47d5b114f582f7cf747c1c8.zip"}, "replies": [{"content": {"summary": {"value": "### Summary: *Analyzing LLMs through the Erotetic Theory of Reasoning (ETR)*\n\nThe paper analyzes large language models (LLMs) through the lens of the **Erotetic Theory of Reasoning (ETR)** — a framework developed by philosophers and logicians to study how questions relate to one another.\n\n### **Background: Key Idea of ETR**\nETR focuses on how the **answer to one question can resolve or transform another question**.  \nFor example:\n\n> “If the light is on, then the power is on.”\n\nThis statement evokes two questions:\n1. Is the light on?\n2. Is the power on?\n\nAnswering *yes* to the first question naturally resolves the second one, concluding that the power is on.\n\n### **Experimental Setup**\nThe authors use **PyETR**, a tool that automatically derives ETR structures from statements, to generate prompts composed of:\n**theme introduction**, a set of **premises**, and a follow-up **question** asking what logically follows from those premises.\nThey source their ETR style reasoning problems from Reason and Inquiry text (Koralus, 2022).\n\n### **Evaluating Logical Correctness of Model Response**\nTo test logical correctness, they use **PySMT**, a logic solver already open-sourced for the community.\nThe paper measures the logical correctness by setting the criterion that if the **negation** of a model’s conclusion contradicts the premises, the conclusion is considered **logically correct**.\n\n### **Evaluating Human-like Fallacies**\nTo assess human-like reasoning errors, the authors check whether the model’s answer matches ETR’s prediction of what a human would infer,  but is not logically correct. These cases are labeled as ETR-predicted fallacies. \nFinally, the **fallacy rate** is defined as the proportion of these human-like fallacies among all logically incorrect responses.\n\n### **Key Findings of This Paper**\n- There is a **negative correlation** between logical ability and fallacy rate:  \n  **Stronger models are more likely to make human-like reasoning mistakes** when they do err.\n- Reversing the **order of premises** in the same logical problem often **reduces fallacy production**, showing that LLMs exhibit **order effects** similar to humans."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper draws ETR reasoning theory into evaluating LLM behavior, suggesting a viewpoint that logically strong models in terms of reasoning ability are more prone to making human-like fallacy errors. This is evaluated by employing existing open-source implementation of ETR logic solver, PyETR, and existing dataset \"Reason and Inquiry text\"."}, "weaknesses": {"value": "### **1. Low-Quality Prompt Design**\n\nThe prompts generated for evaluation appear to be poorly constructed, making it unclear whether models could interpret or respond to them properly.\n\n**Evidence**: The reported correlation between model performance (as measured by LLM Elo scores) and logical correctness is nearly zero\n(r = 0.004, p = 0.981; ρ = −0.04, p = 0.777). This indicates that the dataset may not reliably capture reasoning ability, suggesting that prompt design, rather than model reasoning, could be the limiting factor.\nThis undermines the paper’s main claim about fallacy trends across model capabilities, since a dataset that fails to differentiate between weak and strong models cannot support such conclusions.\n\n### **2. Lack of Methodological Clarity**\n\nSeveral crucial details are omitted or insufficiently explained, making it difficult to assess or reproduce the study.\n\na. The statement “Pre-analysis integrity checks identified 17 items whose fallacy status did not meet the final specification” is ambiguous. The paper never defines what “final specification” refers to or how it was determined.\n\nb. The authors mention creating “12 themes for natural language framings of logical problems,” yet these themes are not listed, exemplified, or referenced in the appendix or main text. Such omissions significantly reduce transparency and reproducibility.\n\nc. The criterion for logical correctness (checking whether the negation of the conclusion contradicts the premises) is introduced without any justification or explanation of the underlying logic principle. While the negation-contradiction test is a valid formal method for entailment checking, it should have been theoretically motivated and explained for readers unfamiliar with formal logic or SMT solvers.\n\n### **3. Conceptual and Logical Weaknesses**\n\nThe paper’s contribution is mainly an application of an existing psychological framework (Erotetic Theory of Reasoning, ETR) to LLM outputs, with little novel insight into model-specific reasoning mechanisms. It feels more like a reinterpretation of known model behaviors through an old theoretical lens rather than a discovery of new phenomena.\n\nThere is a logical circularity in the argument: the paper assumes that ETR is a valid description of human reasoning and then uses it as both the theoretical predictor and evaluation criterion for LLMs. This conflates explanatory alignment with empirical validation — the models might appear “ETR-like” simply because the evaluation framework enforces it.\n\nThe study treats fallacy alignment with ETR as evidence of human-likeness, but it never justifies why human-like fallacies should increase with logical ability. This causal interpretation is speculative and unsupported by formal or cognitive reasoning theory.\n\n### **4. Poor Exposition and Notation Clarity**\n\nSection 2, which introduces ETR, is very difficult to follow. The text uses non-standard and undefined notations without clear definitions or examples. The presentation of PyETR is redundant and unfocused, repeating information available in prior works without connecting it directly to this study’s experimental design.\n\nMoreover, the citation style is inconsistent: citations should be enclosed in parentheses unless grammatically integrated into the sentence. Several sentences lack periods or contain minor grammatical errors, making the text unnecessarily difficult to read. These stylistic and formatting lapses reflect insufficient editorial care, which reduces the paper’s overall readability and professionalism.\n\n### **5. Lack of rigorous experimental design and analysis**\nThere is no ablation or control analysis demonstrating that the observed “fallacy-blocking” from reversing premises is statistically robust or independent of surface-level linguistic cues. The paper conflates logical validity (a formal property) with psychological plausibility (a behavioral trait), without acknowledging that these are distinct dimensions of reasoning."}, "questions": {"value": "The questions mainly concern the clarity of the notations, the ETR-style logical expressions, and the undefined logical operators that appear throughout the paper.\nFor example, what does “p” represent in the reported Pearson/Spearman correlations? I assume it refers to the p-value, but such notation should be explicitly defined, not used without explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o6GWS5rOIP", "forum": "1HjzhdTEC7", "replyto": "1HjzhdTEC7", "signatures": ["ICLR.cc/2026/Conference/Submission18241/Reviewer_k8G8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18241/Reviewer_k8G8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706895606, "cdate": 1761706895606, "tmdate": 1762927975001, "mdate": 1762927975001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Note that I have previously reviewed this paper for another conference. The paper is substantially similar, and I copy some of the review here.\n\nThis paper shows that language models which are “stronger” (according to Chatbot Arena Elo score) produce more “human-like” errors, specifically those described by the “erotetic theory of reasoning” (ETR), which posits that humans reason by maintaining disjunctive alternatives and filtering these alternatives when taking on new information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the results and analyses to be sound, interesting, and worth sharing. \n* interesting and important result about how models interpret disjunctive normal forms in ways similar to humans\n* the use of the ETR model is novel and the analyses are sound\n\nI also appreciate that the authors have addressed my previous concerns, copied here:\n* The Chatbot Arena may not be the best proxy for \"strong\" models, given that human preferences on the arena may be prone to surface features like formatting (which the arena tries to account for) and sycophancy. Recent work has also demonstrated the fallibility of the leaderboard (https://arxiv.org/abs/2504.20879). I think a plausible alternate interpretation of the results is that Chatbot Arena measures (amoong other things) how well a model interprets natural language in the way that humans do. Given that humans interpret disjunctive normal forms in a particular way, the ELO score reflects this. Thus, intead of speaking to patterns in the behavior of models of various \"strength\", these results might instead be speaking more to what Chatbot Arena Elo is measuring\n* I think it’s a bit misleading to call it an inverse scaling law, which most people would I think interpret as indicate worse absolute performance with scaling. Rather, here the result is about the proportion of errors that fit the etoteric pattern\n* I may have misunderstood the methods, but I believe that the evaluation set mostly revolves around disjunctive normal forms and slight variations thereof. I think this is a relatively narrow set of behaviors, and the claim in the title may be overreaching given this narrowness"}, "weaknesses": {"value": "No substantial concerns"}, "questions": {"value": "I would also be interested to see the analysis separately for non-reasoning and “reasoning” models (esp those models trained via long-range RL, such as the o-series from OpenAI). And also models that are trained more heavily on e.g. code rather than human language. But this is not so much a concern as a curiosity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "urFmMQr0HN", "forum": "1HjzhdTEC7", "replyto": "1HjzhdTEC7", "signatures": ["ICLR.cc/2026/Conference/Submission18241/Reviewer_mN8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18241/Reviewer_mN8S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976133836, "cdate": 1761976133836, "tmdate": 1762927974641, "mdate": 1762927974641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses the ETR to evaluate whether LLM errors match human fallacy patterns across 383 reasoning problems and 38 models. The key finding is,  higher-capability models (Chatbot Arena Elo score) produce a significantly larger proportion of human-like fallacies (ρ = 0.360, p = 0.0265),  but no improvement in overall correctness. This suggests scaling increases alignment with human error patterns rather than improving logical reasoning accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a mature method for evaluating human fallacies, which has a solid theoretical foundation and provides good reproducibility based on the open-source pyETR implementation.\n2. Extensive experimental validation was conducted on a wide range of models (of different sizes and architectures), and the results are quite comprehensive.\n3. Insightful experimental designs explored how human thought patterns manifest in large-scale models."}, "weaknesses": {"value": "1. Experiments indicate that the moderately low correlation weakens the experimental claims put forward in the paper. \n2. The paper lacks theoretical and principled analysis, focusing instead on assessing the similarity between LLMs and humans in reasoning errors and presenting observed experimental results. However, it fails to explore potential causes for this phenomenon or propose solutions to the problem, resulting in limited practical applicability."}, "questions": {"value": "1. This paper evaluates model performance solely through the relatively comprehensive Elo metric. However, it remains unclear whether the reasoning fallacy test is more inclined toward models capable of generating long reasoning chains, or whether it is more closely related to the model's ability to answer questions in the STEM domain.\n2. Could a more comprehensive set of metrics be employed to holistically evaluate the model's performance, encompassing aspects such as answer diversity and reasoning capabilities? Further analysis of the relationship between human-like errors and these capabilities would strengthen the article's conclusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W7oTD5YyJl", "forum": "1HjzhdTEC7", "replyto": "1HjzhdTEC7", "signatures": ["ICLR.cc/2026/Conference/Submission18241/Reviewer_EveU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18241/Reviewer_EveU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071727721, "cdate": 1762071727721, "tmdate": 1762927974021, "mdate": 1762927974021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks into whether LLMs generate predictable fallacies like humans using ETR. Using the PyETR framework, authors generate synthetic reasoning problems and evaluate 38 LLMs. Their findings indicate that as models get more capable (with chatbotarena elo as a proxy) the predictability of the inaccurate responses increases (they get more human-like). The paper highlights that more capable models don’t just make fewer errors, they make more human-like ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  I resonate well with evaluating the human-likeness of reasoning of LLMs - LLMs are trained with trillions of tokens that are human written, and it is expected that human cognitive biases should show in their generations. And in that sence I find the paper quite interesting\n\n2. The paper also brings in the research question of predictability of errors. And shows that more capable model are more predictable as well\n\n3. Authors have done a good job at addressing a bigger audience than the ones familiar with cog-sci. I feel the paper is written well enough for someone not familiar with ETR to read and appreciate this. And I commend this.\n\nThat being said, I have concerns too. Please see weaknesses."}, "weaknesses": {"value": "Weakness: \n\n1. My first and major concern is the scope of the work. I think the section 4 needs more insights. I feel the scope is too limited in its current form.\n\n2. I think the paper can benefit from some anecdotal analysis, i.e. if the authors could give more insights of the form \"the reasoning pattern in model X is like Y, and hence ….\" instead of a high level correlation metric reported."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9p9fuq2Xl3", "forum": "1HjzhdTEC7", "replyto": "1HjzhdTEC7", "signatures": ["ICLR.cc/2026/Conference/Submission18241/Reviewer_J4ty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18241/Reviewer_J4ty"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154776665, "cdate": 1762154776665, "tmdate": 1762927973669, "mdate": 1762927973669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}