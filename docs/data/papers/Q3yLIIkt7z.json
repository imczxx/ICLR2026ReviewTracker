{"id": "Q3yLIIkt7z", "number": 11227, "cdate": 1758193875369, "mdate": 1763568550098, "content": {"title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime", "abstract": "Neural scaling laws underlie many of the recent advances in deep learning, yet their theoretical understanding remains largely confined to linear models. In this work, we present a systematic analysis of scaling laws for quadratic and diagonal neural networks in the feature learning regime. Leveraging connections with matrix compressed sensing and LASSO, we derive a detailed phase diagram for the scaling exponents of the excess risk as a function of sample complexity and weight decay. This analysis uncovers crossovers between distinct scaling regimes and plateau behaviors, mirroring phenomena widely reported in the empirical neural scaling literature. Furthermore, we establish a precise link between these regimes and the spectral properties of the trained network weights, which we characterize in detail. As a consequence, we provide a theoretical validation of recent empirical observations connecting the emergence of power-law tails in the weight spectrum with network generalization performance, yielding an interpretation from first principles.", "tldr": "We derive a phase diagram of scaling laws for diagonal and quadratic neural networks via a bridge to LASSO and matrix compressed sensing, predicting both generalization and the emergence of power-law weight spectra.", "keywords": ["Scaling laws; Neural networks; LASSO and matrix compressed sensing; Random matrix theory; Approximate message passing; High dimensional Statistics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f286c5ca866a75872e9f81ded1bfb86b23cb55f.pdf", "supplementary_material": "/attachment/26af31ac5131fd897e6bf803f9ab765b5b201275.zip"}, "replies": [{"content": {"summary": {"value": "The paper obtains precise limits of the test loss and weight spectrum for two models of shallow (linear/quadratic) networks. A complete phase diagram is given as a function of the regularization intensity and effective sample size."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper gives a very complete answer for the scaling laws of the regularized global minimum in these two simplified models. These networks implicitely add a L1-type of bias, comparison to previous work on random feature models/kernel methods which are in a L2 setting. The tools for the L2 setting are now well established (Random Matrix Theory etc), but the L1 case was to my knowledge less studied. \n\nThe paper is very clear, and the authors have done a great job of explaining the different phases and relate them to behavior of the weight spectrum, which helps a lot to not be lost in the many big formulas."}, "weaknesses": {"value": "The paper only studies diagonal linear networks and quadratic networks with fixed second layer weights, which is basically a shallow fully-connected linear network with symmetric weights. These are very simple models, far from anything used in practice, but one has to start somewhere."}, "questions": {"value": "- Do the L1 models that you consider outperform their L2 counterparts in general or in specific settings? And if so by how much? Since you seem to consider true functions that are not exactly sparse but only have a certain decay, I would imagine that the advantage of the L1 approach would appear for sufficiently steep decay of the true function spectrum?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RFUAt5Q8D8", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Reviewer_XduY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Reviewer_XduY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772118822, "cdate": 1761772118822, "tmdate": 1762922389682, "mdate": 1762922389682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical framework for neural scaling laws in shallow neural networks that go beyond the random features (lazy training) regime by explicitly modeling feature learning. It focuses on two analytically tractable architectures:\n\n1) Diagonal networks — equivalent to LASSO (l1-regularized linear regression).\n\n2) Quadratic networks — equivalent to matrix compressed sensing (low-rank estimation with nuclear norm regularization).\n\nThrough these mappings, the authors derive:\n\nA complete phase diagram (Fig. 1, p. 6) of generalization excess risk as a function of sample size, model dimension and weight decay.\n\nA universal scaling law showing crossovers between plateaus, fast- and slow-decay regimes, and double-descent–like interpolation peaks.\n\nA connection between scaling phases and spectral properties of the learned weights, yielding a theoretical explanation for the empirically observed power-law weight spectra in modern deep networks.\n\nThe analysis combines approximate message passing (AMP), state evolution, and random matrix theory, yielding results that match both Bayes-optimal rates and extensive numerical simulations (Figs. 2–6, pp. 7–25)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel analytical unification: \nThe paper builds an elegant bridge between feature-learning neural networks, LASSO, and matrix compressed sensing, allowing exact asymptotic characterization of generalization in previously intractable regimes.\n\nComprehensive phase diagram: Figure 1 neatly summarizes all scaling regimes and their transitions. It captures empirical phenomena (plateaus, overfitting peaks, fast decays) reported in large-scale scaling-law studies (Kaplan et al. 2020; Paquette et al. 2024).\n\nSpectral–generalization link: The “universal error decomposition” (Result 3) expresses the generalization error as an explicit functional of the weight eigenvalue spectrum, providing the first first-principles explanation of heavy-tailed spectra linked to generalization quality."}, "weaknesses": {"value": "Restricted architecture scope: Results hold for shallow (two-layer) diagonal or quadratic networks; extensions to multilayer or non-polynomial activations remain speculative. It’s unclear how much insight transfers to modern deep transformers or CNNs.\n\nDependence on heuristic AMP validity: The central results (e.g., Eq. 19) assume non-asymptotic validity of state evolution, supported empirically but not proven. The authors acknowledge this (Sec. 2.4) as a conjecture, so the paper’s main theorems are partially non-rigorous.\n\nTarget-model assumptions: The teacher–student setup with Gaussian inputs and power-law eigenvalues is mathematically convenient but idealized. Real data distributions or nonlinear activations could alter scaling transitions."}, "questions": {"value": "1) Can the spectral–generalization decomposition (Eq. 17) be extended to multi-layer or nonlinear activations?\n\n2) How sensitive are your phase boundaries to non-Gaussian inputs or correlated features?\n\n3) Is there a principled path toward proving the non-asymptotic AMP conjecture (Eq. 19)?\n\n4) Does the observed \\lambda^{-2/3} interpolation scaling have a connection to known double-descent exponents in overparameterized deep models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ka4jPS9xRC", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Reviewer_jRAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Reviewer_jRAX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954362803, "cdate": 1761954362803, "tmdate": 1762922388835, "mdate": 1762922388835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary rebuttal"}, "comment": {"value": "We thank all reviewers for their detailed and constructive feedback. We find the overall positive assessment of our work encouraging. To the best of our knowledge, our work is the first to establish a direct and interpretable mathematical connection between spectral properties and generalization scaling laws, and we believe this perspective may inspire substantial follow-up work.\n\nBelow, we address two of the common concerns raised by the reviewers. The other points are addressed in specific answers to the reviewers, and we warmly welcome any further question.\n\n**Specific architecture**: Several reviewers noted that our analysis focuses on specific architectures and characterizes only the global optimum rather than the optimization dynamics. We acknowledge this as both a natural shortcoming but also as a strength: it captures relevant phenomena, yet it is *tractable*.  \n\nThe technical cornerstone of the analysis is the mapping between the quadratic network and matrix compressive sensing. Such a mapping is specific to the quadratic case, and does not directly extend to deeper architectures or other activations. Extending these results to multilayer networks or non-polynomial activations is a major open challenge requiring new ideas, rather than a technical refinement of our current methods. We note that Barbier et al. (2025) recently proposed a non-rigorous extension to general MLPs; if their conjectures hold, similar error scalings should emerge in the universal phase they identify.\n\n**Training dynamics**: Regarding training dynamics, we agree that this is an important and complementary direction. For quadratic networks, the scaling of online SGD risk in the noiseless case was analyzed by Arous et al. (2025). More broadly, GD dynamics can be studied through dynamic mean-field theory (DMFT); for example, Fan et al. (2025) established convergence rates for related models. We expect this framework could eventually yield training-time scaling laws as well, though such an analysis is beyond the scope of our current work.\n\n**References**: \n\n*Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, and Rudy Skerk. Statistical\nphysics of deep learning: Optimal learning of a multi-layer perceptron near interpolation. arXiv preprint\narXiv:2510.24616, 2025.*\n\n*Gerard Ben Arous, Murat A Erdogdu, N Mert Vural, and Denny Wu. Learning quadratic neural\nnetworks in high dimensions: SGD dynamics and scaling laws. arXiv preprint arXiv:2508.03688, 2025.*\n\n*Zhou Fan, Justin Ko, Bruno Loureiro, Yue M Lu, and Yandi Shen. Dynamical mean-field analysis\nof adaptive langevin diffusions: Replica-symmetric fixed point and empirical bayes. arXiv preprint\narXiv:2504.15558, 2025*"}}, "id": "V1L1De9Nc7", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763566685211, "cdate": 1763566685211, "tmdate": 1763566685211, "mdate": 1763566685211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work extends neural scaling law theory  by analyzing quadratic and diagonal neural networks in the feature learning regime, using tools from matrix compressed sensing and LASSO to map out how excess risk scales with data, model size, and weight decay. The authors identify distinct scaling regimes, plateaus, and spectral signatures (like power-law tails in the weight distribution) and show how these phenomena predict generalization, providing a first-principles explanation for trends observed empirically in large neural networks and the correlation between power-law tails in the weight distribution and generalization in neural networks."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper gives a unified phase diagram of generalization error across sample size, width, and regularization for diagonal and quadratic networks (true feature learning, not just kernels).\n\n2. Exploring the theoretical relationship between the weight spectrum and generalization is an important open problem, and this paper provides an explanation for it in certain special neural network architectures. It analytically characterizes the trained weight spectra (bulk, spikes, heavy tails) in every regime and links these spectral features directly to underfitting, overfitting, and approximation error. \n\n3. It explains benign vs harmful overfitting and double-descent–like peaks from first principles, and shows how optimal regularization suppresses noisy bulk without destroying learned signal."}, "weaknesses": {"value": "**If you address the issues in the Weaknesses and Question sections, I will increase the score.**\n\n1. The writing of the paper could be improved. Some of the notation is not explained very clearly (see  Questions sections), and also it seems the paper does not include a conclusion section.\n2. The current setting is limited to special two-layer neural networks, which allows the study of their excess risk minimizer to be reduced to matrix compressed sensing and Lasso problem. I would like to know whether, for more general two-layer (or even multi-layer) neural networks, there is a similar way to reduce the problem. If this is difficult, do you think it would be possible instead to study the training dynamics of the weight spectrum under (stochastic) optimizers in order to explore the relationship between heavy-tailed weight spectra and training dynamics?\n3. In the paper, you assume that the spectrum of the target network weights follows a power-law distribution (which is  heavy-tailed ). Is this assumption necessary[1]? and how much does this heavy-tailed structure affect the final weight spectrum of the student network?\n4. Minor: It looks like in Figure 3, the third plot in the first row should be labeled with the exponent $-2 + \\frac{1}{\\gamma}$.\n\n\n[1] Gurbuzbalaban, Mert, Umut Simsekli, and Lingjiong Zhu. \"The heavy-tail phenomenon in SGD.\" International Conference on Machine Learning. PMLR, 2021."}, "questions": {"value": "In addition to the issues mentioned in the Weaknesses section, I may also have the following questions.\n\n1. In line 473, what does the first  $R$ represent? In addition, what is the relationship between the $R_{n,d}$ of ERM mentioned here (line 479) and the $R$ that appears in equations (11), (14), and (17)?\n\n2. There are also some papers [2][3][4][5][6][7][8][9][10] that discuss the relationship between the spectrum of the weights / features / data and generalization or neural scaling law. Could you provide some comments and discussion on these papers?\n\n\n\n\n[2] Bartlett, Peter L., et al. \"Benign overfitting in linear regression.\" Proceedings of the National Academy of Sciences 117.48 (2020): 30063-30070.\n\n[3] Simsekli, Umut, et al. \"Hausdorff dimension, heavy tails, and generalization in neural networks.\" Advances in Neural Information Processing Systems 33 (2020): 5138-5151.\n\n[4] Hodgkinson, Liam, et al. \"Generalization bounds using lower tail exponents in stochastic optimizers.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Wang, Yutong, Rishi Sonthalia, and Wei Hu. \"Near-interpolators: Rapid norm growth and the trade-off between interpolation and generalization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\n[6] Wang, Zhichao, et al. \"Spectral evolution and invariance in linear-width neural networks.\" Advances in neural information processing systems 36 (2023): 20695-20728.\n\n[7] Dandi, Yatin, et al. \"A random matrix theory perspective on the spectrum of learned features and asymptotic generalization capabilities.\" arXiv preprint arXiv:2410.18938 (2024).\n\n[8] Worschech, Roman, and Bernd Rosenow. \"Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra.\" arXiv preprint arXiv:2410.09005 (2024).\n\n[9]  Kothapalli, Vignesh, et al. \"From spikes to heavy tails: Unveiling the spectral evolution of neural networks.\" Transactions on Machine Learning Research (2025).\n\n[10] Arous, Gérard Ben, et al. \"Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws.\" arXiv preprint arXiv:2508.03688 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YMgt3bV8eY", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Reviewer_d5QU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Reviewer_d5QU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956504799, "cdate": 1761956504799, "tmdate": 1762922388196, "mdate": 1762922388196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work extends neural scaling law theory  by analyzing quadratic and diagonal neural networks in the feature learning regime, using tools from matrix compressed sensing and LASSO to map out how excess risk scales with data, model size, and weight decay. The authors identify distinct scaling regimes, plateaus, and spectral signatures (like power-law tails in the weight distribution) and show how these phenomena predict generalization, providing a first-principles explanation for trends observed empirically in large neural networks and the correlation between power-law tails in the weight distribution and generalization in neural networks."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper gives a unified phase diagram of generalization error across sample size, width, and regularization for diagonal and quadratic networks (true feature learning, not just kernels).\n\n2. Exploring the theoretical relationship between the weight spectrum and generalization is an important open problem, and this paper provides an explanation for it in certain special neural network architectures. It analytically characterizes the trained weight spectra (bulk, spikes, heavy tails) in every regime and links these spectral features directly to underfitting, overfitting, and approximation error. \n\n3. It explains benign vs harmful overfitting and double-descent–like peaks from first principles, and shows how optimal regularization suppresses noisy bulk without destroying learned signal."}, "weaknesses": {"value": "**If you address the issues in the Weaknesses and Question sections, I will increase the score.**\n\n1. The writing of the paper could be improved. Some of the notation is not explained very clearly (see  Questions sections), and also it seems the paper does not include a conclusion section.\n2. The current setting is limited to special two-layer neural networks, which allows the study of their excess risk minimizer to be reduced to matrix compressed sensing and Lasso problem. I would like to know whether, for more general two-layer (or even multi-layer) neural networks, there is a similar way to reduce the problem. If this is difficult, do you think it would be possible instead to study the training dynamics of the weight spectrum under (stochastic) optimizers in order to explore the relationship between heavy-tailed weight spectra and training dynamics?\n3. In the paper, you assume that the spectrum of the target network weights follows a power-law distribution (which is  heavy-tailed ). Is this assumption necessary[1]? and how much does this heavy-tailed structure affect the final weight spectrum of the student network?\n4. Minor: It looks like in Figure 3, the third plot in the first row should be labeled with the exponent $-2 + \\frac{1}{\\gamma}$.\n\n\n[1] Gurbuzbalaban, Mert, Umut Simsekli, and Lingjiong Zhu. \"The heavy-tail phenomenon in SGD.\" International Conference on Machine Learning. PMLR, 2021."}, "questions": {"value": "In addition to the issues mentioned in the Weaknesses section, I may also have the following questions.\n\n1. In line 473, what does the first  $R$ represent? In addition, what is the relationship between the $R_{n,d}$ of ERM mentioned here (line 479) and the $R$ that appears in equations (11), (14), and (17)?\n\n2. There are also some papers [2][3][4][5][6][7][8][9][10] that discuss the relationship between the spectrum of the weights / features / data and generalization or neural scaling law. Could you provide some comments and discussion on these papers?\n\n\n\n\n[2] Bartlett, Peter L., et al. \"Benign overfitting in linear regression.\" Proceedings of the National Academy of Sciences 117.48 (2020): 30063-30070.\n\n[3] Simsekli, Umut, et al. \"Hausdorff dimension, heavy tails, and generalization in neural networks.\" Advances in Neural Information Processing Systems 33 (2020): 5138-5151.\n\n[4] Hodgkinson, Liam, et al. \"Generalization bounds using lower tail exponents in stochastic optimizers.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Wang, Yutong, Rishi Sonthalia, and Wei Hu. \"Near-interpolators: Rapid norm growth and the trade-off between interpolation and generalization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\n[6] Wang, Zhichao, et al. \"Spectral evolution and invariance in linear-width neural networks.\" Advances in neural information processing systems 36 (2023): 20695-20728.\n\n[7] Dandi, Yatin, et al. \"A random matrix theory perspective on the spectrum of learned features and asymptotic generalization capabilities.\" arXiv preprint arXiv:2410.18938 (2024).\n\n[8] Worschech, Roman, and Bernd Rosenow. \"Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra.\" arXiv preprint arXiv:2410.09005 (2024).\n\n[9]  Kothapalli, Vignesh, et al. \"From spikes to heavy tails: Unveiling the spectral evolution of neural networks.\" Transactions on Machine Learning Research (2025).\n\n[10] Arous, Gérard Ben, et al. \"Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws.\" arXiv preprint arXiv:2508.03688 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YMgt3bV8eY", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Reviewer_d5QU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Reviewer_d5QU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956504799, "cdate": 1761956504799, "tmdate": 1763712866360, "mdate": 1763712866360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied the neural scaling law and analyzed the spectra of the trained weight matrix of two-layer diagonal networks and quadratic neural networks. Their teacher network has the same architecture as the student model, but with certain noise. By transferring diagonal networks to the LASSO problem and transferring quadratic neural networks to matrix compressed sensing, the authors can utilize  Generalized Approximate Message Passing (GAMP)  and its state evolution to compute the final excess risks and the spectral distributions of trained weight matrices. This heuristic computation works well for a power law target, a general ridge parameter, input dimension, and effective sample size. As a result, this paper showed how the scaling of the sample size and regularization affect the generalization, and how different scalings lead to different spectral behavior of the trained weight matrix (e.g., rank collapse, bulk+spikes, heavy-tailed distribution)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, and its results are of interest to the deep learning theory community. To the best of my knowledge, this is the first paper that theoretically explains the relationship between the spectral behavior of learned weights and generalization errors, as well as the 5+1 phases for trained weights observed by Mahoney and Martin in 2019."}, "weaknesses": {"value": "The neural network architectures are too specific. I understand that for general two-layer neural networks, the AMP is too complicated to analyze. However, it would be better to explain the difficulty, if there is any other simple architecture that can still work beyond quadratic or diagonal networks, and what the expected results are for general cases. It would be insightful to provide such information or discussion.\nAdditionally, there is a lack of training in dynamic analysis. This paper only used the state evolution to analyze the solution of the LASSO and matrix compressed sensing, but did not study the scaling of the training time or computational scaling."}, "questions": {"value": "1. What is $\\varepsilon$ in Figure 1? Figure 1 is nice and very important for the presentation. However, it is a little bit messy in the x and y axes. There is a line $\\lambda = \\sqrt{n_{\\text{eff}}/d}$, but when $n_{\\text{eff}} = 0$, why do you have $\\lambda = 1/d$? You should explain the red lines in the caption of the figure.\n\n2. For VIa and VIb, both risks decay fast, but the spectra of weights behave differently. One is bulk+spikes, and the other is heavy-tailed. Does this mean that we do not need heavy-tailed spectra to get a good generalization error? Martin & Mahoney, 2021a, and Martin et al., 2021, argued that networks with heavy-tailed spectra exhibit optimal generalization capabilities.\n\n3. In results 2, does the trained weight still have independent entries? $\\hat{\\theta}_i$ are independent for different $i$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t8MU3s9Rgh", "forum": "Q3yLIIkt7z", "replyto": "Q3yLIIkt7z", "signatures": ["ICLR.cc/2026/Conference/Submission11227/Reviewer_Bf9w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11227/Reviewer_Bf9w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762893619974, "cdate": 1762893619974, "tmdate": 1762922387772, "mdate": 1762922387772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}