{"id": "q0X9SiXiRO", "number": 4242, "cdate": 1757645636755, "mdate": 1759898044093, "content": {"title": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models", "abstract": "Parameter-efficient fine-tuning (PEFT) has become a de facto standard for adapting Large Language Models (LLMs). However, we identify a critical vulnerability within popular low-rank adaptation methods like LoRA: their tendency to exacerbate \"Catastrophic Inheritance\"—the unchecked propagation of biases, noise, and data imbalances from pre-training. This phenomenon can degrade model robustness and fairness, undermining the benefits of efficient adaptation. To address this, we introduce Bias-Alleviating Low-Rank Adaptation (BA-LoRA). Our approach is founded on a principled decomposition of Catastrophic Inheritance into three core challenges: Knowledge Drift, Representation Collapse, and Overfitting to Noise. BA-LoRA systematically mitigates these issues by incorporating a trio of targeted regularizers—consistency, diversity, and SVD—designed to preserve core knowledge, enforce representational richness, and stabilize the low-rank updates. We conduct comprehensive evaluations on a suite of natural language understanding (NLU) and generation (NLG) tasks using diverse, prominent open-source language models (e.g., LLaMA-2-7B and DeBERTa-v3-base). Our results show that BA-LoRA not only outperforms state-of-the-art LoRA variants in terms of performance and stability, but also demonstrates quantitatively superior robustness and bias mitigation on targeted evaluations. This confirms its ability to counteract the adverse effects of Catastrophic Inheritance. The code is available at https://anonymous.4open.science/r/p5q9a1z8.", "tldr": "We show PEFT can exacerbate \"Catastrophic Inheritance\" of pre-training biases and propose BA-LoRA, a regularization framework that systematically mitigates this issue in LoRA, yielding more robust, state-of-the-art models.", "keywords": ["Parameter-Efficient Fine-Tuning", "PEFT", "LoRA", "Bias Mitigation", "Catastrophic Inheritance", "Representation Learning", "Robustness", "Large Language Models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2accc7793a8711f7ed4a8cf71aaf6568d54f0985.pdf", "supplementary_material": "/attachment/a6b96de497f0f2def3202dfcca0e7ece7b4f3826.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces BA-LoRA, a PEFT method designed to mitigate catastrophic inheritance, including the propagation of biases, noise, and data imbalances from pre-trained LLMs. BA-LoRA addresses three failure modes via three regularizers applied in the output space, i.e., consistency regularizer, diversity regularizer, and SVD regularizer.\nExperiments evaluate BA-LoRA on NLG and NLU tasks, achieving state-of-the-art among LoRA variants. A controlled study shows BA-LoRA's gains are amplified on models pre-trained with noisy data. Ablation studies validate each regularizer's contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel decomposition of catastrophic inheritance into three actionable failure modes. Each failure mode is directly linked to a specific regularizer (consistency, diversity, SVD), creating a coherent and interpretable architecture.\n2. BA-LoRA sets a new state-of-the-art performance with strong empirical validation, on both NLG benchmarks and NLU tasks. Moreover, performance gains are even higher on noisier pre-trained models, directly validating its core hypothesis about mitigating inherited noise."}, "weaknesses": {"value": "1. The comparison between RoBERTa-base and T5-base shows that BA-LoRA's advantage is significantly larger on T5 (3.26 vs 1.11 points). However, the models differ in both architecture (encoder-only vs encoder-decoder), which should be controlled to strengthen the claim. This makes it difficult to attribute the performance difference solely to the 'noisier data' factor.\n2. The SVD regularizer uses different normalization schemes: NLU uses the sum of all singular values, while NLG uses the Frobenius norm. What is the theoretical justification for this difference? Understanding whether this choice is principled (e.g., due to task differences) or empirical is important for the generalization of the method beyond NLU and NLG.\n3. The method introduces too many hyperparameters, including the tradeoff parameters $\\lambda$s, temperature T, number of top-K, and SVD rank. Ablation studies on their sensitivity can enhance the usability of BA-LoRA in practice."}, "questions": {"value": "1. In Table 1, the gain of superior performance mainly depends on the MBPP benchmark, where BA-LoRA gains 36.86, whereas the second-best baseline only achieves 25.74. It would be valuable if the author could analyze why BA-LoRA performs particularly well in this dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pL7kdZJ3el", "forum": "q0X9SiXiRO", "replyto": "q0X9SiXiRO", "signatures": ["ICLR.cc/2026/Conference/Submission4242/Reviewer_4aVv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4242/Reviewer_4aVv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760912303158, "cdate": 1760912303158, "tmdate": 1762917250004, "mdate": 1762917250004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors state that LoRA can lead to Catastrophic Inheritance, which hurts robustness and fairness. \nThey propose Bias-Alleviating LoRA (BA-LoRA), which decomposes the issue into Knowledge Drift, Representation Collapse, and Overfitting to Noise. \nBA-LoRA adds three regularizers to preserve core knowledge, maintain representational richness, and encourage robust low-rank behavior."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Important topics\n2. Three well-motivated losses mapped cleanly to three failure modes; easy to plug into existing LoRA workflows.\n3. Broad NLU/NLG coverage with ablations that attribute gains to each component."}, "weaknesses": {"value": "1. Although the framework names are shared, the diversity and SVD regularizers differ for NLU vs. NLG, so it reads like two papers rather than one universal design.\n2. Some studies are missing in the paper (e.g., impact of different r/T/k, larger models)"}, "questions": {"value": "1. The paper builds on PiSSA initialization. How much of BA-LoRA’s improvement remains if you start from random low-rank adapters or standard LoRA init? Do the three regularizers still deliver comparable gains?\n2. How sensitive are results to the SVD rank, distillation temperature, and the top-k entropy window?\n3. How well does BA-LoRA scale to larger models (e.g., 13B, 70B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CA09GIGEyd", "forum": "q0X9SiXiRO", "replyto": "q0X9SiXiRO", "signatures": ["ICLR.cc/2026/Conference/Submission4242/Reviewer_KjTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4242/Reviewer_KjTf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761132341464, "cdate": 1761132341464, "tmdate": 1762917249636, "mdate": 1762917249636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present *Catastrophic Inheritance* in LLMs, a propagation of biases, noise, and data imbalances from pre-training into fine-tuning models, and ways to mitigate it. They identify: **Knowledge Drift**, where the model forgets pre-trained knowledge while learning new tasks; **Representation Collapse**, where fine-tuning on imbalanced data causes a lack of output diversity; and **Overfitting to Noise**, where the model learns spurious correlations that impact generalization. They offer three regularizers to combat these issues in Natural Language Understanding and Generation tasks: consistency regularizer for Knowledge Drift, diversity regularizer for Representation Collapse, and Singular Value Decomposition regularizer for Overfitting to Noise.\n\n1. The consistency regularizer is based on a knowledge distillation approach via KLD between pre-trained and fine-tuned model probability distributions to preserve foundational knowledge.\n2. The diversity regularizer is based on penalizing off-diagonal elements in a covariance matrix or maximizing entropy within the most plausible tokens to promote diversity in the model’s predictions across a batch.\n3. The SVD regularizer is based on encouraging low-rank structure by maximizing the ratio of spectral energy in the top-k singular values to learn robust features.\n\nThey test the method **Bias-Alleviating Low-Rank Adaptation (BA-LoRA)** in mathematical reasoning, coding, and conversational AI for NLG, as well as on the GLUE benchmark for NLU, using models such as LLaMA-2-7B and DeBERTa-v3-base. Additionally, they look at gains in models that were trained with clean or noisy data, as well as t-SNE visualizations of the features to see the impact of data imbalance, and perform ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers an original perspective on catastrophic inheritance with clear methodology and strong experimental evaluation, making it a significant and well-presented contribution.\n\n---\n\n- The abstract is very clear, well written, and easy to understand.\n- The experimental setup is thoroughly described, with clear reporting of hyperparameters.\n- The results are evaluated over multiple random seeds.\n- The work covers a wide range of setups and datasets, reflecting a comprehensive and up-to-date evaluation.\n- The experiment comparing noisy vs. clean data is particularly valuable and insightful.\n- The appendix offers a lot of information on the setup, different models, sensitivity analyses, and performance across rank. This is very solid experimental work, and it should be highlighted more prominently in the main text, as a substantial amount of effort and insight resides there."}, "weaknesses": {"value": "### Methodology and Experiments\n* A comparison between NLU and NLG is missing. The methodology section duplicates the description of the regularizers; what is missing is the motivation for changes required to adapt the approach to NLG, followed by a clear presentation of that modified setting. Because of this, the methods section feels unsatisfactory.\n* Despite introducing a regularizer for forgetting of pre-trained knowledge, the authors never directly evaluate forgetting. Overall, the first regularizer remains insufficiently probed.\n\n### Fairness & Comparability of Evaluation\n* Sourcing scores from original publications with “comparable” setups may not be a valid comparison if the experimental configurations are not identical.\n* C4 is much larger and should arguably have been sampled or sliced to make comparisons fairer.\n\n### Computational Considerations\n* The computational costs of the different steps should be described (e.g., randomized SVD).\n\n### Initialization Choices\n* PiSSA seems like a questionable initialization strategy, it has been shown in [1] that PiSSA induces forgetting of pre-trained knowledge, as the adapters are initialized with the “core” knowledge. It seems BA-LoRA mitigates an issue amplified by this initialization.\n\n### Related Work\n* The related work section is too short and lacks explicit comparison to this work.\n\n[1] MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning. (2025) Hanqing Wang and Yixia Li and Shuo Wang and Guanhua Chen and Yun Chen"}, "questions": {"value": "1. Page 2 L86, can you provide a reference if this been shown before in LoRA?\n2. How does the model manage to mitigate the Knowledge Drift? And how does PiSSA initialization relate to the Knowledge Drift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dV1FjnbkIk", "forum": "q0X9SiXiRO", "replyto": "q0X9SiXiRO", "signatures": ["ICLR.cc/2026/Conference/Submission4242/Reviewer_nsxF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4242/Reviewer_nsxF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666084269, "cdate": 1761666084269, "tmdate": 1762917249351, "mdate": 1762917249351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use a bunch of existing regularization techniques combined with the existing LoRA variation to improve the fine-tuning performance. The paper evaluates the method on Llama2 7B and shows that it performs better than many existing methods.\n\nIt's good to know that combining all these regualrziation techniques together improves the performance but I don't quite understand the novelty of this paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well motivated.\n\nThe empirical results seem to be promising."}, "weaknesses": {"value": "- The base model used for evaluation is extremely out of date, Llama2 is released in 2023, and I am not sure if the conclusion drawn is transferable to newer models.\n\n- The method has 3 hyperparameter to tune, and the paper does not provide any guidance."}, "questions": {"value": "- Can the author clarify what are the actual contributions of this paper?\n\n- How are the 3 lambdas chosen?\n\n- Why is BA-LoRA better than full model fine-tuning?\n\n- The lines in Fig.2 are confusing in that: why the proposed method improves training performance. Isn't the main question to be addressed about generalization ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b0YktdgBWQ", "forum": "q0X9SiXiRO", "replyto": "q0X9SiXiRO", "signatures": ["ICLR.cc/2026/Conference/Submission4242/Reviewer_SgzW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4242/Reviewer_SgzW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669940338, "cdate": 1761669940338, "tmdate": 1762917249051, "mdate": 1762917249051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to add three additional regularizers in LoRA fine-tuning: 1) distillation loss between pretrained and fine-tuned models to reduce output shift; 2) penalty on correlation between output classes or entropy of outputs to promote diversity; 3) penalty on high-frequency components in the output logit matrix."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The empirical advantage of combining the three regularizers is supported by improved empirical results."}, "weaknesses": {"value": "The purpose of the proposed methods is not clear to me; see below."}, "questions": {"value": "The purpose of the paper is unclear. At the beginning, the paper states that the goal is to evade/eliminate catastrophic inheritance, i.e. the undesired traits of a pretrained model (e.g. imbalances, biases, spurious correlations) being kept/exacerbated during fine-tuning, but the paper first applies a distillation-style regularizer to prevent shift from the pretrained model outputs. This appears to be self-contradictory. If the outputs of the pretrained model are undesired and need to be fixed during fine-tuning/post-training, then knowledge drift is what we desire. The case is similar in the other two subtasks: L90-92 attribute deteriorated performance due to the fine-tuning data quality, instead of the pretraining data. I think further explanation is critical for a consistent paper.\n\nL85-86: It is claimed that LoRA exacerbates catastrophic inheritance; this makes sense, but I don't see any empirical evidence supporting that in the paper.\n\nL155&L201: It is the outputs given pretraining or fine-tuning data? They have completely different implications. Also, how do you obtain the outputs of a un-finetuned encoder (e.g. Deberta) on NLU tasks? I assume that an extra linear projection head needs to be attached.\n\nL159&L206: The reason for using T^2 is not really clear and needs further explanation.\n\nL161: Should use \\citep in this case.\n\nL166: I'm confused by the purpose of this regularizer. Classes in different categories may not be orthogonal to each other, and some classes can be inherently correlated. Also, why is correlation a sign of a lack of diversity? If the goal is to promote diversity/avoid over-confidence, similar entropy-based regularizers like the one used in NLG (L214) should also work.\n\nL177: Why do we expect the logit matrix to be low-rank? Is there any specific reason? How is the high-frequency component in the output logits related to the data noise? Particularly, if each of the sample certainly belongs to a class and D=N, the logit should be full-rank.\n\nL180: What is \"spurious intra-batch variations\"? I assume that samples within each batch should be independently sampled.\n\nL250: The selection of regularization weights appears to be quite arbitrary. Tuning three hyperparameters can be challenging and can be highly varied between tasks. Appendix C.2. actually demonstrates the issue, as the trend on MATH and GSM8K in Fig.4 (a) are different. Also, the change in accuracy in Fig. 4 is downplayed with the large difference in scale of the accuracy on the two tasks. I would suggest putting the lines on different figures. \n\nL1038 states that the regularizers steer the model along the creativity-robustness spectrum, but both MATH and GSM8K are reasoning tasks, and I doubt if MATH can be used as a proxy for creativity.\n\nSec 3.2.2: In addition to differences in training data, there are considerable differences in the model architecture and data size between RoBERTa and T5, hence I doubt if the difference in improvements can be attributed to noise resilience.\n\nTo summarize, I fear that there are too many claims and assumptions in this paper that are not sufficiently supported by concrete evidence. I would recommend the authors to   \n1) Make their goals clear.  \n2) Provide empirical evidence that the issues (as for this paper, forgetting, diversity, and noise) exist by dedicated evaluations, especially with LoRA, e.g., by comparing generation diversity in the original, fully fine-tuned, and LoRA fine-tuned models. Standard benchmarks on reasoning, understanding, or commonsense cannot directly support your claim.  \n3) Show that each of the regularization terms can improve the issue, respectively.  \n4) Demonstrate the synergy of the trio.\n\nThere are other papers discussing alleviating knowledge drift in LoRA fine-tuning, e.g.   \nSmith, James Seale, et al. \"Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA.\" Transactions on Machine Learning Research (2024).  \nChen, Haolin, and Philip N. Garner. \"Bayesian parameter-efficient fine-tuning for overcoming catastrophic forgetting.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CmfbTp40xf", "forum": "q0X9SiXiRO", "replyto": "q0X9SiXiRO", "signatures": ["ICLR.cc/2026/Conference/Submission4242/Reviewer_N4aD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4242/Reviewer_N4aD"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756609707, "cdate": 1761756609707, "tmdate": 1762917248725, "mdate": 1762917248725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}