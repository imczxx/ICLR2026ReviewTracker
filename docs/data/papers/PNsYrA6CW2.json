{"id": "PNsYrA6CW2", "number": 15763, "cdate": 1758255037434, "mdate": 1759897283822, "content": {"title": "Fine-tuning is Not Enough: Rethinking Evaluation in Molecular Self-Supervised Learning", "abstract": "Self-Supervised Learning (SSL) has shown great success in language and vision by using pretext tasks to learn representations without manual labels. Motivated by this, SSL has also emerged as a promising methodology in the molecular domain, which has unique challenges such as high sensitivity to subtle structural changes and scaffold splits, thereby requiring strong generalization ability. However, existing SSL-based approaches have been predominantly evaluated by naÃ¯ve fine-tuning performance. For a more diagnostic analysis of generalizability beyond fine-tuning, we introduce a multi-perspective evaluation framework for molecular SSL under a unified experimental setting, varying only the pretraining strategies. We assess the quality of learned representations via linear probing on frozen encoders, measure Pretrain Gain by comparison against random initialization, quantify forgetting during fine-tuning, and explore scalability. Experimental results show that several models, surprisingly, exhibit low or even negative Pretrain Gain in linear probing. Graph neural network-based models experience substantial parameter shifts, and most models derive negligible benefits from larger pretraining datasets. Our reassessments offer new insights into the current landscape and challenges of molecular SSL.", "tldr": "We propose a unified evaluation framework for molecular SSL beyond fine-tuning", "keywords": ["Self-Superviseds Learning", "Molecule", "Lineaer probing", "Fine-tuning", "Pretrain Gain", "Paratemeter Shfit"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9352823966ad2b085d74badbd9e04e22340de6e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits evaluation methodologies in molecular self-supervised learning. It argues that the prevalent fine-tuning performance metric fails to fully capture the generalization and representation quality of pretrained models. To address this, the authors propose a multi-perspective evaluation framework consisting of:\n- Linear probing to test frozen encoder representations,\n- Pretrain Gain, measuring improvements over random initialization,\n- Parameter Shift to quantify forgetting during fine-tuning, and\n- Scalability analysis across dataset sizes.\n\nExperiments on eight representative molecular SSL methods (GROVER, GraphCL, KANO, ChemBERTa, etc.) reveal surprising findings: many models show low or even negative Pretrain Gain under linear probing, substantial parameter shifts for GNNs, and negligible scalability. The study provides a diagnostic reassessment of how molecular SSL should be evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a key methodological weakness in current molecular SSL literature, the overreliance on fine-tuning metrics, and offers a concrete alternative evaluation paradigm.\n2. Comprehensive experimental design: The authors control for non-pretraining factors (architecture, datasets, heads, dimensions), enabling fair cross-model comparison. This is rare in molecular ML.\n3. Empirical insights: The finding that some SSL models yield negative Pretrain Gain is striking and valuable for the community."}, "weaknesses": {"value": "1. Insufficient comparative experiments. Lack of comparative results of HIV and MUV datasets; If multiple evaluation indicators are related to parameters, please add the analysis of the parameters related to the prediction head. \n2. Lack of theoretical foundation. Pretrain gain/parameter shift, while intuitive, lacks theoretical foundation. On the other hand, fine-tuning is to better adapt the model to new tasks, which cannot account for forgetfulness and is not conducive to downstream tasks. \n3. The comparison of Figure 2 is unreasonable. Random initialization does not learn about pre-training, and cannot explain the difference between linear probing and FT. \n4. Scalability section is shallow. Although dataset sizes vary, the study does not explore larger orders of magnitude, limiting conclusions about true scaling laws."}, "questions": {"value": "1. Figure 2 How does the author explain the phenomenon of overemphasizing negative returns, but in fact FT helps to obtain positive gains? \n2. How sensitive are your results to the downstream head architecture (e.g., 1-layer vs. 2-layer MLP)? \n3. Can I visualize the difference in weights before and after fine-tuning (Eq.2)? \n4. Could you add results with larger pretraining datasets (e.g., >10M) or pretrained models from prior work to further validate the scalability? \n5. Have you considered evaluating out-of-domain generalization as part of the framework?\n6. See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9qmePVgZhT", "forum": "PNsYrA6CW2", "replyto": "PNsYrA6CW2", "signatures": ["ICLR.cc/2026/Conference/Submission15763/Reviewer_94XP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15763/Reviewer_94XP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705975645, "cdate": 1761705975645, "tmdate": 1762925996906, "mdate": 1762925996906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out the limitation in traditional evaluation of molecular pretrained models primarily based on fine-tuning performance with inconsistent setup, and thus proposes a new evaluation paradigm for molecular pretrained models with more unified setting and evaluation metrics across different perspectives to better understand and isolate the effect from pretraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper indeed points out the previous limitation and difficulty to obtain a more fair and unified performance comparison over various molecular pretrained models\n- Through their designed benchmark evaluation across different perspectives, there are several insights regarding different architecture of the pre-training models and they include a study on data scaling effect"}, "weaknesses": {"value": "- As a benchmark paper that particularly aiming for molecular pretrained models, the baselines used for evaluation are rather outdated, lack of latest and more powerful molecular pretraining models, like [1, 3, 4]\n- The datasets for pretraining and evaluation are also small and outdated, Zinc is only 2M, we can include larger datasets for pretraining, for instance the datasets include in [2]. Also, the evaluation datasets only contain classification tasks without any regression tasks, like some basic ones: esol, lipo, ...\n- Although the idea that we want to evaluate over not only fine-tuning performance is valid and reasonable, but the metrics design might not be that precise and at a high level, for instance, the forgetting is reflected by the distance in parameters. Then, should we also have a more comprehensive definition and evaluation of the forgetting of the knowledge instead of referring only to the parameters. \n- The paper presentation needs some refinement, especially the quality of the plots included are not that professional.\n\n[1]. Xia, Jun, et al. \"Mole-bert: Rethinking pre-training graph neural networks for molecules.\" (2023).\n\n[2]. Beaini, Dominique, et al. \"Towards foundational models for molecular learning on large-scale multi-task datasets.\" arXiv preprint arXiv:2310.04292 (2023).\n\n[3]. Luo, Yizhen, et al. \"Molfm: A multimodal molecular foundation model.\" arXiv preprint arXiv:2307.09484 (2023).\n\n[4]. Ji, Xiaohong, et al. \"Uni-mol2: Exploring molecular pretraining model at scale.\" arXiv preprint arXiv:2406.14969 (2024)."}, "questions": {"value": "- Do you expect the evaluation of current framework can be extended to 3D pretrained models and the pretraining including LLM in the process and generalize to future molecular foundation models\n- others please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A3003r8V6s", "forum": "PNsYrA6CW2", "replyto": "PNsYrA6CW2", "signatures": ["ICLR.cc/2026/Conference/Submission15763/Reviewer_VNaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15763/Reviewer_VNaQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954010737, "cdate": 1761954010737, "tmdate": 1762925996327, "mdate": 1762925996327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a multi-perspective evaluation framework for molecular self-supervised learning (SSL) that extends beyond the traditional fine-tuning-only evaluation common in this domain. The authors propose four evaluation strategies: (1) linear probing to assess learned representation quality, (2) pretrain gain to quantify benefits of pretraining versus random initialization, (3) parameter shift analysis to measure forgetting during fine-tuning, and (4) scalability experiments with varying pretraining dataset sizes. Under a unified experimental setup controlling for hidden dimensions and downstream architecture, the paper evaluates eight molecular SSL methods (GROVER, AttributeMask, ContextPred, EdgePred, GraphLoG, GraphCL, KANO, ChemBERTa) on six downstream molecular property prediction tasks. Key findings include: several models exhibit low or negative pretrain gain in linear probing, GNN-based models experience substantial parameter shifts during fine-tuning, and most models show negligible benefits from larger pretraining datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The empirical work is extensive and demonstrates significant effort:\n    - Training and evaluating eight SSL methods across multiple downstream tasks, dataset scales, and multiple repetitions represents a substantial computational and organizational undertaking.\n    - The attempt to standardize experimental conditions (hidden dimensions, prediction heads, datasets) across diverse methods identifies (and aims to address) a real gap in prior work where each method was evaluated under different conditions.\n- The paper makes a valuable contribution by questioning the sole reliance on fine-tuning performance as an evaluation metric for molecular SSL:\n    - The proposed multi-perspective evaluation framework, particularly the pretrain gain metric, provides a more nuanced view of pretraining effectiveness.\n    - The unified experimental setup represents a commendable effort to enable fair comparisons across methods.\n- The paper tackles an important problem in molecular SSL evaluation:\n    - The finding that high fine-tuning performance does not guarantee high-quality pretrained representations (e.g., KANO's negligible pretrain gain despite top fine-tuning performance) is a critical insight.\n    - This could influence future research directions in how molecular SSL methods are evaluated."}, "weaknesses": {"value": "- There are some serious issues with the experimental design that limit the conclusions that can be drawn:\n    - The authors confound architecture and pretraining strategy:\n        - Architecture (GNN vs. Transformer) and pretraining strategy (e.g., masking, contrastive learning) are two independent axes that should be evaluated separately.\n        - A proper experimental design would either: (1) evaluate all combinations in a grid (each pretraining strategy on each architecture), or (2) hold one axis constant while varying the other.\n        - Instead, the paper confounds these factors: GNN-based models use molecular graphs while Transformer-based models use SMILES strings, and each model uses its own specific pretraining strategy. Hybrid approaches further complicate the analysis rather than clarifying it.\n        - This makes it impossible to determine whether performance differences arise from the architecture choice, the pretraining strategy, the input modality, or some interaction between them. This severely limits the ability to draw conclusions about the effectiveness of pretraining strategies themselves.\n    - The authors do not control for model capacity in a principled manner:\n        - Fixing hidden dimension to 300 across all models does not ensure fair comparison. Hidden dimension means different things for different architectures (e.g., Transformer attention heads vs. GNN message passing dimension).\n        - The appropriate comparison would control for total parameter count, not hidden dimension.\n    - Critical regularization details (weight decay, dropout rates, etc.) are not discussed, and it's unclear whether these were controlled across models or left at their original paper-specific settings:\n        - These have major impact on fine-tuning behavior and the parameter shift metric.\n        - This omission makes it difficult to interpret the results or reproduce the experiments.\n- The parameter shift metric (Eq. 2) is seriously flawed:\n    - It sums L2 distances across all parameters, making it fundamentally incomparable across models with different architectures.\n    - Models have different parameter counts, yet the metric is an absolute sum.\n    - Parameters have different scales across architectures (e.g., normalization layers vs. weight matrices).\n    - Weight decay and other regularization techniques significantly affect this metric, yet are not discussed.\n    - Reparameterizations that produce identical functional outputs yield different shift values. For example, consider a simple model $f(x, W) = \\text{LN}(xW)$. Both $f(x, W)$ and $f(x, cW)$ (where $c$ is a constant) produce the same output, but have different parameter shift values.\n    - The correlation shown in Figure 7 between parameter shift and performance gap does not validate the metric's utility for cross-model comparison.\n    - A better approach: use functional similarity metrics such as CKA or CCA on hold-out embeddings from pretrained vs. fine-tuned models, which are architecture-agnostic and measure what matters---representation similarity.\n- The regression results have significant presentation issues:\n    - Tables 11-16 appear to average MSE across datasets with different target scales, which is meaningless as MSE values for different regression tasks are not comparable.\n    - Further, because the scales of the targets and MSEs are different, it's much harder to interpret all the other results for regression (e.g., pretrain gain figure).\n    - Finally, there seems to be a reversal of linear probing trends for regression versus classification. This is interesting but not discussed adequately (please see my question about this below).\n- While linear probing is a reasonable metric to include in such a study, its utility for evaluating representation quality in deep pretrained models is questionable:\n    - Linear probing evaluates whether frozen representations are directly applicable to downstream tasks with minimal adaptation (i.e., only a linear or shallow classifier). However, in modern deep learning (vision, NLP, etc.), adapter-based methods and LoRA consistently extract substantially more performance from the same pretrained models while maintaining comparable parameter efficiency. This suggests that the ability to perform well under linear probing is not strongly indicative of the overall quality or usefulness of learned representations---rather, it reflects one specific (and arguably limited) notion of representation utility.\n    - An interesting alternative direction would have been to evaluate these models using parameter-efficient fine-tuning methods like LoRA alongside (or instead of) linear probing, as this might provide more reliable insights into representation quality.\n- The model scaling experiments are underdeveloped:\n    - The main paper focuses primarily on data scaling but appears to also have conducted model scaling experiments with the hidden dimension 1200 results (Table 17).\n    - If extensive model scaling experiments were done, it would be good to present them in a similar manner to the data scaling experiments: Create a visualization showing performance as a function of model size at fixed data size.\n    - If the full grid of model sizes and data sizes have been explored, then some visualization of the full scaling surface would be valuable (perhaps some sort of Pareto front analysis).\n    - The existing model scaling results should be better integrated into the main discussion.\n- Many figures have poor aspect ratios that impede readability (Figures 2, 4, 5, 6, 8, 9, 10)."}, "questions": {"value": "1. What are the weight decay settings for each model? How sensitive are your conclusions to these regularization choices?\n2. Looking at the regression results, it seems like the trends that were observed for linear probing (that about half the methods showed negative pretrain gain) are now gone for regression tasks (i.e., fine-tuning always helps). Provided that this was a major finding in the classification experiments, can you discuss why the regression results differ? Is this due to differences in task nature, dataset characteristics, or something else?\n    - The case of ChemBERTa is a particularly interesting one to investigate further, as the linear probing pretrain gain is massive for regression, but fairly negligible for classification. What do you think explains this discrepancy?\n\n**Minor Comments and Nits**\n\n- Line 152: \"GNNs are used to extract graph structure\" appears to be an incomplete sentence.\n- Line 213: \"fine-tuning\" should be capitalized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NPSAXxbgRf", "forum": "PNsYrA6CW2", "replyto": "PNsYrA6CW2", "signatures": ["ICLR.cc/2026/Conference/Submission15763/Reviewer_eMxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15763/Reviewer_eMxA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987245012, "cdate": 1761987245012, "tmdate": 1762925995290, "mdate": 1762925995290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an evaluation framework for SSL methods in the molecular domain. In this work, a scaffold split is employed as a standard approach to evaluate model performance on unknown scaffolds, thereby testing the methods' ability to generalize. The primary objective of this research is to compare multiple molecular SSL methods by measuring the performance gain achieved when the entire model is fine-tuned versus when only a linear prediction head is trained. Additionally, knowledge forgetting is quantified in the fine-tuned models by calculating the difference in weights before and after fine-tuning. Finally, the study examines the relationship between the size of the pre-training dataset and model performance. This research leads to several noteworthy conclusions. For example, transformer models are less likely to forget knowledge learned during pre-training, and some models produce representations that are less useful for the target task than embeddings generated by a randomly initialized model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The study includes different families of SSL models, which leads to interesting conclusions about how different architectures and pretraining techniques behave.\n- All experiments were repeated three times, and standard deviation values are reported, making the evaluation more robust.\n- A scaffold split is adopted, which is a standard for evaluating model generalization in the molecular domain.\n- The evaluation offers various perspectives on pretrained models: performance gain, knowledge forgetting, and scalability.\n- The conclusions drawn from this study may be useful for researchers working on the molecular SSL methods or readers who want to find a useful pretraining scheme.\n- The paper is well-written, with a clear evaluation objective."}, "weaknesses": {"value": "- The techniques used for evaluating SSL approaches in this work are not new, and many of them were adapted from the papers introducing SSL methods or from other domains (for example, linear probing was used in SimCLR, and the difference of weights was evaluated here [1]).\n- Some figures are of low quality. For example, Figure 2 appears to have been scaled down in height.\n- There are numerous references to tables and figures in the appendix, and the main text does not make sense without consulting these tables and figures. The main text of the paper should be self-contained, with the appendix serving only to explain experimental details and to present additional experiments.\n- The code is not available, which makes it impossible for other researchers to use this framework (scaffold splits should be pre-defined for better reproducibility, and all the metrics should be clearly defined).\n- (minor) Section 3.3 begins with a small letter.\n\n[1] Li, D., & Zhang, H. (2021). Improved regularization and robustness for fine-tuning in neural networks. Advances in Neural Information Processing Systems, 34, 27249-27262."}, "questions": {"value": "1. Have you verified whether weight drift actually causes forgetting? For example, you could verify whether these models can still perform the original pretraining task on a validation set, achieving a loss comparable to that of the original pre-trained weights. \n2. Do you think that the flat pretrain gain plots (Figure 4) may be caused by the small number of model parameters, or are they the inherent inability of graph models to learn meaningful information from these pre-training tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gV3qps6mpH", "forum": "PNsYrA6CW2", "replyto": "PNsYrA6CW2", "signatures": ["ICLR.cc/2026/Conference/Submission15763/Reviewer_SvRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15763/Reviewer_SvRS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002643130, "cdate": 1762002643130, "tmdate": 1762925994552, "mdate": 1762925994552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}