{"id": "hWyeiVFZ0q", "number": 3132, "cdate": 1757339865727, "mdate": 1759898107219, "content": {"title": "BrainMoE: Towards Universal EEG Foundation Models with Channel-wise Mixture-of-Experts", "abstract": "Electroencephalography (EEG) is pivotal for brain-computer interface (BCI) and healthcare applications, with foundation models offering a promising paradigm for generalized decoding. However, current models typically apply a uniform strategy across all channels, overlooking the brain's inherent functional heterogeneity. This one-size-fits-all approach limits model adaptability and performance, as different EEG tasks activate distinct neural pathways.\nTo address this, we introduce BrainMoE, a novel universal EEG foundation model featuring a channel-wise Mixture-of-Experts (MoE) architecture. The core innovation is the dynamic assignment of specialized experts to different channels for more nuanced decoding. BrainMoE first employs a ChannelFormer to capture distinct representations for each channel. A router then uses these representations to intelligently select and weight the most relevant experts. This mechanism allows BrainMoE to tailor its decoding strategy on a per-channel basis, aligning its computation with the varied spatial demands of diverse EEG tasks.\nComprehensive experiments on seven downstream tasks across nine benchmarks demonstrate that BrainMoE achieves significant performance gains, setting a new state-of-the-art on six datasets. These results validate that channel-wise specialization is a critical step towards more powerful and truly universal EEG foundation models, showcasing the robust capability and generalizability of our approach.", "tldr": "", "keywords": ["EEG", "Foundation Models", "MoE"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/774e9675d3e42e9b6f451f836f48898865293e70.pdf", "supplementary_material": "/attachment/df456a282808f47b7519d3e43f74e2a2aafe879a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces the famous MOE technique for a transformer-based pretraining for EEG. Downstream task performances as well as several ablations and analyses support the gains from this. The paper also introduces ChannelFormer for properly encode each channel to be routed for k experts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is clear to follow with great figure and table presentations.\n- Incorporation of MoE for channel-wise routing seems domain specific and well adapted.\n- Comparison and Ablation studies are thorough, including several state of the art.\n- Correctly identifies limitations in the conclusion section"}, "weaknesses": {"value": "- Motivation not so well supported: lines 54-57 are not well supported, as to what it means by \"specialized functions of different brain regions\". This claim requires an interpretation study in Figures 4,6,7 to be improved, including more application driven analyses, in relation to disease or known pathways. \n- Related to the above point, the relative gain from MoE integration seems marginal.\n- Novelty and technical contributions are marginal."}, "questions": {"value": "- Would MoE integration to other baselines surpass this model, judging by that the ablation without MoE performs much worse?\n- Also related to above question, integrating MoE seems possible for most modern architectures with minimal effort. \n- Will this method generalize to electrode locations unseen during training?\n- Can you show the expert coupled with the spatial location of those EEGs? Do experts specialize in space or time? Do the expert distribution also vary in task e.g., resting state vs. motor imagery. These interpretations may further strengthen the novelty claim of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XTZOOEDdU3", "forum": "hWyeiVFZ0q", "replyto": "hWyeiVFZ0q", "signatures": ["ICLR.cc/2026/Conference/Submission3132/Reviewer_J6Yf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3132/Reviewer_J6Yf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760989814987, "cdate": 1760989814987, "tmdate": 1762916564738, "mdate": 1762916564738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrainMoE, a universal EEG foundation model that integrates a channel-wise Mixture-of-Experts (MoE) architecture. The key idea is to dynamically assign specialized experts to individual EEG channels, reflecting the brain’s functional heterogeneity. The model employs a ChannelFormer to learn channel-level embeddings, followed by a channel-wise MoE block that activates the most relevant experts selected by a router to learn spatial information. Pre-trained using a masked EEG reconstruction (MAE) objective on the Temple University Hospital EEG corpus, BrainMoE achieves new state-of-the-art performance on several EEG decoding benchmarks (emotion recognition, motor imagery, sleep staging, seizure detection, etc.). The paper also includes detailed ablations and visualizations showing expert specialization patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of a channel-wise MoE represents a creative adaptation of sparse expert models to EEG processing, capturing spatial interactions.  \n\n2. The manuscript is well organized and features high-quality figures.  \n\n3. The experiments span nine datasets and seven EEG decoding tasks, demonstrating BrainMoE’s strong generalization ability.  \n\n4. Visual analyses of expert activations across channels and layers provide insights into task-dependent routing, a step toward interpretability in large EEG models."}, "weaknesses": {"value": "1. **[Major]** Although introducing MoE for spatial learning is interesting, it is the only major novelty of this paper. The other functional blocks in the framework are either existing components or incremental adaptations.  \n\n2. **[Major]** There is no novel pre-training strategy; the model uses the commonly adopted mask-and-reconstruction (MAE) objective, which limits methodological innovation.  \n\n3. **[Major]** No standard deviation (std) is reported in the experimental evaluation. Previously published EEG foundation models typically report results averaged over five random seeds. The lack of multiple runs prevents proper assessment of generalization and robustness.  \n   - Have you run the experiments using different seeds?  \n   - If so, how were these seeds selected?  \n   - Random seed choice can significantly affect performance [a].  \n   [a] Liu, Huan, et al. *“LibEER: A comprehensive benchmark and algorithm library for EEG-based emotion recognition.”* IEEE Transactions on Affective Computing (2025).  \n\n4. The evaluation metrics differ between Table 1 and the analysis experiments.  \n\n5. For the study of MoE configuration (Figure 3), **balanced accuracy (balanced ACC)** is missing and would be more appropriate for datasets with balanced classes.  \n\n6. There is no discussion of model size or computational complexity (FLOPs) compared to baselines.  \n\n7. No reproducible code or pretrained weights are provided. \n \n8. Hyperparameter tables for pre-training and fine-tuning are missing."}, "questions": {"value": "1. Can the authors clarify what specific new design elements, beyond the channel-wise MoE, distinguish BrainMoE from previous foundation models such as LaBraM, BIOT, or CBraMod?  \n\n2. What motivates the architectural choices for the other functional modules — were they directly reused or substantially modified?  \n\n3. Were all experiments conducted with a single random seed? If not, how were the seeds chosen?  \n\n4. Could the authors report the mean ± standard deviation of the performance metrics over multiple runs?  \n\n5. If experiments were conducted only once, how can readers trust the robustness of the reported results? \n \n6. Why are the evaluation metrics (e.g., accuracy, F1-score, balanced accuracy) inconsistent across tables? \n \n7. Why was balanced accuracy (balanced ACC) not included in Figure 3 for the MoE configuration study?\n  \n8. What are the total parameters, active parameters during inference, and FLOPs of BrainMoE compared to other EEG foundation models? \n \n9. Do the authors plan to release the code and pretrained weights for reproducibility? Without such resources, how can the community validate or extend BrainMoE?  \n\n10. Could the authors include detailed hyperparameter settings (e.g., learning rate, masking ratio, optimizer type, batch size, dropout rate)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aQJzze5y1u", "forum": "hWyeiVFZ0q", "replyto": "hWyeiVFZ0q", "signatures": ["ICLR.cc/2026/Conference/Submission3132/Reviewer_nBSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3132/Reviewer_nBSh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141193689, "cdate": 1761141193689, "tmdate": 1762916564572, "mdate": 1762916564572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new foundation model for EEG signals. Its innovation lies in the design of dynamic assignment of channel-wise mixture-of-experts to different channels for better decoding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is based on the concept that better EEG foundation models will come from designing mechanisms that can be tailored to the various pre-training tasks instead of having a uniform data structure for all tasks. The authors claim that a mixture-of-channel-wise-experts is the potential answer to this identified problem / hypothesis and they provide a series of experiments, comparisons and ablation studies to make that point."}, "weaknesses": {"value": "The paper starts with some strong statements: claiming that segmentation of EEG into patches and “monolithic” transformer layers are used in the current foundation models but yet these are exactly the same blocks / tools the paper uses in the proposed method. In my view, the whole methodology keeps many elements from CBraMod and adds some complex building blocks without significant innovation or theoretical benefits. From the paper, I am not convinced how only channel information is so significant for a foundation model trained on only TUEG. The paper does not provide any signal reconstruction metrics or samples. Important building blocks are not even described in the main paper (like the decoder for which the reader needs to go to the appendix). The paper also misses important implementation details: e.g. how the fine-tuning of the models has been performed. The activations of Figure 4 do not provide much interpretable insights - but a strong signal that the model overfits on different datasets.\n\nWriting:\nThe paper also needs some re-writing. Section 2.1 is a currently very complicated to understand without reading it a few times. For example, h is introduced in 130 line and the reader needs to reach line 139 to understand the connection with e. Also some grammar / English checks in the paper are needed - for example “We replaces” line 126. Related work could be moved in the main paper rather than the Appendix. \n\nOverall:\nIt feels that this work is still at an early stage. Feels an over-complicated method with lack of evidence of its benefit. The paper needs to be updated and more evidence needs to be added to convince of its merit."}, "questions": {"value": "1. How has the fine-tuning process taken place for all models (baselines and foundation models) ? The difference is so small that we cannot be sure that this is an actual benefit or just unfair fine-tuning for some models\n2. Which layers are fine-tuned ? In Figure 4, have the experts / router been fine-tuned ?\n3. If the whole foundation model has been fine-tuned along with the classification layer, have the authors tried experiment where the foundation models are frozen and the classification layer only is getting trained? \n4. How about the reconstruction ? Any examples here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kbBQRYfBR2", "forum": "hWyeiVFZ0q", "replyto": "hWyeiVFZ0q", "signatures": ["ICLR.cc/2026/Conference/Submission3132/Reviewer_XwSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3132/Reviewer_XwSu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618163737, "cdate": 1761618163737, "tmdate": 1762916564334, "mdate": 1762916564334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrainMoE, a novel channel-wise Mixture-of-Experts (MoE) EEG foundation model. This architecture is designed to dynamically assign specialized processing experts to different EEG channels. The authors posit that this approach enables task-adaptive decoding, claiming superior performance over prior models across multiple diverse downstream benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper features clear and well-designed figures that effectively illustrate the proposed BrainMoE architecture and its core concepts.\n\n- The authors conduct comprehensive experiments across nine diverse downstream tasks, validating the empirical effectiveness and initial generalizability of BrainMoE as an EEG foundation model."}, "weaknesses": {"value": "- There is a significant lack of logical connection and justification between the stated problem (distinct neural circuit activation across tasks) and the proposed solution (channel-wise MoE expert assignment). Specifically, the model uses channel embeddings $\\mathbf{c}_i$, derived solely from the entire temporal sequence of each channel, to determine the informational content and importance of a brain region for a specific task. It is not sufficiently justified, either theoretically or empirically, that the univariate temporal dynamics of a single channel provide the necessary and sufficient information to determine its critical role in a task for expert routing. This methodological choice fundamentally undermines the persuasiveness of the expert assignment mechanism.\n\n- The absence of statistical significance tests (e.g., $p$-values or confidence intervals) fundamentally weakens the evidence supporting the claimed effectiveness of the proposed method over baselines. Observed performance gains may be due to chance. \n\n- The inconsistent and poorly justified use of evaluation metrics across the paper compromises the reliability and fairness of the reported results. Specifically, Table 1 uses Balanced Accuracy and Weighted F1; Figure 2 and Table 2 add Cohen’s Kappa; and Figure 3 uses only Cohen’s Kappa and Weighted F1. Furthermore, the Evaluation Metrics section on page 5 entirely omits any mention of Cohen’s Kappa. \n\n- The comparison against referenced models (LaBraM and CBraMod) is potentially unfair. Foundation model performance typically scales with size, a trend noted in EEG models [1]. BrainMoE contains 150 M total parameters (with 18 M active), whereas the baselines reportedly have only 4–5 M parameters. Performance evaluation at a comparable parameter scale is essential to clarify whether the performance gain is attributable to the novel MoE architecture itself or simply the increased model size."}, "questions": {"value": "- On page 4, the dimension of the input feature $\\mathbf{W}_j$ is defined. What is the explicit shape of the weight matrix $W_j$ used in the routing mechanism?\n\n- Was the proposed method exclusively evaluated in a subject-independent setting (i.e., cross-subject validation)? If so, how does the model perform in a subject-dependent setting (i.e., within-subject validation)? Furthermore, is the MoE architecture's efficacy maintained or diminished under these subject-dependent scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GI2gbpTuLz", "forum": "hWyeiVFZ0q", "replyto": "hWyeiVFZ0q", "signatures": ["ICLR.cc/2026/Conference/Submission3132/Reviewer_CUHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3132/Reviewer_CUHN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795724785, "cdate": 1761795724785, "tmdate": 1762916564182, "mdate": 1762916564182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}