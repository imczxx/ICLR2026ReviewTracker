{"id": "hR0BbcVMSY", "number": 19676, "cdate": 1758298254598, "mdate": 1759897026388, "content": {"title": "Robust deterministic policy gradient for disturbance attenuation", "abstract": "Reinforcement learning (RL) has achieved remarkable success across various control and decision-making tasks. However, RL agents often show unstable and low performance when it encounter environments with unexpected external disturbances and model uncertainty. Therefore, it is crucial to develop RL agents that can sustain stable performance under such conditions.\nTo address this issue, this paper proposes an RL algorithm called robust deterministic policy gradient (RDPG) based on adversarial RL and $H_\\infty$ control methods. We formulate a maxmin objective function motivated by $H_\\infty$ control, which enables both the agent and the adversary to be trained in a stable and efficient manner.\nIn this formulation, the user seeks a robust policy to maximize the objective function, while an adversary injects disturbances to minimize it.\nFurthermore, for high-dimensional continuous control tasks, we introduce robust deep deterministic policy gradient (RDDPG), which combines the robustness of RDPG with the stability and learning efficiency of deep deterministic policy gradient (DDPG). Experimental evaluations in MuJoCo environments demonstrate that the proposed RDDPG outperforms baseline algorithms in terms of robustness against both external disturbances and model parameter uncertainties.", "tldr": "", "keywords": ["robust reinforcement learning", "deterministic policy gradient"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73cd009025cc2734c82874c6aa786ad11567f751.pdf", "supplementary_material": "/attachment/fffc6edca2b85017ddab7475683d192749141cbd.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes Robust Deterministic Policy Gradient (RDPG), an RL algorithm that integrates principles from control theory and adversarial RL to improve robustness against external disturbances and model uncertainty. The approach formulates the control problem as a two-player zero-sum game between a user agent (policy) and an adversary (disturbance generator), resulting in Robust Deep Deterministic Policy Gradient (RDDPG), which combines the RDPG framework with the stability and sample efficiency of DDPG. \n\nExperiments on four MuJoCo environments show that RDDPG improves robustness to both random external disturbances and model parameter variations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ The work targets a practically important problem: robustness to disturbances and uncertainties in RL control systems, and connects classical robust control with modern adversarial RL.\n\n+ The RDDPG variant is well integrated with DDPG’s structure, making it readily applicable to continuous control tasks.\n\n+ The paper is clearly organized, with thorough background sections connecting robust control theory and RL."}, "weaknesses": {"value": "-- Using random disturbances as the robustness test might not sufficiently challenge the agents. A more meaningful evaluation would be to identify the worst-case adversary for each method and test policies against those learned adversaries.\n\n-- The only robust RL comparison is RARL (2017). Without more recent baselines, it is difficult to assess how competitive RDDPG truly is.\n\n-- Evaluation is confined to four MuJoCo tasks; broader coverage (e.g., Ant or other benchmarks) would strengthen generality.\n\n-- (minor) The legends and axis labels in Figure 2 are small and difficult to read; improving their clarity would help interpret the results.\n\n-- Beyond the integration of control and adversarial training, the paper’s distinct algorithmic or theoretical contributions are limited. It is not clear what new insights are offered beyond the straightforward combination of two existing frameworks. The authors should emphasize why prior robust RL or adversarial methods cannot operate effectively in off-policy deterministic settings, and how RDPG specifically overcomes these limitations."}, "questions": {"value": "Conceptually, is RARL a special case of RDPG where $J_2 = 1$? What additional benefits does the H∞ normalization introduce?\n\nCan RDPG handle stochastic or observation-based disturbances, such as in POMDP or sensor noise settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MeDtLiZlWa", "forum": "hR0BbcVMSY", "replyto": "hR0BbcVMSY", "signatures": ["ICLR.cc/2026/Conference/Submission19676/Reviewer_9Dr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19676/Reviewer_9Dr9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858010596, "cdate": 1761858010596, "tmdate": 1762931523360, "mdate": 1762931523360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper is trying to exploit $H_\\infty$ techniques for robust RL.\n\n\nHonestly, I am having trouble understanding the core message of the paper, despite my reasonable experience in the Robust RL."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Using $H_\\infty$ techniques for robust RL seems novel to me.  However, I don't understand the clear advantages over the traditional RL approaches [1,2,3]. \n\n\n\n[1]@inproceedings{\nkumar2024efficient,\ntitle={Efficient Value Iteration for s-rectangular Robust Markov Decision Processes},\nauthor={Navdeep Kumar and Kaixin Wang and Kfir Yehuda Levy and Shie Mannor},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=J4LTDgwAZq}\n}\n\n[2]@inproceedings{\nkumar2025nonrectangular,\ntitle={Non-rectangular Robust {MDP}s with Normed  Uncertainty Sets},\nauthor={Navdeep Kumar and Adarsh Gupta and Maxence Mohamed ELFATIHI and Giorgia Ramponi and Kfir Yehuda Levy and Shie Mannor},\nbooktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},\nyear={2025},\nurl={https://openreview.net/forum?id=Xx0cJGXU7n}\n}\n\n[3]@unknown{unknown,\nauthor = {Zouitine, Adil and Bertoin, David and Clavier, Pierre and Geist, Matthieu and Rachelson, Emmanuel},\nyear = {2024},\nmonth = {06},\npages = {},\ntitle = {RRLS : Robust Reinforcement Learning Suite},\ndoi = {10.48550/arXiv.2406.08406}\n}"}, "weaknesses": {"value": "Particularly, I don't understand fully the relation between $H_\\infty$ control and Robust RL. Maybe authors can make little more effort to make it more clear.\n\nPaper also misses substantial amount of relevant literature in robust RL."}, "questions": {"value": "Q1) Can this approach be used for simple finite state-action space in the tabular setting? If so can you please, make an explicit connection between $H_\\infty$ control and robust RL in this simplest setting.\n\nQ2) The approach in the paper works for which kind of uncertainty sets among sa-rectangular, s-rectangular and non-rectangular? Does the approach takes into the account for the amount of uncertainty in the model, that is, how does the algorithm adapt for different levels of uncertainty sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "GghRzKvnzL", "forum": "hR0BbcVMSY", "replyto": "hR0BbcVMSY", "signatures": ["ICLR.cc/2026/Conference/Submission19676/Reviewer_dw81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19676/Reviewer_dw81"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089541158, "cdate": 1762089541158, "tmdate": 1762931522841, "mdate": 1762931522841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a deep RL algorithm that is robust to environmental uncertainty. The algorithm is motivated by $H_\\infty$ control theory. Empirical evaluation demonstrates the proposed method's effectiveness in several control tasks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The idea of deriving a robust objective from $H_\\infty$ control theory is interesting. The paper is well organized."}, "weaknesses": {"value": "The paper lacks mathematical rigor in several places, which makes me question the correctness of this paper. Below I list specific issues.\n\n---\n\nIn Section 3.1, the authors derive the $H\\_\\\\infty$ norm of the system $\\\\|T\\_\\\\pi\\\\|\\_{\\\\infty}$:\n\n$$\n\\\\left\\\\|T\\_{\\\\pi, \\\\mu}\\\\right\\\\|\\_{\\\\infty}\\\\underbrace{=}\\_{(a)}\\\\frac{\\\\left\\\\|\\\\mathbf{o}\\_{0: \\\\infty}\\\\right\\\\|\\_{L^2}}{\\\\left\\\\|\\\\mathbf{w}\\_{0: \\\\infty}\\\\right\\\\|\\_{L^2}}\\\\underbrace{=}\\_{(b)}\\\\frac{\\\\mathbb{E}\\\\left[\\\\sum\\_{k=0}^{\\\\infty} \\\\gamma^k r\\_{k+1} \\\\mid \\\\pi\\_\\\\theta, \\\\mu\\_\\\\phi\\\\right]}{\\\\mathbb{E}\\\\left[\\\\sum\\_{k=0}^{\\\\infty} \\\\gamma^k\\\\left\\\\|w\\_k\\\\right\\\\|\\_2^2 \\\\mid \\\\mu\\_\\\\phi\\\\right]}\n$$\n\nI am confused about this derivation:\n\n1. How is $\\\\mathbf{o}\\_{0:\\\\infty}$ connected to the reward sequence $r\\_{1:\\\\infty}$? The relationship is not defined in the text. Also, the right-hand side appears to be missing a square root.\n2. After (a) the expression should include $\\\\sup\\_w$.\n\n---\n\nI believe Eq. (1) is incorrect in general. To make it hold, one needs structural assumptions on the reward function $r$. For example, when $\\\\gamma=0$ Eq. (1) reduces to\n\n$$\nJ^{*} = \\\\min\\_\\\\mu \\\\max\\_\\\\pi r(\\\\pi, \\\\mu) = \\\\max\\_\\\\pi \\\\min\\_\\\\mu r(\\\\pi, \\\\mu)\\\\;,\n$$\n\nwhich obviously does not hold for a general $r$. It does hold under special conditions, such as when $r(a\\_1,a\\_2)$ is concave in $a\\_1$ and convex in $a\\_2$, by Sion's minimax theorem.\n\n---\n\nAbout the equation in line 189:\n- Is this a definition? If so, denote it with $\\\\triangleq$ or $\\\\coloneqq$ instead of $=$.\n- If it is a claimed equality, why does the second equality hold?\n\n---\n\nDefine\n\n$$\nf(\\\\theta, \\\\phi) = \\\\frac{\\\\mathbb{E}\\\\left[\\\\sum\\_{k=0}^{\\\\infty} \\\\gamma^k r\\_{k+1} \\\\mid \\\\pi\\_\\\\theta, \\\\mu\\_\\\\phi\\\\right]}{\\\\mathbb{E}\\\\left[\\\\sum\\_{k=0}^{\\\\infty} \\\\gamma^k\\\\left\\\\|w\\_k\\\\right\\\\|\\_2^2 \\\\mid \\\\mu\\_\\\\phi\\\\right]}.\n$$\n\n- According to line 189, the algorithm performs gradient ascent-descent on $f(\\\\theta,\\\\phi)$ to solve $\\\\max\\_\\\\theta \\\\min\\_\\\\phi f(\\\\theta,\\\\phi)$.\n- This ascent-descent makes sense if the max-min and min-max are equivalent. However, it is non-trivial to guarantee this duality even under some structural assumptions on $r$, because the denominator depends on $\\\\phi$, which complicates convexity/concavity arguments.\n\nOverall, the mathematical portion of the paper needs rigorous and clear arguments. At minimum, the authors should address the above issues.\n\n---\n\nAdditionally, I have concerns about the novelty. It is unclear what advantages this method offers over existing robust deep RL methods, such as the work at https://arxiv.org/pdf/2205.07344. This also proposes a robust deep RL algorithm, and it has several theoretical guarantees at least in the tabular setting (see https://arxiv.org/abs/2212.10439)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MSMunlaR17", "forum": "hR0BbcVMSY", "replyto": "hR0BbcVMSY", "signatures": ["ICLR.cc/2026/Conference/Submission19676/Reviewer_njZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19676/Reviewer_njZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188751308, "cdate": 1762188751308, "tmdate": 1762931522219, "mdate": 1762931522219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an adversarial variant of the reinforcement learning problem, where in addition to learner adaptively choosing their action, an adversary adaptively chooses a disturbance (as a function of the state). Loosely inspired by H-infty control, the paper proposes a minimax objective that trains a policy against an adversarial agent to maximize the expected reward per unit \"energy\" (or l2 norm) of disturbance injected into the system. The paper then proposes a further deterministic policy (actor-critic style) algorithm that evaluates it on 4 Mujoco environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In general, in any minimax setting, it is important to constrain the adversary, e.g., via regularization, step size etc. The paper puts forward a reasonable alternative proposal that limits the l2 norm of the disturbances the adversary can produce.\n\nThe proposal shows reasonable empirical gains for the settings tested in the paper over RARL."}, "weaknesses": {"value": "Despite the presentation, the connection with H-infty control is at most skin deep. Classical robust control results guarantee minimax performance against arbitrary adversary (across all policies, that is, rather than one restricted to a given policy class), and this has provable links to model uncertainty via the small gain theorem. Both these characteristics are absent here. Further, I'm not sure why line 160 follows. The numerator here looks like a l1 norm, and the denominator is a l2-squared norm, very far from the ratio of two l2 norms.\n\nInstead, the results here are better understood in terms of how to best constrain an adversary: the paper suggests the as the sum of squares of the produced disturbances. With this framing, I would expect a more complete evaluation than compares this to parameter norm bounds, step size limits, and/or, for example, entropy regularization. The paper compares solely to Pinto et al 2017 (published 8 years ago), which has ~1K citations, and hence quite a bit of follow up work. Can the authors comment on how the paper compare to more recent adversarial RL works? And if they would be suitable for comparisons?\n\nStarting with line 190, the work denotes the denominator purely as a function of the adversaries policies (J_2^{\\mu_\\phi}). But this seems like either a mistake or approximation, because the disturbances indeed are produced by \\mu_\\phi acting on states, but the distribution over states is jointly arrived at using both the learner's and the adversary's policies. This would dictate how one differentiates through the dynamics.\n\nWhile robustness to disturbances results are mismatched in the sense they consider zero-mean disturbances, which is in the stochastic control territory rather than robust control. It would be more encouraging to see performances against \"difficult\" disturbances.\n\nOn a similar note, model uncertainties are well considered. But here there is a vast literature on robust MDPs (both with and without function approximation) that is tailored to these settings. But no comparisons are present."}, "questions": {"value": "I would like the authors to address the points raised in the last section and point out any misunderstanding where necessary.\n\nMinor comment: On line 111, y_{0:\\infty} should be o_{0:\\infty}."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gn8C9E92gq", "forum": "hR0BbcVMSY", "replyto": "hR0BbcVMSY", "signatures": ["ICLR.cc/2026/Conference/Submission19676/Reviewer_CW1A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19676/Reviewer_CW1A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762827187278, "cdate": 1762827187278, "tmdate": 1762931521550, "mdate": 1762931521550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}