{"id": "bxGQx7mrnR", "number": 23239, "cdate": 1758341157535, "mdate": 1763746609239, "content": {"title": "S-Chain: Structured Visual Chain-of-Thought for Medicine", "abstract": "Faithful reasoning in medical vision–language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce \\textbf{S-Chain}, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,  LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.", "tldr": "A new, large-scale expert-annotated dataset for medical visual question answering", "keywords": ["visual chain-of-thought", "medical MLLMS"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c4bf6422fb7b95c5e8b4e471101ceb71c42a985e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces S-Chain, a large-scale dataset of 12,000 expert-annotated brain MRI images with structured visual chain-of-thought reasoning for Alzheimer's disease diagnosis. The dataset features bounding-box annotations, standardized clinical grading (MTA/GCA/Koedam scales), and multi-step reasoning chains across 16 languages (700k QA pairs total). The authors benchmark medical and general-purpose VLMs, demonstrating that expert-grounded SV-CoT supervision outperforms GPT-4 synthetic CoT by 4-15% accuracy. Additional experiments explore integration with RAG and the faithfulness of reasoning-grounding alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. High-quality expert annotation with 700 hours of 3-doctor consensus using standardized clinical scales (Scheltens/Pasquier/Koedam). The 100% inter-annotator agreement demonstrates rigorous quality control.\n\n2. Evaluation across medical VLMs (ExGra-Med, LLaVA-Med) and general VLMs (Qwen2.5-VL, InternVL2.5) with informative ablations. \n\n3. Clear empirical gains: 8-15% over base models and 4-5% over GPT-4 synthetic CoT. Multilingual support (16 languages) enhances accessibility, though the practical benefits remain undervalued."}, "weaknesses": {"value": "1. The paper addresses a single disease (Alzheimer's), single task (3-class severity grading), and a single modality (brain MRI), yet claims to establish principles \"for medicine\" broadly. In addition, the task is not differential diagnosis (AD vs. vascular dementia vs. Lewy body dementia vs. normal aging) but merely grading pre-diagnosed dementia patients into Non/Mild/Moderate severity levels. This is fundamentally a simpler task that bypasses the challenging diagnostic reasoning physicians actually perform.\n\n2. Only 64 patients total (55 train, 9 test). While the paper touts \"12,000 images,\" these are simply multiple slices from the same 64 patients. The test set of 9 patients is far too small to establish statistical significance—a single misclassified patient affects ~11% of test images.\n\n3. Medical imaging encompasses dozens of anatomies, hundreds of diseases, and diverse modalities. The 700-hour annotation requirement for this single-disease dataset makes the approach impractical for comprehensive medical AI.\n\n4. The paper's own experiments reveal the task is trivial, questioning the need for complex CoT. Traditional computer vision (classification model) would likely achieve comparable or better results with higher reliability and lower cost.\n\n5. Section 4.3B shows models with correct bounding boxes (60.4%) versus shuffled boxes (55.4%) differ by only 5%. This indicates models primarily learn from text keywords (e.g., \"hippocampus\") rather than actual spatial coordinates. The 700 hours spent meticulously drawing bounding boxes provided minimal value—text-only CoT could likely achieve similar results at ~10% of the annotation cost.\n\n6. The paper compares against GPT-4.1-generated synthetic CoT, which predictably performs poorly (mIoU 4.2-4.3, Figure 10 shows hallucinated boxes). But GPT-4.1 is not a medical model and not designed for precise spatial localization.  Reasonable baselines include: (a) medical segmentation models (nnU-Net, MedSAM) for ROI localization + LLM for text reasoning, (b) specialist AD classification models from prior literature on OASIS/ADNI datasets, and (c) rule-based systems using established MTA/GCA/Koedam thresholds. This\n\n7. Slice-Level vs. Patient-Level Label Confusion. Table 2 note states: \"A patient may show different labels across slices (e.g., Non-Dementia in one slice, Mild-Dementia in another)\" Does each slice get independent expert rating? Or do all slices inherit patient-level CDR? If independent, what's the inter-rater agreement for slice-level labels? \n\n8. Section D.1: Experts select slices showing \"clearest pathological changes\" For Moderate patients, do experts cherry-pick slices with severe atrophy? For a CDR=2 (Moderate) patient with 170 slices, how many are labeled Moderate vs. Mild vs. Non?\n\n9. In medical setting, usually we only care about patient-level metrics not slice-level metrics, the test set is too small for statistical significance."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yP7kqM55Yh", "forum": "bxGQx7mrnR", "replyto": "bxGQx7mrnR", "signatures": ["ICLR.cc/2026/Conference/Submission23239/Reviewer_UPiW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23239/Reviewer_UPiW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613559071, "cdate": 1761613559071, "tmdate": 1762942571295, "mdate": 1762942571295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We would like to withdraw our submission. After reviewing the initial feedback, we feel that the evaluation focused primarily on methodological aspects, whereas our main contribution—an expert-curated dataset for structured visual CoT reasoning in clinical contexts, which was submitted to the Dataset and Benchmark track. Since the value and intent of the dataset were not fully aligned with the review emphasis, we believe withdrawal is the most appropriate course of action at this time. We thank you AC and Reviewers for handling our submission.\n\nRegards\n\nAuthors"}}, "id": "PqcCUyPgLq", "forum": "bxGQx7mrnR", "replyto": "bxGQx7mrnR", "signatures": ["ICLR.cc/2026/Conference/Submission23239/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23239/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763746607990, "cdate": 1763746607990, "tmdate": 1763746607990, "mdate": 1763746607990, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces S-CHAIN, a large-scale medical dataset featuring Structured Visual Chain-of-Thought (SV-CoT) annotations. It defines a four-stage reasoning framework (Q1 to Q4; from lesion localization to final diagnosis) and demonstrates that fine-tuning visual-language models on these structured annotations significantly enhances accuracy, faithfulness, and visual grounding. Furthermore, integrating MedRAG further improves diagnostic reasoning and overall model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The major strength of this paper is the release of a clinically validated dataset, which ensures that all annotations are verified by medical experts.\n\n2. While the technical contribution is somewhat limited, the efforts to design the four-stage reasoning framework, construct the dataset, and make it publicly available are highly valuable."}, "weaknesses": {"value": "1. The dataset is limited to MRI scans for dementia.\n\n2. The dataset does not consider the volumetric (3D) characteristics of the original MRI scans."}, "questions": {"value": "1. I am curious about the performance when the RL-based fine-tuning method is applied instead of SFT.\n\n2. I am curious about the performance of a conventional object detection or classification model trained on the collected dataset instead of using an LLM-based model. This would show how well a model specifically trained for this dataset can perform, and such results might even outperform those obtained from LLM-based methods."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xwQvAVXlt0", "forum": "bxGQx7mrnR", "replyto": "bxGQx7mrnR", "signatures": ["ICLR.cc/2026/Conference/Submission23239/Reviewer_kxWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23239/Reviewer_kxWm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847407436, "cdate": 1761847407436, "tmdate": 1762942571059, "mdate": 1762942571059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents S-Chain (Structured Visual Chain-of-Thought), the large-scale medical visual reasoning benchmark that explicitly aligns visual evidence with step-by-step reasoning processes. S-Chain consists of 12,000 expert-annotated medical images, including ROI bounding boxes and structured reasoning chains (SV-CoT), and is further expanded into 16 languages with 700,000 question–answer pairs. The benchmark models the clinical diagnostic workflow through four sequential stages: (1) lesion localization, (2) lesion description, (3) grading or severity assessment, and (4) disease classification. This framework emphasizes causal consistency across visual evidence–reasoning–conclusion links, enabling both training and evaluation of multimodal medical large language models (e.g., ExGra-Med, LLaVA-Med, Qwen2.5-VL, InternVL2.5). Experimental results show that expert-supervised reasoning chains from S-Chain outperform GPT-4.1–generated CoTs by 10–15% in accuracy, interpretability, and visual alignment. The paper further explores integration with MedRAG for external knowledge enhancement and introduces a lightweight ROI–CoT alignment regularization to improve reasoning faithfulness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tS-Chain introduces the first expert-annotated Structured Visual Chain-of-Thought (SV-CoT) benchmark, covering 12k medical images across 16 languages, effectively filling the gap in evaluating visual–reasoning consistency within the medical domain.\n2.\tThe four-stage structured reasoning process (localization → description → grading → diagnosis) mirrors real clinical diagnostic logic, enabling models to generate traceable and interpretable reasoning paths while mitigating hallucinations and semantic drift.\n3.\tAcross multiple medical and general multimodal models, S-Chain outperforms GPT-generated reasoning data, achieving notable improvements in metrics such as F1, mIoU, and BLEU, with further performance gains observed when integrated with MedRAG for external knowledge augmentation."}, "weaknesses": {"value": "1.\tThe dataset is primarily based on the OASIS Alzheimer’s MRI collection, resulting in a relatively narrow disease scope and imaging modality coverage, which limits generalization and transferability to broader clinical contexts.\n2.\tThe annotation process required approximately 700 hours of work by three medical experts, posing scalability challenges for expanding to diverse disease types or multi-center datasets in the future.\n3.\tThe methodological contribution is limited, leaning more toward a dataset-oriented study. The proposed CoT–ROI alignment regularization yields only a modest 1–2% improvement, indicating limited methodological innovation and theoretical depth.\n4.\tSome comparisons may not be entirely fair—performance differences with GPT-4.1–generated CoTs could be influenced by teacher prompting variations, lacking strict control-variable experiments."}, "questions": {"value": "1.\tHow do the authors quantify consistency between visual evidence and textual reasoning in SV-CoT, and how is causal correctness measured?\n2.\tIf transferred to radiology, endoscopy, or pathology, does the four-stage reasoning framework remain applicable without major redesign?\n3.\tWere expert annotations conducted under ethical/IRB approval? Could multilingual expansion introduce risks of patient re-identification or leakage of sensitive attributes?\n4.\tIs the CoT–ROI alignment regularizer stable across modalities? Are there signs of negative transfer in certain settings?\n5.\tCan S-Chain support temporal or multi-stage reasoning (e.g., longitudinal follow-up) to better match real clinical workflows?\n6.\tHas inter-rater reliability among annotators been evaluated to ensure label consistency and robustness?\n7.\tDoes the complementarity between SV-CoT and MedRAG degrade under data noise or modality mismatch, and how is this mitigated?\n8.\tWhat safeguards are in place to prevent misleading or overconfident diagnostic explanations, especially in automated report generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gNdPF6bmDZ", "forum": "bxGQx7mrnR", "replyto": "bxGQx7mrnR", "signatures": ["ICLR.cc/2026/Conference/Submission23239/Reviewer_Wbm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23239/Reviewer_Wbm2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904341701, "cdate": 1761904341701, "tmdate": 1762942570698, "mdate": 1762942570698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces S-Chain, a dataset of 12,000 expert-annotated medical images with structured visual reasoning across 16 languages. It links visual evidence to reasoning steps, and sets a new benchmark for grounded and explainable medical AI."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A large-scale dataset\n2. Expert-involved annotation pipeline"}, "weaknesses": {"value": "1. The dataset mainly consists of Alzheimer’s disease (AD) MRI figures, introducing a significant bias that limits the generalization and undermines the broad claim of “Chain-of-Thought for Medicine.”\n2. The proposed Chain-of-Thought (CoT) approach appears too rigid, resembling a predetermined analytical workflow rather than flexible, natural reasoning.\n3. The results show Gemini 2.5 Flash performing much better than all other models, which seems unusual and raises concerns about the evaluation setup or fairness. The authors should clarify possible reasons for this discrepancy and verify the experimental consistency."}, "questions": {"value": "1. The dataset appears to focus primarily on AD MRI data. Could the authors elaborate on how this potential bias might affect the generalizability of the dataset? Are there any plans to extend S-Chain to cover a broader range of diseases or imaging modalities to better support the “for Medicine” claim?\n\n2. The proposed Chain-of-Thought (CoT) framework seems relatively structured. Could the authors clarify whether this design can show generalization to other clinical analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "odKAzPJgU6", "forum": "bxGQx7mrnR", "replyto": "bxGQx7mrnR", "signatures": ["ICLR.cc/2026/Conference/Submission23239/Reviewer_Nk7y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23239/Reviewer_Nk7y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931084314, "cdate": 1761931084314, "tmdate": 1762942570188, "mdate": 1762942570188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}