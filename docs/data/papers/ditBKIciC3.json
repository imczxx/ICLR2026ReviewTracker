{"id": "ditBKIciC3", "number": 13989, "cdate": 1758226547224, "mdate": 1763647679058, "content": {"title": "Beyond Spectra: Eigenvector Overlaps in Loss Geometry", "abstract": "Local loss geometry in machine learning is fundamentally a two-operator concept. When only a single loss is considered, geometry is fully summarized by the Hessian spectrum; in practice, however, both training and test losses are relevant, and the resulting geometry depends on their spectra together with the alignment of their eigenspaces. We first establish general foundations for two-loss geometry by formulating a universal local fluctuation law, showing that the expected test-loss increment under small training perturbations is a trace that combines train and test spectral data with a critical additional factor quantifying eigenspace overlap, and by proving a novel transfer law that describes how overlaps transform in response to noise. As a solvable analytical model, we next apply these laws to ridge regression with arbitrary covariate shift, where operator-valued free probability yields asymptotically exact overlap decompositions that reveal overlaps as the natural quantities specifying shift and that resolve the puzzle of multiple descent: peaks are controlled by eigenspace (mis-)alignment rather than by Hessian ill-conditioning alone. Finally, for empirical validation and scalability, we confirm the fluctuation law in multilayer perceptrons, develop novel algorithms based on subspace iteration and  kernel polynomial methods to estimate overlap functionals, and apply them to a ResNet-20 trained on CIFAR10, showing that class imbalance reshapes train–test loss geometry via induced misalignment. Together, these results establish overlaps as the critical missing ingredient for understanding local loss geometry, providing both theoretical foundations and scalable estimators for analyzing generalization in modern neural networks.", "tldr": "Loss geometry depends not only on train and test Hessian spectra but also on alignment of eigenspaces; we derive a universal fluctuation law, explain covariate shift and multiple descent, and develop scalable estimators for overlaps in large NNs.", "keywords": ["hessian", "overlap", "eigenvector", "geometry", "ridge regression", "noise", "free probability", "algorithms", "CIFAR", "high dimensional statistics", "generalization", "covariate shift", "double descent", "multiple descent", "random matrix theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1eb0da7b2866682c9eb0087dd9428e8da206be7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors first formalize the local fluctuation at a local minimum of a model, and then the authors analyze the eigen space of the Hessian of the training loss landscape during these fluctuations. Finally, the authors relate the training Hessian to the Hessian eigen space of the test loss landscape to explain loss behavior during covariate shift and commonly studied optimization behavior, such as multiple descent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The reviewer believes that the contribution of this paper is strong. Through analyzing the loss landscape between training and testing, the authors explain the behavior of a couple of important scenarios in modern deep learning, such as covariate shifts (commonly happen in applications), and multiple descent (commonly observed during optimization of over-parameterized models).\n2. The author not only provides analysis and empirical results with synthetic training/testing environments but also extends their findings to more complex datasets such as CIFAR. \n3. The work is novel."}, "weaknesses": {"value": "Minor:\n1. The related work seems to slightly lag behind, for exammple, regularizers that encourage cross-domain invariance have recent advancements such as [1]. Maybe the related work can also touch on sharpness-aware optimization and other robust optimization techniques that focus on controlling the gradient and the Hessian. It will round out the related work nicely to tie it back to the more application side of machine learning. \n2. A suggestion on notation: in eq 6, a notation $q$ is used, but it was only defined until the page after. Then also a parameter $\\alpha = q^{-1}$ is used through out the paper. Perhaps the author can tidy up this notation and stick with $\\alpha$\n\nMajor:\n1. The presentation of the work can be improved. The reviewer sincerely hope that the authors can use the additional page during rebuttal to strengthen/clarify some of the theoretical points and figures in the manuscript. See following for details. \n\n\n[1]: Hasan, Ali, Haoming Yang, Yuting Ng, and Vahid Tarokh. \"Elliptic Loss Regularization.\" In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Perhaps the progression from eq 1 to eq 2 can be improved with a slight introduction on how eq 2 is achieved? Similarly, this can be improved through the development of eq 5-7 (or refer to the appendix). \n2. On line 156, the authors assumed $\\mathbb{E}z = 0$, is this assumption realistic during scenarios such as covariate shift? If the noise $\\epsilon$ is caused by covariate shift while the training data is centered, does this mean $\\mathbb{E}z \\neq 0$?\n3. How should one intuitively understand the paragraph from lines 214 to 217. Does this essentially mean that if the noisy part of the train landscape aligns with the more important part of the test landscape, we will see increased loss value during evaluation?\n4. In Proposition 1, can the authors explain conceptually what *\"X is free from A, B\"* means? Does it simply mean independence?\n5. Figure 1 a) seems confusing. What is the purpose of the red contour in Figure 1a)? How are they related to the cyan? Figure 1 b) and c) are cleaner and more understandable. \n6. Figure 3 b), does each color of the contour mean a specific ranking of eigenvalue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mtTVO3o0GN", "forum": "ditBKIciC3", "replyto": "ditBKIciC3", "signatures": ["ICLR.cc/2026/Conference/Submission13989/Reviewer_tXsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13989/Reviewer_tXsV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606421762, "cdate": 1761606421762, "tmdate": 1762924486556, "mdate": 1762924486556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors derive a universal local fluctuation law considering the train/test loss Hessian spectra and overlaps of their eigenspaces, and apply it to ridge regression, trying resolve the puzzle of multiple descent.\nThey also empirically validate their theory with MLPs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This paper provides a new perspective on the importance of the eigenspace overlaps.\n- It explains the multiple descent phenomenon from this perspective."}, "weaknesses": {"value": "- As (3) is very important equation in the paper, I'd like to know how to obtain the equation in detail. The explanation is very unclear to me.\n    - The approximations of (1) and (2) are not clear. In what sense, they are approximated? (e.g., $O(\\\\|w-w_0\\\\|^2), O(\\\\|\\epsilon\\\\|^2)$)\n    - What is the definition of \n        - $J_{\\text{train}}(w,\\epsilon)$ (Is it $J\\_{\\text{train}}(w+\\epsilon)$?)\n        - $H_{\\text{train}}$? If it depends on $w$ as written in the paper ($H_{\\text{train}}:=d\\nabla^2 J_{\\text{train}}(w,0)$), then (1) is not quadratic and $\\Delta w$ is not just $-H^{-1}_{\\text{train}}z$. \n        - $\\Delta J_{\\text{test}}$ (Is it $J_{\\text{test}}(w_0+\\Delta w,\\epsilon)-J_{\\text{test}}(w_0,\\epsilon)$? or without $\\epsilon$?)\n    - The approximations of (1) and (2), but equality in (3). How do we get the exact equality in (3) from (1) and (2)?\n- The caption of Fig 2a says \"$J_\\text{{\\color{red}train}},\\Delta J_{\\text{test}}$ and bias\" but in the panel it says \"$J_{\\text{test}},\\Delta J_{\\text{test}}$, Bias.\"\n- em dash? (L41, L222, L262, L348, L353, L361, ...)\n- errata? $\\mu_\\Sigma = p_1\\delta_{s_1^{\\color{red}2}}+p_2\\delta_{s_2^2}$\n- What is $\\tilde\\lambda{\\color{red}'}$ in (9)?\n- errata? (L265) Isn't it $s_1^2,s_2^2=2^0,2^{-4}$? The eigenvalues in Fig 1(b) should be $s_i^2$ as written in (8)."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xzGlANJ57z", "forum": "ditBKIciC3", "replyto": "ditBKIciC3", "signatures": ["ICLR.cc/2026/Conference/Submission13989/Reviewer_GPhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13989/Reviewer_GPhb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728795796, "cdate": 1761728795796, "tmdate": 1762924486178, "mdate": 1762924486178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the use of loss geometry for both train and test sets, specifically the eigenvector overlap between the two, rather than just their spectra, as measure of loss geometry and to predict generalization. It shows that this measure explains covariate shift and multiple descent through a unified lens via simulations on synthetic data. It also develops an efficient estimator to show how this measure can quantify how class imbalance can induce misalignment in train-test loss geometry."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes interesting contributions to use eigenvalue overlap between train and test Hessians, rather than just their spectra, as measure of loss geometry and generalization, and presents a scalable numerical estimator and empirical validations."}, "weaknesses": {"value": "The paper is missing an expanded related work section with a detailed discussion on most closely related works. As someone who is not very familiar with the literature on random matrix theory, and eigenspace overlap estimators, it is hard to judge the novelty of this work relative to other works. It would be good to add a more detailed discussion comparing and contrasting the work with most closely related work.\n\nThe experimental methodology in Section 3.4 is somewhat strange. It is stated that “a CIFAR-10 trained ResNet-20 was obtained from Chen”, and then “5000 train and test examples were randomly selected to define train and test Hessians”. A more convincing experiment would be to train it from scratch using the selected train set samples. Additionally, why not use imbalanced train set with balanced test set to compare the effect of imbalance?"}, "questions": {"value": "Can authors add details on compute and runtime for the CIFAR-10 results in Section 3.4? Is it possible to use the proposed method for larger models?\n\nSuggestions to improve writing/readability:\n\n1. The paper uses hyphens instead of em dashes in almost every occurrence, and in some cases, it uses en dashes instead of hyphens, please fix.\n2. The paper uses $J$ to denote the loss. I suggest using $\\mathcal{L}$, or simply $L$, which is more standard. $J$ could be misinterpreted as denoting the Jacobian.\n3. In line 157, the expectation terms are missing square brackets.\n4. In line 162, the order of $H_{\\text{test}}$ and $C_{\\text{train}}$ is swapped. \n5. In line 215, it should be ‘significant’.\n6. In Eq. (8), it should be $\\delta_{s_2}^2$.\n7. In Fig. 1, panel (b) is missing the y-axis label, please clarify.\n8. In most cases, the word ‘traces’ is used to refer to the solid lines in the plots. I suggest simply using ‘solid lines’. \n9. In most cases the subfigures are referred to as, e.g., Fig. 1a instead of Fig. 1(a), please fix.\n10. Fig. 2 caption states “Traces in panel a) correspond to gold and blue lines”. Please clarify.\n11. In line 323, the phrase “geometric cartoon” should be rephrased.\n12. In line 377, ‘2d’ should be ‘2D’."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RnWTDT63E5", "forum": "ditBKIciC3", "replyto": "ditBKIciC3", "signatures": ["ICLR.cc/2026/Conference/Submission13989/Reviewer_zuhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13989/Reviewer_zuhF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782124767, "cdate": 1761782124767, "tmdate": 1762924485313, "mdate": 1762924485313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how the relationship between the 2nd order shape of the train and test objectives affects generalization. They then use the general results to study several regression settings, including covariate shift. Finally, they extend the results to neural networks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The results are interesting and provide an additional perspective from which to view generalization. I am not familiar enough with the details of the related literature to know if this is the first work to consider this, but I trust other reviewers will know more. The paper also contains a very large number of results."}, "weaknesses": {"value": "As written, this paper reads like a physics paper. In order to be published in a computer science / ML venue, I think it needs a bit more exposition describing notation, exactly the approximations being made in Section 3, and what the operators in 3.1.2 represent in the ML setting. \n\n1. Is $\\Delta J_\\text{test} = J_\\text{test}(w_0 + \\Delta w) - J_\\text{test}(w_0)?$\n2. What is the source of the noise $\\epsilon$? Is it sampling noise, label noise, general?\n3. It would help to break up Section 3 into Theorem statements.\n4. It would help to have prose at the start of Section 3, 3.1 describing what those sections do.\n5. Consider separating out preliminaries for notation and results. The authors should be more explicit about things like what the noise is, what exactly $\\Delta J_\\text{test}$ is, etc.\n\nIt seems like a nice paper, but I would recommend either submitting it to a different venue or writing it more in the style of ML theory papers for publication at an ML conference."}, "questions": {"value": "Feel free to clarify the points above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vnF18fDEJT", "forum": "ditBKIciC3", "replyto": "ditBKIciC3", "signatures": ["ICLR.cc/2026/Conference/Submission13989/Reviewer_G7EJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13989/Reviewer_G7EJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149257830, "cdate": 1762149257830, "tmdate": 1762924484520, "mdate": 1762924484520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the alignment between eigenvectors of train and test loss Hessians and its impact on generalization, extending prior works that focused mainly on eigenvalue spectra. A universal local fluctuation law is formulated to demonstrate the predictive role of eigenvector alignment for test loss. The theory is further applied to linear regression under covariate shift and multiple descent scenarios. Experiments extend the framework to neural networks, including MLPs and ResNet-20 trained on CIFAR-10, with a novel algorithm estimating eigensubspace alignment in the bulk of small eigenvalues."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a novel and original analysis revealing the importance of eigenvector alignment between train and test Hessians, which is an interesting and underexplored aspect in the literature.\n- A new framework is introduced to evaluate the alignment between eigenvectors associated with the bulk of small eigenvalues.\n- The analysis in linear regression provides concrete intuition on covariate shift and multiple descent."}, "weaknesses": {"value": "-  Although the eigenvector alignment analysis is theoretically interesting, it is unclear how this insight could translate into practical benefits for model training, as test data are not available during training.\n-  The experiments involving neural networks in Sections 3.3 and 3.4 lack clear motivation. Section 3.3 partially supports the theory in Section 3.1 but does not provide concrete insights into MLP generalization or learning dynamics. The purpose of the figures at the bottom of Figure 4 is also unclear and needs further explanation. Section 3.4 proposes an interesting scalable estimation method for bulk subspace alignment, yet the analysis does not seem to make it a central element of the argument, which leaves its importance underemphasized. Moreover, the section concludes by examining train–test Hessian misalignment under test-class imbalance, without providing additional insights into generalization.\n-  The paper’s organization could be improved for readability. Key equations (e.g., (13)) and concepts (e.g., smoothing kernels in (10)) are relegated to the appendix. Terms such as “error increment” and “bias” in Figure 2(a, c) should be explicitly defined in the main text.\n\n**Minor comments**\n\n- The caption of Figure 4 (referring to $H_{test}$) does not match the main text (referring to $H_{train}$)."}, "questions": {"value": "-\tPlease refer to the points raised in the weaknesses section.\n-\tThe developed theory resembles the Takeuchi information criterion (TIC). Could the authors discuss potential connections or differences? (see, e.g., Thomas et al., On the interplay between noise and curvature and its effect on optimization and generalization, AISTATS 2020). \n-\tHow can the analysis be adapted to account for stochastic optimization?\n-\tHow robust is the estimator proposed in Section 3.4? Some ablation studies on its hyperparameters would strengthen the argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "USC4Pt6RzH", "forum": "ditBKIciC3", "replyto": "ditBKIciC3", "signatures": ["ICLR.cc/2026/Conference/Submission13989/Reviewer_Rqbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13989/Reviewer_Rqbg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233982616, "cdate": 1762233982616, "tmdate": 1762924483922, "mdate": 1762924483922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}