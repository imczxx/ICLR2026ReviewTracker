{"id": "vH7OAPZ2dR", "number": 22705, "cdate": 1758334579517, "mdate": 1759896851449, "content": {"title": "GLASS Flows: Efficient Inference for Reward Alignment of Flow and Diffusion Models", "abstract": "The performance of flow matching and diffusion models can be greatly improved at inference time using reward adaptation algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the *sampling* method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a ''flow matching model within a flow matching model'' to sample Markov transitions. As we show in this work, this ''inner'' flow matching model can be retrieved from any pre-trained model without any re-training, effectively combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models.", "tldr": "We improve inference-time reward alignment of flow matching and diffusion models by proposing a novel sampling paradigm that enables more efficient exploration.", "keywords": ["Flow Matching; Diffusion Models; Reward Alignment; Reward Adaptation; Inference-time scaling; Feynman-Kac Steering; Markov transitions; Sampling methods"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e0071afe1359e3fc7984cc288da6c842034819b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **GLASS Flows**—a sampling paradigm that lets a pre‑trained flow/diffusion model simulate **stochastic Markov transitions** using only **ODE integration**. The key idea is to view a transition $x_t \\rightarrow x_{t'}$ as its own conditional generation problem and to construct an “inner” flow-matching model $u_s(\\bar x_s\\mid x_t,t)$ whose terminal state $\\bar x_{s=1}$ is distributed as the desired transition $p_{t'|t}(\\cdot\\mid x_t)$. Technically, the paper defines a **GLASS transition** by a bivariate Gaussian joint\n$(X_t,X_{t'})\\sim\\mathcal N(z\\mu,\\Sigma)$ with a tunable correlation $\\rho$ across time (no cross‑dimension correlation), inducing $p_{t'|t}$. In practice, GLASS is a drop‑in replacement wherever reward‑time algorithms need stochastic transitions: (i) posterior sampling and value‑function estimation $V_t$, (ii) Sequential Monte Carlo (SMC; Feynman–Kac steering), and (iii) guidance. Experiments on SiT/DiT ImageNet‑256 and FLUX (768×1360) show better few‑step posterior sampling (lower FID at small inner steps $M$), higher correlation in $V_t$ estimation, and SMC reward gains without sacrificing GenEval alignment; with $M=1$ the method reduces to DDIM."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper empirically demonstrates that GLASS transitions retain ODE‑level efficiency yet produce stochastic branches needed for SMC/search. This answers a real bottleneck noted in prior work: practitioners prefer ODE/probability‑flow sampling for speed (e.g., EDM design space) while SMC/search need sampling from $p_{t'|t}$ (typically SDE‑based).\n- Casting a transition $p_{t'|t}$ as an inner flow and showing that its denoiser is just the sufficient statistic re‑timing of the existing denoiser is elegant and broadly applicable. It explains in one stroke how to obtain stochasticity from an ODE‑driven model without re‑training. The mapping via $S(x)=\\frac{\\mu^\\top\\Sigma^{-1}x}{\\mu^\\top\\Sigma^{-1}\\mu}$ and $t^\\*$ is simple to implement and leverages any pre‑trained flow/diffusion network.\n- Plug‑and‑play application to Feynman–Kac SMC (GLASS‑FKS) and reward guidance improves reward metrics while preserving GenEval alignment, and does so on strong modern backbones (SiT/DiT and FLUX). This makes the method attractive to practitioners who already deploy ODE sampling but need stochastic branches.{index=6}"}, "weaknesses": {"value": "The statement that GLASS “eliminates the trade-off between efficiency and stochasticity” is strong, yet Tables 1 and 4 show roughly comparable rather than strictly superior performance. Missing wall-clock time and variance analyses weaken this claim.\n\nThe influence of correlation $ \\rho $ and inner-scheduler choices (CondOT vs. alternatives) is underexplored. Additional ablations could clarify their role in balancing stochasticity and sample quality."}, "questions": {"value": "1. How does GLASS handle near-singular covariance matrices when $ |\\rho| \\approx 1 $? Are any clipping or jitter techniques applied to maintain numerical stability?\n\n2. Is $ g(t) = \\sigma_t^2/\\alpha_t^2 $ guaranteed to be invertible under learned or non-monotonic schedules? If not, how is $ t^\\* $ computed?\n\n3. Could $ \\rho $ be learned or dynamically adjusted during inference to balance exploration and stability, especially in SMC or value-guided sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zt9vWyFAKJ", "forum": "vH7OAPZ2dR", "replyto": "vH7OAPZ2dR", "signatures": ["ICLR.cc/2026/Conference/Submission22705/Reviewer_1Ktu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22705/Reviewer_1Ktu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900845712, "cdate": 1761900845712, "tmdate": 1762942348156, "mdate": 1762942348156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **GLASS Flows**, a plug-and-play sampler for diffusion/flow-matching models that realizes stochastic transitions via an inner ODE (instead of an SDE). By reparameterizing a pretrained denoiser into a ``GLASS denoiser\" using sufficient statistics, the method constructs a transition kernel \\(p_{t'|t}\\) without retraining, unifying ODE-level quality with SDE-like randomness. This improves posterior sampling and downstream inference-time reward adaptation (e.g., SMC/FKS, reward guidance, search). At matched NFE, GLASS approaches ODE quality, outperforms SDE, and yields stronger prompt following/reward alignment on large T2I models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Theory to practice bridge: Closed-form construction of a stochastic transition via an inner ODE.\n* Drop-in practicality: No finetuning; works with CFG, SMC/FKS, reward guidance, and value-estimation pipelines.\n* Quality & alignment: Consistent gains on prompt-following and reward-driven metrics at the same NFE, while retaining randomness for search/SMC.\n* Clear complexity lens: Cost summarized by NFE = \\(K \\times M\\) (transitions × inner steps), enabling fair compute-matched comparisons."}, "weaknesses": {"value": "1. **Choice of $\\rho$.**   \nThe paper fixes $\\rho = 0.4$  for most experiments, but this parameter directly controls the stochasticity of the transition. It would be helpful to show how varying $\\rho$ in $[0, 1]$ affects generation quality, prompt adherence, and diversity, and to explain why $\\rho = 0.4$  was chosen. In addition, please comment on whether different models (e.g., SiT vs. FLUX) show similar sensitivity to $\\rho$ or require different optimal values.\n\n2. **Under-evaluated relation to DTM/Transition-Matching.**  \nDTM is claimed as the  $\\rho = 1$ special case, but there may lacks head-to-head comparison.\n\n3. **Positioning vs post-training with reward models (RL/DPO/GRPO).**  \nRecent post-training approaches such as GRPO[1,2] or DPO also leverage reward models for alignment, but differ by updating model parameters rather than modifying inference. It would be helpful if the authors could clarify the conceptual connection and distinction between these paradigms, and possibly comment on whether GLASS could be complementary to such training-based methods or directly compared under similar reward setups.\n\n[1] DanceGRPO: Unleashing GRPO on Visual Generation\n\n[2] MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jtxtWIKI9E", "forum": "vH7OAPZ2dR", "replyto": "vH7OAPZ2dR", "signatures": ["ICLR.cc/2026/Conference/Submission22705/Reviewer_56nT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22705/Reviewer_56nT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989594736, "cdate": 1761989594736, "tmdate": 1762942346841, "mdate": 1762942346841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GLASS Flows, a new inference-time sampling paradigm that constructs an inner flow matching model within a pre-trained one to efficiently simulate stochastic evolution, combining the stochasticity of SDEs with the efficiency of ODEs without any retraining, thereby significantly improving the performance and efficiency of flow and diffusion models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces an elegant sampling method (GLASS flow) that successfully enhances sample diversity without compromising quality.\n\nThis represents a fundamental improvement and holds significant potential for integration into a wide range of reward alignment frameworks."}, "weaknesses": {"value": "The paper lacks a direct quantitative comparison of sample diversity between the proposed GLASS flow and standard SDE sampling. Providing metrics to substantiate the diversity claims would strengthen the paper significantly."}, "questions": {"value": "1.  **On SDE Performance in Few-Step Sampling (Figure 2):**\n    Could the authors elaborate on the poor performance of SDE sampling in the few-step regime, as shown in Figure 2? Typically, reducing sampling steps in diffusion models results in blurriness or structural artifacts. However, the samples shown (e.g., in Figure 9, which seems related) exhibit high-frequency residual noise, suggesting the denoising process is incomplete rather than just imprecise. This seems counter-intuitive. [1] also analyzed the poor image quality produced by time-reversal SDE sampling and proposed an improvement that enables SDE sampling to generate high-quality images.\n\n\n2.  **Baselines for Few-Step Sampling:**\n    * For the experimental setup in Figure 2 (middle), what is the FID-vs-steps curve for standard ODE sampling? This baseline is crucial for contextualizing the performance of GLASS.\n    * How does GLASS compare against other state-of-the-art few-step solvers, such as DPM-Solver++ or UniPC? These methods are specifically designed for high-fidelity generation in the low-NFE regime, and a comparison of FID scores would be highly informative.\n\n3.  **Best-of-N Sampling (Table 1):**\n    The results for \"Best-of-N\" sampling using GLASS are not presented in Table 1. Given that Best-of-N is a common technique in reward alignment, could the authors provide the performance metrics for a Best-of-N(GLASS) approach?\n\n4.  **Applicability to Online RL Methods:**\n    Could the proposed GLASS flow be integrated into online reinforcement learning algorithms for flow matching models, such as DDPO[2] or Flow-GRPO[3]? A discussion on its potential applicability, or any inherent limitations for these types of methods, would add significant value to the paper.\n\n[1] Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching\n\n[2] Training Diffusion Models with Reinforcement Learning\n\n[3] Flow-GRPO: Training Flow Matching Models via Online RL"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oKs4mv5TPh", "forum": "vH7OAPZ2dR", "replyto": "vH7OAPZ2dR", "signatures": ["ICLR.cc/2026/Conference/Submission22705/Reviewer_JMLt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22705/Reviewer_JMLt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998310482, "cdate": 1761998310482, "tmdate": 1762942344648, "mdate": 1762942344648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Sorry for my late review!\n\nThis paper addresses the inefficiency of inference-time reward adaptation for flow matching and diffusion models, which currently require sampling stochastic Markov transitions \\(p_{t'|t}(x_{t'} \\mid x_t)\\) via SDEs even though ODE sampling is far more efficient but deterministic, thereby inducing a trade-off between stochastic exploration and runtime. The authors propose Gaussian Latent Sufficient Statistic (GLASS) Flows, which construct an “inner” flow matching model that realizes a rich family of Gaussian Markov transitions (including DDPM transitions as a special case) as ODE trajectories in an auxiliary time \\(s\\), enabling ODE-based sampling of stochastic transitions directly from a pre-trained model. Technically, the method treats \\((x_t, x_{t'})\\) as two noisy measurements of a latent variable \\(z\\), compresses them into a sufficient statistic \\(S(x_t, x_{t'})\\), and shows that the standard marginal denoiser \\(D_t\\) of the base model can be reused—with an appropriate input and time reparameterization—to define a GLASS denoiser and the corresponding conditional velocity field \\(u_s(x_s \\mid x_t; t)\\) without any retraining. The paper proves that integrating this inner ODE from \\(s=0\\) to \\(s=1\\) samples from the desired GLASS transition in the continuous-time limit, analyzes discretization error, and argues that GLASS Flows inherit the computational efficiency and stability of ODE samplers while providing controlled stochastic evolution comparable to SDEs. Empirically, the authors first show on base text-to-image generation with SiT and FLUX that GLASS-based transitions match ODE sampling in FID and alignment metrics and substantially outperform DDPM-style SDE sampling at matched compute, thus eliminating the efficiency–stochasticity trade-off at the base model level. They then plug GLASS Flows into Sequential Monte Carlo Feynman–Kac steering for inference-time reward adaptation on FLUX, across four reward models (CLIP, PickScore, HPSv2, ImageReward) and benchmarks such as GenEval and PartiPrompts, demonstrating that FKS with standard SDE transitions fails to beat a simple Best-of-\\(N\\) ODE baseline whereas FKS–GLASS consistently improves both the target reward and GenEval scores without extra cost. Finally, combining GLASS-based FKS with gradient-based reward guidance further boosts reward scores and alignment metrics, showing that GLASS Flows serve as a simple, theoretically grounded, and practically effective drop-in replacement for SDE sampling that enables more powerful and efficient inference-time reward alignment of large text-to-image flow and diffusion models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a method for inference-time reward adaptation by reframing stochastic Markov transitions as an inner flow matching problem and leveraging a Gaussian latent sufficient statistic to re-use existing denoisers, yielding a unified ODE-based mechanism that subsumes DDPM-style transitions instead of proposing yet another bespoke reward adaptation algorithm.\n- The technical development is careful and well-grounded, deriving GLASS transitions from a joint Gaussian model, justifying the sufficient-statistic construction, proving that the induced velocity field satisfies the continuity equation, and supporting the method with extensive experiments on both base generation and multiple reward-adaptation setups, including compute-matched comparisons against strong ODE/SDE baselines.\n- The exposition is generally clear and well structured: the ODE–SDE trade-off is crisply articulated, the roles of marginal vs. conditional flows are distinguished, the GLASS denoiser is given an intuitive “two noisy measurements” interpretation, and the paper repeatedly connects theory to practical design choices (e.g., how to plug GLASS into FKS and reward guidance)."}, "weaknesses": {"value": "The core technical step in GLASS Flows—the use of a linear sufficient statistic over multiple Gaussian observations to recover the original denoiser at a reparameterized time—appears mathematically very close to the training-equivalence result in TADA (arXiv:2506.21757), in particular Proposition 3.1. In GLASS, Proposition 2 shows that the GLASS denoiser (D_{\\mu,\\Sigma}(x_t, x_{t'})) can be written as the original denoiser evaluated at a suitably reweighted statistic and an effective time determined by (\\mu^\\top \\Sigma^{-1}\\mu); TADA’s Proposition 3.1 establishes an essentially identical form for an augmented Gaussian state, again via a reweighted linear combination and an effective SNR. Since the underlying Gaussian-conjugacy/sufficient-statistic argument is the same, it would be helpful if the authors could explicitly cite TADA and briefly discuss how their Proposition 2 relates to Proposition 3.1 there, while emphasizing that GLASS applies this mechanism in a different context (Markov transitions and reward alignment).\n\n\n1) In Equation (1), the transition is defined as $p_{t'|t}(x_{t'}|x_t)=\\mathbb{P}[X_{t'}=x_t|X_t=x_t]$. The left-hand variable in the probability should be $x_{t'}$ rather than $x_t$.\n\n2) In Algorithm 1 (left panel), the function `D(x,t)` returns:\n  - `(1 / (dot_alpha_t * sigma_t - alpha_t * dot_sigma_t)) * (sigma_t * u - dot_sigma_t)`\n- The factor of $x$ multiplying $\\dot{\\sigma}_t$ seems to be missing. The standard denoiser identity is:\n  - $D_t(x)=\\frac{1}{\\dot{\\alpha}_t\\sigma_t-\\alpha_t\\dot{\\sigma}_t}\\left(\\sigma_t u_t(x)-\\dot{\\sigma}_t x\\right)$.\n\n3) In the GLASS denoiser, $S(\\mathbf{x})=\\frac{\\mu^\\top\\Sigma^{-1}}{\\mu^\\top\\Sigma^{-1}\\mu}[x_t,\\bar x_s]^\\top$. With the natural (CondOT) choice $\\bar\\alpha_0=0$, we have $\\mu=(\\alpha_t,\\,\\bar\\alpha_s+\\bar\\gamma\\alpha_t)^\\top$. When $t=0$ and $s=0$, both $\\alpha_t=0$ and $\\bar\\alpha_s=0$, resulting in $\\mu=\\mathbf{0}$ and potential division by zero in both $S(\\x)$ and $(\\mu^\\top\\Sigma^{-1}\\mu)^{-1}$. This edge case arises in the first Euler call at $s=0$ in Algorithm 1 (right panel), and should be handled explicitly in the implementation. The correct limiting behavior at $s=0$ is that $\\bar X_0$ carries no additional information about $Z$ beyond $X_t$ (since $\\bar X_0=\\bar\\gamma X_t+\\bar\\sigma_0\\epsilon$ does not depend on $Z$ given $X_t$). Thus:\n  - $\\lim_{s\\to 0} D_{\\mu(s),\\Sigma(s)}(x_t,\\bar x_s) = D_t(x_t)$.\n\n4) Since $p_{t'|t}$ is constructed from a joint $p_{t,t'}$ with correct marginals $p_t$ and $p_{t'}$, for any fixed pair $(t,t')$:\n  - $\\int p_{t'|t}(x'|x) p_t(x)\\,dx = p_{t'}(x')$.\n- Therefore, under exact integration of the inner ODE, composing GLASS transitions across a grid $0=t_0<\\cdots<t_K=1$ should produce $X_{t_k}\\sim p_{t_k}$ at every step (and hence $X_1\\sim p_1$), for any $\\rho\\in[-1,1]$. This is an important property: GLASS sampling preserves correct marginals even when $\\rho$ differs from the DDPM value (though the multi-time joint may differ from that of a particular SDE/ODE). Since this property is central to GLASS as a sampling scheme, the paper would benefit from making it explicit with a brief proof in the main text or appendix.\n\n5) In Appendix A.1, the GLASS guidance is formulated as:\n  - $u_s^r(\\bar x_s|x_t,t)=u_s(\\bar x_s|x_t,t)+\\beta_{t^\\star}\\frac{\\sigma_{t^\\star}^2}{\\alpha_{t^\\star}}w_2(s)\\nabla_y r(D_{t^\\star}(y))\\big|_{y=\\alpha_{t^\\star}S(\\x)}$.\n- This follows from replacing $D_{\\mu,\\Sigma}$ with $D_{\\mu,\\Sigma}^r=D_{t^\\star}^r(\\alpha_{t^\\star}S(\\x))$ in the $w_2(s)D$ term, treating guidance as a \"denoiser-level\" correction. However, readers familiar with $u_t^r=u_t+c_t\\nabla V_t(x)$ in the standard ODE domain may find this formulation unclear. A brief derivation showing that GLASS guidance is the \"denoiser-based\" guidance analog of $D_t^r=D_t + \\frac{\\sigma_t^2}{\\alpha_t}\\nabla V_t$ transplanted into the $w_2(s)D$ structure would clarify the construction.\n\n\n6) For $\\rho=\\frac{\\alpha_t\\sigma_{t'}}{\\sigma_t\\alpha_{t'}}$, the paper claims that $p_{t'|t}^{\\text{DDPM}}=p_{t'|t}^{\\text{GLASS}}$. The proof in Appendix A.2 is sound, computing the conditional of the forward noising process (OU skeleton) and matching covariances. One minor note: the paper should explicitly state that $|\\rho|\\le 1$ holds in this case, since $\\alpha_t/\\alpha_{t'}<1$ and $\\sigma_{t'}/\\sigma_t<1$ for $t<t'$.\n\n\n\n\n\n7) Each GLASS inner step requires one neural network call to $u$ (then $D$ via the reparameterization) and a few $O(1)$ operations for the 2×2 inverse, $t^\\star$ lookup/inversion of $g$, and forming $S(\\x)$. This is comparable to the SDE per-step cost (one NN call), supporting the claim that GLASS can be swapped into SMC at essentially zero additional cost.\n\n8) Computing $D_t$ from $u_t$ requires $\\dot\\alpha_t,\\dot\\sigma_t$. Many deployed models expose continuous-time schedulers, but some libraries wrap discrete-time models and approximate derivatives. The paper should mention the requirement that $\\alpha_t,\\sigma_t$ be differentiable and clarify how discrete-time models are handled in practice.\n\n9) GLASS transitions achieve stochasticity by sampling the inner initial condition $\\bar X_0\\sim \\mathcal{N}(\\bar\\gamma x_t,\\bar\\sigma_0^2 I)$, while the subsequent evolution follows a deterministic ODE. Clarifying this distinction would help readers who might associate \"stochastic transitions\" exclusively with SDEs.\n\n[1] Chen, T., Zheng, H., Berthelot, D., Gu, J., Susskind, J., & Zhai, S. (2025). TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics. arXiv preprint arXiv:2506.21757."}, "questions": {"value": "The weight function $w_1(s)=\\partial_s\\bar\\sigma_s/\\bar\\sigma_s$ can become numerically unstable if $\\bar\\sigma_s$ approaches zero. In your setup, $\\bar\\sigma_1^2=\\sigma_{t'}^2(1-\\rho^2)$, which remains positive unless $\\rho=\\pm 1$. Since the method allows $\\rho\\in[-1,1]$, the paper should warn users that $\\rho=\\pm 1$ leads to degenerate cases where $\\bar\\sigma_1=0$ and $w_1$ becomes large near $s=1$, which could be numerically problematic with Euler integration. In practice, should users avoid $\\rho$ values too close to $\\pm 1$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WbhBttNr1j", "forum": "vH7OAPZ2dR", "replyto": "vH7OAPZ2dR", "signatures": ["ICLR.cc/2026/Conference/Submission22705/Reviewer_dnmw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22705/Reviewer_dnmw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22705/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763092120008, "cdate": 1763092120008, "tmdate": 1763092120008, "mdate": 1763092120008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}