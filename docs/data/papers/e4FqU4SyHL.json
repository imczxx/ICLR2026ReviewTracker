{"id": "e4FqU4SyHL", "number": 17123, "cdate": 1758272493140, "mdate": 1759897195257, "content": {"title": "Synthesizing Multimodal Verifiable Game Data to Boost VLMs' General Reasoning", "abstract": "Vision-language reinforcement learning (RL) has primarily focused on narrow domains (e.g. geometry or chart reasoning). This leaves broader training scenarios and resources underexplored, limiting the exploration and learning of Vision Language Models (VLMs) through RL. We find video games inherently provide rich visual elements and mechanics that are easy to verify. To fully use the multimodal and verifiable reward in video games, we propose Game-RL, constructing diverse game tasks for RL training to boost VLMs general reasoning ability. To obtain training data, we propose Code2Logic, a novel approach that adapts game code to synthesize game reasoning task data, thus obtaining the GameQA dataset of 30 games and 158 tasks with controllable difficulty gradation. Unexpectedly, training solely on GameQA would help VLMs obtain better out of domain generalization, demonstrating the value of Game-RL for enhancing VLMs general reasoning. Furthermore, this suggets that RL can lead to generalizable improvements in VLMs' reasoning abilities, and notably, video games may serve as valuable scenarios and resources to bring this generalization.", "tldr": "", "keywords": ["Vision Language Model", "Reasoning", "Data Synthesis", "Game Playing", "Visual Question Answering", "Data Sets or Data Repositories", "Benchmarks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edbf0ae50b875263bd0418390c5b1e430264609d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the lack of large-scale, high-quality data for RL training in video game contexts, this work:\n(1) Proposes *Code2Logic*, a pipeline that leverages LLMs to synthesize reasoning data based on game code; and\n(2) Constructs *GameQA* a large dataset of diverse tasks and 140K VQA examples. After post-training MLLMs on this dataset, the authors report improved reasoning performance across multiple general-domain benchmarks, achieving better out-of-domain generalization compared to other comparable datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "•\tData Quality: A wide range of video games are selected, ensuring diversity and strong generalization potential in the training data. The synthesis process leverages LLMs across multiple stages and includes manual validation, which guarantees both the efficiency and reliability of the data generation pipeline.\n•\tClarity: The approach is clearly presented, supported by a well-illustrated pipeline diagram and numerous examples in the appendix. The experiments are reported in a detailed and organized manner. \n•\tSignificance: The experiments cover a diverse set of models and benchmarks, outperforming most comparable datasets. Comprehensive ablation studies, such as scaling effect analyses, further strengthen the empirical evidence. Together, these results provide strong support for the overall effectiveness of GameQA."}, "weaknesses": {"value": "•\tGame-RL can to construct large-scale datasets for diverse games at relatively low cost. Nevertheless, the idea of synthesizing game-based benchmarks to enhance MLLMs’ reasoning ability via RL is a well-established and commonly explored direction. There already exist many high-quality, large-scale benchmarks—such as VisualSphinx (https://arxiv.org/abs/2505.23977) VGRPBench (https://arxiv.org/abs/2503.23064) and other recent game-based reasoning datasets—that share similar objectives, and some of them also asserts that RL training on their dataset can improve MLLMs’ reasoning ability in general-domain. Therefore, the overall objective of this paper is not highly novel."}, "questions": {"value": "•\tAs mentioned in Section 2.2, the QA templates are either (1) human-designed and refined by LLMs, or (2) generated by LLMs and manually validated by humans. What is the proportion between these two approaches, and what are the respective time costs involved in each process?\n•\tThe *Code2Logic* pipeline employs LLMs in multiple stages while keeping humans in the loop. As I understand it, this process still requires considerable manual effort. Have the authors considered adopting a more agentic approach to further reduce human labor by leveraging LLM agents more extensively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DVzzyGBYcq", "forum": "e4FqU4SyHL", "replyto": "e4FqU4SyHL", "signatures": ["ICLR.cc/2026/Conference/Submission17123/Reviewer_D6PT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17123/Reviewer_D6PT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599556816, "cdate": 1761599556816, "tmdate": 1762927121190, "mdate": 1762927121190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for verifiable synthetic vision-language data generation with video games, by training VLMs on this synthetic data with GRPO, they get improvements in other benchmarks for vision language models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The verifiable synthetic data generation is a timely and important problem given the landscape of LLM/VLM trainings with RL.\n2. Using games to improve vision capabilities in a verifiable way is an interesting contribution."}, "weaknesses": {"value": "1. The games used for data generation are all grid-like games with very simple interfaces.\n2. Games are inherently sequential-decision making tasks, so it feels strange that the authors did not then test performance on any of the game benchmark they mention. This seems like a natural extension to the project so far.\n3. The performance improvements of 1 to 2% are quite small, and using a single seed might not be enough to properly test this."}, "questions": {"value": "1. How does performance change on sequential decision making tasks requiring vision? Perhaps on some of the game benchmarks you mentioned? It feels weird mentioning other game environments and then not testing performance on them.\n\n2. Would it be possible to test more seeds to get error bars? Are the improvements significant or within error range? I understand that these experiments are likely quite expensive, but at least 3 seeds on some of the trainings and tasks could be useful, especially given the tight margin of improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VOgpiNU6XN", "forum": "e4FqU4SyHL", "replyto": "e4FqU4SyHL", "signatures": ["ICLR.cc/2026/Conference/Submission17123/Reviewer_hgzh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17123/Reviewer_hgzh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839334195, "cdate": 1761839334195, "tmdate": 1762927120793, "mdate": 1762927120793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the narrow domain focus of current reinforcement learning (RL) methods for Vision-Language Models (VLMs), arguing that such limitations hinder the development of general reasoning. To this end, the authors propose Game-RL, an approach that leverages diverse and verifiable video game environments for training. To generate the necessary training data, they introduce Code2Logic, an LLM-assisted, three-stage pipeline for synthesizing game code, task templates, and ultimately, game-based VQA samples. The primary tangible contribution is the resulting GameQA dataset, a large-scale collection of 140K question-answer pairs spanning 30 distinct games and 158 tasks, categorized by cognitive skills. The authors demonstrate that training VLMs solely on GameQA via an RL algorithm (GRPO) yields consistent performance improvements across a suite of seven external, general-purpose vision-language benchmarks, thereby providing evidence for the transferability of reasoning skills learned within these game environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear and sound. The authors correctly identify a critical limitation in the current VLM training paradigm, the over-reliance on narrow, static domains like geometry or chart reasoning. The proposal to use video games as a more dynamic, verifiable, and diverse training environment for fostering general reasoning is an interesting idea. \n- The GameQA dataset could be a substantial contribution to the community. Its scale (30 games, 158 tasks, 140K samples), diversity across four distinct cognitive categories, and controllable difficulty levels make it a valuable resource for both training and benchmarking VLMs. The effort invested in its creation is evident, and it provides a tangible asset that can spur further research in this area.\n- The paper presents a comprehensive and rigorous experimental evaluation. The authors test their approach not just on a few models but on a range of popular open-source VLMs. Crucially, evaluation is not confined to their own dataset; they demonstrate generalization by testing on seven diverse external benchmarks. The inclusion of ablations on data quantity and game diversity further strengthens the quality and credibility of the empirical findings.\n- The paper is well-written, logically structured, and easy to follow. The Code2Logic pipeline is explained with a clear diagram (Figure 1), and the examples provided throughout the paper and appendix effectively illustrate the nature of the tasks in GameQA. This clarity makes the work accessible and its contributions understandable."}, "weaknesses": {"value": "1. The novelty is somewhat incremental and limited. The Code2Logic pipeline, while well-executed, is fundamentally a specific instance of LLM-based synthetic data generation. This approach is becoming increasingly common, and the paper does not sufficiently differentiate its technical contribution from existing work in program-aided or LLM-driven data synthesis. It seems to be more of an extensive engineering protocol than a novel, generalizable method. Similarly, Game-RL is an application of an existing RL algorithm (GRPO) to a new dataset. The name might imply a novel RL framework tailored for games, which is not the case. The contribution is thus more of an empirical finding (\"RL on game data generalizes\") rather than a new RL technique.\n2. There is a lack of reproducibility in data generation. The Code2Logic pipeline is described as \"LLM-assisted,\" but the degree of human intervention appears substantial, undermining its scalability and reproducibility. The appendix notes an average of 7.5 hours of human effort per game. This suggests a highly iterative and manual process of prompt engineering, code verification, and template refinement. For the work to be truly reproducible, the authors should provide a much more detailed account of this human-in-the-loop process, including the specific prompts, failure cases encountered with the LLM, and the nature of the manual corrections required.\nInsufficient Justification for the RL Setup: The design of the reinforcement learning component is not adequately justified and appears suboptimal for the stated goal of improving reasoning.\n3. The use of a sparse, binary (0/1) reward based only on the final answer is a weak signal for complex, multi-step reasoning tasks. It provides no credit for partially correct reasoning chains and fails to guide the model on how to arrive at the correct answer. The analysis field already present in the GameQA dataset seems perfectly suited for developing a more informative, process-based reward signal. The paper would be significantly stronger if it explored this or at least provided a detailed justification for why this simpler reward scheme was chosen.\n4. The use of a powerful VLM (Qwen2.5-72B) as a judge introduces a potential ceiling effect and a source of bias. The paper provides no evaluation of this judge's accuracy on the GameQA tasks. Without knowing the reliability of the reward model, it is difficult to assess the quality of the training signal the agent receives. The authors should report the judge's agreement rate with human annotations on a sample of the data.\n5. While the paper claims to boost \"general reasoning,\" the connection is supported by relatively small performance gains on the external benchmarks. Furthermore, the selection of the 30 games appears somewhat arbitrary. The claim would be more convincing if the game selection process can demonstrate diversity and wide coverage."}, "questions": {"value": "1. The paper highlights the Code2Logic pipeline as a key contribution for generating the GameQA dataset. However, Appendix E.3 notes an average of 7.5 hours of human effort was required per game. This suggests a significant reliance on manual oversight. Therefore, \na) What are the most common failure modes of the LLMs in this pipeline? What kinds of errors (e.g., logical inconsistencies in game rules, un-compilable code, nonsensical QA pairs) required the most frequent manual intervention?\nb) Given this reliance on expert human-in-the-loop validation, how do you envision this approach scaling to a much larger and more diverse set of games (e.g., hundreds or thousands) without becoming prohibitively expensive and time-consuming?\n2. Justification of the Reward Design. The RL training utilizes a sparse, binary reward based on the final answer's correctness. However, your GameQA dataset commendably contains detailed, step-by-step reasoning chains in the \"analysis\" field for each question. This rich information seems ideal for process-based supervision or a denser reward signal that could more effectively guide the model toward correct reasoning patterns.\na) Did you experiment with using these ground-truth reasoning steps to shape the reward signal (e.g., via sequence matching or a separate reward model trained on the analysis text)?\nb) If not, could you elaborate on the rationale for choosing a sparse, outcome-based reward? Especially for complex reasoning, why is this preferable to process supervision, which is often considered more effective for teaching reasoning?\n3. Reliability of the LLM-as-a-Judge. The entire Game-RL training loop hinges on the accuracy of the Qwen2.5-72B model acting as the judge for reward allocation. The validity of your experimental results is therefore contingent on the reliability of this judge.\nCould you please provide an evaluation of the judge model's performance on the GameQA test set? Specifically, what are its accuracy, precision, and recall when compared against the ground-truth answers? An agreement score against human evaluators on a representative sample would be particularly insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4BQTH6Iwh6", "forum": "e4FqU4SyHL", "replyto": "e4FqU4SyHL", "signatures": ["ICLR.cc/2026/Conference/Submission17123/Reviewer_amrf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17123/Reviewer_amrf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877019996, "cdate": 1761877019996, "tmdate": 1762927120415, "mdate": 1762927120415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of improving the general reasoning capabilities of VLMs, arguing that current RL training is overly focused on narrow domains like geometry and charts. The authors propose Game-RL, a framework that leverages video games as a rich, verifiable, and scalable source of training tasks. They introduce Code2Logic, a novel pipeline that uses LLMs to synthesize a large-scale dataset, GameQA, by first generating game code and then using that code to create verifiable VQA tasks. The central finding is that training VLMs on GameQA using GRPO with a simple outcome-based reward signal unexpectedly improves performance not only on unseen games but also across a suite of 7 diverse, out-of-domain general reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is clear writing and easy to follow.\n\n* Dataset contribution: GameQA spans 30 games / 158 tasks with explicit difficulty control and verifiable answers.\n\n* GRPO on GameQA yields consistent improvements on diverse general benchmarks"}, "weaknesses": {"value": "* The Code2Logic pipeline is presented as highly scalable, but Section 2.4 and Appendix F.4 reveal a significant reliance on manual verification at every step (code, data engine, and augmented samples). Furthermore, the data augmentation relies on paraphrasing from InternVL2.5-78B, and data quality checks use commercial LLMs. This \"human-in-the-loop\" and \"proprietary-LLM-in-the-loop\" requirement makes the process less automated and scalable than implied.\n\n* The work only generates massive amounts of game QA data; I don't believe this is the primary task that a 'game agentic' model should be excelling at. Instead, the model should be interacting directly within the code-driven game to generate trajectories for SFT or RFT, or to perform RLVR."}, "questions": {"value": "* In Appendix B.3, you note that training on a subset of 4 games led to better generalization than training on 10 games. You speculate this is due to \"random factors\" or a \"well-chosen set.\" Could you elaborate on this?\n\n* The Code2Logic pipeline works well for the 30 games presented, which are largely deterministic, static, or turn-based. How do you see this framework extending to more complex, real-time games with non-deterministic physics (e.g., Angry Birds) or continuous action spaces (e.g., a racing game)? Does the reliance on LLMs to generate the game code fundamentally limit the complexity of the games that can be synthesized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6s2GH0f1r", "forum": "e4FqU4SyHL", "replyto": "e4FqU4SyHL", "signatures": ["ICLR.cc/2026/Conference/Submission17123/Reviewer_gdbT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17123/Reviewer_gdbT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919418382, "cdate": 1761919418382, "tmdate": 1762927119980, "mdate": 1762927119980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Game-RL, a framework for vision-language reinforcement learning that uses video game environments as multimodal, verifiable training data. Using the proposed Code2Logic pipeline, game code is transformed into reasoning-oriented visual question–answering (VQA) tasks, producing the GameQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Scalable data-generation pipeline: Code2Logic programmatically maps game code to reasoning logic and auto-generates verifiable multimodal QA data.\n- Demonstrates that purely synthetic, self-verifiable environments can modestly improve general VLM reasoning—important for RL reproducibility.\n- Diverse benchmark coverage (3D perception, pattern matching, planning, reasoning).\n- Clear, reproducible methodology; good visualizations and qualitative examples."}, "weaknesses": {"value": "- Small gains on external benchmarks are statistically and practically modest; no significance tests or efficiency comparisons.\n- Lack of ablation isolating contributions of Code2Logic data vs RL itself (no SFT vs RL comparison on the same data).\n- Evaluator bias: rewards rely on QwQ-32B, potentially aligning to its own style and inflating self-consistency.\n- Unclear verification metrics: “verifiable” is claimed, but no automated correctness guarantees are quantified."}, "questions": {"value": "- What proportion of games required manual correction or external code reuse?\n- What is the importance of task templates? \n- What happens if you just SFT on GameQA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtNupJJLMX", "forum": "e4FqU4SyHL", "replyto": "e4FqU4SyHL", "signatures": ["ICLR.cc/2026/Conference/Submission17123/Reviewer_grko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17123/Reviewer_grko"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998362174, "cdate": 1761998362174, "tmdate": 1762927119408, "mdate": 1762927119408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}