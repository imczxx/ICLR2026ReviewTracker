{"id": "l8YvuTQ0FA", "number": 19186, "cdate": 1758294232295, "mdate": 1759897053359, "content": {"title": "Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?", "abstract": "Exogenous Markov Decision Processes (Exo-MDPs) capture sequential decision-making with independent exogenous dynamics, arising in applications such as inventory control, energy storage, and resource management. Prior work in approximate dynamic programming demonstrates that pure exploitation can be highly effective, with convergence in certain settings but no general regret guarantees. In contrast, reinforcement learning approaches to Exo-MDPs almost exclusively rely on explicit exploration via optimism or hindsight optimization, leaving open whether exploitation alone can achieve provable guarantees. We resolve this question by proving the first near-optimal regret bounds for pure exploitation strategies under linear function approximation. Our key technical contribution is a novel analysis based on counterfactual trajectories and post-decision states, which yields regret bounds polynomial in the endogenous feature dimension, exogenous state space, and horizon, and importantly independent of the endogenous state and action cardinalities. Experiments on synthetic and resource management benchmarks confirm that pure exploitation surpasses exploration-based methods.", "tldr": "", "keywords": ["Reinforcement Learning", "Exogenous Markov Decision Processes", "Regret Analysis", "Linear Function Approximation", "Exploration-free"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f18702104bc036959b833f74ce9223f85a8fe32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper shows that pure exploration—model estimation with greedy action execution—is sufficient for rate-optimal regret in exogenous MDPs when the exogenous process is the only unknown. The paper covers both the tabular MDP case and the case of MDPs with linear function approximation. In both cases, they show a regret of order $O(\\sqrt{K})$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow. Overall, the authors do a good job of providing intuition about the results and explaining why the algorithm works.\n- I am not up to date with the literature on exogenous MDPs, but from what I can gather, the results seem novel and interesting (I did not have time to check all the proofs)."}, "weaknesses": {"value": "- The main weakness of the paper is that the setting is quite specific. In particular, the assumption that the only unknown component is the exogenous process seems quite strong to me. The authors justify this choice as being common in the literature, but I do not have enough knowledge of the literature to judge this claim. However, I can understand that in certain operational research applications, this assumption might be reasonable."}, "questions": {"value": "- Could you provide some intuition as to why pure exploration is sufficient to achieve rate-optimal regret in this setting? Is it because the endogenous process is fully known, and the exogenous process is independent of the actions, so that it can be learned at a \"fast\" rate (approximately $1/\\sqrt{K}$) without impacting the regret too much?\n- In the linear function approximation setting, could you comment on how the regret depends on the problem parameters (such as d, anchor points, etc.)? Is this dependence optimal, or could it be improved?\n- The assumption that anchor points are known seems strong. Could you comment on this assumption? Is it possible to relax it?\n\nOverall, I think the paper provides interesting and novel results on exogenous MDPs, even though the setting is quite specific."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yUiZ8XP3QK", "forum": "l8YvuTQ0FA", "replyto": "l8YvuTQ0FA", "signatures": ["ICLR.cc/2026/Conference/Submission19186/Reviewer_4oif"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19186/Reviewer_4oif"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761209538153, "cdate": 1761209538153, "tmdate": 1762931187418, "mdate": 1762931187418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies exogenous MDPs where the noise in the transitions is independent on the action taken by the learner.\nUnder this setting the authors show that no exploration mechanism is needed in Bandits Problem and Tabular MDPs where the transition dynamic is known.\nMoreover, the result is extended to the linear case."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I think that the question tackled here is interesting, as several problems of practical interest are usually solved by algorithms that do not use any exploration mechanism."}, "weaknesses": {"value": "I think that the assumption about the existence of the anchor set is not strong; however, I do think that it is a strong assumption that the learning algorithm knows this set.\nWhy is it not possible to guarantee the invertibility of $\\Sigma$ by defining $\\Sigma = \\sum^N_{n=1} \\phi_n \\phi_n^T + \\beta I$ where $\\beta$ is a small scalar and $I$ is the identity matrix ? In this way, switching from Least Square to Ridge regression it should be possible to avoid the assumption.\n\nWhy do you use the anchor points to define the matrix $\\Sigma$ instead of using the data collected during the policy rollout phase? It seems weird to me that the rollout phase only uses the encountered exogenous states to estimate the exogenous to exogenous transition matrix. In contrast, the encountered states and actions are discarded."}, "questions": {"value": "1) How is the initial exogenous state chosen? Is it sampled from a fixed distribution, or can it be chosen adversarially?\n\n2) If the transition dynamics (the $f$) in tabular MDPs were not known, would pure exploitation still suffice?\n\n3) In the tabular guarantees, there is no dependence in the number of endogenous states and actions ($S$ and $A$). However, the feature dimension $d$ shows up in Theorem 2? If I represent a tabular MDP by choosing the features that are one one-hot encoded vector of dimension $d = SA$, I would get a dependence on $\\sqrt{SA}$  if I apply your Theorem 2 in this setting, while Theorem 1 avoids this dependence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ndbSeEMVrI", "forum": "l8YvuTQ0FA", "replyto": "l8YvuTQ0FA", "signatures": ["ICLR.cc/2026/Conference/Submission19186/Reviewer_8gd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19186/Reviewer_8gd3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761389958787, "cdate": 1761389958787, "tmdate": 1762931186870, "mdate": 1762931186870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper makes a contribution to the reinforcement learning literature attempting to characterize when greedy policies are sufficient for provable guarantees, or when exploration is not necessary. The authors show that this is indeed the case for exogenous MDPs, where there is an exogenous Markovian component that is unaffected by the learner's actions. Their variant of LSVI achieving sublinear regret crucially hinges on an assumption that the endogenous transitions are deterministic and known, as well as three technical assumptions on an anchor set that is closed under the Bellman operator with non-negative residuals. A tabular method only requires the former."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper makes a solid contribution to the RL literature. Understanding exactly when greedy exploitation is sufficient for provable guarantees is a topic that has gotten more attention lately. \n- This tends to occur when there is sufficient environment noise, and Exo-MDPs appear to be one such case. To my knowledge, this is novel.\n- The authors tackle bandits and both tabular and linear MDPs, showing that this holds somewhat more broadly than just in an isolated case. \n- The paper is largely well written, save for a few issues on clarity."}, "weaknesses": {"value": "1. Not much intuition is provided on exactly why such a positive result is possible. At a very very high level that probably amounts to skimming over a lot of subtleties, it seems to be because the learner can decouple the exogenous transitions, of which no exploration is necessary to learn them, from the endogenous transitions, whom are deterministic and known. Assuming that one can do so, the whole problem then reduces to learning the exogenous transitions for input to learning a Q-function via the standard LSVI procedure.\n2. The assumptions are quire strong. Needing the existence of an anchor set that is known to the learner, plus additional assumptions on said set, is quite strong in the RL literature (I am reading this as its alternate name, a coreset). It is understandable that strong assumptions are necessary, but one could also (for instance) assume that the minimum eigenvalue of the design matrix is bounded under any policy. Characterizing exactly why this is needed, how it can be obtained (it is folklore that Frank-Wolfe-like procedures get you one, but it should be stated), and the relevant intuition would go a long way towards helping the unfamiliar reader and justifying these assumptions.\n3. The proofs in the appendix, especially for the linear MDP section, are quite poorly written. The authors state that they make a counterfactual analysis. It is unclear to me where this is done, or what exactly this means. This is striking -- I've seen the linear MDP proofs many, many times, but it's not clear to me where it happens."}, "questions": {"value": "1. How much of this relies on the assumption of known deterministic endogenous dynamics?\n2. Can the anchor set assumption be weakened? What could replace it?\n3. Where is the counterfactual analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fyn6kjX4FE", "forum": "l8YvuTQ0FA", "replyto": "l8YvuTQ0FA", "signatures": ["ICLR.cc/2026/Conference/Submission19186/Reviewer_4nVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19186/Reviewer_4nVr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868482454, "cdate": 1761868482454, "tmdate": 1762931186150, "mdate": 1762931186150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a pure exploration learning framework in Exo-MDPs, where the state decomposes into exogenous and endogenous components. Starting from a simple Exo-Bandit warm-up, they show that in tabular Exo-MDPs, one can estimate the exogenous transition kernel and computed the policy via dynamic programming, deriving a theoretical regret bound. For linear function approximation, they introduce an algorithm called LSVI-PE, which combines post-decision state and counterfactual trajectory analysis, and provide a corresponding theoretical analysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They present the first near-optimal regret bound for Exo-MDPs under linear function approximation and theoretically establish that it is independent of the endogenous state and action cardinalities. Furthermore, they rigorously prove that the exogenous process in EXO-MDPs evolves independently of the policy, thereby removing the need for explicit exploration. This is supported by $\\tilde{\\mathcal{O}} (\\sqrt{K})$ regret guarantees in both tabular and linear function approximation settings."}, "weaknesses": {"value": "1. The modelling assumptions are somewhat restrictive, as the theoretical results rely on the exogenous state space being discrete and on the endogenous transition and reward functions being known.\n2. The regret bound scales linearly with $|\\Xi|$ in both the tabular and linear function approximation settings, which may limit scalability. In particular, LSVI-PE can exhibit degraded performance when the anchor placement is suboptimal and $\\lambda_0$ becomes small."}, "questions": {"value": "Given the relatively small experimental scale—in terms of state/action set sizes, horizon length, and the number of episodes—and that the baseline is limited to an optimism-augmented variant, would it be feasible to scale up the experiments and include additional baselines, particularly other pure-exploitation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aaHvfKsQkY", "forum": "l8YvuTQ0FA", "replyto": "l8YvuTQ0FA", "signatures": ["ICLR.cc/2026/Conference/Submission19186/Reviewer_odvK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19186/Reviewer_odvK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991984607, "cdate": 1761991984607, "tmdate": 1762931185750, "mdate": 1762931185750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}