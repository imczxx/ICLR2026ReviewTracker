{"id": "Y3auRTgDEX", "number": 12051, "cdate": 1758205416059, "mdate": 1759897537199, "content": {"title": "CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition", "abstract": "Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios.  This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers.  Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech.  We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.", "tldr": "", "keywords": ["speech corpus ; code-switching; speech recognition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da4fbe46abf9a2499b42d90a0236de107e304b83.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a large, high-quality dataset of real conversational Mandarin-English code-switching, addressing the lack of authentic resources in this area. It provides detailed speaker and topic diversity, and demonstrates that training ASR models with this data significantly improves code-switching recognition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work presents a large-scale, spontaneous Mandarin-English code-switching speech dataset, filling a notable gap in the field. The paper invested substantial effort in collecting and processing real conversational data, which is of great value to the research community. The dataset addresses the scarcity of authentic code-switching conversational data and will facilitate further advancements in ZH-EN CS ASR.\n2. The paper provides detailed analyses of the dataset’s composition, including speaker demographics (gender, age, region), topic distribution, and utterance statistics. Such thorough documentation enhances the dataset’s utility and transparency for future research.\n3. Experiments on SenseVoice-small and other state-of-the-art models show that fine-tuning with this dataset can effectively improve code-switching ASR performance. The results validate the practical value of the resource."}, "weaknesses": {"value": "1. **Insufficient Detail on Code-Switching Text Construction** The one of most critical aspect of a code-switching dataset is how the code-switched utterances are generated. The paper does not provide sufficient detail on whether the code-switching segments were produced spontaneously by speakers or guided by prompts. If speakers were given too much freedom, there is a risk that some utterances may be unnatural or overly contrived. For example, in Table A.2, some “sports” examples contain long English segments that are rarely observed in natural code-switching. The authors should clarify the construction process and discuss its impact on data authenticity.\n2. **Limited Duration of Mixed (Code-Switching) Data** According to Figure 3, the mixed-language (code-switching) portion comprises only about 30 hours, less than one-third of the total dataset.\n3. **Lack of Comparison with Synthetic or Segmented Data Approaches** Previous studies have shown that training with high-quality TTS-synthesized code-switching data or segmented real utterances can substantially improve code-switching ASR performance. However, this paper does not include experiments comparing the effectiveness of real conversational data versus synthetic or segmented data. Such comparisons would help highlight the unique advantages of the presented dataset."}, "questions": {"value": "This work represents a major effort and a valuable resource for the community. I sincerely appreciate the authors’ dedication and the significant investment of time and funding. However, I feel that it falls short of the standards expected for a technical paper at a research conference. Beyond the resource construction and baseline experiments, the paper lacks deeper research insights and thoughtful analysis that would help the community better understand the challenges and opportunities in code-switching speech recognition. \n\n1. While the paper demonstrates the effectiveness of fine-tuning on SenseVoice-small, the test and training sets are from the same source. It would be valuable to know whether fine-tuning on CS-Dialogue leads to performance degradation on other open ASR benchmarks, indicating potential overfitting or lack of generalization.\n\n2. Given the proven benefits of high-quality TTS-synthesized code-switching data, could the authors add experiments comparing models trained on real versus synthetic data to better illustrate the value of authentic conversational recordings?\n\n3. The dataset contains a substantial amount of pure Mandarin and pure English data, which are already well-represented in existing corpora. What is the motivation for including these segments, and could their presence bias the test set, potentially inflating the observed improvements from fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0yyV3EO8og", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Reviewer_M8ep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Reviewer_M8ep"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459830729, "cdate": 1761459830729, "tmdate": 1762923027511, "mdate": 1762923027511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Formal Complaint Concerning Reviewer “csnc”: Misconduct and Conflict of Interest"}, "comment": {"value": "We sincerely appreciate the reviewers’ time and effort in evaluating our submission. However, we must raise **serious concerns regarding the integrity of one review** and the associated **ethics flag**, which appear to violate the principles of anonymity, impartiality, and factual accuracy in the review process.\n\n---\n\n#### **1. Strong similarity with a prior ACL review and potential conflict of interest**\n\nThe *“Ethical Concerns”* section in this review is **nearly identical** in structure, phrasing, and reasoning to the ethical comments our team received in a prior ACL ARR submission of an earlier version of this work.\nIn particular, the highly distinctive and uncommon critique about *“participant comfort and potential pressure caused by staged language switching”* provides **strong evidence that the two reviews originate from the same individual**.\n\nAfter review by the ACL Ethics Committee, our paper was found to have **no ethical issues**, and the reviewer’s concern was overruled.\n\nThat prior review was **officially invalidated by the ACL chairs** after our complaint of reviewer misconduct, in which the reviewer contacted our team privately and hinted at an unethical proposal. \n\nThis high degree of textual and conceptual overlap strongly suggests that the same individual may be reviewing our paper again under a different identity. Such overlap, if verified, would constitute a serious breach of the double-blind policy and a potential conflict of interest.\n\nMoreover, this reviewer assigned a unreasonably low overall score (2), whereas other reviewers provided balanced and constructive assessments (8, 8, and 4).\nThe combination of the extreme score discrepancy, content overlap, and the history of prior misconduct raises serious concerns about possible bias and retaliation.\n\n---\n\n#### **2. Improper Ethics Flag before rebuttal**\n\nBefore the rebuttal phase began, the reviewer selected\n**“Flag for Ethics Review: Yes – Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers).”**\nAt that point, **no communication had occurred** between authors and reviewers, making it impossible for any “exchange” or unprofessional behavior to have taken place.\n\nFurthermore, the reviewer marked confidence = 5 (absolute certainty) while the flagged statement is factually incorrect, which raises additional concerns regarding the reviewer’s intent and objectivity.\n\n---\n\n#### **3. Request for further action**\n\nWe respectfully request that the Area Chair review this situation carefully and, if appropriate, escalate it to the Ethics Committee or Program Chairs for investigation.\nWe are prepared to provide supporting evidence upon request, including:\n\n* The full text of the prior ACL ARR review and our formal complaint,\n* The corresponding *“Ethical Concerns”* sections from both reviews, and\n* The prior correspondence related to the confirmed misconduct case.\n\nOur intention is **not to challenge the review system**, but to **protect its integrity** and ensure fairness for all participants.\n\nThank you very much for your attention and for upholding the standards of ethical and transparent peer review."}}, "id": "6ark1UUHaX", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763021889443, "cdate": 1763021889443, "tmdate": 1763027887156, "mdate": 1763027887156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Formal Complaint Concerning Reviewer “csnc”: Misconduct and Conflict of Interest"}, "comment": {"value": "We sincerely appreciate the reviewers’ time and effort in evaluating our submission. However, we must raise **serious concerns regarding the integrity of one review** and the associated **ethics flag**, which appear to violate the principles of anonymity, impartiality, and factual accuracy in the review process.\n\n---\n\n#### **1. Strong similarity with a prior ACL review and potential conflict of interest**\n\nThe *“Ethical Concerns”* section in this review is **nearly identical** in structure, phrasing, and reasoning to the ethical comments our team received in a prior ACL ARR submission of an earlier version of this work.\nIn particular, the highly distinctive and uncommon critique about *“participant comfort and potential pressure caused by staged language switching”* provides **strong evidence that the two reviews originate from the same individual**.\nAfter review by the ACL Ethics Committee, our paper was found to have **no ethical issues**, and the reviewer’s concern was overruled.\n\nThat prior review was **officially invalidated by the ACL chairs** after our complaint of reviewer misconduct, in which the reviewer **contacted our team privately and hinted at an unethical proposal**. \n\nWe also identified an **additional strong indicator of reviewer identity overlap**. The comment:\n\n> *“4. Breadth of CS dataset review: Coverage beyond Mandarin–English has improved but remains brief and not fully systematic.”*\n\ncontains the phrasing *“has improved but remains…”*, which presupposes familiarity with a previous version of our work. Such continuity of critique would not be possible under a proper double-blind review process, further strengthening the evidence that the same individual may be involved in both reviews.\n\nTaken together, these observations strongly suggest that the same individual may be reviewing our work again under a different identity. If verified, this would constitute a serious breach of the double-blind review policy and a significant conflict of interest.\n\nMoreover, this reviewer assigned a unreasonably low overall score (2), whereas other reviewers provided balanced and constructive assessments (8, 8, and 4).\nThe combination of the extreme score discrepancy, content overlap, and the history of prior misconduct raises serious concerns about possible bias and retaliation.\n\n---\n\n#### **2. Improper Ethics Flag before rebuttal**\n\nBefore the rebuttal phase began, the reviewer selected\n**“Flag for Ethics Review: Yes – Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers).”**\nAt that point, **no communication had occurred** between authors and reviewers, making it impossible for any “exchange” or unprofessional behavior to have taken place.\n\nFurthermore, the reviewer marked confidence = 5 (absolute certainty) while the flagged statement is factually incorrect, which raises additional concerns regarding the reviewer’s intent and objectivity.\n\n---\n\n#### **3. Request for further action**\n\nWe respectfully request that the Area Chair review this situation carefully and, if appropriate, escalate it to the Ethics Committee or Program Chairs for investigation.\nWe are prepared to provide supporting evidence upon request, including:\n\n* The full text of the prior ACL ARR review and our formal complaint,\n* The corresponding *“Ethical Concerns”* sections from both reviews, and\n* The prior correspondence related to the confirmed misconduct case.\n\nOur intention is **not to challenge the review system**, but to **protect its integrity** and ensure fairness for all participants.\n\nThank you very much for your attention and for upholding the standards of ethical and transparent peer review."}}, "id": "6ark1UUHaX", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763021889443, "cdate": 1763021889443, "tmdate": 1763113141539, "mdate": 1763113141539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Formal Complaint Concerning Reviewer “csnc”: Misconduct and Conflict of Interest"}, "comment": {"value": "We sincerely appreciate the reviewers’ time and effort in evaluating our submission. However, we must raise **serious concerns regarding the integrity of one review** and the associated **ethics flag**, which appear to violate the principles of anonymity, impartiality, and factual accuracy in the review process.\n\n---\n\n### **1. Strong similarity with a prior ACL review and potential conflict of interest**\n\nThe *“Ethical Concerns”* section in this review is **nearly identical** in structure, phrasing, and reasoning to the ethical comments our team received in a prior ACL ARR submission of an earlier version of this work.\nIn particular, the highly distinctive and uncommon critique about *“participant comfort and potential pressure caused by staged language switching”* provides **strong evidence that the two reviews originate from the same individual**.\nAfter review by the ACL Ethics Committee, our paper was found to have **no ethical issues**, and the reviewer’s concern was overruled.\n\nThat prior review was **officially invalidated by the ACL chairs** after our complaint of reviewer misconduct, in which the reviewer **contacted our team privately and hinted at an unethical proposal**. \n\nWe also identified an **additional strong indicator of reviewer identity overlap**. The comment:\n\n> *“4. Breadth of CS dataset review: Coverage beyond Mandarin–English has improved but remains brief and not fully systematic.”*\n\ncontains the phrasing *“has improved but remains…”*, which presupposes familiarity with a previous version of our work. Such continuity of critique would not be possible under a proper double-blind review process, further strengthening the evidence that the same individual may be involved in both reviews.\n\nTaken together, these observations strongly suggest that the same individual may be reviewing our work again under a different identity. If verified, this would constitute a serious breach of the double-blind review policy and a significant conflict of interest.\n\n---\n### **2. Inconsistent Scoring and Indicators of Potential Reviewer Bias**\n\nAlthough the reviewer explicitly states that “this study represents significant effort and provides a valuable resource for the community,” they nevertheless assign Contribution = 1 (poor), Soundness = 1 (poor), and an overall score of 2, which is internally inconsistent with their own summary and with the actual contribution of a 104-hour spontaneous Mandarin–English code-switching dialogue dataset, the largest of its kind and one that fills a clear gap in existing corpora.\nIn contrast, the other reviewers provided balanced and constructive assessments (8, 8, and 4). \n\nThe combination of this extremely low and contradictory scoring, the content overlap with prior reviews, and the history of misconduct raises serious concerns about possible bias or retaliation.\n\n\n---\n\n### **3. Improper Ethics Flag before rebuttal**\n\nBefore the rebuttal phase began, the reviewer selected\n**“Flag for Ethics Review: Yes – Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers).”**\nAt that point, **no communication had occurred** between authors and reviewers, making it impossible for any “exchange” or unprofessional behavior to have taken place.\n\nFurthermore, the reviewer marked confidence = 5 (absolute certainty) while the flagged statement is factually incorrect, which raises additional concerns regarding the reviewer’s intent and objectivity.\n\n---\n\n### **4. Request for further action**\n\nWe respectfully request that the Area Chair review this situation carefully and, if appropriate, escalate it to the Ethics Committee or Program Chairs for investigation.\nWe are prepared to provide supporting evidence upon request, including:\n\n* The full text of the prior ACL ARR review and our formal complaint,\n* The corresponding *“Ethical Concerns”* sections from both reviews, and\n* The prior correspondence related to the confirmed misconduct case.\n\nOur intention is **not to challenge the review system**, but to **protect its integrity** and ensure fairness for all participants.\n\nThank you very much for your attention and for upholding the standards of ethical and transparent peer review."}}, "id": "6ark1UUHaX", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763021889443, "cdate": 1763021889443, "tmdate": 1763351187341, "mdate": 1763351187341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study introduces the creation of a speech dataset for code-switching between English and Mandarin Chinese. It also presents results using different ASR frameworks to demonstrate the quality of the dataset. Although ASR corpora for code-switching between English and Chinese have been explored for at least the past decade, this dataset outperforms its competitors in terms of size (100 hours, compared to 10-20 hours for previous datasets and up to 63 hours for SEAME). While the low ASR performance warrants further discussion, this study represents significant effort and provides a valuable resource for the community."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The CS-Dialogue dataset will be made freely available for all academic purposes.\n\n3. well-written"}, "weaknesses": {"value": "## Major Weaknesses:\n\n1. No multi-seed runs reported; variance/standard deviation or confidence intervals are missing, so result stability is unclear.\n\n2. Handling of English accents in transcription is unspecified; guidelines focus on Mandarin, leaving English variation policies unclear.\n\n3. Possible regional/accent bias is not analyzed; no stratified evaluation by region/accent or related error analysis.\n\n4. Inter-participant relationship is not reported (acquainted vs. strangers), which can affect dialogue style and code-switching behavior.\n\n5. No cross-corpus generalization or zero-shot tests (e.g., TALCS, SEAME, DOTA-ME-CS); external validity remains unproven.\n\n\n## Minor Weaknesses:\n\n1. Authors state the \"monolingual\" blocks were not strictly enforced and CS was allowed, but there is no quantitative validation (e.g., comparison of switch-rate/CS types to naturally occurring corpora).\n\n2. Recording setup and modality: Conducted via an online platform and audio-only retained, but it's unclear whether any sessions were co-located, how devices/network artifacts were normalized, or whether mixed setups occurred.\n\n3. Generalization to unequal-status language pairs: Mentioned conceptually in related work, but no empirical evidence or analysis.\n\n4. Breadth of CS dataset review: Coverage beyond Mandarin–English has improved but remains brief and not fully systematic.\n\n5. Value of full-dialogue context: An intra-corpus ablation (previous turns 0 -> 3) shows gains, but there is no validation against non-dialogue corpora or a \"utterance-only\" baseline constructed for a like-for-like comparison."}, "questions": {"value": "1. Licensing remains vague (“free for academic use”); no explicit open license or clarification on redistribution/commercial use."}, "flag_for_ethics_review": {"value": ["Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"]}, "details_of_ethics_concerns": {"value": "1. No IRB/ethics committee approval number or institutional review details are provided.\n\n2. Informed consent is described at a high level, but specifics are missing (consent form contents, withdrawal rights, compensation, risk/benefit disclosure, retention period, cross-border data safeguards).\n\n3. Anonymization is insufficiently detailed: use of pseudonyms is mentioned, but the de-identification pipeline is unclear (e.g., voice masking, metadata scrubbing).\n\n4. Modality/monitoring is under-specified: sessions used an online platform, but it’s not stated whether any video was recorded/observed and, if so, how it was handled.\n\n5. Participant comfort and potential pressure: the staged language timing/instructions may bias behavior; mitigation steps to minimize coercion and preserve natural switching are not detailed.\n\n6. Data access & governance: release is academic-only; policies for third-party redistribution, derivative datasets, and compliance auditing are not specified."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2GSCTTOQL9", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Reviewer_csnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Reviewer_csnc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839134803, "cdate": 1761839134803, "tmdate": 1762923027001, "mdate": 1762923027001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CS-Dialogue, a large-scale Mandarin-English code-switching dataset consisting of 104 hours of spontaneous conversations from 200 bilingual speakers. CS-Dialogue includes full-length, naturalistic dialogue recordings with comprehensive manual transcriptions. \n\nBenchmark ASR experiments with transformer-based models (Transformer, Conformer, Branchformer) and pre-trained models like Whisper and SenseVoice-Small demonstrate the dataset’s challenges in code-switching ASR and show substantial room for improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Large-scale and spontaneous: 104 hours of fully transcribed dialogue data, larger and more natural than many existing CS datasets.\n\n2. Full-length dialogues: Enables study of contextual code-switching patterns beyond isolated utterances.\n\n3. Rigorously annotated: Manual transcription with word-for-word fidelity, preservation of disfluencies, accents, and exact pronunciations."}, "weaknesses": {"value": "1. Limited language pair: Only Mandarin-English code-switching, excluding other important bilingual combinations.\n\n2. Controlled environment: Recordings in quiet settings with smartphone microphones, limiting acoustic variability compared to wild scenarios.\n\n3. Speaker bias: All speakers are native Chinese with strong English proficiency; no native English speakers code-switch to Mandarin."}, "questions": {"value": "1. How does the annotation team handle ambiguous or unclear speech segments during transcription to ensure correctness?\n\n2. What specific procedures were used during quality control to verify transcript accuracy systematically?\n\n3. Are there error rates or inter-annotator agreement statistics reported for transcription quality evaluation?\n\n4. How do frontier models like Gemini and GPT handle the acoustic and phonetic variations in code-switched speech?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "1. What specific measures were implemented to ensure participant privacy?\n\n2. Have safety considerations been taken into account regarding potentially sensitive topics discussed during the dialogues?\n\n3. Are there known or observed disparities in model performance across different speaker subgroups or topics? How is fairness evaluated or mitigated?\n\n4. Has any analysis been conducted on whether the code-switching behavior in the dataset reflects social or cultural biases that could affect model generalization or fairness?"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Qkg3qGQV3", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Reviewer_zMCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Reviewer_zMCH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977772046, "cdate": 1761977772046, "tmdate": 1762923026535, "mdate": 1762923026535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new dataset of Mandarin-English spontaneous conversational code-switched (CS) speech. It comprises full dialogues and appears to be very carefully transcribed. Although many Mandarin-English datasets are available, the authors note that this is the largest one to date that comprises fully spontaneous speech, and the only one to contain full dialogues.\n\nExtensive baseline automatic speech recognition (ASR) experiments are carried out using pre-trained models, models trained only on the data, and fine-tuned models. Results confirm that CS remains challenging for ASR, even for very large foundation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a really thorough, well designed piece of work. Great care has been taken in the data collection, and the result is a corpus that will be of great use to many researchers.\n\nAlthough some would say that a \"dataset\" paper does make a major contribution, I would argue that this is a particularly high-quality offering, both in terms of the usefulness of the data, the collection methodology, and the paper itself.\n\nYou did a great job of clearly differentiating your work from previous Chinese-English datasets."}, "weaknesses": {"value": "I have listed some minor weaknesses below. The only significant one that I noted was that there was no comment on the way in which the way CS behaviour was enforced would have affect the type of switching observed in the data.\n\nThe participants were instructed to first speak Chinese, then mixed-language and then English. This design partly explains why the data is so rich in code-switching (compared to other datasets where utterances are mostly monolingual, so relatively few CS instances can be obtained) but it does worry me that it may make switching somewhat forced and unnatural. For example, it may remove the freedom of speakers to switch for topic-related or social reasons, resulting in (perhaps) syntax playing a more dominant role than in truly natural switching.  *See Questions for further discussion of this point*.\n\nThe authors should have provided on the instructions given to participants, and perhaps conducted validation that the slightly forced design did not influence their behaviour.\n\nWere the Chinese and English monolingual portions of the dataset strictly monolingual? What if the participants strongly felt that it would be natural to switch in these sections, even though they had been instructed not to?\n\nBecause Chinese and English have somewhat equal status as major languages, I wonder whether any findings would generalise to language pairs where the two languages play unequal roles, or where switching would be more topic enforced? (For example, in many languages, speakers might be need to switch to English for technical vocabulary)."}, "questions": {"value": "The authors will recognise that I have reviewed this paper previously when submitted to the ACL ARR.  I previously gave the paper good scores, and since this version is very little changed from the previous one, I have for the most part left identical review comments for the new meta reviewers.  This includes the \"weaknesses\" above – though I have removed one from my previous review that I feel has been addressed.\n\nI appreciate the new inclusion of CS-related works in other language pairs, although would have liked more discussion of these in the text.\n\nI am including here my previous comments (in block quote) along with previous author rebuttal (in italics) with comments about whether the suggestions have been addressed in the new version.  The first of this features in the section above.  The others were just minor comments that I am just including here.\n\n> The participants were instructed to first speak Chinese, then mixed-language and then English. This design partly explains why the data is so rich in code-switching (compared to other datasets where utterances are mostly monolingual, so relatively few CS instances can be obtained) but it does worry me that it may make switching somewhat forced and unnatural. For example, it may remove the freedom of speakers to switch for topic-related or social reasons, resulting in (perhaps) syntax playing a more dominant role than in truly natural switching. \n\n- _Participants were not strictly restricted to a single language within the designated monolingual segments and could code-switch naturally if it felt appropriate. Likewise, purely monolingual speech was permitted within the code-switching segment. The transcriptions faithfully capture the actual utterances spoken, ensuring that the dataset authentically reflects real conversational dynamics._\n\nWhere did you explain this in the revised manuscript?  I could not see it.\n\n> When discussing SEAME (perhaps the most widely used English-Mandarin dataset) you imply that the only difference is that it is a paid dataset. Yours being free doesn't make it a contribution in itself. (The table shows that there are other differences – SEAME doesn't include full dialogues – and you should comment on this in the text).\n\n- _SEAME: Thank you for the suggestion. We will revise our discussion of SEAME to emphasize key distinctions beyond accessibility, particularly noting that CS-Dialogue provides full-length dialogues with complete transcriptions, unlike SEAME._\n\nI feel that you didn't do this, merely removing reference to SEAME being a paid dataset.\n\n> Did the speakers know each other? If so, in what capacity? It's not made clear if they were physically in the same room, or using a virtual platform – and if so, was it audio-visual or audio-only?\n\n- _Speaker Interaction & Setting: The dialogues were conducted via an audio-only virtual platform, and participants generally did not know each other beforehand._\n\nYou did make it more clear about the platform used, but I still did not see information about whether the participants were acquainted with each other before the data collection.\n\n> You comment on accent accommodation in the transcriptions. I assume this was for Chinese only. Did you do anything to accommodate English accents, or where they considered to all be the same?\n\n- _English Accents: You are correct. While our transcription protocol explicitly accommodated regional Chinese accents (Sec 3.2.1, point 4), English accents were not systematically differentiated during annotation._\n\nI did not see any comment on this in the revised manuscript.\n\n> The tokens per second speaking rate isn't very useful. Without an understanding of what exactly the tokens are and how they vary between languages. Perhaps phonemes per second might have been better - I assume that the speaking rates weren't really dramatically different between English and Chinese. If they were, it would cast doubt on the English proficiency of the speakers.\n\n- _Speaking Rate Metric: We acknowledge the potential limitations of measuring speaking rate in tokens per second. We will consider alternative metrics, such as phonemes per second, to provide a more linguistically meaningful comparison._\n\nThis was not done – the speaking rate metric was simply removed, which is a shame.\n\n- _Table 8 & 9 Wording: We will revise the captions to explicitly state that the results are \"on the CS-Dialogue test set\" to avoid ambiguity.\nResults Breakdown by Language Type: While we already report WER and CER separately for English and Chinese segments (Sec 5.1 Metrics) alongside overall MER, we acknowledge that further breaking down results by EN/CN/Mixed segments within the test set could provide additional insights._\n\nThanks, you addressed these comments.\n\n> why did you choose to fine-tune only Whisper, when it was one of the worst-performing foundation models?\n\n- _Fine-Tuning Choice & Protocol: We selected Whisper for fine-tuning because it is one of the most widely used multilingual ASR foundation models. Details on the fine-tuning protocol, including hyperparameters (learning rate, epochs, batching), are provided in *Appendix D._\n\nMy question wasn't about why Whisper was chosen in the first place, but why was Whisper the _only_ model that was fine-tuned?  There was no discussion of this in the revised text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TRDvV1Tgyi", "forum": "Y3auRTgDEX", "replyto": "Y3auRTgDEX", "signatures": ["ICLR.cc/2026/Conference/Submission12051/Reviewer_9Q5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12051/Reviewer_9Q5M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167674286, "cdate": 1762167674286, "tmdate": 1762923026156, "mdate": 1762923026156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}