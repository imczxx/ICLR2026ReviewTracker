{"id": "UG3xdOcITO", "number": 23134, "cdate": 1758340042384, "mdate": 1759896830970, "content": {"title": "G-Verifier: Geometric Verifier for Robust 3D Point Cloud Semantic Search with Spatial Relation Reasoning", "abstract": "Semantic search in 3D point clouds is a fundamental task for Spatial Intelligence and embodied AI, yet it becomes particularly challenging when queries involve precise spatial relationships and current large-scale vision-language models often falter in these scenarios. Their reliance on monolithic, implicit attention mechanisms struggles to disentangle semantic attributes match from complex spatial geometric constraints, leading to unreliable localization. To address this issue, we introduce G-Verifier, a geometric verification module that enhances existing 3DVG frameworks by explicitly decoupling the semantic attributes match and spatial reasoning processes. Our approach realizes a Propose, Select, then Verify paradigm, where G-Verifier acts as a post-hoc re-ranker, adjudicating semantically-filtered candidates based on explicit geometric facts. The core of our module is the Rotary Spatial-Relationship Embedding (RoSE), a structured representation that dynamically fuses high-level object semantics with an explicit 3D geometric encoding. We train this module using a specialized language-alignment strategy on our new large-scale dataset, 3D-SpAn, which contains 285,177 structured spatial relationship annotations. Experiments on a challenging, manually-verified benchmark demonstrate the effectiveness of our approach. Our module itself achieves high F1-score(0.96) on a relational understanding proxy task, validating its strong discriminative power. When integrated into the end-to-end pipeline, G-Verifier improves grounding accuracy, increasing Acc@0.50(+2.50%) over a strong baseline. Our work validates that a decoupled verification approach is a promising direction for improving the geometric reasoning capabilities of large-scale 3D vision-language models.", "tldr": "", "keywords": ["Spatial Reasoning", "Spatial Relationship", "3D Vision-Language", "Point Clouds", "Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f9b1c43a23d1be020c02bc7f2883c2e1e33b2ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces G-Verifier, a module to encode explicit spatial relations between objects to re-rank the object proposals (candidates) in 3DVG. G-Verifier outputs RoSE features for each candidate and applies contrastive learning to implement the re-ranking. This paper also introduces 3D-SpAn, a large-scale 3DVG dataset with structured explicit spatial relationship annotations, to train and evaluate G-Verifier.  Experimental results show that adding G-Verifier improves the backbone model (Grounded 3D LLM [a]).\n\na. [Grounded 3D-LLM with Referent Tokens](https://arxiv.org/abs/2405.10370)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is well-structured.\n2. Body texts are easy to follow.\n3. The proposed 3D-SpAn dataset adds value to the 3DVG community.\n4. Disentangling semantics and spatial relations processing can be a good attempt."}, "weaknesses": {"value": "1. Many typos, missing spaces, and incorrect citation formats, such as \"F1-score(0.96)\" in line 028, \"...geometry. (Xu et al. (2024b)).\" in line 070, and \"(MLP) Haykin (1994) to\" in line 302.\n\n2. Over-squeezed spaces between images and body text. Figure 1 & 2 abuse \\vspace.\n\n3. In line 157, the authors misrefer to Figure 6 in the appendix, and Figures 2 & 6 are the same.\n\n4. Vigor [a] and CoT3DRef [b] should be discussed because they have been attempts to decouple the semantic and geometric information of 3DVG, which is very relevant to your core argument.\n\n5. G-Verifier and the proposed 3D-SpAn dataset seem to target a simplified scenario where object relations are a pre-defined closed set, like SR3D [c]. However, real-world scenarios may be users verbalizing natural, lengthy descriptions without fixed templates, as NR3D [c]. It is unclear to me how G-Verifier will tackle such natural descriptions.\n\n6. G-Verifier is a second-stage refinement on another model's outputs—Grounded 3D-LLM itself has had the ability to propose object candidates and select the most possible one. That is, G-Verifier is a \"plug-in\" to backbone 3DVG models, analogous to CoT3DRef. In this case, G-Verifier should be evaluated on several backbone models to verify its general benefits. The only experiment on performance improvement is on Grounded 3D-LLM [d] (Table 2), where numbers seem not significant, given that the extra stage and parameters are used.\n\n7. The novelty (settings, engineering techniques, and insights) of G-Verifier is limited. G-Verifier merges explicit spatial info into the last process, while the main reason why previous works did implicit learning is because they consider scenarios where spatial info is complex and descriptions have open forms.\n\na. [Data-Efficient 3D Visual Grounding via Order-Aware Referring](https://arxiv.org/pdf/2403.16539)  \nb. [Chain-of-Thoughts Data-Efficient 3D Visual Grounding](https://arxiv.org/abs/2310.06214)  \nc. [ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes](https://github.com/referit3d/referit3d)  \nd. [Grounded 3D-LLM with Referent Tokens](https://arxiv.org/abs/2405.10370)"}, "questions": {"value": "I may raise my score if authors address the above weaknesses. My major concerns include limited novelty, simplified settings, and insufficient experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x0Bk39TreZ", "forum": "UG3xdOcITO", "replyto": "UG3xdOcITO", "signatures": ["ICLR.cc/2026/Conference/Submission23134/Reviewer_GRtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23134/Reviewer_GRtD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760914101949, "cdate": 1760914101949, "tmdate": 1762942525860, "mdate": 1762942525860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes G-Verifier, a post-hoc geometric verification module intended to improve spatial reasoning in 3D visual grounding. The design follows a Propose, Select, Verify pipeline, where the final verification stage re-ranks candidate objects using a newly proposed RoSE relational embedding. Experiments show moderate improvements on a constructed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and motivated by well-known weaknesses of current 3DVG/3D-LLM models.\n- The re-ranking idea is conceptually intuitive and easy to integrate into existing systems.\n- The new dataset appears reasonably large and might be useful to the community."}, "weaknesses": {"value": "- **Lack of Novelty**: The core idea, introducing a post-hoc re-ranking module using relation embeddings, is well-established in both 2D grounding and classical IR-style pipelines. Most components (contrastive alignment, RoPE encoding, relation-type embeddings, weighted score fusion) are direct adaptations of existing methods. The paper does not demonstrate what is fundamentally new beyond recombining known elements. If the authors present the dataset contribution as their primary contribution, they should emphasize the associated analysis and insights instead of stressing methodological novelty.\n- **Confusing Experimental Design**: A critical issue is that the paper does not include comparisons against any existing 3D visual grounding or spatial reasoning baselines. The experiments only evaluate variants of the proposed method on a custom dataset, which makes it impossible to assess contribution or practical significance.\n- **Marginal Practical Impact**:  The reported improvements are small (e.g., +2.5% Acc@0.50) and limited to a curated evaluation split rather than standard benchmarks. There is no evidence that the proposed verifier improves generalization in broader or real-world conditions.\n- **Insufficient Detail in Data**: The model relies heavily on the newly constructed 3D-SpAn dataset. However: The pseudo-labeling process is under-specified. The dataset is not benchmarked against alternatives. It is unclear whether the module performs well without this dataset, limiting reproducibility and applicability.\n- **Limited Insight Into Failure Cases**: The analysis focuses on overall performance but does not sufficiently explore when and why the verifier fails. Certain relation types show no improvement (e.g., behind, right of), suggesting that the approach may not truly address viewpoint-dependent spatial reasoning, despite claiming to.\n- **Conceptual Gap in the Claimed Contribution**: The paper’s thesis is that decoupling semantic and geometric reasoning is necessary. However, the verifier still depends on semantic embeddings and does not provide insight into how “geometric reasoning” is being meaningfully separated rather than just appended."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NVBmPH2fFA", "forum": "UG3xdOcITO", "replyto": "UG3xdOcITO", "signatures": ["ICLR.cc/2026/Conference/Submission23134/Reviewer_6Rhx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23134/Reviewer_6Rhx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447225429, "cdate": 1761447225429, "tmdate": 1762942525635, "mdate": 1762942525635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces G-Verifier, a geometric verifier and re-ranker designed for post-processing grounding results. To train this module, the authors constructed a large-scale 3D spatial relation annotation dataset, 3D-SpAn, and employed a specific language alignment training strategy. Experimental results indicate that the G-Verifier module possesses discriminative capabilities. When integrated into the end-to-end pipeline, it significantly enhances the baseline model's localization accuracy while maintaining high stability. However, as a post-processing module, the paper lacks sufficient experiments demonstrating the method's generality and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper designed a post-hoc re-ranker, G-Verifier, which can be conveniently integrated into existing 3DVG frameworks.\n\n2. The paper construct the large-scale 3D-SpAn dataset, featuring structured spatial relation annotations, holds significant value for relevant research within the community.\n\n3. The results in the paper clearly demonstrate G-Verifier's effectiveness in enhancing spatial relation reasoning robustness, even when compared against a strong baseline model."}, "weaknesses": {"value": "1. The paper primarily compares against the authors' re-implementation of the Grounded 3D-LLM baseline. While strong, it lacks direct comparison with the results reported in the original Grounded 3D-LLM paper or other recent SOTA 3DVG methods that also focus on spatial relations.\n2.  G-Verifier functions as a post-processing step, introducing additional computation. The paper does not explicitly analyze the resulting increase in inference time or computational resource requirements.\n3.  The authors acknowledge that using \"inverse querying\" to generate anchor pseudo-labels may introduce noise. Further discussion or quantification of this noise's impact on model training, final performance, and the model's robustness to such noise is recommended.\n4.  The rationale behind the chosen feature fusion method within ROSE could be more clearly articulated. Additionally, the specific advantages of 3D RoPE compared to alternative 3D relative position encoding methods could be further elaborated.\n5.  Does the method merely overfit to the authors' proposed dataset? Can this post-processing approach improve the model's general grounding capabilities?\n\nAlthough this is a post-processing method, its overall simplicity and seamless integration with other methods are commendable. Therefore, if the authors can address these concerns, I will consider increasing my score."}, "questions": {"value": "see in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CuputJimDb", "forum": "UG3xdOcITO", "replyto": "UG3xdOcITO", "signatures": ["ICLR.cc/2026/Conference/Submission23134/Reviewer_wVNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23134/Reviewer_wVNo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607667465, "cdate": 1761607667465, "tmdate": 1762942525298, "mdate": 1762942525298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}