{"id": "JqksGFSl62", "number": 13662, "cdate": 1758220572784, "mdate": 1763562101695, "content": {"title": "SchemixQA and CoRe-VLM: A Benchmark and Collaborative Refinement (CoRe) Framework for Visual Question Answering on Technical Schematics", "abstract": "We present SchemixQA, a multimodal benchmark for evaluating Vision–Language Models (VLMs) on Visual Question Answering (VQA) over technical schematics. Unlike previous VQA datasets focused on natural images, SchemixQA targets structured domains such as circuits, flowcharts, logic gates, P&I diagrams, and state diagrams, each paired with natural language questions and multiple reference answers. To address this setting, we introduce CoRe-VLM (Collaborative Refinement for VLMs), the first actor–critic inspired refinement framework for schematic VQA. In CoRe-VLM, an actor VLM generates answers, while a critic VLM verifies them and provides corrective feedback. A fallback mechanism ensures robustness by reverting to the actor’s output when the critic introduces errors. We benchmark seven state-of-the-art VLMs, including GPT-4o, Gemini, Qwen2 and LLaVA, under single-pass and CoRe-VLM inference. The results show that CoRe-VLM consistently improves lexical (Exact Match, BLEU, ROUGE-L) and semantic (BERTScore, Macro/Micro-F1) metrics, with especially strong gains for weaker open-source actors when paired with a strong critic. Together, SchemixQA and CoRe-VLM establish a new foundation for domain-specific multimodal reasoning.", "tldr": "", "keywords": ["SchemixQA", "CoRe-VLM Framework", "VLM", "LLM", "Actor-Critic Refinements"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/779c18cd70f1d4483809e9c426786a0be555f8ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new benchmark on Schemetic QA and provides a method called CoRe-VLM, which iteratively refines the proposed solutions. CoRe-VLM is based on the actor-critic framework, and it consistently improves several metrics. These contributions work together to head towards better domain-specific multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important problem of domain-specific QA, although this task has also been heavily studied in recent years.\n\n2. This paper proposes a working method to benefit various LLM-based methods, while achieving good accuracy numbers in all metrics."}, "weaknesses": {"value": "1. The benchmark contribution is considered weak:\n\n(1) 500 images in total are not a lot compared to the existing similar large-scale VQA dataset, such as scientific QA. \n\n(2) 3,906 questions in total are also not enough for training. For evaluation, there are already a lot of existing VQA benchmarks.\n\n(3) It is not certain what this dataset is designed for. Although it claims to be domain-specific, it covers multiple diverse domains. \n\n(4) The dataset construction procedure is not clear. See questions in the section below.\n\n2. The advantage of CoRE-VLM has not been established. As it's an inference-only enhancement algorithm, comparing to single-pass LLMs is not fair. At least, CoRE-VLM should compare to other reasoning approaches, such as ToT or CoT. Also, it's not clear whether the authors use the proposed benchmarks to do development or fine-tuning. They should test the method in other standard benchmarks as well.\n\n3. The paper presentation quality is quite low.\n\n(1) About the block diagram, it uses dizzy fonts. Figures overlap with texts. Texts are not aligned.\n\n(2) Mathbb(1) was rendered wrongly in EQ (1). Too many descriptions of metrics.\n\n(3) Texts in Figure 4 are too small."}, "questions": {"value": "About the dataset construction procedure:\n\n1. How do human reviewers conduct a review? How many hours or human efforts does it cost to construct the dataset? How much is the payment for human annotators? These questions are standard questions for high-quality benchmark papers in top-tier ML conferences.\n\n2. How to measure the collected data quality? Is it enough to cover those deep fields?\n\n3. Do similar questions already exist in commonly used benchmarks? Such as MMMU or ScientificQA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kmCy4eVLUz", "forum": "JqksGFSl62", "replyto": "JqksGFSl62", "signatures": ["ICLR.cc/2026/Conference/Submission13662/Reviewer_EMrn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13662/Reviewer_EMrn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760802390793, "cdate": 1760802390793, "tmdate": 1762924232239, "mdate": 1762924232239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SchemixQA, a multimodal benchmark for Visual Question Answering (VQA) on technical schematics, including circuits, flowcharts, logic gates, and P&I diagrams. To address this specialized domain, the authors propose CoRe-VLM, an actor–critic  framework in which an actor VLM generates candidate answers and a critic VLM evaluates, provides corrective feedback, and assigns a confidence score. A fallback mechanism ensures robustness when critic feedback is unreliable. The framework wraps around existing VLMs without requiring retraining. Experiments show consistent improvements across multiple actor models on lexical and semantic metrics (Exact Match, BLEU, ROUGE-L, BERTScore, Macro/Micro F1)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- **Reasoning-focused design:** SchemixQA stratifies questions by type, targeting multiple reasoning dimensions, from symbolic recognition to functional and structural reasoning. This fine-grained evaluation is more detailed than CircuitVQA. However, the relation between reasoning types and question types remains unclear.\n- **Diversity of schematic categories:** Includes circuits, flowcharts, logic gates, P&I diagrams, and state diagrams, covering **digital, analog, and hand-drawn styles**, making it more diverse than existing datasets like ElectroVizQA.\n- **Quality control:** Questions are generated using GPT-5 and refined by technical experts, with three human-verified reference answers per question.\n- **Inference-time refinement framework:** CoRe-VLM is easy to apply to existing models and demonstrates consistent performance gains.\n- **Robustness features:** Confidence-based stopping, fallback mechanism, and optional self-consistency make the approach more reliable.\n- **Comprehensive evaluation metrics:** Both lexical (BLEU, ROUGE-L, Exact Match) and semantic (BERTScore, F1) metrics are reported."}, "weaknesses": {"value": "- **Dataset transparency:** Neither the SchemixQA dataset nor the CoRe-VLM code is publicly released. Combined with missing prompts and unclear model setup, this significantly limits reproducibility and prevents other researchers from validating or building upon the work.\n- **Scale:** 500 diagrams and ~3.9k questions are modest in size compared to large natural image datasets.\n- **Model setup unclear:** It is not specified whether actor/critic models are zero-shot or fine-tuned on SchemixQA. The potential effect of training on the dataset is not assessed.\n- **CoRe-VLM novelty:** While the actor–critic inference wrapper is novel in application to schematic VQA, the general concept of iterative refinement or self-critique is related to prior works in multimodal VLMs and LLMs (self-refinement, reranking, verification frameworks). The paper does not adequately discuss these related approaches.\n - **Citation format issue:** The paper uses numeric citations (e.g., `(10)`), which does not follow ICLR template. ICLR guidelines require author-year format with `\\citet{}` or `\\citep{}`.\n- **Table formatting:** Table 4 should highlight the best results to improve clarity and readability."}, "questions": {"value": "**Questions for the Authors**\n\n1. Were the actor and critic models evaluated zero-shot or fine-tuned on SchemixQA? If fine-tuned, please specify the training setup, data splits, and hyperparameters.\n2. Can you explicitly map question types to the reasoning dimensions (symbolic, functional, structural)? Consider providing an annotated mapping table or examples to support claims about symbolic, functional, and structural reasoning.\n3. How does CoRe-VLM differ from existing self-refinement, reranking, or verification frameworks in VLMs and LLMs (e.g., self-consistency, Reflexion, or RLAIF-inspired inference)?  What aspects of CoRe-VLM are unique to schematic reasoning?\n\n**Actionable Feedback**\n1. Publish GPT-5 prompts used for question generation and for actor–critic refinements. Make the SchemixQA dataset, prompts, and CoRe-VLM code publicly available to ensure reproducibility and enable further research.\n2. Explicitly connect question types to the reasoning skills they are intended to evaluate. \n3. Update citations to author-year format per ICLR guidelines.    \n4. Highlight best results in tables (e.g., Table 4) for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tOw3ZlhQMf", "forum": "JqksGFSl62", "replyto": "JqksGFSl62", "signatures": ["ICLR.cc/2026/Conference/Submission13662/Reviewer_47dW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13662/Reviewer_47dW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666112484, "cdate": 1761666112484, "tmdate": 1762924231925, "mdate": 1762924231925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SchemixQA, a benchmark for schematic VQA. It also proposes CoRe-VLM, which employs an actor–critic style interaction with a fallback mechanism to improve schematic VQA ability. Experiments show the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The new benchmark is useful for evaluating schematic VQA ability.\n2. The experiments show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The comparing VLMs are outdated. New VLMs like GPT-5, gemini 2.5 pro, and other VLMs with thinking (CoT) ability could have self-critic ability to answer the question. But these methods are not included in the evaluations.\n2. The evaluation metrics do not include LLM as judge. Some methods with thinking ability may produce long and structured responses, which is hard to extract the answers and affects the results.\n3. The inference time and efficiency is not reported. How much time is needed for the verification and feedback process?"}, "questions": {"value": "1. What is the performance of new models?\n2. How much time is needed for the verification and feedback process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WunzEbPyK2", "forum": "JqksGFSl62", "replyto": "JqksGFSl62", "signatures": ["ICLR.cc/2026/Conference/Submission13662/Reviewer_zghu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13662/Reviewer_zghu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841187723, "cdate": 1761841187723, "tmdate": 1762924231539, "mdate": 1762924231539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SchemixQA is a multimodal benchmark for visual question answering on technical schematics, addressing gaps in natural-image VQA datasets. It includes 500 schematic diagrams (circuits, flowcharts, logic gates, P&ID, etc.) and 3,906 expert-annotated QA pairs covering tasks from symbol recognition to functional reasoning. The paper also introduces CoRe-VLM, an actor–critic VLM framework where a critic verifies and refines the actor’s answers. Evaluations on seven leading VLMs (including GPT-4) show that CoRe-VLM significantly boosts accuracy, enabling smaller models to rival or surpass larger ones on complex schematic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is thoughtfully designed with a taxonomy of question types across multiple schematic categories (circuits, flowcharts, state diagrams, etc.), ensuring a broad range of reasoning challenges beyond superficial text matching. \n2. Questions require compositional logic (e.g. truth-table analysis), reading diagram text labels, relational connectivity reasoning, and arithmetic counting, reflecting comprehensive test coverage.\n3. The collaborative refinement approach is clearly explained. By separating the roles of answer generation and answer verification, the method leverages two models for their complementary strengths. Notably, CoRe-VLM is model-agnostic and works at inference-time without requiring any retraining, making it a lightweight, plug-in enhancement for existing VLMs."}, "weaknesses": {"value": "1. All experiments are conducted on the new SchemixQA dataset, so it remains unclear how well the CoRe-VLM strategy generalizes to other domains or VQA tasks. \n2. The actor–critic refinement is promising, but its effectiveness outside technical schematics (e.g. on natural image VQA or chart QA) is not demonstrated, leaving its broader impact somewhat speculative.\n3. The CoRe-VLM pipeline’s success depends on having a strong critic model. In the paper, Gemini 2.0 Flash Lite was chosen as critic to drive improvements. If the critic is weaker or inaccurate, the refinement might stagnate or even introduce errors. This suggests the approach’s robustness could vary with critic choice, an aspect not deeply explored in the main paper."}, "questions": {"value": "Provide a more granular breakdown of performance across the different question types or reasoning skills in SchemixQA. Identifying which categories (e.g. logical reasoning vs. simple identification) remain hardest for models would highlight where future work should focus. It would also reveal whether CoRe-VLM disproportionately helps certain question types (for example, perhaps it excels at correcting counting errors but less so at understanding schematic functionality)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uZ19dk9NNq", "forum": "JqksGFSl62", "replyto": "JqksGFSl62", "signatures": ["ICLR.cc/2026/Conference/Submission13662/Reviewer_Q78V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13662/Reviewer_Q78V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968136807, "cdate": 1761968136807, "tmdate": 1762924230985, "mdate": 1762924230985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}