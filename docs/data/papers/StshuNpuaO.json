{"id": "StshuNpuaO", "number": 14730, "cdate": 1758242642088, "mdate": 1759897352472, "content": {"title": "LayerMix Law: Scaling Law for Large Language Models on Quality-Weighted Mixture Data with Repetition", "abstract": "Upweighting high-quality data in large language model (LLM) pretraining typically improves performance. However, the limited availability of high-quality data—particularly in overtrained regimes—means that stronger upweighting often increases repetition, which can degrade performance. This creates a fundamental trade-off between data quality and data repetition. In this paper, we systematically investigate how varying data quality and repetition affects models across different scales. Concretely, we partition the source corpus into buckets based on quality scores and sample from each bucket with different weights, thereby constructing training sets with diverse scales, quality distributions, and repetition levels. We then train a family of models on these datasets to measure performance across conditions. Building on these observations, we introduce a theoretical framework analogous to scaling laws, which we call \\textbf{LayerMix Law}. LayerMix Law predicts model loss as a function of consumed tokens, model size, sampling weights, and repetition levels. The key intuition is to view training as the accumulation of information from data, where the amount of information is governed by data quality, while model scale and repetition determine the information gained per training step. We show that LayerMix Law accurately predicts the model performance on unseen data recipes at larger computation scale (up to 7B parameter run with 425B token, each x2 invest compute), with 0.15\\% average absolute error and 0.96\\% maximum absolute error, which enables efficient search for optimal data recipes without costly additional experiments. Moreover, LayerMix Law extrapolates reliably to different degrees of overtraining, providing a efficient tool for selecting data recipes under varying computational budgets.", "tldr": "", "keywords": ["SCALING LAW", "LARGE LANGUAGE MODELS"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf3c1ddcc22522e9db0e5acdcb503b6903cebc36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LayerMix Law, a new scaling law framework designed to model large language model (LLM) performance when training data varies in quality and repetition. LayerMix Law explicitly captures the trade-off between using limited high-quality data (which may require repetition) and including lower-quality data to reduce overfitting. They propose an Information Quantity metric to quantify total learned information and demonstrate a power-law relationship between this metric and model loss. Empirically, across 27 training runs (252M–1.2B models) and extrapolations up to 7B parameters, LayerMix Law accurately predicts model loss under unseen mixtures and overtrain degrees with <1% error, providing a principled tool for data recipe optimization under data constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good Motivation: The paper tackles a critical and realistic problem in LLM training. It clearly explains why traditional scaling laws fail under such conditions and motivates the need for a data-aware scaling framework.\n2. Comprehensive Experiments: Experiments are extensive and well-structured, covering multiple model sizes (252M–7B), data mixtures, and overtrain ratios. The consistent results across interpolation and extrapolation settings strongly support the proposed LayerMix Law.\n3. Problem Formalization: The formulation of Information Quantity elegantly links data quality, repetition, and compute into a unified model. It provides a clear theoretical basis for understanding and predicting LLM performance across heterogeneous data mixtures."}, "weaknesses": {"value": "1. Lack of Optimization Guidance: Although the law connects information gain to model loss, it does not analyze how to derive optimal mixture ratios or repetition levels under fixed compute. This limits its practical applicability for data recipe design.\n2. Format and Clarity Issues:\nNaming conventions like Q1_V1 and Q1_V2 are unintuitive, and equation formatting could be improved for readability."}, "questions": {"value": "While the paper successfully establishes a theoretical connection between information gain and model performance, it stops short of providing actionable guidance on how to optimize data mixtures in practice. The LayerMix Law defines how loss depends on parameters like sampling weights $w_d$, token scale K, and repetition $R_d$, yet it does not analyze how to choose or optimize these variables to achieve the best results under a given compute budget. For example, once the relationship $L = \\alpha \\cdot \\text{info}^{-\\beta}$ is known, a natural next step would be to derive the optimal allocation of quality buckets $w_d$ that minimizes loss for fixed compute or token count. However, the paper does not explore this direction. There is no gradient-based or analytical discussion of how $w_d$, K, or S interact to yield an optimal configuration. Moreover, since the practical value of a scaling law lies in guiding future training strategies without brute-force grid search, the absence of such optimization analysis weakens the applicability of LayerMix Law for real-world data curation or scaling decisions. Including even a preliminary sensitivity or optimization study would make the framework much more actionable and impactful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ejSNcG1qgJ", "forum": "StshuNpuaO", "replyto": "StshuNpuaO", "signatures": ["ICLR.cc/2026/Conference/Submission14730/Reviewer_3Vjy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14730/Reviewer_3Vjy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760967759492, "cdate": 1760967759492, "tmdate": 1762925092320, "mdate": 1762925092320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LayerMix Law, a data-aware scaling law for large language models (LLMs) that explicitly accounts for data quality and data repetition—two important factors often ignored by traditional scaling laws.\nExisting scaling laws assume an unlimited supply of diverse, high-quality data and tend to fail when datasets are upsampled or repeated. \nHowever, real-world LLM pretraining frequently requires upweighting scarce high-quality data or reusing existing data, leading to a fundamental trade-off between data quality and diversity.\nThe authors study this trade-off and propose both a theoretical formulation and empirical validation of the resulting “LayerMix Law.” \nThe framework extends classical scaling laws by embedding data-quality weights and repetition dynamics into performance prediction, offering a practical tool for selecting and optimizing pretraining datasets under limited compute and data availability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a promising extension of scaling laws by incorporating data-quality weighting and repetition effects, which are crucial in today’s data-constrained LLM training.\n2. The authors effectively combine ideas from scaling theory, data attribution, and mixture modeling into a single predictive law that connects information accumulation with model loss.\n3. The paper reports comprehensive experiments—27 controlled pretraining runs (252M–1.2B parameters) with systematically varied data mixtures and repetition levels—and further validates extrapolation on 7B-parameter models. The resulting predictions achieve an average absolute error of only 0.15%, demonstrating impressive fit accuracy.\n4. The paper is well-organized, clearly motivated, and easy to follow. Figures and explanations illustrate the intuition behind the proposed law, and the experimental design is well-documented."}, "weaknesses": {"value": "1. The proposed information-theoretic formulation appears heuristic rather than theoretically derived. The framework would be more convincing if grounded in established information theory or accompanied by ablations comparing alternative functional forms.\n2. All experiments are conducted on English Common Crawl data and relatively small models (≤7B). It remains unclear whether the fitted parameters generalize across domains (e.g., code, multilingual), which limits the broader applicability of the proposed law.\n3. Although the paper cites related work (e.g., Data Mixing Laws (Ye et al., 2025), CMR Scaling Law (Gu et al., 2024)), it does not provide quantitative comparisons against these or traditional scaling-law baselines. This omission makes it difficult to assess the true improvement brought by LayerMix Law.\n4. The analysis focuses exclusively on validation loss and perplexity. It would strengthen the paper to show whether the predicted improvements in pretraining loss translate into downstream task gains (e.g., MMLU, HellaSwag accuracy)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ciob8PNsCU", "forum": "StshuNpuaO", "replyto": "StshuNpuaO", "signatures": ["ICLR.cc/2026/Conference/Submission14730/Reviewer_P372"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14730/Reviewer_P372"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720325597, "cdate": 1761720325597, "tmdate": 1762925091804, "mdate": 1762925091804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the scaling law taking data quality and repeats into consideration. Concretely, authors split the source data into different buckets with different quality scores and then model a function of consumed tokens, model size, sampling weights, and repetition levels by  treating the training as the accumulation of information from data. And the final scaling law shows a reliable prediction across different settings."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1) It is a very important problem to fit a scaling law in terms of data quality and repeats. We are running out of data very soon. For both small and large models. And we are also looking for better compute efficiency from using more times of high quality data, but also getting struggle with the overfitting effects and diminishing return from that.\n2) The insight of treating the learning as a process of information accumulation is neat and intuitively sound."}, "weaknesses": {"value": "1) I think one assumption of this work is the definition of quality is very reliable. But how can we trust that so much? \n2) Viewing the learning as a process of information accumulation is good. However, it cannot explain some aggressive overfitting -> If we repeat much more times, the model would get worse and worse when training for more steps. It is not an accumulation of info for sure.\n\nMinor: Line 428: LayerMix Law also generate well .... \"generate\" -> \"generalise\"?\nMissing reference: https://arxiv.org/abs/2305.13230 -> A very early work studying the repeat data problem also considered how the data quality matters."}, "questions": {"value": "1) I don't understand why the name is \"layer mix law\". It is confusing because of the \"layer\" is widely used in model architecture. At the first glance, I thought this work is a model arch paper. How about \"quality mix law\"?\n2) If we treat the learning process as a func of information accumulation, and we also take forgetting into consideration, are there any insights to order/shuffle the data for a smarter data schedule? For example, shall we repeat early more aggressively and then repeat a few more times again at the very late stage of the training to leverage the forgetting effect? No exps needed here. Just want to hear more insights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5qBvGMEUtl", "forum": "StshuNpuaO", "replyto": "StshuNpuaO", "signatures": ["ICLR.cc/2026/Conference/Submission14730/Reviewer_MZkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14730/Reviewer_MZkc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742828289, "cdate": 1761742828289, "tmdate": 1762925091119, "mdate": 1762925091119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The central focus of this paper is to investigate the effects of data quality and data repetition on the pre-training of large models. To this end, the authors introduce a novel metric, termed \"Information Quantity,\" and establish its relationship with both data quality and data repetition. Subsequently, by modeling a power-law relationship between this \"Information Quantity\" and the val loss, the paper indirectly examines how data quality and duplication impact the model's training dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The power-law relationship established between \"Information Quantity\" and validation loss is robustly validated by the experiments presented in this work, demonstrating a commendable degree of originality. Furthermore, the overall experimental procedure of the paper is relatively comprehensive and complete."}, "weaknesses": {"value": "To establish quantitative relationships, the paper hypothesizes several formulations (e.g., Equations 1, 6, 8, and 9). However, these assumptions lack sufficient justification, and their credibility is questionable.\n\nThe definitions of key concepts in the manuscript are ambiguous and appear arbitrary. Specifically, in Section 3.1 (Training Data Sampling, lines 168-174), within the formulation H(w, K, S, B), the meaning of the key variable 'S' is not introduced. Furthermore, the relationship between the associated variables 'w' and 'S' is not elucidated. The specific referent for 'B' also remains unclear. Additionally, the substitution of 'I' with f_{d} \\cdot M_{d} in Equation 5, as well as the definition of f_{d} itself, requires substantial further justification or supporting evidence.\n\nThe justification for the preliminary experiments lacks rigor. Regarding the persistence of the Loss-C scaling law (line 206), demonstrating a good fit (fitting) is insufficient evidence on its own. To claim empirical validity, the authors should have used the fitted law to make further predictions; the accuracy of these predictions would then serve to validate the law. Concurrently, the \"traditional law\" (i.T., the OpenAI law) relates to (C-min), not simply 'C'. The authors must clarify this distinction and elaborate on the relationship between their findings and this established principle.\n\nThe Related Work section is missing several representative recent publications that are relevant to the scope of this study:\n\n-Observational Scaling Laws and the Predictability of Langauge Model Performance\n\n-Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law\n\n-RegMix:Data Mixture as Regression for Language Model Pre-training"}, "questions": {"value": "1. please refer to 'Weakness'; \n\n2.How about providing some specific traindata proportion/ratio suggestions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HNVvQV3ZCx", "forum": "StshuNpuaO", "replyto": "StshuNpuaO", "signatures": ["ICLR.cc/2026/Conference/Submission14730/Reviewer_9Rpn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14730/Reviewer_9Rpn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895621598, "cdate": 1761895621598, "tmdate": 1762925089652, "mdate": 1762925089652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}