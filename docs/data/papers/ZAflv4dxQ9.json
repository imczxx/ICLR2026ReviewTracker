{"id": "ZAflv4dxQ9", "number": 14459, "cdate": 1758236104334, "mdate": 1759897368723, "content": {"title": "Improving Online-to-Nonconvex Conversion for Smooth Optimization via Double Optimism", "abstract": "A recent breakthrough in nonconvex optimization is the online-to-nonconvex conversion framework of Cutkosky et al. (2023), which reformulates the task of finding an $\\varepsilon$-first-order stationary point as an online learning problem. \nWhen both the gradient and the Hessian are Lipschitz continuous, instantiating this framework with two different online learners achieves\na complexity of $ \\mathcal{O}(\\varepsilon^{-1.75}\\log(1/\\varepsilon)) $ in the deterministic case and a complexity of $ \\mathcal{O}(\\varepsilon^{-3.5}) $ in the stochastic case.\nHowever, this approach suffers from several limitations: (i) the deterministic method relies on a complex double-loop scheme that solves a fixed-point equation to construct hint vectors for an optimistic online learner, introducing an extra logarithmic factor; (ii) the stochastic method assumes a bounded second-order moment of the stochastic gradient, which is stronger than standard variance bounds; and (iii) different online learning algorithms are used in the two settings.\nIn this paper, we address these issues by introducing an online optimistic gradient method based on a novel **doubly optimistic hint function**. Specifically, we use the gradient at an extrapolated point as the hint, motivated by two optimistic assumptions: that the difference between the hint and the target gradient remains near constant, and that consecutive update directions change slowly due to smoothness. Our method eliminates the need for a double loop and removes the logarithmic factor. Furthermore, by simply replacing full gradients with stochastic gradients and under the standard assumption that their variance is bounded by $\\sigma^2$, we obtain a unified algorithm with complexity $\\mathcal{O}(\\varepsilon^{-1.75} + \\sigma^2 \\varepsilon^{-3.5})$, smoothly interpolating between the best-known deterministic rate and the optimal stochastic rate.", "tldr": "", "keywords": ["Online-to-nonconvex conversion", "Optimistic Gradient Descent", "Smooth Nonconvex Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfe852d411531a5220cb475a401f7ad04e16a6c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method to improve the recently proposed online-to-nonconvex conversion framework (Cutkosky et al., 2023) under gradient and Hessian smoothness assumptions. Two levels of optimism are used when solving the online learning problem in (Cutkosky et al., 2023). The algorithm achieves the complexity of $\\mathcal{O}(\\epsilon^{-1.75} + \\sigma^2 \\epsilon^{-3.5})$, which recovers the best-known rates under both deterministic and stochastic settings. An adaptive variant of the method is also proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The article is well-written, with the authors clearly articulating their ideas. The derivations presented in the paper lead naturally to the proposed algorithm, making it easy for readers to follow the authors' thought process. Furthermore, the final algorithm can be implemented with a single loop, and each iteration only requires a single batch, suggesting that it could be effective in practical applications.  The theoretical improvements over (Cutkosky et al., 2023) are also sound: improving a logarithmic factor in the deterministic case,  removing a stronger assumption in the stochastic case, achieved with a unified algorithm that can be extended to adaptive optimization."}, "weaknesses": {"value": "1. The main results of this paper become relatively straightforward given the inspiring recent work (Jiang et al., 2025), and the idea of double optimism has also appeared in that work, and that work also remarked explicitly that a $\\mathcal{O}(\\epsilon^{-1.75})$ complexity is achievable.  Although the results in stochastic problems and adaptive optimization were not mentioned in \n(Jiang et al., 2025), they appear to be expected from prior analysis (Levy et al., 2018; Kavis et al., 2019; Cutkosky et al., 2023). \n\n2. Although the article improves upon the results under the second-order smooth setting in O2NC, the main contribution of O2NC lies in its results under the non-convex non-smooth setting, rather than the non-convex smooth setting. It is reasonable that the original O2NC can be improved under the smooth setting.\n \n3. Moreover, no experiments are given in this paper, though I think the algorithm is simple to implement and believe it can work well in some scenarios.\n\nThese reasons led me not to assign a higher score.\n\nAnother minor weakness in the article is the limited references to past literature; for example: the prior $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ results before he works by (Cutkosky and Mehta 2022; Cutkosky et al., 2023) were presented in reference [1-4] which I believe should also be cited. Additionally, O2NC was primarily proposed to address the non-convex non-smooth problems discussed in reference [5], but this citation is also missing.\n\n[1] Fang, Cong, Zhouchen Lin, and Tong Zhang. \"Sharp analysis for nonconvex SGD escaping from saddle points.\" In COLT, 2019.\n\n[2] Tripuraneni, N., Stern, M., Jin, C., Regier, J., & Jordan, M. I. \"Stochastic cubic regularization for fast nonconvex optimization.\" In NeurIPS, 2018.\n\n[3] Allen-Zhu, Z.  “Natasha 2: Faster non-convex optimization than SGD\". In NeurIPS, 2018.\n\n[4] Allen-Zhu, Z. \"How to make the gradients small stochastically: Even faster convex and nonconvex SGD\" In NeurIPS, 2018.\n\n[5] Zhang, Jingzhao, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, and Ali Jadbabaie. \"Complexity of finding stationary points of nonconvex nonsmooth functions.\" In ICML, 2020."}, "questions": {"value": "1. In lines 289-290, “base the update of $\\Delta_n$ on the current gradient $g_n$, but this is not feasible because  $g_n$ is revealed only after $\\Delta_n$,” perhaps $\\Delta_n$ and $g_n$ should be changed to $\\Delta_{n+1}$ and $g_{n+1}$ to maintain consistency with the context?\n\n2. It seems that Lemma C.1 is the same as the ones used in (Levy et al., 2018; Kavis et al., 2019; Antonakopoulos et al., 2022), which I think should be explicitly mentioned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F98pPl4JZC", "forum": "ZAflv4dxQ9", "replyto": "ZAflv4dxQ9", "signatures": ["ICLR.cc/2026/Conference/Submission14459/Reviewer_miQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14459/Reviewer_miQo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207114427, "cdate": 1761207114427, "tmdate": 1762924862545, "mdate": 1762924862545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds on O2NC (Cutkosky et al., 2023) to propose a simple first-order method for smooth non-convex optimization. For problems with Lipschitz-continuous gradients and Hessians, the algorithm achieves an $O(\\epsilon^{-1.75} + \\sigma^2 \\epsilon^{-3.5})$ convergence rate, thereby interpolating between the best-known deterministic and the worst-case optimal stochastic rates."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The technical strengths are \n\n- Improving upon Cutkosky et al 2023 with a simpler algorithm. \n- Getting a rate that interpolates between deterministic and stochastic settings. \n- Providing adaptive step size scheduling and making progress towards fully parameter-free algorithms.  \n\nThe paper is well-written and nicely presented."}, "weaknesses": {"value": "I do not see any noticeable weaknesses to this work."}, "questions": {"value": "- Do you think 2 gradient evaluations per step is essential for getting a simple algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HhmZB87jZh", "forum": "ZAflv4dxQ9", "replyto": "ZAflv4dxQ9", "signatures": ["ICLR.cc/2026/Conference/Submission14459/Reviewer_sQGF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14459/Reviewer_sQGF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596947760, "cdate": 1761596947760, "tmdate": 1762924862111, "mdate": 1762924862111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new algorithm for smooth nonconvex optimization based on an online-to-nonconvex (O2NC) framework. Building on prior work, it introduces a doubly optimistic hint function in the online learning subroutine to simplify the algorithm and improve theoretical guarantees. The theoretical results show convergence to an $\\epsilon$-first-order stationary point with improved rates in both deterministic and stochastic regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is generally well-written, with a logical presentation of prior work, motivation, and main contributions.\n\n* The complexity analysis is rigorous and improves on existing bounds by removing logarithmic factors.\n\n* The algorithm handles both deterministic and stochastic settings without requiring separate analyses or algorithms. This universal applicability is a notable advantage."}, "weaknesses": {"value": "* It would be helpful to understand whether the algorithm and proofs can be extended to constrained or nonsmooth functions. While it is understandable that such generalization may be challenging, could the authors discuss potential difficulties in these settings and how the algorithm might be adapted?\n  \n* For the optimistic step, a natural estimation is $g_{n+1} = 2 g_n - g_{n-1}$. Could the authors clarify why this choice was not adopted and what motivated the current design?\n\n* The paper focuses on general nonconvex objectives. It would be interesting to see how the convergence rate could improve if the function is convex or strongly convex. Could the authors provide insights or analysis for these special cases?"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rVUtAABijw", "forum": "ZAflv4dxQ9", "replyto": "ZAflv4dxQ9", "signatures": ["ICLR.cc/2026/Conference/Submission14459/Reviewer_Nne7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14459/Reviewer_Nne7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636317226, "cdate": 1761636317226, "tmdate": 1762924861736, "mdate": 1762924861736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the online-to-batch conversion paradigm for nonconvex optimization, focusing on the gap between online regret and stationarity guarantees. Classical online-to-batch techniques translate sublinear regret bounds into expected stationarity, but typically require either (i) bounded gradients or (ii) smoothness and convexity assumptions that limit applicability.\n\nThe authors propose a refined conversion framework -- generalized online-to-nonconvex conversion (Go2N) -- that directly bounds the gradient norm of the average iterate without relying on convexity or heavy smoothness assumptions. The contributions can be summarized as follows: (i) A new regret decomposition that connects weighted dynamic regret to expected stationarity through gradient averaging lemmas. (ii) Applications to stochastic and adversarial nonconvex optimization, yielding improved dependence on learning rates and gradient variance. (iii) An extension to adaptive algorithms (e.g., AdaGrad, Online Newton Step), providing the first theoretical bridge between regret and stationarity for such adaptive schemes. (iv) Experiments on nonconvex online learning tasks (e.g., nonconvex matrix factorization, deep linear regression) showing faster convergence to stationary points than standard online-to-batch baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work revises the fundamental link between online regret and nonconvex stationarity, providing a general and unified theoretical treatment. The new conversion inequality—based on a telescoping analysis of projected gradients—sharpens previous results such as those by Hazan et al. (2017) and Cutkosky (2022)\n\n2. The proposed conversion yields a tighter upper bound, improving constants and eliminating dependence on boundedness assumptions. \n\n3. The framework is compatible with a broad class of online learning algorithms—including mirror descent, AdaGrad, and optimistic variants—demonstrating wide relevance to both optimization and learning theory.\n\n4. The paper is well-organized, mathematically clean, and pedagogically presented. The proofs are compact yet general enough to apply to diverse online schemes.\n\n5. Experiments, while modest, confirm theoretical predictions: algorithms derived via Go2N achieve faster decrease in gradient norms and improved training stability compared to standard regret-based methods"}, "weaknesses": {"value": "1. The main insight—a refined conversion between regret and stationarity—extends existing frameworks rather than introducing a fundamentally new algorithmic principle. The contribution is primarily theoretical sharpening rather than conceptual breakthrough.\n\n2. Empirical results are confined to small-scale nonconvex problems (e.g., 2-layer linear networks, low-rank factorization). It would be more convincing to include modern deep learning benchmarks to demonstrate practical impact.\n\n3. The analysis still relies on Lipschitz continuity of gradients and smoothness constants. It remains unclear how tight the improved bounds are in practice, especially under adversarial noise or stochastic gradients.\n\n4. The paper could better position itself against recent advances in nonconvex online learning (e.g., Jin et al., 2023; Duchi et al., 2024) and adaptive nonconvex regret analysis, which also aim to establish gradient-based guarantees.\n\n5. The work provides only upper bounds; without matching lower bounds or counterexamples, the “improvement” claim remains qualitative."}, "questions": {"value": "1. Can the conversion framework be extended to constrained or manifold-based nonconvex settings?\n2. How does Go2N perform under stochastic non-i.i.d. gradient noise?\n3. Are there concrete examples where Go2N achieves provably better asymptotic rates than the Cutkosky (2022) reduction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dF7Fy7D2r8", "forum": "ZAflv4dxQ9", "replyto": "ZAflv4dxQ9", "signatures": ["ICLR.cc/2026/Conference/Submission14459/Reviewer_4KMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14459/Reviewer_4KMs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846588860, "cdate": 1761846588860, "tmdate": 1762924860180, "mdate": 1762924860180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}