{"id": "Nbl43eAVaE", "number": 1347, "cdate": 1756874601373, "mdate": 1763704325703, "content": {"title": "MAGREF: Masked Guidance for Any-Reference Video Generation with Subject Disentanglement", "abstract": "We tackle the task of any-reference video generation, which aims to synthesize videos conditioned on arbitrary types and combinations of reference subjects, together with textual prompts. This task faces persistent challenges, including identity inconsistency, entanglement among multiple reference subjects, and copy-paste artifacts. To address these issues, we introduce MAGREF, a unified and effective framework for any-reference video generation. Our approach incorporates masked guidance and a subject disentanglement mechanism, enabling flexible synthesis conditioned on diverse reference images and textual prompts. Specifically, masked guidance employs a region-aware masking mechanism combined with pixel-wise channel concatenation to preserve appearance features of multiple subjects along the channel dimension. This design preserves identity consistency and maintains the capabilities of the pre-trained backbone, without requiring any architectural changes. To mitigate subject confusion, we introduce a subject disentanglement mechanism which injects the semantic values of each subject derived from the text condition into its corresponding visual region. Additionally, we establish a four-stage data pipeline to construct diverse training pairs, effectively alleviating copy-paste artifacts. Extensive experiments on a comprehensive benchmark demonstrate that MAGREF consistently outperforms existing state-of-the-art approaches, paving the way for scalable, controllable, and high-fidelity any-reference video synthesis. The code and video demos are available in the supplementary materials.", "tldr": "", "keywords": ["Video generation; Diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a78e6c6d7c1c4f3f4bd2500069a98395c8c33785.pdf", "supplementary_material": "/attachment/535323e9c9270387046e58b902eb18b2d2b6c3fd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MAGREF, a unified framework for any-reference video generation, addressing challenges such as identity inconsistency, subject entanglement, and copy-paste artifacts. The method integrates masked guidance and subject disentanglement mechanisms, along with a four-stage data pipeline. Experiments show that MAGREF achieves superior performance over existing methods, demonstrating its effectiveness for scalable for  subject-driven video synthesis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The citations are comprehensive, suggesting that the authors are well-versed in the field of subject-driven generation.\n\n\nThe experiments are extensive, encompassing various open-source and commercial baselines, including both single-subject and multi-subject settings. These results clearly demonstrate the effectiveness of MAGREF."}, "weaknesses": {"value": "If the base image-to-video model does not use channel concatenation to fuse the reference images, the effectiveness of this method would be limited."}, "questions": {"value": "(1)  The overall spatial resolution for all subjects is fixed. During inference, how are reasonable spatial resolutions allocated to different subjects? If a subject is very large, such as the background, this implies that the spatial resolution for other subjects will be scaled down, which may lead to information loss.\n\n(2) In Equation (6), is $M^k_{\\text{sub}}$  a typographical error? Should it instead be written as $M^i_{\\text{sub}} $?\n\n(3) Is Equation (6) also applied during inference? In addition, can Equation (6) be extended to scenarios involving subject interactions, where the subject masks may overlap?\n\nAddressing my concerns would lead me to reconsider and potentially raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Skbzkwi6nd", "forum": "Nbl43eAVaE", "replyto": "Nbl43eAVaE", "signatures": ["ICLR.cc/2026/Conference/Submission1347/Reviewer_pDBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1347/Reviewer_pDBr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832709736, "cdate": 1761832709736, "tmdate": 1762915744132, "mdate": 1762915744132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "We sincerely thank all the reviewers for their constructive comments and recognition of this work. We have taken all suggestions seriously and conducted extensive experiments. The revised content has been highlighted in blue in the updated manuscript. Below is a summary of the main responses and improvements:\n\n1. Added a quantitative table on reference image scalability experiment.\n2. Included a qualitative experiment on reference image scalability in Appendix Figure 7.\n3. Analyzed a computational token comparison as the number of reference images increases.\n4. Added a table comparing different I2V concatenation methods.\n5. Added a table showing the effect of explicit position prompts on the left-right spatial relationship.\n6. Added more qualitative comparison cases in the supplementary materials.\n\nWe hope our responses below convincingly address all reviewers’ concerns. We thank all reviewers’ time and efforts again!"}}, "id": "j7q65T4ntV", "forum": "Nbl43eAVaE", "replyto": "Nbl43eAVaE", "signatures": ["ICLR.cc/2026/Conference/Submission1347/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1347/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1347/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704655434, "cdate": 1763704655434, "tmdate": 1763704655434, "mdate": 1763704655434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARGREF,  a unified masked guidance design by combining region-aware masking with pixel-wise channel concatenation, to inject reference cues at the channel level. It further proposes a subject disentanglement mechanism that maps text semantics to their corresponding visual regions, cleanly separating identities and mitigating cross-reference confusion without extra identity modules. The model achieved SOTA performance in consistent subject-driven video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposed a novel structure to condition video generation on multiple images by combining multiple images into one. It also proposed data-pipeline to collect large-scale\n- The proposed subject disentanglement mechanism is novel and effective in text-prompt alignment for different subjects in the image.\n- The results show that the proposed framework is better than other baselines. Ablation study proves that all the proposed module is meaningful.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "The paper is in general good, some minor points:\n- The computational power is not mentioned e.g. how many GPUs have been used\n- No details about the dataset e.g. source/size and possible privacy problem for human face data"}, "questions": {"value": "- What is the value of C_m?\n- What is the base T2V model that MAGREF is fine-tuned on?\n- For the composed reference image, does the resolution matter?\n- For the composed reference image, does the organization matter?\n- In 4.3, what is result in Table 3 / 4 tested on? the score seems different from Table 1 / 2?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "hidden concern for use of human videos"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dFfjxfW0jn", "forum": "Nbl43eAVaE", "replyto": "Nbl43eAVaE", "signatures": ["ICLR.cc/2026/Conference/Submission1347/Reviewer_B9UU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1347/Reviewer_B9UU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901081896, "cdate": 1761901081896, "tmdate": 1762915743958, "mdate": 1762915743958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To solve the issue of any-reference video generation task, this paper proposes a unified framework, MAGREF. The framework contains region-aware masking mechanism and subject disentanglement mechanism. Experiments show the scalable, controllable, and high-fidelity any-reference video synthesis results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  The writing is easy to understand, and the painting is well-drawn.\n-  The proposed region-aware masking method preserves subject identity without backbone changes.\n-  Experimentally, the paper achieves best single-ID and multi-subject score."}, "weaknesses": {"value": "- 1.\tIn this paper, multi-subjects are introduced into the videos through a blank canvas. Several subjects are directly added to the canvas with their pixels values. Although this function is useful, my major concerns are listed below:\n -      a)\tThe canvas size is limited. How many subjects can be placed on the canvas without harming the model’s generation ability?\n -      b)\tThe positions of these subjects are randomly shuffled during training, in my opinion, the locations of different subjects may contain implicit relationship, such as decide the distance of two subjects in the generated videos. However, this is not discussed in the paper and related ablations are not considered.\n -      c)\tSimilar to the absolute locations, the scale of each subject in the canvas is still missing. Because such model relies on the explicit vision cues to catch the details of the reference subjects, the scale is an important factor that needs to be considered.\n- 2. In subsection “Pixel-wise channel concatenation”, the composited image I_{comp} is encoded by VAE encoder and then concatenated with noised video latents along the channel dimension. I cannot find technically something new that differs from existing methods. Existing methods also apply the pixel-wise image/video with VAE encoder and concatenate the latent with noised video.\n- 3. The paper claims that the methods support arbitrary subject categories in Lines 098-103, but in their methods, it is unclear how the model support such ability.\n- 4. The qualitative comparison in Fig. 5 seems cannot demonstrate the superiority of the proposed methods, such as compared with close-source method Kling1.6 or the open-source method VACE. Besides, why does the multi-subject result of Skyreels method shows show a poor first frame?\n- 5. The experimental results lack more deep analysis to explain why the proposed methods outperform the previous methods in qualitative and quantitative results.\n-6. The failure cases and more analyses should be discussed."}, "questions": {"value": "See above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VkFUwNvo6o", "forum": "Nbl43eAVaE", "replyto": "Nbl43eAVaE", "signatures": ["ICLR.cc/2026/Conference/Submission1347/Reviewer_fwvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1347/Reviewer_fwvD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908264898, "cdate": 1761908264898, "tmdate": 1762915743817, "mdate": 1762915743817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MAGREF, a framework for any-reference video generation that combines region-aware masked guidance with subject disentanglement to support arbitrary combinations of human, object, and environment references. It achieves superior identity preservation and visual quality compared to open-source and proprietary baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Technically sound: Masked guidance and pixel-wise concatenation are simple yet effective extensions of I2V backbones.\n\n2. Comprehensive results: The experiments and ablations clearly support the claimed improvements."}, "weaknesses": {"value": "1. My major concern is that the pixel-wise channel concatenation may limit scalability when the number of reference subjects grows. It is unclear how the model handles more subjects simultaneously. Would temporal or latent-level concatenation yield more flexible conditioning in such cases?\n2. While pixel-wise concatenation effectively preserves subject appearance, it may inherently limit global-level customization. Since it injects spatially grounded features, the model mainly captures concrete subject geometry rather than abstract global styles (e.g., tone, lighting, art style). Temporal or latent-level fusion could offer more flexibility for such style control. Can this framework be extended to global element customization, such as atmosphere, or texture?"}, "questions": {"value": "As seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XWOgzJXNXz", "forum": "Nbl43eAVaE", "replyto": "Nbl43eAVaE", "signatures": ["ICLR.cc/2026/Conference/Submission1347/Reviewer_YBDZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1347/Reviewer_YBDZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1347/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974887060, "cdate": 1761974887060, "tmdate": 1762915743618, "mdate": 1762915743618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}