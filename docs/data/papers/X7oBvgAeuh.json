{"id": "X7oBvgAeuh", "number": 682, "cdate": 1756770239256, "mdate": 1759898247485, "content": {"title": "FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning", "abstract": "Medical artificial intelligence systems have achieved remarkable diagnostic capabilities, yet they consistently exhibit performance disparities across demographic groups, causing real-world harm to underrepresented populations. While recent multimodal reasoning foundation models have advanced clinical diagnosis through integrated analysis of diverse medical data, reasoning trainings via reinforcement learning inherit and often amplify biases present in training datasets dominated by majority populations. We introduce Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach that promotes equitable learning across heterogeneous clinical populations. FairGRPO employs adaptive importance weighting of advantages based on representation, task difficulty, and data source. To address the common issue of missing demographic labels in the clinical domain, we further employ unsupervised clustering, which automatically discovers latent demographic groups when labels are unavailable. Through comprehensive experiments across 7 clinical diagnostic datasets spanning 5 clinical modalities across X-ray, CT scan, dermoscropy, mammography and ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2% against all vanilla and bias mitigated RL baselines, while improving F1 score by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO progressively improves fairness throughout optimization, while baseline RL methods exhibit deteriorating fairness as training progresses. Based on FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance while demonstrating significantly reduced disparities across demographic groups. Our code, models, and fairness evaluation framework are publicly available at this anonymous link: https://anonymous.4open.science/r/fairness_submission-D923/.", "tldr": "FairGRPO is a new RL method that reduces demographic disparities in medical AI while improving diagnostic accuracy, making clinical AI systems more equitable for underserved populations.", "keywords": ["Machine Learning", "AI for Healthcare", "Multimodal Learning", "Reinforcement Learning", "Fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/402f2e3025fba02b2bd7eeda01e1c436a5a1b7cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this manuscript, the authors focus on an important but under-explored task, fairness in medical artificial intelligence systems. The authors aim to improve the fairness and effectiveness during the reinforcement learning phase of vision language models. And they propose to modify the advantages to address the limitations. Through experiments on 7 clinical diagnostic datasets with 2 representative VLMs, the authors demonstrate the effectiveness of their proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths**\n1. This manuscript focuses on a under-explored but important task, mitigating unfairness in the medical reasoning.\n\n2. The experimental datasets are comprehensive. The authors collect 7 clinical diagnostic dataset spanning 5 clinical modalities, which is can adequately verify whether the method is effective.\n\n3. The experimental section considers a comprehensive set of dimensions, including effectiveness, fairness, efficiency, case study, and others."}, "weaknesses": {"value": "### **Weaknesses**\n\n1. My primary concern regarding this paper is the lack of sufficient novelty in the proposed approach. The modification of advantages is easy and lack of theoretical analysis. And the clustering is a common approach in fairness machine learning without demographic information.\n\n2. The authors conduct experiments on Qwen-2.5-VL-7B and MedGemma-4B. Although these two models are representative, it is better to conduct more experiments on more VLMs."}, "questions": {"value": "### **Questions**\n\n1. Could the authors provide more theoretical or empirical analysis of the proposed method/motivation?\n\n2. How about the performance of the proposed FairGRPO on the other mainstream VLMs?\n\n3. Could the authors provide more case studies to prove the importance of fairness in medical AI systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OvpQPjEcFs", "forum": "X7oBvgAeuh", "replyto": "X7oBvgAeuh", "signatures": ["ICLR.cc/2026/Conference/Submission682/Reviewer_PPRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission682/Reviewer_PPRM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899442772, "cdate": 1761899442772, "tmdate": 1762915581143, "mdate": 1762915581143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Modern medical AI systems may show performance disparities wrt demographic groups (e.g., by race, gender, age) because training data is heavily skewed toward majority groups. This paper addresses fairness in reinforcement learning for clinical reasoning. The paper proposes a method that weighs samples according to the representation of the demographic group (underrepresented groups get higher weight),  task difficulty, and data source. Whenever demographic labels are not available, clustering is used to create implicit demographic groups. The technique is used to train a multi-modal clinical model, FairMedGemma-4B and its performance is analyzed for different clinical tasks. The performance across demographic groups tends to be more homogeneous. In addition, the fairness continually improves during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "One of the first works to embed group fairness considerations into the RL for clinical reasoning models"}, "weaknesses": {"value": "Not clear how the group fairness influences individual performance, which is what is most important."}, "questions": {"value": "Do you have an intuition on how the group fairness policies affect individual metrics of fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kh2JtwEIyc", "forum": "X7oBvgAeuh", "replyto": "X7oBvgAeuh", "signatures": ["ICLR.cc/2026/Conference/Submission682/Reviewer_TS5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission682/Reviewer_TS5D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931941850, "cdate": 1761931941850, "tmdate": 1762915580856, "mdate": 1762915580856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FairGRPO, a reinforcement learning algorithm that mitigates demographic bias in clinical vision–language models. FairGRPO adaptively re-weights learning signals based on representation, task difficulty and data source, and uses unsupervised clustering to infer latent demographic groups when labels are missing. Evaluated on seven multimodal clinical datasets with Qwen-2.5-VL-7B and MedGemma-4B, the method improves both fairness and accuracy over RL and fairness mitigation baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses fairness in reinforcement learning for multimodal clinical models, a critical but underexplored area. By focusing on fairness in critic-free RL (e.g., GRPO-style optimization), the work bridges the gap of fair ML for healthcare communities.\n2. The proposed method of scaling advantages inversely by group representation and task difficulty is simple to compute and adds negligible runtime overhead.\n3. Experiments span seven clinical datasets covering diverse imaging types, which demonstrates the generalization ability of FairGRPO.\n4. The paper presents fairness trajectories during optimization, showing that FairGRPO progressively improves fairness instead of degrading it, as seen in baseline RL methods. The qualitative case studies vividly demonstrate how fairness-aware training improves the model’s reasoning trace and diagnostic correctness."}, "weaknesses": {"value": "1. The theoretical grounding for the proposed fairness optimization is limited. It is unclear whether the scaling guarantees convergence or prevents overcompensation.\n2. The paper employs K-means clustering on reward vectors to infer latent demographic groups, but does not analyze the robustness of this clustering step. The number of clusters, initialization, and metric choice could influence group assignments, potentially introducing instability or even amplifying unintended biases in underrepresented groups. \n3. The design of group discovery lacks interpretability. The relation between the learned task-specific difficulty patterns and demographic groups is unclear. Adding some case studies may help validate the design of the group discovery.\n4. In Table 2, the improvement in fairness and task performance is limited and inconsistent. On MedGemma-4B, for fairness-unaware baselines, there is little or no fairness improvement compared with vanilla GRPO in EOD and FPR diff. For fairness-aware baselines, there is little or no task improvement compared with GRPO+DRO in Acc and F1. Some discussion of such observations may be added."}, "questions": {"value": "1. See some questions in Weaknesses.\n2. This paper focuses on fairness in medical AI systems, but it gives limited attention to how FairGRPO’s reasoning outputs could integrate with real-world clinical workflows. Including a discussion of interpretability would make the contribution more actionable for medical AI practitioners.\n3. I did not find the released FairMedGemma-4B. Maybe I'm missing something?\n4. Typo in line 370 \"faieness\" -> \"fairness\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cUdaz11E0z", "forum": "X7oBvgAeuh", "replyto": "X7oBvgAeuh", "signatures": ["ICLR.cc/2026/Conference/Submission682/Reviewer_Tq3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission682/Reviewer_Tq3Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968022791, "cdate": 1761968022791, "tmdate": 1762915580710, "mdate": 1762915580710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Fairness-aware Group Relative Policy Optimization (FairGRPO), a reinforcement learning approach designed to train multimodal reasoning foundation models with reduced performance disparities across demographic groups. The method aims to mitigate biases and promote equitable clinical decision-making.\n\nFairGRPO extends GRPO by introducing an adaptive importance-weighting mechanism that normalizes rewards using inverse temperature scaling, where the scaling factor depends on each group’s representation size and performance. Groups can be either explicitly defined (via demographic labels) or implicitly discovered. In the latter case, the authors apply K-means clustering on reward-based representations, selecting the number of clusters using the elbow method. The training objective follows GRPO’s policy gradient formulation with clipped importance sampling.\n\nThe authors also release FairMedGemma-4B, a fairness-aware clinical vision–language model trained with FairGRPO. It achieves state-of-the-art diagnostic performance while significantly reducing disparities across demographic groups, as demonstrated on seven clinical datasets spanning five imaging modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe paper addresses fairness in reinforcement learning for multimodal foundation models, a highly relevant problem in clinical reasoning where demographic disparities can have critical implications.\n\n•\tThe presentation is clear and easy to follow, with well-explained formulations and experimental design.\n\n•\tFairGRPO extends Group Relative Policy Optimization with an adaptive importance-weighting mechanism that normalizes rewards via inverse-temperature scaling based on group size and performance. The approach is conceptually sound and well motivated by recent fairness literature.\n\n•\tThe authors also consider cases where explicit demographic labels are unavailable, proposing a clustering strategy on reward-based representations to infer latent groups.\n\n•\tExperiments on seven clinical datasets spanning five imaging modalities show consistent fairness gains and competitive or superior accuracy. The release of FairMedGemma-4B, a fairness-aware clinical vision–language model, further strengthens the paper’s practical contribution."}, "weaknesses": {"value": "1. The clustering-based grouping used when demographic labels are unavailable is intuitive but not further analyzed. It remains unclear what the clusters capture in practice, or under which conditions they would align with meaningful demographic or clinical subgroups. \n\n2. The proposed FairGRPO objective is reasonable and empirically effective, but a theoretical analysis or discussion of the training objective and convergence behavior would provide a deeper understanding of its effect, rather than relying solely on intuition and empirical evidence.\n\n3. Potential overfitting or generalization issues when up-weighting minority groups are not discussed. It would be helpful to analyze whether the scaling mechanism might amplify noise in small or underrepresented populations.\n\n4. Table 2 does not include standard deviations. Also, a breakdown of performance for each dataset (similar to Table 2 but disaggregated) would improve transparency.\n\n5. The information in Tables 4–17, which report dataset- and group-level metrics, is difficult to parse in its current format, even though these results seem to be relevant to understand the performance at a dataset/group level for each method. Could these be summarized?"}, "questions": {"value": "Please, see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "isvQrDGqeZ", "forum": "X7oBvgAeuh", "replyto": "X7oBvgAeuh", "signatures": ["ICLR.cc/2026/Conference/Submission682/Reviewer_cwor"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission682/Reviewer_cwor"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017017468, "cdate": 1762017017468, "tmdate": 1762915580563, "mdate": 1762915580563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}