{"id": "f17aMuQIPz", "number": 24245, "cdate": 1758354554465, "mdate": 1759896774505, "content": {"title": "INPO: Image-based Negative Preference Optimization for Concept Erasure in Text-to-Image Diffusion Models", "abstract": "Text-to-image diffusion models have achieved remarkable generative performance, yet they are susceptible to memorizing and reproducing undesirable concepts, such as NSFW content or copyrighted material. While concept erasure has emerged as a promising approach to remove undesirable concepts from pre-trained models, existing methods still suffer from prompt-dependence, architecture-dependence, and unstable training dynamics, which limit their effectiveness and generalization. In this work, we propose Image-based Negative Preference Optimization (INPO), a novel model-agnostic framework for concept erasure that unifies joint image–text supervision under a principled preference optimization paradigm. By formulating the target concept as a negative preference, INPO inherits the stable optimization dynamics of Negative Preference Optimization (NPO), thereby mitigating the instability of prior gradient-ascent-based methods. To achieve precise and controllable erasure, INPO further incorporates a concept mask for localized suppression and an adaptive negative scaling strategy that dynamically modulates optimization strength according to erasure progress. Extensive experiments on the latest FLUX model demonstrate that INPO achieves precise and consistent erasure across a variety of tasks, including object, IP, style and NSFW content, while preserving the model’s overall generative capabilities, highlighting the robustness, reliability and practical applicability of INPO for safe and controllable image generation.", "tldr": "", "keywords": ["Diffusion Model", "Concept Erasure"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73d212fa4542eb77f553a0f38245092adbbb2a15.pdf", "supplementary_material": "/attachment/0348549292c68ed28f74dea3610307a048df8f7d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposed a method to erase concepts from Diffusion based generative models using an adaptation of negative preference optimization. The report results on nudity, objects and style erasure on FLUX.1-Dev."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- \"Adaptive Erasure Trajectories\" in section 4.3 is very interesting and makes the approach easily useable without defining a target concept. \n- The results on applying NPO for concept erasure are promising and can be beneficial for future research.  \n- The paper in general is well written and is easy to follow."}, "weaknesses": {"value": "- The authors talk about \"Architecture-dependence\" in the introduction as motivation behind their approach but proceed to only show results on FLUX. \n- The experiments evaluations are lacking. I'm in particular interested in how the approach benchmarks on SDv1.4 on adversarial prompt datasets (Ring-A-Bell, MMA Diffusion, etc). Especially in comparison with newer approaches such as AdvUnlearn, AGE. \n- The paper talks about prompt-dependence but fails to cite or acknowledge [1].\n- There are many relevant works that have not been cited. While I am not listing all here, please add relevant works. \n\n[1] Pham, Minh, et al. \"Prompt-Agnostic Erasure for Diffusion Models Using Task Vectors.\""}, "questions": {"value": "- How do you find the concept masks M? \n- What is the performance on multi-concept erasure such as on celebrity 100 dataset from MACE [1]? \n- Have you re-run red-teaming attacks on FLUX or have you reused prompt sets that were optimized for SDv1.4? I'm surprised to see such low numbers for the baseline nudity generation on FLUX. \n\n[1] Lu, Shilin, et al. \"Mace: Mass concept erasure in diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xOWPLTEwhB", "forum": "f17aMuQIPz", "replyto": "f17aMuQIPz", "signatures": ["ICLR.cc/2026/Conference/Submission24245/Reviewer_iHuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24245/Reviewer_iHuL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760920528399, "cdate": 1760920528399, "tmdate": 1762943015483, "mdate": 1762943015483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a preference optimization approach for concept erasure in text-to-image diffusion models. This paper formulates the unwanted concept as a negative preference, and then applies negative preference optimization to steer the model to remove the unwanted concepts. The proposed method also designs concept masks and an adaptive negative scaling strategy to improve the unlearning quality. The experiments are conducted on several types of concept erasure tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Concept erasure is crucial and practical for real-world trustworthy generative model developments.\n2. The proposed method is reasonable to formulate the concept erasure as a negative preference optimization problem.\n3. The overall paper is easy to follow and well structured."}, "weaknesses": {"value": "1. While the proposed method can erase the unwanted concepts described by the target prompts, it remains unclear whether the proposed method is able to handle the rephrased prompts that can be used to recover the target concepts. It would be beneficial to discuss or clarify this potential robustness concern. \n2. The proposed approach relies on Eq.14 to preserve the unrelated concepts that are not affected. However, how to decide the preservation set? It seems impractical to cover every concept in this preservation set. More clarifications or explanations for this concern are helpful."}, "questions": {"value": "1. Can this proposed method be applicable to handle multi-concept erasure scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XtPkQUD3Zt", "forum": "f17aMuQIPz", "replyto": "f17aMuQIPz", "signatures": ["ICLR.cc/2026/Conference/Submission24245/Reviewer_yYkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24245/Reviewer_yYkH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761068217684, "cdate": 1761068217684, "tmdate": 1762943014801, "mdate": 1762943014801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces INPO, a framework for concept removal in text-to-image diffusion models. It extends Negative Preference Optimization (NPO), originally proposed for LLM unlearning, to the text-to-image diffusion setting, where the target concept to remove is treated as the negative preference.  The method further incorporates (1) a concept mask to spatially focus the loss on relevant regions and (2) an adaptive negative scaling strategy that reduces gradient strength once the target concept is sufficiently suppressed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed adaptation of NPO to text-to-image diffusion models is intuitive and empirically effective. It achieves strong performance across diverse erasure types (object, IP, style, NSFW) and remains stable on modern architectures like FLUX.\n* Evaluation against red-teaming attacks (MMA-Diffusion, P4D, etc.) is impressive and shows practical robustness."}, "weaknesses": {"value": "1. It would be great to have an evaluation to include prior benchmarks (as used in ESD, CA, or EA) with a broader set of prompts across multiple concepts. In addition, listing the exact prompts used for current experiments would further enhance reproducibility.\n2. An ablation on the role of eta, gamma, and tau (Eq. 12) would be useful, especially since Table 5 in the Appendix shows different settings (e.g., η=3 for style removal vs. 1 for other types). This would help assess the robustness and interpretability of the adaptive scaling.\n3. Section 3.2 defines a prior loss over concepts, c′,  to preserve, but it is not specified which concepts or datasets are used for this (COCO? random prompts?). Adding more details regarding this would help improve the reproducibility of the method. \n4. Reporting some quantitative measure (e.g., CLIP similarity to neighboring concepts before/after erasure) would clarify whether INPO doesn’t affect any nearby concepts, e.g., Monet style when removing Van Gogh. \n\n Minor points:\n1. It's unclear what the mask should be for abstract or global attributes such as artistic style or NSFW tone. A clarification and any ablation regarding this would help."}, "questions": {"value": "Please look at the weakness section. Specifically, if the evaluation checks the generation on a wide variety of prompts for each target concept. Or maybe including the performance on the same set of target concepts and prompts as done in previous works. In addition, clarifying some of the implementation details would help strengthen the paper and improve its reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fJNCGxoNZH", "forum": "f17aMuQIPz", "replyto": "f17aMuQIPz", "signatures": ["ICLR.cc/2026/Conference/Submission24245/Reviewer_w843"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24245/Reviewer_w843"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554883453, "cdate": 1761554883453, "tmdate": 1762943013882, "mdate": 1762943013882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon Negative Preference Optimization (NPO) and introduces Image-based Negative Preference Optimization (INPO), a model-agnostic framework for concept erasure in text-to-image diffusion models. To achieve precise concept erasure, it introduces a concept mask and an adaptive negative scaling strategy that dynamically adjusts the erasure strength based on the model’s learning state. In addition, it includes a prior preservation loss to retain the model's generative capabilities for non-target concepts.\nExperiments are conducted across various domains, including objects, copyrighted content, artistic styles, and inappropriate content."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of using relative score difference as an indicator to adaptively control the erasure strength is novel.\n- The paper provides comprehensive comparison with prior concept erasure methods under diverse red-teaming settings, showing consistent results."}, "weaknesses": {"value": "- The overall contribution and novelty are limited. The work primarily adapts NPO’s objective to diffusion models for concept erasure, and the use of the concept mask is relatively straightforward.\n- The paper lacks theoretical or empirical validation for the claimed instability of gradient-ascent-based unlearning methods in diffusion models. In the original NPO paper, a toy experiment was conducted to compare forget quality, model utility, and divergence rate between gradient-ascent- and NPO-trained LLMs, but similar analyses are missing here.\n- The evaluation scope for object, IP, and identity erasure is narrow. For object erasure, only 3 objects are tested, while prior works (e.g., MACE, ESD, UCE, RECE, EAP) evaluated 10 objects from either CIFAR-10 or Imagenette and reported per-object and overall results. Similarly, IP and identity erasure are evaluated on only 1 or 2 concepts, which is insufficient to demonstrate effectiveness. Moreover, IP and identity erasure should be treated as different domains and evaluated separately.\n- Although the paper claims that the proposed method is model-agnostic, it excludes many major methods (e.g., MACE, RECE) that have been tested on U-Net-based diffusion models (e.g., Stable Diffusion v1.4). Including experiments on such architectures would strengthen the claim of generality. In addition, these methods are not restricted to U-Net-based models. They can still be applied to DiT-based diffusion models as long as the model architecture has a cross-attention mechanism between image and text features.\n- How long does the proposed method take to erase a target concept from a diffusion model?\n- (minor) How are the hyper-parameters $\\eta$, $\\gamma$, and $\\tau$ chosen?\n- (minor) Typos:\n  - Line 173: “acent” -> “ascent”\n  - Line 178: “defined the as NPO” -> “defined the same as NPO”\n  - Line 186: “As Eq. 3” -> “As shown in Eq. 3”"}, "questions": {"value": "See the weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bLTrdU24JX", "forum": "f17aMuQIPz", "replyto": "f17aMuQIPz", "signatures": ["ICLR.cc/2026/Conference/Submission24245/Reviewer_zvWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24245/Reviewer_zvWY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922997372, "cdate": 1761922997372, "tmdate": 1762943013561, "mdate": 1762943013561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}