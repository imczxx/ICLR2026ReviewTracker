{"id": "4GiBscHW1k", "number": 24714, "cdate": 1758359579509, "mdate": 1759896753088, "content": {"title": "Meta-RL Induces Exploration in Language Agents", "abstract": "Reinforcement learning (RL) has enabled the training of Large Language Model (LLM) agents to interact with the environment and to solve multi-turn longhorizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LAMER, a general Meta-RL framework that enables LLM\nagents to actively explore and learn from the environment feedback at test time. LAMER consists of two key components: (i) a cross-episode training framework to encourage exploration and long term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across four different environments demonstrate that LAMER significantly improves performance over RL baselines, with more than 13% gains on Sokoban and more than 20% gains on MineSweeper and Webshop. It also generalizes better when evaluated on more challenging or previously unseen environments compared to the RL trained models. Overall, our results demonstrate that meta-reinforcement learning provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.", "tldr": "", "keywords": ["Large Language Model", "Agent", "Reinforcement Learning", "Meta Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77bd21140ebb2c35e5964c6a94bc2a2eda3d6cfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a meta-reinforcement learning framework, LaMer, which is designed to induce exploration and adaptation in large language model agents. Unlike conventional single-episode RL setups, LaMer uses a cross-episode training paradigm, where an agent interacts with the same environment across multiple episodes, balancing exploration and exploitation through a trajectory discount factor. The method leverages in-context policy adaptation via reflection, enabling agents to improve within trials without parameter updates.\n\nThe authors evaluate the method on Sokoban, MineSweeper, WebShop, and ALFWorld show that LaMer outperforms prompting-based and RL baselines (PPO, GRPO, GiGPO). It achieves a good marginal performance gain on complex environments and improved generalization to harder or out-of-distribution tasks. The framework introduces a bridge between meta-learning and RL for LLM-based agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel framework design: The cross-episode MetaRL formulation extends standard RL to multi-episode exploration, introducing a new axis of “meta-time” learning that aligns well with LLMs’ in-context adaptation capabilities. The introduction of the trajectory discount factor provides interpretable control over the exploration–exploitation tradeoff.\n\n- Improvements over the current baselines over benchmarks: LaMer consistently outperforms RL baselines across all benchmarks (Table 1), achieving +20% absolute improvement on Minesweeper and WebShop. Figures 3–5 show substantial increases in trajectory diversity and test-time scaling behavior, supporting the exploration hypothesis.\n\n- Good generalization evidence: Demonstrated robustness to task difficulty (Figure 5) and generalization to unseen ALFWorld tasks (Table 2). Shows that meta-learned exploration strategies can generalize beyond the training distribution, which is an important step toward adaptive, open-ended agents.\n\n- Broader impact: Provides a reasonable path to unify RL-based training and in-context learning for future meta-learning language agents. Bridges recent work in test-time compute scaling and reasoning through reflection."}, "weaknesses": {"value": "- Limited novelty in algorithmic components: While the formulation is clean, most components, cross-episode return accumulation and reflection-based adaptation, are incremental combinations of known techniques. The paper could better articulate what is fundamentally new beyond adapting Meta-RL for LLMs.\n\n- Insufficient analysis of failure cases: The paper lacks qualitative examples or ablation studies on when reflection adaptation fails (e.g., misleading feedback loops or hallucinated self-reflection). As well, it does not have analysis of how context length or reflection prompt quality impacts learning efficiency.\n\n- Sequential dependency and compute inefficiency: Section 5.6 notes that MetaRL training is 2–3× slower due to sequential episodes, but does not propose mitigation strategies or quantify the trade-off precisely (e.g., wall-clock vs. sample efficiency).\n\n- Evaluation fairness concerns: It is unclear whether RL baselines had access to equivalent multi-episode feedback or if they were constrained to single-episode objectives, potentially overstating MetaRL’s advantage."}, "questions": {"value": "- Meta-objective design: How sensitive is performance to the trajectory discount factor? Could adaptive scheduling of this factor improve learning stability?\n\n- Reflection mechanism: Did the authors experiment with alternative reflection prompt formats or automatic summarization of past episodes? It would be great to add it in the ablations. \n\n- Training efficiency: Can rollout dependencies be partially parallelized (e.g., via curriculum or pseudo-batch episodes) without breaking credit assignment? It will result higher training efficiency. \n\n- Exploration quality metrics: Besides trajectory diversity entropy, are there other quantitative metrics used to measure exploration effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OUWQLYM0hr", "forum": "4GiBscHW1k", "replyto": "4GiBscHW1k", "signatures": ["ICLR.cc/2026/Conference/Submission24714/Reviewer_Qdy5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24714/Reviewer_Qdy5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760745754758, "cdate": 1760745754758, "tmdate": 1762943172136, "mdate": 1762943172136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Meta-RL framework for LLM agents, combining cross-episode RL and in-context reflection. The idea is to train the model to learn to explore by first gathering information, then exploiting it across the following episodes. Results show clear gains over RL and prompting baselines across Sokoban, MineSweeper, Webshop, and ALFWorld. The approach seems effective at leveraging multi-episode structure and reflection to adapt at test time, though it’s unclear whether the gains truly stem from “learning to explore” or from other confounding factors such as longer contextual horizons or additional information flow between episodes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is clear and well motivated. It’s conceptually simple and general. The evaluation is broad and the study of generalization is interesting. The paper is clear and well-written."}, "weaknesses": {"value": "**What is actually learned?**\nIt’s not so clear to me whether the agent is only conditioned on the reflection from the previous episode or on the previous history too? If conditioned on both, it would be good to ablate the reflection mechanism and see how performance holds. Ablating history and only keeping reflections might be interesting too. \n\nIs the feedback generation capacity trained too? Or just leveraging a frozen model? Maybe the approach doesn’t actually train exploratory behaviors but only improves feedback generation and feedback use in the following episode. \n\nTo make sure it’s not the case, it would be interesting to visualize exploration strategies in the first episode: what are agents doing? Maybe through videos on a website. Looking and analysing generated feedback might be useful too. Overall I would like to know more about the exploratory behaviors this paper claims is being trained. \n\nIt might be interesting too to feed a first episode collected by either 1) random actions, or 2) the base model; then generate feedback and run the second episode with the trained Meta-RL agent. Is there a drop in performance? If so, it means the meta-RL agent truly learned to explore. If not, then exploration doesn’t matter so much, what matters is the feedback offering a “second chance”. \n\nAre the different episodes in a group the same task instance? e.g. with mines in the same spot in mine sweeper? I assume it’s the case, otherwise exploration would be useless, right? In that case, in Sokoban and MineSweeper, it may not be so much about “learning to explore” as it is about giving a second chance the agent can actually leverage (second chances can’t be leveraged in the standard RL condition because there is no inter-episode memory). \n\n\n\n**Missing information about training task distribution**\nIt’s unclear how different training tasks are from each other. If they’re similar, “learning to explore” might just mean reusing the same strategy in all task instances? Are agents adapting their exploration (1st episode) to the task instance? Here again looking at replays would be useful.\n\n**Comparisons**\nThe RL baseline controls for total experience but doesn’t control for the length of experience the agent has access to at any given moment. One way to control for this would be to run the RL agents on longer episodes instead of running it on longer trajectories. \n\n**Generalization to harder tasks**\nI don’t know if you can say that meta-RL generalizes better here: performance drops by similar percentages in both cases. Meta-RL doesn’t “hold better”, it just starts higher. This is a bit of an overclaim. The AlfWorld generalization study is more convincing. \n\n\n**Typos:**\n* fari comparison -> fair\n* Tabel 1 -> Table 1"}, "questions": {"value": "* How is the “empirical distribution over distinct trajectories” computed exactly?\n* Are prompting baselines also given access to memories across episodes? Reflection only? or histories too?\n* In addition to “bold” indicating best performance, it is useful to make other performance that are not statistically significantly worse than the best (e.g. underlined)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qHbR6IHpsY", "forum": "4GiBscHW1k", "replyto": "4GiBscHW1k", "signatures": ["ICLR.cc/2026/Conference/Submission24714/Reviewer_xsGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24714/Reviewer_xsGo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761148808330, "cdate": 1761148808330, "tmdate": 1762943171889, "mdate": 1762943171889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses meta-reinforcement learning (Meta-RL) to train large language models (LLMs) as language agents across multiple game environments. The inner loop of Meta-RL is implemented as a simple reflection step, where the LLM is prompted to analyze and reflect on previous episodes. The outer loop is optimized using the GiGPO algorithm. The authors show that this approach improves performance compared to both RL and prompting-based baselines across various games. Their analysis further suggests that Meta-RL enhances the exploratory behavior of LLM agents, leading them to produce more diverse trajectories."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and easy to follow, with clearly stated hypotheses and a clean experimental design. The idea is simple and promising to help improve LLM-agents to explore in RL tasks. \n\n- It presents strong empirical results showing that meta-RL with reflection substantially improves the performance of LLM agents across multiple game environments."}, "weaknesses": {"value": "- The paper does not analyze the reflections generated by the LLM. Do they make sense? Do they evolve over time? Is performance improvement mainly driven by changes in the policy or in the reflections themselves?\n- The paper does not analyze the reflections generated by the LLM. Do they make sense? Do they evolve over time? Is performance improvement mainly driven by changes in the policy or in the reflections themselves?\n- All experiments are conducted with a single LLM (Qwen3-4B). It would be important to test whether the results generalize across different model architectures and sizes.\n- Lack of technical details:\n    - The paper does not explain how trajectory diversity is computed.\n    - Key training hyperparameters (e.g., batch size, learning rate, training budget) are not reported.\n    - For procedurally generated environments such as Sokoban, it is unclear whether the same random seeds are used across episodes within a task—both in the RL and meta-RL settings."}, "questions": {"value": "- Would your method also apply to more classical reasoning tasks where LLMs are trained with RL, such as MATH, BigBench, or similar benchmarks?\n\n- Do you think your approach could work even without the reflection step — relying solely on in-context learning from previous trajectories provided in the prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uSIccfCpsB", "forum": "4GiBscHW1k", "replyto": "4GiBscHW1k", "signatures": ["ICLR.cc/2026/Conference/Submission24714/Reviewer_73EN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24714/Reviewer_73EN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668188029, "cdate": 1761668188029, "tmdate": 1762943171669, "mdate": 1762943171669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the LAMER meta-RL algorithm which makes agents explore better. The algorithm involves training the agent on sequences of related expisodes. After each algorithm there is a reflection step where the agent can write notes which are given to figure episodes. Since the meta-rl algorithm rewards the agent for reward across the entire sequence, the agent is incentivized to explore in earlier episodes and exploit in later ones.\n\nThey evaluate on Sokovan, Minesweeper, Alfworld, and Webshop, and find that LaMer produces both better exploration/diversity and higher overall reward."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and is written clearly to explain how the authors are tackling the problem of exploration during RL.\n\nWhile the Meta-RL framework itself is not novel, the application to LLMs is. The paper shows significant gains over single episode training across multiple benchmarks. The paper also shows out of distribution generalization compared to non meta learning on unseen benchmarks."}, "weaknesses": {"value": "See the \"questions\" section.\n\nBeyond this I would be interested in comparing against pass@k metrics for meta-exploration that have previously been explored in RL (for example, see Walder et. al Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems).  (I do not think this is necessary for this to be a good paper! Just a suggestion for extension.)"}, "questions": {"value": "- Why are there no seeds or error bars? RL experiments can have quite a lot of variance and it is typical to do at least 3 experimental seeds for each run.\n- How much of the gain is attributed to the self reflection prompting versus the multi episode credit assignment?\n   - Would be interesting to see an ablation of whether the reflection step is important or if merely showing the agent past episodes is sufficient.\n- Qualitatively, did you notice the sampled trajectories to be more diverse? Entropy or similarity is an easily hackable objective. I'd also appreciate a bit more description on how exactly diversity was calculated, this was not entirely clear to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YupAlSFk7D", "forum": "4GiBscHW1k", "replyto": "4GiBscHW1k", "signatures": ["ICLR.cc/2026/Conference/Submission24714/Reviewer_3QcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24714/Reviewer_3QcN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116481464, "cdate": 1762116481464, "tmdate": 1762943171458, "mdate": 1762943171458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}