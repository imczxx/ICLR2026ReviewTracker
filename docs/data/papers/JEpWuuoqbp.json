{"id": "JEpWuuoqbp", "number": 785, "cdate": 1756818087630, "mdate": 1759898242023, "content": {"title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "abstract": "In the realm of image style transfer, existing algorithms relying on single reference style images encounter formidable challenges, such as severe semantic drift, overfitting, color limitations, and a lack of a unified framework. These issues impede the generation of high quality, diverse, and semantically accurate images. In this study, we introduce StyleWallfacer, an innovative unified training and inference framework, which not only addresses various issues encountered in the style transfer process of traditional methods but also unifies the framework for different tasks. This framework is designed to advance the development of this field by enabling high-quality style transfer and text driven stylization. First, we propose a semantic-based style injection method that uses BLIP to generate text descriptions strictly aligned with the semantics of the style image in CLIP space. By leveraging a large language model to remove style-related descriptions from these descriptions, we create a semantic gap. This gap is then used to fine-tune the model, enabling efficient and drift-free injection of style knowledge. Second, we propose a data augmentation strategy based on human feedback, incorporating high-quality samples generated early in the fine-tuning process into the training set to facilitate progressive learning and significantly reduce its overfitting. Finally, we design a training-free triple diffusion process using the fine-tuned model, which manipulates the features of self-attention layers in a manner similar to the cross-attention mechanism. Specifically, in the generation process, the key and value of the content-related process are replaced with those of the style-related process to inject style while maintaining text control over the model. We also introduce query preservation to mitigate disruptions to the original content. Under such a design, we have achieved high-quality image-driven style transfer and text-driven stylization, delivering high-quality style transfer results while preserving the original image content. Moreover, we achieve image color editing during the style transfer process for the first time, further pushing the boundaries of controllable image generation and editing technologies and breaking the limitations imposed by reference images on style transfer. Our experimental results demonstrate that our proposed method outperforms state-of-the-art methods.", "tldr": "", "keywords": ["Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be15598f975fe4fcb51034150ffa3ade99831064.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes StyleWallfacer, a unified style transfer framework addressing semantic drift, overfitting, and color limits via semantic gap–based style injection, human-feedback progressive augmentation, and a training-free triple diffusion process enabling controllable text/image-driven stylization and color editing, outperforming prior one-shot methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.This paper is well-written, easy to follow.\n\n2.The style results look good."}, "weaknesses": {"value": "1.This paper claims that StyleWallfacer is the first text-based color editing method; however, to the best of my knowledge, StyleStudio [1] had already introduced a text-driven framework that enables users to edit color attributes using natural language prompts.\n\n2.The descriptions in Introduction about \"The three-body problem\" and \"Wallfacer Plan\" are confused and entirely unrelated to the method proposed in this paper.\n\n3.The proposed framework is basically a combination of existing methods. For example, Style LoRA is widely used in AIGC community and decoupled content prompts are explored in StyleShot[2] and StyleCrafter[3]. Attention-based Style/Text control also used in StyleAligned[4] and AlignedGen[5]. Human feedback data augmentation is used in StyleDrop[6]. This paper produces limited insight to style transfer.\n\n4.The citation command \\cite was incorrectly. \\citet (or \\cite) is used when the author’s name is part of the sentence — the citation is integrated into the text (e.g., Smith (2020) proposed...). \\citep is used when the citation is parenthetical, meaning it appears in brackets as supplementary information (e.g., ...as shown in previous work (Smith, 2020).).\n\n5.Most of the style references in this paper are paintings, lacking higher-level styles such as 3D, line art, and pixel art.\n\n6.Many state-of-the-art (SOTA) style transfer methods, such as AlignedGen[5], Attention Distillation [7], and OmniStyle [8], are not included in the comparison.\n\n[1]StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements, CVPR.\n\n[2]StyleShot:StyleShot: A SnapShot on Any Style, TPAMI.\n\n[3]StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter, TOG.\n\n[4]Style Aligned Image Generation via Shared Attention, CVPR.\n\n[5]AlignedGen: Aligning Style Across Generated Images, NIPS.\n\n[6]StyleDrop: Text-to-Image Generation in Any Style, NIPS.\n\n[7]Attention Distillation: A Unified Approach to Visual Characteristics Transfer, CVPR.\n\n[8]OmniStyle: Filtering High Quality Style Transfer Data at Scale, CVPR."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zY3OI89thc", "forum": "JEpWuuoqbp", "replyto": "JEpWuuoqbp", "signatures": ["ICLR.cc/2026/Conference/Submission785/Reviewer_evmJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission785/Reviewer_evmJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623034405, "cdate": 1761623034405, "tmdate": 1762915604085, "mdate": 1762915604085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StyleWallfacer, a unified framework that tackles key challenges in style transfer by leveraging semantic-driven style injection, progressive data augmentation, and a training-free triple diffusion process to achieve high-quality, diverse stylization with preserved content fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Semantic-Driven Style Injection: A novel method using BLIP and LLMs to create and exploit a semantic gap in CLIP space, enabling precise, drift-free style knowledge injection.\n\n2. Progressive Learning via Human Feedback: An innovative data augmentation strategy that iteratively incorporates high-quality generated samples to reduce overfitting and enhance learning.\n\n3. Training-Free Triple Diffusion Process: A clever inference mechanism that manipulates self-attention features to seamlessly blend style and content while maintaining textual controllability, unifying image-driven and text-driven stylization."}, "weaknesses": {"value": "1. Limited Novelty: The core contribution of this paper is somewhat incremental. The claim of being \"the first\" to achieve color editing during style transfer is overstated, as numerous existing style transfer methods already offer text-guided local or global color control.\n\n2. Incomplete Experimental Comparisons: The chosen baseline methods are not state-of-the-art. To properly validate the proposed method's advantages, comparisons against more recent and advanced techniques are necessary.\n\n3. Factual Inaccuracy in Illustration: There appears to be an error in Figure 2(b), where the text prompt \"a face\" seems misplaced and should likely be \"a church\" to match the content of the image. \n\n4. Non-Standard Terminology: The use of \"HFRL\" is unconventional in the community where \"RLHF\" (Reinforcement Learning from Human Feedback) is the prevalent and standardized term. This non-standard notation may cause confusion for readers."}, "questions": {"value": "1. In the style knowledge injection phase, could you clarify the comparative performance between training a Style LoRA on a pre-collected set of style images versus your iterative collection approach? Which method proves more effective, and was an ablation study conducted to determine this?\n\n2. Given that the proposed method requires inference through three diffusion models, what is its computational efficiency? Could you provide a direct comparison with other methods in terms of both output quality and efficiency?\n\n3. The choice of using \"HFRL\" instead of the more common \"RLHF\" is notable. Could you please explain the rationale behind this terminological decision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9tKH41zl86", "forum": "JEpWuuoqbp", "replyto": "JEpWuuoqbp", "signatures": ["ICLR.cc/2026/Conference/Submission785/Reviewer_XG4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission785/Reviewer_XG4e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625700365, "cdate": 1761625700365, "tmdate": 1762915603954, "mdate": 1762915603954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified style transfer framework called StyleWallfacer which supports both image-driven and text-driven style transfer. StyleWallfacer consists of three key components: a style knowledge injection method based on semantic differences, a progressive learning method based on human feedback for few-shot datasets, and a training-free triple diffusion process that manipulates the features of self-attention layers in a manner similar to the cross-attention mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method supports both image-driven and text-driven style transfer.\n2. This paper aims to address the issues present in existing style transfer methods, such as semantic drift, overfitting, and color limitations, which are both meaningful and challenging.\n3. Extensive experiments are conducted to evaluate the performance of the proposed method."}, "weaknesses": {"value": "1. This paper is not well-organized and lacks clear presentation: i) The abstract and sections like the introduction in this paper are too lengthy and could benefit from being more concise; ii) The main paper lacks a 'Related Work' section, which makes it harder for readers to understand the developments in the style transfer field; iii) The images presented in this paper have some obvious issues. On one hand, they are too small to be clearly seen; on the other hand, the colorful backgrounds added to the figures blend with inside images, making it hard to distinguish the details.\n\n2. The novelty of the proposed method is limited. (i) The proposed data augmentation strategy based on human feedback (which incorporates high-quality samples generated early in the fine-tuning process into the training set to facilitate progressive learning and significantly reduce its overfitting) has been explored in many previous methods, such as StyleDrop [1]. (ii) The proposed training-free triple diffusion process (which replaces the key and value of the content-related process with those of the style-related process to inject style while maintaining text control over the model) share similar key ideas with StyleID. \\\n[1] StyleDrop: Text-to-Image Generation in Any Style. NeurIPS 2023.\n\n3. The baselines selected in this paper are not the most state-of-the-art methods currently available. For example, in the text-driven style transfer task, this paper adopted DreamBooth (2023), LoRA version of DreamBooth, Textual Inversion (2023), and SVDiff (2023) as baselines. However, more state-of-the-art methods such as DEADiff (2024) [2], StyleShot (2025) [3], ArtAdapter (2024) [4], StyleStudio (2025) [5] are all neglected. Similar issues exist in other tasks as well. \\\n[2] DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations. CVPR 2024. \\\n[3] Styleshot: A snapshot on any style. TPAMI 2025. \\\n[4] ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation. CVPR 2024. \\\n[5] StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements. CVPR 2025.\n\n4. In Figures 7 and 8, the method proposed in this paper does not show a clear advantage over other methods.\n\n5. The paper should be self-contained. It is recommended to carefully plan the length of each section and avoid placing important ablation study results in the supplementary materials; instead, they should be included in the main paper."}, "questions": {"value": "Please see **Weaknesses**.\n\nOthers:\n\nWhat is the time efficiency of different methods? Some evaluations regarding this should be conducted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RrtIR8pUdb", "forum": "JEpWuuoqbp", "replyto": "JEpWuuoqbp", "signatures": ["ICLR.cc/2026/Conference/Submission785/Reviewer_nmHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission785/Reviewer_nmHW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641042518, "cdate": 1761641042518, "tmdate": 1762915603775, "mdate": 1762915603775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StyleWallfacer, a high-quality style transfer framework. Its core innovation is a training-free, triple diffusion process that leverages a semantically fine-tuned style LoRA, enabling image stylization and text-based stylization while preserving content structure. The author claimed they effectively overcomes key limitations like semantic drift and overfitting, outperforming existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper demonstrates a substantial experimental effort, including a detailed exploration of hyperparameters.\n\n+ The methodological design of the triple-diffusion process is complex and represents a significant engineering endeavor."}, "weaknesses": {"value": "- Overclaim of Contribution: The claim of a \"unified style transfer framework\" is a significant overstatement. Numerous existing works (e.g., [A,B,C]) have already demonstrated frameworks capable of handling both image- and text-guided style transfer within a single model. The paper does not sufficiently differentiate its \"unification\" from this established literature.\n\n[A] Wang Z, Zhao L, Xing W. Stylediffusion: Controllable disentangled style transfer via diffusion models[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7677-7689.\n\n[B] Wang Y, Liu R, Lin J, et al. OmniStyle: Filtering High Quality Style Transfer Data at Scale[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 7847-7856.\n\n[C] Wang H, Wu P, Rosa K D, et al. Multimodality-guided image style transfer using cross-modal gan inversion[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 4976-4985.\n\n\n- Flawed Motivation on Color Consistency: The paper argues that faithful color reproduction in prior one-shot methods is a limitation. However, color consistency is often an explicit user preference and a desired feature in one-shot style transfer. \n\n- Unsubstantiated Computational Efficiency: The proposed triple-diffusion framework, reliant on multiple parallel denoising paths and inversion techniques, is inherently computationally expensive and memory-intensive. The authors fail to report time consumption and memory usage compared to baseline methods, making it impossible to assess the practical cost of their reported performance gains.\n\n- Impractical and Unjustified Human-Feedback Loop: The introduction of a Human-Feedback Reinforcement Learning (HFRL) stage requiring manual selection of up to 100 samples is highly impractical for real-world style transfer applications. A significant portion of the experiments is conducted under this setting, yet the authors provide no solid ablation studies to demonstrate the critical necessity of this burdensome step for achieving competitive performance.\n\n- Poor Presentation and Readability: The paper lacks a clear, high-level pipeline diagram, making the overall workflow difficult to follow. The quality of the figures is exceptionally poor and visually unprofessional, which severely hinders the ability to evaluate the claimed results."}, "questions": {"value": "Please address the concerns raised in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9YgBxqSmjZ", "forum": "JEpWuuoqbp", "replyto": "JEpWuuoqbp", "signatures": ["ICLR.cc/2026/Conference/Submission785/Reviewer_xEMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission785/Reviewer_xEMm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827965152, "cdate": 1761827965152, "tmdate": 1762915603652, "mdate": 1762915603652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}