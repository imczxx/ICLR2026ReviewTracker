{"id": "90KLDfIjvW", "number": 17327, "cdate": 1758274734819, "mdate": 1759897182107, "content": {"title": "Causal pieces: analysing and improving spiking neural networks piece by piece", "abstract": "We introduce a novel concept for spiking neural networks (SNNs) derived from the idea of *linear pieces* used to analyse the expressivity and trainability of artificial neural networks (ANNs). We show that the input and parameter space of an SNN decomposes into distinct regions — which we call *causal pieces* — where the output spikes are caused by the same subnetwork. For integrate-and-fire neurons with exponential synapses, we prove that, within each causal piece, output spike times are locally Lipschitz continuous with respect to the input spike times and network parameters. Furthermore, we prove that the number of causal pieces is a measure of the approximation capabilities of SNNs. In particular, we demonstrate in simulation that parameter initialisations which yield a high number of causal pieces on the training set strongly correlate with SNN training success. Moreover, we find that SNNs with purely positive weights exhibit a surprisingly high number of causal pieces, allowing them to achieve competitive performance on benchmark tasks. We believe that causal pieces are a powerful and principled tool for improving SNNs, and may also provide new ways of comparing SNNs and ANNs in the future.", "tldr": "We introduce the concept of causal pieces to analyse spiking neural networks (SNNs) and demonstrate that their number is a measure of expressivity.", "keywords": ["Spiking Neural Networks", "Expressivity", "Approximation Theory", "Time-to-First-Spike Coding", "Linear Pieces", "Parameter Initialization"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40fd7bd21b8a8fb9f99c429804d8c844f49d8cb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Causal pieces that is a method to analyze the degree of expressivity of a given SNN. This method is theoretically backed only for a particular type of neurons (IF neurons with exponential synaptic inputs, which fire once at the most) though. It is found that the number of causal pieces is a good measure of SNN expressiveness and the learning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Effort on theoretical understanding of SNNs. This paper theoretically attempts to understand SNNs (time-dependent models with rich dynamics), particularly, their expressiveness and learning capability, in terms of causal pieces. The deduction from theoretical understanding well aligns with the experimental results."}, "weaknesses": {"value": "Considering simple models only: the proposed theoretical analysis is for IF neurons with exponential synaptic inputs, which fire merely once at the most. For realistic neurons that fire the unlimited number of spikes, it is not very straightforward to define causal subnets and pieces since they likely vary upon time. \n\nLimited new findings out of theoretical understanding: The deductions from the understanding are not very surprising, like the relation between SNN expressiveness and number of causal pieces, learning capability and number of causal pieces, and the good learning performance for SNNs with positive weights only. The last one is quite clear even without this theoretical understanding since negative fan-in weights often cause dead neurons that degrade the expressiveness and learning capability. Further, the conclusion was drawn from SNNs on only toy datasets."}, "questions": {"value": "Have the authors attempted to prove the theoretical deductions on datasets of high complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n0a4sQHgci", "forum": "90KLDfIjvW", "replyto": "90KLDfIjvW", "signatures": ["ICLR.cc/2026/Conference/Submission17327/Reviewer_tJr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17327/Reviewer_tJr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884024762, "cdate": 1761884024762, "tmdate": 1762927252735, "mdate": 1762927252735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces “causal pieces,” a way to decompose spiking neural networks into locally Lipschitz regions, provides algorithms to count them, and proves that more pieces imply greater expressivity (under stated nLIF assumptions). Empirically, it shows that initial piece count strongly correlates with training success, that depth, especially early layers, inflates piece counts, and that positive weight SNNs can attain many pieces and competitive accuracy on simple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides up to five contributions.\n\nThe figures are beautiful, the theoretical proofs are solid, and the experiments align well with the claims."}, "weaknesses": {"value": "Writing of Abstract and Introduction. The abstract looks messy. It mainly lists contributions and is almost the same as the second-to-last paragraph in the Introduction. I suggest improving the writing and readability of both sections.\n\n“We believe that causal pieces are a powerful and principled tool for improving SNNs, and may also provide new ways of comparing SNNs and ANNs in the future.” The benefits of the proposed method for comparing SNNs and ANNs are not further described in the main text, which creates a mismatch between the abstract and the main paper."}, "questions": {"value": "1. The writing of Section 3.4 should be reorganized. It describes the experimental setting and results after the demonstration and the argument derived from it.\n\n2. Figure 3. The correlation study uses only a single dataset, which is insufficient to support the conclusion: “In particular, we demonstrate in simulation that parameter initialisations which yield a high number of causal pieces on the training set strongly correlate with SNN training success.”\n\n3. Section 3.6. The results are credible for these experimental setups (flattened inputs, simple architectures), but “competitive” performance is shown only against specific baselines, not state-of-the-art spiking systems or modern vision-task backbones."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vBUnD4YtFW", "forum": "90KLDfIjvW", "replyto": "90KLDfIjvW", "signatures": ["ICLR.cc/2026/Conference/Submission17327/Reviewer_pLzs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17327/Reviewer_pLzs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913279697, "cdate": 1761913279697, "tmdate": 1762927251770, "mdate": 1762927251770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an innovative analytical tool called \"Causal Pieces\" for spiking neural networks (SNNs). The paper demonstrates that the number of causal pieces is a measure of the expressive power of an SNN, and experiments show that the number of causal pieces at network initialization is highly positively correlated with the final accuracy. Furthermore, the paper finds that even SNNs with only positive weights can exhibit a large number of causal pieces and achieve good performance on benchmark tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces and formalizes the \"causal piece,\" a novel and natural abstraction for SNNs that partitions the input-parameter space into discrete regions, each governed by an identical causal subnetwork.\n- Provides rigorous proof that the number of causal pieces serves as a quantitative measure of an SNN’s expressive capacity, directly linking this number to the network's approximation bounds.\n- Empirically demonstrates a strong positive correlation between the number of causal pieces at initialization and final accuracy, offering a principled and highly valuable metric for SNN initialization."}, "weaknesses": {"value": "1. **Model-Specific Limitations**: All theoretical proofs and primary experiments are based on a simplified neuron model: the nLIF, which assumes no leakage and that each neuron fires at most once (single-spike coding).\n\n2. **Questionable Generalizability**: While the authors suggest in the discussion that the \"causal piece\" concept could extend to more common and complex models (e.g., leaky LIF, multi-spike coding, and recurrent SNNs), the paper provides no rigorous proof or experimental support for these claims. This generalization currently remains speculative \"future work.\"\n\n3. **Expressivity vs. Generalization**: The paper proves a link between the number of causal pieces and approximation ability (fitting capability), correlating it with training accuracy. However, the authors rightly concede (end of Sec 3.1) that \"having many pieces does not translate into the SNN generalising well.\" A network with an excessive number of pieces might simply be overfitting the training data. The relationship between causal piece count and the more critical metric of generalization remains unclear.\n\n4. **Limited Experimental Validation**: The empirical evaluation is primarily restricted to relatively simple datasets (like MNIST and Yin Yang). It lacks a systematic comparison on larger-scale, temporally complex tasks against strong, established baselines.\n\n5. **Finding is Conceptually Intuitive**: The positive correlation between the number of \"causal pieces\" (as a measure of partitioning complexity) and the network's expressive power is, at a high level, conceptually intuitive. While the paper provides a valuable, SNN-specific formalization, the core finding itself is not entirely surprising."}, "questions": {"value": "1. **Computational complexity**: For modern large scale networks, what are the time and memory complexities of computing causal pieces with your method?\n\n2. **Saturation vs exponential growth**: Why does the number of pieces in deep networks appear to saturate rather than grow exponentially?\n\n3. **Training dynamics**: During training or across different samples, how does the total number of causal pieces change? Is there a sample level correlation between prediction accuracy and the assigned causal piece regions?\n\n4. **Generalization on more complex datasets**: Beyond training accuracy, when controlling for model size and dataset size, what is the relationship between the number of pieces and test accuracy on datasets such as CIFAR100 or ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pWwW0kr7tb", "forum": "90KLDfIjvW", "replyto": "90KLDfIjvW", "signatures": ["ICLR.cc/2026/Conference/Submission17327/Reviewer_raxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17327/Reviewer_raxD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076666818, "cdate": 1762076666818, "tmdate": 1762927251379, "mdate": 1762927251379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}