{"id": "nNmd4pPSck", "number": 24053, "cdate": 1758352186169, "mdate": 1763582454723, "content": {"title": "A Unifying Perspective on Unsupervised Reinforcement Learning Algorithms", "abstract": "Many sequential decision-making domains, from robotics to language agents, are naturally multi-task on the same set of underlying dynamics. Rather than learning a policy for each task separately, unsupervised reinforcement learning (URL) algorithms pretrain without reward, then leverage that pretraining to quickly obtain performant policies for complex tasks. To this end, a wide range of algorithms have been proposed to explicitly or implicitly pretrain a representation that facilitates quickly solving some class of downstream RL problems. Examples include Goal-conditioned RL (GCRL), Mutual Information Skill Learning (MISL), Successor Feature learning (SF), among others. Amid these disparate objectives lies the open problem of selecting the appropriate representation for sequential decision-making in a particular domain. This paper brings a unifying perspective to all these distinct algorithmic frameworks that make use of the sequential data in some way to predict future outcomes. First, we show that these seemingly disjoint algorithms are, in fact, approximating a common intractable representation learning objective under differing assumptions. We illuminate how these methods make use of embeddings that compress equivalent states to tractably optimize the objective. Finally, we show that assumptions governing practical URL methods create a performance-efficiency tradeoff that can help guide algorithm selection.", "tldr": "To unify a wide range of unsupervised RL algorithms: GCRL, Mutual information skills learning, Successor Features, World Models etc., using a successor measure as the unifying framework.", "keywords": ["Unsupervised Reinforcement Learning", "Representation Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09726ea30310f54b67a4f67b6afdefd8c8d47593.pdf", "supplementary_material": "/attachment/6a0d203ba00d49afe7513ff5e6c53f53433f32c2.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a unified framework for unsupervised reinforcement learning (URL) algorithms. The foundation of this framework is the successor measure. Within this framework, the paper proposes a single algorithm that unifies different classes of URL algorithms. The algorithm consists of two phases: a reward-free phase, in which the successor measure is learned for any policy, and a reward-based phase, in which the optimal policy is inferred using the learned successor measure and the given task reward.\nThe paper shows that different classes of URL algorithms (GCRL, MISL, SF, PSF, PVF, controllable representations, and world models) fall under the proposed framework. What differentiates each class is the assumptions about the policy class and the class of rewards over which the successor measure is estimated; those two assumptions influence the choice of the policy inference procedure.\nThe paper also suggests that different URL algorithms learn different state abstractions or notions of state equivalence to make learning tractable (either implicitly or explicitly). Finally, it highlights the trade-offs of each URL algorithm class in terms of final performance and the number of trainable parameters in the inference phase, which is proportional to the efficiency of the policy inference procedure, and discusses the implications of the proposed theoretical framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clear and easy to read and follow.\n- The proposed framework covers a wide range of URL algorithms."}, "weaknesses": {"value": "- The paper does not cover URL methods that optimize state entropy; the state entropy can be connected to entropy over the successor measure, but the paper did not mention any connections.\n- Some GCRL algorithms do not fall under this framework, for example, non-probabilistic GCRL methods that use some notion of distance as the reward.\n- The analysis is somewhat agnostic to the settings of the pre-training (offline vs online), which might affect the policy inference phase and the set of tasks the agent assumed by the URL algorithm class."}, "questions": {"value": "- In Theorem 4.6, if I understand correctly, the successor measure $M^{\\pi_z}(s, s^+)$ is the objective when we use the posterior form of the variational distribution $q(z \\mid s, s^+)$. Can you also include the successor measure when we use $q(z \\mid s)$? Furthermore, would it be possible to explicitly show the link between the mutual information objective and the successor measures to make the connection clearer?\n- Why are entropy/state coverage-based methods not included in the analysis? I guess most of them are considered exploration methods rather than Unsupervised Representation Learning (URL) algorithms? Can you clarify this in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VATPFzJhId", "forum": "nNmd4pPSck", "replyto": "nNmd4pPSck", "signatures": ["ICLR.cc/2026/Conference/Submission24053/Reviewer_EHY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24053/Reviewer_EHY5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761055452040, "cdate": 1761055452040, "tmdate": 1762942915363, "mdate": 1762942915363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies aspects of the unsupervised RL problem, in which several samples are collected from a reward-free environment in order to improve the performance on a variety of downstream tasks (with their respective rewards) later on. The paper aims to unify a family of approaches, including goal-conditioned RL, skill discovery with mutual information, successor features, under the same umbrella. First, the paper casts each of these methods as an approximation of the (intractable) objective of learning the successor measure (discounted probability of reaching a state in the future) for any policy. Then, the paper characterises the approximation through the lens of state aggregations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a unifying framework for a group of unsupervised RL approaches, an area of research that can definitely benefit from some clarity given the (sometimes confusing) variety of objectives and methods in the literature;\n- The paper is original to the best of my knowledge;\n- The paper provides specific results on the approximation of the unifying objective given by the different approaches;\n- The paper proposes an ideal (intractable) characterization of unsupervised RL as the problem of learning the successor measure for any policy."}, "weaknesses": {"value": "- The paper provides some interesting insights, but, in my opinion, the case of how future research can benefit from the presented findings is rather weak (Sec. 6);\n- The title and abstract are perhaps too far reaching in their claim. The paper deals with a sizeable set of unsupervised RL approaches, but doesn't cover the entirety of previous literature, including some notable areas like policy pre-training;\n- The paper only provides an informal definition of URL, as a setting composed by a reward-free phase to learn a representation to be used in a subsequent reward-based phase, but does not clarify the learning objective rigorously (e.g., how is the efficiency of the reward-based phase measured?)\n- The paper seems to violate the mandatory citation style of ICLR.\n\n**Evaluation**\n\nIn my opinion, the premise of the paper, coarsely, that the unsupervised RL problem is equivalent to learning approximate successor features is of great interest (if verified). However, I am providing a negative evaluation based on: (1) I am concerned the unsupervised RL problem definition is not rigorous enough, which makes the whole paper stands on shaky foundations, (2) I am not sure how the proposed unification brings value to the community and will help future research. I am reporting detailed comments below."}, "questions": {"value": "**(Major) What does unsupervised RL means**\n\nThe paper characterize an URL problem as a learning problem in which there is a reward-free phase (i) devoted to learn a representation to be used in a reward-based phase (ii) to infer the optimal policy (same MDP in both phases). I think the definition of the two phases is not formal enough.\n\n(i) reward-free phase\n\nIt's important to clarify what are the constraints of this phase.\n- Are we allowed to take infinite samples? If so, one would argue that learning the transitions of the MDP is enough to solve whatever RL problem afterwards. \n- Does the representation need to be succinct? Perhaps the transition model is not compact enough, but it's hard to say which direction to take without a formal specification of the objective in the subsequent phase.\n\n(ii) reward-based phase\n\nThe stated objective \"improved sample efficiency, computational efficiency, and/or wall clock time, compared to the standard reward-based RL approach of learning the policy from scratch only once the reward is given\" is far from clear. Especially,\n- What does *improved* means? I guess if sample efficiency goes from $N$ without any representation to $N - 1$ with the representation, we wouldn't be satisfied with the reward-free phase.\n- What is standard RL here? Are we comparing with the theoretical lower bound or a specific RL method?\n- Sample efficiency, computational efficiency, wall clock time... this is arguably too broad to be able to say anything interesting. I would suggest to narrow the definition. Theoretical works are typically much more formal in setting the objective, e.g., Xie et al., Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning, 2021.\n\nI concede that this characterization escaped the unsupervised RL community for a while, but I believe it is necessary to prove the premise of the paper.\n\n**(Major) Main hypothesis**\n\nThe paper is based on the following\n\n*we hypothesize that these methods [URL] learn how the distribution of future states is affected by the policy (successor measure) by treating states with similar properties as equivalent (state feature equivalence)*\n\nUnfortunately, I am skeptical that this is true. There's a sizeable literature in URL (e.g., Liu and Abbeel, Behavior From the Void: Unsupervised Active Pre-Training, 2021) in which the efficiency of the policy inference in the reward-based phase is improved by using the reward-free phase to learn a policy that collects \"good\" data. I don't see how we can prove that an implicit successor measure is learned here.\n\nMy take is that saying that this analysis \"unifies URL\" is too bold of a claim. I think it shall be formally clarified which subset of URL methods is unified here.\n\n**(Minor) The value of unification**\n\nIt is important to note that unifying various algorithms into a unique framework does not necessarily meet the standard for publication at a top ML conference. While the authors provide some discussion on how the unification will help future research (Sec. 6), I think the main points are mostly speculative.\n\n**(Minor) Formal result**\n\nOther than showing that a bunch of different methods can be understood as approximating a successor measure, it would be interesting to see whether it can be proved that any reward-free algorithm that improves the efficiency of policy inference with reward by a certain amount has to implicitly learn successor measures. See the paper Richens et al., General agents need world models, 2025 for potential inspiration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fWRYnp1iON", "forum": "nNmd4pPSck", "replyto": "nNmd4pPSck", "signatures": ["ICLR.cc/2026/Conference/Submission24053/Reviewer_mkCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24053/Reviewer_mkCJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814627441, "cdate": 1761814627441, "tmdate": 1762942915111, "mdate": 1762942915111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified framework for diverse unsupervised RL (URL) algorithms. The framework concerning Reward-Free Phase and Reward-based Phase is used to construct this unified framework. Within this framework, the core concept is the successor measure M_\\pi(s,a,s+). \n\nTo make it tractable, different URL algorithm families define parametric approximation of the policy class using different latent representations. The authors give a thorough theoretical explanation about all these diverse URL algorithms.\n\nI appreciate the authors can list the table to make the paper clear."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors use a unified framework to combine seemingly different unsupervised RL algorithms, which I think is a very novel topic.\n2. All the assumptions and theoretical process are clarified (but I do not carefully check the correctness of the theoretical process).\n3. The table and figure are clear."}, "weaknesses": {"value": "1. Unfortunately, I feel that the author's integration of various methods comes across as somewhat forced. This is because the author imposes additional assumptions that are not originally required by the URL method. These assumptions are necessary to fit the method into the framework, but when they are not met, the method can no longer be incorporated. Therefore, I believe further analysis is needed to determine when these assumptions are violated.\n2. When all assumptions are satisfied, these methods are incorporated into the framework. However, the author fails to further explore the applicability of these methods, that says, when a certain method should be used and the distinct focus of different methods. I believe such discussion is necessary to inspire the development of new algorithms based on this framework.\n3. The citation on line 977 of Appendix B is missing.\n\nSince I have not thoroughly checked the theory in detail and find the aforementioned weaknesses to be quite evident, I am inclined to assign a borderline reject score at this stage, with relatively low confidence."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hQ0fgv15D9", "forum": "nNmd4pPSck", "replyto": "nNmd4pPSck", "signatures": ["ICLR.cc/2026/Conference/Submission24053/Reviewer_kSjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24053/Reviewer_kSjZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979437002, "cdate": 1761979437002, "tmdate": 1762942914724, "mdate": 1762942914724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a unifying framework for unsupervised reinforcement learning (URL) algorithms, arguing that seemingly disparate methods (including Goal-Conditioned RL, Mutual Information Skill Learning, Successor Features, Proto-Successor Measures, Proto-Value Functions, Controllable Representations, and World Models) can be viewed as approximating a common successor measure objective under different assumptions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The core idea of unifying disparate URL methods through the lens of successor measures is interesting and could provide value to the community's understanding of these algorithms.\n- The paper covers seven algorithm families and derives connections between each and the proposed unified objective."}, "weaknesses": {"value": "- While I think the idea of a unifying framework for Unsupervised RL relevant and interesting, I think the paper is incomplete:\n    - the paper presents a numerous amount of methods and how they relate to the unified objective, but it should also justify why such unification is useful in the first place. In prior work [1], this has been done by finding and mixing synergies from different domains and showing that they can improve one another. Here this is quickly done with a very limited amount of tasks. \n    - More analysis should be expected. In toy environments, I would expect to see how the different methods approximate $M^\\pi$ in the reward-free phase: which method explores which part of the space, and why? How does this affect the reward-based phase?\n- I do not understand the presence of a World Models paragraph. World models are not unsupervised RL algorithms. They're just models trained on a dataset, and the question is: how do you build the dataset, which will be used to better approximate $M^\\pi$ (with the world model)? For getting diverse data, we can use unsupervised RL algorithms, such as RL algorithms with prediction disagreement rewards [2].\n- Also, Figure 1 is difficult to read, and there should be at least some info about the tasks in the core paper (readers should not need to go at the end of appendix to find the task descriptions). \n\n\nMinor:\n- missing citations in first paragraph of 4.2.\n- Section 4.3, I don't understand why using the vector reward here $\\textbf{r}$ and the feature matrix $\\Phi$ without introducing them. Why not simply using the simple formula $r=\\phi^T w$ ?\n- \"While MISL approaches have large variation in their overall algorithms, the core has always been to maximize the mutual information between states and ‚Äúskills‚Äù ($I(S, Z)$) or between transitions and skills ($I(S, S‚Ä≤; Z)$)\" I would suggest soften the claim a bit, as there are other objectives like $I(S',Z ; S)$ [3] and $I((S,S'),Z)$ [4]\n- there's a formatting issue at line 1452\n\n[1] Choi, Jongwook, et al. \"Variational empowerment as representation learning for goal-based reinforcement learning.\"\n[2] Sekar, Ramanan, et al. \"Planning to explore via self-supervised world models.\" \n[3] Sharma, Archit, et al. \"Dynamics-aware unsupervised discovery of skills.\" \n[4] Laskin, Michael, et al. \"Cic: Contrastive intrinsic control for unsupervised skill discovery.\""}, "questions": {"value": "- Did you notice any other relevant synergies between the described methods?\n- I can see that there are two kinds of tasks described in appendix E, which can be randomized, how many of these environments did you generate to evaluate the performance of each algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aWs6FJVdE4", "forum": "nNmd4pPSck", "replyto": "nNmd4pPSck", "signatures": ["ICLR.cc/2026/Conference/Submission24053/Reviewer_aYbj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24053/Reviewer_aYbj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087268841, "cdate": 1762087268841, "tmdate": 1762942914147, "mdate": 1762942914147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles unsupervised / reward-free reinforcement learning (URL) as a pretraining paradigm that learns task-agnostic structure which can later be reused for efficient downstream policy inference once a reward is specified. The authors argue that several prominent URL families: Goal-Conditioned RL (GCRL), Mutual-Information Skill Learning (MISL), Successor Features (SF), Proto-Successor Measures (PSM), Proto-Value Functions (PVF), Controllable Representations (CR), and World Models, can be understood through a single unifying theory: they each (explicitly or implicitly) learn approximations to a successor measure $M^\\pi$, i.e., the discounted measure over future states visited when starting from  $(s,a)$ and following policy $\\pi$. Under this view, downstream policy optimization for any reward becomes linear in $M^\\pi$, which is an attempt to clarify why URL pretraining can enable fast and efficient policy inference across many tasks. The paper formalizes this perspective, frames each family as making tractable approximations to a conceptually intractable unified objective, and highlights design trade-offs between expressivity (size of the represented policy class) and inference efficiency (cost of search once rewards are given)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "To support the conceptual claims, the paper presents a four-rooms gridworld study with a broad task distribution. The key result (Fig. 1) visualizes a trade-off between downstream performance and reward-phase training cost across URL families, consistent with the unifying theory‚Äôs expressivity‚Äìefficiency view. The paper offers an interesting perspective but there are some observations and questions."}, "weaknesses": {"value": "- The theoretical unification is novel and timely; it convincingly reframes disparate URL families within a single successor-measure lens. To further guide readers, I recommend adding a concise schematic taxonomy that (i) situates URL within the broader RL landscape, and (ii) maps each covered family, GCRL, MISL, SF/PSM/PVF, Controllable Representations, World Models, to the unified view. A figure could show: a top level split (standard reward-based RL vs. reward-free pretraining/URL), within URL, nodes per family annotated with its learned representation RRR, the policy class it supports, and the induced state-abstraction metric. edges/arrows indicating how each family is a tractable approximation to the intractable unified objective, among others. A companion ‚Äúcross-walk‚Äù table aligning each family‚Äôs objective with its $d$, $Œ†$, and how it leverages successor measures would make the unification operational and substantially improve reader orientation.\n\n- If one wished to design a new URL method within the authors‚Äô unified framework, the natural control knob is the choice of similarity/metric $d$ that induces the state abstraction $\\phi$. Making this design space explicit would turn the framework from descriptive to prescriptive and guide practitioners on how to instantiate new methods. Concretely, I suggest: (i) add a small theorem/assumption block stating minimal conditions on $d$ to guarantee the successor-measure‚Äìbased state equivalence via $\\phi$ and (ii) include a small sensitivity study that swaps ùëë with some functions like: cosine, RBF kernel,  energy score, among others, in your gridworld to show how changes the expressivity and inference-efficiency trade-off and the induced abstractions.\n\n- The paper unifies several major URL families, but influential directions such as curiosity-driven exploration, empowerment-based methods, and contrastive predictive control are not explicitly discussed. A short clarification on how these paradigms would fit within the successor-measure view (or whether they fall outside its scope) would strengthen the completeness of the framework and help position its boundaries.\n\n- Minor stylistic note: the section title ‚ÄúConsequences of the Unification‚Äù reads slightly ambiguous, as ‚Äúconsequences‚Äù can carry a negative connotation in some linguistic and academic contexts. A more affirmative phrasing such as ‚ÄúImplications of the Unification‚Äù, or ‚ÄúTheoretical and Practical Implications‚Äù might highlight the constructive nature of the results. Although this is a very minor stylistic comment."}, "questions": {"value": "- Table 1 introduces a method-dependent similarity $d$ to induce state-abstraction/equivalence, and the text notes that different URL families instantiate $d$  via norms, dot-products, or KL-divergences (thereby operationalizing $\\phi$  and the successor-measure view). This is a powerful unifying hook, but it naturally raises design-space questions the paper could address more explicitly. (a) What happens if we change $d$? beyond the listed instances, could cosine, kernel/RBF, or energy-based similarities yield new algorithmic families (rather than retrofitting existing ones)? (b) Which assumptions must $d$ satisfy to preserve the successor-measure equivalence that underlies the abstraction. Some guidance linked to Def. 5.1 and your $\\phi$-based equivalence would make the framework more actionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7iKKXO7SNx", "forum": "nNmd4pPSck", "replyto": "nNmd4pPSck", "signatures": ["ICLR.cc/2026/Conference/Submission24053/Reviewer_pWhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24053/Reviewer_pWhB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213771589, "cdate": 1762213771589, "tmdate": 1762942913787, "mdate": 1762942913787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}