{"id": "U7CezlrHy3", "number": 24260, "cdate": 1758354683596, "mdate": 1759896773993, "content": {"title": "COP-Q: Safety-First Reinforcement Learning with Cholesky Ordered Projection", "abstract": "Using uncertainty in Q-values to mitigate overestimation, enhance exploration, and ensure safety has proven effective in single-objective deep Q-learning. However, when learning vector-valued Q-functions for correlated goals, uncertainties become intertwined across objectives. Conventional approaches either treat uncertainty in each objective independently or collapse them into a scalarized dimension, often resulting in unstable learning, low sample efficiency, limited exploration, and particularly unsafe behaviours. To address these challenges, this study proposes Cholesky Ordered Projection Q-learning (COP-Q), a novel method that guides safety-first exploitation and exploration using full multi-objective uncertainty. We first introduce generalized multi-objective confidence bounds for Q-values via covariance matrix factorization. For priority-ordered objectives, such as in safety-critical or cost-constrained reinforcement learning, Cholesky factorization is employed to incorporate inter-objective covariance into confidence bounds in a conditionally sequential manner. The lower bound yields conservative temporal difference targets to reduce overestimation, while the upper bound assigns optimistic Q-values to promote exploration. COP-Q is evaluated on standard MuJoCo and velocity-constrained SafetyVelocity-v1 benchmarks, demonstrating robust safety performance and competitive total returns. The proposed method is compatible with various deep Q-learning frameworks with minimal computational overhead, making it practical for a wide range of multi-objective and constrained reinforcement learning tasks.", "tldr": "Guiding exploitation and exploration by multi-objective uncertainty in Q-values for safety-first reinforcement learning", "keywords": ["Safe reinforcement learning", "Multi-objective reinforcement learning", "Uncertainty quantification"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/911745fdf22c75764dddbd327c78fde226f3237e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In multi-objective RL, uncertainties of Q functions for different objectives may become intertwined, introducing extra challenges. Existing methods treat uncertainty in each objective independently or collapse them into a scalarized dimension. In this work, the authors introduce Cholesky Ordered Projection Q-learning (COP-Q), using full multi-objective uncertainty by covariance matrix factorization. Extensive experiments on standard MuJoCo and velocity-constrained SafetyVelocity-v1 benchmarks, demonstrating the effectiveness of COP-Q."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Figures 1 and 2 clearly show the insights and contributions of this work.\n\n- It is interesting and novel to introduce Cholesky Ordered Projection into RL.\n\n- Extensive experiments in both standard settings and safe settings show that COP-Q can handle different objectives in the same time."}, "weaknesses": {"value": "- I'm curious that if the Scalarization weight is fixed, what is the major difference between multi-objective RL and single-objective RL (R = u^T r)?\n\n- As mentioned in lines 146-149, the Scalarization weight u might be fixed or changed. However, I find that the main algorithm seems designed for fixed u, what about handling the changing u (if I have any misunderstanding, please point it out)?\n\n- In Fig. 1, the authors mention that the uncertainty of Q total may be low, but the uncertainty of each Q value may be high. Are there any theoretical or experimental observations supporting this insight?\n\nOverall, I think the idea of this work is novel, but there are still some concerns. I'd like to adjust my score if the authors can address my concerns."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kVBqSD5yc8", "forum": "U7CezlrHy3", "replyto": "U7CezlrHy3", "signatures": ["ICLR.cc/2026/Conference/Submission24260/Reviewer_xEpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24260/Reviewer_xEpw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761052795940, "cdate": 1761052795940, "tmdate": 1762943020424, "mdate": 1762943020424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to quantify uncertainty in multi-objective reinforcement learning.\nThis is realized using Cholesky factorization in the multi-objective Q-space.\nBy having a richer representation of the uncertainty, the overall performance is slightly improved."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper explicitly considers the uncertainty in the multi-objective Q-space.\n- The approach follows a hierarchical schema, where certain objectives (e.g., safety) are prioritized over others.\n- Experiments indicate a slight improvement over compared methods."}, "weaknesses": {"value": "- The paper is extremely difficult to follow, in particular, Sec. 4.1:\nThe applied steps (Eq. 6-8) require more explanations, clearly motivating what the goal of the subsequent transformations is, and properly explaining all variables. e.g., $R$. Similar holds for Eq. (10), where $C_{clip} = CR$ is introduced to drop $R$, but never used again. One can assume that $C_{clip}$ became $L_{clip}$ through Sec. 4.2, but this connection can be made clearer, or avoid dropping R altogether.\n- The paper also does not state clearly its assumptions / properly define the variables. For example, the paper says that (A.1) always holds if $C$ is symmetric. However, for $C=-I$, this clearly does not hold for any $u$, indicating that there are certain restrictions on which values $C$ can take.\n- It is unclear what is meant by the priorization of the objectives in Sec. 4.2. Are these given by the scalarization vector $u$? \n- The term \"uncertainty\" is not well defined."}, "questions": {"value": "- Can you explain the term \"the level of optimism when facing uncertainty\" more intuitively than simply stating (13)?\n- Does it matter how $u$ is set to determine the priorization of the objectives and vice-versa? \n- Can you quantify the \"minimal computational overhead\" of your approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HCEW9AAZLP", "forum": "U7CezlrHy3", "replyto": "U7CezlrHy3", "signatures": ["ICLR.cc/2026/Conference/Submission24260/Reviewer_cZ7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24260/Reviewer_cZ7N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511325707, "cdate": 1761511325707, "tmdate": 1762943020169, "mdate": 1762943020169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "COP-Q introduces a novel multi-objective Q-learning method leveraging Cholesky factorization to incorporate inter-objective covariance into uncertainty estimation. It prioritizes safety-critical objectives via ordered projection, yielding conservative TD targets for overestimation reduction and optimistic bounds for exploration. Evaluated on MuJoCo and SafetyVelocity-v1, COP-Q shows robust safety, competitive returns, and improved sample efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative Uncertainty Modeling: First work to integrate Cholesky factorization into multi-objective Q-learning, capturing objective correlations and priorities (e.g., safety-first).\n\n2. Strong Empirical Results: Outperforms baselines in safety-critical tasks while maintaining high returns.\n\n3. Theoretical Soundness: Confidence bounds generalize clipped double-Q learning, with rigorous projection derivations."}, "weaknesses": {"value": "1. Assumes Q-values follow multivariate Gaussian (Eq 5), but no ablation on its validity.\n\n2. Limited Task Diversity: Experiments focus on locomotion; lacks MORL Pareto-frontier or high-dim task validation.\n\n3. Variance in Exploration: REDQ+COP-OAC shows high variance in Humanoid (Fig 5), attributed to UTD ratio but unverified.\n\n4. Sec 4.3 uses biased covariance estimator (denominator = N+1). Justify this choice.\n\n5. Why there is no result about COP-Q-svc on halfcheetah experiment in Figure 4 top."}, "questions": {"value": "Please refer to Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dWdhAnW2zI", "forum": "U7CezlrHy3", "replyto": "U7CezlrHy3", "signatures": ["ICLR.cc/2026/Conference/Submission24260/Reviewer_URhw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24260/Reviewer_URhw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717205264, "cdate": 1761717205264, "tmdate": 1762943019772, "mdate": 1762943019772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cholesky Ordered Projection Q-learning (COP-Q) to enhance safety-first exploitation and exploration for vector-valued Q-functions. In particular, COP-Q first introduce the generalized multi-objective confidence bounds for Q-values and then employ Cholesky factorization to encodes full multi-objective uncertainty following the priority structure of objectives. COP-Q achieves good performance and training efficiency compared with existing baselines on MuJoCo benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces Cholesky Ordered Projection into Multi-Objective RL so that different objectives can be considered with different priority.\n2. COP-Q shows good performance and training efficiency compared with existing baselines on MuJoCo benchmarks."}, "weaknesses": {"value": "### Concerns on Mujoco  benchmarks\n1. In Figure 4, the training curve in halfcheetah seems to be wrong.\n   1. There are only two curves in the figure.\n   2. The blue curve (COP-Q-vsc) on top is exactly same as Cholesky on bottom\n      1. Since Cholesky is same as COP-Q-svc in other environments, there is something wrong in the figure.\n\n2. For baseline in MuJoCo, why doesn't COP-Q compare with MORL methods, such as RMORL [1], PGMORL [2], MORL-Adaptation [3], MO-MPO [4].\n\n[1] He, X., Hao, J., Chen, X., Wang, J., Ji, X., & Lv, C. (2024). Robust multiobjective reinforcement learning considering environmental uncertainties. *IEEE Transactions on Neural Networks and Learning Systems*, *36*(4), 6368-6382.\n\n[2] Xu, J., Tian, Y., Ma, P., Rus, D., Sueda, S., & Matusik, W. (2020, November). Prediction-guided multi-objective reinforcement learning for continuous robot control. In *International conference on machine learning* (pp. 10607-10616). PMLR.\n\n[3] Yang, R., Sun, X., & Narasimhan, K. (2019). A generalized algorithm for multi-objective reinforcement learning and policy adaptation. *Advances in neural information processing systems*, *32*.\n\n[4] Abdolmaleki, A., Huang, S., Hasenclever, L., Neunert, M., Song, F., Zambelli, M., ... & Riedmiller, M. (2020, November). A distributional view on multi-objective policy optimization. In *International conference on machine learning* (pp. 11-22). PMLR.\n\n### Concerns on Constrained benchmarks\n\n1. While this paper compares on SafetyVelocity-v1, it may have following concerns\n   1. The feasible set of SafetyVelocity-v1 is not tight. The relationship between reward and cost are more likely to be independent to each other.\n      1. For example, the agent can achieve the maximum rewards on a wide range of costs. \n   2. This issue will simplify the testing cases.\n2. Thus it is suggested to test on more benchmarks, whose feasible set is tighter.\n   1. BulletSafetyGym:\n      1. BallRun, BallCircle, DroneRun, DroneCircle, AntRun, AntCircle\n   2. SafetyGymnasium:\n      1. PointCircle1-v0, PointCircle2-v0, CarCircle1-v0, CarCircle2-v0, \n3. It is suggested to compare with more recent methods, such as RLSF [1]\n4. What are the criteria for selecting the cost threshold?\n   1. It is better to test on multiple thresholds across a wide range of costs.\n5. The cost in Figure 6 and Table E.3 seems to be unmatched.\n6. It is better to list the results of PPOSimmerPID and CUP in Table E.3.\n\n[1] Reddy Chirra, S., Varakantham, P., & Paruchuri, P. (2024). Safety through feedback in Constrained RL. *Advances in Neural Information Processing Systems*, *37*, 139938-139967.\n\n### Other Concerns\n\n1. In most figures, some methods seems that it hasn't converged within setting episodes.\n   1. For example, walker2d and ant in Figure 5.\n   2. For example, most on-policy method in Figure 6.\n   3. What is the performance after convergence?\n   4. Does COP-Q performs better than these methods after convergence?"}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IEH5XoE1rP", "forum": "U7CezlrHy3", "replyto": "U7CezlrHy3", "signatures": ["ICLR.cc/2026/Conference/Submission24260/Reviewer_6saY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24260/Reviewer_6saY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838417140, "cdate": 1761838417140, "tmdate": 1762943019274, "mdate": 1762943019274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}