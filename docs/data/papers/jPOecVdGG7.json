{"id": "jPOecVdGG7", "number": 3628, "cdate": 1757488494747, "mdate": 1759898077752, "content": {"title": "Chain of Atoms: Fine‑grained Semantic Evaluation for Image–caption Data via Atomic Decomposition", "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have achieved remarkable progress across a wide range of domains, largely benefiting from the availability of large-scale multimodal datasets, particularly image-caption corpora. Nevertheless, the community has long lacked a universal and standardized data quality assessment framework specifically designed for such corpora. In this paper, we propose the Chain-of-Atoms (CoA) evaluation framework along with a corresponding Bottom2Up data sampling strategy. CoA decomposes both captions and images into minimal information units and computes precision and recall as objective sub-metrics. By reweighting these sub-metrics dynamically, we introduce a style-adaptive $F_1$ (SAF1) metric to achieve better correlation with human preference. To enhance the capability of semantic decomposition, we apply the proposed Bottom2Up strategy to construct a balanced and large-scale training dataset. We also establish CoA Bench, a standardized benchmark for fine-grained image-caption evaluation. Experimental results on CoA Bench and other downstream tasks demonstrate that CoA effectively filters noisy training samples, significantly improves the robustness and training efficiency of MLLM. Specifically, CoA‑based data filtering during MLLM pre-training reduces the training data by 81.5% without causing performance degradation.", "tldr": "We propose Chain-of-Atoms (CoA), a fine-grained framework for evaluating and filtering multimodal datasets, along with a corresponding data generation method that enables more efficient and robust MLLM training with less but cleaner data.", "keywords": ["Image–Caption Evaluation; MLLMs; CoT; Data Filtering; Semantic Unit Decomposition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f654511fe9f2258bff2bcadd1cb44af922057ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Chain-of-Atoms (CoA) evaluation framework, designed to address the issues of lack of interpretability and insufficient granularity in existing metrics when assessing the quality of large-scale image-caption datasets. The core contribution of CoA is its atomic decomposition mechanism, which systematically breaks down image and text semantics into structured atomic units, enabling an interpretable and atomic-level quantification of semantic accuracy. To achieve this, the authors develop and rely on a specially trained CoA-MLLM, which integrates unit extraction, semantic matching, and score calculation within a single forward pass. Furthermore, CoA incorporates the SAFI metric to better simulate differential human preferences regarding description style, and it utilizes the Bottom2Up strategy to address the challenge of lacking large-scale, fine-grained annotated evaluation data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. CoA exhibits fine-grained decomposition and high interpretability. It incorporates principles from Scene Graph Generation to structure its decomposition. Its atomic-level diagnostic breaks down semantics into minimal atomic units, allowing for the precise diagnosis of which minimal fact has failed between the image and description. This capability provides clear and actionable feedback for MLLM data cleaning and model diagnosis.\n\n2. CoA can effectively filter out noisy training samples, which improves the robustness and training efficiency of MLLMs. Specifically, applying CoA for pre-processing MLLM pre-training data can reduce the required volume of training data without compromising the capabilities of the final trained model."}, "weaknesses": {"value": "1. By breaking down complex image-text semantics into isolated Minimum Visual/Text Units and simplistic subject-verb-object structures, the framework fails to capture higher-order meanings. Specifically, it seems unable to properly assess more abstract dimensions like emotional tone or atmospheric mood, since such holistic information cannot be adequately represented by merely discrete atomic units.\n\n2. The CoA metric is not a standalone universal standard independent of any model—its effectiveness heavily depends on a dedicated CoA-MLLM. This design tightly couples the evaluation framework with the benchmark model used as the \"judge\". Specifically, since the MLLM itself may have capability deficiencies or training biases, the visual/text units that it outputs are uncertain and dynamic, which casts doubt on the metric’s reliability since what it might be measuring is the stability of the MLLM’s reasoning rather than the true semantic quality of the data. To mitigate this issue, the authors could consider incorporating auxiliary models, such as object detection models, to assist the MLLM in extracting more robust visual and textual units.\n\n3. The proposed SAF1 metric aims to simulate human preferences for descriptive style by dynamically adjusting the weights of recall and precision. However, this approach appears more aligned with the authors’ subjective definition of a \"reasonable style\" based on observations from a specific dataset, rather than a comprehensive simulation of complex human preferences. The paper lacks empirical evidence to demonstrate that SAF1’s rankings and tendencies align with human subjective evaluations of style. This absence of validation undermines SAFI’s effectiveness as an objective tool."}, "questions": {"value": "1. Could the authors elaborate on how this approach can be systematically extended to capture and evaluate non-compositional properties such as emotional tone or atmospheric mood, which may be lost in such a decomposition?\n\n2. How do the authors ensure the CoA metric measures true semantic quality rather than just the stability and biases of the specific CoA-MLLM \"judge\"?\n\n3. What empirical evidence demonstrates that the SAF1 metric reliably aligns with diverse human preferences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mQ07FXhNhB", "forum": "jPOecVdGG7", "replyto": "jPOecVdGG7", "signatures": ["ICLR.cc/2026/Conference/Submission3628/Reviewer_DNAM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3628/Reviewer_DNAM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378837600, "cdate": 1761378837600, "tmdate": 1762916885964, "mdate": 1762916885964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CoA, a reference-free evaluation framework for image–caption pairs. It decomposes images into Minimal Visual Units (MVUs) and captions into Minimal Textual Units (MTUs) using a fine-tuned MLLM that also aligns units and assigns correctness. From these, it computes precision (factual correctness of caption atoms) and recall (visual coverage), and combines them into a style-adaptive F1 (SAF1) that weights recall more for detailed captions and less for terse ones. The authors introduce a 500-item CoA Bench, plus Bottom2Up sampling to span diverse precision/recall regimes. They show strong correlations to human judgments, and demonstrate that SAF1-based filtering of noisy datasets improves downstream multimodal training (pretraining and SFT)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Structured outputs (<box>/<scene>/<textatom>/<result>) make scores auditable and facilitate error analysis.\n2. SAF1’s interpolation between F1 and precision is simple, tunable, and better aligned with human tolerance for omission vs. hallucination.\n3. CoA Bench separates precision and recall and triangulates with human-rated SAF1, offering a fuller view than one-number caption scorers."}, "weaknesses": {"value": "1. CoA-MLLM both extracts MVUs/MTUs and judges matches. There’s no human-labeled gold standard for either the atom boundaries or the match function, so validity of precision/recall is never independently established. Errors in extraction propagate directly into both metrics, meaning “good” SAF1 may just reflect the evaluator’s own inductive biases. The paper needs a small but carefully annotated atom-level dataset to break this circle; it doesn’t provide one.\n2. Running an MLLM for decomposition/matching may be costly at web scale; latency/cost vs. CLIP-like metrics is not quantified.\n3. Demonstrations center on one training recipe/backbone; broader replication would increase confidence.\n4. A 500-item test set provides limited statistical power, especially when split across precision/recall strata."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8YLnAje0Yd", "forum": "jPOecVdGG7", "replyto": "jPOecVdGG7", "signatures": ["ICLR.cc/2026/Conference/Submission3628/Reviewer_kw44"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3628/Reviewer_kw44"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721963818, "cdate": 1761721963818, "tmdate": 1762916885744, "mdate": 1762916885744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of evaluating the fine-grained semantic quality of image-caption pairs, a key challenge in multi-modal large language model (MLLM) development. It proposes Chain of Atoms (CoA), a mage caption evaluation framework that decomposes a subjective score into two objective sub-metrics, and introduces a Style-Adaptive F1 (SAF1) metric to enhance interpretability and style adaptability. The paper also presents a Bottom2Up sampling strategy to generate fine-grained image caption evaluation datasets. Experiments demonstrate that CoA outperforms existing methods in interpretability and adaptability, and effectively improves MLLM training efficiency and robustness when used for data filtering. However, the study used LLaVA-1.5-7B as a baseline but utilized the 150k SFT data from LLaVA-1.0 instead of the original 665k SFT data for LLaVA-1.5-7B. This data discrepancy might have led to a lower performance of the reproduced baseline, potentially reducing the difficulty of demonstrating the performance improvement of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CoA can help redefine image-caption quality evaluation, moving beyond superficial or black-box metrics.\n2. Complex concepts (e.g., MVU/MTU decomposition, SAF1 weighting) are explained in a structured and easy-to-follow manner"}, "weaknesses": {"value": "1. As noted, the use of insufficient SFT data for the LLaVA-1.5-7B baseline may affect the accuracy of performance comparisons. This needs to be rectified to ensure the validity of the method's performance evaluation.\n2. Comparing pre-trained data selected randomly versus data filtered using the proposed method at the same quantity may better demonstrate the method's effectiveness."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JM46K2N30r", "forum": "jPOecVdGG7", "replyto": "jPOecVdGG7", "signatures": ["ICLR.cc/2026/Conference/Submission3628/Reviewer_J9bf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3628/Reviewer_J9bf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732262507, "cdate": 1761732262507, "tmdate": 1762916885302, "mdate": 1762916885302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current multi-modality datasets may contain some mistakes or semantic mismatches, which affect the model training. In this paper, the authors propose a novel evaluation method that decomposes images and texts into minimal semantic units and obtains a comprehensive score. Besides, this paper proposes a new benchmark. Extensive experiments demonstrate the effectiveness of pthe roposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper is easy to follow and understand.\n2.\tThe motivation is clear, and the proposed method to decompose image-text data pairs into atoms is effective and useful.\n3.\tThe paper conducts comprehensive experiments."}, "weaknesses": {"value": "1. As a data evaluation model, CoA should be compared against other scores, such as CLIPScore and PAC-S, etc. \n\n------\nActually, from my perspective, this paper is **comprehensive** and **consolidated**. No obvious weakness or questions."}, "questions": {"value": "Please see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oIzMAf7cM1", "forum": "jPOecVdGG7", "replyto": "jPOecVdGG7", "signatures": ["ICLR.cc/2026/Conference/Submission3628/Reviewer_EWYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3628/Reviewer_EWYz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970118515, "cdate": 1761970118515, "tmdate": 1762916885010, "mdate": 1762916885010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}