{"id": "nCDgdjH6oV", "number": 6235, "cdate": 1757960714632, "mdate": 1763677149450, "content": {"title": "Neural Catalog: Scaling Species Recognition with Catalog of Life–Augmented Generation", "abstract": "Open-vocabulary species recognition is a major challenge in computer vision, particularly in ornithology, where new taxa are continually discovered. While benchmarks like CUB-200-2011 and Birdsnap have advanced fine-grained recognition under closed vocabularies, they fall short of real-world conditions. We show that current systems suffer a performance drop of over 30\\% in realistic open-vocabulary settings with thousands of candidate species, largely due to an increased number of visually similar and semantically ambiguous distractors. To address this, we propose Visual Re-ranking Retrieval-Augmented Generation (VR-RAG), a novel framework that links structured encyclopedic knowledge with recognition. We distill Wikipedia articles for 11,202 bird species into concise, discriminative summaries and retrieve candidates from these summaries. Unlike prior text-only approaches, VR-RAG incorporates visual information during retrieval, ensuring final predictions are both textually relevant and visually consistent with the query image. Extensive experiments across five bird classification benchmarks and two additional domains show that VR-RAG improves the average performance of the state-of-the-art Qwen2.5-VL model by 18.0%.", "tldr": "A visual re-ranking framework to improve open-vocabulary recognition ability of MLLM.", "keywords": ["Open Vocabulary Recognition", "RAG", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9713e374ae4471a1679e5a8d9bc8d1bc18254661.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NeuralCatalog, a retrieval-augmented generation (RAG) framework for open-vocabulary bird species recognition. It leverages Wikipedia as an external knowledge source, retrieving concise species descriptions to complement visual features from images. By integrating retrieval, re-ranking, and multimodal reasoning within a large language model, NeuralCatalog enables accurate recognition of both seen and unseen species, achieving state-of-the-art performance in bird species recognition."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of open-vocabulary species recognition is practical and of scientific interest. \n2. The improvements obtained are strong and non-trivial in nature."}, "weaknesses": {"value": "I believe this paper has several weaknesses that are not adequately addressed in the current submission. My main concerns center on positioning and evaluation.\n\nW1. While the paper’s motivation around biological species recognition is strong, the inclusion of a new Pokémon dataset feels abrupt and insufficiently justified. As presented, it appears more like an attempt to expand the technical scope rather than a well-motivated extension of the main problem.\n\nW2. The evaluation against encoder-only models raises concerns about fairness. Since the proposed method ensembles multiple encoder-based VLMs (e.g., CLIP), comparing against these same models individually in Tables 2 and 3 seems misaligned. Moreover, the paper omits several relevant baselines from recent zero-shot prompting and retrieval works [1,2,3], which weakens the empirical positioning of the method.\n\nW3. The retrieval pool includes over 11,000 species—substantially larger than prior benchmarks—yet it remains unclear whether species appearing in test sets (e.g., CUB, iNat) are excluded from the retrieval corpus during evaluation. If not, this introduces a fairness issue, as the model could indirectly access prior knowledge of test species during inference.\n\nReferences\n[1] Prompting Scientific Names for Zero-Shot Species Recognition - EMNLP 2023\n\n[2] Visual Classification via Description from Large Language Models - ICLR 2023\n\n[3] Learning Customized Visual Models with Retrieval-Augmented Knowledge - CVPR 2023"}, "questions": {"value": "Please refer to the weaknesses outlined above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R7LNZOSoQS", "forum": "nCDgdjH6oV", "replyto": "nCDgdjH6oV", "signatures": ["ICLR.cc/2026/Conference/Submission6235/Reviewer_2xRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6235/Reviewer_2xRi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757312709, "cdate": 1761757312709, "tmdate": 1762918562724, "mdate": 1762918562724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VR-RAG (Visual Re-ranking Retrieval-Augmented Generation), a system for large-scale fine-grained species identification, focusing mainly on birds.\nThe authors argue that current closed-world benchmarks (e.g., CUB-200, NABirds) fail to reflect the real complexity of biodiversity recognition, where species are numerous, visually similar, and new species are continuously added.\nVR-RAG combines three pretrained components:\n\nMultimodal retrieval using CLIP / OpenCLIP / SigLIP to find candidate species descriptions from a curated Wikipedia-derived knowledge base (11 k+ species).\n\nVisual re-ranking using DINOv2 to reorder retrieved candidates by pure image–image similarity, claimed to suppress environmental bias.\n\nReasoning with a pretrained MLLM (e.g., Qwen2.5-VL or InternVL) that receives the image and top-k candidate summaries and generates the final predicted species name.\nThe authors report consistent improvements in retrieval quality (mRR@k) and final classification accuracy across five bird datasets, FishNet, and Pokémon."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Ambitious and environmentally meaningful problem framing: scaling biodiversity recognition to real-world, open-vocabulary conditions.\n\n- Clear modular system design that combines retrieval, re-ranking, and reasoning.\n\n- Demonstrated cross-domain generality (birds, fish, Pokémon).\n\n- The curated textual summaries for >11 k species constitute a useful resource for the community."}, "weaknesses": {"value": "- Scientific novelty – The system reuses existing pretrained encoders and an MLLM without new learning mechanisms. The main novelty is the composition, not a new algorithmic contribution.\n\n- Prompt sensitivity – The MLLM reasoning stage relies on a single fixed prompt; no ablations on phrasing, candidate ordering, or number of candidates are shown. The robustness of the reported gains is therefore uncertain.\n\n- Intra-species visual variability – Many bird species exhibit strong sexual dimorphism (male vs. female), seasonal plumage differences, or distinct juvenile appearances. The current setup appears to use one reference image and one textual summary per species. This could bias retrieval toward a single morph and hurt generalization. Please discuss or evaluate using multiple reference embeddings per species.\n\n- Single-object assumption – Datasets used contain one main bird per frame; it is unclear how the framework handles multi-object or occluded scenes.\n\n- Visual re-ranking explanation – The paper attributes improvement to DINOv2’s self-distillation producing object-centric features that suppress background noise. While plausible, this claim is unverified; no attention visualizations or controlled background tests are provided.\n\n- Scope of contribution – Since no training is performed, the paper would benefit from clearly positioning itself as a system-level integration study rather than implying a new learning framework."}, "questions": {"value": "- Include prompt ablations for the MLLM reasoning step.\n\n- Discuss or extend to multi-reference per species to address male/female, juvenile, or seasonal variation.\n\n- Provide visual evidence or controlled tests showing that DINOv2’s self-distilled embeddings indeed suppress environmental noise.\n\n- Clarify scientific scope—system integration vs. methodological innovation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8kf1pypjTu", "forum": "nCDgdjH6oV", "replyto": "nCDgdjH6oV", "signatures": ["ICLR.cc/2026/Conference/Submission6235/Reviewer_BZEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6235/Reviewer_BZEM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860202636, "cdate": 1761860202636, "tmdate": 1762918562225, "mdate": 1762918562225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Proposes VR-RAG, a framework combining retrieval and reasoning for open-vocabulary species recognition.\n- Connects encyclopedic text (Wikipedia) with visual cues for grounding unseen species.\n- Builds a large benchmark of 11 k+ bird species with GPT-4o-refined summaries and curated anchor images.\n- Designs a two-stage pipeline: multimodal retrieval, visual re-ranking, and MLLM-based reasoning. The method demonstrates strong cross-domain generalization across birds, fish, and Pokémon datasets"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the paper is clear and significance.\n- The authors construct a large-scale benchmark of more than 10k bird species and  produce concise, discriminative summaries aligned with visual evidence.\n- The proposed VR-RAG framework demonstrated substantial accuracy gains over all baseline models across five bird datasets and two cross-domain settings."}, "weaknesses": {"value": "- The idea of combines retrieval + re-ranking + MLLM reasoning is an intuitive and common way to bridge textual and visual knowledge bases. The contribution to novelty is limited.\n- Wikipedia tends to emphasize well-studied taxa, resulting in regional and taxonomic biases that leave numerous cryptic or underrepresented species undocumented. How are species without Wikipedia pages handled? Is coverage gap quantified?\n- Some design choice are not deeply analyzed, such as the number of retrieved candidates, or alternative retrieval architectures."}, "questions": {"value": "- The paper mentioned the data curation using GPT-4o to filter. Have authors check whether this step will bring in bias and noise?\n- The study focuses mainly on classification and retrieval accuracy, without exploring more demanding downstream tasks such as visual question answering.\n- How much gain comes from re-ranking vs. GPT-4o summary quality alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rgDHvPIfhr", "forum": "nCDgdjH6oV", "replyto": "nCDgdjH6oV", "signatures": ["ICLR.cc/2026/Conference/Submission6235/Reviewer_xe6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6235/Reviewer_xe6N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967615935, "cdate": 1761967615935, "tmdate": 1762918561749, "mdate": 1762918561749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a RAG-based framework for species classification that uses wikipedia summaries of species as the knowledge database. Firstly, an LLM is used to summarize the Wikipedia articles and then are divided into chunks. Multimodal embeddings of anchor images and the text chunks are created which are used to retrieve appropriate text chunks given a query image, which is embedded with a ensemble of VLMs. Finally, an MLLM is used to rerank the retrieved articles for final prediction. The proposed RAG-based system achieves state-of-the-art performance in various species classification benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a RAG-based species classification pipeline using wikipedia article summaries and multimodal ensemble-based retrieval method."}, "weaknesses": {"value": "1. Limited technical novelty as it is a simple RAG system that is used everywhere now but applied to species image classification. How is the framework different than just an application (RAG + x) where x is some problem? What are some exclusive properties of the proposed system that is tailored to species classification only? Can it be applied to other problems?\n2. What is the computational overhead of the proposed RAG system as compared to the other VLMs. What is the cost-benefit ratio of using this system? What is the inference time given a query? \n3. Is it possible to integrate additional modalities as done by recent works such as TaxaBind? Would incorporating additional modalities improve retrieval and the overall understanding capabilities of the model.\n4. How robust is the proposed RAG framework to the database? Can the authors provide experiments for ablating the size of the database and robustness to noisy articles in the database? \n5. Can you provide some uncertainty estimates to when reranking is not useful? What are the concrete failure cases when reranking is incorrect and retrieval is accurate?\n6. In general, since this is in applications track, the paper needs to have more systems level analysis and experiments. I dont see particular systems level innovations.\n7. All the experiments are provided on commonly used bird classification benchmarks. Any use cases of the RAG system beyond identification of common species? Like can you provide some experiments for novel species identification?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "obfp964aDp", "forum": "nCDgdjH6oV", "replyto": "nCDgdjH6oV", "signatures": ["ICLR.cc/2026/Conference/Submission6235/Reviewer_PS7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6235/Reviewer_PS7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6235/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188190421, "cdate": 1762188190421, "tmdate": 1762918561276, "mdate": 1762918561276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}