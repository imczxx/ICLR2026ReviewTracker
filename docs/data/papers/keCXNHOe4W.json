{"id": "keCXNHOe4W", "number": 23001, "cdate": 1758337943203, "mdate": 1759896836428, "content": {"title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "abstract": "Reinforcement learning (RL) is a human-designed framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLMs at inference time – a phenomenon known as in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, called ICRL prompting. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning for self-improvement on a given task. After the LLM generates a response at the current round, we give numerical scalar feedback on the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal at inference time, just like an RL algorithm. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Surprisingly, in some experiments, the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a new paradigm for test-time scaling.", "tldr": "", "keywords": ["Test-Time Scaling", "Inference-Time Improvement", "LLMs", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54e9203793a7d773465dfb7edd52aacd2d2a72f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a minimal in-context RL prompting framework where an LLM iteratively generates responses to a task, receives scalar numerical rewards, and conditions future outputs on previous responses and rewards. Over multiple rounds prompting, the model’s behavior exhibits classical RL traits, e.g., reward maximization and context-dependent improvement, demonstrating that reward-driven self-improvement can emerge from prompting."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written, and the method can be adapted to various environments with minimal modifications. Results span arithmetic puzzle solving, open-ended writing, and interactive text environments, suggesting some generality."}, "weaknesses": {"value": "The method is extremely simple and does not fine-tune the model via RL training. It relies only on inference-time prompting with scalar rewards. If the authors could directly fine-tune the parameters of the large language model, just like training a policy network in reinforcement learning, it would increase the complexity of the method and also enhance the novelty of the paper.\n\nThe reward provider is the same LLM as the policy, which will amplify bias and risks reward hacking by style, for example, the model may learn to write to please itself rather than to solve the task objectively. I suggest the authors use an additional large model as the reward model.\n\nThe “explore” and “exploit” text labels are strong control signal and might explain some gains independent of any learned internal RL mechanism. The ablation shows the method’s reliance on hand-crafted prompt semantics. More neutral or masked forms of reward or context (e.g., numeric tags without the word “Reward”, randomized labels) could test how much the model depends on semantic priors. I suggest removing the \"reward\" prompt during the prompting process, or placing the reward signal in other positions of prompting to test the performance of this algorithm."}, "questions": {"value": "1. typo: “At time step 0, an initial state S_0 sampled from p_0.” at line 101.\n2. In line 110, using θ_t and θ_{t+1} to distinguish the policy network before and after weight updates is inappropriate, because t represents the reinforcement learning time step. If the authors' method does not perform policy updates at every interaction with the environment, then using t and t+1 to make this distinction is not suitable.\n3. At line 113, I think it is inappropriate to arbitrarily name the reinforcement learning policy update process (in-weight RL), as this only adds to the burden of understanding and does not substantially improve the method.\n4. Since the state transition of the reinforcement learning environment can be constructed by continuously adding prompts, why not directly fine-tune the parameters of the LLM, rather than only using the inference capability of the LLM?\n5. Without a strict compute-matched comparison against Best-of-N, ToT/MCTS, or tuned self-revision, it’s hard to isolate where the gains come from. Though, ICRL provides some scaling and context-length analysis, stronger budget-normalized comparisons would be useful.\n6. If the environments in the experimental section can be modeled as reinforcement learning environments, why is it necessary to use LLMs to complete these tasks? These tasks are not complex; for example, in the game of 24, I believe that as long as a reward function is set, classical RL methods can also accomplish such tasks.\n7. I suggest the authors add credit-assignment analysis to better demonstrate which outputs in the decision sequence the model's performance is strongly correlated with.\n8. In Figure 2, the performance variance of ICRL Preset in the game of 24 is very large. Does this indicate that the method is highly unstable? Why does the Self-Refine method have stronger learning efficiency than the method proposed in this paper on Creative Writing? Why does the ICRL Autonomous method only perform excellently on Science World?\n9. For creative writing, Alpaca-Eval is a proxy. Pairwise win-rates are useful but should be complemented with human studies or cross-judge robustness to prevent evaluation overfitting. For game of 24, Best-of-N uses a ground-truth solver to pick the best sample, while ICRL uses its own reward r. So, some comparisons are not fully like-for-like across selection rules.\n10. I suggest the authors to add Compute-matched comparisons against Best-of-N, ToT/MCTS, and tune Self-Refine/Reflexion under equal token/cost budgets.\n11. I suggest the authors to Mask or scramble the “Reward” semantics to test whether numeric tags (without the word “Reward” or with randomized tokens) still work, which will probe whether improvements require semantic understanding vs. mere structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper’s core idea is intentionally simple: reuse the model’s own history of attempts and numeric scores to steer the next attempt. It does not apply RL training (no weight updates, no policy gradients); it banks entirely on inference-time behavior and the model’s instruction following. This makes it easy to deploy, at the same time, it also means the “RL emergence” claim remains hypothesis-level, not conclusively distinguished from search/selection + prompting effects."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oUczTOB6tS", "forum": "keCXNHOe4W", "replyto": "keCXNHOe4W", "signatures": ["ICLR.cc/2026/Conference/Submission23001/Reviewer_iBtQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23001/Reviewer_iBtQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463089602, "cdate": 1761463089602, "tmdate": 1762942471346, "mdate": 1762942471346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an in-context RL framework that provides a task description, and then judges the answers of the model to provide a reward that is used by the LLM to improve its answers. The authors evaluate the method on different text-based RL problems, where the method outperforms other self-revision techniques like Self-Refine and Reflexion."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. The method seems sound and is able to achieve good performance. There is good diversity in the tasks that the method is evaluated on, ranging from math puzzles to creative writing."}, "weaknesses": {"value": "I am unsure about the novelty of the method. It seems that a number of works on ICRL already exist in the literature. The authors state that existing works are \"restricted to bandit or simulated environments [...], falling short of addressing complex open-ended tasks in the real world\", which seems a bit vague. There are other papers that do in-context reinforcement learning that also tackle complex sequential decision-making tasks, e.g., [1-4]. One of the claimed contributions is the \"ICRL prompting framework\", but it is unclear how this framework differs from existing frameworks used in ICRL. I would greatly appreciate it if the authors could elaborate on the exact novelty in their method and setting.\n\n[1] Michael Laskin et al. \"In-context Reinforcement Learning with Algorithm Distillation.\" The Eleventh International Conference on Learning Representations.\n[2] Yarik Menchaca Resendiz, and Roman Klinger. \"PARL: Prompt-based Agents for Reinforcement Learning.\" arXiv preprint arXiv:2510.21306 (2025).\n[3] Ethan Brooks et al. \"Large Language Models can Implement Policy Iteration.\" Advances in Neural Information Processing Systems 36 (2023).\n[4] Sili Huang et al. \"In-context decision transformer: Reinforcement learning via hierarchical chain-of-thought.\" arXiv preprint arXiv:2405.20692 (2024)."}, "questions": {"value": "1. Figure 2/3 middle + right: The intuition of the returns is not clear, which makes it hard to assess the performance level and the significance of the performance differences between the different configurations. Is the agent actually able to solve the task? What is the difference between two agents that have X difference in return? Are both solving the tasks more or less, or is there a perceptible difference in the quality of the solutions? Maybe there is a more intuitive metric for these tasks than the return that would make it easier to assess the performance of the agents.\n\n2. Figures 2 and 3: What exactly do the shaded areas represent? \n\n3. Line 370 mentions a \"length-controlled win rate\": How is that defined?\n\n4. Figure 3: It is surprising that the \"zero rewards\" configuration actually improves and even reaches somewhat decent performance. Perhaps the authors could elaborate on how the agent is able to improve without any kind of feedback.\n\n5. Figure 3 middle: Some of the curves seem incomplete. ICRL Autonomous is still improving, so it is unclear whether it would eventually reach the performance of ICRL Preset and Self-Refine. Is there a reason why some curves only go to trial 50 here?\n\nTypos:\n\nLine 124: \"can only from\": word missing\n\nLine 139: \"funciton\" --> \"function\"\n\nLine 298: \"The task in challenging\"\n\nLine 444: \"criting\" --> \"writing\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yVS2LTMFBs", "forum": "keCXNHOe4W", "replyto": "keCXNHOe4W", "signatures": ["ICLR.cc/2026/Conference/Submission23001/Reviewer_nrTz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23001/Reviewer_nrTz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834950036, "cdate": 1761834950036, "tmdate": 1762942470758, "mdate": 1762942470758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-round prompting framework, ICRL Prompting, to argue that LLMs exhibit an emergent capability for in-context reinforcement learning (ICRL). The method appends a history of the LLM's prior (response, scalar reward) pairs to the context. The authors demonstrate that this minimal mechanism, which relies solely on scalar feedback, achieves performance improvements on several benchmarks and outperforms baseline methods that rely on textual feedback."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this paper is its novelty. It reveals an interesting phenomenon: simple scalar reward signals within the context may be sufficient to drive self-improvement in LLMs. This claim is supported by impressive empirical results, where the method consistently outperforms strong baselines like Self-Refine and Reflexion on several benchmarks. The experimental design includes rigorous and fair baseline comparisons, notably by allowing baselines like Self-Refine to use an equally growing context and by comparing ICRL against an overpowered Best-of-N baseline that uses the ground-truth reward for selection. The inclusion of a computational cost analysis in the appendix further strengthens the paper's claims of practical utility."}, "weaknesses": {"value": "The paper's most significant weakness is a methodological flaw in its core experiments: a self-referential bias. In several key benchmarks, the reward signal r is generated by the LLM itself. This introduces a high risk of reward hacking, where the policy model may simply be learning to overfit the biases of its own evaluation model rather than optimizing the true task metric r*. The paper fails to provide sufficient evidence to rule out this possibility.\n\nThis methodological issue is compounded by the paper's reliance on phenomenological observation (a \"duck test\") for its central claim that LLMs are RL learners. The paper offers no mechanistic analysis explaining how the Transformer architecture actually utilizes the (response, reward) history to achieve policy improvement.\n\nIn addition to this lack of mechanistic depth, the paper does not include a systematic robustness analysis of reward signal quality. The ablation study tests zero rewards, but this is not equivalent to testing the method's tolerance for noisy or incorrect rewards, which is critical for real-world application. This limited scope of evidence leads to the final weakness: the paper's conclusion is overclaimed. All empirical evidence is derived from text-based environments, making the extension of its claims to general-purpose tasks unsupported."}, "questions": {"value": "1. The ablation study tests zero rewards but not noisy rewards. Could the authors provide a sensitivity analysis showing how ICRL's performance degrades if 20% or 50% of the scalar rewards in the context are intentionally replaced with incorrect values?\n\n2. The claim of emergent RL capability is currently based on external behavior. To provide mechanistic evidence, could the authors provide a preliminary interpretability analysis (e.g., attention maps) to show whether the model systematically attends more to tokens associated with high-reward history when generating a new response?\n\n3. The performance gap between ICRL (90% on Game of 24) and the Best-of-N baseline (49%), which used the true reward r*, is striking. What is the authors' intuition for this gap? Does it imply that correct solutions are so sparse in the model's base distribution that simple sampling even with a perfect r* selector fails to find them, whereas ICRL is actively learning to generate them?\n\n4. Could the authors please clarify the context truncation strategy mentioned in Appendix B.1 (used to fit \"at least 32 prior attempts\")? Is this a simple FIFO queue, or a more complex strategy based on reward (e.g., prioritizing high-reward experiences)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pEFQInnPwV", "forum": "keCXNHOe4W", "replyto": "keCXNHOe4W", "signatures": ["ICLR.cc/2026/Conference/Submission23001/Reviewer_J3p5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23001/Reviewer_J3p5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901069231, "cdate": 1761901069231, "tmdate": 1762942470356, "mdate": 1762942470356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}