{"id": "4ZU8v4s3IR", "number": 9984, "cdate": 1758154558988, "mdate": 1763684938408, "content": {"title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation", "abstract": "Large language models (LLMs) are typically deployed under diverse memory and compute constraints. Existing approaches build model families by training each size independently, which is prohibitively expensive and provides only coarse-grained size options. In this work, we identify a novel phenomenon that we call boomerang distillation: starting from a large base model (the teacher), one first distills down to a small student and then progressively reconstructs intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training. This process produces zero-shot interpolated models of many intermediate sizes whose performance scales smoothly between the student and teacher, often matching or surpassing pretrained or distilled models of the same size. We further analyze when this type of interpolation succeeds, showing that alignment between teacher and student through pruning and distillation is essential. Boomerang distillation thus provides a simple and efficient way to generate fine-grained model families, dramatically reducing training cost while enabling flexible adaptation across deployment environments.", "tldr": "We identify a phenomenon called boomerang distillation, where distilling a teacher model into a student model enables us to reconstruct intermediate-sized models by incorporating teacher layers into the student with no additional training.", "keywords": ["knowledge distillation", "pretraining", "adaptive compute", "model interpolation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/faf1868c7d9ec31a240e186ed01fb66f727c5996.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces and empirically validates a novel phenomenon termed \"boomerang distillation.\" The method provides a highly efficient, zero-shot technique for creating a family of models with fine-grained sizes that interpolate between a small, distilled \"student\" model and a large \"teacher\" model. The authors conduct extensive experiments across multiple model families (Qwen, Pythia, Llama) and sizes, demonstrating that this method produces models whose performance scales smoothly with size. These interpolated models consistently and significantly outperform naive layer pruning and often match or even surpass the performance of models of equivalent size that are trained individually via standard distillation, all while dramatically reducing the computational cost of creating model families."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This article proposes a simple, three-stage scheme for zero-shot model size interpolation, which is both novel and highly practical. The ability to generate a series of model sizes after just one training run significantly reduces the computational resources required to build and deploy model families.\n- The paper’s claims are supported by extensive and well-designed experiments. This method has been validated across multiple architectures (Qwen, Pythia, Llama) and scales, demonstrating its universality. The comparison with relevant baselines is thorough and effectively highlights the superiority of the proposed technique.\n- The paper is exceptionally well-written and presented. Figure 1 provides a clear and intuitive overview of the entire process. Figures 2, 3, and 4 are particularly effective in visualizing core results: the smooth performance interpolation of boomerang distillation contrasts with the poor or unstable performance of the baselines. These figures make the core claims convincing and easy to grasp.\n- The ablation studies are also highly comprehensive, providing crucial support for the paper’s conclusions."}, "weaknesses": {"value": "- The article mentions an inconsistency in performance between Llama-3.2-3B and the Qwen/Pythia Family. This limits the universality of the Boomerang Distillation method, suggesting that its successful application may require non-trivial, model-specific analysis—slightly undermining the \"zero-shot\" simplicity of the patching step.\n- The article notes in Appendix E.1 that there is a lower bound for model compression when using this method. However, consider a scenario where we aim to distill DeepSeek-R1, a 671B model: in practice, we can obtain a 1.5B model through data distillation. This size reduction far exceeds the experimental lower bound of the method, revealing certain limitations of Boomerang Distillation."}, "questions": {"value": "- Would the same effect still hold if simple modifications are made to the student model? For instance, the internal structure of the student model might differ from that of the teacher model—such as altered activation functions—even when their hidden dimensions are identical. If the current effect remains unchanged, what conclusions could be drawn from this?\n- In Section 3.2 of the article, it is mentioned that low-quality datasets lead to the need for direct model distillation methods like Boomerang Distillation. Could openly available high-quality datasets be used to validate this conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "p813wc5W2U", "forum": "4ZU8v4s3IR", "replyto": "4ZU8v4s3IR", "signatures": ["ICLR.cc/2026/Conference/Submission9984/Reviewer_Cce4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9984/Reviewer_Cce4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761331522203, "cdate": 1761331522203, "tmdate": 1762921417214, "mdate": 1762921417214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is concerned with knowledge distillation for dynamic inference. It observes that by treating the layer-dropped LM as the student LM and the original LM as the teacher LM, the student LM could dynamically adjust the inference budget by inserting dropped layers again into the student LM. With the inserted layers, the performance of the student LM could also be improved. The method proposed is termed boomerang distillation in this paper. The method enables a fine-grained model family with only one pass of distillation, reducing the expected cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is actually a very counter-intuitive observation that the layer-dropped student could be recovered by adding the dropped layers back.\n2. The one-pass distillation essentially enables efficient model family derivation, which may serve as a cost-effective way in both academia and industry."}, "weaknesses": {"value": "1. The experiments are rather limited to model scales smaller than 10B, which might constrain the application of the method to larger-scale LMs.\n2. In Figure 2, there seems to be a sudden performance improvement in generation accuracy along the insertion of layers, however, the improvement in classification accuracy is rather smooth. Further elaborations on the phenomenon should be attached here."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IEdC5aebcP", "forum": "4ZU8v4s3IR", "replyto": "4ZU8v4s3IR", "signatures": ["ICLR.cc/2026/Conference/Submission9984/Reviewer_dyqi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9984/Reviewer_dyqi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815854680, "cdate": 1761815854680, "tmdate": 1762921416861, "mdate": 1762921416861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores an interesting phenomenon termed Boomerang Distillation. The authors discover that, under certain constraints, adding back layers from a distilled model can produce a spectrum of intermediate model sizes without any additional training. They further show that these interpolated models achieve performance comparable to standard distilled models of the same size."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The phenomenon presented is both interesting and novel.\n- The presentation is clear, and the experimental results are comprehensive and well-executed."}, "weaknesses": {"value": "- The paper lacks an in-depth explanation of the phenomenon. While a rigorous theoretical analysis may be unrealistic, including some qualitative observations or a proof-of-concept experiment to illustrate the underlying rationale would be beneficial.\n- In the related work section, the authors compare their approach to model interpolation and claim that prior interpolation methods operate solely in weight space. However, interpolating across different architectures can also be regarded as a special case of weight-space interpolation (where certain weight matrices are zero). Moreover, LlamaFlex[1] discusses interpolation between models of different sizes in its “Router Interpolation” section, which seems relevant and worth acknowledging.\n- This work focuses exclusively on constructing student models through layer dropping. It would be valuable to discuss whether the observed phenomenon also applies to width or head pruning. While interpolation in these cases may be more challenging, such discussion would provide a more complete perspective.\n\n[1] LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Mm7QI6orU4", "forum": "4ZU8v4s3IR", "replyto": "4ZU8v4s3IR", "signatures": ["ICLR.cc/2026/Conference/Submission9984/Reviewer_s5vT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9984/Reviewer_s5vT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047055207, "cdate": 1762047055207, "tmdate": 1762921416528, "mdate": 1762921416528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Boomerang Distillation, a novel approach for knowledge distillation that allows zero-shot model size interpolation. The key idea is to train the teacher and student jointly in a bidirectional distillation loop (“boomerang” scheme), where gradients from both directions are exchanged in a dynamically balanced manner. The method achieves smooth performance interpolation across different model scales and architectures (e.g., ViT and ResNet families), showing competitive results on ImageNet, CIFAR, and transfer benchmarks, while maintaining efficiency and data-free adaptability.\nThe paper introduces a technique termed boomerang distillation, in which a large “teacher” model is first distilled into a smaller “student” model, and then a family of intermediate-sized models is generated without further training by gradually re-inserting blocks from the teacher into the student. The resulting models interpolate in size (number of parameters) and in performance smoothly between student and teacher, and in many cases match or outperform traditional distilled models of the same size. The authors analyse the necessary conditions and compare the method to layer-pruning baselines, showing favorable results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Enables flexible interpolation between different model sizes without retraining, which is practical for model deployment across devices.\n2.Demonstrated across multiple architectures and datasets with consistent improvements and smooth interpolation curves.\n3.The paper goes into details about what components are necessary: e.g., the alignment loss, the initialization from teacher weights, etc. They also compare against layer-pruning baselines. This helps the community understand when/why the method works."}, "weaknesses": {"value": "1.It remains unclear how the method scales to much larger teacher models (tens or hundreds of billions of parameters) and whether the interpolation remains as clean in those regimes.\n2.The paper focuses on language models. It would strengthen the work to show applicability in other modalities (vision, multimodal) or architectures for broader impact.\n3.While no further training is required for each intermediate size, the approach still requires the initial distillation and alignment of a student model—which may itself be costly.\n4.The method assumes that student and teacher share compatible hidden dimensions and layer structure to allow patching. This may limit generality to heterogeneous architectures or differently sized hidden layers."}, "questions": {"value": "1.How sensitive is the method to the relative capacity of teacher and student (e.g., extreme size gaps)?\n2.If the teacher and student have different hidden dimensions, can patching still be applied?\n3.If the model is multimodal with fusion layers, could inserting teacher blocks disrupt cross-modal interactions, and how might you control or evaluate that risk?\n4.What is the practical GPU cost increase compared to standard KD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eQ6Sgb8azc", "forum": "4ZU8v4s3IR", "replyto": "4ZU8v4s3IR", "signatures": ["ICLR.cc/2026/Conference/Submission9984/Reviewer_7Ngh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9984/Reviewer_7Ngh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095378860, "cdate": 1762095378860, "tmdate": 1762921416112, "mdate": 1762921416112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment"}, "comment": {"value": "We thank the reviewers for their positive and helpful feedback. We appreciate how reviewers thought boomerang distillation was a \"novel” [7Ngh, s5vT, Cce4] and \"counter-intuitive” [dyqi] phenomenon that is “highly practical” [Cce4]. They found our paper to be “exceptionally well-written” [Cce4] and “clear” [s5vT], and our experiments “highly comprehensive” [Cce4] and “well-executed” [s5vT]. Below, we address the main concerns brought up by the reviewers. \n\n1. **Scaling to larger teacher models**\n\nReviewers 7Ngh and dyqi noted that our experiments were limited to models below 10B parameters. We ran new experiments on Pythia-12B and are running experiments on Qwen3-14B-Base, which will be added to the paper in the next few days. The results on Pythia-12B,  shown in Figure 14 of the revised paper, demonstrate that the boomerang distillation phenomenon is consistent even for larger model sizes. With these new experiments, we have now shown that boomerang distillation occurs across three orders of magnitude of model size (100M to 10B scale).\n\n2. **Lower bound on compression rate of the teacher**\n\nReviewers 7Ngh and Cce4 asked how much the teacher can be compressed. We added new experiments in Appendix E.2 showing that we can compress the teacher model up to 8.7x to initialize the student by manually choosing which layers to drop. We note that in the paper, we largely focus on the setting where the student model is initialized with every other teacher layer rather than maximally compressing the student, since initializing with every other layer provides maximum granularity in the possible interpolated models.\n\n3. **Architectural constraints of the student model**\n\nReviewers 7Ngh, s5vT, and Cce4 correctly note that the student’s architecture is constrained to have the same types of layers and same hidden dimension as the teacher model. While we agree this is a limitation, boomerang distillation offers significant computational advantages and a high degree of flexibility: we can create interpolated models between the student and the teacher of different sizes without any additional training. We compare boomerang distillation to ShortGPT and LaCO, two well-known pruning methods that also require the student to have the same architecture as the teacher. Our results show that we significantly outperform them, especially at high pruning rates, while offering the same degree of flexibility. Extending boomerang distillation to heterogeneous student-teacher pairs with different widths and head dimensions is an exciting future direction. We have updated our limitations section to highlight this potential restriction and future work.\n\n4. **Additional clarifications**\n\nIn response to reviewer s5vT’s request for an in-depth explanation and proof-of-concept experiment demonstrating why boomerang distillation works, we have provided **additional proof-of-concept results** in Appendix K showing that as the student layers are patched, the student model better approximates the last layer activations of the teacher. We have also revised our discussion in Section 2 to include an explanation of the intuition behind boomerang distillation. \n\nAdditionally, we have provided new experimental results in Appendix L showing the computational resources required for boomerang distillation (14.53-19.17x less than distilling individual models) as requested by Reviewer 7Ngh. \n\nWe thank the reviewers for their other comments requesting clarifications on boomerang distillation with other modalities [7Ngh], related work and router interpolation [s5vT], generation versus classification accuracy trends [dyqi], setup for Llama versus Qwen/Pythia [Cce4], and experimental datasets [Cce4]. We have revised the writing in the limitations, related work, and experimental sections to address these concerns. \n\nAgain, we are grateful to the reviewers for their constructive feedback, which led to additional experiments and revisions that have made the paper stronger and clearer. Between this general response and the individual responses below, we believe we have addressed all key concerns, and we welcome any further discussion or clarification."}}, "id": "LS1eN2rY95", "forum": "4ZU8v4s3IR", "replyto": "4ZU8v4s3IR", "signatures": ["ICLR.cc/2026/Conference/Submission9984/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9984/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission9984/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684783343, "cdate": 1763684783343, "tmdate": 1763684783343, "mdate": 1763684783343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}