{"id": "G8E67yaz8I", "number": 7014, "cdate": 1758004897656, "mdate": 1759897877913, "content": {"title": "Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models", "abstract": "Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice.\nIn the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently fed into a T2I backbone, which uses the enhanced instruction as textual input.\nTo further explore the core advantage of this paradigm, we conduct comprehensive studies on the Bridge Feature and Bridge Adapter.\nOur framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application.", "tldr": "", "keywords": ["Unified Muiltimodal Models", "Understanding Enhances Generation", "Diffusion Models", "Adapters"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d1d0b55c35138b321278c0c843b18f71e2f0fe5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Forge-and-Quench, a framework designed to enhance the fidelity and detail richness of images generated by Multimodal Large Language Models (MLLMs) and Text-to-Image (T2I) backbones. In the generation process, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a Bridge Adapter. This feature acts as a link, refining the generation process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The article is well-written, and the presentation is very clear.\n2. The authors conducted experiments on multiple datasets, including not only automatic evaluation but also extensive qualitative analysis."}, "weaknesses": {"value": "1. The novelty appears incremental: prompt recaptioning has already proven highly effective in works like DALL-E3 and Emu3, becoming common knowledge. Bridge Feature is essentially a SigLIP representation, and the adapter resembles a tiny diffusion model. Prior efforts (e.g., BLIP3-o) have denoised CLIP features to condition another diffusion model for image generation. The paper reads more like an engineering refinement of BLIP3-o via recaptioning tricks, wrapped in a “Forge-and-Quench” narrative.\n2. The main experiments pit the method against a pure diffusion baseline. Today’s leading image-generation models (e.g., Qwen-Image) already leveraged the visual-understanding power of MLLMs to improve generation. Simply introducing an MLLM already boosts DiT performance, so I remain skeptical of the gains attributed to the authors’ intricate designs; more competitive baselines should be included.\n3. Performance on Geneval actually drops. Since Geneval prompts are short, prompt rewriting should help (as shown in Emu3). A decline seems to suggest that the enhanced prompt and Bridge Feature fed into the DiT do not act synergistically.\n4. If we only recaption prompts, how well does a training-free approach—feeding the rewritten text directly into a freeze DiT such as flux—perform on Geneval and WISE? The authors should analyze this. For example, WISE already provides rewritten prompts; I think authors can compare t2i performance directly using these prompts for freezed DiT against the original prompt + proposed FaQ framework.\n5. With the main baseline already limited, ablation studies are crucial. I do not see why COCO-30K FID was chosen as the sole metric for these ablations."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PKj4iYy5Et", "forum": "G8E67yaz8I", "replyto": "G8E67yaz8I", "signatures": ["ICLR.cc/2026/Conference/Submission7014/Reviewer_NTLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7014/Reviewer_NTLm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761076717278, "cdate": 1761076717278, "tmdate": 1762919224778, "mdate": 1762919224778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method that leverages a Bridge Feature to enrich semantic information in the text-to-image generation process. Without relying on any additional reference images, the approach produces high-quality visual features to guide the image synthesis, thereby improving both the fine-grained detail and overall quality of the generated images. The authors validate their method on two T2I backbones — FLUX.1-dev and MeiGen-Image — and evaluate its performance across five benchmarks, measuring improvements in terms of image quality, instruction-following capability, and other relevant dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces a novel Bridge Feature approach that generates high-quality visual features from text alone, effectively bridging the gap in scenarios where traditional T2I methods rely on real image features.\n* The proposed framework is modular, with both the MLLM and T2I backbones kept frozen while only lightweight adapters are trained, which reduces training cost and improves portability across different model architectures.\n* The method is validated on two distinct T2I backbones and evaluated using five benchmarks covering various dimensions."}, "weaknesses": {"value": "* The composition of the training data is not explicitly discussed, which makes it difficult to assess the proposed method’s performance in both in-domain and out-of-domain scenarios. Since the Bridge Feature requires supervised training, further comparison between the proposed method and the base model in out-of-domain cases would be necessary to understand its generalization capability.\n\n* The paper mentions that input prompts are first expanded into Enhanced Text $ t^* $, a process which empirically may alter the fine-grained details of the generated images. However, the potential impact of this semantic expansion on image fidelity and alignment with user intent is not sufficiently discussed in the experimental section."}, "questions": {"value": "* Could the authors clarify the domain composition of the training dataset? Are there any experimental results that explicitly evaluate the proposed method’s ability to generalize across domains?\n\n* In the Forge-and-Quench framework, what is the performance when only the Enhanced Text $ t^* $ is used as input, or when only the Bridge Feature is used, without the other? Such an ablation would help quantify the individual contributions of semantic enrichment and synthetic visual features to the final image quality.\n\n* Since the Injection Adapter needs to be trained specifically for a given T2I backbone, this design could limit its generalization to other architectures. Could the authors provide more details on the adapter’s training cost, and how it compares to the overall framework’s training complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TtcvtwmtJW", "forum": "G8E67yaz8I", "replyto": "G8E67yaz8I", "signatures": ["ICLR.cc/2026/Conference/Submission7014/Reviewer_ifvy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7014/Reviewer_ifvy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761144397691, "cdate": 1761144397691, "tmdate": 1762919224212, "mdate": 1762919224212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Forge-and-Quench, a unified multimodal framework that aims to enhance the fidelity and detail quality of generated images by tightly coupling an MLLM with a Text-to-Image diffusion backbone.\nThe central insight is that current unified multimodal models (e.g., MetaQuery, BLIP3-o) treat the MLLM as a static prompt rewriter, passing only high-level semantic embeddings to the diffusion model. The authors argue this creates an informational bottleneck that limits fine-grained detail control. Extensive experiments on FLUX.1-dev and MeiGen-Image backbones show that Forge-and-Quench substantially improves FID, GPT-Fidelity, and visual realism, while maintaining comparable prompt-alignment (GenEval, DPG-Bench) and even enhancing world-knowledge reasoning (WISE)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper’s modular adapter design elegantly separates understanding (Forge) and generation (Quench), enabling compatibility with a variety of existing MLLM and T2I models without requiring joint pre-training or large-scale end-to-end optimization.\n2.The method demonstrates consistent qualitative improvements in visual fidelity and texture realism across both tested backbones, showing that injecting a virtual visual feature can strengthen fine-grained detail synthesis while maintaining reasonable instruction alignment."}, "weaknesses": {"value": "1.The enhanced-text stage is not novel, as similar reasoning-based prompt enrichment has been extensively studied in earlier works such as Bagel, T2I-R1 and GoT, yet the paper does not acknowledge or discuss these predecessors specifically. The bridge-feature component largely replicates what Emu2, Seed-X, PUMA have already done with CLIP-space feature injection, and the paper fails to analyze how its approach differs conceptually.\n2. The method performs worse on GenEval and DPG-Bench benchmarks, but the paper does not provide any analysis or explanation for these drops.\n3.The qualitative section does not actually show the MLLM’s internal reasoning or thinking process, leaving the “understanding-to-generation” connection mostly unsubstantiated.\n4.The approach is highly sensitive to the choice of target visual feature space, with ablations showing large disparities and pronounced noise fragility for SigLIP2, yet the paper provides no principled rationale or robustness strategy beyond defaulting to SigLIP-ViT.\n5.The qualitative evaluation includes only one example per prompt, which makes the comparison potentially cherry-picked rather than representative. When I attempted to reproduce examples with FLUX, the “distorted zebra” artifact highlighted in the paper turned out to be rare and atypical."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mvnwY6hhRj", "forum": "G8E67yaz8I", "replyto": "G8E67yaz8I", "signatures": ["ICLR.cc/2026/Conference/Submission7014/Reviewer_x7Vi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7014/Reviewer_x7Vi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789616945, "cdate": 1761789616945, "tmdate": 1762919223711, "mdate": 1762919223711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Forge-and-Quench, a framework that uses an MLLM to refine instructions and a Bridge Adapter to convert them into a latent visual “Bridge Feature,” guiding a T2I model toward higher fidelity and detail. The authors claim that this structure effectively transfers understanding-derived visual insights into the generation pipeline. This design is also compatible with different MLLMs and T2I backbones while minimizing training costs and preserving the original model’s multimodal understanding ability. Experiments across two models show improved image fidelity, richer details, and better instruction-following behavior, along with enhanced use of world knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, and the overall framework is easy to understand and follow.\n\n2. The observation that introducing a reference image can improve fidelity and detail richness in generated images is insightful and valuable. The experimental results effectively demonstrate this contribution."}, "weaknesses": {"value": "1. The primary weakness lies in the design of how the reference image is created. The authors propose using a Bridge Adapter to map the MLLM-generated text into a SigLIP image feature via a rectified flow model, but this design seems unintuitive. The mapping step still introduces information loss. In contrast, BLIP-3o directly generates the SigLIP image embedding in a rectified flow manner via their unified model and then generate a high-fidelity image from that embedding, which is more consistent and avoids this loss. In fact, BLIP-3o’s architecture naturally follows the same conceptual pipeline as the authors’ idea, first produce an abstract image representation, then generate it.\n\n2. Another concern is that the comparison is insufficient. The authors only compare against their base model and omit comparisons with alternative frameworks that achieve similar functionality, such as BLIP-3o. Including such comparisons or at least providing discussion would strengthen the argument for effectiveness.\n\n3. A minor weakness: the authors should clarify why their method achieves better performance on the WISE benchmark, which focuses on reasoning-driven image generation. The current results suggest that the improvement may come primarily from the MLLM component rather than from the proposed architectural innovations.\n\nAt this stage, I lean toward a weak accept because the paper provides interesting insights. If the authors can satisfactorily address the issues raised in my review, I would be open to maintaining or even improving my score."}, "questions": {"value": "1. Is the proposed reference-image creation pipeline better than BLIP-3o?\nSince the Bridge Adapter still introduces information loss when mapping text → SigLIP feature, why not directly generate SigLIP features as BLIP-3o does? What concrete advantages does the proposed design offer?\n\n2. Why does the method outperform on the WISE benchmark?\nIs the performance gain mainly due to the MLLM component, or is it attributable to the proposed Bridge Adapter  design? Please clarify the source of improvement.\n\n3. Where is the citation of the backbone, MeiGen-Image and FLUX.1-dev? \nThe paper does not clearly cite the backbone models used, MeiGen-Image and FLUX.1-dev, especially MeiGen-Image which is difficult to locate or search online. Please provide proper citations or references (e.g., paper / project page / GitHub link) and clarify how these backbones are used in your framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DBaOMdbWu1", "forum": "G8E67yaz8I", "replyto": "G8E67yaz8I", "signatures": ["ICLR.cc/2026/Conference/Submission7014/Reviewer_r2dT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7014/Reviewer_r2dT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812753660, "cdate": 1761812753660, "tmdate": 1762919223406, "mdate": 1762919223406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}