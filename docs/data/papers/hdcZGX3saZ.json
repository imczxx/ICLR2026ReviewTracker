{"id": "hdcZGX3saZ", "number": 18852, "cdate": 1758291532792, "mdate": 1759897077730, "content": {"title": "SNOV: A Scalable Near-global Optimal Verifier for   Neural Networks under Large Perturbations", "abstract": "Neural networks excel across many tasks, but their use in safety-critical settings (e.g., power grids) is hindered by concerns about robustness to input perturbations. Verification methods provide formal guarantees, yet existing approaches face a trade-off: exact verifiers certify global optimality but struggle to scale, while relaxation-based methods scale efficiently but yield loose bounds. We propose a hybrid verifier that achieves $\\varepsilon$-global optimality by combining a nonlinear programming (NLP) local solver with convex-relaxation–based bound propagation inside a branch-and-bound framework. Convex relaxations narrow the feasible region and warm-start the NLP, while the NLP supplies tight upper bounds that guide branching and enable early termination. The proposed strong branching strategy effectively selects neurons with high sensitivity and instability, quickly tightening bounds within each branch. Parallel GPU execution further accelerates convergence. Experiments on  high-dimensional image classification benchmarks demonstrate substantial speedups, significant branch reduction, and negligible optimality gaps compared to six state-of-the-art baselines. Our results highlight the effectiveness of integrating local exact solvers with scalable relaxation methods to certify robustness of modern neural networks.", "tldr": "SNOV integrates nonlinear programming with convex relaxations in branch-and-bound, enabling scalable near-global optimal verification that unites efficiency and rigor for trustworthy AI in safety-critical domains.", "keywords": ["neural network verification", "global optimal", "nonlinear programming", "branch and bound", "bound propagation", "parallel computing", "scalability", "upper bounds", "lower bounds", "power flow", "power grids"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b4e3ceb2bc166b9095de7d4662478831aa5fbbe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript introduces SNOV, a framework that combines $\\beta$-CROWN lower bounds with a NLP solver for upper-bounds, all within the Branch-and-Bound (BaB) algorithm, enhanced by slope-guided warm starts and low-rank KKT updates. Some preliminary results related to verifications of neural networks are presented."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Although the BaB framework is well established for neural network verification, the authors aim to enrich it with engineering heuristics, such as slope-guided warm starts and low-rank KKT updates, that can reduce the overall computational cost."}, "weaknesses": {"value": "At present, the paper does not convincingly showcase modeling capabilities on substantive application cases, and the theoretical developments seem largely incremental relative to known CROWN/BaB hybrids. The experimental section lacks the breadth and controls needed to support the efficiency claims (comprehensive baselines, ablations, and scaling studies). Clarity also suffers from repetition, ambiguous formulations, and language issues. \n\nSubstantial improvements could be made by addressing both the general comments and the specific remarks listed below.\n\n### Major points\n\n- Lack of consistency: there are some objects that are not denoted in the same way throughout the paper: feasible set is sometimes $\\mathcal{X}$, sometimes $\\mathcal{C}$, sometimes $\\mathcal{B}$; the radius of the ball is $\\gamma$, $r$, $\\epsilon$ or $\\epsilon_B$; the linear function of the last layer is $c$ or $\\xi$. There’s inconsistent use of bold versus regular text. Some sentences are very informal, e.g., page 3, lines 149–150, or page 7, lines 360–362. Moreover, some objects or notations are used without being properly defined, e.g., per-neuron slopes, unstable fractions, strong-branching score, etc. Overall, the paper is extremely hard to read and follow, and presentation needs to be improved.\n\n- Literature review is quite incomplete. Not all exact verification methods are MIP-based, and for example, SAT/SMT methods are never discussed. Similarly, approximate methods encompass a much larger number of works (abstract interpretation, zonotope domains, SDP relaxations) than the ones mentioned in Section 3.2.\n\n- Experimental setup is not sufficient for deriving rigorous conclusions. Indeed, it is based on verifying **one single instance**, and the detailed network architecture—crucial for understanding the true problem size—is never mentioned. The paper also lacks a systematic analysis of how performance scales with perturbation magnitude.\n\n- Computing exact optimal values $f^{*}$ with “exact or high-fidelity solvers” remains highly non-trivial in high-dimensional settings. No details are provided about this, despite this value being used to measure absolute and relative gaps.\n\n### Minor points\n\n- There should be a clear distinction between a given function, say $f$, and its evaluation at a particular point $x\\in\\mathbf{R}^n$, i.e., $f(x)$.\n- If $\\gamma$ is a real number, there is need to write things like $||\\gamma||_{\\infty}$. This is related to the lack of consistency point.\n- The first part of Section 6 includes quite extensive solver-configuration details (e.g., sort_domain_interval=1) that are not essential for understanding the paper and should be moved to the Appendix. This would free space to describe the network architectures and core notions more clearly."}, "questions": {"value": "1. Which exact $\\beta$-CROWN variant is used? A short algorithmic description would help.\n\n2. Why does splitting change the KKT system with rank at most $3$ in your formulation?\n\n3. What is the exact \"strong-branching score\" used (formula, weights, normalization)?\n\n4. In what sense exactly does the framework go beyond known BaB + LiRPA frameworks? Can you isolate a lemma or proposition that is new (not just a rephrasing) and explain its impact on tightness or complexity?\n\n5. How do your results aggregate over larger sets (e.g., $100$–$1000$ test inputs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3cztXCDxhZ", "forum": "hdcZGX3saZ", "replyto": "hdcZGX3saZ", "signatures": ["ICLR.cc/2026/Conference/Submission18852/Reviewer_bdeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18852/Reviewer_bdeZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760601585180, "cdate": 1760601585180, "tmdate": 1762930820417, "mdate": 1762930820417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the past years, neural network verifiers based on linear relaxations have been shown to be able to scale to verify large neural networks when combined with efficient branching heuristics. However, these methods struggle to provide tight certificates when perturbation radii are large. This work proposes a hybrid verification approach which combines convex-relaxation-based bound propagation methods to obtain lower bounds with more precise nonlinear-programming solvers for obtaining upper bounds. Both methods run in parallel and exchange information, enabling better branching decisions, fast warmstarts in the NLP solver and accelerated convergence to the true optima. The experimental evaluation demonstrates the capability of the proposed method which achieves a precision comparable to that of MIP solvers in significantly less time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The verification of neural networks is an important research topic\n- Tackling the inability of bound-propagation-based verifiers to scale to large perturbation radii is a valuable contribution\n- The experimental results are impressive, showing that the proposed method achieves MIP-like precision in significantly less time\n- The authors introduce formulations which also enable the approach to be extended to transformers"}, "weaknesses": {"value": "- My biggest concern about the paper is the empirical evaluation. In neural network verification, we are generally interested in whether a network is robust or non-robust on a particular input for a given perturbation. There is little benefit in being able to obtain very precise bounds on the specification if loose bounds are already sufficient to prove that a property holds. Looking at the results, it seems that a number of experiments are done for cases where even the true lower bound is $<0$, this means that a counterexample exists. A cheap algorithm such as projected gradient descent could just be run to obtain counterexamples, this is done by any neural network verification tool before bound propagation is even started. Comparing the performance of different bound propagation methods in such cases doesn't make a lot of sense. I understand that PGD might not always find a counterexample, but with such large perturbations and lower bounds well below zero it should not be hard to find counterexamples. The only case where verification is actually possible (true lower bound $>0$) is that shown in Table 2. However, the cheap lower bounds obtained by $\\alpha$-CROWN here are already sufficient to verify robustness, hence, SNOV does not provide any benefits in this case and is significantly slower than $\\alpha$-CROWN and even slower than MIP. A proper evaluation should run SNOV against other state-of-the-art verifiers (such as GCP-CROWN [1]) on established benchmarks (such as those from [2]) and compare the verification time as well as the number of verified instances.\n- Although the general concept of the SNOV verifier makes sense to me, there are a number of details on the algorithm (e.g. the rank-3 warm-starts) that are missing in the paper, therefore I am unable to assess whether the overall algorithm is correct. The authors also repeatedly mention that the NLP yields \"incumbents and dual-like signals that drive strong branching\" but the strong branching heuristic and the information flow from the NLP to $\\beta$-CROWN is not explained in the paper.\n- It is unclear how $\\alpha$-CROWN and the other competing methods are evaluated. Is only a single bound propagation pass performed with these methods? If so, it is not surprising that they would produce looser bounds than the proposed method. SNOV should be evaluated against a complete verifier which performs branching as well (such as $\\alpha, \\beta$-CROWN and also GCP-CROWN which is enhanced with cutting planes). By running those for the same time budget as SNOV, a proper comparison between the methods would be possible.\n- The runtime of the algorithm is significantly higher than that of other methods such as $\\alpha$-CROWN and seems to explode when evaluated on slightly larger datasets such as CIFAR10 (see Table 3). I am therefore unsure whether the proposed method would actually scale to networks and datasets of practical size.\n- The empirical evaluation lacks important details: Details on the neural network architectures that are evaluated are missing. The authors say that \"We implement SNOV for MLP, CNN, ResNet, and Transformer architectures across three benchmark suites.\", does this mean that all results in the paper are averages across these architectures? Separate results should be reported for these architectures. Besides this, the methods should be evaluated for multiple inputs and not only for one input as is currently done.\n- Section 3.1 is quite imprecise and dense, therefore difficult to understand. E.g. what do the authors mean by \"induce loose big-M\" or \"binaries explode\"? This should be extended to provide more context and explain the points that are being made here in more detail.\n- The section on \"Low-rank KKT updates\" (line 234ff) is dense and difficult to understand. A lot of the concepts being referred to here are not introduced.\n- The algorithm introduces a number of hyperparameters ($\\vartheta, \\phi$) but there are no ablation studies and little justification for how these are selected.\n- The authors state that some experiments are run on a small Mac machine while others are run on a server with 64 GPUs. It is unclear which experiments are run on which machine which is important to be able to assess the runtimes that are provided. Besides this, the type of GPUs and CPUs should be provided, and it should be clarified whether the approach runs on all 64 GPUs in parallel.\n- The paper is full of typos, grammatical errors and incomplete sentences. I tried listing some of them below but eventually stopped taking note of all of them while reading. I would encourage the authors to thoroughly revise the paper with a focus on grammar and language.\n\n\n### Minor weaknesses and typos\n- The notation in the paper is somewhat chaotic and never properly introduced. The authors use $l, u$ as well as $\\underline{l}, \\overline{u}$ and $\\underline{L}, \\overline{U}$ to denote lower and upper bounds. E.g. in line 65, both are used in exactly the same context. The notation should be clarified and, if applicable, unified.\n- Line 130: Shouldn't the objective function being minimised here be $f(x)$ which is previously introduced as the function representing the specification? $s(x)$ is never introduced\n- Figure 3: What are $\\xi(0)$ and $\\xi(1)$? $\\xi$ is introduced as the \"specification coefficients\" in line 122, but it's unclear what the indexing refers to here.\n- Figure 6: The overapproximation area in the left part of the figure extends outside the blue bounds which is incorrect. The figure should be corrected so that the overapproximation is entirely contained in the lower/upper bounds.\n\n- Line 121: is a task-specific specification **is** represented by function --> is a task-specific specification represented by **a** function\n- Line 122: One of the common specification is --> One of the common specification**s** is\n- Line 123: see Section for particular examples. --> Which section to the authors refer to here?\n- Line 303: \"ReLU can be written as the solution of the projection quadratic programming\" --> this is not a sentence and I don't understand what it means. The authors should fix the grammatical errors here.\n- Line 352: We observe that bound propagation based methods efficiently produce the bounds but far loose --> We observe that bound propagation based methods efficiently produce **bounds, but that they are far too loose**\n- Line 353: The relative relaxation gap $\\overline{\\Delta}_{0.1}$ are between --> The relative relaxation gap $ \\overline{\\Delta} _{0.1} $ **is** between \n- Line 353: \"Table 1 and Table 2.\" is not a valid sentence, this needs to be rewritten.\n- Line 355: Section5 --> Section 5\n- Table 1 shows that NLP solver reaches up to 7 times faster than MIP --> Table 1 shows that **the** NLP solver reaches up to 7 times faster than MIP. Also what is reached here? The sentence makes no sense in its current form\n- Line 361: making the MIP **is** faster than NLP --> making the MIP faster than NLP\n- Line 364: algorithm taking advantages of --> algorithm taking **advantage** of\n- Line 365: the efficiency of bound propagation method --> the efficiency of **the** bound propagation method\n- Table 1 and all other tables: \"when large perturbations\" should be replaced with \"**under** large perturbations\". Also \"Verifying One Image of MNIST Dataset\" should be \"Verifying One Image of **the** MNIST Dataset\"\n- Line 401: with the lower bounds of α−CROWN method --> with the lower bounds of **the** α−CROWN method\n- Line 403: to take **a** hundreds of seconds --> to take hundreds of seconds\n- Line 404: the proper initialization of NLP solver --> the proper initialization of **the** NLP solver\n- Line 405: the efficiency of NLP has about 18 times improvement --> the efficiency of NLP **improves by a factor of 18$**\n- Line 407: As shown in Section 5 **that** bound propagation method like α−CROWN is sensitive to large perturbations --> As shown in Section 5, bound propagation method**s** like α−CROWN **are** sensitive to large perturbations,\n- Line 408: Grammar, producing a too loose lower bounds --> producing loose lower bounds\n\n### References\n\n[1] Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C.-J. & Kolter, J.Z. (2022) General Cutting Planes for Bound-Propagation-Based Neural Network Verification. doi:10.48550/arXiv.2208.05740.\n\n[2] Brix, C., Bak, S., Johnson, T.T. & Wu, H. (2024) The Fifth International Verification of Neural Networks Competition (VNN-COMP 2024): Summary and Results. doi:10.48550/arXiv.2412.19985."}, "questions": {"value": "- How are the competing bound propagation methods run? Is branching conducted for these?\n- How were the hyperparameters selected?\n- Which experiments are run on which of the machines and are all of the GPUs used in parallel?\n- The authors claim to solve the problem to $\\epsilon$-optimality but then employ a heuristic stopping criterion which, as far as I see, does not guarantee $\\epsilon$-optimality. Could the authors clarify what exactly they mean when describing the optimality here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7KTrvQYzcE", "forum": "hdcZGX3saZ", "replyto": "hdcZGX3saZ", "signatures": ["ICLR.cc/2026/Conference/Submission18852/Reviewer_xn2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18852/Reviewer_xn2U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395748583, "cdate": 1761395748583, "tmdate": 1762930819723, "mdate": 1762930819723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel neural network verifier, named SNOV, that combines state-of-the-art branch-and-bound based on linear relaxations with the use of of a non-linear-programming solver (NLP) to compute upper bounds, as opposed to the customary use of local grandient-based optimizers and primals from the lower bounding algorithms.\nThis is aimed at what the authors call the \"large-perturbation regime\".\nExperimental results show that SNOV is significantly faster than the employed MILP solver on the considered benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea to use NLP for BaB upper bounds is novel and potentially very interesting for the neural network verification community. \nIt does seem to be significantly faster than the considered MILP solver on the provided benchmarks."}, "weaknesses": {"value": "What I believe to be the main weaknesses of the paper are related to the presentation and the experimental section.\n\n**Presentation**.\n\nThe presentation assumes familiarity with concepts related to interior-point methods. I do not think this is reasonable, and the authors should provide at least a basic introduction in the appendix. KKT conditions are mentioned (and are related to some of the technical improvements) without any technical explanation. Given that neural network verification is a mixed community, with people coming from ML, from formal methods, and from optimization, this appears to be particularly necessary.\n\nFurthermore, important details are omitted: a new branching strategy is introduced, but no details are presented. It is unclear to me what networks are employed for the experiments. It is also not quite clear what is the precise purpose of the complementarity reformulation.\n\nAdditionally, I would encourage the authors to tone down the narrative a bit. For instance, it's hard to say that the proposed algorithm has \"consistent improvements in scalability and reliability over six state-of-the-art baselines\". Improvements are in either scalability or accuracy, and in a very limited experimental setup (see below), and most of these baselines are really far from the state-of-the-art for neural network verification (IBP has never been, for instance, it is mostly used for training purposes).\n\n**Experiments**.\n\nThe experiments appear to be carried out over extremely small networks (judging from the MILP runtimes), and over relatively small perturbation radii (it is common to use up to $\\epsilon=0.3$ and $\\epsilon=8/255$ for MNIST and CIFAR-10, respectively).\nFurthermore, no branch-and-bound baseline is provided. \nSpeed improvements over MILP solvers are not surprising of their own, and provide no indication on whether the proposed approach would be beneficial to the state-of-the-art (for instance, whether it speeds up alpha-beta-CROWN).\nThe fact that no code is provided makes it even harder to assess the experimental results.\n\n\nGiven the inconclusive experimental section and the rushed presentation, I do not believe the paper is ready for publication at this stage. But I would be happy to increase my score if these concerns are addressed."}, "questions": {"value": "- Are you running the NLP sequentially over each branch-and-bound subdomain? If so, is the point associated to the current subdomain  upper bound feasible for it (in other words, does it satisfy all split constraints)? How can this scale? \n- How are intermediate pre-activation bounds set for the MILP? MILP solvers appear to benefit from very tight bounds (see discussion in the GCP-CROWN paper).\n- Can you include other scalable BaB methods (the standard alpha-beta-CROWN, at the very least) among the baselines?\n- Could you please comment on \"A Branch and Bound Framework for Stronger Adversarial Attacks of ReLU Networks\", Zhang et al., ICML22? This work focuses on improving the upper bounding part within branch and bound, and it seems extremely relevant for the submission. It would be very important to also benchmark against it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gjFdLZHIvC", "forum": "hdcZGX3saZ", "replyto": "hdcZGX3saZ", "signatures": ["ICLR.cc/2026/Conference/Submission18852/Reviewer_TAhY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18852/Reviewer_TAhY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582902908, "cdate": 1761582902908, "tmdate": 1762930818954, "mdate": 1762930818954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SNOV, an exact neural network verifier that combines NLP-based primal heuristics with dual bounds obtained from $\\alpha$-$\\beta$-CROWN.\n\nOverall, the paper presents several interesting ideas, but I find the presentation to be lacking several important mathematical details, especially regarding the parts that appear to be new in the paper. Furthermore, numerical results are incomplete / hard to follow, and I found some flaws in the experimental setting / solver comparison. My score is largely motivated by the fact that key information is missing from the paper, and the limitations of the numerical experiments. I believe the core ideas have some merit but the paper needs a significant re-write before being ready for publication."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper considers large input-domain perturbations, which is less commonly tackled in existing verification literature\n* The paper leverages primal and dual information to accelerate the efficiency of NLP primal search and dual $\\alpha$-$\\beta$-CROWN bound propagation\n* The idea of low-rank KKT updates following branching on a neuron activation is new, however its presentation could have been more exhaustive"}, "weaknesses": {"value": "* I do not consider the \"hybrid scheme\" (Section 3.2) of SNOV to be a strong novelty. As noted in the introduction, existing verifiers combine a primal (to find adversarial examples if they exist) and a dual component (to obtain certified bounds). For instance, $\\alpha-\\beta$-CROWN implements primal heuristics like gradient-based attacks.\n* Section 3 partitions existing works into \"exact\" MIP-based vs \"approximate\" branch-and-bound methods; I do not agree with the paper's classification. \n  * The standard terminology in the NN verification literature is to distinguish between\n    * _complete_ verifiers: methods that are guaranteed to either certify robustness or find an adversarial example given sufficient time. Virtually all such methods are based on a mixed-integer representation of the trained neural network, input domain and verification property; methods and tools differ in how they evaluate primal/dual bounds and in their implementations. $\\alpha$-$\\beta$-CROWN, Marabou, nnenum, CORA, etc... are complete verifiers.\n    * _incomplete_ verifiers: methods that may terminate without a definitive answer, i.e., which are heuristic in nature. CROWN, $\\alpha$-CROWN or gradient-based attacks are incomplete. Note that complete verifiers often combine a incomplete verifier with a branch-and-bound scheme.\n  * the paper's classification between Mixed-Integer Programming and Branch-and-Bound does not capture the fact that i) MIP-based methods rely on branch and bound for completeness and ii) the methods cited in Section 3.2 are based on MIP formulations.\n\n* Building on the above comment, Section 3 would benefit from a re-organization, and deserves additional relevant references such as i) existing verifiers such as CORA, Marabou, nneum, etc.. and ii) existing works that propose primal algorithms, eg.:\n  * [_Optimization Over Trained Neural Networks: Taking a Relaxing Walk_](https://arxiv.org/abs/2401.03451)\n  * [_Nonlinear Optimization with GPU-Accelerated Neural Network Constraints_](https://arxiv.org/abs/2509.22462)\n\n* Several methodological components are mentioned but not explained, e.g. the paper makes several mentions of \"strong branching scores\" but these do not appear to be described anywhere in the paper\n* Tables 1-7 have inconsistent notations: some use $|\\gamma|_{\\infty}$, some use $\\gamma$. \n* The use of $\\gamma$ notation also conflicts with the notation $\\epsilon_{B}$ used to define the input domain at the beginning of Section 6. * In Tables 4 & 5, subscript $u$, $u_{ini}$ and $u_{adj}$ are not defined.\n* Table 3 and Table 6 are identical\n* When solving verification tasks, one can terminate the solve as soon as an adversarial example is found or the instance is proven to be robust. Adding this termination criterion would likely affect the performance results reported in Section 6\n* Several claims, e.g., lower number of branch-and-bound nodes, are not supported by any results\n* Parts of the text are not proper English sentences, e.g. \"Table 1 and Table 2.\" (l. 353)"}, "questions": {"value": "* Can the authors comment on the choice of representing ReLU activation using complementarity constraints as opposed to simply representing them as nonlinear functions in the NLP formulation?\n* How would the complementarity constraint approach handle non-piecewise activation functions, e.g., sigmoid?\n* Section 4 makes several mentions of $\\beta$-CROWN as the method used to obtain node-level lower bounds. Should this have been $\\alpha$-CROWN instead? Several parts of the text refer to algorithmic components of $\\alpha$-CROWN."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RriX9Ml0A8", "forum": "hdcZGX3saZ", "replyto": "hdcZGX3saZ", "signatures": ["ICLR.cc/2026/Conference/Submission18852/Reviewer_Lits"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18852/Reviewer_Lits"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756210189, "cdate": 1761756210189, "tmdate": 1762930818445, "mdate": 1762930818445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}