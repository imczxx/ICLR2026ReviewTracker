{"id": "HhBxIQ0CA2", "number": 13817, "cdate": 1758223124479, "mdate": 1759897410611, "content": {"title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation", "abstract": "Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision–language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy.\nTo this end, we propose a compact 3B-parameter Vision–Language–Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer \"Is this the target object?\" and \"Why should I take this action?\" The reasoning process unfolds in three stages: \"think\", \"think summary\", and \"action\", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.", "tldr": "", "keywords": ["object navigation", "language-driven object navigation", "embodied ai", "vlm", "vla"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1729a6c93747c439439d9a3b678112ffd307a8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VISOR (VIsual Spatial Object Reasoning), a novel framework for language-driven object navigation embodied in a compact, unified 3B-parameter Vision-Language-Action (VLA) model. The work identifies key limitations in two dominant paradigms: (1) end-to-end policy-based methods that struggle with generalization and explainability, and (2) modular, pipeline-based methods that suffer from error propagation and high inference costs. VISOR proposes an elegant middle ground by reformulating the navigation task as an explicit, image-grounded reasoning problem. Instead of predicting low-level actions, the agent selects from a set of high-level waypoint candidates projected directly onto its panoramic visual observation. The core of the method is a three-stage generative process—\"think\", \"think summary\", and \"action\"—which forces the model to articulate its reasoning before making a decision. To train this model, the authors introduce WAYS-Bench, a new dataset built upon GOAT-Bench, which provides supervision for waypoint selection and includes reasoning traces generated by a large-scale LLM. The model is trained in two stages: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) post-training. The authors demonstrate that VISOR, despite its compact size, achieves strong generalization to unseen environments and provides inherent explainability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's primary strength lies in its reformulation of navigation from a low-level control problem to a high-level, visually-grounded waypoint selection task. Projecting candidate waypoints as labels onto the panoramic image and having the model reason about which label to choose is a highly innovative approach that elegantly discretizes the action space and naturally integrates with the reasoning capabilities of VLMs.\n- The paper makes a compelling case for the \"CURE\" properties. By creating a unified, single-model agent of a practical size (3B parameters) that eschews external modules like object detectors or segmenters, VISOR presents a more architecturally clean and deployment-friendly alternative to complex, multi-model pipelines.\n- The creation of WAYS-Bench is a significant contribution in its own right. As the first dataset designed for supervised fine-tuning of waypoint selection with reasoning traces, it provides a valuable resource for the community and enables a new direction of research in training reasoning-capable navigation agents."}, "weaknesses": {"value": "- The reasoning capability of VISOR during the SFT stage is fundamentally bootstrapped by distillation from a much larger, proprietary model (GPT-4o). This dependency is a critical aspect of the method. The performance is thus capped by the quality of the teacher's reasoning traces, and it's unclear how much of the reasoning ability is \"learned\" versus \"memorized\" from these distilled examples.\n- The waypoint generation relies on projecting depth data and applying DBSCAN clustering. This crucial preprocessing step is presented as a fixed procedure, but its robustness is not analyzed. It is sensitive to the quality of depth maps, camera parameters, and the choice of clustering hyperparameters, and potential failures in this stage would directly propagate to the policy.\n- The results in Table 3 show that VISOR is outperformed by other methods (e.g., MTU3D, Uni-NaVid) on the \"Val Seen\" splits of the OVON benchmark. While the paper emphasizes generalization, this performance gap suggests that the explicit reasoning and waypoint selection process may be less efficient or direct than other methods in familiar environments.\n- All experiments are conducted in the Habitat simulator. While this is a standard and challenging benchmark, the sim-to-real gap remains a major hurdle in robotics. The reliance on panoramic RGB-D data and a shortest-path planner makes direct real-world deployment non-trivial. The absence of any real-world validation is a notable limitation for an embodied AI paper."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCYj9zs9Fw", "forum": "HhBxIQ0CA2", "replyto": "HhBxIQ0CA2", "signatures": ["ICLR.cc/2026/Conference/Submission13817/Reviewer_oKXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13817/Reviewer_oKXw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562339743, "cdate": 1761562339743, "tmdate": 1762924345644, "mdate": 1762924345644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **VISOR**, a ~3B-parameter Vision–Language–Action agent for language-guided object navigation. VISOR reframes high-level control as **selecting a labeled waypoint directly on the panoramic image**, then executing a shortest-path planner to that waypoint. The agent consumes panoramic RGB together with an online top-down map; candidate waypoints are obtained by inverse-projecting depth to valid pixels, clustering them, and overlaying randomly sampled alphanumeric labels on the panorama to enforce image-grounded reasoning. The model produces structured outputs with three tags—`<think>`, `<think summary>`, and `<action>`—to improve interpretability. Training uses supervised fine-tuning on **WAYS-Bench**, followed by RL post-training with **GSPO**, combining a format reward and an action reward that covers both label choice and **Stop**. To mitigate reward hacking, RL relies on a Stop/non-Stop balanced dataset and retains KL regularization. The model is evaluated on **CoIN-Bench** (InstanceObjectNav) and **OVON** (ObjectNav)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach makes it easy for practitioners to understand the model’s outputs and the reasons behind them via structured reasoning tags.\n2. The paper introduces waypoint-level supervision to support training reasoning-capable embodied navigation agents, addressing a gap not covered by existing datasets.\n3. The method and experimental conclusions are described clearly, making the overall contribution easy to follow."}, "weaknesses": {"value": "1. The paper should clearly argue—ideally with ablations—the necessity of using three cameras/panoramic inputs for the proposed method.\n2. The absolute performance lags SOTA. On OVON, for example, *Val Seen* SR: DAgRL 41.3 vs. VISOR-GSPO 21.7; *Val Unseen* SR: MTU3D 40.8 vs. VISOR-GSPO 22.0. On CoIN-Bench, VISOR-GSPO improves over SFT but SR remains modest overall.\n3. The paper should quantify the contributions of the `<think>` and `<think summary>` components to final performance, and provide deeper analysis of the respective roles of SFT and RL during training. Based on current results, the **incremental gains from RL** appear limited and warrant further investigation."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mbF246xPsG", "forum": "HhBxIQ0CA2", "replyto": "HhBxIQ0CA2", "signatures": ["ICLR.cc/2026/Conference/Submission13817/Reviewer_roEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13817/Reviewer_roEC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905510643, "cdate": 1761905510643, "tmdate": 1762924345084, "mdate": 1762924345084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing language-driven object navigation methods fall into two categories: one trains VLMs in an end-to-end manner but shows limited generalization and explainability; the other adopts modular zero-shot pipelines but falls short in learning and computation cost. This paper proposes VISOR, a compact, unified, reasoning-capable, and explainable VLA model for the language-driven object navigation task. This 3B model jointly reasons about object recognition and navigation, whose outputs include thinking, thinking summary, and final action. The authors propose WAYS-Bench for the embodied waypoint selection task to SFT Qwen2.5-VL-3B and then perform RL post-training to improve the navigation performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Building an intelligent navigation model is a long-standing goal for the community of VLMs and embodied AI. This paper proposes CURE properties that VISOR possesses: compact, unified, reasoning-capable, and explainable.\n- Endowing navigation models with thinking capability is a good practice. This facilitates the learning of navigation task and enhances the explainability, and the authors provide some representative qualitative studies and discussions owing to this advantage.\n- This paper collects a dataset for the embodied waypoint selection task, which can serve as a data source for the cold start of navigation post-training."}, "weaknesses": {"value": "- I have some concerns about the input modality that make the comparison unfair. VISOR incorporates BEV image as input, which provides global information and could mitigate the challenge of navigation task.\n- The results are not strong enough compared to recent navigation models such as Uni-NaVid and MTU3D. For example, using DINO features should not be an excuse, as VISOR leverages pretrained Qwen2.5-VL model in turn.\n- Oracle stop can be used for analysis but should not be used for fair comparison. We know that the prediction of stop action is a significant problem in the navigation task. In particular, the oracle stop model cannot even surpass other baselines significantly.\n- The improvement from RL post-training is limited. Current bottleneck in navigation appears not to be SFT’s generalization gap, which makes the necessity and advantage of RL not solid."}, "questions": {"value": "- Reward design. I am curious about “training proved difficult, and the model was not able to converge” in Line 1017. So this means, the final results come from the discrete reward while the continuous reward cannot converge? If that is, why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zOoASv293X", "forum": "HhBxIQ0CA2", "replyto": "HhBxIQ0CA2", "signatures": ["ICLR.cc/2026/Conference/Submission13817/Reviewer_m1Td"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13817/Reviewer_m1Td"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988493839, "cdate": 1761988493839, "tmdate": 1762924344145, "mdate": 1762924344145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VISOR, a compact (~3B parameter) Vision-Language-Action (VLA) agent for language-driven object navigation tasks. Unlike prior work that relies on stitched multi-model pipelines or end-to-end embeddings, VISOR performs explicit image-grounded spatial reasoning for both object recognition and action selection, making its policies inherently explainable and generalizable. The method is trained via supervised fine-tuning on the new WAYS-Bench dataset (which provides waypoint-level supervision and reasoning traces) and further refined with a customized RL post-training. The paper provides extensive empirical validation against strong baselines on benchmark tasks, analyzes its strengths and limitations, and details dataset and training procedures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Unified and Compact Model:** VISOR is implemented as a single, end-to-end model with 3B parameters, removing the need for large, external object detectors or segmented multi-model pipelines. This directly addresses one of the field's notable practical bottlenecks.\n- **Explicit Reasoning and Explainability:** The agent generates detailed reasoning traces (“<think>”, “<think_summary>”, and “<action>”), providing action-level explainability—an advance over black-box action selection models.\n- **Dataset Contribution:** The introduction of WAYS-Bench, with waypoint-level supervision and LLM-generated reasoning traces, is a substantive practical and methodological resource for the community. Statistics in Table 1 illustrate dataset diversity.\n- **Robust Generalization:** VISOR prioritizes reasoning capabilities that generalize better on “Val Unseen” splits, as shown in Table 2 (CoIN-Bench) and Table 3 (OVON), where performance drops are less severe than conventional policies.\n- **Thorough Experimental and Failure Analysis:** The model’s performance is evaluated on multiple challenging benchmarks using standard metrics (SR/SPL), and weaknesses are transparently analyzed with supporting qualitative evidence. The inclusion of ablation and oracle-stop analyses further clarifies algorithmic bottlenecks.\n- **Interpretability through Visualization:** Figures (e.g., Figure 1 illustrates spatial reasoning via cluster-based waypoint selection; Figure 2 shows multi-stage, step-wise reasoning in navigation), giving concrete evidence of VISOR’s unique capabilities."}, "weaknesses": {"value": "1. **Empirical Performance Lagging on Seen Categories:**\n   - VISOR persistently lags behind the strongest baseline methods (e.g., RL, DAgRL, Uni-NaVid, and MTU3D) in raw performance measures (SPL, SR) on Val Seen and Synonym splits (see Table 3). For example, on OVON Val Seen, VISOR (GSPO) achieves SPL=12.48 / SR=21.7 vs DAgRL’s SPL=21.2 / SR=41.3 and MTU3D’s SPL=23.6 / SR=55.0, indicating that its effectiveness in familiar settings is limited.\n   - The rationale for the lower numbers, richer, more flexible natural instructions, and the absence of external modules is acknowledged, but the (frankly, large) gap compromises the perceived impact.\n2. **Limited Oral Clarity on Architectural Novelty vs. Prior Reasoning Pipelines:**\n   - While the paper positions VISOR against pipeline approaches, the “reasoning traces” paradigm is heavily influenced by prior works (React, PIVOT, ReAct, Goetting et al., 2024; Nasiriany et al., 2024). The main architectural distinction, doing all within a unified smaller model, is important, but the discussion could be stronger on *why/how* the explicit spatial reasoning with panoramic and top-down maps is uniquely beneficial beyond concatenating input modalities and tags. Figure 1, while illustrative, does not make this “wholly new reasoning” distinction sufficiently sharp.\n3. **Incomplete Engagement with Key Recent Related Work:**\n   - Several directly relevant recent papers are *not* cited or discussed, missing significant context. In particular:\n        - *Wen et al., 2024 (VLTNet)*—Directly addresses zero-shot object navigation with VLM-based reasoning, very close to VISOR’s target.\n        - *Pan et al., 2023 (LangNav)*; *Yu et al., 2024 (VLN-Game)*—Present alternative vision-language navigation strategies and equilibrium search.\n        - *Raychaudhuri et al., 2025 (MLFM)*; *Kuang et al., 2024 (OpenFMNav)*; *Liu, 2024*—Each advances vision-language model-based spatial/semantic map reasoning or open-set navigation.\n        - *Zhu et al., 2022*—Provides diagnosis of VLN agent failure modes, which could have strengthened comparative discussion.\n     - This incomplete treatment both weakens claims of originality and leaves the reader uncertain if the “CURE” properties are realized elsewhere.\n4. **Spatial and Depth Reasoning Remain Weak:**  \n    - The failure diagnostics in Appendix C and supporting visualizations (Figures 5, 6, 8) show systematic mistakes: hallucination of labels (incorrect spatial placement or scene element), left-right confusion, and poor depth estimation. The “Markovian Setup” problem is described in the text and visible in Figure 7, but little is done to ablate or mitigate these weaknesses. Without memory/history, the agent is prone to errors in last-mile navigation, where feature-based policies often excel.\n5. **Insufficient Ablation on Component Contributions:**\n   - The impact of reasoning traces, panoramic input, and top-down map is not individually ablated. Readers are left wondering which design choices are responsible for generalization gains, and whether, for example, simpler memory or attention augmentations could close the gap with less explainable policies.\n6. **Mathematical Detail Only at the Level of Standard RL Loss and SFT; No Tight Model-Reasoning Formalization:**\n   - The loss functions (Equation for $\\mathcal{L}_{\\mathrm{SFT}}$ and GSPO) are standard in supervised/sequence RL, and not unique to VISOR. The paper does not formalize the reasoning traces process as an algorithmic/optimization object beyond the input-output prompt structure. There is no clear mathematical framework connecting the agent's reasoning outputs to improved sample efficiency or generalization. This is particularly relevant since a key claim of the paper is about “human-like reasoning.” A reader is likely to expect tighter equations or formal reasoning for *why* this reasoning enables improved performance or explainability.\n7. **Dataset Collection (“Ground-truth Reasoning”) Blurring Human and LLM Roles:**\n   - The use of GPT-4o to retrospectively generate ground-truth reasoning traces for SFT may not always result in authentic or diverse human-like strategies (see Figure 3 for sample traces). This proxy could introduce bias: if the model simply learns to mimic LLM-generated justifications that do not always match the semantic nuances required for true generalization, this could cap performance.\n8. **Limited Analysis of Efficiency/E2E Latency:**\n   - While VISOR is more efficient and compact than stitched pipelines, there is no explicit timing analysis for model inference speed, nor is there a clear accounting of compute/latency improvements over strong modular or larger baselines. This undermines some claims around “practical efficiency.”\n9. **Minor:**\n   - Some figures and tables, e.g., Figure 2 and Figure 3, would benefit from clearer legends and color coding, as it can be challenging to align the visualization with textual step descriptions for those unfamiliar with the setup.\n   - Occasional typos and grammatical slips (e.g., “efficixmally” in system prompt snippet) in the Appendix/prompt code.\n\n## Potentially Missing Related Work\n\n1. Wen, C., Huang, Y., Huang, H. (2024): Zero-shot Object Navigation with Vision-Language Models Reasoning. Close conceptual overlap: VLTNet integrates vision-language and reasoning for zero-shot navigation; should be cited and compared in Related Work and compared as an alternative in Results.\n2. Pan, B., Panda, R., Jin, S. (2023): LangNav: Language as a Perceptual Representation for Navigation. Directly relevant to language-driven navigation; should be discussed in Section 5 and cited after initial task framing.\n3. Yu, B., Liu, Y., Han, L. (2024): VLN-Game: Vision-Language Equilibrium Search for Zero-Shot Semantic Navigation. Alternative zero-shot navigation framework; should be noted as baseline or related comparison.\n4. Raychaudhuri, S., Cancelli, E., Campari, T. (2025): MLFM: Multi-Layered Feature Maps for Richer Language Understanding in Zero-Shot Semantic Navigation. Presents attribute/relation-centric navigation; discuss as a complementary approach in Related Work.\n5. Kuang, Y., Lin, H., Jiang, M. (2024): OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models. Shares goals of generalization; compare and discuss weaknesses/strengths.\n6. Zhu, W., Qi, Y., Narayana, P. (2022): Diagnosing Vision-and-Language Navigation: What Really Matters. Provides insight into failure modes, valuable for failure analysis and benchmarking.\n7. Liu, W. (2024): Towards Vision and Language Models Aided Object Navigation. Hierarchical vision-language navigation; should be discussed in Section 5 as a relevant recent effort.\n8. Stone, A., Xiao, T., Lu, Y. (2023): Open-World Object Manipulation using Pre-Trained Vision-Language Models. For positioning in relation to use of pre-trained VLMs.\n\nEach should be integrated into the literature review (Section 5) and, where appropriate, Experimental Results and Failure Analysis, to sharpen the paper’s conceptual originality and positioning."}, "questions": {"value": "- Can the authors clarify the unique gains attributable to each architectural element: panoramic view, top-down map, and explicit reasoning tags, via ablation studies, or, at the very least, a detailed qualitative analysis?\n- How does the use of LLM-generated (rather than human-generated) reasoning traces as “ground-truth” affect generalization? Any empirical evidence that agents trained on such traces adopt meaningful human spatial strategies, especially in ambiguous instructions?\n- Given systematic spatial and depth perception errors (Figure 6, Figure 8, repeated left-right confusion), what candidate approaches might address these issues? Are there plans to fuse in memory/history, or would hybrid (learned + symbolic) representations help?\n- How much does the explicit “reasoning” in <think> and <think_summary> actually contribute to policy improvement versus simply providing post-hoc explainability?\n- Can the authors report wall-time and GPU-budget comparison against leading multi-model pipelines, especially those that use larger LLMs or external object detectors (e.g., MTU3D, Uni-NaVid)? Is VISOR’s claimed efficiency realized in practice, or are savings offset by prompt engineering and additional reasoning outputs?\n- What plans exist for systematically evaluating the credibility and diversity of reasoning traces provided by the agent versus human raters?\n- Are there meaningful limitations in transferring VISOR to real-world robotics or other simulators, especially regarding depth, occlusion, or physically dynamic environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IQiVqEFnuG", "forum": "HhBxIQ0CA2", "replyto": "HhBxIQ0CA2", "signatures": ["ICLR.cc/2026/Conference/Submission13817/Reviewer_hxPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13817/Reviewer_hxPK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13817/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022262253, "cdate": 1762022262253, "tmdate": 1762924343567, "mdate": 1762924343567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}