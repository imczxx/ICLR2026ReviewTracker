{"id": "8VthdYWjHt", "number": 8409, "cdate": 1758082259380, "mdate": 1759897786136, "content": {"title": "Visual Grounding Meets Language: CeAS and RAG for Bengali Long-Range Video Reasoning", "abstract": "Long-range video question answering (VQA) remains a challenging task, especially\n in low-resource languages like Bengali, due to limited linguistic tools and\nthe need for multi-step temporal reasoning. To address these challenges, we propose\n a training-free framework for Bengali Long-range Video Reasoning (BLrVR).\nOur approach adapts the EgoSchema benchmark to Bengali through high-quality\ntranslation and contextual validation. We introduce a novel prompting strategy,\nCeAS (Close-ended Answer Selection), which integrates structured roles, task\ncues, and strict constraints to guide LLM reasoning. Additionally, we explore a\nRetrieval-Augmented Generation (RAG) variant that fuses relevant caption context \nwith external evidence for enriched inference. Empirical results show that\nCeAS achieves state-of-the-art performance, surpassing RAG in precision, recall,\nand runtime efficiency, despite matching in accuracy and F1-score. We further\nbenchmark different captioners, LLMs, retrievers, and prompting schemes, providing\n a comprehensive evaluation of components crucial to BLrVR success. Our\nfindings demonstrate that structured prompting can outperform retrieval-heavy \nmethods in both effectiveness and efficiency for low-resource multimodal reasoning.\n The code is publicly released at: https://github.com/anaxy-code/Bengali-Long-Range-Video-Reasoning", "tldr": "CeAS and RAG for Bengali Long-Range Video Reasoning", "keywords": ["Long-range Video Reasoning", "Bengali Language Processing", "Visual Question Answering", "Large Language Models", "Close-ended Answer Selection", "Retrieval-Augmented Generation", "Multimodal Captioning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a2703ba907ddf948d764859dbd81d077af70ee9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a training-free framework for Bengali Long-range Video Reasoning (BLrVR), evaluated on the EgoSchema benchmark. The authors further propose a new prompting strategy, termed Close-ended Answer Selection (CeAS), which incorporates structured role definitions, task-specific cues, and strict reasoning constraints to enhance LLM inference. Experimental results demonstrate that CeAS attains state-of-the-art performance, outperforming RAG methods in terms of precision, recall, and runtime efficiency, while maintaining comparable accuracy and F1-scores."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an underexplored research area by focusing on long-range video reasoning in the Bengali language, thereby contributing to the broader goal of enhancing multimodal understanding in low-resource linguistic settings.\n2. The paper presents a Bengali-translated version of the EgoSchema benchmark, which, if publicly released, would serve as a valuable resource for the research community."}, "weaknesses": {"value": "1. The paper lacks a clear technical contribution that meets the standards of ICLR. The proposed framework primarily combines existing techniques, such as CoT prompting and RAG, without introducing novel algorithmic insights or theoretical advancements relevant to the machine learning community.\n\n2. The experimental section primarily consists of comparisons with existing methods, which makes the work appear more as an empirical evaluation than a contribution with methodological innovation.\n\n3. Despite emphasizing the exploration of the Bengali language, the paper does not introduce any language-specific model adaptations or linguistic considerations tailored to Bengali. This weakens the central claim of advancing multimodal reasoning for low-resource languages.\n\n4. The paper’s organization could be improved. The Method section omits critical implementation details of the proposed CeAS approach, with some necessary explanations relegated to the appendix, making it difficult for readers to fully understand and reproduce the method.\n\n5. For qualitative examples presented in Bengali, providing English translations would enhance accessibility and clarity for a broader research audience."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I436W4u0je", "forum": "8VthdYWjHt", "replyto": "8VthdYWjHt", "signatures": ["ICLR.cc/2026/Conference/Submission8409/Reviewer_R2yv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8409/Reviewer_R2yv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767547297, "cdate": 1760767547297, "tmdate": 1762920311111, "mdate": 1762920311111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a training-free framework for long-range video question answering in Bengali. The authors adapt the EgoSchema dataset to Bengali and propose two reasoning approaches — a prompting-based method (CeAS) and a retrieval-augmented generation (RAG) variant. The system uses visual captioning via multimodal LLMs and LLM-based reasoning to answer multiple-choice questions. Experiments compare different LLMs, captioners, and prompting schemes on the Bengali-translated EgoSchema subset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tAddresses a low-resource language (Bengali), which is a relevant but niche topic.\n-\tLong-range video reasoning is an important problem"}, "weaknesses": {"value": "-\tLack of novelty: There is no clear difference between the proposed method and other established ones like VideoAgent [1,2].\n-\tLimited modality: Relying only on textual captions loses visual information critical for video reasoning.\n-\tOutdated baselines: the baselines in experiments are quite outdated (2022, 2023)."}, "questions": {"value": "-\tCan the authors clarify the difference between the proposed method with other approaches like VideoAgent [1,2]?\n-\tIs there any quantitative or qualitative evidence that your proposed method performs better than other recent methods?\n\n[1] Fan, Yue, et al. \"Videoagent: A memory-augmented multimodal agent for video understanding.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n[2] Wang, Xiaohan, et al. \"Videoagent: Long-form video understanding with large language model as agent.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eRFNJXDSLA", "forum": "8VthdYWjHt", "replyto": "8VthdYWjHt", "signatures": ["ICLR.cc/2026/Conference/Submission8409/Reviewer_UEwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8409/Reviewer_UEwK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948333140, "cdate": 1761948333140, "tmdate": 1762920310381, "mdate": 1762920310381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BLrVR, a training-free framework for Bengali long-range video question answering. \nThe approach adapts the EgoSchema dataset into Bengali through machine translation and human validation, and evaluates two reasoning frameworks: 1. CeAS, a structured close-ended prompting strategy with role specification and task cues\n2. RAG, a retrieval-augmented generation variant leveraging external textual evidence.\n\nExperiments benchmark various multi-modal LLMs (Gemini-2.0/1.5, Gemma2-9B) as captioners and reasoning modules, reporting moderate gains in precision and recall for CeAS over RAG on the translated Bengali EgoSchema subset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper adapts long-range video reasoning to Bengali is appreciated and valuable for inclusive AI research, given the paucity of multimodal datasets in South Asian languages.\n\n2. The paper provides some empirical comparisons across captioners, LLMs, and prompting variants (CeAS, CoT, Plan-and-Solve), offering a practical reference for low-resource language setups.\n\n3. The framework is training-free and modular, making it easy to reproduce and extend to other low-resource settings."}, "weaknesses": {"value": "1. Limited technical novelty: I think the core contributions (CeAS prompt design, RAG baseline) are largely incremental and primarily involve reusing existing prompting and retrieval frameworks in a Bengali setting. The novelty lies more in application and data translation rather than in algorithmic innovation.\n\n2. Dataset contribution is minimal: The “Bengali EgoSchema” is simply a translated version of an existing benchmark with limited linguistic validation; there is no evidence of new video content, annotation schema, or task definition.\n\nalso, results hover around 69% accuracy, with small differences between methods. The evaluation lacks ablations on reasoning depth, temporal grounding fidelity, or cross-language generalization, which would be necessary for an ICLR-style contribution + missing important comparison/discussion with the line of related work [1-4] (and more).\n\n\nOverall, I feel the focus on prompt structure, translation quality, and linguistic adaptation aligns more naturally with ACL/EMNLP tracks (e.g., multilingual or low-resource multimodal reasoning), rather than ICLR’s emphasis on technical advances in learning representations or modeling architectures.\n\n\n[1] A simple llm framework for long-range video question-answering.  \n[2] Videotree: Adaptive tree-based video representation for llm reasoning on long videos.  \n[3] Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos.  \n[4] VideoLucy: Deep Memory Backtracking for Long Video Understanding"}, "questions": {"value": "Please see the weakness section, overall, I have doubts about the area of contribution is aligned with ICLR requirement or not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3eUH89Obvz", "forum": "8VthdYWjHt", "replyto": "8VthdYWjHt", "signatures": ["ICLR.cc/2026/Conference/Submission8409/Reviewer_GP3y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8409/Reviewer_GP3y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025830380, "cdate": 1762025830380, "tmdate": 1762920309973, "mdate": 1762920309973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}