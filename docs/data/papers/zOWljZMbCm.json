{"id": "zOWljZMbCm", "number": 25365, "cdate": 1758367200755, "mdate": 1763717823145, "content": {"title": "Unlocking the Potential of Weighting Methods in Federated Learning Through Communication Compression", "abstract": "Modern machine learning problems are frequently formulated in federated learning domain and incorporate inherently heterogeneous data. Weighting methods operate efficiently in terms of iteration complexity and represent a common direction in this setting. At the same time, they do not address directly the main obstacle in federated and distributed learning -- communication bottleneck. We tackle this issue by incorporating compression into the weighting scheme. We establish the convergence under a convexity assumption, considering both exact and stochastic oracles. Finally, we evaluate the practical performance of the proposed method on real-world problems.", "tldr": "", "keywords": ["Convex optimization", "Compression", "Stochastic optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12c0e78991de803466d13242539a4a086de4c9b1.pdf", "supplementary_material": "/attachment/520f58702b7336ac6d99c4af9f9b3b9ef5a559c9.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ADI, a unified federated learning algorithm designed to jointly tackle communication bottlenecks and data heterogeneity. By integrating a difference compression scheme within a min-max optimization framework, ADI dynamically learns to weight clients based on their data's informativeness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe work is highly relevant to the scope of ICLR, as it contributes a novel approach to a fundamental challenge in federated learning.\n-\tThe proposed ADI algorithm is novel in its effective integration of difference compression with an agnostic weighting scheme.\n-\tThe paper provides a comprehensive theoretical analysis that guarantees convergence in a convex setting."}, "weaknesses": {"value": "-\tThe weight update rule in Algorithm 1 appears to contradict the paper's min-max objective, potentially invalidating the core weighting mechanism.\n-\tInsufficient explanation of hyperparameters.\n-\tThe interpretation of the weight dynamics in Figure 3 oversimplifies the mechanism, conflating high loss with data uniqueness without deeper analysis.\n-\tA significant gap exists between the paper's convex theoretical analysis and its non-convex deep learning experiments.\n-\tThe empirical validation is limited to a single dataset (CIFAR-10), which is insufficient to demonstrate the algorithm's generalizability."}, "questions": {"value": "1.The paper's narrative suggests weights are assigned to clients with unique data. Could the authors elaborate on how the algorithm would behave in the following more nuanced scenarios, where the concepts of uniqueness and high loss might diverge?\n(a) A client with a unique data class that is very easy to learn. Would its weight initially spike and then fall once the model quickly masters its data, potentially ending up below average?\n(b) A client whose data classes are common across the network, but whose specific samples are particularly noisy or inherently difficult (i.e., 'hard examples'). Would this client receive a persistently high weight despite its data not being distributionally unique?\n\n2.Could the authors provide some intuition on why the dynamics of the agnostic weighting scheme, proven to be stable in the convex setting, appear to remain effective in the non-convex landscape of deep learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5GOfFk4hoC", "forum": "zOWljZMbCm", "replyto": "zOWljZMbCm", "signatures": ["ICLR.cc/2026/Conference/Submission25365/Reviewer_tXwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25365/Reviewer_tXwb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622545189, "cdate": 1761622545189, "tmdate": 1762943414949, "mdate": 1762943414949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how to **combine adaptive weighting in federated learning (FL)** with **communication compression** by casting weighting as a **min–max (agnostic) formulation** and proposing an **extragradient-style** method, **ADI** (Alg. 1), that communicates only *compressed* model updates while exchanging one scalar (local loss) per client to update weights. The objective is written as\n$$\\min_{\\theta}\\max_{\\pi\\in\\Lambda}\\;\\sum_{i=1}^M \\pi_i f_i(\\theta),$$\nwhere $\\pi$ lies in a convex subset of the simplex (an agnostic-weighting set $\\Lambda$) so that weights are learned during training with negligible additional communication (one loss per client) compared to gradients. The paper proves **(i)** convergence in a deterministic convex setting, **(ii)** extensions to **stochastic local oracles**, and **(iii)** **partial participation** with sampling rate $p$ under **unbiased compressors** characterized by a variance parameter $\\omega$. Complexity bounds are provided in terms of the **gap function** and **bits-on-wire**. Experiments on classification (ResNet/CIFAR) qualitatively compare ADI to DIANA and show weight stabilization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear motivation**: Formulating adaptive weighting as a saddle-point problem makes explicit how to **learn client weights** while preserving a communication-efficient pipeline; local losses are the only extra information needed at the server, which is indeed cheaper than transmitting even compressed gradients. \n- **Method design**: ADI integrates an **optimistic/extragradient**-style update for the $(\\theta,\\pi)$ saddle-point operator together with **compressed communications** and server-side memory terms, avoiding full-gradient transmission. \n- **Theoretical backbone**: The paper establishes monotonicity of the saddle-point operator and analyzes convergence under convexity and unbiased compression assumptions.  \n- **Stochastic & partial participation**: Beyond exact gradients, the analysis covers **stochastic oracles** and **partial participation** ($p<1$), with explicit dependence on $\\omega$ and $p$. \n- **Communication accounting**: The paper makes the **bits-on-wire** explicit through an expected density $q_\\omega$ and states iteration/bit complexities (including the $p$-scaling for partial participation). \n- **Connection to literature**: The related-work section situates the approach amid **agnostic FL** (AFL) and fairness-weighting methods as well as extragradient methods for saddle-point problems and compression."}, "weaknesses": {"value": "- **Novelty positioning is underdeveloped**. The algorithmic ingredients—**agnostic weighting** (AFL), **extragradient/optimistic updates**, and **unbiased compression with memory**—are all known. The paper does not convincingly demonstrate **strict improvements** over prior **saddle-point + compression** methods such as **MASHA** or over **DIANA/EF21**-style approaches when combined with weighting, nor does it provide tight side-by-side **complexity comparisons** to prove advantages (e.g., in dependence on $\\omega, M, p$). \n\n[1] Mishchenko, Gorbunov, Takáč, Richtárik. “Distributed Learning with Compressed Gradient Differences.” _arXiv:1901.09269_, 2019. ([arXiv](https://arxiv.org/abs/1901.09269 \"[1901.09269] Distributed Learning with Compressed Gradient Differences\"))\n\n[2] Richtárik, Sokolov, Fatkhullin. “EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback.” _NeurIPS 2021_ (proceedings version).\n\n[3] Beznosikov, Richtárik, Diskin, Ryabinin, Gasnikov. “Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees.” _NeurIPS 2022_. ([NeurIPS Proceedings](https://proceedings.neurips.cc/paper_files/paper/2022/hash/5ac1428c23b5da5e66d029646ea3206d-Abstract-Conference.html \"Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees\"))\n\n\n- **Ambiguous constants and scalings**. The rates use $\\tilde L$, $L$, $M$, $\\omega$, and sometimes exhibit **non-intuitive mixes** (e.g., $\\tilde L \\sqrt{\\omega}$ and $L\\sqrt{\\omega^3/M}$ in bit complexity). The paper should (i) define $\\tilde L$ precisely near the main theorem; (ii) factor the bounds to highlight the **leading-order** dependence on $M,\\omega,p$; and (iii) provide **regimes** where ADI is provably preferable to DIANA/MASHA. \n- **Theory audit (tightness and steps)**. \n  - The **variance accounting** (e.g., the transition from (40)–(44) to the final bound) involves several compressions of sums across clients; a concise **lemma** summarizing how unbiased compression interacts with the server memory ($h^k$) would improve readability and verifiability. \n- **Empirical evaluation is limited**. \n  - The **baselines** are insufficient: no **AFL** (Mohri et al., 2019) or **q-FFL** variants (Li et al., 2020) are included, although these are central **weighting** competitors; likewise, **MASHA** (saddle-point with compression) is missing. \n\n[4] Mohri, M., Sivek, G., & Suresh, A. T. “Agnostic Federated Learning.” _ICML 2019 (PMLR v97)_, pp. 4615–4625.\n\n[5] Li, T., Sanjabi, M., Beirami, A., & Smith, V. “Fair Resource Allocation in Federated Learning.” _arXiv:1905.10497_ \n\n\n  - Plots are primarily **qualitative**; **error bars**, **multiple seeds**, **bytes-on-wire vs. accuracy** with equalized compute, **participation-rate sweeps** ($p$), and **compressor-strength sweeps** ($\\omega$) are needed. The paper does show weight stabilization, but does not quantify the impact on accuracy/fairness trade-offs. \n- **Reproducibility details**. Hyperparameters and training schedules are not reported with sufficient granularity (optimizers, LR decay, momentum/weight decay, batch sizes per client, number of local steps $K$ vs. rounds), nor is **code** provided; these are required for an ICLR-standard empirical section."}, "questions": {"value": "1. **Comparative complexity**: For the main deterministic and stochastic results, can the authors **tabulate** iteration and bit complexity side-by-side against **DIANA**, **EF21**, and **MASHA**, fixing a common set of assumptions (convexity, unbiased compressors), to show **when ADI is strictly better** (or at least not worse) in the leading-order dependence on $M,\\omega,p$? \n2. **Non-convex case**: Can the analysis be **extended** to **non-convex** $f_i$, aligning with the deep-network experiments? If not, please clarify this limitation in the conclusions.\n3. **Weighting set $\\Lambda$ & prox**: What is $\\Lambda$ concretely in the experiments (simplex, KL-ball around uniform, box constraints)? The proofs rely on KL-based prox on $\\pi$; please **state $\\Lambda$** near the algorithm and connect it to the constants in the rates. \n4. **Partial participation**: The corollary yields an $\\omega/\\!p$ dependence in bits. Can you **empirically verify** this scaling by varying $p$ on a fixed task and compressor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No clear ethics red flags."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GMr1Kxvxdl", "forum": "zOWljZMbCm", "replyto": "zOWljZMbCm", "signatures": ["ICLR.cc/2026/Conference/Submission25365/Reviewer_KP2w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25365/Reviewer_KP2w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764838564, "cdate": 1761764838564, "tmdate": 1762943414143, "mdate": 1762943414143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADI (Agnostic DIANA), a novel federated learning algorithm that effectively integrates weighting methods, such as agnostic weighting for handling data heterogeneity, with communication compression techniques like DIANA-inspired gradient compression to address both statistical and system challenges in federated settings. The key contributions include a theoretically grounded framework with convergence guarantees under convex assumptions, empirical validation on benchmarks like CIFAR-10 showing improved robustness under non-IID data, and a practical approach that reduces communication overhead without compromising model performance. While the work is promising, it would benefit from broader experimental validation and extensions to non-convex scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper’s most striking strength lies in its high degree of originality. Rather than merely refining existing methods, it resolves a crucial yet under-explored problem through a creative synthesis. The authors perceptively recognize that, in federated learning, weighting schemes (which tackle data heterogeneity) and communication-compression techniques (which alleviate bandwidth bottlenecks) have long evolved as parallel, separate research strands. Deeply integrating the two represents a high-potential void. The proposed ADI algorithm successfully embeds an agnostic-weighting minimax formulation within a DIANA-style compressed-communication mechanism, delivering genuine interdisciplinary innovation and opening a fresh, important research avenue for the federated-learning community.\n\n- Methodologically, the work demonstrates solid quality. The algorithm is not a hasty patchwork; it is carefully engineered. It borrows mature tools from optimization theory—e.g., the optimistic extra-gradient method for saddle-point problems—and marries them with state-of-the-art variance-reduced compression, revealing a deep understanding and skillful command of relevant techniques. Moreover, the paper supplies a strict convergence analysis covering realistic scenarios such as exact gradients, stochastic oracles, and partial client participation, furnishing a firm mathematical foundation for the algorithm’s efficacy. This balanced emphasis on theory and algorithmic craft greatly enhances completeness and credibility.\n\n- The manuscript is exceptionally clear, enabling readers to grasp sophisticated ideas with ease. Starting from an articulate exposition of the twin challenges—data heterogeneity and communication bottlenecks—it gradually motivates their joint treatment, then elaborates method, theory, and experiments in a fluent, step-wise fashion. Key notions (unbiased compressors, agnostic objective) are rigorously defined; Algorithm 1 is accompanied by detailed pseudo-code and plain explanations, substantially improving reproducibility and allowing audiences to pinpoint the technical contribution accurately.\n\n- The results carry notable significance for propelling federated learning from theory to practice. By directly confronting the two toughest deployment constraints, the work is poised to yield practical algorithmic frameworks that simultaneously accommodate statistical heterogeneity and curtail communication costs. Systematic empirical comparisons robustly demonstrate the effectiveness of weighting approaches under compression, underscoring the study’s potential real-world influence."}, "weaknesses": {"value": "- The theoretical convergence analysis of the paper is based on a key assumption: the local loss functions of the clients are convex. This assumption severely limits the practical value of the theoretical results because the primary application scenarios of federated learning (such as training deep neural networks) are inherently non-convex. This greatly diminishes the practical guidance provided by the theoretical guarantees. An actionable suggestion for improvement is that the authors should explicitly acknowledge this limitation in the discussion section and attempt to provide some heuristic convergence analysis or observations on standard non-convex models. For example, they could briefly discuss the possibility of extending the analysis to weaker non-convex assumptions, such as the Polyak-Łojasiewicz condition, which would significantly enhance the depth and relevance of the theoretical section.\n\n- The experimental section relies primarily on the CIFAR-10 dataset and a single non-IID partitioning method based on the Dirichlet distribution. This setup is too narrow to adequately demonstrate the generality and robustness of the proposed method. Specifically, validation is lacking for larger-scale or different modality datasets, as well as for more extreme or real-world heterogeneity scenarios. To address this weakness, the authors must conduct additional experiments. An actionable recommendation is to test the algorithm on more challenging benchmarks, such as CIFAR-100, a federated learning text dataset, or a real non-IID dataset from the LEAF benchmark. This would strongly demonstrate the method's scalability and universality, moving beyond a mere proof-of-concept that is effective only in a specific setting.\n\n- The paper's core innovation lies in the combination of weighting methods and compression techniques. However, there is a lack of in-depth analysis of how these two components interact. A key unanswered question is: does the noise introduced by compression interfere with or distort the dynamic process of weight assignment? For instance, under high compression rates, could weight updates become unstable or biased? The authors need to provide a more detailed analysis to clarify this mechanism. A specific method for improvement is to design a controlled experiment that systematically varies the compression rate and then observes and analyzes the trajectory, stability, and final distribution of client weights. Such an analysis would not only verify the robustness of the method but also profoundly reveal its internal working mechanism, thereby greatly strengthening the paper's contribution."}, "questions": {"value": "1. Regarding the practical utility and generalizability of the theoretical assumptions: The theoretical analysis in the paper is built on a strong convexity assumption. However, the primary application scenarios of federated learning (e.g., training neural networks) are inherently non-convex. Could the authors provide any evidence—such as experiments on standard non-convex benchmarks or a discussion on extending the theory to non-convex settings—that the convergence guarantees of the ADI algorithm still hold or remain informative in more practical non-convex environments? This is crucial for assessing the practical value of your theoretical contributions.\n\n2. On the universality and robustness of the experimental conclusions: The current experimental validation is mainly conducted on the CIFAR-10 dataset and under a specific non-i.i.d. setting. To demonstrate the broad applicability of the ADI method, could the authors present results in more challenging scenarios? For instance, does ADI consistently outperform baseline methods on larger-scale datasets or under different types of non-i.i.d. distributions—such as real-world federated benchmark datasets? This is essential for evaluating the significance of your approach.\n\n3. Toward a deeper analysis of the interaction between core components: The paper’s key innovation lies in combining weighting methods with compression techniques. However, the analysis of how compression specifically influences the dynamics of weight allocation (as illustrated in Figure 3) remains superficial. Under high compression rates, could the noise introduced by compression interfere with weight convergence or even bias the weight allocation? Could the authors provide a more in-depth analysis—such as visualizing weight trajectories under varying compression rates—to clarify this critical interaction mechanism? Clarifying this would significantly strengthen your methodological contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sJDM7qdjkg", "forum": "zOWljZMbCm", "replyto": "zOWljZMbCm", "signatures": ["ICLR.cc/2026/Conference/Submission25365/Reviewer_P4ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25365/Reviewer_P4ke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25365/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973627243, "cdate": 1761973627243, "tmdate": 1762943413723, "mdate": 1762943413723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}