{"id": "oEMJzGB5du", "number": 11044, "cdate": 1758187798222, "mdate": 1759897612578, "content": {"title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient", "abstract": "Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), \nyet it often struggles with generalization compared to reinforcement learning (RL). \nIn this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference:\nSFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. \nBuilding on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. \nOTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. \nAt each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. \nThe ground-truth token from the supervised data is then used to provide a reward signal to these samples. \nGuided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level,\ncapturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation.\nThrough extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning,\nwe demonstrate that OTR consistently outperforms standard SFT. \nOur findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization,\noffering a promising new direction for fine-tuning LLMs.", "tldr": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Supervised Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df5708382887e444704df6103011cb226582fcc3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces OTR, a training algorithm based on supervised data inspired in RL that treats each token generation as a single-step rollout of the policy. It then uses a reward function that takes the value 1 if the correct token was sampled and a hyperparameter $\\beta$ otherwise."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1) The paper is extremely well-written and clear\nS2) The proposed algorithm is shown to improve results on a range of models and reasoning datasets."}, "weaknesses": {"value": "The main weakness of this paper is that the claim that OTR is a generalization of DFT is false. This method is a Monte-Carlo approximation of the DFT objective which can be computed without the additional variance. The reason is the following. Take the OTR objective, replacing the empirical estimate of the expectation with the expectation is approximating:\n1) $$\\frac{1}{T}\\sum_{t=1}^T E_{a_t\\sim \\pi_\\theta(\\cdot|s_t)} {R(a_t, x_t)\\nabla_\\theta  \\log \\pi_\\theta(a_t|s_t)}$$\n\nNow, consider (a constant baseline):\n2) $$\\frac{1}{T}\\sum_{t=1}^T E_{a_t\\sim \\pi_\\theta(\\cdot|s_t)} {\\beta \\nabla_\\theta  \\log \\pi_\\theta(a_t|s_t)} = 0$$\n\nThis is because $E_{a_t\\sim \\pi_\\theta(\\cdot|s_t)} {\\beta \\nabla_\\theta  \\log \\pi_\\theta(a_t|s_t)} = E_{a_t\\sim \\pi_\\theta(\\cdot|s_t)} {\\beta \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{ \\pi_\\theta(a_t|s_t)}} = \\beta \\sum_a{\\nabla_\\theta \\pi_\\theta(a|s_t)} = \\beta \\nabla_\\theta(1) = 0$\n\nThus, subtracting 2 from 1, we get:\n3) $$\\frac{1}{T}\\sum_{t=1}^T E_{a_t\\sim \\pi_\\theta(\\cdot|s_t)} {(R(a_t, x_t) - \\beta) \\nabla_\\theta  \\log \\pi_\\theta(a_t|s_t)}$$\n\nNow, because now all actions $a_t \\neq x_t$ have reward 0, we are left with:\n4)$$ = \\frac{1}{T}\\sum_{t=1}^T (1-\\beta) \\pi_\\theta(x_t|s_t) \\nabla_\\theta  \\log \\pi_\\theta(x_t|s_t)$$\n\nDiscarding the proportionality constant we are left with \n5) $$ \\propto \\frac{1}{T}\\sum_{t=1}^T \\pi_\\theta(x_t|s_t) \\nabla_\\theta  \\log \\pi_\\theta(x_t|s_t)$$\n\nwhich is the DFT loss multiplied by an additional factor $\\frac{1}{T}$. This factor, however, is biasing the objective, see: Liu, Zichen, et al. \"Understanding r1-zero-like training: A critical perspective.\" arXiv preprint arXiv:2503.20783 (2025).\n\nThe above analysis also shows that whatever effect seen by setting different values of $\\beta$ should disappear by dividing the learning rate by $1 - \\beta$.\n\nNow, I am not saying that because this paper boils down to a high-variance version of DFT it shouldn't be published. DFT is concurrent work and in the current landscape of Machine Learning with new papers pouring out with breakneck speed every week, it is fair to recognize two contributions coming out simultaneously. However, if I haven't made a mistake in my analysis, and please correct me if I did, the paper should be thoroughly restructured in order to acknowledge these points."}, "questions": {"value": "I don't need to ask any further clarifying questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AyIdBPbiHy", "forum": "oEMJzGB5du", "replyto": "oEMJzGB5du", "signatures": ["ICLR.cc/2026/Conference/Submission11044/Reviewer_mP3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11044/Reviewer_mP3t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767826859, "cdate": 1760767826859, "tmdate": 1762922224057, "mdate": 1762922224057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes One-Token Rollout (OTR), a novel algorithm for supervised fine-tuning of LLMs that incorporates principles from policy gradient methods in RL. The authors argue that the generalization gap between SFT and RL stems primarily from SFT's use of static, off-policy data, as opposed to RL's on-policy sampling. OTR addresses this by reframing each token prediction as a single-step RL trajectory, performing Monte Carlo rollouts to sample candidate tokens from the model's current distribution, and using the ground-truth token to assign rewards (1 for matches, beta < 1 for mismatches). This transforms SFT data into a token-level on-policy signal, leading to a loss that dynamically weights the ground-truth and penalizes sampled incorrect tokens. Experiments on mathematical reasoning, code generation, and general benchmarks using Qwen-series models show OTR outperforming standard SFT, with better generalization and reduced catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- OTR demonstrates consistent improvements over SFT baselines across diverse benchmarks, including in-domain math tasks (GSM8K, MATH) and out-of-domain code/general reasoning (HumanEval+, MMLU-Pro). Notably, it mitigates performance degradation on non-target capabilities (fewer instances of drops below base model levels, aligning with the goal of preserving pre-trained knowledge.\n- The paper's perspective on the SFT-RL gap is relevant and timely. The derivation from policy gradient to token-level loss is well explained and connects to concurrent work like Dynamic Fine-Tuning."}, "weaknesses": {"value": "- OTR bears strong similarities to established regularization methods in LLMs, such as label smoothing, which softens one-hot targets to prevent overconfidence by redistributing probability mass to non-ground-truth classes. More importantly, the penalty term for sampled incorrect tokens (via beta < 0) closely resembles unlikelihood training (Welleck et al, 2019, which is not cited in the paper), where the model is trained to assign lower probabilities to negative samples drawn from its own distribution to combat degeneration; while framed as an RL innovation, OTR could be viewed as a rediscovery or minor extension of these ideas, and the paper lacks direct comparisons to such priors, potentially overstating novelty.\n- Despite positioning OTR as bridging the SFT-RL gap through \"on-policy\" simulation, the single-token rollout and direct penalization of non-ground-truth samples make the \"onlineness\" aspect vanish in practice. In full RL (e.g., policy gradients for LLMs), on-policy learning involves multi-step trajectories and exploration of novel behaviors; here, rewards are rigidly tied to fixed ground-truth tokens, reducing it to supervised contrastive learning rather than true online adaptation. This limits the method's ability to discover behaviors beyond the initial data distribution, as noted in related work on policy gradients.\n- Relatedly, the one-step nature of OTR makes MC rollouts redundant. Given that SFT already provides access to the full model distribution (logits) and target (ground-truth token) at each step, the expectation in Eq. 4 could be computed exactly using the temperature-scaled softmax (\\Pi'), e.g., as \\Pi'(xt) log \\Pi(xt) + \\beta ∑_{a ≠ xt} \\Pi'(a) log \\Pi(a), which is feasible for typical vocab sizes (~100k) on GPUs. This would eliminate sampling variance and cost (K=256), potentially making OTR more efficient and stable. Why use monte-carlo estimation when exact, efficient, closed-form solution is readily available? The paper does not explore or justify why an approximate MC approach is preferred over the closed-form alternative, especially since DFT achieves similar weighting without sampling.\n- While DFT is discussed, broader baselines like unlikelihood training or entropy-regularized SFT are absent, weakening claims of superiority."}, "questions": {"value": "- Given the single-token framing, how does OTR handle long-range dependencies in sequences, and does the \"on-policy\" simulation truly enable exploration akin to multi-step RL, or is it effectively off-policy with model-sampled negatives?\n- Why use Monte Carlo sampling instead of exact computation over the distribution? Have the authors experimented with a closed-form version of the loss, and if so, how does it perform in terms of stability and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QChn8jgev8", "forum": "oEMJzGB5du", "replyto": "oEMJzGB5du", "signatures": ["ICLR.cc/2026/Conference/Submission11044/Reviewer_z5Y3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11044/Reviewer_z5Y3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761416667771, "cdate": 1761416667771, "tmdate": 1762922222748, "mdate": 1762922222748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes a new language model finetuning method, called one-token rollout (OTR), which repurposes supervised finetuning (SFT) data for on-policy RL. The main idea is to use the next token in an output as a reward signal. For each token, instead of applying the standard cross-entropy loss, OTR samples multiple candidate tokens from the current policy and performs a policy gradient update, where the reward of the ground truth token is set to 1 and the reward of other tokens is set to a lower (negative) value. OTR is demonstrated to generalize better than standard SFT across several benchmarks and base models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Relatively well-written and easy to follow.\n\n2. The idea behind OTR is conceptually simple. I also find that it is sufficiently motivated through existing observations on the benefits of on-policy data.\n\n3. I appreciate the comparison to the concurrent dynamic finetuning (DFT) method in Section 3.4. It helps clarify the distinction of OTR (despite not being strictly necessary due to concurrent work guidelines of ICLR).\n\n4. The empirical evaluation considers several different benchmarks, including in-distribution and out-of-distribution ones."}, "weaknesses": {"value": "1. It seems that OTR incurs an additional computational cost over SFT due to the need of generating multiple candidate tokens for each token in a response. It would be beneficial to:\n    - Report for the existing results what the compute difference (e.g., FLOPS or runtime) was between OTR and SFT.\n    - If it is non-negligible, run experiments where both OTR and SFT are allocated the same amount of compute. For example, can let SFT run for more optimization steps until it reaches the same compute that OTR used (based on the same data).\n    - Straightforwardly discuss this limitation, specifying the exact computational overhead that OTR incurs compared to SFT.\n\n2. The empirical evaluation compares OTR only to SFT. There are a plethora of methods to finetune language models. Thus, it is difficult to assess the significance of OTR, the sole contribution of this paper. In particular, how does it compare to RL with verifiable rewards (RLVR)? While RLVR employs a different type of supervision (reward on answer vs a demonstration of a reasoning chain and answer), I believe this is still a necessary comparison given that verifiable rewards in the domains considered can be easier to obtain than high quality demonstrations.\n\n3. The gains of OTR over SFT are in many cases quite small. The paper does not report standard deviations, or any other measure of statistical significance. So it is unclear in how many of these cases the difference is just due to noise.\n\n\n\nReview Summary and Recommendation\n---\n\nOverall, I find this paper to be on the borderline. On the one hand, it is relatively well-written and proposes a simple (in a good sense) method for converting SFT into an on-policy method. However, the empirical evaluation is somewhat lacking. I would be willing to reconsider my score if the authors are able to treat the comments above regarding the empirical evaluation and additional compute required by OTR.\n\n\nAdditional (More Minor) Comments\n---\n\n1. In the SFT definition (Equation (1)), conditioning on the prompt is missing. Also, as far as I am aware, the standard formulation does not normalize across the length of the sequence, rather across the number of sequences.\n\n2. The explanation starting from line 151 on the use of temperature for sampling is rather standard. It is a subjective matter, but in my opinion it can be significantly shortened (e.g., there is no need to define the alternative sampling policy and explaining the generation process, rather it is possible to just say that a temperature greater than 1 is used to encourage exploration).\n\n3. I do not believe it is necessary to write a stop gradient around the reward in Equation (8). At least, it is not standard to do so."}, "questions": {"value": "--"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QTKjtPffPD", "forum": "oEMJzGB5du", "replyto": "oEMJzGB5du", "signatures": ["ICLR.cc/2026/Conference/Submission11044/Reviewer_bCYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11044/Reviewer_bCYc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667824968, "cdate": 1761667824968, "tmdate": 1762922222050, "mdate": 1762922222050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a method to improve on supervised fine-tuning (SFT) of large language models (LLMs) by doing one-token rollouts at every subsequence of an SFT dataset. The method samples K token completions of at every step of every sequence in the training dataset, and defines a REINFORCE-like loss where the reward is the fraction of K tokens that match the next ground truth token. \n\nQwen models are fine-tuned using both the one-token rollout (OTR) method and SFT on a math training dataset, and evaluated on a wide suite of math and code generation benchmarks. Results show that OTR generally improves over SFT.\n\nAblation experiments show the importance of adding a slightly negative reward on tokens that are not equal to the ground-truth next token helps with training stability and improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a simple way of implementing on-policy REINFORCE-like improvement of LLMs without doing full reinforcement learning, such as using GRPO. The method is quite simple to implement given an SFT training dataset.\n\n2. Results show improvements over SFT fine-tuning for Qwen models, across a wide set of math and code generation benchmarks. The paper reports out-of-domain generalization improvements with OTR training as well. \n\n3. Ablations are included to show the necessity of using a small negative reward for incorrectly generated tokens."}, "weaknesses": {"value": "1. One key missing aspect of this paper is a measure of complexity of the proposed method. In the introduction the paper claims that it's method \"improves model generalization without incurring the computational cost of full sentence generation.\" However, as presented in the paper, for every training set sequence of length T tokens, OTR samples K tokens, which equals a total of T*K sampled tokens. This is the same as sampling K length-T completions for the prompt corresponding to the training set sequence. Note that K=256 is quite high in the paper experiments.\n\nIt is difficult to evaluate the claims of this work without any analysis of its additional complexity, which is not in the current paper.\n\n2. Related to Weakness (1), SFT is not the correct baseline for OTR. Since the method takes on-policy samples (and from the above analysis requires TK samples per training dataset sequence), an online RL baseline is more appropriate. For example, an RL run which samples 1 full trajectory per training dataset sequence (requiring T samples), performs an agent update iteration, then repeating this K times, would match the compute used. It would be much more helpful to compare the performance improvement achieved by OTR with this baseline to evaluate the effectiveness of this method.\n\n3. The paper only trained using Qwen models. This limits the evaluation of the generality of the proposed method across model types."}, "questions": {"value": "In \"training details\" it is mentioned that OTR and SFT were trained using identical settings. Why is this? Shouldn't each setting be optimized in order to obtain the highest improvement? \n\nWhy are only Qwen models used? Why not other open-source models with similar number of parameters?\n\nIn equation (6), why does the policy \\pi have a ' mark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9YpOZaudz3", "forum": "oEMJzGB5du", "replyto": "oEMJzGB5du", "signatures": ["ICLR.cc/2026/Conference/Submission11044/Reviewer_hxWy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11044/Reviewer_hxWy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704823165, "cdate": 1761704823165, "tmdate": 1762922220740, "mdate": 1762922220740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}