{"id": "fZI202aAzD", "number": 5978, "cdate": 1757949296853, "mdate": 1762933939357, "content": {"title": "Improving Zero-shot Low-light Object Detection via Handling of Motion Blur", "abstract": "Zero-shot low-light object detection (ZLOD) presents great challenges as it aims to generalize detectors from the daytime domain to the nighttime domain without target data. Existing methods primarily focus on learning illumination consistency through daytime and synthetic nighttime image pairs, but they ignore a crucial characteristic that commonly co-exists with low illumination, i.e., motion blur. Nighttime images are particularly susceptible to motion-induced blur due to the long exposure times of cameras. Thus, solely considering illumination reduction without motion blur may be sub-optimal for ZLOD. To this end, we propose a novel Illumination-Blur Consistency (IBC) framework for ZLOD. Specifically, we synthesize nighttime images by considering illumination reduction and motion blur generation under a unified pipeline to access the complex nighttime domain. Then, we explore illumination-blur equivariant representations at the region and instance levels for better model adaptation. Consequently, IBC enables detectors to effectively generalize to the nighttime domain without relying on any dark data. Experimental results demonstrate the superior generalizability of our method. We also introduce a novel dataset named NightVision to expand the capacity of existing low-light benchmarks for community development.", "tldr": "This paper collects a low-light dataset and propose an effective illumination-blur consistency framework for zero-shot low-light object detection.", "keywords": ["low-light object detection", "zero-shot", "motion blur"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a3bad47242db97486868500ed0a72ea4dabfda71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an Illumination-Blur Consistency framework for zero-shot low-light object detection. It builds upon prior zero-shot day-night adaptation methods (especially Similarity Min-Max), which modeled illumination consistency through controllable exposure-guided image darkening and feature-level similarity maximization. The new contribution introduces motion blur as an additional degradation factor co-occurring with low illumination. They also release a new low-light dataset, NightVision, containing 10K images and 18 categories. Experiments on ExDark and NightVision report moderate gains over previous zero-shot baselines including Sim-MinMax."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Writing.** The paper is clearly written and easy to understand\n\n- **Clear motivation.** The paper presents a natural extension of the illumination-consistency idea from Sim-MinMax to include motion blur. The proposed exposure-guided blur generation uses simple but physically interpretable camera-motion trajectories.\n\n- **Extensive experiments.** Experiments cover both synthetic and real blurry data and include ablation studies confirming that the blur component provides measurable improvements."}, "weaknesses": {"value": "- **Incremental novelty.**\nThe IBC framework reuses the core exposure-guided darkening function and similarity-based consistency training introduced in Sim-MinMax. The main new element is the addition of a blur kernel generator controlled by the same exposure factor and a region-level feature alignment. Conceptually, this is a straightforward extension of the existing framework rather than a fundamentally new idea.\n\n- **Limited methodological depth.** The multi-level consistency loss is essentially a variant of existing contrastive consistency learning (BYOL), also used in Sim-MinMax without deeper theoretical insight. \n\n- **Lack of broader insight.**\nThe paper still focuses narrowly on illumination and blur, with little discussion of other nighttime factors (e.g., sensor noise, non-uniform lighting, glare). The “illumination-blur equivariance” claim remains qualitative.\n\nOverall I think this paper looks like an extension of the Sim-MinMax paper rather than a introducing a brand-novel approach towards the ZLOD task. Although some components are proposed by this paper, e.g., the blur synthesis module, I think this is not enough to champion the acceptance of this paper. Nevertheless, I acknowledge this paper proposes a valid approach and achieves SoTA results. Therefore, I recommend borderline reject."}, "questions": {"value": "- Have the authors considered joint training and the blurring function and the detection network or using learned blur kernels? Currently the deblurring module is completely based on heuristics. How sensitive are results to the manually designed camera-motion trajectories?\n\n- It would strengthen the paper to show the darkening results of Sim-MinMax and the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KCI7n4FGOs", "forum": "fZI202aAzD", "replyto": "fZI202aAzD", "signatures": ["ICLR.cc/2026/Conference/Submission5978/Reviewer_tpMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5978/Reviewer_tpMS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530652133, "cdate": 1761530652133, "tmdate": 1762918389047, "mdate": 1762918389047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "XSMbx9gTHP", "forum": "fZI202aAzD", "replyto": "fZI202aAzD", "signatures": ["ICLR.cc/2026/Conference/Submission5978/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5978/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762933937479, "cdate": 1762933937479, "tmdate": 1762933937479, "mdate": 1762933937479, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the low light and motion blur, the paper provides the NightVision dataset and proposes an Illumination-Blur Consistency (IBC) framework for Zero-shot low-light object detection (ZLOD). In detail, IBC first synthesizes nighttime images with image darkening and motion blur generation. And then the paper explores illumination-blur equivariant representations at the region and instance levels. Experimental results demonstrate the effectiveness of the proposed method on the proposed NightVision dataset and the ExDark dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The methodology section is presented in a formal, equation-based manner, and figures are aesthetically rendered."}, "weaknesses": {"value": "1.\tIn the \"motion blur learning\" section of the related work, how do existing methods address motion blur, particularly in the field of object detection? There is a lack of systematic summarization. Furthermore, the distinction between the proposed method in this paper and existing approaches for handling motion blur has not been clarified.\n2.\tIn the \"Low-light Object Detection\" section of the related work, only a brief description is provided regarding how zero-shot object detection methods operate, while the intrinsic connections and differences among them are not discussed.\n3.\tIn Lines 277–278, why select the last three feature layers to ensure consistency at both the region and instance levels?\n4.\tThe Exdark dataset considers low illumination, motion blur, multi-angle, multi-category, and multi-weather conditions. Compared to Exdark, what is the core advantage of the dataset proposed in this paper?\n5.\tThe methodological innovation is limited. Compared to existing zero-shot low-light object detection methods, this paper merely extends the consideration to motion blur. As shown in Figure 1, the overall pipeline remains unchanged from previous approaches.\n6.\tCurrently, there are integrated algorithms proposed for low-light image enhancement and deblurring, such as [1]. What would be their performance if applied to the zero-shot low-light object detection domain?\n\n[1] Lv X, Zhang S, Wang C, et al. Fourier priors-guided diffusion for zero-shot joint low-light enhancement and deblurring[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 25378-25388."}, "questions": {"value": "1.\tWhat is the innovation of the proposed method in this paper compared to existing motion blur learning approaches?\n2.\tCompared to existing zero-shot low-light object detection methods, apart from further considering motion blur, what is the innovation of the proposed method in this paper?\n3.\tThe Exdark dataset considers low illumination, motion blur, multi-angle, multi-category, and multi-weather conditions. Compared to Exdark, what is the core advantage of the dataset proposed in this paper?\n4.\tIn Lines 277–278, what is the rationale for selecting the last three feature layers to ensure consistency at both the region and instance levels?\n5.\tCurrently, there are integrated algorithms proposed for low-light image enhancement and deblurring, such as [1]. What would be the effect if they were applied to the zero-shot low-light object detection domain?\n\n[1] Lv X, Zhang S, Wang C, et al. Fourier priors-guided diffusion for zero-shot joint low-light enhancement and deblurring[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 25378-25388."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TgyJr9ybb7", "forum": "fZI202aAzD", "replyto": "fZI202aAzD", "signatures": ["ICLR.cc/2026/Conference/Submission5978/Reviewer_psuM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5978/Reviewer_psuM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897659888, "cdate": 1761897659888, "tmdate": 1762918388827, "mdate": 1762918388827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the core challenge of ZLOD, namely domain generalization without target dark data. It points out that existing methods only focus on illumination consistency while neglecting the motion blur associated with long camera exposure times. To tackle this issue, the paper proposes the IBC framework: the ENP regulates illumination reduction and motion blur generation via a single exposure factor to synthesize realistic nighttime images; the MMA resolves feature misalignment through region-level and instance-level losses to learn illumination-blur equivariant representations. Meanwhile, the paper constructs the NightVision dataset, covering multi-angle, multi-weather, and multi-size scenarios, with diversity surpassing existing benchmarks. Experimental results demonstrate that IBC outperforms zero-shot and enhancement-based methods on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper accurately captures the coexistence of motion blur and low-light conditions in ZLOD. The proposed IBC framework achieves efficient domain generalization: The ENP regulates light reduction and motion blur generation. Formula 3 incorporates comprehensive velocity information to model positions, while the exposure factor e can regulate exposure time—both lay the foundation for generating realistic datasets. To address the mismatch between the bounding boxes of blurred objects and static objects, the MMA performs feature alignment at two scales (region-level and instance-level), resolving the issue of feature misalignment. Additionally, the authors present a comprehensive NightVision dataset. The experiments are rich in content and have a clear structure."}, "weaknesses": {"value": "Is it reasonable to use a single exposure factor to uniformly regulate illumination and blur in ENP? It ignores practical interfering factors such as camera image stabilization and object motion speed, Specifically, the interference of the camera's anti shake mechanism and the motion state of objects in real scenes were not taken into account.; the ablation experiments fail to eliminate the interference of module interaction, and the contribution of MMA requires experimental clarification. Reference formats are inconsistent and need to be standardized."}, "questions": {"value": "1. Regarding the design of \"uniformly regulating illumination and blur with a single exposure factor\" in ENP, could you supplement validation experiments in real-world scenarios to further support its physical rationality?\n\n2. Detection methods specifically optimized for motion blur have not been included in the comparative experiments. Could you supplement the relevant comparison results?\n\n3. The existing ablation experiments fail to eliminate the interaction interference between ENP and MMA, and it is necessary to clarify the unique contribution of MMA in addressing feature misalignment.\n\n4. Is it possible to achieve more accurate modeling of motion trajectories based on Kalman filtering, which could potentially enhance the accuracy and robustness of trajectory estimation by effectively accounting for noise and dynamic variations in real-world shooting scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2THJgwm0iH", "forum": "fZI202aAzD", "replyto": "fZI202aAzD", "signatures": ["ICLR.cc/2026/Conference/Submission5978/Reviewer_nbLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5978/Reviewer_nbLy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762602137226, "cdate": 1762602137226, "tmdate": 1762918388563, "mdate": 1762918388563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}