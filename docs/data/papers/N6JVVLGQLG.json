{"id": "N6JVVLGQLG", "number": 6395, "cdate": 1757979138660, "mdate": 1763071346983, "content": {"title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought", "abstract": "We propose MIRA (Multimodal Imagination for Reasoning Assessment), a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional Chain-of-thought (CoT) methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images --- such as sketches, structural diagrams, or path drawings --- to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". \nTo solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone (e.g., tracking a die’s movement on a board and summing the face-down values after each roll). To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers.  We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT (Text-CoT) input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts.  To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings.  Experimental results show that existing multimodal large language models (MLLMs), including strongest private models (e.g., GPT-5, o3, Gemini 2.5 Pro) as well as strong open-weight models (e.g., Qwen2.5-VL, GLM 4.5V), perform poorly when relying solely on textual prompts.  However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. \nWe also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting.  These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.", "tldr": "A benchmark that requires multimodal system to go beyond just text reasoning and 'think with images'", "keywords": ["Visual Cot", "Benchmark", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/194dabe67b7467505c6a89ccbc71fb646f2ff2a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper construct a benchmark for evaluating multimodal problems reasoning performance. It measures the performance of MLLMs  under both text-only CoT and Visual-CoT input settings.\n\nFor text-only CoT, the evaluation includes two modes: one using a generic system prompt, and another using a task-specific system prompt tailored to the specific question. For Visual-CoT, the evaluation uses human-prepared images as the visual chain of thought. Under all evaluation settings, the MLLMs show limited performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces high-quality benchmark with human labeling and inspection. The benchmark spans 20 task types and includes 546 carefully designed examples. On this benchmark, current models perform poorly, indicating that it presents a challenging set of tasks for existing models.\n\n2. By providing well-designed text CoTs and visual CoTs, the authors tested the upper performance of current models on this task. Even with high-quality simulated Visual-CoT reasoning (manually annotated intermediate images), the models still struggled to answer these questions effectively. These simulated Visual-CoT reasoning traces can serve as a foundation for future research on enabling models to autonomously generate their own internal Visual CoTs."}, "weaknesses": {"value": "1. As a benchmark-oriented study, and the process does not involve automated construction by models, it may not align closely with ICLR’s primary interests. It would likely be more suitable for the data track.\n\n2. In addition to providing text-CoT prompts and simulated visual CoTs for questions to test the model’s upper-bound performance under high-quality CoT conditions, there doesn’t seem to be a fundamental difference between this work and other multi-modality reasoning benchmarks, such as [1]. Of course, [1] was released within about a month of the ICLR submission deadline, it can be considered a contemporaneous work. I still expect the authors to include a more detailed discussion comparing their approach with similar works in this area.\n\n\n3. I suggest that the authors include the CoT construction process in the main text, as this is crucial for understanding the experiments. Especially the fact that the simulated visual CoTs are human-constructed. This raises a question: these human-constructed CoTs might function more like visual hints than genuine model reasoning traces, have the authors considered comparing their benchmark results with studies where models autonomously generate visual CoTs? For instance, see [2].\n\n[1] Li, C., Wu, W., Zhang, H., Li, Q., Gao, Z., Xia, Y., ... & Wei, F. (2025). 11plus-bench: Demystifying multimodal LLM spatial reasoning with cognitive-inspired analysis. arXiv preprint arXiv:2508.20068.\n\n[2] Li, C., Wu, W., Zhang, H., Xia, Y., Mao, S., Dong, L., Vulić, I., & Wei, F. (2025). Imagine While Reasoning in Space: Multimodal Visualization-of-Thought. In Forty-second International Conference on Machine Learning (ICML)."}, "questions": {"value": "Please check the comments in Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g1BcJ1nWSS", "forum": "N6JVVLGQLG", "replyto": "N6JVVLGQLG", "signatures": ["ICLR.cc/2026/Conference/Submission6395/Reviewer_XjS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6395/Reviewer_XjS7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386185353, "cdate": 1761386185353, "tmdate": 1762918701433, "mdate": 1762918701433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wL77T2avyR", "forum": "N6JVVLGQLG", "replyto": "N6JVVLGQLG", "signatures": ["ICLR.cc/2026/Conference/Submission6395/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6395/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763071346026, "cdate": 1763071346026, "tmdate": 1763071346026, "mdate": 1763071346026, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIRA, a novel benchmark designed to evaluate the visual reasoning capabilities of Multimodal Large Language Models (MLLMs). The authors argue that current Chain-of-Thought (CoT) prompting methods are limited to the textual domain and fail on tasks that are intrinsically visual, where humans often resort to sketching or visualizing intermediate steps. MIRA consists of 546 challenging problems across four domains (Geometry, Physics, Puzzles, and Causal Transformations) that require such intermediate visual reasoning. Experiments on a wide range of state-of-the-art MLLMs show that MIRA is highly challenging, underscoring the limitations of current models and the importance of this reasoning modality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed three-level evaluation protocol is a major strength. By systematically comparing Direct Evaluation, Text-CoT, and Simulated Visual-CoT, it provides clear evidence for their central claims.\n\n2. The authors have conducted an extensive evaluation across a large and representative set of MLLMs, including top-tier closed-source models and various open-weight alternatives."}, "weaknesses": {"value": "1. **Limited Scale and Generality of the Benchmark**: The size of this benchmark (546 examples) is relatively modest, which could limit the statistical power of the conclusions. Besides, it remains an open question how well the findings would generalize to more common, real-world visual reasoning scenarios that are less puzzle-like.\n\n2. **Lack of a Random Baseline**: The paper does not provide a random-chance baseline for its tasks.With reported accuracies in most tasks under 20%,  it is difficult to ascertain whether the models are exhibiting any meaningful reasoning or simply performing near (or even below) random guessing.\n\n3. **Uniformly High Task Difficulty Limits Diagnostic Granularity**: The consistently low performance across nearly all models raises a question about the benchmark's difficulty calibration. While demonstrating the limitations of current SOTA models is a valid goal, the tasks may be uniformly too difficult to serve as a fine-grained diagnostic tool. A benchmark that includes a spectrum of difficulty levels (e.g., easy, medium, hard) would be more valuable. It would allow researchers to identify the \"breaking point\" of different models and architectures, revealing partial capabilities rather than just wholesale failure."}, "questions": {"value": "Given that the performance of even the strongest models is quite low across most tasks, have the authors considered establishing a human performance baseline for the MIRA benchmark? This data would be valuable for calibrating the absolute difficulty of the tasks and providing a clear \"upper bound\" to better contextualize the models' scores and quantify the gap to human-level reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U5WoudS7cI", "forum": "N6JVVLGQLG", "replyto": "N6JVVLGQLG", "signatures": ["ICLR.cc/2026/Conference/Submission6395/Reviewer_xQL5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6395/Reviewer_xQL5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454315393, "cdate": 1761454315393, "tmdate": 1762918685104, "mdate": 1762918685104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIRA, a new benchmark designed to evaluate the reasoning capabilities of MLLMs in tasks where generating intermediate visual steps is essential. The authors argue that standard textual CoT prompting is insufficient for problems requiring spatial or geometric reasoning, which humans often solve by \"drawing to think.\" The MIRA benchmark includes 546 problems annotated with gold-standard, step-by-step visual intermediate states to facilitate a novel \"Visual-CoT\" evaluation. Experiments on state-of-the-art models show they perform poorly with standard inputs and textual CoT prompting. However, when models are provided with the intermediate visual cues in the Visual-CoT setting, their performance consistently and significantly improves, highlighting the current limitations of text-only reasoning and the critical role of visual CoT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes an interesting scenario that requires the generation of intermediate visual images to solve reasoning tasks.\n\n2. The curated intermediate CoT images provided in the dataset are of high quality."}, "weaknesses": {"value": "1. **Inconsistent Answer Formats:** The benchmark employs a wide variety of answer formats, including multiple-choice, free-form text, numeric values, lists, and even custom coordinate-based formats (e.g., the localizer task). This inconsistency makes it difficult to perform robust statistical analysis or establish a consistent \"random guess\" baseline for comparison across the diverse task types.\n\n2. **Unclear Narrative and Analysis:** The paper's core objective is not clearly articulated; it remains ambiguous about the specific MLLM ability MIRA is intended to measure. The manuscript dedicates excessive space to discussing the limitations of textual CoT and contrasting it with Visual-CoT, which detracts from the benchmark's own merits. This is compounded by a lack of fine-grained error analysis; the paper seems to attribute the models' poor performance primarily to the failures of textual CoT without a deeper investigation. It doesn't analyze *why* models fail on the visual tasks themselves (e.g., perception, spatial misinterpretation, logical error, failure to track state).\n\n3. **Limited Scale and Discriminative Power:** The dataset scale is small. As a result, the reported results across various models are not sufficiently discriminative. Most models perform similarly poorly, making it difficult to use the benchmark to rank or meaningfully distinguish their relative reasoning capabilities.\n\n4.  **Visual-CoT Setting Misalignment:** The Visual-CoT setting does not effectively evaluate a reasoning trajectory. By providing the gold-standard intermediate images as inputs, the benchmark does not evaluate the model's reasoning trajectory. Instead, it reframes the task as a multi-image reasoning problem where the intermediate images act as \"hints\" rather than as a \"thought process\" complementary to the model's own reasoning. This setup fails to capture the dynamic, interleaved nature of a true visual reasoning process. A more effective evaluation might involve providing these visual cues in an interactive, simulative manner, rather than as a single, complete set of inputs."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LQwnTfDJgV", "forum": "N6JVVLGQLG", "replyto": "N6JVVLGQLG", "signatures": ["ICLR.cc/2026/Conference/Submission6395/Reviewer_RL6B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6395/Reviewer_RL6B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457626494, "cdate": 1761457626494, "tmdate": 1762918683829, "mdate": 1762918683829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce MIRA, a new benchmark for model evaluation where reasoning across modalities -- i.e., in text and images -- helps solve the given tasks. They create a pipeline for generating this data, involving complexity-conditioned programmatic and manual generation with manual verification. They evaluate three different settings: direct evaluation, text-only chain of thought, and 'simulated visual-CoT reasoning' (giving the MLLM the correct intermediate drawing that it would ideally make, but that current models are apparently not capable of making)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper successfully identifies shortcomings in existing datasets and makes a convincing argument for why it is needed. \n- The pipeline for data generation and for evaluation are set up well, providing some degree of trust in the dataset.\n- The results on the benchmark suggest new directions for the field to explore, highlighting types of questions that remain unsolved even by strong closed models."}, "weaknesses": {"value": "- Tool-augmented methods are mentioned as part of the motivation (page 2) but not evaluated on the benchmark as far as I can tell. It would be interesting to see these results.\n- 4.2 suggests Visual-CoT data may be a way forward with these types of questions, but these need to be manually created if I understand correctly. In what contexts would they actually be helpful given new problems won't have them at inference time, or will need to be annotated for that specifically? When is it easier for the human to provide the Visual-CoT annotation than just the final answer itself?\n- Additional details such as the breakdown of problem sources and how many are manual vs not would be good to have known.\n- Some additional references for intermediate visual reasoning: Zebra-CoT (Li et al), Whiteboard-of-Thought (Menon et al), SketchAgent (Vinker et al)."}, "questions": {"value": "- The motivation of simulated Visual-CoT makes sense to me as an evaluation setting, but how could it be actually applied to new problems as the authors allude to in 4.2? \n- What is human performance on the benchmark?\n- Are any methods that attempt to create intermediate visual representations evaluated other than the two unified MLLMs Bagel and Janus-Pro, which can't produce images on the way to text answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRoDipxtl6", "forum": "N6JVVLGQLG", "replyto": "N6JVVLGQLG", "signatures": ["ICLR.cc/2026/Conference/Submission6395/Reviewer_zfSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6395/Reviewer_zfSt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953849218, "cdate": 1761953849218, "tmdate": 1762918682017, "mdate": 1762918682017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}