{"id": "zRt7J8Ej7M", "number": 24244, "cdate": 1758354553851, "mdate": 1759896774549, "content": {"title": "FAKER: Generating Frequency-based Artificial Attributes via Random Walks for Non-attribute Graph Representation Learning", "abstract": "A key challenge for Graph Neural Networks (GNNs) is their reliance on initial node features, while many real-world graphs lack such attributes due to privacy constraints or limitations in data collection. Existing adjacency-only approaches attempt to learn representations directly from topology. However, they often inherit the sampling biases of random walks, leading to skewed embeddings. To address these limitations, we propose FAKER, a diagnosis‑driven, model‑agnostic framework that synthesizes artificial node attributes from topology alone. FAKER first analyzes group-level visit signals from random walks with Power Spectral Density (PSD) to quantify low-frequency persistence bias and high-frequency switching bias. The resulting quantified scores then drive an adaptive sampler that produces a balanced corpus without distributional assumptions by reweighting transitions and allocating additional walks. A lightweight co‑occurrence encoder trained on this corpus yields dynamic features, which are merged with a compact structural summary and standardized to form plug‑and‑play attributes for any GNN. Across four benchmarks, FAKER establishes state-of-the-art results among adjacency-only baselines for node classification and link prediction. It also matches or outperforms feature-using methods on three datasets. Ablation and robustness studies show that the improvements result from the frequency-domain diagnosis and adaptive allocation, rather than from the number of walks or sensitive hyperparameter tuning.", "tldr": "", "keywords": ["graph neural network", "attribute-missing", "random walk", "artificial node features", "non-attributed graph"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea7f90330f2f0f2d257a827c080430c848f9dbd2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a node attribute generation framework designed primarily for Graph Neural Networks (GNNs). The paper relies on a random walk–based embedding mechanism and introduces a novel strategy to augment the set of walks by generating additional random walks in order to manipulate node occurrence frequencies. The generated random walks (i.e., node sequences) are then used to learn node embeddings using the SkipGram approach, which serves as node features for the GNN models. The approach is evaluated on node classification and link prediction tasks and outperforms baseline approaches across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-organized and includes illustrative figures that help in understanding of the proposed framework. \n- Addressing the problem of node featureless graphs is a meaningful and relevant challenge for the GNN community.\n- The two-step random walk design offers an intuitive way to control node appearance frequency and its influence on embedding quality."}, "weaknesses": {"value": "- The proposed framework largely builds upon existing random walk–based embedding approaches (e.g., DeepWalk, Node2Vec) and provides a limited methodological novelty. \n- The paper lacks a deeper theoretical justification for the proposed components. Since the paper proposes a node attribute generation approach based on random walks, a GNN or an inductive method that uses these artificial features cannot handle new nodes inductively without re-training, undermining one of the key advantages of many GNN approaches.\n- The datasets used (e.g., Cora, Citeseer) are quite small and sparse, and the train/validation/test splits for link prediction seem harsh. Some baseline results also lack standard deviation values."}, "questions": {"value": "- In Table 1, FAKER-E performs similarly to the variants of the proposed approach except on the Cora dataset. Can the authors elaborate on whether the generated node features truly enhance GNN learning or mainly replicate the effects of existing embeddings?\n\n- For the baseline models lacking standard deviations, were the results copied directly from the original papers? If that's the case, how comparable are the experimental setups?\n\n- Could the authors clarify the motivation behind using a two-step random walk procedure? Why not apply a biased walk directly, since the node visiting frequency is degree-proportional for an unbiased walking strategy?\n\n- Have the authors considered an alternative to explicit walk generation, such as reweighting node pairs during optimization to control co-occurrence frequencies?\n\n- Could the authors explain the almost perfect link prediction scores on the Computer and Photo datasets? Are these results expected given dataset density or model bias?\n\n- It is unclear why Node2Vec, a closely related random walk–based embedding method, was not included as a baseline.\n\n- Are there any empty neighbor set cases as indicated in Eq. 302? Do the networks contain such isolated nodes or very small disconnected components?\n\n- Several hyperparameters (e.g., a=0.1, b=8 in Eq. 7) and architectural decisions (e.g., maximum hop K=2) are not motivated or analyzed. An additional ablation study for these choices could be provided.\n\n**Minor clarity issues:**\n- The symbol, $\\psi$ in Subsection 3.1 is undefined.\n- Table and figure captions are sometimes incomplete (e.g., Table 3 does not specify that it refers to node classification results. Figure 1's caption lacks task indication).\n- It is unclear which random walk generation method is used initially (uniform or biased)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8qBzwnWtp3", "forum": "zRt7J8Ej7M", "replyto": "zRt7J8Ej7M", "signatures": ["ICLR.cc/2026/Conference/Submission24244/Reviewer_Uvtf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24244/Reviewer_Uvtf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471013041, "cdate": 1761471013041, "tmdate": 1762943015205, "mdate": 1762943015205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of learning graph representations without initial node features, proposing FAKER, a model-agnostic framework that synthesizes artificial attributes from topology alone. The method diagnoses frequency-domain biases in random walks using power spectral density, then employs adaptive sampling and a lightweight co-occurrence encoder to generate node features for GNNs. The paper clearly defines the problem and presents a systematic solution combining bias diagnosis and attribute synthesis. Experimental results show consistent improvements over baselines and competitive performance with feature-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The issue studied in this article is novel.\n\n2. The motivation of the article is clear.\n\n3. The techniques used are solid."}, "weaknesses": {"value": "1. The computational complexity of the method needs to be discussed, along with validation on large datasets.\n\n2. Although scenarios with completely missing attributes exist, they are relatively rare. Comparisons with different baseline methods under varying missing rates could better validate the effectiveness of the method."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iqIvzERQaf", "forum": "zRt7J8Ej7M", "replyto": "zRt7J8Ej7M", "signatures": ["ICLR.cc/2026/Conference/Submission24244/Reviewer_6AKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24244/Reviewer_6AKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825407162, "cdate": 1761825407162, "tmdate": 1762943014194, "mdate": 1762943014194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses the problem of creating base embeddings that can be used as node attributes for graph neural networks.  The authors identify that existing methods based on random walks suffer from inherent sampling biases (e.g., degree bias, where hubs are over-sampled and low-degree nodes are under-sampled). To tackle this problem, the authors propose FAKER that arguments the random walk data for training by balancing the walks with strong low-frequency and high-frequency components. The results are improved performance for node classification and link prediction tasks, with various baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is well-motivated. Instead of attempting to debias embeddings in a post-hoc manner (e.g., via the loss function), FAKER tackles the sampling bias \"at its source\" by adaptively correcting the walk generation process itself.\n3. The experiments compared various baseline models across four datasets and confirmed the consistency of the results. \n4. Conducted ablation study to identify the source of improvements. \n5. Parameter sensitivity check (Fig 6)"}, "weaknesses": {"value": "The paper frames the problem as \"degree bias\" (Figure 2) and attributes its success to fixing this. However, Negative Sampling in SGNS already have the mechanisms for mitigating degree bias (Residual2Vec). If (a) the primary issue is degree bias, (b) SGNS solves it, and (c) PSD only corrects for the degree bias, then the \"FAKER w/o PSD\" variant (which is essentially walks + SGNS) should perform similarly to the full FAKER model. The fact that it is not the case means that either (a), (b), (c) is false. Given  dramatically outperforms the \"FAKER w/o PSD\" variant strongly suggests the PSD mechanism is correcting for something more complex or different than just the first-order degree distribution that SGNS already handles, such as the degree assortativity. \n\nThe paper's core claim is that it creates a \"balanced corpus,\" but it never provides evidence of this. A crucial piece of analysis is missing. The paper should have included a \"post-correction\" version of Figure 2. This would verify that the walk-visit shares for hubs and leaves actually did move closer to their true node shares after the PSD correction was applied. Without this, it is impossible to confirm that the degree bias was, in fact, mitigated. \n\nIt is also unknown if the correction may have introduced new biases. The performance gain is clearly from the PSD mechanism, but what it is truly capturing is left unexplored. Given that the PSD diagnosis focuses on \"persistence\" and \"switching,\" it is highly plausible that it is capturing higher-order topological properties. For instance, Persistence (Z_LF) in a hub group is a proxy for high degree-assortativity, andSwitching (Z_HF) between hub and leaf groups is a proxy for degree-disassortativity. The performance gain may be from balancing these more complex structural transitions, not purely from fixing the node-degree distribution. The paper's explanation feels like an oversimplification."}, "questions": {"value": "1. Could the authors provide an analysis of the final, corrected walk corpus (W_final)? Specifically, a reproduction of Figure 2 showing the post-PSD-correction visit shares for hubs and leaves? This would be critical to verifying the central claim that the degree bias was mitigated.\n2. The paper attributes the performance gain to degree debiasing, yet the SGNS model already handles this. Have the authors considered that the PSD mechanism is capturing more complex, higher-order biases? For example, could Z_LF and Z_HF be acting as proxies for balancing degree-assortativity?\n3. Following Q1, did the authors analyze whether the adaptive sampling, while correcting for persistence/switching, might have inadvertently introduced a new, different sampling bias (e.g., skewing the representation of \"other\" nodes)?\n4. In the ablation study, how does the \"FAKER w/o PSD\" variant compare directly against a standard node2vec (or DeepWalk) + SI baseline? This would help isolate the contribution of the SI component from the PSD mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6Cpm0QX8Gl", "forum": "zRt7J8Ej7M", "replyto": "zRt7J8Ej7M", "signatures": ["ICLR.cc/2026/Conference/Submission24244/Reviewer_AuTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24244/Reviewer_AuTj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994497484, "cdate": 1761994497484, "tmdate": 1762943013674, "mdate": 1762943013674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}