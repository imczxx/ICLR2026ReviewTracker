{"id": "SVpw8RJL6c", "number": 22212, "cdate": 1758327826934, "mdate": 1759896879917, "content": {"title": "One-Step Flow Policy Mirror Descent", "abstract": "Diffusion policies have achieved great success in online reinforcement learning (RL) due to their strong expressive capacity. However, the inference of diffusion policy models relies on a slow iterative sampling process, which limits their responsiveness. To overcome this limitation, we propose Flow Policy Mirror Descent (FPMD), an online RL algorithm that enables 1-step sampling during flow policy inference. Our approach exploits a theoretical connection between the distribution variance and the discretization error of single-step sampling in straight interpolation flow matching models, and requires no extra distillation or consistency training. We present two algorithm variants based on rectified flow policy and MeanFlow policy, respectively. Extensive empirical evaluations on MuJoCo and visual DeepMind Control Suite benchmarks demonstrate that our algorithms show strong performance comparable to diffusion policy baselines while requiring orders of magnitude less computational cost during inference.", "tldr": "", "keywords": ["reinforcement learning", "flow models", "flow policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/541d987a400797be00874d7ace2b1dff8cd6f472.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Summary:\nThis work proposes Flow Policy Mirror Descent (FPMD), a one-step flow policy that is trained alongside the standard flow model. However, the trained one-step policy doesn't require any additional distillation or consistency training after training an ordinary diffusion/flow policy. Because the resulting policy generates actions with only a single denoising step, its inference time is reduced drastically. The effectiveness of the proposed method is demonstrated on various simulated online RL benchmarks, including image-based environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper tackles an interesting and relevant approach in RL, more specifically w.r.t efficient policy representations with higher capacity than known Gaussian-based policies.  \n\n- The paper is well-motivated and has a nice story such that the reader can follow the steps to understand it."}, "weaknesses": {"value": "- Please see the Questions below. \n\n- Minor: I recommend running a grammar check. There are several grammatical issues, such as missing articles in the text."}, "questions": {"value": "- The proposed method employs importance sampling in Eq. 7. However, importance sampling is known to have high variance, especially in higher-dimensional tasks/action spaces. Is there any analysis/observation regarding this fact? What is the highest action dimensionality considered in the experiments? \n\n- Section 3.2 states that the target distribution converges to an almost deterministic distribution. Could the authors please provide an intuition for this? The optimal solution for the mirror descent policy is given as pi_old exp(Q(s,a)/\\lambda), however, to my understanding, if \\ lambda is rather large, this essentially means the new policy is very similar to the old policy. Doesn't this mean that it depends on how lambda is chosen rather than the converged solution? \n\n- Could the paper provide an intuition behind Eq. 10? I think this could help improve the reader's understanding \n\n\n- When comparing the results between classic model-free RL approaches to diffusion/flow-based approaches, it is interesting that for the gym environments, the diffusion/flow-based approaches seem to perform better (Table 1). However, looking at the vision-based environments, except for the dog environments, it looks like this gap is much smaller, or even closed for some environments, where the confidence intervals seem to overlap. Is there an intuition on this observation?  \n\n- Is lambda fixed, or optimized? If fixed, is there an intuitive way to choose its values? From my understanding, it depends on Q's value range. If optimized, how exactly is it optimized? \n\n\n- The proposed framework does not consider the mostly used Maximum Entropy Reinforcement learning framework as used in the well-known SAC algorithm, but also used in diffusion-based policies such as proposed by O. Celik et al. 2025 (see citations in paper), where it seems that higher returns on the gym environments, namely the Ant and the more high-dimensional Humanoid environments for diffusion-based policies are reported. Is there a plan to compare against this objective variant as well? \n\n\n- Connected to the question before, is there a possibility to extend the proposed framework to the Maximum entropy objective? How much does the assumption w.r.t. the deterministic, i.e., low variance target distribution, restrict the proposed method from applying it to the maximum entropy RL framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I5EFSARW2o", "forum": "SVpw8RJL6c", "replyto": "SVpw8RJL6c", "signatures": ["ICLR.cc/2026/Conference/Submission22212/Reviewer_qRLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22212/Reviewer_qRLo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839176366, "cdate": 1761839176366, "tmdate": 1762942117546, "mdate": 1762942117546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a limitation of diffusion policies in online reinforcement learning -- their high inference cost. While diffusion models are highly expressive, they require a slow, iterative sampling process (many denoising steps) to generate an action, making them unsuitable for real-time applications.\n\nThe authors propose Flow Policy Mirror Descent (FPMD), an online actor-critic algorithm that uses a flow-based generative model as the policy. The core insight is a theoretical connection: in straight-interpolation flow models, the single-step sampling error is bounded by the variance of the target distribution. After the model converges, during exploitation, the optimal policy converges and typically has low variance. As this variance drops, the 1-step sampling error of the flow model also drops, allowing for highly efficient, single-step inference without any architectural changes, distillation, or consistency training.\n\nTo train this flow policy in an online RL setting, the authors derive a novel, practical loss function based on Policy Mirror Descent (PMD). This loss uses importance-weighted samples from the old policy to approximate the flow matching objective.\n\nThe paper presents two variants: FPMD-R (Rectified Flow) uses multi-step sampling during training but achieves 1-step sampling at inference. FPMD-M (MeanFlow) aims for 1-step sampling during both training and inference.\n\nExperiments on MuJoCo and DeepMind Control Suite benchmarks show that FPMD achieves performance comparable to state-of-the-art diffusion policies while being 4x-10x faster at inference, matching the speed of simple Gaussian policies like SAC."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "FPMD-R/M match or exceed diffusion policy baselines on most tasks.\n\nThe inference time speed-ups are clearly demonstrated in Figures 1 and 3.\n\nThe method is built on a solid foundation. Proposition 2 provides the theoretical bound for the 1-step error, and the derivation of the L_FPMD loss from the PMD objective appears sound."}, "weaknesses": {"value": "The core importance-sampling loss (Eq. 9) is a relatively standard technique for applying generative models in reward-based learning, similar to reward-weighted objectives in image/video generation. \n\nThe core premise that the flow policy converges to an efficient 1-step model, relies on the optimal policy having low variance (being near-deterministic). This assumption, while true for the benchmarked tasks, may not hold for more complex or multi-task where the optimal policy itself is inherently stochastic or multi-modal. The paper's main benefit might not generalize to these settings.\n\n Following the points above, the paper could be viewed as a straightforward combination of a standard Actor-Critic algorithm with a Rectified Flow policy, primarily leveraging known properties of flow models rather than providing deep new insights into either RL or generative models."}, "questions": {"value": "(Related to Weakness 2) How would FPMD perform in a task where the optimal policy is truly multi-modal (e.g., a scenario with two equally good but distinct solutions)? Would the policy \"collapse\" to a single mode to gain the 1-step sampling benefit, or would it maintain multi-modality and suffer a larger 1-step discretization error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1w79gmnnlf", "forum": "SVpw8RJL6c", "replyto": "SVpw8RJL6c", "signatures": ["ICLR.cc/2026/Conference/Submission22212/Reviewer_GQsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22212/Reviewer_GQsJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986516171, "cdate": 1761986516171, "tmdate": 1762942117177, "mdate": 1762942117177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two expressive policies based on flow-based models that permit one-step sampling at evaluation time. The first model builds on rectified flow, which still requires multi-step sampling during training. For this model, the authors theoretically show that as the online policy’s stochasticity shrinks, the one-step sampling would have small error. Thus, after sufficient training, using the vector field at the first time step for all time steps will induce small errors, which is empirically demonstrated in a MuJoCo environment. On the other hand, the second model building on MeanFlow parameterizes the average velocity (for arbitrary window) directly, which permits one-step sampling in both training and evaluation time. Empirically, both models are shown to achieve competitive performance in state-based Gym MuJoCo and visual DeepMind Control Suite tasks, compared to diffusion-based policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The papers have several strengths:\n1. The study problem is an interesting topic in reinforcement learning with continuous actions. With the recent development of expressive generative models, using them for policy representation is an interesting direction. The paper addresses an important limitation of current generative approaches – high inference time.\n2. The paper reveals a novel insight that speeding up flow-based models with fewer sampling steps could be reasonable as the distribution becomes less stochastic, which aligns with the policy becoming more deterministic during training in RL. Such a property is not exploited in generative modeling as stochasticity is desirable and maintained there.\n3. The paper is well-written. The mathematical notation is clear, and the text is easy to follow."}, "weaknesses": {"value": "Despite the above strengths, there are also a few weaknesses of the paper:\n1. Lack of controlled experiments to classic model-free RL approaches. The proposed methods perform very similarly to DPMD with their “proxy confidence intervals” overlapping with each other most of the time (note that a proper confidence interval should be used). It is reasonable to hypothesize that the actor update makes a relatively small influence on the training, which may not have been ruled out without further experiments (or clarifications on the current results).\n2. Insufficient discussion of the limitations of the proposed approach.\n  - By considering a partition function weighted loss for convenience, the matching objective puts more emphasis on states with higher action values, which is not necessarily ideal. The paper lack a discussion of potential consequences of doing so.\n  - It does not discuss the training time of the proposed approaches. While evaluation (inference/test) time speed could be a crucial consideration, training time is also an important factor. The paper should disclose the training time for each model and compare it with other approaches.\n3. While the fixed point view seems to be useful, the paper didn’t show the contraction property of the MeanFlow operator, which is a crucial fundamental property that guarantees a fixed point.\n4. Some presentations of the empirical results could be misleading. 1) The highlight in the table does not take into account the randomness of experiments. Different methods often perform very similarly but only one of them is highlighted. 2) The claim in Lines 454 is based on results with high randomness and might not hold in general (e.g., for a different set of seeds)."}, "questions": {"value": "Could the authors clarify the below questions:\n1. What do the shaded areas show in Figure 2?\n2. Are there learning curves for the Gym MuJoCo experiments?\n3. What’re the hyperparameters and critic updates for classic model-free RL methods in different settings? Are they consistent with the proposed methods?\n4. Could the authors test classic model-free RL methods (SAC or DDPG) with the same configurations as the proposed methods whenever applicable? For example, the critic update, network configurations, best-of-n sampling, etc. should be used for them.\n\nOther minor suggestions that do not impact the recommendation:\n1. Line 045: distillation -> distilling\n2. Line 139: mixed use of $a$ and $x$.\n3. Line 142: the random variables should be written out explicitly in Eq. 4\n4. Line 195: “…(is) the distribution…”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S6evsL1HZh", "forum": "SVpw8RJL6c", "replyto": "SVpw8RJL6c", "signatures": ["ICLR.cc/2026/Conference/Submission22212/Reviewer_onHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22212/Reviewer_onHY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990909741, "cdate": 1761990909741, "tmdate": 1762942116975, "mdate": 1762942116975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method to learn and evaluate flow-based policies that recover performance comparable to diffusion policy methods at a fraction of the computational cost. They adapt existing loss functions to the setting where the goal is to match a target mirror descent policy (the typical closed-form policy used for this setting) and take advantage of the fact that this target policy becomes more deterministic, reducing discretization error over time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and easy to follow. \n\nIt is useful to investigate the new MeanFlow approach for learning policies in RL."}, "weaknesses": {"value": "The paper does not sufficiently place this work relative to other papers in RL using flow policies in RL. The paper does cite several works and say: \n“Compared to these methods, ours is the only method that achieves an effective balance between policy distribution expressiveness and action sampling efficiency, by introducing a practical training objective equivalent to the flow matching objective and enabling one-step action generation.”\nHowever, this is an insufficient description of what is arguably the most important related work. The primary focus in the paper is on contrasting to diffusion policies. This is understandable, as the goal is to reduce the computational cost compared to this more popular strategy. However, it is key to better explain why existing alternatives do not already solve this problem. Further, at least one previous method from this literature should be included in the experiments, rather than only including diffusion approaches. \n\nThroughout the paper there is also a tendency to restate previous results, without making it clear that is it not strictly new. Let me give a few examples. \n1. The importance sampling approach used to sample from the mirror descent target policy is the standard approach (when using a mode-covering KL); though this sampling is never stated to be strictly new in Section 3.1, it is either implied or the reader is left unsure if this is a new strategy introduced here.   \n\n2. Proposition 1 is restated from Hu, used to then prove Proposition 2 that shows that discretization error is low if the variance is low. In their paper, Hu does talk about the ramifications of the variance being zero, so Prop 1 has been used before for somewhat similar reasoning. It would be better here to avoid restated Proposition 1, and potentially just writing you own new proposition, with some context for what Hu showed\n\n3. Proposition 3 seems to focus on proving that minimizing LCFM is equivalent to minimizing LFM. But I believe this was already shown in (Yaron et al., 2022). Is that the case? \n\nThe final theory, Proposition 4 and Theorem 5, seems to state mostly basic facts about convergence of operators given contraction. However, the key criteria that is useful to prove here is that the contraction condition is satisfied for this operator. The current proof seems to largely restate what was in the main text, focused on how one samples and drops constants. Do you believe this is a contraction?  \n\nThere is a key issue in the experiments that results are reported over 5 seeds. This is insufficient to make strong claims about performance. To make statistically significant claims, you could either run more seeds (justifying why that number is enough) or potentially aggregate over the environments and make claims at this aggregate level. For individual environments, you could look at individual runs to see behavior, rather than making strong claims about one method being better than another. One such strong claim is in Figure 2: “FPMD outperforms all baselines with NFE=1 sampling”. This cannot really be said, given the results there.\n\nThe paper also makes a claim that Mean Flow should be better when training under 1-step sampling compared to standard flow matching. This should be better demonstrated in your experiments. I did notice that in the appendix, you ran FPMD-R with fewer than 20 sampling steps during training. This is an important result and should be highlighted in the main body, and should also potentially include more than just 1, 10 and 20 sampling steps. It would also be useful to report training costs to show this tradeoff and better motivate FPMD-M, since otherwise FPMD-R and FPMD-M are relatively similar. Even better would be to also include another algorithm that uses flow matching, to identify if your specific objectives also provide improvements. \n\n(Putting Minor Points here, so there is no separate box. These are not major issues) \n- Typo on line 114: the citations need to be in brackets\n- Same on lines 120, 121\n- On line 418: “diffusion policy method SDAC” – this method is not mentioned anywhere else in the paper\n- The acronym “NFE” is used a lot in the main text, but is only explained in the appendix (Number of Function Evaluations)\n- “Important sammpling” on line 268\n- Hanging “, which leads to” at the end of page 5\n- The “rectified” flow is never defined, although the paper where it was originally introduced is cited.\n- The claim “Although flow policy in Section 3.1 can achieve one-step sampling during inference of a trained policy, it still requires multiple sampling steps when sampling from πold during training. MeanFlow policy reduces this computational cost by using one-step sampling throughout the training process.” This sentence makes it sound like this is a requirement in the algorithm, when in reality this is due to empirical evidence suggesting that too few sampling steps in training result in suboptimal performance.\n- The Peters et al citation is good for the KL loss, in Eq (1), but other works are more pertinent than the other two. For example, consider the MPO paper or a very nice overview paper called “Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning”, Vieillard et al., NeurIPS 2020."}, "questions": {"value": "1. Can you discuss in more detail how your approach differs from previous work using flow policies?\n\n2. Can you provide some aggregation across environments, to make stronger claims, or justify why the number of seeds is sufficient?\n\n3. Can you clarify the above questions on the theory? Is Proposition 3 new, or largely restating what is given in Yaron et al, and can you show the operator is a contraction?\n\n4. Can you clarify the training stability + cost differences between FPMD-R and FPMD-M?\nYou present both learning curves and final performance tables for the visual tasks, but not the state-based tasks. Any reason for this?\n\n5. “The numbers show the best mean returns and standard deviations over 1M frames and 5 random seeds.” – For the descriptions in tables 1 and 2, what does it mean to say the “best standard deviation”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xt3vNhlL3J", "forum": "SVpw8RJL6c", "replyto": "SVpw8RJL6c", "signatures": ["ICLR.cc/2026/Conference/Submission22212/Reviewer_fH6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22212/Reviewer_fH6c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012980428, "cdate": 1762012980428, "tmdate": 1762942116713, "mdate": 1762942116713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}