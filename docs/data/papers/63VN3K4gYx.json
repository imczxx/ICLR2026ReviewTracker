{"id": "63VN3K4gYx", "number": 383, "cdate": 1756737469173, "mdate": 1759898264029, "content": {"title": "Revisiting Multivariate Time Series Forecasting with Missing Values", "abstract": "Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-prediction framework that uses imputation modules to fill in missing values, followed by forecasting on the imputed data. However, this framework overlooks a critical issue: there is no ground truth for the missing values, making the imputation process susceptible to errors that can degrade prediction accuracy.\nIn this paper, we conduct a systematic empirical study and reveal that imputation without direct supervision can corrupt the underlying data distribution and actively degrade prediction accuracy. To address this, we propose a paradigm shift that moves away from imputation and directly predicts from the partially observed time series. We introduce **C**onsistency-**R**egularized **I**nformation **B**ottleneck (CRIB), a novel framework built on the Information Bottleneck principle. CRIB combines a unified-variate attention mechanism with a consistency regularization scheme to learn robust representations that filter out noise introduced by missing values while preserving essential predictive signals. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of CRIB, which predicts accurately even under high missing rates. Our code is available in [https://anonymous.4open.science/r/CRIB-F660](https://anonymous.4open.science/r/CRIB-F660).", "tldr": "An IB-based direct predicting method for multivariate time series forecasting with missing values", "keywords": ["Time Series Forecasting with Missing Values"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1934bf435b0111c92e932e647e16e1a0ae4d54b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of multivariate time series forecasting with missing values and proposes CRIB, a direct prediction approach that integrates the Information Bottleneck theory, a unified-variate attention mechanism, and consistency regularization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- S1: The paper conducts an empirical study that analyzes and reveals the limitations of existing “imputation-then-prediction” MTSF-M methods.\n- S2: The paper conducts extensive experiments to demonstrate the effectiveness of CRIB, covering four datasets, multiple baselines, and various missing patterns."}, "weaknesses": {"value": "- W1: The novelty of the paper is not clearly articulated, and the motivation behind each technical module is insufficiently explained. It appears that the method mainly combines existing components such as the Information Bottleneck, Unified-Variate Attention, and Consistency Regularization.\n- W2：The paper uses an empirical study to analyze and argue that the imputation step in imputation-then-prediction methods is unreasonable. However, the details of the empirical study are not clearly explained.\n- W3：The paper lacks comparisons with the latest MTSF-M baselines, such as GinAR.\nGinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing, KDD 2024.\n- W4：The paper contains some inconsistencies between the text and figures. For example, $L_{conc}$ in Figure 2(d) is not mentioned in the main text."}, "questions": {"value": "- Q1: How were the distribution and correlation map in Figure 1 illustrated? What are the specific implementation details?\n- Q2：Could you clarify the challenges of using direct prediction to address the MTSF-M problem, and explain what specific issues each module of CRIB tackles and what their respective technical novelties are?\n- Q3：How does the paper demonstrate that consistency regularization with the IB theory can retain essential task-relevant information while filtering out irrelevant noise caused by missing values?\n- Q4：How is the Imputed variant implemented in the experiment? Is there any information leakage between the training of the completion task and the prediction task?\n- Q5：How are the three missing patterns, point, block, and column, simulated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wFZjAFeC6R", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_nmeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_nmeJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760603271667, "cdate": 1760603271667, "tmdate": 1762915508518, "mdate": 1762915508518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the challenges posed by missing data, the authors introduce the CRIB. CRIB integrates a unified variable attention mechanism with a consistency regularization scheme. Comprehensive experiments conducted on four real-world datasets demonstrate the effectiveness of CRIB, which achieves accurate predictions even under high missing rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The article is well-written.\n2. Prediction with missing values ​​is a significant issue.\n3. The existing experiments demonstrate the model's excellent performance."}, "weaknesses": {"value": "1. The motivation has already been explored. To my knowledge, the limitations of the \"impute-then-predict\" paradigm have been repeatedly discussed [1-3]. Additionally, CSDI also addresses the lack of authenticity in missing values during training and proposes a self-supervised learning strategy. Therefore, the authors should present a more compelling motivation. Furthermore, some imputation works that do not explicitly emphasize their applicability to prediction tasks (e.g., TimeNet, TimeMixer++) and some prediction works capable of directly handling missing values should be considered as competitors in this paper and discussed and compared in detail. Otherwise, this approach may lead to an overestimation of the paper's contributions.\n\n2. The \"impute-then-predict\" approach is not the mainstream framework adhered to by existing methods. Some datasets naturally contain missing values, and many prediction works do not perform an additional imputation step—they directly make predictions. The \"impute-then-predict\" paradigm generally involves simulating missing data through random masking, which is more akin to a self-supervised learning approach.\n\n3. The paper selects a relatively weak-performing model (DLinear) (which only consists of a few fully connected layers) and a heterogeneous model (TimesNet) to construct the \"impute-then-predict\" paradigm and uses them as the primary targets of criticism. The rationale for this motivation is insufficient. This experimental setup appears more like a \"fragile\" control group constructed to highlight the advantages of the proposed method, rather than a fair evaluation of the state-of-the-art or most commonly used paradigms in the real world. A more convincing analysis should directly compare with specialized, powerful imputation models (e.g., CSDI, ImputeFormer) or advanced prediction models capable of directly handling missing values (e.g., appropriate variants of D2STGNN, TimesNet), and delve into the potential limitations of these methods.\n\n4. Furthermore, the authors' first contribution claims: \"We perform a systematic empirical analysis of the dominant imputation-then-prediction paradigm for MTSF-M. We reveal that, guided only by a prediction objective, imputation modules can corrupt the observed data distribution and degrade prediction performance\" seems exaggerated. The authors use a toy example to support their research, which is not systematic or concrete. Reliable findings should be based on either theoretical foundations or extensive experiments.\n\n5. What challenges would other models face if performing direct prediction? Regarding these challenges, what specific strategies has your model implemented?\n\n6. Although the authors introduce the information bottleneck theory, what is the utility of the lower bound proven in Section 3.4.2? In my view, if it's just an isolated value, it's difficult to grasp its meaning. For instance, the authors should introduce the lower/upper bounds of other methods for comparison to illustrate the significance of this bound.\n\n7. Several imputation models are missing, such as PriSTI, CSDI, and ImputeFormer. Additionally, some spatiotemporal forecasting models are missing, e.g., D2STGNN,STID,PatchSTG. Some time series models are also missing, such as DUET, CycleNet, and TimesNet. Some commonly used time series data sets, such as Weather, Traffic, etc., need to be considered.\n\n8. Furthermore, I recommend denormalizing all prediction results; this is standard practice for PeMS and Metr-LA datasets.\n\n9. I suggest adding MAPE as a third metric.\n\n10. The work appears to be a combination of mature components. The authors' temporal analysis model is a standard Transformer, and the theory is derived from the well-established mutual information/variational inference theory. Therefore, it seems more like an engineering effort of combination rather than an original contribution.\n\n11. Did the authors evaluate the performance of an ablated variant without the Consistency Regularization term? This is necessary to demonstrate that the performance gain indeed stems from this module.\n\n12. The authors' loss function involves three hyperparameters, which could clearly lead to optimization difficulties. Therefore, a detailed sensitivity analysis is required. Figure 4 alone is insufficient.\n\n13. The code cannot be run directly. It would be better if you can add more detailed instructions for using the code.\n\nReference:\n\n[1] Tashiro, Yusuke, et al. \"Csdi: Conditional score-based diffusion models for probabilistic time series imputation.\" Advances in neural information processing systems 34 (2021): 24804-24816.\n\n[2] Peng, J., Yang, M., Zhang, Q., & Li, X. (2025). S4M: S4 for multivariate time series forecasting with Missing values. arXiv preprint arXiv:2503.00900.\n\n[3] Chen, X., Li, X., Liu, B., & Li, Z. (2023). Biased temporal convolution graph network for time series forecasting with missing values. In The Twelfth International Conference on Learning Representations."}, "questions": {"value": "W1-W13."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MLRvR7R2y1", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_2Cic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_2Cic"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467744371, "cdate": 1761467744371, "tmdate": 1762915508303, "mdate": 1762915508303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CRIB, a novel method for Multivariate Time Series Forecasting with Missing Values, demonstrating state-of-the-art performance across four diverse datasets in an extensive experimental evaluation. The work addresses a critically important real-world challenge. However, several significant limitations temper these strengths. The practical relevance is undermined as experiments use artificially induced missing data on complete, highly periodic datasets, failing to test on real-world, non-stationary, or high-dimensional data. The problem setup's generalizability is weakened by an imputation strategy that ignores realistic missing mechanisms (e.g., MNAR). Furthermore, the core innovation of unified-univariate attention lacks theoretical grounding, and claims of superiority over other models remain unconvincing without deeper analysis into why they fail. Consequently, while CRIB shows promising results, its validity and applicability to genuine real-world scenarios are questionable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper addresses the increasingly important problem of Multivariate Time Series Forecasting with Missing values, a critical challenge in real-world applications such as healthcare, environmental monitoring, and industrial IoT, where data completeness cannot be assumed.\n\nS2. The experimental section is extensive and methodologically sound, covering four diverse datasets\n\nS3. The proposed method, CRIB, achieves state-of-the-art performance across all datasets and metrics"}, "weaknesses": {"value": "W1. The paper claims to address real-world scenarios where ground-truth values are unavailable. However, the two-stage imputation strategy—training TimesNet on 10% missing data and applying it to higher missing rates—does not reflect real-world missing mechanisms (e.g., MNAR due to sensor failure or human behavior). The absence of discussion on missing mechanisms (MCAR/MAR/MNAR) weakens the practical motivation and limits the generalizability of the problem setup.\n\nW2. The paper asserts superiority over existing methods but fails to explain why models like SAITS or BiTGraph degrade under high missing rates. A deeper analysis—e.g., error decomposition, attention pattern visualization, or gradient sensitivity—would strengthen the motivation. Without such analysis, the claim that \"existing methods fail\" remains superficial and unconvincing.\n\nW3. The unified-univariate attention is a core innovation, yet the paper offers no theoretical analysis of its properties—e.g., representational capacity, gradient flow, or robustness to missing data. Why is this structure more suitable for MTSF-M than standard self-attention or cross-variate attention? Without such grounding, the method appears heuristic rather than principled.\n\nW4. Experiments are conducted on only four datasets: PEMS-BAY, Metr-LA, ETTh1, and Electricity. These are all highly periodic, low-noise, relatively low-dimensional and . The lack of testing on non-stationary, high-dimensional (e.g., >1000 variates), or irregularly sampled datasets limits the generalizability of the results and raises concerns about overfitting to specific data characteristics\n\nW5. The experimental evaluation is conducted on only four datasets. While these are commonly used in time series forecasting, they share highly similar characteristics: strong periodicity, low noise, regular sampling, and relatively low dimensionality (tens to hundreds of variables). Crucially, none of these datasets contain real missing values. Instead, the authors artificially inject missing data under controlled patterns (point, block, column), which significantly weakens the practical relevance of the study."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LZSyabF4K4", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_ek3n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_ek3n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951056139, "cdate": 1761951056139, "tmdate": 1762915508165, "mdate": 1762915508165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes that imputing missing values may be inaccurate, which can affect the accuracy of time simulation and thereby harm the precision of future predictions. To mitigate the errors brought by imputation, the paper further proposes a novel framework called CRIB, constructed based on the information bottleneck principle. CRIB combines a unified variable attention mechanism with a consistency regularization scheme to learn robust representations, filter out noise introduced by missing values, and meanwhile preserve the fundamental predictive signals. The experimental results of the paper show that the proposed method outperforms existing classic baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes that imputing missing values may be inaccurate, which can affect the accuracy of time simulation and thereby harm the precision of future predictions. To mitigate the errors brought by imputation, the paper further proposes a novel framework called CRIB, constructed based on the information bottleneck principle. CRIB combines a unified variable attention mechanism with a consistency regularization scheme to learn robust representations, filter out noise introduced by missing values, and meanwhile preserve the fundamental predictive signals. The experimental results of the paper show that the proposed method outperforms existing classic baselines."}, "weaknesses": {"value": "1. The experiments in this paper are unreasonable. From the source code, this paper fills the missing value part with 0, which is actually imputing the data with a value of 0, severely disrupting the original temporal patterns. In the subsequent forward and backward propagation processes, the filled 0 values are involved in the calculations. This will seriously affect the model's optimization and lead to training failure. This may also be why the baselines in Figure 1 perform poorly, as they are trained under conditions where the data patterns are severely disrupted (real values are randomly replaced with 0). It may be that even simple non-model cubic function interpolation would perform better than the models trained in Figure 1. Missing value parts should not be involved in calculations. They should be handled using masks during the network computation process, or using a process like neural CDE for processing, instead of filling the missing parts with zero at the input stage.\n\n2. Due to the unreasonable handling of missing values, many of the modules proposed in the paper are unreliable. For example, the KL divergence under IB theory and the consistency constraints under noise are closely related to setting missing values to 0 for participation in forward and backward propagation, but this is meaningless. For example, random noise will replace the 0 value positions with more meaningful values, and the effectiveness may be because the 0 values are replaced.\n\n3. Although the logic of using INFORMATION BOTTLENECK GUIDANCE in the paper is reasonable, the approach is confusing. Calculating the Gaussian posterior based on features and making it close to the standard Gaussian prior with the KL divergence seems to have no supervisory information. Why can this minimize mutual information and filter out irrelevant information?\n\n4. Some parts of the paper are vaguely described. W/o IB means removing the compactness and informativeness guidance of IB. The former is Eq. 9, but what is the latter? The two are mixed in the ablation study, and the effect of removing Eq. 9 alone is not shown.\n\n5. The paper lacks key baselines. For example, baselines for handling irregular and missing data, such as Neural ODE or CDE.\n\n6. The paper flattens all variable patches to calculate attention, which is not a very efficient or reasonable approach. When the number of variables is large, e.g., a traffic dataset, this method will become infeasible due to the overly long patch sequence.\n\n7. The experiments in the paper are not solid. The main experiments are only conducted on four datasets, and the ablation studies are only performed on one dataset."}, "questions": {"value": "Although the logic of using INFORMATION BOTTLENECK GUIDANCE in the paper is reasonable, the approach is confusing. Calculating the Gaussian posterior based on features and making it close to the standard Gaussian prior with the KL divergence seems to have no supervisory information. Why can this minimize mutual information and filter out irrelevant information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "quHX0J6Hwb", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_K95D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_K95D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991196327, "cdate": 1761991196327, "tmdate": 1762915507974, "mdate": 1762915507974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the critical task of Multivariate Time Series Forecasting with Missing Values (MTSF-M), where missing values are prevalent in real-world scenarios. It first points out the flaw of the current mainstream \"imputation-then-prediction\" framework: since there is no ground truth for missing values, unsupervised imputation is prone to errors, which corrupts the underlying data distribution and degrades forecasting accuracy—a conclusion validated by the authors’ systematic empirical study. To address this issue, the paper proposes a paradigm shift: abandoning independent imputation and directly forecasting from partially observed time series."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a critical limitation of the mainstream \"imputation-then-prediction\" framework in MTSF-M—unsupervised imputation’s lack of ground truth leading to data distribution corruption and forecasting accuracy degradation—and validates this via systematic empirical studies, which provides a clear and necessary critique of existing methods. Additionally, the proposed paradigm shift, direct forecasting from partially observed data, and the CRIB framework  demonstrate theoretical innovation."}, "weaknesses": {"value": "The paper’s central claim—shifting from “imputation-then-prediction” to direct forecasting via an IB-based framework—is conceptually relevant but lacks sufficient novelty to meet ICLR standards. The critique of imputation-induced distribution corruption is not fully original. Prior works have already questioned the imputation paradigm and proposed direct prediction methods, yet these are not adequately discussed or contrasted. The proposed “CRIB” framework combines two existing components—Information Bottleneck (IB) and consistency regularization—without demonstrating a non-trivial, innovative integration. The paper fails to explain how CRIB’s design solves unique challenges that prior IB-based or consistency-regularized models cannot address. The paper only compares against “imputation-then-prediction” methods but omits state-of-the-art direct forecasting baselines This one-sided comparison overstates CRIB’s performance."}, "questions": {"value": "The paper only claims CRIB is effective but does not compare it to SOTA MTSF-M models. Could you add a head-to-head comparison with recent methods?\nThe paper references \"four real-world datasets\" but provides no context to assess generalizability. Please list the dataset names, domains, time series length, and number of variables for each dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3ZvE0JAoJt", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_g59r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_g59r"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999431099, "cdate": 1761999431099, "tmdate": 1762915507861, "mdate": 1762915507861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new paradigm for time-series forecasting with missing data based on the Information Bottleneck Principle. They argue that current forecasting with missing data tends to use imputation modules to recover the missing data, then do the forecasting. This is claimed to be flawed because of the lack of ground truth for missing data, which can corrupt the observed data and hurt the performance. As an alternative approach, they (i) propose an architecture that handles the sparse nature of the observed data through patching embedding and unified-variate attention, and (ii) include the consistency regularization loss additionally to the usual reconstruction and compactness losses, along with data augmentation to learn invariance to missing patterns. They show that their scheme provides strong MAE and MSE performance on four datasets against twelve baselines with four missing rates settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper achieves strong performance against the baselines (Table 1), and multiple ablations that show the importance of the different modules (Table 2 and Figure 4), including the Consistency loss and the Unified-Variate Attention. In particular, Figure 4 (a) shows the importance of (i) IB (ii) Consistency loss, and (iii) Uni-Attn, and Figure 4 (b) shows the robustness to hyperparameters. Also, Table 2 shows that CRST-IB can perform significantly better than w/o for well-chosen consistency regularization weights."}, "weaknesses": {"value": "There is too much information in Figure 1: it is hard to read. You should simplify it and put your method on the right. It would also be nice to add more qualitative comparison with the competitors. Although it is said in the Related Work section, it should be made clearer in the paper that Compactness and Informativeness losses already exist and are not part of the novelty. This needs to be clearer both in 3.4.1, 3.4.2, and precise references of previous work in the proofs in Appendix B. \n\nAlso, the justification of the consistency loss, e.g with \"The core intuition is that the model’s prediction should be invariant to the missingness.\" is not clear. Can you better explain that?\n\nIssues/typos:\n\nL.42: $\\\\beta \\in \\\\mathbb{R}^{+}_{*}$\n\nEq.3: Shouldn't it be $\\sin(t/10000^{2m/P})$ instead (and same for cos(...)) ? Otherwise, the embedding doesn’t depend on $m$ when the parity is fixed.\n\nL.277-282: \n\nSign issue in Eq.10\n\n$\\\\mathbb{E}\\_{p(z, y)}[\\log q_\\\\theta(y \\mid z)]= \\textcolor{red}{-} \\mathbb{E}\\_{p(z, y)}[\\frac{1}{2 \\\\sigma^2}\\||y-\\\\hat{y}\\||^2+\\frac{T}{2} \\\\log \\left(2 \\\\pi \\\\sigma^2\\right)]$\n\nFigures 3 and Figure 4a: It should be better to have consistency in color for the method (e.g., yellow)."}, "questions": {"value": "See weaknesses, also:\n\nWhat are the main limitations of the method ? Is that the amount of hyperparameters ? How did you choose these ($\\\\alpha, \\\\beta, \\\\gamma$, 10% in the additional random masking, variance of the added Gassian noise) and what are the chosen hyperparams for the Figure 4a ? Limitations should be acknowledged in a separated section.\n\nAre the competitors comparable in terms of computational cost ? Did you retrain the baselines from scratch ?\n\nIt would be nice to add the model trained with clean data as well (missing rate = 0) to get a bound on the achievable performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9JiZiiRPbS", "forum": "63VN3K4gYx", "replyto": "63VN3K4gYx", "signatures": ["ICLR.cc/2026/Conference/Submission383/Reviewer_bbCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission383/Reviewer_bbCN"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762192157642, "cdate": 1762192157642, "tmdate": 1762915507745, "mdate": 1762915507745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}