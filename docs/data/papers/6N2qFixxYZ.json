{"id": "6N2qFixxYZ", "number": 12473, "cdate": 1758208061299, "mdate": 1759897507764, "content": {"title": "DES-LOC: Desynced Low Communication Adaptive Optimizers for Foundation Models", "abstract": "Scaling foundation model training with Distributed Data Parallel~(DDP) methods is bandwidth-limited.\nExisting infrequent communication methods like Local SGD were designed to synchronize model parameters only and cannot be trivially applied to adaptive optimizers due to additional optimizer states.\nHeuristic approaches that keep states local or reset them lack guarantees and can be unstable in compute‑efficient batch regimes; conversely, Local Adam synchronizes all states uniformly and is provably convergent but triples communication costs.\nWe propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of optimizers assigning independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence. Our theoretical analysis shows that while parameter synchronization dominates the asymptotic rate in-expectation, high-probability convergence guarantees require at least infrequent synchronization of the second momentum. Furthermore, we prove that more frequent momentum sync permits larger stable step sizes. Experiments on language models of up to 1.7B show that DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local Adam, enabling 1.3x–2.1x wall‑clock speedups over DDP for 1-13B models on 100Gb/s links. Furthermore, unlike previous heuristic methods, DES-LOC is robust to worker failures offering a scalable, efficient, and fault-tolerant solution for foundation model training.", "tldr": "We propose provably convergent local adaptive optimizers with decoupled sync frequencies, empirically reducing communication 170x vs. DDP and 2x v s. Local Adam (prior SOTA), reducing time by 1.3x-2.1x , validated up-to billion-scale models.", "keywords": ["Distributed Training", "Foundation Models", "Large Language Models", "Optimizers", "Communication Efficiency", "Federated Learning", "Distributed Systems", "Optimization Theory", "Scaling", "Robustness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f41043d82d247c4897c7e3d58315cc80627e917.pdf", "supplementary_material": "/attachment/e94fa9be3f5b37e8b75c08012cd0e7977bc18a26.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DES-LOC, a family of desynced low-communication adaptive optimizers that assign independent synchronization periods to model parameters and optimizer momenta (first/second). Theoretical analysis shows (i) convergence for SGDM in non-convex settings and for Adam in weakly convex settings, and (ii) that more frequent momentum sync allows larger stable step sizes, while high-probability bounds require at least infrequent second-moment sync. Empirically (GPT-style LMs up to 1.7B parameters), DES-LOC achieves 170× less communication vs DDP and 2× less vs Local Adam, with 1.3–2.1× wall-clock speedups (aided by a wall-clock model for larger scales) and shows robustness to worker failures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality. Decoupling sync periods for parameters and momenta, grounded in a half-life/step-size argument, moves beyond Local Adam’s uniform sync and earlier heuristics that lacked guarantees or failed under failures.\n2. Quality. Convergence under standard assumptions with a clear theorem for SGDM and high-probability Adam bounds. Analysis explains why more frequent momentum sync permits larger stable steps and why $\\beta_2$ cannot be left unsynced indefinitely.\n3. Quality (empirics). Solid experimental design with explicit RQs, ablations over $K_x, K_u, K_v$ and comparisons showing DES-LOC matches Local Adam’s perplexity with 2× fewer state communications and drastically less than DDP.\n4. Clarity. Clear algorithm block and figures.\n5. Significance. Communication reductions (170× vs DDP; 2× vs Local Adam) and modeled wall-clock speedups directly target a primary FM training bottleneck (bandwidth), with fault-tolerance a notable practical plus."}, "weaknesses": {"value": "There is no evidence for a larger-scale model."}, "questions": {"value": "Could you add a $\\ge$7B measured run?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XRVY1Gme51", "forum": "6N2qFixxYZ", "replyto": "6N2qFixxYZ", "signatures": ["ICLR.cc/2026/Conference/Submission12473/Reviewer_JkEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12473/Reviewer_JkEm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883365770, "cdate": 1761883365770, "tmdate": 1762923350354, "mdate": 1762923350354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DES-LOC, an optimization algorithm designed for decentralized training of foundation models, with a particular focus on large language models. The main objective is to improve communication efficiency during large-scale training in order to reduce wall-clock time.\nThe proposed method builds upon existing approaches such as FedAvg, FedOpt, and more specifically LocalAdam, where workers perform local parameter updates before averaging the weights with a central server. This approach contrasts with classical Distributed Data Parallel (DDP) methods, where synchronization occurs at every training step.\nDES-LOC modifies the LocalAdam algorithm by decoupling the synchronization frequencies of the parameters and the optimizer states. While LocalAdam synchronizes both every K steps, this can lead to unnecessary communication overhead. The authors argue, and empirically demonstrate, that the first and second moment estimates (i.e., the momentum terms) evolve more slowly than the model parameters themselves, allowing for less frequent synchronization of these states.\nIn practice, the authors recommend using default ratios of Ku = 3Kx and Kv = 6Kx, or alternatively determining these based on the half-lives of β₁ and β₂. The paper also provides a convergence guarantee under standard assumptions commonly accepted in the optimization community.\nExtensive experiments on LLM training show that DES-LOC achieves performance comparable to LocalAdam while reducing communication costs by approximately half. Overall, the paper presents a well-motivated and empirically validated contribution to improving efficiency in decentralized large-scale model training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated, and the overall writing is clear and easy to follow. The analysis based on the half-life times provides useful intuition for why less frequent parameter updates do not negatively affect model training.\n- I particularly appreciated the qualitative analysis of the upper bound on model convergence (Theorem 1), which effectively shows that DES-LOC enables the use of a larger learning rate in practice.\n- The authors’ claims are well supported by extensive experiments, including analyses of rate of change, convergence behavior in practical LLM training scenarios, and full model training comparisons (in wall-clock days) with DDP and other baselines, which are especially compelling."}, "weaknesses": {"value": "- While the method is supported by both theoretical analysis and experimental results, its practical novelty is somewhat limited. The main contribution lies in decoupling parameter updates from optimizer updates, which constitutes a relatively minor modification of the existing LocalAdam algorithm.\n- The authors did not conduct experiments under very low-bandwidth conditions. Demonstrating the method’s effectiveness in scenarios with limited network capacity (e.g., 1 Gb/s or 10 Gb/s) would further strengthen its practical relevance.\n- The title of the paper refers to “foundation models,” but the large-scale experiments focus exclusively on LLM training. Although these results are already impressive, the authors could either adjust the paper’s scope to explicitly center on LLMs or include an additional experiment involving the training of a vision foundation model to better match the stated scope."}, "questions": {"value": "- What would be the practical relevance of DES-LOC for training scenarios where workers are connected through significantly lower-bandwidth networks?\n- Could the proposed desynchronization strategy also be applied to optimizers that use only a single momentum term (e.g., Muon)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tgRsUryqVs", "forum": "6N2qFixxYZ", "replyto": "6N2qFixxYZ", "signatures": ["ICLR.cc/2026/Conference/Submission12473/Reviewer_cfsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12473/Reviewer_cfsD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990808899, "cdate": 1761990808899, "tmdate": 1762923349417, "mdate": 1762923349417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose DES-LOC, a communication-efficient optimization algorithm that reduces the frequency of optimizer state communication compared to local Adam. The authors prove the convergence of DES-LOC in the non-convex case (for SGDM) and for Adam under weakly convex objectives. The author’s key insight over Local Adam is that the first and second moments can be synchronized at a rate on the order of their half-lives, much less frequently than their parameters need to be synchronized. In experiments pre-training language models, the authors show that significantly less than DDP and up to 2x less than Local Adam while achieving similar final performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to understand. \n- I think experimental settings are well structured.\n- From the math in the main paper, the convergence guarantees look sound (I did not check the appendix). \n- The idea of synchronizing optimizer states relative to their decay half-lives is intuitive to me, and I think it's an interesting direction of inquiry for the distributed optimization community."}, "weaknesses": {"value": "- My main concern is the lack of comparison to existing methods such as DiLoCo [1] and MuLoCo [2]. Neither method requires synchronizing the optimizer states; therefore, they are trivially more communication-efficient than DES-LOC and have been shown to perform competitively to DDP in practice. I am aware of your experiments in Figure 6 showing a DES-LOC method with Nesterov momentum, but from my understanding, these still synchronize optimizer states. \n- Following from my concern above, since the communication efficiency benefits of DES-LOC are already realized by proposed methods in the literature [1,2], I’m not sure how strong the contribution is. Could the authors comment on this?\n- I wonder if synchronizing the optimizer states in DES-LOC can provide a benefit beyond [1,2]. I would be surprised if it does not, and I believe experiments showing this would greatly strengthen your results.\n\n\n[1][DiLoCo: Distributed Low-Communication Training of Language Models]\n\n[2][MuLoCo: Muon is a practical inner optimizer for DiLoCo]"}, "questions": {"value": "- Are the comparisons to the DDP baseline FLOP-matched?\n- Figure 3 (a), (b), (d) have the same rectangular pattern in the loss curve. Why is this the case?\n\n**Suggestions**:\n- I have trouble scrolling through your .pdf because the figures take long to load. Perhaps subsampling the point used for plotting can help.\n- I have trouble distinguishing different loss curves in your plots. Perhaps smoothing could help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P9N2NpsCFJ", "forum": "6N2qFixxYZ", "replyto": "6N2qFixxYZ", "signatures": ["ICLR.cc/2026/Conference/Submission12473/Reviewer_KN8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12473/Reviewer_KN8y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762216769684, "cdate": 1762216769684, "tmdate": 1762923348890, "mdate": 1762923348890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical argument to synchronize local optimizer states (first and second moments) for fedavg/diloco. Specifically, it shows local momentum needs to be synced; however, it can be infrequent wrt outer updates, and this frequency depends on the momentum coefficients. It improves over vanilla diloco and reduces communication wrt syncing states in every outerstep."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The theory-backed argument to have infrequent momentum synchronization is insightful and makes sense. The connection to beta (momentum coefficient) and the synchronization frequency is intuitive. \n2. The results show the benefit of synchronizing momemtum terms with two optimizers and show no degradation wrt to local Adam.\n3. Writing is very clear, and the toy example illustrated the need for momentum synchronization."}, "weaknesses": {"value": "1. The main theorem is for SGD+momentum, however, the method is proposed for adaptive optimizers. This is a limitation as adaptive optimisers are popular and the frequency of synchronization is derived from the theory for SGDM and applied to adam/adopt.\n2. The contribution over local Adam is marginal as local Adam already established that optimizer states needs to be synchronized and also connected the convergence bound to momentum coefficients. Please clarify the main contributions.\n3. Even with momentum sync, the results are worse than DDP. This raises a question of whether diloco-type methods (weight averaging method) would match DDP (gradient averaging)?"}, "questions": {"value": "1. The method doesn't seem specific to adaptive methods. Would it be better to frame it more generally? Would it work with muon-type optimizers?\n2. Is it correct to say local SGD (ie, fedavg/diloco) is sharing only model parameters? It actually shared the parameter differences (pseudo gradients) and uses them with an outer optimizer. Would it make sense to frame the method as parameter-sharing methods need optimizer states to be synced rather than relating them to diloco type methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w2QOrBklOV", "forum": "6N2qFixxYZ", "replyto": "6N2qFixxYZ", "signatures": ["ICLR.cc/2026/Conference/Submission12473/Reviewer_26GM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12473/Reviewer_26GM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219414422, "cdate": 1762219414422, "tmdate": 1762923348480, "mdate": 1762923348480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}