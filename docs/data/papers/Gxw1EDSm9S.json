{"id": "Gxw1EDSm9S", "number": 23143, "cdate": 1758340157192, "mdate": 1759896830439, "content": {"title": "Auto-SWE-Bench: A Framework for the Scalable Generation of Software Engineering Benchmark from Open-Source Repositories", "abstract": "Benchmarks like SWE-bench have shaped the evaluation of Large Language Models (LLMs) on complex software engineering tasks, although such efforts remain limited due to manual curation, static datasets, and narrow coverage of issue types. We introduce SWE-Bench Atlas, a fully automated framework for generating high-fidelity, repository-level coding tasks from open-source GitHub projects. Unlike synthetic benchmarks such as SWE-Smith, SWE-Bench Atlas draws directly from SOTA-model-breaking pull requests and encompasses diverse issue types, capturing a broader spectrum of real-world software engineering demands. The framework is structured as a pipeline of six automated components: (1) a sourcing module that broadly identifies pull requests with high test coverage and complexity; (2) an agentic Dockerization system that guarantees reproducibility of historically accurate software environment for every task; (3) a “state-based” test classification with automatic log parsing for more granular test feedback; (4) a curation module to repair or augment tasks with low test alignment; (5) an annotation module to guarantee issue clarity and test-to-issue alignment; and (6) a trajectory curation system leveraging human-in-the-loop refinement to resolve SOTA-model-breaking issues. Our initial private  benchmark consists of 5,909 instances from 3,154 repositories across 7 languages. On a subset of 488 instances of this benchmark, today’s strongest models perform as follows: gpt-5-2025-08-07 (24.34% pass@1), claude-opus-4-20250514 (18.46%), gemini/gemini-2.5-pro (9.53%), and gpt-4o (3.65%). Supplementing our findings, a public release of the 488-task subset is soon to follow. We also demonstrate proof-of-value of our data by showing fine-tuning based improvements on the SWE-bench multilingual benchmark. By producing dynamic, diverse challenging tasks across programming languages and repositories, SWE-Bench Atlas enables scalable evaluation and advancement of AI coding and reasoning abilities of next-generation AI systems.", "tldr": "", "keywords": ["Benchmark", "SWE Bench", "Evaluation of Large Language Model", "Coding Benchmarks", "Coding Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b67216e9b1a02e3d5dc676f8d3928fe8f2067e24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presented SWE-Bench Atlas, an automated framework for generating scalable, multilingual software engineering benchmarks from GitHub pull requests. It uses a six-stage pipeline to ensure reproducibility and quality, producing challenging tasks for evaluating LLMs. The tool addressed limitations of manual curation and demonstrates value for both evaluation and model fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ focus on a practical task\n+ the framework is well-structured"}, "weaknesses": {"value": "1. lack of novelty:\n\nThe paper’s contributions are primarily integrative rather than innovative. Many of the components, i.e., such as LLM-powered Dockerization, log parsing, and quality scoring, build on existing ideas and tools (e.g., SWE-Agent, SetUpAgent, LLM-as-a-judge). While the combination of these elements is new, the individual techniques are not. \n\n2.  Overemphasis on engineering:\n\nThis work is a strong engineering effort that addresses practical bottlenecks in benchmark creation. However, it does not propose new scientific ideas or evaluation frameworks. It is more akin to a tooling paper or system demo than a core research contribution.\n\n3. Lack data quality assurance:\n\n SWE-Bench Atlas's automated approach, while scalable, may not fully overcome the fundamental data quality issues that manual curation (like in SWE-bench Verified) was designed to address."}, "questions": {"value": "1. How to ensure the data quality when using SWE-Bench Atlas to collect new data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IPKRAfcKka", "forum": "Gxw1EDSm9S", "replyto": "Gxw1EDSm9S", "signatures": ["ICLR.cc/2026/Conference/Submission23143/Reviewer_24mQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23143/Reviewer_24mQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666961648, "cdate": 1761666961648, "tmdate": 1762942530937, "mdate": 1762942530937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **SWE-Bench Atlas**, a fully automated framework for generating high-fidelity, repository-level coding tasks from open-source GitHub projects. It builds a complete pipeline through six automated modules: the **Sourcing Module**, **Agentic Dockerization System**, **State-Based Test Classification**, **Curation Module**, **Annotation Module**, and **Trajectory Curation System**. Experiments show that its initial validation set contains **5,909 tasks from 3,154 repositories**. On this benchmark, **GPT-5 (2025-08-07)** achieves a **24.34% pass@1**, significantly outperforming other models, demonstrating the benchmark’s difficulty and discriminative power. Fine-tuning experiments on the multilingual SWE-bench benchmark further verify the value of the generated data. Overall, the paper’s objective is practically meaningful; however, it suffers from **limited novelty, missing methodological details, insufficient experimental validation, and noticeable writing quality issues**"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a practical and fully automated framework for generating high-fidelity, repository-level coding tasks from open-source GitHub projects, addressing key challenges in evaluating large language models for software engineering.\n\n2. In terms of data scale and coverage, it includes **5,909 tasks from 3,154 repositories**, and fine-tuning results demonstrate the value of the generated trajectories."}, "weaknesses": {"value": "**Low Novelty:** The main improvement of Auto-SWE-Bench lies in automating SWE-Smith’s process, with only minor modifications. The work lacks substantial innovation. \n\n**Lack of Methodological Details:** The authors did not disclose critical experimental details, such as the prompt templates used at each stage and fine-tuning parameter configurations. \n\n**Unclear Writing and Presentation:** The paper lacks clear illustrations or diagrams. Some experimental results should be presented in tables for clarity, making the paper difficult to follow. \n\n**Lack of Baseline and Model Comparison:** In Table 1, the authors did not include systematic comparisons with strong baselines such as SWE-Fixer-72B, SWE-Gym-32B, and SWE-Agent-LM-32B. \n\n**Lack of Ablation Studies:** The paper does not include systematic ablation experiments, for example, on the *Self-Correcting Iterative Refinement Loop* and related components. \n\n**Lack of Task Characteristics and Cost Analysis:** The paper does not report the structural distribution or difficulty characteristics of task instances (e.g., #Lines edited, #Files edited, #Functions edited). It also lacks cost analysis, including the computational resources required at each stage. \n\n**Writing and Formatting Errors:** There is a typo at line 126, missing citations at lines 298 and 304, and the layout of Table 2 is not well formatted. Also, the titles of paper are inconsistent."}, "questions": {"value": "1.Could the authors further clarify the conceptual and methodological differences between **SWE-Bench Atlas** and **SWE-Smith**, beyond the automation improvements? \n\n2.Please provide a detailed **cost analysis** of the entire automation process, including computational resource usage and time overhead at each stage. It is also recommended to include **prompt templates**, parameter configurations.\n\n3.comparisons with baselines such as **SWE-Fixer-72B**, **SWE-Gym-32B**, and **SWE-Agent-LM-32B** to improve clarity and reproducibility. \n\n4.In **Section 3.1 (Stage 1: Programmatic Sourcing)**, the paper does not explicitly state that candidate tasks should be selected only when the corresponding GitHub issues are resolved **and** the commits modify test files in the repository — which would indicate that the user likely wrote or updated tests to verify the issue’s fix.\n\n5.Could the authors provide more detailed **case studies** and examples of **challenging cases**?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s4miOjrvA9", "forum": "Gxw1EDSm9S", "replyto": "Gxw1EDSm9S", "signatures": ["ICLR.cc/2026/Conference/Submission23143/Reviewer_AnxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23143/Reviewer_AnxX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755715956, "cdate": 1761755715956, "tmdate": 1762942530552, "mdate": 1762942530552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **SWE-Bench Atlas**, an automated framework for generating large-scale software-engineering benchmarks from open-source repositories. The system integrates six components—sourcing, agentic Dockerization, hybrid log parsing, automatic quality analysis, curation with contextual hints, and trajectory generation. It produces 5.9 K validated tasks from 3 K repositories across seven programming languages and evaluates state-of-the-art models on a 488-task subset. The goal is to provide a scalable, contamination-resistant, and continuously extendable benchmark for AI-based software-engineering agents."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important problem: the lack of scalable, dynamic SWE benchmarks beyond static datasets such as SWE-Bench and SWE-Bench Verified.\n- The proposed pipeline is conceptually comprehensive, covering sourcing, environment setup, and trajectory curation.\n- The agentic Dockerization and hybrid log-parsing components are practical and potentially reusable by future research.\n- The dataset scale and diversity are impressive, and the benchmark results confirm strong task difficulty and clear model separation."}, "weaknesses": {"value": "- **Presentation quality is poor.**\nThe paper reads like a technical report rather than a polished conference paper—verbose descriptions, inconsistent formatting, missing figures. This seriously hurts readability.\n\n- **Weak novelty.**\nMost components follow existing procedural automation ideas. The “agentic” framing is overstated; the system is largely a scripted pipeline rather than a genuine multi-agent process.\n\n- **Related work gap.** \nThe paper overlooks SWE-Flow (ICML 2025), an earlier work with a similar automated SWE data-generation pipeline. This omission undermines the claimed novelty."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HTsXx9DImT", "forum": "Gxw1EDSm9S", "replyto": "Gxw1EDSm9S", "signatures": ["ICLR.cc/2026/Conference/Submission23143/Reviewer_ks1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23143/Reviewer_ks1c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922779768, "cdate": 1761922779768, "tmdate": 1762942530185, "mdate": 1762942530185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-Bench Atlas, an automated framework for the large-scale generation of software engineering benchmark tasks from open-source GitHub projects. The authors claim the framework addresses limitations of existing benchmarks like SWE-bench—namely scalability, data contamination, diversity, and environment reproducibility—through a six-stage automated pipeline. Using this framework, they generated a dataset of 5,909 instances, evaluated leading LLMs on a 488-task subset, and conducted a fine-tuning experiment to show that the generated data improves model performance on other benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Significance of the Problem: The paper addresses a critical and timely research problem: the scarcity of scalable, high-fidelity, and contamination-free software engineering benchmarks for evaluating the code generation capabilities of Large Language Models.\n\n2. Substantial Engineering Effort: The authors demonstrate considerable technical skill through the development of a complex, multi-stage automated pipeline. The successful integration of diverse components—from data collection and environment creation to automated quality control—represents a significant engineering achievement.\n\n3. Potential Contribution to the Community: The commitment to release a public subset of 488 tasks is a valuable contribution. If realized, this dataset will serve as a novel and important evaluation resource for the research community."}, "weaknesses": {"value": "Despite its ambitious goals, the paper suffers from severe weaknesses in novelty, experimental validation, and presentation. Its contribution is thus highly limited and falls significantly short of the acceptance standards for a top-tier venue like ICLR.\n\n*   **Significant Lack of Novelty:** The paper's primary contribution is an engineering assembly of existing techniques rather than a fundamental methodological innovation.\n    *   **Agentic Dockerization:** The proposed method heavily overlaps with recent work (e.g., *SetUpAgent*) without clearly articulating its unique contributions or substantive advantages.\n    *   **LLM Judge:** Using LLMs for quality assessment is now a common paradigm. Its application here is a straightforward extension and lacks methodological novelty.\n    *   **Trajectory Curation:** This component appears to be a direct application of an existing tool (*SWE-Agent*) rather than novel research.\n    *   **Overall:** The work reads more like a complex engineering report than a research paper with significant scientific insight. It combines modules without demonstrating any synergistic effect (\"1+1>2\").\n\n*   **Insufficient and Poorly Presented Experiments:** The experimental validation is weak and fails to support the paper's claims.\n    *   **Unconvincing Fine-tuning Results:** The key results in Table 1, meant to demonstrate the data's value, show only marginal gains. These improvements are presented without any statistical significance analysis (e.g., confidence intervals or standard deviations), making them unconvincing.\n    *   **Superficial Benchmarking:** The results are limited to raw `pass@1` scores. The paper lacks any error analysis, discussion of failure modes, or performance breakdown across different task types, which is essential for a benchmark paper.\n    *   **Unsubstantiated Diversity Claims:** The claim of dataset \"diversity\" is supported only by the number of repositories. Deeper analysis of task complexity, code churn, or problem type distribution is critically missing.\n\n*   **Poor Writing Quality and Lack of Rigor:** The paper is difficult to follow and lacks scientific precision.\n    *   **Poor Readability:** The complete absence of figures or diagrams to illustrate the complex six-stage pipeline is unacceptable. It forces readers to guess the system's architecture from dense text.\n    *   **Overly Promotional Tone:** The manuscript is filled with grandiose claims (\"paradigm shift,\" \"holistic\") that are not substantiated by evidence.\n    *   **Poor Structure:** Critical information, including the main related work comparison (Table 2) and full leaderboards, is relegated to the appendix. This leaves the core arguments in the main paper incomplete and unsupported."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dtyAnz788S", "forum": "Gxw1EDSm9S", "replyto": "Gxw1EDSm9S", "signatures": ["ICLR.cc/2026/Conference/Submission23143/Reviewer_aZo6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23143/Reviewer_aZo6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003496512, "cdate": 1762003496512, "tmdate": 1762942529823, "mdate": 1762942529823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}