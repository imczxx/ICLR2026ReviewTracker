{"id": "YbwJ04H49a", "number": 7129, "cdate": 1758008798811, "mdate": 1763695455461, "content": {"title": "MULocBench: Beyond Code Benchmark for Multi-Type Software Issue Localization", "abstract": "Accurate project localization (e.g., files and functions) for issue resolution is a critical first step in software maintenance. \nHowever, existing benchmarks for issue localization, such as SWE-Bench and LocBench, are limited. \nThey focus predominantly on pull-request issues and code locations, ignoring other evidence and non-code files such as commits, comments, configurations, and documentation. \nTo address this gap, we introduce MULocBench, a comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects. \nComparing with existing benchmarks, MULocBench offers greater diversity in issue types, root causes, location scopes, and file types, providing a more realistic testbed for evaluation. \nUsing this benchmark, we assess the performance of state-of-the-art localization methods and five LLM-based prompting strategies. Our results reveal significant limitations in current techniques: even at the file level, performance metrics (Acc@5) remain below 40%. This underscores the challenge of generalizing to realistic, multi-faceted issue resolution. To enable future research on project localization for issue resolution, we publicly release MULocBench at https://huggingface.co/datasets/somethingone/MULocBench.", "tldr": "", "keywords": ["Issue Localization; LLM; SWE-Bench;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f08cf8ee419fd3f038fac6b7c1f2a47524150075.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents MULocBench, a new dataset for evaluating issue localization in software projects. The authors argue that prior benchmarks are too narrowly focused on code-only bug fixes. In contrast, their benchmark includes 1,100 issues from 46 Python projects, featuring a wider variety of issue types. The authors use this benchmark to evaluate several existing state-of-the-art localization tools as well as five simple LLM prompting strategies. Their main reported finding is that all evaluated methods perform poorly, suggesting that realistic issue localization remains a significant open challenge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does an excellent job of articulating the limitations of existing benchmarks. The argument for a more diverse and realistic benchmark that reflects real-world software engineering practices is clear, compelling, and well-supported. \n2. The paper is exceptionally well-written, clearly structured, and easy to follow. Figures and tables are informative and effectively communicate the key results and comparisons."}, "weaknesses": {"value": "1. While the proposed benchmark and the accompanying evaluation are valuable, the paper's contribution is primarily empirical rather than methodological. It does not introduce a new technique to address the challenges it highlights. This focus on benchmarking, while important, may limit the paper's methodological novelty.\n2. A major weakness is the lack of deep analysis into the failure modes of the evaluated models. The paper compellingly shows that current models fail, but it stops short of providing generalizable insights into why they fail. A qualitative analysis of specific failure cases would have been invaluable. For instance, are the failures due to long-context reasoning, misunderstanding of code semantics, or ambiguity in the issue descriptions? \nA useful point of comparison is the well-received SWE-bench (ICLR 2024). However, its contribution was strengthened by a large, realistic, challenging and novel benchmark. And addressing these challenges is poised to drive significant innovations at both the applied and fundamental levels. \nWithout such insights, it is difficult for ML researchers to draw actionable conclusions for developing better models. While this is crucial work, its primary audience might be at top ESE venues (e.g., ICSE, FSE, ASE), where such benchmark and task-definition papers are highly impactful. I am open to raising my score if the authors' rebuttal can convincingly address the major concerns outlined above."}, "questions": {"value": "1. Regarding the \"location clarity\" check mentioned around line 613: was this assessment performed manually by human annotators, or was an automated method (e.g., using an LLM) employed? \n2. To better understand the nature of the challenge, could the authors provide a qualitative analysis of a few representative failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s6v5juP336", "forum": "YbwJ04H49a", "replyto": "YbwJ04H49a", "signatures": ["ICLR.cc/2026/Conference/Submission7129/Reviewer_QzsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7129/Reviewer_QzsC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552773355, "cdate": 1761552773355, "tmdate": 1762919296199, "mdate": 1762919296199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MULocBench, a new benchmark for software issue localization. The authors argue that existing benchmarks, such as SWE-Bench and LocBench, are too narrowly focused on bug fixes resolved via pull requests. To address this limitation, they have constructed MULocBench, a diverse dataset of 1100 issues from 46 popular Python projects. The paper then reports on an extensive empirical study evaluating both state-of-the-art specialized tools and several LLM-based prompting strategies on this new benchmark. The central finding is that all evaluated methods perform poorly, with even the top-performing approaches failing to exceed 40% Acc@5 at the file level. This highlights a significant gap between current model capabilities and the demands of realistic software maintenance tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The primary contribution, the MULocBench dataset, is a significant asset for the community. The paper details a systematic methodology for data collection, filtering, and annotation. The public release of this diverse and realistic benchmark is a welcome contribution that will undoubtedly facilitate future research in this domain.\n2. The experiments are thorough and well-designed. The authors evaluate a wide range of modern techniques, from traditional IR-based methods to sophisticated LLM agents."}, "weaknesses": {"value": "The paper's primary weakness lies in its lack of analytical depth. While the low performance scores compellingly demonstrate that current models fail, the paper offers very little insight into why they fail. The analysis remains largely descriptive, functioning more as a report of performance metrics than a scientific investigation into the root causes of failure."}, "questions": {"value": "1. The decision to exclude line-level evaluation due to low accuracy (<1%) seems premature. This low performance is, in itself, a highly significant finding that underscores the difficulty of the task. Could you elaborate on why function-level localization is considered \"sufficient for developers,\"(Line 288) given that practical fixes often require pinpointing a specific line or a small block of code within a function?\n2. To ensure the benchmark is free from temporal data leakage (i.e., issues being part of the models' training data), could you provide more details on the date range (e.g., creation or resolution dates) for the issues included in MULocBench?\n3. The LLM landscape evolves rapidly. Have you considered evaluating more recent or capable models, such as Reasoning Model (Deepseek-R1, claude-thinking)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OVlImhBzde", "forum": "YbwJ04H49a", "replyto": "YbwJ04H49a", "signatures": ["ICLR.cc/2026/Conference/Submission7129/Reviewer_KfGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7129/Reviewer_KfGY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554051802, "cdate": 1761554051802, "tmdate": 1762919295823, "mdate": 1762919295823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors observe that existing localisation or SE benchmarks for LLMs restrict code localisation tasks to source-code locations, especially in the context of bug fixing. They remark that this does not cover the diversity of tasks in a software project, and propose to create a new benchmark that has a diversity of tasks and locations. They propose MULocBench and form this benchmark by mining issues from the top 50 most-starred GitHub projects. They filter to those issues that have clear resolutions and extract the location and associated metadata. Crucially, they allow location references to be Project Files, Runtime Files, configurations, user-authored files, etc.\nTo demonstrate the need for the new benchmark, the authors consider existing approaches validated on previous benchmarks, and re-evaluate them in terms of Acc(@1,@5), and P, R, F1(@inf?), showing that performance is lower at the file, class, and function levels.\nTo allow applying previous approaches, the result was restricted to the subset of the benchmark that targets source-code files.\nTo evaluate the full benchmark, the authors consider five settings: closed-book, project-structure, location-hint, and two pipelines, considering the last two settings. They find that performance differs by target file type, showing a previously unexplored dimension for fault localisation.\n\nMinor remark: Section 5 uses a potentially old name of the benchmark (PLocBench)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Diversity of issue types and target file types.\n- Benchmark is far from saturation under current approaches and a best-effort model for the full dataset.\n- Higher difficulty task given the heterogeneity of artefacts that have to be handled."}, "weaknesses": {"value": "- Limited to Python.\n- Unclear handling of issues closed with \"works as intended\" or similar null-reference issues.\n- Diverges from how fault localisation is defined in software engineering (this is countered by naming the new setting issue localization)."}, "questions": {"value": "- Q1: How are \"works as intended\" or similar null-reference issues handled? Do you expect a null response from the LLM? Do you expect the implied but not stated reference from documentation on how an API/function should behave? (Please note that I mean null-reference from the issue only; there may be an implied location given the context, for example, what function/API the issue discusses)\n- Q2: In software engineering, the line-level fault localisation is usually defined as \"within n lines of target\". Did you consider such a relaxation to enable line-level performance assessment? If you did, what is the performance at this granularity? (I expect the result to be similar to the function level, but curious, especially if it is different)\n- Q3: Per Figure 8, performance in any out-of-project setting degrades severely. Is this a training data availability issue, or do you assume there is a more fundamental reason? Can you speculate on this aspect? (The question is especially due to these new settings being unique to the issue localisation setting, as a generalisation of fault localisation/bug fixing)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8zcSUEOZgZ", "forum": "YbwJ04H49a", "replyto": "YbwJ04H49a", "signatures": ["ICLR.cc/2026/Conference/Submission7129/Reviewer_5shn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7129/Reviewer_5shn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974051068, "cdate": 1761974051068, "tmdate": 1762919295340, "mdate": 1762919295340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MULocBench, a comprehensive dataset designed to advance the evaluation of software issue localization methods by addressing the limitations of existing benchmarks. The dataset includes a diverse range of issue types, root causes, location scopes, and file types, encompassing non-code files such as commits, comments, configurations, and documentation. An evaluation of current localization methods and LLM-based prompting strategies reveals significant limitations, with performance metrics remaining below 40% even at the file level. The authors have made MULocBench publicly available to support future research in project localization for issue resolution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduction of a comprehensive dataset that addresses the limitations of existing benchmarks by including a diverse range of issue types, root causes, location scopes, and file types. \n2. Inclusion of non-code files such as commits, comments, configurations, and documentation, providing a more realistic testbed for evaluating localization methods.\n3. Public release of MULocBench, facilitating future research and development in project localization for issue resolution, with a close perspective to practical software maintenance."}, "weaknesses": {"value": "1. The dataset consists solely of issues from GitHub Python projects, which may limit its applicability to other programming languages or platforms.\n2. Performance metrics (Acc@5, F1) remain below 40% even at the file level, indicating that current methods may not be effective in real-world scenarios.\n3. The paper does not provide detailed information on the specific characteristics of the issues included in MULocBench, such as their complexity or the nature of the root causes."}, "questions": {"value": "1. What criteria were used to select the 46 GitHub Python projects included in MULocBench?\n2. What specific characteristics of the issues in MULocBench contribute to the challenges in localization, and how can these be mitigated?\n3. How do the authors plan to address the identified limitations in current localization methods in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mHgWbrDufo", "forum": "YbwJ04H49a", "replyto": "YbwJ04H49a", "signatures": ["ICLR.cc/2026/Conference/Submission7129/Reviewer_MvkM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7129/Reviewer_MvkM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997789715, "cdate": 1761997789715, "tmdate": 1762919294473, "mdate": 1762919294473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Thanks for all reviewers’  insightful comments. We have incorporated the rebuttal content into our revision to answer the reviewers’ questions. \n\nMULocBench aims to answer two fundamental questions: (1) What do real-world software issues actually look like? and (2) How well do current methods perform on such realistic issue-localization tasks? The contribution is as follows: \n\n* **Location Extraction Method**.  We propose a construction method to extract locations for issue resolution from pull requests, commits, and comments. \n* **Systematic Taxonomy**. We define a taxonomy for issue localization across four key dimensions: issue types, root causes, file scopes, and file types for a comprehensive understanding of real-world issues.\n* **Comparative analysis**. Our comparison with existing benchmarks reveals their limitations in in-project, code-only bug-fix datasets. In contrast, MULocBench captures the true complexity of realistic issues, including diverse issue types, heterogeneous file types, and out-of-project file scopes. \n* **Evaluation and Analysis**: We conduct an extensive evaluation of state-of-the-art localization methods and prompting-based LLMs. The results reveal consistently poor performance, provide insights into common failure modes, and highlight promising directions for future research.\n\nTo address reviewers’ suggestions, we have made the following modifications:\n\n* **Failure Analysis**. We have added failure analysis and directions to address the limitations in Section 7 and included five representative failed cases in Appendix A.11.\n\n*  **Extended Experiments**. We have added line-level evaluation experiment in Appendix A.7 , temporal analysis in Appendix A.8 , expanded cross-project analysis in Appendix A.10, and evaluation on DeepSeek-R1 in Sections 5 and 6.\n\n\n*  **Clarifications and Emphasis**. We have clarified the selection of the Python project, the distinction between issue localization and fault localization, and data construction details. We also emphasized that our work fundamentally expands the problem space of the issue localization, and provides a testbed for future research."}}, "id": "vpz8gaughP", "forum": "YbwJ04H49a", "replyto": "YbwJ04H49a", "signatures": ["ICLR.cc/2026/Conference/Submission7129/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7129/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission7129/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722339119, "cdate": 1763722339119, "tmdate": 1763722420309, "mdate": 1763722420309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}