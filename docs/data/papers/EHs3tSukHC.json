{"id": "EHs3tSukHC", "number": 4892, "cdate": 1757785427683, "mdate": 1759898006827, "content": {"title": "Off-Policy Safe Reinforcement Learning with Cost-Constrained Optimistic Exploration", "abstract": "When formulating safety as limits of cumulative cost, safe reinforcement learning (RL) learns policies that maximize rewards subject to these constraints during both data collection and deployment. While off-policy methods offer high sample efficiency, their application to safe RL faces substantial challenges from constraint violations caused by estimation bias in value functions and cost-agnostic exploration. To address these challenges, we propose Constrained Optimistic eXploration Q-learning (COX-Q), an off-policy primal-dual safe RL method that integrates cost-bounded exploration and conservative distributional RL. \nFirst, we introduce a novel cost-constrained optimistic exploration strategy that resolves gradient conflicts between reward and cost in the action space, and adaptively adjusts the trust region to control constraint violation in exploration. Second, we adopt truncated quantile critics to mitigate estimation bias. The quantile critics also quantify distributional, risk-sensitive epistemic uncertainty for guiding exploration. Experiments across velocity-constrained robot locomotion, safe navigation, and complex autonomous driving tasks demonstrate that COX-Q achieves high sample efficiency, competitive safety performance during evaluation, and controlled data collection cost in exploration. The results highlight the proposed method as a promising solution for safety-critical applications of RL agents.", "tldr": "An off-policy primal-dual safe reinforcemen learning approach with cost-bounded optimistic exploration.", "keywords": ["constrained reinforcement learning", "safe reinforcement learning", "safe exploration", "epistemic uncertainty quantification"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0146e0d23d3c0a3cfa9900b093b94998034be0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an off-policy primal–dual safe reinforcement learning (RL) algorithm named COX-Q, aiming to improve sample efficiency and reduce estimation bias. The method integrates a cost-constrained optimistic exploration strategy to adjust the trust region for safe exploration. Additionally, the authors propose to mitigate estimation bias through the use of truncated quantile critics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The empirical evaluations are comprehensive, with three applications."}, "weaknesses": {"value": "1. The presentation of the theoretical results is weak. The paper does not clearly state the assumptions underlying the proposed method, leaving it unclear under what conditions the approach is valid or applicable. The policy’s action distribution is simply assumed to be Gaussian without justification. Moreover, Lemma 1 and Lemma 2 are rather trivial and do not appear to offer substantial theoretical contribution. If these are presented as lemmas, one would expect a main theorem or stronger result to follow—otherwise, the theoretical section lacks depth.  \n2. Even if the lemmas are accepted, the authors fail to explain how these results contribute to improving sample efficiency or mitigating estimation bias. While the empirical evaluations suggest some performance gains, it remains unclear under what circumstances the proposed method is effective. The framework does not appear to generalize well.  \n3. Although the experimental coverage is broad, the presentation quality is poor. For instance, no quantitative results are provided to support the claim of reduced estimation bias. In addition, the explanations are poorly organized and often lack logical coherence, which makes the empirical section difficult to follow.\n4. There is no pseudo code for clarification and implementation."}, "questions": {"value": "Given the major conceptual and presentation issues identified above, I do not have specific technical questions for the authors. The paper requires substantial clarification and restructuring."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sohFC9wdqD", "forum": "EHs3tSukHC", "replyto": "EHs3tSukHC", "signatures": ["ICLR.cc/2026/Conference/Submission4892/Reviewer_ARdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4892/Reviewer_ARdC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379959950, "cdate": 1761379959950, "tmdate": 1762917742992, "mdate": 1762917742992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes COX-Q, an off-policy safe reinforcement learning algorithm that achieves efficient and safety-aware exploration by integrating optimism, gradient conflict resolution, and uncertainty estimation. Building upon the optimistic actor-critic framework, COX-Q introduces a cost-constrained optimistic exploration strategy that balances reward maximization with constraint satisfaction. It employs a Policy-MGDA mechanism to align reward and cost gradients, ensuring exploration occurs only in directions that improve performance without increasing risk, and an adaptive step-length controller to prevent constraint violations during updates. Additionally, COX-Q leverages Truncated Quantile Critics (TQC) to obtain uncertainty-aware and bias-reduced value estimates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and timely problem by addressing the challenges associated with implementing off-policy reinforcement learning algorithms in the context of safe RL, where maintaining safety during exploration remains a key concern.\n\n2. The proposed framework integrates gradient conflict resolution with mechanisms to mitigate value estimation biases, effectively addressing both reward overestimation and cost underestimation—two critical issues that commonly affect off-policy safe RL methods.\n\n3. The paper presents a comprehensive empirical evaluation across diverse domains, including locomotion, navigation, and autonomous driving tasks, and benchmarks the proposed method against strong and relevant baselines."}, "weaknesses": {"value": "[W1] Relevant Comparisons are Missing\n\nThe concept of gradient manipulation or resolution is not entirely new in Safe RL, as several works [1–3] have proposed similar approaches. To convincingly demonstrate the advantages of the gradient resolution method presented in this paper, it is essential to compare against these baselines. While some of these methods report results in the context of on-policy RL, their underlying ideas can be readily adapted to off-policy settings.\n\nRecent work [4] addresses the problem of cost underestimation, which is also a focus of our method. Therefore, it represents an important baseline for evaluating the effectiveness of our approach in mitigating cost underestimation.\n\n[W2] Lack of Ablations\n\nThe proposed method comprises two key components: (i) cost-constrained optimistic exploration via Gradient Resolution, and (ii) mitigation of value estimation bias via Truncated Quantile Critics (TQC). The paper would benefit significantly from ablation studies that quantify the contribution of each component individually. Specifically, it would be valuable to demonstrate:\n\n1. the performance of gradient resolution alone compared to prior works [1–3], and\n\n2. the effect of addressing value estimation bias via TQC compared to [4].\n\nSuch ablations would clearly establish the incremental value of each component and provide a stronger empirical justification for the proposed approach.\n\nReferences\n[1] Gu et al., Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation (2024)\n[2] Chow et al., Safe Policy Learning for Continuous Control (2020)\n[3] Liu et al., Constrained Variational Policy Optimization for Safe Reinforcement Learning (2022)\n[4] Gao et al., Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration (2025)"}, "questions": {"value": "My questions pertain to the weaknesses highlighted above:\n\n1. How does the proposed gradient resolution method for optimistic exploration differ from existing approaches in the literature?\n\n2. In the context of safe exploration, preventing constraint violations during early learning is critical. While overestimation of cost can promote safe exploration, it may also lead to overly conservative behavior. How does the proposed method better balance safety and exploration compared to existing baselines such as MICE [1]?\n\nReference\n[1] Gao et al., Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration (2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6qv5I4T3cO", "forum": "EHs3tSukHC", "replyto": "EHs3tSukHC", "signatures": ["ICLR.cc/2026/Conference/Submission4892/Reviewer_EnSs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4892/Reviewer_EnSs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991484460, "cdate": 1761991484460, "tmdate": 1762917742650, "mdate": 1762917742650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles a key gap in off-policy safe RL: optimistic exploration improves sample efficiency but often violates cost constraints, because reward and cost gradients point in different directions and cost critics are biased. The authors propose a primal-dual off-policy algorithm that performs cost-constrained optimistic exploration, and adaptively shrinks the exploration step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies the off-policy safety pain point, i.e., optimism without cost control, and a concrete mechanism of policy-MGDA + cost-aware step-length selection to make OAC-style exploration safe.\n2. Both key steps are derived with closed-form solutions. The SMARTS results, though single-seeded, are non-trivial and show the method still works with large networks."}, "weaknesses": {"value": "1. Safety still depends on cost-critic accuracy. The whole cost-bounded exploration, c.f. Sec. 4.2., assumes the lower/mean cost estimates are reliable. In sparse-cost navigation the paper shows underestimation, and COX-Q behaves like ORAC. A discussion for robustifying the cost critic is missing. I am curious how the authors would further improve this.\n2. The lemmas give local, per-step solutions, but there is no theorem that the whole off-policy procedure respects the CMDP constraint under function approximation and replay. However the abstract emphasizes controlled data-collection cost. I believe this gap should be made explicit.\n3. Some baselines are slightly weakened. CAL is run with UTD=1 but CAL's strength is precisely high UTD."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Qvv00LsbzQ", "forum": "EHs3tSukHC", "replyto": "EHs3tSukHC", "signatures": ["ICLR.cc/2026/Conference/Submission4892/Reviewer_cj7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4892/Reviewer_cj7m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997987387, "cdate": 1761997987387, "tmdate": 1762917742140, "mdate": 1762917742140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}