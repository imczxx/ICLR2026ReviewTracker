{"id": "SnDBJegMX8", "number": 10440, "cdate": 1758171546566, "mdate": 1759897650585, "content": {"title": "MulVuln: Enhancing Pre-trained LLMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection", "abstract": "Software vulnerabilities (SVs) pose a critical threat to safety-critical systems, driving the adoption of AI-based approaches such as machine learning and deep learning for software vulnerability detection. Despite promising results, most existing methods are limited to a single programming language. This is problematic given the multilingual nature of modern software, which is often complex and written in multiple languages. Current approaches often face challenges in capturing both shared and language-specific knowledge of source code, which can limit their performance on diverse programming languages and real-world codebases. To address this gap, we propose MULVULN, a novel multilingual vulnerability detection approach that learns from source code across multiple languages. MULVULN captures both the shared knowledge that generalizes across languages and the language-specific knowledge that reflects unique coding conventions. By integrating these aspects, it achieves more robust and effective detection of vulnerabilities in real-world multilingual software systems. The rigorous and extensive experiments on the real-world and diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven programming languages, demonstrate the superiority of MULVULN over thirteen effective and state-of-the-art baselines. Notably, MULVULN achieves substantially higher F1-score, with improvements ranging from 1.45% to 23.59% compared to the baseline methods.", "tldr": "", "keywords": ["Multilingual Vulnerability Detection", "Deep Learning", "Pre-trained Language Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f8be0bee531a4d393450acfc3a51590757f34da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method for enhancing software vulnerability detection by generalizing across multiple programming languages. The proposed solution utilizes CodeT5, a pretrained language model (PLM) as a backbone that includes general parametric knowledge of multiple languages. The input to the PLM is then augmented by several extra tokens designed to signal which programming language is being used. The embeddings for those extra tokens are learned during the training of the model, and the model further incentivized to select the embeddings associated with the correct language through two proposed approaches: key parameter query, and parameter masking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method seems like a lightweight add-on to pretrained language models that slightly improves their performance on vulnerability detection, and may be relevant for other software-related tasks as well.\n- The “parameter selection via key-parameter query” method seems interesting as it bears resemblance to the attention mechanism in transformer models, despite the fact that it showed less impressive results than the “language-aware parameter masking”"}, "weaknesses": {"value": "- The related work section did not cite any of the existing works targeting multi-lingual software vulnerability detection (e.g., [A]. [B], [C]. These paper should have been discussed in the related work, and even used as baselines for comparison.\n    \n- The authors mentioned polyglot applications (i.e., projects including multiple languages) to motivate their proposed solution, but this type of software was not present in the considered dataset and was thus never evaluated in the experiments.\n    \n- The external validity of the work is questionable. Although the proposed detector was trained on the training set of the REEF dataset, and was evaluated on the test set of the same dataset, this might not be enough. Other datasets (from the plethora of available datasets on vulnerability detection) should have been considered to prove the efficacy of the proposed method in various settings.\n \n[A] Zhang, Boyu, Triet HM Le, and M. Ali Babar. \"MVD: A Multi-Lingual Software Vulnerability Detection Framework.\" *arXiv preprint arXiv:2412.06166* (2024).\n\n[B] Zhang, Ting, et al. \"Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection.\" *arXiv preprint arXiv:2503.01449* (2025).\n\n[C] Yu, Junji, et al. \"A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection.\" *Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis*. 2025."}, "questions": {"value": "- Why did the authors not cite related works on multi-lingual vulnerability detection in their related work? and why were performance comparisons not conducted with those recent works?\n    \n- In section 3.1 you mention that you consider function-level binary classification of vulnerabilities. However this exact problem has come under strong criticism from recent work [A], namely it fails to incorporate contextual information which are crucial to deciding whether software is vulnerable. How would you defend this choice?\n    \n- For multi-lingual projects, could a weighted sum of language-specific parameters $P_X$ be used instead of the argmax in equations (1) ad (2)?\n    \n- For pretrained language models, have you experimented with chain-of-thought prompting? You could  elicit the model to first output the used programming language, which would naturally condition the upcoming generated tokens deciding whether the code is vulnerable. It would be interesting to compare this to the learnable conditioning proposed by your solution.\n    \n\n[A] Risse, Niklas, Jing Liu, and Marcel Böhme. \"Top score on the wrong exam: On benchmarking in machine learning for vulnerability detection.\" *Proceedings of the ACM on Software Engineering* 2.ISSTA (2025): 388-410."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6rFyaBCxF", "forum": "SnDBJegMX8", "replyto": "SnDBJegMX8", "signatures": ["ICLR.cc/2026/Conference/Submission10440/Reviewer_YvFQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10440/Reviewer_YvFQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760879556643, "cdate": 1760879556643, "tmdate": 1762921743718, "mdate": 1762921743718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of multilingual software vulnerability detection by proposing MulVuln, a framework that augments pre-trained language models with a parameter pool specifically designed to capture both shared (cross-lingual) and language-specific knowledge in source code. The approach combines the strengths of PLMs for general semantic representation with dynamic selection of language-tailored parameters via either key-parameter querying or language-aware masking. The method is rigorously evaluated against thirteen baselines on the challenging REEF dataset, demonstrating superior F1 and recall, as well as strong performance on top vulnerability types (CWEs) and across seven diverse programming languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method consistently outperforms a wide range of strong baselines and modern LLMs.\n- The paper provides explicit interpretability through visualizations that offer valuable insights into the inner workings of the model.\n- The research ensures reproducibility and offers practical value by providing sufficient implementation details and using a realistic dataset."}, "weaknesses": {"value": "- Although the performance of multilingual code vulnerability detection is commendable, some related work on vulnerability detection based on LLMs  (beyond merely utilizing LLMs themselves) has not been cited or thoroughly analyzed. Due to the lack of a clear comparison with LLM-based vulnerability detection work, its originality is limited.\n- There is limited analysis of the computational overhead. The design introduces language-specific parameter matrices on top of parameter-rich PLMs, which may incur additional computational costs.\n- Although recall and F1 are highlighted (arguably reasonable in vulnerability detection due to prioritizing recall), MulVuln’s precision often increases less than recall. In Table 1, some baselines achieve comparable or even higher precision. Given the application (where false positives may incur costs), more discussion or tuning toward precision-oriented use cases is warranted."}, "questions": {"value": "- The main benefit is claimed around the dynamic query mechanism for associating code (possibly ambiguous or mixed-language) with the right parameter. How does the model behave for code samples with mixed or embedded scripting languages, or where the language cannot be reliably determined upfront? Are there empirical results for such edge cases?\n- Results currently lack confidence intervals/statistical significance analysis. Are the observed improvements in F1-score over the best PLM/LLM baselines robust to different seeds or test splits? Could the authors report mean and variance over multiple runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hYrfwBArIe", "forum": "SnDBJegMX8", "replyto": "SnDBJegMX8", "signatures": ["ICLR.cc/2026/Conference/Submission10440/Reviewer_SqYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10440/Reviewer_SqYM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363506552, "cdate": 1761363506552, "tmdate": 1762921743036, "mdate": 1762921743036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MulVuln, a novel approach for multilingual software vulnerability detection. The authors highlight that most existing AI-based detection methods are limited to a single programming language, which is insufficient for modern software systems that are often complex and written in multiple languages. MulVuln is designed to capture both shared knowledge that generalizes across different languages and language-specific knowledge that reflects unique coding conventions. The approach was evaluated on the real-world REEF dataset, which includes 4,466 CVEs across seven different programming languages. The experiments demonstrated that MulVuln significantly outperformed thirteen state-of-the-art baselines, achieving an F1-score improvement of 1.45% to 23.59%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Important Problem: The paper correctly identifies and addresses a significant, practical gap in SVD research: the lack of effective, multilingual models for real-world codebases.\n\nClear Methodology: The proposed MulVuln approach is simple, intuitive, and clearly explained. The two selection mechanisms (instance-based query vs. language-aware training) are sensible explorations of the design space."}, "weaknesses": {"value": "**Limited and Unclear Empirical Evaluation**: The experimental design suffers from two significant gaps regarding contemporary baselines: (1) While the paper states the use of various prompting strategies (zero-shot, few-shot, and instruction-based few-shot prompting), the final result in RQ1 is an aggregate, single score for all LLMs. (2) The paper overlooks several highly relevant and recently published baselines based on both PLMs [1-2] and LLMs [3-4], which significantly weakens the claim of achieving state-of-the-art performance. \n\n[1] Distinguishing Look-Alike Innocent and Vulnerable Code by Subtle Semantic Representation Learning and Explanation\n\n[2] SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection\n\n[3] Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data\n\n[4] Collaboration to Repository-Level Vulnerability Detection\n\n\n**Limited Scalability Discussion**: The model's design, especially Eq. 2, assumes a closed set of $S$ languages, with $S$ parameter matrices. This does not scale well to dozens of languages and offers no clear path for handling languages unseen during training (a critical aspect of true multilingual generalization)."}, "questions": {"value": "1. The authors state that \"zero-shot, few-shot and instruction-based few-shot prompting were adopted for DeepSeek-Coder, Code Llama, Llama 3, GPT-3.5-Turbo and GPT-4o\". However, the authors later claim that LoRA fine-tuning was also \"applied\" (l. 365). Please explicitly specify which of the above models were actually LoRA-fine-tuned and which were only prompt-engineered. Besides, RQ-1 reports a single bar per model; it is impossible to tell whether the number comes from zero-shot, few-shot or instruction-based few-shot. Clarify the exact prompting protocol used for each reported result.\n\n2. Does LoRA fine-tuning of LLMs surpass the performance of MulVuln? Present a head-to-head comparison (MulVuln vs. LoRA-LLM) on the same test split so that the benefit of your adaptation strategy can be quantified.\n\n3. If CodeT5-base already delivers strong results, have you experimented with larger checkpoints of the CodeT5+ family (e.g., 2B or 16B parameters)? \n\n4. Can MulVuln generalise to unseen programming languages, or at least adapt from a handful of training samples in a new language? Report zero-/few-shot transfer results on at least one language never seen during training to validate the claim of language-agnostic vulnerability detection."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XECKtXvprR", "forum": "SnDBJegMX8", "replyto": "SnDBJegMX8", "signatures": ["ICLR.cc/2026/Conference/Submission10440/Reviewer_2csG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10440/Reviewer_2csG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924257119, "cdate": 1761924257119, "tmdate": 1762921742539, "mdate": 1762921742539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MulVuln, a multilingual vulnerability detection approach that augments pre-trained language models with a learnable parameter pool to capture both shared and language-specific knowledge. The method selects appropriate parameters via key-based matching or language masking, concatenates them with input embeddings, and processes them through CodeT5's encoder. Experiments on the REEF dataset covering 7 programming languages show F1-score improvements of 1.45-2.81% over fine-tuned CodeT5/CodeT5+."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a important problem of multilingual vulnerability detection with a clear and intuitive approach. The experimental evaluation is comprehensive, covering 7 programming languages on the REEF dataset with 4,466 CVEs and comparing against 13 diverse baselines spanning deep learning models, pre-trained language models, and large language models. The proposed two-component design balances shared cross-language knowledge with language-specific features, and the visualizations provide useful insights into how the parameter pool operates. The writing is generally clear and the methodology is well-explained."}, "weaknesses": {"value": "The most critical flaw is the absence of parameter-efficient fine-tuning baselines like Prefix-Tuning, LoRA, and Adapters, which are directly comparable to the proposed approach and essential for establishing novelty—without these comparisons, the contribution reduces to applying existing prefix-tuning techniques to vulnerability detection. All experimental results lack statistical rigor with single-run evaluations, no error bars, and no significance testing, making it impossible to determine whether the improvements are meaningful or simply noise. The claims about learning \"language-specific knowledge\" are inadequately supported, with no analysis of what the parameter pool actually encodes, no parameter similarity matrices across languages, and no cross-language transfer experiments to validate the separation of shared versus specific features. Critical ablation studies are missing, particularly varying the parameter pool size S beyond the default value of 7 and testing different query functions beyond the [CLS] token. The generalization capabilities remain completely untested through leave-one-language-out experiments, temporal splits, or cross-project evaluation, which is problematic given the dataset's severe imbalance that goes unaddressed. Several experimental results are unexplained, such as why DeepSeek-Coder with 6.7B parameters achieves only 48.61% F1 while the much smaller CodeT5 performs better, and why GPT-4o's precision (74.54%) vastly exceeds MulVuln's (57.51%). The theoretical justification is entirely absent, with no explanation for why prepending 5 learnable tokens should capture language-specific knowledge or why the particular loss formulation in Equation 3 is appropriate."}, "questions": {"value": "see in the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vuTHpmXVB8", "forum": "SnDBJegMX8", "replyto": "SnDBJegMX8", "signatures": ["ICLR.cc/2026/Conference/Submission10440/Reviewer_zUau"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10440/Reviewer_zUau"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139408685, "cdate": 1762139408685, "tmdate": 1762921741982, "mdate": 1762921741982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}