{"id": "crID4ZT2NP", "number": 22162, "cdate": 1758326945141, "mdate": 1759896883172, "content": {"title": "Trace Length is a Simple Uncertainty Signal in Reasoning Models", "abstract": "Uncertainty quantification for LLMs is a key research direction towards addressing  hallucination and other issues that limit their reliable deployment. In this work, we show that reasoning trace length is a simple and useful confidence estimator in large reasoning models. Through comprehensive experiments across multiple models, datasets, and prompts, we show that trace length performs in comparable but complementary ways to other zero-shot confidence estimators such as verbalized confidence. Our work reveals that reasoning post-training fundamentally alters the relationship between trace length and accuracy, going beyond prior work that had shown that post-training causes traces to grow longer in general (e.g., \"overthinking\"). We investigate the mechanisms behind trace length's performance as a confidence signal, observing that the effect remains even after adjusting for confounders such as problem difficulty and GRPO-induced length bias. We identify high-entropy or ``forking’’ tokens as playing a key role in the mechanism. Our findings demonstrate that reasoning post-training enhances uncertainty quantification beyond verbal expressions, and establish trace length as a practical confidence measure for large reasoning models.", "tldr": "We show that in reasoning models, trace length is a competitive zero-shot uncertainty quantification method.", "keywords": ["Calibration", "Uncertainty quantification", "reasoning models", "uncertainty", "confidence"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/350bc5e40a77f055db5eeaf0fe01ead0380f5059.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors analyze how reasoning trace length can serve as a zero-shot uncertainty measure for large reasoning models with extensive experiments. Empirical results demonstrate that after reasoning post-training, trace length correlates strongly with correctness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work presents a novel insight that a simple signal like trace length can serve as confidence estimator.\n2. The authors investigate extensive evaluations across multiple models, datasets, and prompts.\n3. The paper is well written with clarity in writing."}, "weaknesses": {"value": "1. The core idea of this work lacks novelty. The notion that longer reasoning shows uncertainty is intuitive and quite straightforward. Therefore, the contribution of this work may come from its empirical validation.\n2. Lines 141–142 need to be revised. Tao et al. (2025) show that linguistic VC performs better than token-probability-based UQ and numeric VC, but not better than multi-sampling approaches. They only compare with consistency rate and majority vote in the Appendix, and the results cannot conclude that linguistic VC outperforms these methods. Moreover, some SOTA methods, such as Semantic Entropy and Kernel Language Entropy, are missing.\n3. Regarding evaluation metrics, I agree that “AUROC and ECE (or Brier) are not necessarily aligned,” and this is a common phenomenon. However, they provide two distinct viewpoints: one measures discrimination, and the other measures how truthfully the confidence score reflects the empirical accuracy rate. Hence, from my understanding, ECE and its variants are still useful in many scenarios. In practice, a good confidence metric should yield both good AUROC and good ECE.\n4. I noticed that the authors use OpenThinker2 and iw-SFT, which are models trained from SFT checkpoints. Can we claim that SFT helps improve the reliability of TL and VC? Also, what about the performance of the SFT-only model? I would like to see a comparison among SFT-only, SFT+RL, and RL-only settings."}, "questions": {"value": "1. In lines 321–322, how do the authors choose the top k (50) highest-entropy tokens? Is the selection based on the whole test set or using a calibration/development set? If it is the former, is that a fair approach?\n2. What is the author's conclusion regarding difficulty? Can the authors show the Spearman correlation between TL and difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qTBbxvyJ6j", "forum": "crID4ZT2NP", "replyto": "crID4ZT2NP", "signatures": ["ICLR.cc/2026/Conference/Submission22162/Reviewer_twQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22162/Reviewer_twQ6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968910107, "cdate": 1761968910107, "tmdate": 1762942095658, "mdate": 1762942095658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how the length of a model’s reasoning trace can serve as a proxy for uncertainty in reasoning-trained LLMs. By analyzing multiple reasoning models (mainly Qwen2.5-based) across ten benchmarks, the authors find that longer reasoning traces are generally associated with lower answer accuracy, indicating higher uncertainty. They compare trace length (TL) against verbalized confidence (VC) and show that TL alone, or combined with VC, can effectively predict model correctness using AUROC as the primary metric."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Simplicity and Practicality:** The proposed uncertainty signal (trace length) is extremely easy to compute and requires no model modification or additional prompts.\n\n**Systematic Evaluation:** The authors evaluate across multiple datasets and models, using both verbal and numerical confidence prompts.\n\n**Interpretability:** The study connects trace length with token-level entropy (“forking tokens”), offering some insight into why longer reasoning may correspond to uncertainty.\n\n**Complementarity:** TL complements verbalized confidence, suggesting possible hybrid approaches for model reliability estimation."}, "weaknesses": {"value": "**Lack of Theoretical Depth:** The paper mainly presents empirical correlations without a solid theoretical explanation of why trace length reflects uncertainty.\n\n**Limited Generalization:** All models are variants of Qwen2.5; the findings may not generalize to other architectures or non-reasoning tasks.\n\n**Shallow Analysis:** Statistical validation and significance testing are minimal; differences between TL and VC are often small.\n\n**No Downstream Demonstration:** The work stops at correlation analysis and does not explore how TL-based uncertainty estimation can improve real-world applications like answer filtering or self-consistency."}, "questions": {"value": "How stable is the correlation between trace length and uncertainty across different reasoning styles or prompt formats (e.g., Tree-of-Thought vs. Chain-of-Thought)?\n\nCould trace length be confounded by model verbosity rather than genuine uncertainty?\n\nHow would TL perform in creative or open-ended tasks where long outputs are not necessarily uncertain?\n\nCan TL be combined with other uncertainty signals (e.g., entropy, logit variance) to build a more principled UQ framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nAIwSB9NJJ", "forum": "crID4ZT2NP", "replyto": "crID4ZT2NP", "signatures": ["ICLR.cc/2026/Conference/Submission22162/Reviewer_Qs7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22162/Reviewer_Qs7z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970030828, "cdate": 1761970030828, "tmdate": 1762942095350, "mdate": 1762942095350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigate the mechanisms behind trace length’s performance as a confidence signal\nThe paper identify high-entropy or “forking” tokens as playing a key role in the mechanism. \nTheir findings demonstrate that reasoning post-training enhances uncertainty quantification beyond verbal expressions, and establish trace length as a practical confidence measure for large reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper aims to tackle an interesting problem, how to better perform zero-shot uncertainty estimation in reasoning models.\n- The paper has presented extensive empirical results to verfiy trace length as a practical confidence measure for large reasoning models."}, "weaknesses": {"value": "- In L149, the paper state that it aims to quantify “how various post-training approaches influence the verbalized confidence abilities of the resulting model.” However, it seems like all these reasoning models are tuned on the traces generated from R1, which raises questions about the generality of the findings.\n- It is unclear to me how trace length is used in practice to judge the confidence of a single response. Does the method rely on a predefined threshold to decide whether a response is “confident” or “uncertain”? If so, how is this threshold determined and validated? Is it dataset dependent?\n- It is unclear whether the conclusion hold across different RL post-training methods. Especially those where the reward function directly optimizes for reasoning trajectories.\n- The paper shows that longer reasoning traces tend to correlate with less confidence answers, however, just based on the emperically experiments, It remains unclear whether trace length causes uncertainty or is simply a byproduct of other unknown confounders."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GmkMSPTesf", "forum": "crID4ZT2NP", "replyto": "crID4ZT2NP", "signatures": ["ICLR.cc/2026/Conference/Submission22162/Reviewer_9Lj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22162/Reviewer_9Lj7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032114819, "cdate": 1762032114819, "tmdate": 1762942094963, "mdate": 1762942094963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies trace length as a signal of a reasoning model's uncertainty. The authors show that trace length can be used as a signal zero-shot, provides complementary uncertainty signal to verbal uncertainty, is not accounted for by other correlates like problem difficulty, and seems to be attributed to the presence of \"forking tokens\" like \"wait\" and \"maybe.\""}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting headline result: trace length correlates with uncertainty, and in a way that complements verbal confidence\n- Thorough, comprehensive experiments: controlling for question difficulty, comparison to verbal confidence. Contributes to the community's understanding of reasoning models and uncertainty.\n- Clear and well-written paper"}, "weaknesses": {"value": "I believe this is a strong scientific result with no weaknesses that impact my recommendation of acceptance. There are limitations of the method, most of which the authors point out:\n- limited generalizability, particularly in low-accuracy regimes where it is particularly important to have uncertainty estimation\n- trace length might only be a reliable (correlational) signal given current model training pipelines, rather than a fundamental property of model reasoning. Specifically:\n\t- if we optimize models for other length-related properties or behaviors that impact length (e.g. exploration), trace length probably wouldn't be a reliable signal of uncertainty any more -- or at least, it would be unclear how the result in this paper generalizes.\n\t- while the paper identifies \"forking tokens\" as a \"mechanism\" for the correlation between trace length and uncertainty, the importance of tokens like \"wait\" or \"maybe\" being forking tokens seems like an artifact of current model training. The authors argue that that forking tokens can more generally be defined as any token where the LLM has \"high entropy in its token distribution,\" but then this seems like a vacuous definition of uncertainty."}, "questions": {"value": "- It seems important to see the procedure for how trace length is used to compute AUROC explicitly written down in the main body of the paper. For verbalized and numerical confidence, we know the range of bins -- for trace length, do you normalize by the min/max trace length for a particular model/dataset combination? \n- Related to above: how much does the normalization across datasets impact the reliability of trace length as a signal? The bins for verbalized confidence seem to work regardless of what dataset we apply them to, but I'm wondering how we should interpret the model's confidence on dataset A where reasoning traces are between 10-100 tokens vs. dataset B where the min/max trace lengths are 50-10000 tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DGmZkywrIF", "forum": "crID4ZT2NP", "replyto": "crID4ZT2NP", "signatures": ["ICLR.cc/2026/Conference/Submission22162/Reviewer_HZUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22162/Reviewer_HZUW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762749886560, "cdate": 1762749886560, "tmdate": 1762942094698, "mdate": 1762942094698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}