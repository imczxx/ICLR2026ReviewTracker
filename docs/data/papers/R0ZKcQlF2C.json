{"id": "R0ZKcQlF2C", "number": 23698, "cdate": 1758347337848, "mdate": 1759896801029, "content": {"title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation", "abstract": "Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question–answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.", "tldr": "ArenaBencher is a framework for automatically evolving benchmarks by using a pool of models to select harder, verified query variants that expose model failures in various task domains.", "keywords": ["automatic evaluation", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2ebfaed1e932743a17d48f0b40cbd9833da0fc5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ARENABENCHER, a framework for automatic benchmark evolution using multi-model feedback. The idea is to iteratively generate new test cases while preserving the core ability of each item and selecting those that degrade performance across several models. The authors claim that the method improves difficulty, fairness, and separability of existing benchmarks such as GSM8K, CSQA, and harmful behavior datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem: benchmark contamination and stagnation under rapid LLM progress.\n\n2. The multi-model feedback mechanism is conceptually interesting and potentially more robust than single-model adversarial rewriting.\n\n3. The framework is clearly modular and described in a step-by-step manner."}, "weaknesses": {"value": "1. Major claims are not empirically justified. Many statements in Sections 1–3 are presented as facts without direct supporting evidence. For example, the claim that multi-model feedback “mitigates bias” is not demonstrated beyond intuition.\n\n2. Experimental definitions are unclear or inconsistent. Key metrics such as fairness are vaguely defined in prose but not rigorously formalized in Section 4. In Table 2, fairness is said to represent “how evenly the performance drop is distributed,” but if so, the original dataset should always be 100% fair by definition, which contradicts reported values (e.g., 84.8 or 82.9). I could not understand what a fairness value of 85 actually means.\n\n3. Evaluation is incomplete. There is no ablation for critical design choices (e.g., the number of sampled models, verifier quality, or extraction accuracy). Moreover, the paper lacks qualitative analysis of failure cases until very late (Figure 2), and even there the discussion remains superficial."}, "questions": {"value": "1. Please provide a precise mathematical definition of fairness and clarify why original datasets do not reach 100 under your metric.\n\n2. Can you provide evidence that multi-model feedback truly outperforms single-model-guided rewrites, beyond performance drop magnitude?At present, I believe that the paired trends in Table 1 alone are not sufficient to draw such a conclusion\n\n3. How sensitive is your framework to incorrect ability extraction or verifier mistakes? Can you quantify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Df3sMW4ymb", "forum": "R0ZKcQlF2C", "replyto": "R0ZKcQlF2C", "signatures": ["ICLR.cc/2026/Conference/Submission23698/Reviewer_Mg4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23698/Reviewer_Mg4w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735758586, "cdate": 1761735758586, "tmdate": 1762942772128, "mdate": 1762942772128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ArenaBencher is a framework for automatic benchmark evolution that addresses data contamination in LLM evaluation. The system extracts test objectives, generates candidate questions, verifies them with LLM-as-a-judge, and uses multi-model feedback (sampling √K models) to select variants that consistently degrade performance across models. It includes iterative refinement using successful candidates as in-context demonstrations. The framework is evaluated on GSM8K (math), Harmful Behaviors (safety), and CommonsenseQA (reasoning) using 6 open-source models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design avoids creating model-specific adversarial examples that may be trivial for some models, helping ensure that updated benchmarks reflect shared weaknesses across diverse architectures rather than exploiting idiosyncrasies of particular systems.\n2. The four desiderata (difficulty, separability, fairness, alignment) provide a principled way to assess benchmark quality.\n3. Testing across math, safety, and reasoning domains demonstrates generalizability.\n4. 95% alignment and 96% correctness on 100 GSM8K samples provides high alignemnt evidence of the augmented data."}, "weaknesses": {"value": "Though the paper is well-written, there are two fundamental weaknesses I have to bring out:\n1. The paper's core motivation is to reduce data contamination's impact on benchmark validity, yet the evaluation metrics do not directly demonstrate reduced contamination or improved prediction of true model capabilities on unseen data by the four metrics (Difficulty, Separability, Fairness, Alignment). The paper needs real-world data validation such as temporal splits or naturally occurring unseen data. For example, GitHub PRs after model training cutoffs provide natural test cases for SWE-bench contamination evaluation (if there is any). The evaluation should test whether ArenaBencher augmentation on augmented SWE-bench better predicts performance on post-cutoff PRs compared to other baselines. Without this validation, it remains unclear whether ArenaBencher actually solves data contamination or merely creates harder variants.\n2. The paper provides no comparative evaluation with baseline methods, making it impossible to assess how good the proposed method is. Two critical types of comparisons are missing. First, the paper does not compare multi-model feedback against single-model augmentation to establish cost-effectiveness, leaving unclear whether sampling multiple models provides proportional improvement over using just one model given the additional API costs. Second, there is no comparison with other multi-model collaboration approaches or simpler augmentation methods such as paraphrasing or rule-based perturbations."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aPws4JB28L", "forum": "R0ZKcQlF2C", "replyto": "R0ZKcQlF2C", "signatures": ["ICLR.cc/2026/Conference/Submission23698/Reviewer_1g77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23698/Reviewer_1g77"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986393236, "cdate": 1761986393236, "tmdate": 1762942771845, "mdate": 1762942771845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a model-agnostic framework designed to automatically evolve benchmarks to counteract the problem of data contamination in large language model (LLM) pretraining corpora.\n\nThe ArenaBencher pipeline operates in four main stages. The authors apply this framework to benchmarks in three domains: mathematical reasoning (GSM8K), commonsense reasoning (CommonsenseQA), and safety (AdvBench Harmful Behaviors). \n\nThe results demonstrate that the evolved benchmarks are more difficult for a range of open-source models, improve the separability, and distribute performance drops fairly across the model pool, all while maintaining alignment with the original task objectives as verified by both automated metrics and human evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is easy to follow. It tackles the urgent and widely recognized problem of benchmark contamination and the resulting inflation of LLM performance metrics. I think the proposed direction of creating dynamic, evolving benchmarks is valuable.\n\nThe authors have conducted comprehensive experiments to validate their points. The framework is tested across three distinct and important domains (mathematical reasoning, commonsense reasoning, and safety), with LLAMA3, Qwen3, and Mistral. \n\nDifficulty, Separability, Fairness, and Alignment are measured. The inclusion of a human evaluation study to validate alignment and correctness further strengthens the empirical results."}, "weaknesses": {"value": "I think the framework lacks a mechanism to balance the competing goals of difficulty and separability. The results in Table 2 also partly show this. The current selection mechanism, which selects candidates with the highest aggregated loss, may favor test cases that are simply too hard for all models, causing their scores to cluster near zero and reducing the benchmark's ability to distinguish between them. A truly discriminative benchmark should maximize the variance in performance across models.\n\nThe design choice of sampling $m = \\lceil\\sqrt{K}\\rceil$ models for feedback should be better justified. Although the paper cites \"classical ensemble heuristics\" from Random Forests and XGBoost, ArenaBencher's objective is different. There is no guarantee that a heuristic designed for decorrelating learners by subsampling features would be optimal for estimating a population statistic by subsampling models.\n\nThe entire framework is dependent on a single proprietary model (GPT-4). The authors commendably include a failure case analysis for the generator (Figure 2), but do not discuss or analyze the robustness of the verifier/judge. A systematic bias or blind spot in the judge model could silently poison the entire evolved benchmark."}, "questions": {"value": "While the related work section effectively contrasts the approach with single-model adversarial attacks, it could be improved by including more literature on dynamic benchmark generation.\n\nThe authors could provide ablation studies or theoretical arguments to support the $m = \\lceil\\sqrt{K}\\rceil$ choice over alternatives.\n\nCould the authors provide an analysis of the computational cost of this process? How does the cost scale with the benchmark size $N$, the model pool size $K$, and the number of refinement rounds $R$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6c44ZZ6db4", "forum": "R0ZKcQlF2C", "replyto": "R0ZKcQlF2C", "signatures": ["ICLR.cc/2026/Conference/Submission23698/Reviewer_gCrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23698/Reviewer_gCrr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160928743, "cdate": 1762160928743, "tmdate": 1762942771612, "mdate": 1762942771612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}