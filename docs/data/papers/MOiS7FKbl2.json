{"id": "MOiS7FKbl2", "number": 9326, "cdate": 1758118956163, "mdate": 1759897731294, "content": {"title": "Microscope: Efficient Diffusion with Two-Stage Dynamics Compression for High-Quality Talking Head Generation", "abstract": "The talking head generation task synthesizes videos from a single portrait image and audio input, animating the portrait to deliver the speech content. Non-autoregressive (NAR) approaches for talking head generation have demonstrated impressive quality and generation speeds by producing video frames in parallel, thereby overcoming the error accumulation problems inherent in frame-wise autoregressive (AR) methods. However, NAR methods face limited practical applications due to prohibitive VRAM requirements, especially when generating long sequences ( $\\leq 1000$ frames) at high resolution ($512 \\times 512$). This paper proposes a novel framework that enables high-quality, non-autoregressive talking head generation while significantly reducing computational resource demands for both training and inference. We enhance efficiency through our Microscope Dynamics Compression Framework (MDCF), a two-stage pipeline achieving 768× compression for pixel-level dynamics latent. Additionally, we introduce a two-phase cascade training strategy to stably optimize the MDCF while effectively alleviating error accumulation during multi-stage compression. Experimental results demonstrate that our framework can non-autoregressively generate talking head videos with 1600+ frames at $512 \\times 512$ on a 16GB GPU, with state-of-the-art quality and inference speed. Our approach represents a significant advancement toward practical, resource-efficient talking head synthesis for real-world applications. The source code will be made publicly available to facilitate further research.", "tldr": "We introduce a non-autoregressive framework for talking head generation, enabling high-quality 512×512 video synthesis of 1600+ frames on a single 16GB GPU with high quality and inference speed.", "keywords": ["Talking Head Generation", "Auto-Encoder", "Two-Stage Compression", "Efficient Video Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e25bb077f15088b57532f444df4a0b1c5c5a7c00.pdf", "supplementary_material": "/attachment/e60710037acd50f3609c255c8c27432344c285ce.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MICROSCOPE, a non-autoregressive diffusion framework for talking head generation, featuring a novel Microscope Dynamics Compression Framework (MDCF) and a Two-Phase Cascaded (TPC) training strategy. The proposed design achieves a 768× compression of pixel-level motion representations, enabling efficient long-term video generation (over 1600 frames at 512×512 resolution) using only 16GB of GPU memory. The approach seeks to address the limitations of autoregressive (AR) methods (error accumulation, slow inference) and prior non-autoregressive (NAR) models (excessive VRAM usage, flow jitter). Experimental results demonstrate superior FID/FVD and efficiency compared to DAWN, Hallo, and Audio2Head, with stable long-sequence generation and reduced motion artifacts."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Impressive Computational Efficiency : Achieving over 1600-frame generation at high resolution on a single 16GB GPU is a notable technical feat. The proposed MDCF delivers strong compression (768×) without severe quality degradation, a clear step forward in resource-efficient diffusion models.\n- Two-Phase Cascaded (TPC) Training Strategy : The separation of training between the Flow-aware Dynamics Extractor (FDE) and Latent Motion Auto-Encoder (LMAE) helps stabilize multi-stage compression and avoid gradient collapse. Ablations confirm substantial improvements in convergence and reconstruction quality when using TPC.\n- Practical Long-Term Video Generation : Unlike most diffusion-based talking head models that are limited to short sequences (≤200 frames), MICROSCOPE demonstrates consistent temporal coherence across 1600+ frames. This establishes a solid baseline for scalable and efficient talking head synthesis.\n- Comprehensive Evaluation : The paper presents quantitative, qualitative, and user studies, including a novel Flow Smoothness (FS) metric correlated with human judgment of motion stability. The combination of efficiency and quality benchmarks is thorough and convincing.\n- Strong Engineering Contribution : The analogy to optical microscopes (multi-stage magnification) is well aligned with the multi-level compression principle. The proposed system is clearly articulated, reproducible, and accompanied by detailed experiments."}, "weaknesses": {"value": "- Lack of Autoregressive (AR) Relevance : Although the paper positions itself against AR models, the presented architecture remains purely non-autoregressive. The claimed benefit for long-term temporal modeling is indirect—mainly due to compression and denoising—rather than actual sequence dependency learning.\n- Limited Conceptual Novelty : The core contribution, MDCF, essentially extends hierarchical VAE-based compression (e.g., latent diffusion) to motion fields. While well-engineered, it feels incremental rather than conceptually groundbreaking.\n- Overemphasis on Flow-Based Representation : The reliance on optical flow constrains expressiveness, leading to overly smooth or rigid facial motion. The model inherits typical flow-based limitations—difficulty capturing micro-expressions or out-of-plane movements.\n- Unclear Core Message : The paper presents multiple intertwined technical elements—compression, cascaded training, flow filtering—but lacks a clear statement of which is the main contribution. The ablation studies confirm effectiveness but not the necessity or originality of each.\n- Missed Connection to Prior Efficient Models : Given its emphasis on compression and inference speed, it would be logical to directly compare against efficiency-oriented baselines such as Audio2Head (GAN-based) or latent diffusion variants under equal conditions. The discussion of trade-offs between compression ratio and generation realism is also missing."}, "questions": {"value": "- AR vs. NAR Trade-off : How does the proposed model ensure long-term temporal consistency without an autoregressive mechanism? Does compression alone suffice for maintaining coherence beyond 1600 frames?\n- Main Takeaway : Among MDCF, TPC, and FS, which component represents the core novelty? How should future work position this framework—as a general latent diffusion scheme or a specialized talking head compressor?\n- Motion Expressiveness : The results appear smooth but sometimes lack local detail. Is there a mechanism to enhance high-frequency motion signals without increasing the latent size?\n- Baseline Relevance : Why not start from Audio2Head, which already offers a compact and efficient architecture for talking head generation? Would integrating MDCF into such a baseline further enhance its efficiency?\n- Generalization and Compression Trade-off : The paper focuses on efficiency, but how well does MDCF generalize to unseen identities or different datasets (e.g., VoxCeleb2)? Does higher compression reduce generalization capacity?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The task involves generating human talking head videos, which inherently raises ethical considerations related to identity manipulation, consent, and potential misuse in deepfake applications. While the paper focuses on technical contributions, responsible research practices—such as data collection transparency, human subject consent, and safeguards against misuse—should be clearly discussed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bIzqCxveJx", "forum": "MOiS7FKbl2", "replyto": "MOiS7FKbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9326/Reviewer_Vs1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9326/Reviewer_Vs1s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423928709, "cdate": 1761423928709, "tmdate": 1762920961691, "mdate": 1762920961691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel non-autoregressive talking head generation framework that compresses motion dynamics, significantly reducing computational resource. This paper introduces two-stage pipeline, Microscope Dynamics Compression Framework (MDCF), which first trains the Flow-aware Dynamics Extractor (FDE) to capture motions and then optimizes the Latent Motion AutoEncoder (LMAE). A Two-Phase Cascaded (TPC) training scheme and an image-guided consistency (IGC) loss stabilize training. Advantages including 768x compression and long videos (more than 1600 frames) at 512x512 on a single 16GB GPU are practical and interesting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and clear. It is also technically sound and grounded. \n2. Two-stage dynamics compression makes sense. The paper also validated the effectiveness of its architectural choices by extensive ablations. \n3. The method is effective and resource-friendly."}, "weaknesses": {"value": "1. Narrow evaluation dataset\nTrain and eval are confined to HDTF. Cross-dataset tests (e.g., LRS3, VoxCeleb2, CelebV..) are needed to support generalization. \n\n2. Small / under-reported user study\nOnly 10 participants and 6 test videos per method  seems not that reliable. Extensive user study is needed. (More participants and more test cases needed) \n\n3. FS metric concerns/fairness \nFS is defined on optical flow gradients. Since your method learns flow and reconstructs,  FS may favor your method. Moreover, isn't there any extreme case where an overly smooth (even wrong) flow achieves a low FS, since the metric measures smoothness rather than motion correctness?\n\n4. Lip sync accuracy comparisons \nIn the Table1 (Main quantitative comparison on HDTF), the performance on lip sync accuracy is not optimal relative to other SOTA models (also, several baselines appear out-of-date. I think more recent and powerful models should be compared. e.g., Hallo2, Hallo3, OmniSync, StableAvatar.. ). More importantly, could you clarify which factor limits lip-sync: (i) does a flow-only motion representation fail to capture local mouth-region? or (ii) is audio conditioning or alignment limited? Relatedly, with flow prediction, do you observe mouth-area artifacts (e.g., teeth not rendered correctly)?  \n\n5. Missing identity-related metric \nThe paper reports FID, FVD and lip-sync metrics (LSE-D, LSE-C), but no identity-similarity metrics provided."}, "questions": {"value": "I would appreciate responses to the questions in the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PKjuKILWfK", "forum": "MOiS7FKbl2", "replyto": "MOiS7FKbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9326/Reviewer_v3Mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9326/Reviewer_v3Mf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810110945, "cdate": 1761810110945, "tmdate": 1762920961419, "mdate": 1762920961419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method to generation talking head videos based on diffusion video generation model. The core idea is to use a two-stage cascaded pipeline to disentangle the facial latent motion prediction and face reconstruction based on motion. The overall design achieves significant runtime speed up and VRAM consumption reduction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed MDCF framework divides the talking face video generation into two cascaded stages. The first flow-aware dynamic extractor disentangle the face dynamics into facial identity and flow motion. The second latent motion AE further compress the flow motion dynamics into low-dim latent for later faster computation.\n- The proposed image guided consistency loss and KL regularization helps the training in mitigating the error accumulation for multi-stage design."}, "weaknesses": {"value": "- Though the high level idea of FDE stage is clear, it is not clear what is the details of it. Specially, how it “aggregates nearby pixels into patches”? What is “a slight compression of the dynamics representation”?\n- How is the model performed when the facial dynamics are not just warping? In the supplementary video, the overall lip/jaw dynamics looks good for warping motions but fails dramatically on lip shape change motions, like /o/, /th/, /b/p/m/ sounds. The issue might come from the FDE stage which is based on flow-based warping."}, "questions": {"value": "- In Section 4.3, it is claimed that the proposed method, especially FDE stage, out performs LIA. It is not clear to me why LIA achieves 4x higher FID/FVD scores in Table 4. It would be helpful to provide comparison for their failure cases.\n- How is the eye blinking animation generated in the out of distribution test video, while no eye blinking animation in comparison video?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "db3kBjdwSc", "forum": "MOiS7FKbl2", "replyto": "MOiS7FKbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9326/Reviewer_PMYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9326/Reviewer_PMYf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986041891, "cdate": 1761986041891, "tmdate": 1762920961097, "mdate": 1762920961097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MICROSCOPE, an efficient diffusion-based framework for audio-driven talking head generation. The key contribution is the Microscope Dynamics Compression Framework (MDCF), which combines a Flow-aware Dynamics Extractor (FDE) and a Latent Motion Auto-Encoder (LMAE) to achieve a 768× compression ratio of motion dynamics while maintaining fidelity. A Two-Phase Cascaded (TPC) training strategy and Image-Guided Consistency (IGC) loss stabilize multi-stage optimization. MICROSCOPE enables long, high-resolution video synthesis 1,and shows strong results on HDTF, outperforming previous non-autoregressive models in both quality and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The two-stage compression framework (FDE–LMAE) is coherent, empirically supported, and yields clear memory and latency benefits.\n2. Clearly motivated by efficiency and scalability issues in diffusion-based talking head generation."}, "weaknesses": {"value": "1. The overall novelty is moderate, as the framework mainly combines existing latent compression and staged training techniques.\n2. Evaluation is limited to HDTF. Generalization to other datasets (e.g., VoxCeleb2 or in-the-wild) remains uncertain.\n3. The approach appears to rely on carefully tuned downsampling factors and loss weights, but the rationale behind these design choices is not clearly explained. Providing more insight or adaptivity in how these parameters are selected would make the framework more convincing."}, "questions": {"value": "1. Could the authors comment on how well the model generalizes to more diverse or in-the-wild datasets beyond HDTF?\n2. How were the downsampling factors and loss weights chosen in practice? Have the authors observed any trade-offs between the high compression ratio (e.g., 768×) and fine-grained motion or expression fidelity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "X6uCN7PowY", "forum": "MOiS7FKbl2", "replyto": "MOiS7FKbl2", "signatures": ["ICLR.cc/2026/Conference/Submission9326/Reviewer_QWbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9326/Reviewer_QWbo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996374004, "cdate": 1761996374004, "tmdate": 1762920960308, "mdate": 1762920960308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}