{"id": "CGALrQfkYi", "number": 4701, "cdate": 1757749704742, "mdate": 1759898019190, "content": {"title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation", "abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem.\nDespite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts.\nThese limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. \nIn this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. \nFirst, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions.\nThis design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. \nSecond, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance.\nThis strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. \nExtensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. \nNotably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.", "tldr": "", "keywords": ["Human Motion Generation", "Autoregressive Diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca362ad87645c2471dba8b00438beab113ab6e0a.pdf", "supplementary_material": "/attachment/e7161c9b3486ae20fec263265b9a579b0ef93152.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes HINT, an autoregressive framework for generating multi-human motion. The method combines a diffusion model with a sliding-window approach. The authors claim two main contributions: 1) a \"Canonicalized Latent Space\" that decouples individual motion from inter-person geometry, and 2) a \"Hierarchical Motion Condition\" strategy that uses local and global conditions to guide the interactivate human motion generation process. The authors claim their method matches offline models and surpasses other autoregressive approaches, achieve improvement on the InterX and InterHuman dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a Canonicalized Latent Space that encodes each person’s motion in their own local coordinates, rather than in world coordinates.\n2. The paper employs a sliding-window, autoregressive generation scheme built on a DiT-based diffusion model.\n3. The experiments are comprehensive."}, "weaknesses": {"value": "1. Although the method improves overall interaction motion quality and produces plausible individual motions, it does not explicitly resolve fine-grained physical interactions between bodies in two-person scenarios. As a result, the generated sequences may lack realistic inter-body contact dynamics or exhibit artifacts such as slipping and penetration during close interactions.\n2. The multi-human results could be better. The teaser does not clearly demonstrate interactive behaviors among the multiple characters. The motions in the teaser look almost copy–pasted across characters."}, "questions": {"value": "1. The paper claims to support online generation. Has the author considered the transition/smoothing issue when generating different motion sequences continuously?\n\n2. For multi-human motion generation, does inference require providing initial motion (a starting pose/sequence) as input? Will it generate unreasonable human motion if the two people are in the distance initially?\n\n3. The paper states that 3-person and 4-person motions can be generated without additional training. How is this achieved in practice? Can the model ensure that every pair of individuals can interact with each other?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qzTul21rzP", "forum": "CGALrQfkYi", "replyto": "CGALrQfkYi", "signatures": ["ICLR.cc/2026/Conference/Submission4701/Reviewer_8Sbr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4701/Reviewer_8Sbr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760808377622, "cdate": 1760808377622, "tmdate": 1762917523441, "mdate": 1762917523441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles multi-human motion generation, an important yet comparatively underexplored area in generative AI. Compared to single-human generation, multi-human settings add significant complexity due to coordination, spatial relations, and temporal consistency across agents. The authors’ decoupling strategy aims to enable spatially aware, time-consistent generation across multiple people. While diffusion models are often constrained to fixed lengths, token-based methods (e.g., VQ-VAE) enable variable-length synthesis; this breaches traditional diffusion-based methods by adopting a sliding-window procedure for continuous generation. Results on established benchmarks appear promising. However, the qualitative evidence shows limited improvements in genuine interactions: most examples are generic, non-contact scenarios with relatively simple motions and limited inter-agent coupling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear motivation for addressing multi-human motion, a topic that demands richer spatial/temporal modeling than single-human generation.\n* A principled decoupling/conditioning design intended to preserve local motion quality while handling inter-person relations.\n* Practical use of a sliding window to extend diffusion beyond fixed-length clips, enabling continuous rollouts.\n* Competitive results on standard benchmarks."}, "weaknesses": {"value": "* **Unspecified text-to-length mapping.** Global conditioning uses the “total frame number” ($T_N$) *according to the textual description*, but the paper does not explain how ($T_N$) is inferred or parsed from text at test time. This is central to the **compositional command** narrative and should be made explicit.\n\n* **Canonicalization and drift across windows.** The approach removes absolute placement and re-injects pairwise transforms. The paper should discuss **how relative transforms are obtained and propagated** (predicted vs. read from history) and whether compounding errors arise across windows—especially for long sequences and when ($N$>2).\n\n* **Limited contribution of word-level text.** The ablation suggests removing word-level embeddings does not change R@Top3 (0.672 $\\rightarrow$ 0.672), which weakens the claim that token/word-level guidance is essential for semantic fidelity. This discrepancy should be analyzed and reconciled.\n\n* **Underpowered user study.** With only ~15 participants and minimal protocol detail, the subjective evaluation provides limited evidentiary value for the broader claims.\n\n* **Interaction richness is not demonstrated.** While the system can produce multiple agents moving concurrently, the evidence for **true, contact-rich interactions** (touch, handoffs, coordinated manipulation) is thin. The demos often look like parallelized single-person motions rather than tightly coupled multi-person behaviors."}, "questions": {"value": "1. **Number of agents from text.** Can the model **infer** how many humans to generate purely from the text description? If yes, how is this determined? If not, what is the intended interface for specifying (N)?\n\n2. **Initialization / first frames.** Does generation require an initial history (e.g., the first frame or a short prefix) for each agent? If text-only generation is supported, please describe the initialization procedure.\n\n3. **Role control for (N $\\geq$ 3).** How does the system distinguish and control different people—especially three or more—when a single prompt describes different roles? Is the model capable of **per-agent conditioning** simply based on text prompts?\n\n4. **Inter-penetration and physical plausibility.** Does the method incorporate **hard constraints** (e.g., collision checks) or rely solely on training penalties? Can the authors provide **contact-rich** examples (e.g., one person touches another’s shoulder or knee; passing an object) and report basic contact/collision metrics?\n\n5. **Spatial-text reasoning.** If the text says “two people shake hands” but the initial distance is large, will the agents **walk toward each other before shaking hands**, or do they attempt to satisfy the prompt without reconciling the spatial precondition?\n\n6. **Out-of-distribution prompts.** How does the model behave on **outside-the-scope** or complex motions not well represented in the training set? The provided demos appear elementary; please comment on robustness and failure modes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "54h33xWoeH", "forum": "CGALrQfkYi", "replyto": "CGALrQfkYi", "signatures": ["ICLR.cc/2026/Conference/Submission4701/Reviewer_wSXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4701/Reviewer_wSXb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882017389, "cdate": 1760882017389, "tmdate": 1762917523124, "mdate": 1762917523124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel hierarchical interaction modeling approach that effectively addresses the entanglement of global positions and individual motion semantics in multi-person motion generation. By decoupling each person's motion into a local space and using multi-person interaction information as a condition for diffusion, the method ensures semantic clarity. Additionally, an online sliding-window strategy built upon this framework guarantees efficient online generation. Experiments validate the effectiveness of both HMC and online generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper insightfully identifies a potential obstacle to generalizing two-person interaction motion generation to larger groups—namely, the entanglement of global information—and proposes and validates a decoupling strategy, thereby demonstrating strong originality and clearly establishing the significance of the problem.\n\n2. The exposition is clear; the authors’ ideas are easy to grasp.\n\n3. The supplementary videos present qualitative multi-person generation results and experiments that confirm the successful extension of HMC to groups larger than two."}, "weaknesses": {"value": "1. Extracting interaction information into the global diffusion pipeline may lengthen the conditioning vector. Extending a two-person scenario to three is still manageable, but scaling to larger groups becomes problematic. Taking agent A’s viewpoint as an example, the paper encodes partner history by using B’s rotation $R$ and translation $T$ relative to A. Section 3.5 further suggests that expanding from two to N people simply means concatenating the partner-history embeddings of all additional partners; therefore, the history length—and hence the condition size—grows exponentially with the number of agents.\n\n2. Multi-person interaction still relies on fine-grained user control. As noted in line 286 (Section 3.3, Global Conditions, Compositional Command Embedding), if the user supplies a full description that contains multiple step-by-step instructions, a sentence-level global token is provided to guide generation. **It remains unclear whether performance drops noticeably when such a global prompt is not given.**"}, "questions": {"value": "1. **Has HINT ever attempted to generate longer clips, such as results lasting one minute** (possibly stitched from shorter segments), with corresponding textual instructions possibly produced by an LLM? Generating long-duration motions can test motion coherence and thereby demonstrate how the sliding-window strategy copes with the challenges of long-term sequence generation.\n\n2. Yet if the user’s instruction is a vague one like “A person first shakes hands with the person opposite them and then hugs the others,” it should still be possible to infer who is “opposite” and who are “the others” by using global information about all agents, and then to generate the correct motion given the prior history. **By converting global information into relative information for each individual agent, does HMC disrupt the understanding of the implicit global semantics?**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wwwmwzxHTe", "forum": "CGALrQfkYi", "replyto": "CGALrQfkYi", "signatures": ["ICLR.cc/2026/Conference/Submission4701/Reviewer_ArWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4701/Reviewer_ArWa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563459204, "cdate": 1761563459204, "tmdate": 1762917521789, "mdate": 1762917521789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HINT is an autoregressive, diffusion-based framework for online multi-human motion generation that maps each person’s motion into a canonicalized latent space and synthesizes future frames with a sliding-window process guided by hierarchical local/global conditions, enabling variable-length sequences and easy scaling to more agents.  It reports strong realism, achieving FID 3.100 on InterHuman and 0.278 on InterX, outperforming online baselines while remaining slightly behind the best offline method in text alignment.  Ablations confirm the necessity of the canonicalized latent space and both condition tiers.  Inference per window is about 1.1 s on a single 3090 GPU (similar to InterMask* and slower than DART†), with overall results validating the approach’s effectiveness for streaming, multi-agent motion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear decomposition (canonicalization + hierarchical conditioning) that makes variable-length, multi-agent generation straightforward to implement. \n- Solid quantitative improvements on realism (FID) with extensive ablations isolating the contribution of each condition. \n- Simple path to >2 agents without retraining (shared weights; partner-history concatenation)."}, "weaknesses": {"value": "- The paper’s problem setting seems already addressed by prior MDM extensions (e.g., priorMDM); \n- Novelty feels incremental—canonicalization + sliding window + standard diffusion conditioning."}, "questions": {"value": "- What is new beyond prior MDM-based streaming/online variants (e.g., priorMDM)? Please add a direct, apples-to-apples comparison and clarify conceptual differences.\n- How do you control error accumulation across windows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aCxBVQLO2X", "forum": "CGALrQfkYi", "replyto": "CGALrQfkYi", "signatures": ["ICLR.cc/2026/Conference/Submission4701/Reviewer_uvFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4701/Reviewer_uvFZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850049992, "cdate": 1761850049992, "tmdate": 1762917521470, "mdate": 1762917521470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}