{"id": "zZvrFzDkwF", "number": 11914, "cdate": 1758204623344, "mdate": 1759897546380, "content": {"title": "CAPNAV: TOWARDS ROBUST INDOOR NAVIGATION WITH DESCRIPTION-FIRST MAPS", "abstract": "While many navigation instructions mention object categories, real-world commands often hinge on fine-grained attributes such as color, shape, and texture. Most Vision-and-Language Navigation (VLN) approaches are not designed for such descriptions: they loosely combine visual and textual features and rarely verify if the perceived object actually matches the stated attributes. We introduce CapNav, a description-first navigation framework that unifies attribute-based language grounding with visual perception to improve object localization. CapNav constructs a 3D instance-level map in which every object is annotated with both semantic embeddings and explicit attribute fields. These are generated through multiview captioning and viewpoint-consistent feature clustering using foundation models (DINOv2, CLIP). The resulting attribute-enriched descriptors are indexed for retrieval and queried through a multimodal large-language model to infer the location of the requested object. A subsequent navigation stage verifies attribute consistency before selecting a goal location. By combining visual grounding with semantic reasoning, CapNav can follow fine-grained, description-based instructions. Experiments in simulated indoor environments demonstrate that CapNav surpasses category-only baselines on multi-object navigation tasks under attribute ambiguity and produces interpretable navigation decisions grounded in structured captions.", "tldr": "CapNav unifies attribute-aware grounding and perception to navigate to precisely described objects, outperforming category-only methods under ambiguous, multi-object scenes.", "keywords": ["Vision-Language Navigation", "Multi-View Captioning", "Attribute-Guided Retrieval", "LLM", "Clustering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf52b0753d0b00bcae2873a023b396b70a700ecc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CapNav, a description-first indoor navigation framework designed to handle attribute-based natural language instructions such as “go to the white sofa with curved edges.” CapNav integrates multi-view captioning, attribute-based 3D mapping, and a multimodal LLM planner to improve object localization and navigation robustness. The system constructs instance-level 3D maps annotated with attributes (color, shape, texture) and verifies attribute consistency before executing navigation. Experiments in simulated Matterport3D environments demonstrate improvements over VLMaps on multi-object navigation tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an important problem—bridging fine-grained visual grounding and natural-language navigation. This direction aligns well with current ICLR interests in multimodal reasoning and embodied AI.\n\n2. The pipeline combines several components (object detection, segmentation, 3D reconstruction, multimodal retrieval) into a unified framework. The end-to-end system demonstrates practical functionality in simulated environments."}, "weaknesses": {"value": "1. Lack of originality and algorithmic contribution. The proposed CapNav mainly combines existing modules—YOLO/SAM for detection, CLIP/DINOv2 for embeddings, and LLMs for captioning and retrieval—under a new name.\n\n2. Writing and structure issues. The Introduction and Related Work sections are overly long and not clearly separated into paragraphs, making it difficult to follow the narrative flow. \n\n3. Experimental evaluation is insufficient. The experiments are conducted only in one Matterport3D scene, with a small number of instructions (9 episodes). Comparisons are limited to VLMaps without any other baselines; no ablation studies, robustness tests, or cross-domain evaluations are provided. Reported gains (71% vs. 52%) are not statistically analyzed, and the small scale of experiments limits generalization.\n\n4. The motivation for the paper is not sufficiently robust. The paper claim that \"People describe indoor destinations with attributes rather than names alone\", I haven't found any evidence to support this."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nKHEL6mTI0", "forum": "zZvrFzDkwF", "replyto": "zZvrFzDkwF", "signatures": ["ICLR.cc/2026/Conference/Submission11914/Reviewer_FZu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11914/Reviewer_FZu3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225265546, "cdate": 1761225265546, "tmdate": 1762922921728, "mdate": 1762922921728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CapNav, a \"description-first\" indoor navigation framework to address the limitation of existing VLN methods in handling fine-grained attribute-aware natural language instructions. CapNav constructs a 3D instance-level map with explicit attribute fields (color, shape, texture) via multi-view captioning and feature clustering (DINOv2, LongCLIP), parses natural language into \"category-attribute-relation\" constraints using a multimodal LLM, and verifies attribute consistency before navigation. Experiments on Matterport3D (Habitat simulator) show CapNav achieves 71.0% subgoal success rate, outperforming VLMaps (51.6%) by 37.6%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies the three core challenges of attribute combinatoriality, view dependence, and perception-planning decoupling in VLN, which are highly relevant to practical applications (e.g., home service robots).\n\n2. The multi-view fusion and pre-goal verification modules directly address the limitations of prior methods—qualitative results show CapNav can disambiguate similar objects (e.g., sofas with different edges) and provide interpretable decisions (traceable to captions).\n\n3. By leveraging LongCLIP and YOLO11x, CapNav handles unseen categories/attributes, which is more practical than closed-vocabulary VLN methods."}, "weaknesses": {"value": "1. Limited to Simulated Environments: All experiments are conducted on Matterport3D (simulated) without real-robot tests. Real-world noise (e.g., depth sensor error, dynamic objects, uneven lighting) may significantly degrade performance, but the paper does not discuss \"sim-to-real\" transfer—this contradicts the title \"towards robust indoor navigation\".\n\n2. Long-Horizon Navigation Ignored: The full-chain completion rate is only 20%, but the paper does not analyze the root cause (e.g., whether subgoal localization errors accumulate, or the planner fails to handle subgoal transitions). It also does not propose any improvements, treating this as \"future work\" without justification.\n\n3. Fixed Attribute Thresholds: Clustering and verification thresholds (e.g., τ_sem=0.65, τ_vol=0.15) are manually set. The paper does not evaluate the sensitivity of these thresholds to different scene clutter levels, nor does it implement the \"online calibration\" mentioned in future work—this makes the framework less adaptive to diverse environments.\n\n4. The paper only briefly mentions FindAnything (2025) and LERF (2023) but does not provide quantitative comparisons. For example, it claims FindAnything \"lacks attribute verification\", but no data is given to show how much CapNav outperforms it—this weakens the argument for CapNav’s superiority.\n\n5. The framework assumes static scenes but does not discuss how to update the 3D map when objects move (e.g., a pillow moved from the sofa to the floor). This is a critical limitation for real indoor navigation, where environments are often dynamic.\n\n6. Some related and important works are missing citations: [1] Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation [2] Bevbert: Multimodal map pre-training for language-guided navigation [3] Instruction-guided path planning with 3D semantic maps for vision-language navigation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n98uqO0q9V", "forum": "zZvrFzDkwF", "replyto": "zZvrFzDkwF", "signatures": ["ICLR.cc/2026/Conference/Submission11914/Reviewer_vyJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11914/Reviewer_vyJN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396009644, "cdate": 1761396009644, "tmdate": 1762922921352, "mdate": 1762922921352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a navigation system that enhances object localization by integrating attribute-based language grounding with visual perception. The system constructs a 3D instance-level map where each object is annotated with semantic embeddings and explicit attribute fields (e.g., color, shape, texture). Using foundation models like DINOv2 and CLIP, the method performs multi-view captioning and feature clustering to create consistent per-instance descriptors. A multimodal LLM is used to parse natural language queries into structured constraints, and a verification stage ensures attribute consistency before navigation. Experiments in simulated environments show that CapNav outperforms a baseline (VLMaps) on subgoal success rate (71.0% vs. 51.6%) in multi-object navigation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Unlike prior methods, CapNav treats attributes as first-class, queryable fields, improving fine-grained object disambiguation.\n2. Navigation choices are traceable to structured captions, aiding transparency and error analysis."}, "weaknesses": {"value": "1. The entire evaluation is conducted on only a single Matterport3D scene . This extremely narrow scope fails to demonstrate the system's performance and robustness across diverse indoor environments with varying layouts, lighting, object types, and clutter levels.\n\n2. The proposed method is compared against only one baseline, VLMaps. This is a critical flaw.\n\n3. The paper fails to justify design choices (e.g., weighting α=0.4/0.6 for embeddings) or isolate the contribution of each component."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3LcxLkbYBq", "forum": "zZvrFzDkwF", "replyto": "zZvrFzDkwF", "signatures": ["ICLR.cc/2026/Conference/Submission11914/Reviewer_Fwbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11914/Reviewer_Fwbc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921262934, "cdate": 1761921262934, "tmdate": 1762922921007, "mdate": 1762922921007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}