{"id": "tKz0XEaZXw", "number": 6037, "cdate": 1757951182642, "mdate": 1759897938615, "content": {"title": "Co-EditBench: Human-Aligned Benchmark for Instruction-Based Image Editing with Multi-Dimensional Assessment", "abstract": "Multimodal large language models (MLLMs) have made significant progress in instruction-guided image editing; however, comprehensively evaluating them in a way that aligns with human judgment remains a considerable challenge. Existing benchmarks often exhibit obvious limitations, including restricted editing types, limited evaluation dimensions, coarse perception of image details, and systematic deviation from subjective aesthetics. To overcome these issues, we proposed a more comprehensive evaluation benchmark, Co-EditBench, for human-aligned evaluation. First, we constructed a diagnostic dataset by crowd-sourcing, to obtain high-resolution, real-world image-instruction pairs covering 16 editing types. Then, to enable a fine-grained and consistent assessment, we define 11 novel evaluation dimensions that dissect “AI artifacts” into traceable visual pathologies. Additionally, we propose a comprehensive automated evaluation pipeline Co-EditEval that leverages multi-dimensional evaluators and a meticulously designed Chain of Thought for contextualized visual reasoning. Extensive experiments demonstrate that Co-EditBench provides a more reliable and nuanced evaluation than existing benchmarks, achieving a significant correlation with human judgments.", "tldr": "We present Co-EditBench, a benchmark for  image editing with over 1,000 image-instruction pairs, 11 evaluation dimensions, and a novel automatic evaluation pipeline.", "keywords": ["Image Editing; Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cedcc2ec46f9a2d38ff591003ad2193ce59cdf18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents **Co-EditBench**, a benchmark for evaluating multimodal large language models (MLLMs) in instruction-guided image editing. It addresses limitations in existing benchmarks by providing a diagnostic dataset with 16 editing types, 11 fine-grained evaluation dimensions, and an automated pipeline (**Co-EditEval**) leveraging contextualized visual reasoning. Experiments show Co-EditBench aligns strongly with human judgment, offering more reliable and nuanced evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive Evaluation Benchmark**: Co-EditBench introduces a diagnostic dataset covering 16 real-world editing types and defines 11 fine-grained evaluation dimensions, addressing gaps in existing benchmarks and enabling nuanced assessments.  \n\n2. **Human-Aligned Evaluation**: The benchmark strongly correlates with human judgments, ensuring evaluations reflect subjective aesthetics and detailed visual perception."}, "weaknesses": {"value": "1. **Incremental Evaluation Approach**: The evaluation method relies on weighted scoring based on existing feature similarity metrics and interpretable Chain-of-Thought reasoning. However, this approach feels incremental, as similar methodologies have been adopted by prior benchmarks like ImageEdit, Wise, and GenEval. The contribution in evaluation design is overstated and lacks substantial novelty.  \n\n2. **Regression to Outdated Metrics**: The inclusion of CLIP-based feature similarity metrics in the evaluation pipeline is concerning, as these metrics have been increasingly abandoned by recent works due to their lack of accuracy and reliability. While interpretable CoT-based evaluation is promising, relying on CLIP undermines the consistency and precision of the evaluation method.  \n\n3. **Missed Opportunity for RL-based Reward Models**: Instead of using feature-based similarity metrics, constructing RL-based reward models to validate multiple evaluation dimensions would provide a more scalable and interpretable solution. Such an approach would allow for checklist-based evaluations that are both flexible and robust, better aligned with current advancements in evaluation methodologies."}, "questions": {"value": "The absence of RL-based reward models for explainability and verification in this work raises concerns about the contribution of its evaluation methodology. RL-based reward models provide a modern and promising approach for validating image edits by offering both scalability and interpretability. Their ability to dynamically adapt to evaluation criteria ensures alignment with human reasoning and increases robustness across diverse editing tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SvmyBW4p9R", "forum": "tKz0XEaZXw", "replyto": "tKz0XEaZXw", "signatures": ["ICLR.cc/2026/Conference/Submission6037/Reviewer_xYjs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6037/Reviewer_xYjs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751679525, "cdate": 1761751679525, "tmdate": 1762918424140, "mdate": 1762918424140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Co-EditBench, a new benchmark for instruction-based image editing that features high-quality, manually verified masks distinguishing edited and non-edited regions. The central contribution is Co-EditEval, an automated evaluation pipeline designed to overcome the poor human alignment of existing metrics. Its key innovation lies in a Tailored Multi-dimensional Evaluation (TME) framework, which adopts a hybrid strategy: employing a CoT-guided MLLM for semantic evaluation and leveraging specialized perceptual metrics for fidelity assessment. Importantly, Co-EditEval incorporates region-aware fidelity computation based on the provided masks and aggregates 11-dimensional evaluation scores under a Completion-Guided principle. Empirical results demonstrate that the pipeline achieves a high Spearman correlation with human judgments, substantially outperforming previous benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper integrates semantic reasoning and fidelity evaluation, with mask-based region awareness improving over global metrics.\n2. The strong 0.889 SROC correlation and clear ablation results validate the necessity of both CoT and TME components."}, "weaknesses": {"value": "The main concern lies in the CoT-based evaluation strategy, which raises several issues:\n1. The evaluation relies on a closed-source model, i.e., Gemini 2.5-Pro, with CoT, which may be cost-prohibitive for other researchers, as it often requires extensive API usage. This could limit the benchmark’s accessibility and usage for other researchers.\n2. The computational and time costs of CoT-based evaluation are unclear. For example, how long would it take to evaluate 1,000 images?\n3. How does the pipeline handle hallucinations, given that CoT outputs are not always accurate, especially for fine-grained edits?"}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aQxdyEWMop", "forum": "tKz0XEaZXw", "replyto": "tKz0XEaZXw", "signatures": ["ICLR.cc/2026/Conference/Submission6037/Reviewer_hfE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6037/Reviewer_hfE2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757985213, "cdate": 1761757985213, "tmdate": 1762918423812, "mdate": 1762918423812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Co-EditBench, a new benchmark for evaluating instruction-based image editing models. It addresses key limitations of existing benchmarks, such as limited editing types, few evaluation dimensions, and weak alignment with human perception. The authors build a dataset of over 1,100 high-resolution image–instruction pairs covering 16 editing types, each with high-quality masks to separate edited and non-edited regions. They define 11 evaluation dimensions grouped into four areas: edit completeness, image quality, non-edited preservation, and identity preservation. The paper also proposes Co-EditEval, an automated evaluation pipeline that uses multiple evaluators (including MLLMs and similarity models) with a Chain-of-Thought (CoT) prompting strategy. Experiments on 25 recent image editing models show that Co-EditBench correlates better with human judgment than previous benchmarks like ImgEdit and GEdit."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive benchmark with rich diversity across editing types and evaluation dimensions.\n\n2. Strong motivation and clear identification of limitations in previous benchmarks.\n\n3. Methodologically sound data collection and annotation process, including human verification.\n\n4. Multi-dimensional evaluation design reflects real-world human judgment.\n\n5. Detailed experimental validation, including large-scale comparison and ablation studies.\n\n6. Good correlation between Co-EditEval scores and human ratings, demonstrating practical relevance."}, "weaknesses": {"value": "1. The paper is somewhat heavy in detail; simplifying explanations in the methods section could improve readability.\n\n2. Although the benchmark is extensive, the dataset’s accessibility and licensing terms are not fully clear (e.g., how others can use the crowd-sourced images).\n\n3. The evaluation pipeline relies on commercial MLLMs like Gemini 2.5-Pro and GPT-4o, which could raise reproducibility concerns.\n\n4. More discussion on potential biases in the crowd-sourced data and MLLM-based evaluators would strengthen the ethical transparency."}, "questions": {"value": "1. Will Co-EditBench and Co-EditEval be released publicly, and under what license?\n\n2. How consistent are the results when using different MLLM evaluators (e.g., GPT-4o vs. Gemini vs. Claude)? Can the evaluation benefit by using an ensemble and averaging the results? \n\n3. Did you consider the computational cost of the full evaluation pipeline? Could a lightweight version be used for faster benchmarking?\n\n4. Are there any plans to expand the benchmark to video editing or 3D scenes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hjIileLm84", "forum": "tKz0XEaZXw", "replyto": "tKz0XEaZXw", "signatures": ["ICLR.cc/2026/Conference/Submission6037/Reviewer_1qPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6037/Reviewer_1qPG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917839614, "cdate": 1761917839614, "tmdate": 1762918423430, "mdate": 1762918423430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets limitations of existing benchmarks for instruction-based image editing, including restricted editing coverage, limited evaluation dimensions, coarse perceptual alignment, and deviation from human preference. \n \nThe authors propose Co-EditBench, a human-aligned benchmark, and introduce an automated evaluation pipeline Co-EditEval.\n \nKey contributions include:\n1. A diagnostic dataset built via crowdsourcing, covering 16 editing types across images and text instructions.\n2. Definition of 11 fine-grained evaluation dimensions to diagnose “AI artifacts” based on edit completeness, image quality, non-edit preservation, and identity preservation.\n3. A multi-dimensional automated evaluation framework leveraging a completion-guided principle based curated aggregation, ensuring secondary metrics cannot exceed edit-completeness performance.\n4. Incorporation of chain-of-thought-based reasoning to improve contextual evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Fine-grained evaluation dimensions: \n\n    Defines 11 sub-metrics grouped into key categories (edit completeness, image quality, non-edit preservation, identity).\n\n2. Completion-Guided Principle: \n\n    Caps secondary scores based on the primary edit-completeness score to prevent superficial improvements from inflating scores.\n\n3. Human alignment verified: \n\n    Human evaluation by 34 annotators demonstrates a correlation between Co-EditEval and human judgment.\n\n4. Comprehensive benchmark results: \n\n    Evaluates 25 SOTA models, revealing widespread weaknesses under a rigorous evaluation setup."}, "weaknesses": {"value": "1. Metric weighting is partly subjective: \n\n    Although weights are defined, the rationale is neither well supported nor ablated.\n\n2. Generality concerns: \n\n    How Co-EditEval performs under unseen editing types is not fully explored. See question 2."}, "questions": {"value": "1. Customized generation:\n\n    Customized generation often involves significant redrawing. In this case, how should the mask be selected? Should the entire image be selected as a mask or...?\n\n2. In-context editing can involve reference images, implicit style transfer, or semantic relation changes between entities (e.g., “Match lighting with the reference image”).\n \n    Can Co-EditEval evaluate in-context editing scenarios where the notion of an edit region is not spatially well-defined?\n \n    Since IC-Edit [1] and RelationAdapter [2] target this type of in-context image editing, including them would help contextualize your benchmark’s applicability.\n\n    [1] Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\n\n    [2] Learning and Transferring Visual Relation with Diffusion Transformers"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fIHgZaTdNM", "forum": "tKz0XEaZXw", "replyto": "tKz0XEaZXw", "signatures": ["ICLR.cc/2026/Conference/Submission6037/Reviewer_q3dS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6037/Reviewer_q3dS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099007953, "cdate": 1762099007953, "tmdate": 1762918422960, "mdate": 1762918422960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}