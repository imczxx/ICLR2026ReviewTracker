{"id": "NNRQW01Xuh", "number": 13526, "cdate": 1758218919539, "mdate": 1759897430871, "content": {"title": "Caduceus: MoE-enhanced Foundation Models Unifying Biological and Natural Language", "abstract": "Multi-modality pre-training on protein sequences with textual descriptions has enabled general-purpose protein language models. However, as the property descriptions span heterogeneous domains, we observe a severe *data interference phenomenon*: distinct protein residues often target domain-specific annotations, revealing partially inconsistent functional mechanisms across sources, which substantially leads to degraded performance. This paper addresses this overlooked issue with a novel *Mixture of LoRA Experts (MoLE)* architecture, by efficiently fusing the knowledge across diverse property domains. Concretely, we introduce **Caduceus**, a family of MoE-enhanced foundation models built with a hierarchical pre-training paradigm to jointly integrate biological and natural language. Employing a property-guided gating router that assigns domain-specific protein tokens to different experts, the dual-granularity alignment approach reconciles signals across diverse functional mechanisms. To extend generalization beyond particular tasks, we further incorporate a multi-task instruction tuning phase, enabling robust protein parsing and natural language question answering. Extensive experiments on 15 benchmarks demonstrate that Caduceus mitigates the intrinsic data interference and consistently delivers the optimal performance. The instruction-tuned Caduceus-Instruct provides precise protein elucidation, significantly surpassing GPT-5, DeepSeek-V3, and Galactica-30B. We will make our model and source code publicly available.", "tldr": "", "keywords": ["Mixture of Experts", "Protein Multi-Modality Learning", "Instruction Tuning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adf0c5bf868a08b296aa1ab9a2bda6858434f548.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work propose that current multimodal pre-training combining protein sequences and diverse textual attribute descriptions suffers from severe data interference—inconsistencies in knowledge mechanisms across different attribute domains lead to degraded model performance. To address this, the authors propose Caduceus based on MoE enhancements.  Experiments are conducted to validate the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this work is clear.\n2. The problem identified is important: it clearly points out the data interference caused by textual descriptions of multi-attribute proteins, which has been overlooked in previous research on multimodal proteins.\n2. This work is well-written and easy to follow."}, "weaknesses": {"value": "1. The proposed method is very simple and lacks of novelty, applying MoE-Lora to Text-protein multimodal LLM.\n2. The improvement in Table 1 might be due to the addition of property desc during pre-training. \n3. Several of the baselines compared were pure sequence models such as GPT-5 and DeepSeek, while the baseline chosen for the QA task was relatively weak. Models like GPT inherently lack the ability to process biological sequences, so the better performance is understandable."}, "questions": {"value": "1. The analysis of the moe part seems inadequate; I think at least an ablation analysis of the number of experts and the rank of lora should be performed.\n2. Comparison with state-of-the-art methods: While comparisons have been made with many methods, a more refined comparison can be made with other recent excellent protein-text multimodal models rather than  pure sequence LLM, under the exact same settings to highlight MoLE’s unique advantages in addressing data interference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lGCtsiujj2", "forum": "NNRQW01Xuh", "replyto": "NNRQW01Xuh", "signatures": ["ICLR.cc/2026/Conference/Submission13526/Reviewer_UYrt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13526/Reviewer_UYrt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892846409, "cdate": 1761892846409, "tmdate": 1762924132328, "mdate": 1762924132328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Caduceus, a multimodal protein model that links biological sequences with natural language. It leverages on the Mixture of LoRA Experts framework, using a property-guided router to send protein tokens to specialized experts and reduce interference between biological domains. Training has two stages: dual-granularity alignment, which connects protein and text representations at both global and local levels, and instruction tuning, which enables question answering with a language model decoder. Caduceus achieves strong results on multiple protein and text benchmarks, showing that domain-specific routing and multimodal training improve performance and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation: Identifies and quantifies the “data interference” problem across protein property domains.\n\n- Effective adaptation: Extends the Mixture of LoRA Experts [1] with a biologically informed gating router that routes protein tokens by property.\n\n- Dual-granularity learning: Combines global and local protein-text alignment to capture both sequence-level and residue-level meaning.\n\n- Hierarchical pipeline: Two-stage training (alignment, instruction tuning) connects representation learning with natural language generative modeling.\n\n- Good empirical results: Achieves state-of-the-art performance across 15 benchmarks with solid ablations.\n\n- Clarity and presentation: Well-structured paper with intuitive figures and transparent methodology.\n\n[1] Wu et al., Mixture of LoRA Experts, ICLR 2024, https://arxiv.org/pdf/2404.13628"}, "weaknesses": {"value": "- Limited novelty: Core MoLE mechanism is based on prior work (e.g., [1], as already cited in this paper), so the main technical innovation of this paper seems to be the domain adaptation and the pipeline. I would suggest authors clarify and distinguish the main technical innovation of this paper given the prior work. \n\n- Concern about data overlap: Pretraining and evaluation datasets may share entries, risking leakage. I would suggest the authors discuss how they ensure there is not data leakage\n\n- Claim issue: In the conclusion, line 478, it is writen that \"we propose the Mixture of LoRA Experts (MoLE) to effectively...\". This may confuse some readers into thinking its a claim of introducing MoLE itself, while MoLE has already been introduced and studied before. The authors may want to rewrite this to avoid potential confusions/concerns. \n\n- [Minor] Concern related to fair comparison: Instruction-tuned model is compared to zero-shot general LLMs. While this is a reasonable baseline, this may not be a fair comparison.\n\n- [Minor] Gap in validation: It would be interesting to see causal or mechanistic tests (e.g., in silico mutagenesis, binding effect studies).\n\n[1] Wu et al., Mixture of LoRA Experts, ICLR 2024, https://arxiv.org/pdf/2404.13628"}, "questions": {"value": "1. Can the authors clarify and distinguish their main technical innovation given the prior work?\n\n2. Can the authors provide a discussion on how they ensure there is not data leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V71ax21W02", "forum": "NNRQW01Xuh", "replyto": "NNRQW01Xuh", "signatures": ["ICLR.cc/2026/Conference/Submission13526/Reviewer_x9cp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13526/Reviewer_x9cp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990648868, "cdate": 1761990648868, "tmdate": 1762924131963, "mdate": 1762924131963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper notices that existing protein language models struggle to deal with proteins' textual descriptions in heterogeneous domains. To mitigate this problem, this paper proposes a mixture-of-LoRA method to fuse knowledge of different domains. Specifically, this paper proposes a hierarchical pretraining method and property-guided gating router. The proposed multi-task instruction tuning also shows effectiveness on benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the paper is well written, with figures as visual illustrations. The Introduction section clearly explains the motivation behind the method. It also makes a comparison to existing methods and identifies their drawbacks.\n\n2. Using mixture-of-LoRA experts for multi-modal protein language modeling is novel to me, and experiments also show the effectiveness of the proposed method.\n\n3. Experiments are conducted on multiple benchmark datasets. Both quantitative and qualitative tasks are conducted to comprehensively show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Usually when we do experiments, we encourage authors to repeat the same experimental setting multiple times and report both mean and standard deviation. However, this paper shows mean but not stddev, which is difficult for readers to judge how significantly the proposed method outperforms baselines.\n\n2. Though this paper proposes an interesting method, it misses to mention and compare to a highly related existing work [1], which uses retrieval-augmented method to integrate knowledge graph with textual descriptions into proteins' amino acid sequences for a multi-modal representation learning.\n\n[1] Zhang, J., Zhang, D. C., Liang, S., Li, Z., Ying, R., & Shao, J. Retrieval-Augmented Language Model for Knowledge-aware Protein Encoding. In Forty-second International Conference on Machine Learning."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CgkO7pmKQs", "forum": "NNRQW01Xuh", "replyto": "NNRQW01Xuh", "signatures": ["ICLR.cc/2026/Conference/Submission13526/Reviewer_xRjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13526/Reviewer_xRjq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013485782, "cdate": 1762013485782, "tmdate": 1762924131599, "mdate": 1762924131599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Caduceus, a multimodal foundation model that unifies large language models and protein language models. Traditional approaches to fusing natural language and protein language domains often suffer from data interference issues. The authors attempt to mitigate this by adopting a mixture of experts (MoE) architecture, where the gating mechanism tries to distinguish between language tokens and protein tokens. Details of the alignment and instruction tuning processes are provided for the development of Caduceus. Experimental results demonstrate the effectiveness of Caduceus across both natural language and life science domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and well motivated overall.  \n- Unifying life science and natural language data modalities is an important research direction to pursue.  \n- The proposed Caduceus method is clearly presented and easy to follow.  \n- The performance of Caduceus appears to be promising."}, "weaknesses": {"value": "- The development process of Caduceus seems to heavily leverage the QA data described in Section 4.1, which makes it challenging for the model to scale.  \n- It is not stated in the paper whether the developed QA dataset will be publicly released for future research, which I strongly encourage the authors to do.  \n- When scaling the model size from 650M to 3B, the accuracy gains appear to be marginal, so I am uncertain about the scaling behavior of Caduceus."}, "questions": {"value": "- DNA sequence models have started to emerge recently, possibly as alternatives to protein language models. For instance, the recent release of AlphaGenome attempts to use DNA sequences to perform downstream tasks directly. I wonder whether Caduceus can be extended to also support DNA sequences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tROVbRV9Im", "forum": "NNRQW01Xuh", "replyto": "NNRQW01Xuh", "signatures": ["ICLR.cc/2026/Conference/Submission13526/Reviewer_6ui8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13526/Reviewer_6ui8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762667600097, "cdate": 1762667600097, "tmdate": 1762924130997, "mdate": 1762924130997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}