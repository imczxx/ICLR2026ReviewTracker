{"id": "A1JfGuXJvM", "number": 14860, "cdate": 1758244773548, "mdate": 1759897344838, "content": {"title": "EDISCO: Equivariant Continuous-Time Categorical Diffusion for Geometric Combinatorial Optimization", "abstract": "Geometric combinatorial optimization problems, such as the Traveling Salesman Problem (TSP), possess inherent symmetries under rotations, translations, and reflections in Euclidean space. These transformations are denoted as E(2). However, existing neural network-based approaches, including recent diffusion-based solvers, fail to exploit these geometric features. This paper presents EDISCO, to the best of our knowledge, the first diffusion-based framework combining E(2)-equivariant graph neural networks with continuous-time categorical diffusion models for solving geometric combinatorial problems. This approach introduces an equivariant score network that respects geometric transformations while operating on discrete edge variables, together with a continuous-time categorical diffusion process that maintains E(2) symmetries throughout the forward and reverse processes. By incorporating geometric awareness directly into the diffusion process, EDISCO achieves notable improvements over the baseline. EDISCO reduces the state-of-the-art TSP optimality gaps on TSP-500 from 0.12\\% to 0.08\\%, TSP-1000 from 0.30\\% to 0.22\\%, and TSP-10000 from 2.68\\% to 1.20\\%. EDISCO demonstrates strong generalizability across problem sizes and also shows remarkable efficiency, requiring only 33\\% to 50\\% of the training data compared to competing diffusion methods across all problem scales.", "tldr": "EDISCO solves geometric problems via equivariant GNNs and continuous-time diffusion, enabling faster inference through advanced solvers while respecting geometric symmetries for SOTA results.", "keywords": ["Diffusion Models", "Graph Neural Networks", "Combinatorial Optimization", "Equivariance", "Geometric Deep Learning", "Continuous-Time Diffusion"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e81bfbe4c68253143553021e1215bb57227b8862.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EDISCO, a diffusion-based framework for Geometric Combinatorial Optimization Problems (GCOPs) (e.g., TSP) that integrates two key innovations: E(2)-equivariant Graph Neural Networks (EGNNs) and continuous-time categorical diffusion. E(2)-equivariance ensures the model respects geometric symmetries (rotations, translations, reflections) of GCOPs, while continuous-time diffusion (modeled as Continuous-Time Markov Chains, CTMCs) enables adaptive numerical solvers and avoids discrete-time approximation errors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. EDISCO combines E(2)-equivariance with continuous-time categorical diffusion for GCOPs, filling a gap where prior diffusion solvers ignore GCOPs' inherent symmetries, which is novel and practical. \n2. The paper is well-structured and easy to follow.\n3. The experimental studies on TSP are relatively thorough and the performance appears promising."}, "weaknesses": {"value": "1. The evaluation scope is relatively limited. EDISCO is only evaluated on routing problems like TSP and CVRP. It would improve the quality of this work if the authors could provide some results on more types of GCOPs (**or at least a subset of**), e.g., node-focused tasks (MIS, Max Clique, Max Cut, Min Vertx Cover, etc., as studied by many NCO works [1-5]; note that DIMES/T2T/DIFUSCO (which you have included for comparison) have also evaluated their solvers on at least MIS), and VRPs with more complex constraints or asymmetric/binary TSPs. Also, current CVRP experiments only cover small scales (20–100 nodes) and lack comparison to more recent CVRP solvers like GLOP or PO[6], etc.\n2. This work seems to have followed the research line of generative neural CO solving, e.g., DIFUSCO and T2T. To my knowledge, some follow-up works have emerged with stronger performance or more applicable geometric CO problems evaluated, e.g., Fast-T2T (Li et al. NeurIPS 2024), COExpander (Ma et al. ICML 2025), DiffUCO (Sanokowski et al. ICML 2024), etc. Also, the RL or local constructive/unsupervised field has also fostered approaches that are more advanced than DIMES and POMO, e.g., BQ-NCO (Drakulic et al. NeurIPS 2023), UTSP (Min et al. NeurIPS 2023), UniCO (Pan et al. ICLR 2025), GOAL (Drakulic et al. ICLR 2025), BOPO (Liao et al. ICML 2025), etc. I suggest the authors provide more comparative results (**or at least considerable discussions**) in this regard. I have listed several [6-13] below for your reference.\n3. The main experiments are done on uniform TSP instances, while including (**at least a subset of**) supplementary evaluations on more diverse data distributions, e.g., Gaussian, cluster, explosive, implosion as in GLOP (Ye et al. AAAI 2024), are conducive to validating a better generalization capability of the proposed architecture and the training process. Also, I'm curious how EDISCO performs on ATSP cases, e.g., those defined in MatNet (Kwon et al. NeurIPS 2021), as 2D-coordinates are absent for input and the inherent symmetries are largely removed. Will these scenario shifts (occurring commonly in real-world applications) immensely harm the capability of the E(2)-equivariant tricks proposed in this paper?\n4. Regarding the adaptive mixing strategy, the linear weight function $w(t)=t$ and deterministic switching threshold $t<0.1$ are chosen via empirical results like a grid search but lack rigorous/theoretical justification for why they outperform the alternatives (those listed  and beyond, e.g., other instance-aware adaptive strategies (e.g., adjusting $w(t)$ based on node density or problem scale)).\n5. For the proposed equivariant architecture, it would be great if the authors apply it to replace the vanilla GNNs used in previous solvers so as to demonstrate the exact performance gain by advancing the neural backbone.\n\nReferences:\n\n[1] A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization (DiffUCO, ICML 2024)\n\n[2] Regularized Langevin Dynamics for Combinatorial Optimization (RLSA, ICML 2025)\n\n[3] Revisiting Sampling for Combinatorial Optimization (iSCO, ICML 2023)\n\n[4] Variational Annealing on Graphs for Combinatorial Optimization (VAG-CO, NeurIPS 2023)\n\n[5] Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets (GFlowNets, NeurIPS 2023)\n\n[6] Preference Optimization for Combinatorial Optimization Problems (PO, ICML 2025)\n\n[7] Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization  (NeurIPS 2024)\n\n[8] COExpander: Adaptive Solution Expansion for Combinatorial Optimization (ICML 2025)\n\n[9] BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization (NeurIPS 2023)\n\n[10] Unsupervised Learning for Solving the Travelling Salesman Problem (UTSP, NeurIPS 2023)\n\n[11] UniCO: On Unified Combinatorial Optimization via Problem Reduction to Matrix-Encoded General TSP (ICLR 2025)\n\n[12] GOAL: A Generalist Combinatorial Optimization Agent Learner (ICLR 2025)\n\n[13] BOPO: Neural Combinatorial Optimization via Best-anchored and Objective-guided Preference Optimization (ICML 2025)"}, "questions": {"value": "Please refer to the Weaknesses part where I've listed my major concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NybgYAUUvr", "forum": "A1JfGuXJvM", "replyto": "A1JfGuXJvM", "signatures": ["ICLR.cc/2026/Conference/Submission14860/Reviewer_ZQPE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14860/Reviewer_ZQPE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760863420196, "cdate": 1760863420196, "tmdate": 1762925212054, "mdate": 1762925212054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "``EDISCO`` introduces the first E(2)-equivariant continuous-time discrete diffusion model for geometric combinatorial optimization. The paper conducts extensive experiments on ``TSP`` and demonstrate in the appendix the feasibility of extending the model to ``CVRP``."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. ``EDISCO`` apply E(2)-equivariance to the TSP and achieves solid results. \n\n2. This paper also present an extension to the CVRP; although the performance is not outstanding, it remains a worthwhile attempt.\n\n3. ``EDISCO`` employs an adaptive mixing strategy to dynamically balance diffusion-based transitions with direct model predictions, and the authors also provide comparative results of alternative approaches.\n\n4. ``EDISCO`` conducts extensive ablation studies to demonstrate the importance of each component. Additionally, it performs robustness experiments on the training data."}, "weaknesses": {"value": "1. This paper lacks generalization experiments on different distributions, such as Gaussian and Cluster distributions of TSP, CVRPLIB for ``CVRP``.\n\n2. For the ``CVRP`` baseline, ``HGS`` might be a better choice.\n\n3. E(2)-equivariance assumption only holds for symmetric, Euclidean distances; its extensibility to non-symmetric problems such as ``ATSP`` deserves discussion."}, "questions": {"value": "1. The ``CVRP`` experiments do not report runtime; could you please provide it?\n\n2. How does the model learn the demand constraints in ``CVRP``? At present, constraints seem to be enforced only during the decoding phase. Moreover, in some ``CVRP`` instances, returning to the depot is not triggered solely by insufficient remaining capacity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lgEi5Dk3bT", "forum": "A1JfGuXJvM", "replyto": "A1JfGuXJvM", "signatures": ["ICLR.cc/2026/Conference/Submission14860/Reviewer_dpjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14860/Reviewer_dpjP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950623301, "cdate": 1761950623301, "tmdate": 1762925211622, "mdate": 1762925211622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EDISCO, the first diffusion-based framework combining E(2)-equivariant graph neural networks with continuous-time categorical diffusion models for solving the Traveling Salesman Problem (TSP). The key innovations are: (1) incorporating E(2) equivariance (rotations, translations, reflections) directly into the diffusion architecture using Equivariant Graph Neural Networks (EGNNs), (2) formulating edge selection as continuous-time Markov chains (CTMCs) enabling analytical tractability and compatibility with higher-order accelerated solvers, and (3) adaptive mixing strategy interpolating between stochastic transitions and deterministic selection. EDISCO achieves state-of-the-art performance, reducing TSP optimality gaps from 0.12% to 0.08% (TSP-500), 0.30% to 0.22% (TSP-1000), and 2.68% to 1.20% (TSP-10000), while requiring only 33%-50% of training data compared to competing diffusion methods and achieving 2-3x speedups with better quality or up to 25x speedups for real-time applications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Novel Technical Integration**: Combining E(2)-equivariant architectures with continuous-time discrete diffusion is interesting. The theoretical foundation is solid, which shows how geometric structure preservation throughout forward and reverse diffusion processes improves both sample efficiency and solution quality. Ablation (Table 3) confirms equivariance removal causes the most significant degradation (1.95%→3.58% on TSP-500, 2.85%→5.06% on TSP-1000).\n\n**Strong Empirical Results**: Consistent state-of-the-art performance across all scales (TSP-50 to TSP-10000). Particularly impressive on TSP-10000 with 1.20% gap (vs. 2.68% previous best) and average 0.088% gap on 29 TSPLIB instances (31.6% relative improvement over T2T). Cross-size generalization is excellent - models trained on TSP-1000 achieve <4.3% gap on all other scales.\n\n**Data Efficiency**: Requires only 33%-50% of training data compared to DIFUSCO/T2T while maintaining or exceeding their performance. Robustness to data quality is demonstrated - training on suboptimal Farthest Insertion solutions (7.5% gap) yields only 0.82% final gap vs. 2.75% for DIFUSCO. Achieves <0.07% gap with just 10% of training data on TSP-50."}, "weaknesses": {"value": "**Limited Problem Scope**: Evaluation restricted to TSP only. Missing: (a) other geometric COPs like Vehicle Routing Problem, Capacitated VRP, or Steiner Tree Problem, (b) non-Euclidean settings or different distance metrics (Manhattan, road networks), (c) real-world constraints (time windows, precedence, capacity limits), (d) comparison with state-of-the-art traditional solvers (Concorde, LKH-3) on wall-clock time and solution quality trade-offs. While TSP is canonical, single-problem evaluation limits generalizability claims.\n\n**Scalability Questions**: While TSP-10000 results are promising, paper doesn't address: (a) performance on even larger instances (100K+ cities common in real logistics), (b) memory requirements scaling - EGNN message passing is O(n²) for fully connected graphs, (c) how architecture depth/width should scale with problem size, (d) whether curriculum learning from smaller problems is always necessary (adds training complexity and time overhead). Tables 13-15 show results but lack analysis of computational scaling patterns."}, "questions": {"value": "**Problem Generalization**: How does EDISCO perform on VRP with time windows or capacitated routing? What architectural modifications are needed for constraints beyond tour connectivity?\n\n**Scalability Analysis**: What are the memory usage and inference time scaling curves for TSP-100 to TSP-10000?\n\n**Solver Comparison**: Compare wall-clock time and solution quality against LKH-3 and Concorde on standard benchmarks. Where does EDISCO fit in the quality-speed Pareto frontier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DcO2ncokyI", "forum": "A1JfGuXJvM", "replyto": "A1JfGuXJvM", "signatures": ["ICLR.cc/2026/Conference/Submission14860/Reviewer_B483"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14860/Reviewer_B483"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000195266, "cdate": 1762000195266, "tmdate": 1762925211137, "mdate": 1762925211137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EDISCO, a novel diffusion-based framework for Geometric Combinatorial Optimization Problems (GCOPs) like the TSP. The authors identify two major gaps in existing state-of-the-art diffusion solvers: (1) they are not E(2)-equivariant, meaning they fail to exploit the inherent rotational and translational symmetries of geometric problems, leading to poor sample efficiency; and (2) they rely on discrete-time diffusion, which limits inference flexibility and can accumulate errors. Towards the two challenges, the paper proposes a combination of two proposed techniques,  E(2)-Equivariant Score Network, and Continuous-Time Categorical Diffusion. Results show that it achieves new state-of-the-art results on large-scale TSPs (e.g., cutting the optimality gap on TSP-10000 from 2.68% to 1.20%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- EDISCO proposes an architectural solution. It builds the symmetry into the model by using an E(2)-equivariant GNN. This is a fundamental change to the network layers, and also new to the community.\n\n- State-of-the-Art Performance: The results are outstanding. EDISCO sets a new SOTA for diffusion-based solvers, particularly on large-scale problems.\n\n- Sufficient experiments especially  on the ablation studies."}, "weaknesses": {"value": "- The results of LKH in Table 1 seems unright (not good enough as seen in literature).\n\n- Hyperparameter Sensitivity: The appendices (F.6) reveal that the EGNN's architectural stabilizers (coordinate step size $\\alpha$ and temperature $\\tau$) are sensitive. Table 12 shows that an $\\alpha \\ge 0.2$ leads to unstable training or \"coordinate collapse\". While the authors found an optimal setting, this introduces new, non-trivial hyperparameters that must be tuned for stable and effective training.\n\n- Stronger baselines e.g. fast t2t are missing."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfYlTjNM3U", "forum": "A1JfGuXJvM", "replyto": "A1JfGuXJvM", "signatures": ["ICLR.cc/2026/Conference/Submission14860/Reviewer_6MUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14860/Reviewer_6MUB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012699787, "cdate": 1762012699787, "tmdate": 1762925210702, "mdate": 1762925210702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}