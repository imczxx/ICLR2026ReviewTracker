{"id": "pym3JRajmW", "number": 12326, "cdate": 1758207068659, "mdate": 1759897516917, "content": {"title": "LLM4EHR: Aligning Clinical Time Series with Medical Event Sequences via Large Language Models", "abstract": "Recent research in clinical machine learning, focusing on the intensive care unit (ICU), has shifted from bespoke supervised models to foundation models, utilising Large Language Models (LLMs). Here, LLMs are fine-tuned on mixtures of complex clinical data modalities, useful for various downstream tasks. However, existing methods do not sufficiently explore the shared temporal structure between the events on Electronic Health Records (EHRs) and clinical Time Series (TS) observations. This limitation potentially leads to less robust and adaptive clinical foundation models, resulting in reduced performance on downstream tasks. To fully exploit this temporal structure, we propose LLM4EHR, a new clinical foundation model trained on ICU data.\nCombining pre-trained LLMs with additional trainable layers, we fine-tune our model to temporally align the EHR and TS modalities. For this, we propose a regularised contrastive objective to jointly learn representations of EHRs and clinical TS. \nSupported by an ablation study, we find that embeddings from LLM4EHR improve performance on various downstream clinical tasks with competitive performance in a few-shot setting. Further, we empirically demonstrate that LLM4EHR learns transferable clinical TS embeddings that can be deployed to new cohorts with minimal performance loss. These findings provide a step towards building more generalisable and performant clinical foundation models.", "tldr": "LLM-powered Multimodal clinical foundation for learning entangled representations of EHR event sequences and clinical time series via contrastive learning.", "keywords": ["Electronic healthcare records", "Time series", "Large lanugage model", "Contrastive learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7adbb8ff6f388c0d56fae17f5921386e588e7ff3.pdf", "supplementary_material": "/attachment/38d8a7af4e944623f7373866d10e52e0c17fece9.zip"}, "replies": [{"content": {"summary": {"value": "​​LLM4EHR is a clinical foundation model that temporally aligns ICU time-series measurements with EHR event sequences using a frozen LLM encoder, patch-wise pooling, and a regularized contrastive objective to learn shared cross-modal representations. In experiments on mimic-iii and eICU, these embeddings improved downstream task performance and transferred to new cohorts with minimal performance loss."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, principled cross-modal alignment using an EHR-similarity-weighted contrastive objective that mitigates class collision.\n- The problem being solved matters, EHR event sequences and physiologic time series are usually modeled separately, losing crucial temporal context that impacts diagnosis, risk prediction, and treatment timing, aligning them can boost accuracy and generalization with fewer labels.\n- Practical temporal handling via non-overlapping time patches that bridge sparse EHR events and dense time-series data.\n- Stable use of a frozen LLM with only new clinical token embeddings trained, leveraging general semantics without drift.\n- Auxiliary autoregressive reconstruction preserves numeric fidelity of physiologic signals, with ablations showing its value.\n- Strong, consistent improvements across multiple ICU prediction tasks and solid cross-dataset transferability."}, "weaknesses": {"value": "- The few-shot claim is weakly supported; the experiments fine-tune on relatively large labeled cohorts and don’t show behavior at truly low-label regimes.\n- Length-of-stay performance lags specialized baselines, and the paper offers limited concrete strategies to mitigate this gap.\n- Heavy reliance on frozen LLM semantics for many new clinical tokens is unvalidated, there's no check that these learned token embeddings are clinically coherent.\n- The choice to normalize target-domain data with source statistics is not standard and could bias transfer results, no ablation compares against target-stats normalization."}, "questions": {"value": "- Why normalize target cohorts with source means/variance, can you provide an ablation with target stats normalization?\n- How are patches with no EHR events handled in the alignment losses and what are the gradient implications?\n-  How did you or would you validate the semantic quality of new clinical token embeddings, and would light adapter-tuning help?\n- I wonder how sensitive are results to patch size and do you support off-diagonal alignment to capture realistic delays between orders, administrations, and physiologic response?\n- Can you show learning curves for 1–5–10% labeled data or k-shot per phenotype to substantiate the few-shot claim?\n- Could you report calibration metrics (brier) and whether the EHR-weighted term improves or harms calibration versus the baseline objective?\nMinor:\n- in line 67, duplicated word: calculating\n- line 169-170, word predictive overused\n- line 346, missing rationale, why the sequences were truncated at 200h?\n- l. 486 should be ETHICS\n- Please fix table headers with mirco instead of macro, and marco -> macro"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "23c6ZtRfQP", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_yyS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_yyS2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528220945, "cdate": 1761528220945, "tmdate": 1762923251699, "mdate": 1762923251699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LLM4EHR, a multimodal clinical foundation model that jointly learns from electronic health record event sequences and clinical time series data. The authors argue that prior approaches fail to capture the temporal dependencies between these two modalities and propose a contrastive alignment objective that aligns TS observations with EHR event embeddings in a shared latent space. The model leverages pre-trained language model embeddings to regularize the contrastive loss and reduce class collision during training. LLM4EHR is trained on MIMIC-III and eICU datasets and evaluated across several downstream prediction tasks such as mortality, phenotyping, decompensation, and length of stay. Results show improved few-shot and cross-dataset generalization compared to baseline models. The paper concludes that the approach improves interpretability and transferability but faces scalability limitations due to memory demands of large LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Below are the strengths of the paper in my opinion:\n1. Proposes a clear methodological contribution for temporal contrastive alignment using LLM embeddings.\n2. Demonstrates consistent performance improvements across multiple clinical prediction benchmarks (mortality, phenotyping, decompensation, and length of stay) under few-shot and cross-dataset settings.\n3. Includes interpretability analysis showing improved consistency of learned embeddings.\n4. Evaluates on diverse datasets (MIMIC-III, eICU, Physio2012, and a private PICU dataset) with transparent experimental setup."}, "weaknesses": {"value": "The major weaknesses of the paper in my opinion follows:\n\n1. Offers an incremental contribution that primarily combines established contrastive and LLM-based methods rather than introducing a new paradigm.\n2. The \"foundation model\" claim is overstated given the limited dataset scale and scope of downstream tasks.\n3. Experimental analysis lacks depth (few ablations, no statistical significance reporting, limited robustness discussion).\n4. Scalability remains a limitation due to high computational requirements for larger LLMs. (Although it is already noted in the paper)\n5. Minimal qualitative or clinical validation beyond numerical benchmarks.\n6. There is no clear comparison against recent multimodal architectures that jointly model structured and unstructured EHR data (e.g., transformer-based fusion models).\n7. The current interpretability for such a clinical task lacks rigour and can be substantially improved."}, "questions": {"value": "1. Please provide precise LLM configuration and compute profile. Please specify the exact sequence length, tokenizer choices, and any truncation rules for EHR events or TS tokens. Also report pretraining steps, batch sizes, device count, total GPU hours, and peak memory. This will help assess scalability, which you list as a key limitation.\n2. You reformulate the commonly used contrastive objective to temporally align TS observations with EHR event sequences. Please formalize the positive and negative pair construction in time, the windowing or lag structure, and how you handle irregular sampling or missing TS. I think adding a figure or pseudo-code that shows how pairs are built over time would greatly improve this part.\n3. Tables show means with parentheses that are std. Please add significance tests for key claims (few-shot gains, cross-dataset transfer.\n(minor comment: For all tables please specify what the numbers in parentheses are (i.e., std).)\n4. You repartition to 70% self-supervised pretraining, 20% fine-tuning, 10% testing. I suspect you did this already but can you please confirm splits are at the patient level and that no patient overlap exists across partitions or datasets (especially since for these datasets a given patient might have multiple visits so the visit id defers but patient id is the same). Describe any harmonization across MIMIC-III, eICU, and PhysioNet to avoid label or feature leakage, especially for remaining length of stay.\n5. You state that, inspired by prior work, the model can make dynamic downstream predictions, such as an hourly mortality forecast. Yet the evaluation emphasizes classification. Can you please provide a forecasting setup with proper rolling origin evaluation, and reconcile this with the later statement that the method is less suitable for regression.\n6. Please enumerate baseline implementations and hyperparameter search spaces in the main text or appendix. Clarify whether recent multimodal fusion baselines were included, not just generic TS or EHR models, since your contribution is cross-modality alignment.\n7. I think adding calibration and other clinically meaningful decision metrics where applicable improves the paper. This will make the cross-dataset claims stronger, especially for mortality risk.\n8. For reproducibility, how will the PICU-dependent steps be handled. (I assume that private data is not going to be released).\n9. Can you please define how continuous TS vectors map to tokens, the discretization or projection used, and how variable-length episodes are handled?\n10. You note poorer regression performance. Please provide a short analysis of failure modes and whether alternative reconstruction losses, discretized targets, or ordinal objectives improved results.\n11. I think the description for the exact few-shot protocol can be improved by providing much more detail in the appendix such as shots per class, selection strategy, number of repeats, and how hyperparameters were tuned without peeking. This is important for interpreting \"few-shot\" gains.\nEvidence: “Few-shot evaluation on MIMIC-III” in the outline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wse7VVBH1E", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_qYea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_qYea"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945328142, "cdate": 1761945328142, "tmdate": 1762923251424, "mdate": 1762923251424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLM4EHR, a clinical foundation model that temporally aligns ICU EHR event sequences with numerical clinical time series (TS) to learn cross-modal patient representations. The method freezes a pre-trained LLM backbone and builds patch-level embeddings for EHR and TS, then trains with a regularized contrastive objective to align modalities, plus a TS reconstruction loss. Experiments on MIMIC-III and eICU, with transfer to PhysioNet 2012 and a paediatric PICU cohort, show consistent gains on classification tasks (phenotyping, decompensation, mortality) and competitive but not SOTA performance on remaining length of stay (LoS) regression. Ablations cover temperature τ, patch size (five-hour windows), and LLM backbone choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Originality: Introduces semantic-weighted contrastive alignment between EHR and TS at the temporal patch level, which is a meaningful extension of multi-modal contrastive learning in clinical settings. \n\n2.Quality: Broad evaluation (few-shot hints, in-domain and cross-dataset) and ablations (τ, patch size/backbone). Cross-dataset mortality results show robust transfer. \n\n3.Clarity: The training objective and data flow are well presented (overview figure, patching diagram, tables)."}, "weaknesses": {"value": "1.Regression performance / numerical fidelity: Remaining LoS performance is only competitive; the paper itself hypothesizes TS embedding distortion. Consider adding variable-level numeric reconstruction, distribution/quantile losses, or hierarchical multi-tasking to improve numerical fidelity and report the impact on LoS. \n\n2.Robustness of semantic weighting (ω): If EHR coding is sparse/noisy or mismatched across sites/ages, ω could mislead alignment. Please simulate label/semantic noise, compare against unweighted or asymmetric weighting schemes, and quantify degradation. \n\n3.Temporal alignment granularity: Fixed, non-overlapping five-hour patches may miss asynchronous or delayed effects common in ICU. Explore adaptive/learned patching, overlapping windows, or soft DTW-like temporal weights."}, "questions": {"value": "Q1 (critical): How robust is ω under coding-system changes or age-group shifts (adult ↔ paediatric)? Please report cross-site/cross-coding ablations or controlled noise experiments (e.g., token description perturbation, increased OOV rate).\nQ2: For LoS, does adding variable-level numeric reconstruction or quantile losses improve RMSE/R² without hurting classification? A small ablation in the appendix would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f9rKaS75oF", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_Xd8N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_Xd8N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762360518454, "cdate": 1762360518454, "tmdate": 1762923250942, "mdate": 1762923250942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes LLM4EHR on general ICU data. LLM4EHR is built on pretrained LLMs to embed unstructured EHR text data, as well as autoregressively recover time series values from their latent representation. The major contribution in LLM4EHR is the temporally aligned embedding of EHR text records and time series records, as well as an additional regularization loss term to address the issue of class collision. Empirically, the author demonstrates that the embeddings from LLM4EHR improve various downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The goal of this work, which is to improve the analysis of ICU data using both time series data and EHR data, has a significant impact and is beneficial to healthcare research.\n * The major novelty in the proposed work, aligning the embeddings of two modalities for a feasible contrastive loss, and an additional regularization loss utilizing the feature of LLM, is reasonable.\n * The experiments cover a variety of downstream tasks, showing both strengths and potential drawbacks of the proposed model."}, "weaknesses": {"value": "* The major concern is that the benchmark multimodal models are not state-of-the-art. For example, some more recent works also study EHR / clinical note + time series representation learning ([1] Ma, Yingbo, et al. \"Global contrastive training for multimodal electronic health records with language supervision.\" arXiv preprint arXiv:2404.06723 (2024).  [2] Wang, Fuying, et al. \"CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis.\" arXiv preprint arXiv:2411.00696 (2024). [3] Cui, Hejie, et al. \"Multimodal fusion of ehr in structures and semantics: Integrating clinical records and notes with hypergraph and llm.\" arXiv preprint arXiv:2403.08818 (2024).). A comparison between the proposed model and more recent multimodal EHR works will strengthen the work significantly. \n * There is no explanation on how the learned embeddings of time series data are used to perform the downstream tasks studied in section 5.2"}, "questions": {"value": "* In Figure 3b, the legend on the top right says  \"0 < w <= \"; something is missing there.\n* Equation 1 confuses me. If v and z are not aligned, then how can avgpool of the same kernel size & stride make aligned patches of v and z? For example, if v has a time length of 12 and z of 9, then a kernel of size 3 will give 4 patches of v and 3 patches of z. How are those patches further aligned?\n * In section 5.2, the paragraph says \"Decompensation and remaining LoS predictions were made hourly, and we evaluated the remaining LoS predictions in days, as in Sheikhalishahi et al. (2020).\" It is very confusing to read, and it will be clearer if the author adds the scale (hourly or daily) of remaining LoS predictions as Decompensation (hourly) in Table 3.\n * The model uses LLM to further embed the time series embedding (Figure 2). Is there any justification for this model design, other than that the AR generation of the next token can be naturally used to recover the time series data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7AC4jaLz6Y", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_2Zsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_2Zsg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762485630624, "cdate": 1762485630624, "tmdate": 1762923250302, "mdate": 1762923250302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLM4EHR, a framework designed to learn joint representations of two modalities in : structured EHR sequences  and numerical clinical time series. The core method is to use the LLM to extract embeddings for  EHR events and to then align them with the time-series data by optimizing a contrastive learning objective. Another innovation is to use the semantic similarity of EHR events to weight the contrastive loss for time series, aiming to mitigate \"class collision.\" The model is evaluated on downstream tasks on MIMIC, eICU, etc and shows superior performance compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The core idea of aligning EHR events and time series is a reasonable research direction. Moving beyond instance-wise alignment can be conceptually interesting. Additionally, the problem of better EHR data use is highly relevant. Developing strong foundation models for EHR data has great value for clinical AI. Additionally, the paper is generally well-structured, and the figures pretty illustrative. Finally, I like the few-shot and cross-dataset evaluation of the model."}, "weaknesses": {"value": "Please see questions. Additionally, I am an emergency reviewer, so I have not had the chance to read the paper in detail. If I have misunderstood or missed anything, please bring it to my notice."}, "questions": {"value": "How is the temporal alignment between EHR and the time series achieved/ how are the two modalities reconciled for patch creation. \nIf a patch contains 6 hours of time series but only 2 EHR events, what is the 'alignment'? What about 1 EHR event? \n\nThe model underperforms on the LoS task, which is supposed to be due to 'distortion in embeddings'  Why does a model designed for temporal understanding underperform on temporal regression problem?\n\nWhy LLM? If i understand correctly, the LLM is a feature embedder.  Comparing the A.8 results, it seems that the more powerful newer LLMs (like llama) do similar to old models like BERT/RobertA. This does not seem to be a framework which at core rely on a LLM knowledge or ability.  \nAdditionally for models like LLama, how were the embeddings obtained? Is it pooling tokens, using a pretrained MLP, etc. please give details.\n\nCan you add comparisons with a direct baseline that combines the embeddings from a pre-trained EHR LLM (like the one you used) and a pre-trained time series model. Is there other experiments that show this compute-heavy training method is better than such a simpler, more interpretable approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t1MUZzntlV", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_aL4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_aL4d"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762582087200, "cdate": 1762582087200, "tmdate": 1762923249470, "mdate": 1762923249470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model for numeric EHR data that integrates text annotations for some observed variables. It proposes a contrastive task to train this model using various learned layers attached to a frozen language model backbone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Integrating language embeddings into clinical TS models is relevant to study and could boost performance.\n- The use of contrastive training between reasonably well-aligned data is promising, given that other multimodal clinical data is challenging to use constrastive training on, due to its poor alignment (e.g., clinical notes versus vitals)."}, "weaknesses": {"value": "- The paper presents a separation between clinical time series and EHR entries that's unclear and doesn't reflect the nature of the data. These are not inherently distinct modalities: most EHR entries in the datasets being described are irregular samples of clinical time series, and are used as such in previous work (e.g., EBCL). For another example, the time series features in the Harutyunyan et al. MIMIC-III baseline are derived from chart events and lab events. While it's not clear how this paper understands chart events, it describes lab events as EHR entries even though they contain the same data as in an irregular time series representation. The practical relevance of this model's multimodality is therefore limited.\n- I couldn't find basic information about the architecture in the text, including what the \"Timeseries embedding\" and \"Timeseries decoder\" blocks in Figure 2 are and how predictions are generated for fine-tuning and inference.\n- Hyperparameter tuning for baseline models seems to be missing.\n- The main evaluation is limited, being a few-shot prediction with only one fraction of labelled training data evaluated. Full-shot results are not given. Since the tasks being evaluated were generated from EHRs without manual labelling, they aren't the kind of medical modelling tasks where few-shot capabilities are particularly relevant."}, "questions": {"value": "- While using a large language model for text embedding is standard, using one to embed time series with no text information seems awkward, and especially a frozen one. Why not use a time series embedding model instead, or at least train the transformer weights?\n- Can you explain the missing elements in Table 3? It's not apparent to me why instances couldn't be constructed for the corresponding models on an hourly basis. While the appendix indicates that EBCL was intended for sequence classification, I would note that their paper does include length-of-stay regression forecasting results.\n- Are there cases in the datasets of multiple episodes corresponding to the same patient, and if so, do you ensure that these remain in the same partition?\n- How are EHR entries used during fine-tuning and evaluation, for your method and for the other methods?\n\t- For one, EBCL is designed to use lab events and other features that are discussed as EHR entries in this paper. Were they provided to your EBCL implementation when training and evaluating it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7zWLiKvMDR", "forum": "pym3JRajmW", "replyto": "pym3JRajmW", "signatures": ["ICLR.cc/2026/Conference/Submission12326/Reviewer_6Vup"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12326/Reviewer_6Vup"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission12326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762817203479, "cdate": 1762817203479, "tmdate": 1762923248999, "mdate": 1762923248999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}