{"id": "kow0ZCJnwo", "number": 23787, "cdate": 1758348413662, "mdate": 1759896797303, "content": {"title": "Graph Recurrent Attention Networks for Solving Satisfiability Problems", "abstract": "In recent years, the use of deep learning for solving Boolean Satisfiability (SAT) problems has gained significant interest. This paper advances such neural-based methods by introducing a **G**raph **r**ecurrent **a**ttention **n**etwork for **SAT** (GranSAT). GranSAT employs two innovative steps to guide the network to search towards satisfaction of clauses: (1) evaluating the truth degree of each clause based on t-conorm fuzzy logic operators, and (2) updating assignments with attention mechanisms, closely aligning with distributed local search methods. Logical states are coupled with recurrently updated hidden states that are used to compute attention values, allowing the model to refine fuzzy assignments while retaining information from previous updates. Experimental results on crafted and random SAT benchmarks demonstrate that GranSAT outperforms existing neural SAT solvers in both performance and generalization. Furthermore, when combined with local search post-processors, GranSAT achieves state-of-the-art performance on random instances, showcasing its effectiveness in solving SAT problems.", "tldr": "", "keywords": ["Boolean Satisfiability", "Graph Neural Networks", "Graph Attention", "Recurrent Neural Networks", "T-conorm"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abd9127579a870acd044e277792095222f41a902.pdf", "supplementary_material": "/attachment/153b6675dbc7c35522bc8362d32935ecfac6042d.pdf"}, "replies": [{"content": {"summary": {"value": "This paper propose GranSAT, a GNN-based model for SAT solving. The soft assignments produced by the model can be refined by post-processing with a local search SAT solver. The experimental results show that GranSAT outperforms NeuroSAT and GAT-SAT on G4SATBench and achieves better results on SAT competition 2018 random track than the baseline solvers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The unsupervised training objective makes model training more efficient. \n2. Combining with classical solvers yields better performance. For example, GranSAT+NuWLS improves PAR-2 and reduces timeouts over strong local-search baselines on the SAT’18 Random track."}, "weaknesses": {"value": "1. Baselines appear dated. The comparisons focus on NeuroSAT and GAT-SAT, which are no longer state of the art among neural SAT solvers. More recent neural and hybrid approaches, SATformer[1], NeuroBack[2] and NLocalSAT[3] are not evaluated. Because these methods also use neural models both to act as neural solver and to guide CDCL or LS solving, comparisons against them should be important. \n[1] Satformer: Transformer-based unsat core learning\n[2] NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks\n[3] NLocalSAT: Boosting Local Search with Solution Prediction\n\n2. The experiment is not convincing enough. Experiments are limited to G4SATBench and the SAT’18 Random track, which are relatively easier. More evaluations should include the latest SAT Competition benchmarks."}, "questions": {"value": "1. Please justify the above weakness. \n2. I wonder in Table 2, do all three solvers include post-processing? or are they just used to predict assignments without any LS solvers, like NeuroSAT default settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nxd86rRJUk", "forum": "kow0ZCJnwo", "replyto": "kow0ZCJnwo", "signatures": ["ICLR.cc/2026/Conference/Submission23787/Reviewer_7m2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23787/Reviewer_7m2n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852941060, "cdate": 1761852941060, "tmdate": 1762942807135, "mdate": 1762942807135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GranSAT, a hybrid neural SAT solver based on Graph Recurrent Attention\nNetworks that operate on fuzzy logic variable assignments. Clause satisfaction is computed deterministically using t-conorm fuzzy operators (Gödel, Łukasiewicz, or Product), while variable assignments are updated via attention-based message passing combined with recurrent hidden states. This continuous relaxation of Boolean logic allows gradient-based learning while maintaining logical structure. When paired with local-search post-processing (NuWLS, MatSat), GranSAT achieves strong empirical performance on several benchmark families (G4SATBench and SAT Competition 2018)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "• Clear and mathematically rigorous formulation.\n• Elegant integration of fuzzy logic with attention and recurrence.\n• Strong empirical performance on multiple benchmarks.\n• Transparent experimental protocol and reproducibility."}, "weaknesses": {"value": "• Most components are well-known; innovation lies mainly in using fixed fuzzy operators.\n• The claimed relation between attention and local search is not supported by analysis.\n• Outdated baselines; unclear definition of “state of the art.”\n• Related Work section lacks critical comparison."}, "questions": {"value": "• Can the authors justify or empirically demonstrate the claimed connection between attention\nand distributed local search?\n• Does fixing clause updates via t-conorms lead to measurable advantages over learned\nupdates?\n• How sensitive is performance to the choice of t-conorm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RgQNTOWiwP", "forum": "kow0ZCJnwo", "replyto": "kow0ZCJnwo", "signatures": ["ICLR.cc/2026/Conference/Submission23787/Reviewer_ZKSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23787/Reviewer_ZKSC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929150318, "cdate": 1761929150318, "tmdate": 1762942806898, "mdate": 1762942806898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for solving SAT problems using a Graph Recurrent Attention Network (GranSAT).\nThe method:\n1.\tEvaluates each clause’s satisfaction state under fuzzy variable assignments using t-conorms.\n2.\tUpdates the variable assignments via attention mechanisms.\nGranSAT produces fuzzy assignments, which can be refined through local search–based post-processing methods.\nThe authors conduct experiments on various benchmarks and compare the performance of GranSAT with state-of-the-art neural and discrete SAT solvers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and easy to read.\n\nThe authors claim that\n•\tGranSAT outperforms existing neural SAT solvers \n•\tWhen combined with local search post-processors, GranSAT achieves state-of-the-art performance on random SAT instances."}, "weaknesses": {"value": "Due to GPU memory limitations, only 195 out of 255 instances were evaluated.\n\nWhile SAT problems are interesting, they are quite abstract. In my opinion, it would have been valuable to include some high-impact, real-world applications that can be formulated as SAT problems and solved using GranSAT."}, "questions": {"value": "•\tLine 194: Instead of using a t-conorm, could this mapping be replaced with a simple neural network whose weights are trained jointly with the Graph Neural Network?\n\n•\tLine 233: In many applications, Transformer- or Mamba-based architectures outperform GRU and LSTM models. Could these architectures be tried here, replacing the GRUs?\n\n•\tHow sensitive is GranSAT to hyperparameters? I understand that in the experiments, the hidden/logical state dimension was set to 32 and the number of attention heads to 4. Were these values found to be optimal? How much do the results vary with different parameter choices?\n\n•\tIt is interesting that in the experiments, GAT-SAT performs best on the hard-CA dataset but completely fails on some of the other tasks.\n\n•\tWhat were the wall-clock training times for the experiments?\n\n•\tBased on Appendix B, my understanding is that the authors used four GPUs. How were the training tasks distributed across these GPUs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ZpeQcVtuD", "forum": "kow0ZCJnwo", "replyto": "kow0ZCJnwo", "signatures": ["ICLR.cc/2026/Conference/Submission23787/Reviewer_DgoC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23787/Reviewer_DgoC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955503282, "cdate": 1761955503282, "tmdate": 1762942806732, "mdate": 1762942806732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes, GranSAT, a graph neural network-based approach to solving the classic satisfiability problems. GranSAT uses the graph recurrent attention networks and alternates two updates: 1) clause's satisfaction state update using t-conorms; 2) variable assignment update using attention mechanisms. GranSAT can be used as a standalone solver and integrated with local search methods for further post-processing. Experimental evaluations are performed on the standard G4SATBench, which was designed to evaluate GNNs on sat solving, and GranSAT outperforms two baselines, i.e., NeuroSAT and GAT-SAT. When integrated with local search (NuWLS), GranSAT is close to or slightly outperforms state-of-the-art local search solvers such as Sparrow2Riss and YalSAT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- the paper is generally well-written; particularly, visual illustrations are very helpful for conveying the essential idea\n- the targeted problem, Boolean satisfiability, is of great importance, and relevant background about graph neural networks and attention mechanism are properly addressed"}, "weaknesses": {"value": "- the proposed method is fairly incremental, given that there are numerous attempts of applying graph neural networks for sat solving since the seminal work NeuroSAT (2019). \n- the chosen baselines (i.e., NeuroSAT and GAT-SAT) are quite limited, omitting many important baselines such as DG-DAGRNN, NLocalSAT, QuerySAT, Graph-QSAT, NSNet, to name a few. \n- the evaluation results of G4SATBench are inconsistent with the original evaluation of NeuroSAT\n- problems of SAT Competition 2018 seem to be quite outdated\n- the highlighted application for local search is not promising, i.e., the performance is close to the baseline NeuroSAT and there is a significant gap from the state-of-the-art  local search solver"}, "questions": {"value": "The accuracy numbers reported in this work are significantly different from the evaluation in the G4SATBench, particularly the results of NeuroSAT. Why is there such a big difference?\n\nWhy is SAT competition 2018 used, instead of the recent competitions (SAT Competition 2024)?\n\nMinor writing issues: \n- Notations like $e_{ij+n}$ is confusing, which may refer to $e_k$ where $k=i*j+n$ or $e_{i, j+n}$. \n- Furthermore, $h_j$ and $h_{j+n}$ are also confusing. What are possible values of $N_i$? Shouldn't $h_{j+n}$ be used when considering $j \\in N_i$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jTh7TAvxM6", "forum": "kow0ZCJnwo", "replyto": "kow0ZCJnwo", "signatures": ["ICLR.cc/2026/Conference/Submission23787/Reviewer_GJ9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23787/Reviewer_GJ9i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286236578, "cdate": 1762286236578, "tmdate": 1762942806495, "mdate": 1762942806495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}