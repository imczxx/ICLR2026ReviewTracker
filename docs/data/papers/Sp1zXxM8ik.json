{"id": "Sp1zXxM8ik", "number": 9530, "cdate": 1758126201524, "mdate": 1759897714023, "content": {"title": "Accelerated Gradient Descent for Faster Convergence with Minimal Overhead", "abstract": "In this paper, we present CT-AGD (Curvature-Tuned Accelerated Gradient Descent), an optimization method for non-convex optimization problems in deep learning training tasks. CT-AGD is a general boosting procedure that accelerates first-order methods by explicitly capturing the local curvature using finite-difference quotients, and the development of heuristics aimed at mitigating noise and bias introduced by stochastic mini-batch training. CT-AGD has a comparable storage and computational overhead as adaptive gradient methods such as Adam. Our extensive experiments demonstrate that CT-AGD achieves the same level of accuracy as the baseline first-order methods, yet reduces the required training epochs by 33% on average.", "tldr": "CT-AGD, an optimization method for non-convex problems in deep learning training tasks.", "keywords": ["optimization", "training", "SGD", "gradient descent", "BFGS"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e90232951b4458595c03a77904f7475c37a41cc9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces **CT-AGD (Curvature-Tuned Accelerated Gradient Descent)**, a boost technique for first-order optimizers (e.g., SGD, Adam). During each epoch, it runs standard first-order updates while accumulating per-coordinate finite-difference quotients as diagonal curvature proxies; at the epoch boundary it applies one extra preconditioned step using the clipped/quantile-robust diagonal. Experiments on CIFAR-10/100 and Tiny-ImageNet with ResNet, Wide-ResNet, and DeiT report comparable accuracy with ≈33% fewer epochs on average."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The approach is architecturally simple and integrates as a once-per-epoch preconditioning step on top of standard first-order methods (e.g., SGD, Adam). This makes it easy to extend in existing training pipelines without major code or infrastructure changes."}, "weaknesses": {"value": "## Major\n\n1. This paper reads like a recycled submission from several years ago. The reviewed and compared works are largely outdated despite rapid progress in optimizers. For an ICLR 2026 submission, the related work should at least minimally cover recent advances (2023–2025). As concrete evidence: there are zero 2025 citations, only one 2024 citation of a handbook [1], no 2023 citations, and just one 2022 citation [2], and that 2022 citation is a background article on carbon cost rather than an optimization paper. In the Related Work section, all cited papers are 2021 or earlier, which signals a significant gap in coverage of the modern optimizer landscape. Please at least update the discussion to include contemporary adaptive, curvature-aware, and large-scale training optimizers for submission.\n\n2. Section 2’s exposition on neural network structures and backpropagation feels overly general and not directly relevant to the proposed method. Consider trimming or removing this material to keep the problem statement focused.\n\n3. The theoretical setup is oversimplified. Convergence is proved only when the objective is an average of convex functions. For relevance to deep learning, a non-convex analysis under standard assumptions (e.g., $L$-smooth objectives) is expected, as is common in optimizer literature.\n\n4. The convergence results offer limited insight. The analysis assumes a diagonal preconditioner with entries clamped to $[\\lambda_{\\min}, \\lambda_{\\max}]$ but does not justify why or when this diagonal meaningfully approximates the Hessian structure. Without such justification, the theory does not explain why the optimizer should be useful in practice.\n\n5. The memory footprint appears large (≈ $5d$ states). This makes the method unlikely to be practical for large-scale neural network training, where memory pressure is already severe.\n\n6. The experiments are mostly at toy scale (e.g., CIFAR-10 with ResNet-20), which limits the external validity of the claims. It remains unclear whether the method is useful in realistic large-model scenarios.\n\n7. CT-AGD is not consistently competitive. In several cases, it is surpassed by vanilla SGD (see Figure 2 and Table 4), which is generally regarded as a weak baseline in modern deep learning optimization.\n\n8. The method introduces numerous hyperparameters, $\\eta_1$, $\\eta_2$, $\\lambda_{\\min}$, $\\lambda_{\\max}$, percentile $\\omega$, on top of Adam’s own settings, but there is no systematic guidance or ablation on how to choose them or how sensitive performance is to these choices.\n\n## Minor\n\n- Lines 139–140: the statement “Structured approximations (e.g., block diagonal and Kronecker factorizations) and stochastic quasi-Newton variants narrow the gap...” needs citations; e.g., KFAC [3].\n- Figure 1 (right): the title says “accuracy vs. epoch,” but the x-axis is iterations. Please correct the label or the title to avoid confusion.\n- The term “accelerated” is typically associated with Nesterov’s accelerated gradient; since the proposed method is unrelated, please explicitly clarify that there is no connection to Nesterov-style acceleration.\n\n## References\n\n[1] Guillermo Garrigos, Gauthier Gidel, and Aymeric Dieuleveut. *Handbook of convergence theorems for (stochastic) gradient methods.* arXiv:2301.11235, 2024.  \n[2] David A. Patterson, Joseph E. Gonzalez, Urs Hölzle, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. *The carbon footprint of machine learning training will plateau, then shrink.* *Computer*, 55(7):18–28, 2022.  \n[3] Martens, James, and Roger Grosse. *Optimizing neural networks with Kronecker-factored approximate curvature.* ICML, 2015."}, "questions": {"value": "1. Why can Equation 16 be regarded as a Hessian approximation? Please provide a minimal but clear explanation or derivation that links the quotient-based estimate to a diagonal Hessian model, including assumptions under which this interpretation holds.\n\n2. The scaling coefficient for the next epoch appears ad hoc. How does this heuristic compare to Levenberg–Marquardt style adaptive damping in terms of stability, convergence speed, and sensitivity to hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZwBI86DFrL", "forum": "Sp1zXxM8ik", "replyto": "Sp1zXxM8ik", "signatures": ["ICLR.cc/2026/Conference/Submission9530/Reviewer_DdLg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9530/Reviewer_DdLg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760760321189, "cdate": 1760760321189, "tmdate": 1762921094129, "mdate": 1762921094129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CT-AGD, a novel procedure to \"boost\" first-order optimizers like SGD and Adam for non-convex deep learning tasks. The method's core mechanism is the direct estimation of a diagonal curvature (Hessian) proxy using finite-difference quotients (\\Delta g / \\Delta \\theta) accumulated during each epoch. This estimate is then ingeniously used in two ways: 1) as a pre-conditioner for a single, additional update step at the end of the epoch, and 2) to compute a scalar learning rate modulator, \\gamma_k, that scales the intra-epoch steps of the next epoch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors present extensive experiments showing that CT-AGD achieves a significant average reduction of 33% in the number of epochs required to reach a target accuracy, while achieving a final accuracy comparable to the baseline optimizers."}, "weaknesses": {"value": "Despite the promising empirical results and the cleverness of the algorithm's design, the paper in its current form suffers from several fundamental weaknesses: \n\n1. The theoretical analysis is critically disconnected from the method's application, providing convex guarantees for a non-convex problem. The algorithmic design is a complex amalgamation of non-trivial heuristics (clamping, weighting, annealing, quantiles) presented without any ablation studies to justify their inclusion or necessity. Finally, the paper's claims of \"minimal overhead\" are questionable, as the method introduces significant optimizer state memory overhead compared to Adam and a non-trivial number of new, sensitive hyperparameters.\n\n2. The novelty of CT-AGD lies in its specific, dual-use of a direct, secant-based diagonal curvature estimate. However, the stability of this estimate is the central challenge, and the paper attempts to solve it by introducing a cascade of heuristics. The lack of justification for these design choices is a major flaw.\n\n3. Instability of the Curvature Estimate: The core of the method is the quotient h_{k,t} := \\Delta g_{k,t} / \\Delta \\theta_{k,t}. In a stochastic setting, this is notoriously unstable. Both the numerator (gradient change) and denominator (parameter change) are noisy. The paper's primary defense against this is a validity mask m_{k,t} that only checks for |\\Delta\\theta_{k,t}| > \\epsilon. This prevents division by zero but does not protect against a very small, non-zero \\Delta\\theta_{k,t}, which would cause the quotient to explode. The subsequent clamping \\Pi_{[\\lambda_{min},\\lambda_{max}]}  is a hard, ad hoc fix for this instability, not a principled solution.\n   \n4. Lack of Ablation Studies: The algorithm is a collection of clever but unverified engineering tricks. The paper provides no ablation studies to demonstrate that this specific combination is necessary or optimal.\n  \n4.1 t-weighting (Eq 16 and Eq 19): The weighted average gives priority to later steps10. Why is this linear t-weighting superior to a simple, unweighted average or a more standard exponential moving average?\n      \n4.2 \\gamma_{k,t} Annealing (Eq 12): The curvature-aware divisor \\gamma_k is linearly annealed back to 1.0 over the course of the next epoch. The justification that the estimate \"loses fidelity\" is intuitive but hand-wavy. What happens if this annealing is removed and \\gamma_{k,t} = \\gamma_k is held constant? What if the annealing is exponential instead of linear? This is a core component of the method and it is completely unevaluated.\n     \n4.3 Quantile Choice (Q_{\\omega}): The use of a low-tail quantile (\\omega=0.1) to compute \\gamma_k is another heuristic to provide robustness. How sensitive is the method to this choice? What if \\omega=0.5 (median) or \\omega=0.01 were used? This introduces a critical new hyperparameter without analysis.\n     \n5. New Hyperparameters: The method is presented as a \"booster,\" but it introduces at least four new, sensitive hyperparameters: \\eta_2 (secondary learning rate), \\lambda_{min}, \\lambda_{max} (clamping bounds), and \\omega (quantile percentile). The authors themselves admit that in cases where CT-AGD is slower, the solution is to tune these parameters, which undermines the \"plug-and-play\" implication."}, "questions": {"value": "see my comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j2zRqWKhbI", "forum": "Sp1zXxM8ik", "replyto": "Sp1zXxM8ik", "signatures": ["ICLR.cc/2026/Conference/Submission9530/Reviewer_FzcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9530/Reviewer_FzcT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373680454, "cdate": 1761373680454, "tmdate": 1762921093666, "mdate": 1762921093666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is concerned with accelerated gradient descent. The core idea of the paper is to make use of some estimation of the local curvature of the function to minimize to accelerate the speed of convergence."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The considered problem is a main bottleneck in deep learning. How to accelerate the learning phase ? First order stochastic methods such as SGD  (a famous variation beeing ADAM algorithm) have proven to be the most interesting type of methods. However, there is still place for improvements in term of speed, theoretical guarantees, ...\n\n* Making use of the local curvature to accelerate first order methods is a sound idea. This is for instance the main ingredient in the famous L-BFGS algorithm. This can be seen as an automated time step algorithm.\n\n* Estimation of the local curvature can be computationally costly. This is the main reasons why heuristics are introduced in this paper to get a good approximation without too much computations."}, "weaknesses": {"value": "* The paper mainly relies on a heuristic to estimate the local curvature. However, the theoretical results of the paper are weak the proof of Theorem 2.1 seems to be a direct adaptation of one of the proofs of the review paper, once the estimation of the Hessian has been clipped, see\nHandbook of Convergence Theorems for (Stochastic) Gradient Methods by Garrigos and Grower \nhttps://arxiv.org/pdf/2301.11235\n\n* The paper claims to solve all the deep learning problems. However, the setting of Theorem 2.1 is the convex setting, which of course is an assumption that is not satisfied in practice.\n\n* The writing of the paper is quite poor. Notations change from one section to another. $\\bar{x_N}$ in Theorem 2.1 does not seem to be even defined in the main part of the paper.\n\n* The authors have acknowledged having used LLMs for bibliography reasearch. This is probably the reason why they cite the archiv preprint by Garrigos and Grower\nhttps://arxiv.org/pdf/2301.11235\nas beeing authored by Garrigos, Gidel, and Dieuleveut.\nThe archiv preprint number they give in reference is indeed the one by Garrigos and Grower. I could not find any archiv preprint by Garrigos, Gidel and Dieuleveut, so this is probably a nice example of hallucination of their LLM.\nI think the authors should be very cautious when using LLMs for bibliography research, and at least double check the LLMs results."}, "questions": {"value": "* By clipping the estimation of the Hessian, you ensure the existence of $\\gamma_\\min$ and $\\gamma_\\max$ at each iteration.\nHowever, how do you ensure that the bounds are uniform through the iterations (i.e. that they do not depend on $k$) ?\n\n* There is an inner loop in the algorithm (to estimate the local curvature). The proof of convergence assumes that no error is mase in this inner loop. How robust is the algorithm to errors within this inner loop ?\n\n* What is the intuitive meaning of equation (26) in Theorem 2.1 ? What is the behavior of the sums of $a_l$ and $b_l$ ? Is this a fast convergence ?  \nThe convergence rate is an ergodic convergence rate. What can be said about $x_N$ ?\n\n* There are exeperimental results on 3 data-sets. Since the claim of the paper is that it accelerates over ADAM, I think that there should be more experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nMR0aWMNmR", "forum": "Sp1zXxM8ik", "replyto": "Sp1zXxM8ik", "signatures": ["ICLR.cc/2026/Conference/Submission9530/Reviewer_a4gh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9530/Reviewer_a4gh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760075495, "cdate": 1761760075495, "tmdate": 1762921093300, "mdate": 1762921093300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops what is called a Curvature-Tuned Accelerated Gradient Descent technique for deep learning, CT-AGD for short. The heuristic technique is a cheap and clever approach to capture the direction of local curvature via first-order differencing and the use of other clever ideas. The authors claim this approach mitigates noise and bias introduced by stochastic mini-batch training. They also claim the heuristic technique introduces minimal storage and computational overhead to the underlying stochastic gradient algorithms. Experiments on Vision tasks were used to show effectiveness at similar level of accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the strength of the paper which stands out to me, is the cleverness of the technique, especially in the way they combine different ideas that form the technique."}, "weaknesses": {"value": "1. The paper, like lot of papers follow the trend of interpreting their techniques as preconditioners, when they really are not.\nThey justify this by using the term 'first-order methods' for the stochastic gradient algorithms under consideration.\nHowever, closely looking at these techniques, including the current paper, they can more simply and better viewed as a choice of iterative learning rates, without overclaiming.\nIn the case of this paper, I think the preconditioning interpretation is one of its weaknesses. The preconditioning framing of the paper while recently popular can be extremely misleading and often obscures the simplicity of the methods, and detracts from intuitive understanding\n\n2. Another weakness lies in the treatment of the interval thresholds $\\lambda_{\\min}$ and $\\lambda_{\\max}$ which are passed off as bounds of the diagonal entries of the true Hessian. These thresholds are not estimated, even if unknown. Instead, they are chosen as hyperparameters which introduces a degree of arbitrariness and opens up a wide range of possibilities that limits the robustness and generalizability of the technique in relation to the underlying methods when compared with another well-tuned learning rate (iterative or fixed). \n\n3. The last weakness is the claim that the proposed technique mitigates noise and bias introduced by stochastic mini-batch training. But these is more attributed to the role of gradient smoothing, which has been called momentum.  Even without momentum, the stochastic gradient algorithm is competitive in these kinds of CNN architecture experiments. In addition, the extent to which the technique boosts the underlying method is marginal."}, "questions": {"value": "Nil"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VPOjYOGvrK", "forum": "Sp1zXxM8ik", "replyto": "Sp1zXxM8ik", "signatures": ["ICLR.cc/2026/Conference/Submission9530/Reviewer_pC3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9530/Reviewer_pC3E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868944882, "cdate": 1761868944882, "tmdate": 1762921093001, "mdate": 1762921093001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}