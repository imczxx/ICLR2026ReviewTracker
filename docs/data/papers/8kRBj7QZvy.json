{"id": "8kRBj7QZvy", "number": 24077, "cdate": 1758352433217, "mdate": 1759896782844, "content": {"title": "Swap-and-Spoil: Untargeted Byzantine Attacks via Class-Consistent View Swaps in Vertical Federated Learning", "abstract": "Vertical Federated Learning (VFL) secures a highly privacy-preserving multi-party training paradigm in which features are vertically distributed across participants for the same sample space. Security attacks against VFL have been gaining attention recently, but most discussions revolve around data poisoning attacks, particularly backdoor attacks. Byzantine attack against a federated learning system can target the main model performance and drop its accuracy with a single adversary participating in the training. While such untargeted Byzantine attacks have been explored in horizontal settings, they still remain underexplored in vertical settings of federated systems.\n\nIn this paper, we demonstrate how an adversary can mount a successful untargeted Byzantine attack that drives down the global model’s inference-time accuracy. To realize this, we perform a consistent cluster-based swapping in the feature space, creating a persistent and poisoned cross-view association during training. The model internalizes this adversary-induced association and, when evaluated on clean, correctly aligned data, fails dramatically. We also show that, the widely-practiced defenses in VFL fail to detect the attack without degrading the model performance. Through this endeavour, our findings establish untargeted Byzantine attacks as a real, underexplored threat to VFL and motivate the design of robust, VFL-specific defenses.", "tldr": "", "keywords": ["Byzantine Attacks", "Vertical Federated Learning", "Federated Learning Security", "Untargeted Poisoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c38ace895d08f8addd87ff2c9b89f46860807c3a.pdf", "supplementary_material": "/attachment/7006d4e807fbea72028a08890b281b3de8d2a3a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces \"Swap-and-Spoil,\" a novel untargeted Byzantine attack specifically designed for VFL. The main contribution is the attack exploits VFL's structure by implementing a class-consistent view swap among malicious participants. This method effectively corrupts the overall model accuracy without specific targeting."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Unlike traditional data poisoning, the paper introduces a novel Byzantine attack mechanism specific to VFL’s structure: a class-consistent view swap. This approach is highly original. It reveals a new vulnerability in VFL systems by presenting an attack that is stealthy and hard to detect with existing defenses. This makes it an important benchmark for developing more robust defenses and gives it significant practical value. The proposed attack mechanism is clearly explained, and experiments support that it effectively degrades overall model performance."}, "weaknesses": {"value": "While the effectiveness of the attack is demonstrated, the paper lacks comparisons with random swap attacks and evaluations of resistance against existing robust aggregation algorithms. Figures and tables, especially Table 5, overflow their boundaries and there are numerous grammatical and spelling errors, necessitating thorough revisions. The definition and implementation of the core concept of “class consistency” should be described more clearly, both mathematically and intuitively."}, "questions": {"value": "Q1: Can the authors define the “class-consistent view swapping” strategy more formally, either mathematically or algorithmically (for example with pseudocode)?\nQ2: Does an adversary need access to the true class labels of the training samples to carry out the attack?\nQ3: By how much does “class-consistency” improve the attack success rate (i.e., the drop in downstream model accuracy) compared to a simple Random Swap Attack (RSA)?\nQ4: Can you empirically demonstrate and quantify how effective the attack remains when a VFL system employs established robust aggregation mechanisms such as Krum, Trimmed Mean, or Median?\nQ5: How does the attack’s effectiveness change when the number of participants is much larger (for example N > 10)? \nQ6: How is the attack affected on datasets where features are distributed across clients in a more complex and heterogeneous way (for example, with high inter-feature correlation or imbalanced feature sets)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ivMiyDbd74", "forum": "8kRBj7QZvy", "replyto": "8kRBj7QZvy", "signatures": ["ICLR.cc/2026/Conference/Submission24077/Reviewer_UYHT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24077/Reviewer_UYHT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295923887, "cdate": 1761295923887, "tmdate": 1762942925962, "mdate": 1762942925962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a new class of untargeted Byzantine attacks in Vertical Federated Learning (VFL), an area where prior work has mainly focused on targeted backdoors or label inference. The authors propose a cluster-consistent feature-swapping attack that poisons the joint feature representation during training. The attack operates in two stages: first, the adversary uses limited label information to infer latent class structure via semi-supervised clustering, and second, it performs class-consistent feature swaps between clusters to create persistent cross-view misalignments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel attack concept:** Introducing untargeted Byzantine corruption via class-consistent view swapping fills a clear gap between random noise and targeted backdoor attacks in VFL.\n\n- **Well-defined threat model:** The assumption of a passive adversary with partial label access is realistic and consistent with real-world cross-organization data sharing."}, "weaknesses": {"value": "- **Simplified setting:** Experiments are limited to two-party VFL on relatively simple datasets (MNIST, FashionMNIST, UCI tabular). The attack’s scalability to multi-party or high-dimensional, real-world VFL remains uncertain.\n\n- **Weak defense discussion:** Although new defense gates are mentioned, they are rudimentary and come with severe utility degradation. The paper stops short of providing meaningful defense insights beyond confirming that existing defenses fail.\n\n- **Missing deeper analysis of attack transferability:** It is unclear how robust the cluster-swapping attack remains when the adversary’s clustering accuracy degrades or when auxiliary label availability is further reduced.\n\n- **No ablation on key assumptions:** The 5% labeled auxiliary dataset assumption is strong; the paper doesn’t quantify how attack effectiveness changes with smaller or noisier supervision."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUp2hKWyBr", "forum": "8kRBj7QZvy", "replyto": "8kRBj7QZvy", "signatures": ["ICLR.cc/2026/Conference/Submission24077/Reviewer_WGiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24077/Reviewer_WGiz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540785640, "cdate": 1761540785640, "tmdate": 1762942925659, "mdate": 1762942925659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the byzantine attack in VFL setting. Specifically, they target the training phase attack that corrupt the top model (hold by the active cient) such that it  learn spurious correlation of the adversary's view and the clean view, and make wrong inference. They propose a two stage training methods, first is using clustering to predict labels of adversary's features, second stage is to swap features and poison models. The two stage methods are well designed for efficient attack. The empirical evaluation shows it can keep good performance and circumvent the defense. However, the experiment lacks baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The attack threat model assumes the passive adversary which is more challengable.\n2. The methods are systematical, including two designed steps."}, "weaknesses": {"value": "1. The first cluster step will alter the embedding distribution and make it misaligned the natural distribution of the honest training embedding, which hurts the model's performance.\n2. There are other clustering or shadow model based methods to predict labels. However, the authors did not compare any.\n3. The baselines are too less only with the random noise attack. However, in introduction, the authors have mentioned other byzantine attacks works like [1] but they did not compare. \n4. There is no ablation studies, like hoe the number of passive clients will affect the attack, and the effect of model layers.\n4. The format of Table is our the margin.\n\n[1] Hijack Vertical Federated Learning Models As One Party"}, "questions": {"value": "1. why \"A lower attack accuracy indicates a stronger attack\"?\n2. How to keep internal consistency in the stage 2, since you have change features to different clusters, which will lead to a different distribution of embedding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K0H4gunpfi", "forum": "8kRBj7QZvy", "replyto": "8kRBj7QZvy", "signatures": ["ICLR.cc/2026/Conference/Submission24077/Reviewer_Vgs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24077/Reviewer_Vgs2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811741102, "cdate": 1761811741102, "tmdate": 1762942925362, "mdate": 1762942925362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an untargeted Byzantine attack specifically tailored to Vertical Federated Learning (VFL). The attack, \"Swap-and-Spoil,\" consists of two stages: (1) the adversary uses a small auxiliary labeled dataset (5%) to learn latent class structures through semi-supervised clustering, and (2) performs class-consistent view swaps during training, exchanging features across inferred clusters. This manipulation disrupts cross-view feature alignment and induces inference-time accuracy collapse on clean data. Experiments show that Swap-and-Spoil effectively bypasses common VFL defenses without significantly affecting training metrics, revealing a critical gap in current security mechanisms."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a new untargeted Byzantine attack tailored to the unique structure of VFL.\n- Distinct from trigger-based or gradient-manipulation attacks, the class-consistent swap strategy is stealthy and generalizable.\n- Strong experimental validation across both visual (MNIST, CIFAR-10) and tabular (UCI-HAR, Mushroom) datasets."}, "weaknesses": {"value": "1. A reconstruction-based or embedding-monitoring defense might still detect statistical inconsistencies caused by swapped feature associations.\n2. Visualization (e.g., t-SNE/PCA) of embeddings before and after swapping would clarify how the attack alters representation space.\n3. The attack depends heavily on the auxiliary labeled data (5%); an ablation study varying this proportion (e.g., 1%, 10%) would improve understanding of feasibility and robustness.\n4. The clustering and contrastive learning steps (SimCLR + GMM) may incur high computational cost for a single adversarial client—this should be quantified.\n5. Sensitivity analysis on the parameter *k* (for top-*k* farthest swaps) is missing.\n6. The method focuses on two-party VFL; discussion on scalability to multi-party settings would strengthen the paper.\n7. The paper should clarify the distinction between \"untargeted degradation\" and the occasional mention of \"target class misclassification,\" as this could confuse readers about the attack objective."}, "questions": {"value": "Please address the aforementioned weaknesses. Specifically, clarify Swap-and-Spoil’s detectability under reconstruction-based defenses, analyze sensitivity to auxiliary data and k, quantify computational overhead, and elaborate on scalability to multi-party settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R8bUGA43Qg", "forum": "8kRBj7QZvy", "replyto": "8kRBj7QZvy", "signatures": ["ICLR.cc/2026/Conference/Submission24077/Reviewer_4K3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24077/Reviewer_4K3h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946710172, "cdate": 1761946710172, "tmdate": 1762942924499, "mdate": 1762942924499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}