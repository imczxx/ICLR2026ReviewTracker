{"id": "IyIaAOihmZ", "number": 20844, "cdate": 1758310802993, "mdate": 1759896955940, "content": {"title": "RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents", "abstract": "Code agents have gained widespread adoption due to their strong code generation capabilities and integration with code interpreters, enabling dynamic execution, debugging, and interactive programming capabilities. While these advancements have streamlined complex workflows, they have also introduced critical safety and security risks. Current static safety benchmarks and red-teaming tools are inadequate for identifying emerging real-world risky scenarios, as they fail to cover certain boundary conditions, such as the combined effects of different jailbreak tools.\nIn this work, we propose RedCodeAgent, the first automated red-teaming agent designed to systematically uncover vulnerabilities in diverse code agents. \nWith an adaptive memory module, RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the most effective red-teaming tools and tool combinations in a tailored\ntoolbox for a given input query, thus identifying vulnerabilities that might otherwise be overlooked.\nFor reliable evaluation, we develop simulated sandbox environments to additionally evaluate the execution results of code agents, mitigating potential biases of LLM-based judges that only rely on static code.\nThrough extensive evaluations across multiple state-of-the-art code agents, diverse risky scenarios, and various programming languages, RedCodeAgent consistently outperforms existing red-teaming methods, achieving higher attack success rates and lower rejection rates with high efficiency. We further validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium, exposing previously unidentified security risks. By automating and optimizing red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective safety assessments of code agents.", "tldr": "", "keywords": ["LLM", "Code generation", "safety", "security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffd0d7f746523887268afa96f8b26824672628c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RedCodeAgent, an automated red-teaming agent designed to identify security vulnerabilities in LLM-based code agents. The system comprises three key components: (1) an adaptive memory module that stores and retrieves successful attack experiences, (2) a toolbox integrating both general jailbreak methods (GCG, AmpleGCG, AdvPrompter, AutoDAN) and a specialized code substitution tool, and (3) simulated sandbox environments for unbiased evaluation. Through extensive experiments across multiple code agents (OpenCodeInterpreter, ReAct, MetaGPT, Cursor, Codeium), benchmarks (RedCode-Exec, RedCode-Gen, RMCbench), and programming languages, the authors demonstrate that RedCodeAgent achieves higher attack success rates and lower rejection rates compared to existing jailbreak methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Red-teaming code agents is a critical but understudied area. As code agents become more widely deployed with execution capabilities, systematic security evaluation is essential. The motivation is well-articulated.\n2. The integration of memory retrieval, dynamic tool selection, and execution-based evaluation is thoughtful. The memory module with trajectory logging and similarity-based retrieval (Algorithm 1) is elegant and effective.\n3. RedCodeAgent consistently outperforms baselines, achieving 72.47% ASR vs 55.46% for no jailbreak on OCI, while maintaining efficiency (121.17s vs comparable baseline costs).\n4. Validation on real-world tools (Cursor, Codeium) and discovery of 82 unique vulnerabilities that all baselines missed demonstrates real-world impact."}, "weaknesses": {"value": "1. The paper relies entirely on automated evaluation methods without any human validation. This raises concerns about evaluation validity, as even spot-checking a subset of results with human annotators would significantly strengthen the validity of the findings. And the evaluation approach is particularly weak for RMCbench, where keyword-matching is used to detect rejections, which could easily miss sophisticated or nuanced refusals that do not contain the predefined rejection keywords.\n2. Section D.4 shows memory helps, but provides minimal insight into what the agent learns. What patterns emerge in successful attacks? What makes certain tool combinations effective? The memory structure includes \"self-reflection\" but no analysis of its quality or utility\n3. Nearly all experiments use GPT-4o-mini, only one ablation with GPT-4o (Section D.9). No exploration of open-source base LLMs (e.g., Llama, Mistral), which limits generalizability claims."}, "questions": {"value": "1. How does RedCodeAgent perform on risk scenarios completely absent from the memory? The current setup accumulates memory during sequential execution.\n2. What is your process for disclosing vulnerabilities to commercial vendors? Have Cursor and Codeium been notified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nMIXBQ7fpm", "forum": "IyIaAOihmZ", "replyto": "IyIaAOihmZ", "signatures": ["ICLR.cc/2026/Conference/Submission20844/Reviewer_WdER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20844/Reviewer_WdER"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490107451, "cdate": 1761490107451, "tmdate": 1762936336352, "mdate": 1762936336352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RedCodeAgent, an automated, adaptive red-teaming agent for LLM-based code agents. It combines (i) a memory module that stores successful attack trajectories and retrieves top-K similar experiences with a length penalty, (ii) a toolbox integrating multiple jailbreak optimizers and a code-substitution module to refine prompts via function-calling, and (iii) a simulation-based evaluation with Docker sandboxes for verifying code execution outcomes.\n- The study evaluates RedCodeAgent on multiple targets: OpenCodeInterpreter (OCI), a ReAct-based agent, MetaGPT, and real-world assistants (Cursor, Codeium). Benchmarks include RedCode-Exec (27 risk scenarios across 8 categories), RedCode-Gen (malware-style function docstrings), and RMCBench. Metrics: Attack Success Rate (ASR), Rejection Rate (RR), and time/efficiency.\n- Results: RedCodeAgent achieves higher ASR and lower RR than jailbreaking baselines (GCG, AmpleGCG, Advprompter, AutoDAN) on OCI/RA and across benchmarks. It also shows effectiveness across languages (Python/C/C++/Java) and on Cursor/Codeium. Ablations suggest retries alone do not close the gap; memory and multi-tool orchestration matter; the agent uncovers vulnerabilities other methods miss (e.g., reverse shell)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality:\n  - Integrates memory-guided retrieval with a length penalty to favor efficient prior trajectories: $S = \\mathrm{CosSim}(e_{\\mathrm{risk}}^q, e_{\\mathrm{risk}}^m) + \\mathrm{CosSim}(e_{\\mathrm{desc}}^q, e_{\\mathrm{desc}}^m) - \\rho \\cdot \\mathrm{len}(m)$.\n  - Systematic orchestration of jailbreak and code-specific tools via function-calling; includes code-substitution tailored to code-agent risks.\n  - Execution-grounded evaluation in Docker, moving beyond LLM-as-judge for code tasks.\n- Quality:\n  - Broad and careful evaluation (multiple agents, benchmarks, languages; real-world assistants). Clear metrics (ASR, RR, time) and ablations (effect of retries; number of tools; memory modes; $\\rho$).\n  - Demonstrates discovery of previously missed vulnerabilities and improved efficiency with memory/tooling.\n- Clarity:\n  - Clear pipeline: retrieval → tool-driven prompt optimization → query → sandbox evaluation → reflection/memory update.\n  - Tables summarize comparative performance; design choices (e.g., top-K, $\\rho$) are stated, with ablations indicating robustness.\n- Significance:\n  - Addresses a pressing problem (code-agent safety) with a scalable, automatable methodology. Real-world assistant results underline practical risk and relevance."}, "weaknesses": {"value": "- Fairness of baselines:\n  - Comparisons pit an iterative, memory-augmented agent against baselines mostly evaluated as single-shot optimized prompts. The “retry” study covers two subtasks; a comprehensive best-of-N or multi-round baseline across all scenarios—budget-matched by iterations/API calls/time—would strengthen claims.\n- Evaluation biases and coverage:\n  - RedCode-Gen relies on LLM-as-judge; despite reasonableness, potential bias remains. Consider cross-checking with lightweight execution proxies where feasible or multi-judge consensus.\n  - While RedCode-Exec covers 27 scenarios, important classes (e.g., SQL injection) are acknowledged as missing. Expanding or reporting generalization to additional realistic risks would improve coverage.\n- Reproducibility and cost:\n  - Core backbone model (GPT-4o-mini) and real-world assistant interfaces are not fully open/API-stable; semi-automated pipelines may hinder replication. Detailed reporting of token/compute budgets per method and per scenario would clarify cost-effectiveness beyond wall-clock time.\n- Safety-of-release considerations:\n  - The agent surfaces workable exploit prompts and reverse-shell procedures. While sandboxing mitigates local risk, the paper should articulate a more concrete responsible-release plan for tools, memory logs, and prompts (redactions, access controls)."}, "questions": {"value": "- Budget-matched baselines:\n  - Can you provide a comprehensive, budget-matched comparison where baselines are allowed the same number of optimization/agent-query rounds as RedCodeAgent across all scenarios? If total iteration parity is hard, report best-of-N (N comparable to your median trajectory length) to bound the gap.\n- Cost accounting:\n  - Please include token counts and API costs per method (aggregate and per-risk) in addition to time and trajectory length, to assess sample and cost efficiency.\n- Memory influence:\n  - In the main results, to what extent do memory entries span across risk indices? You mention “Independent” mode for main tables; can you confirm there is no cross-index leakage? Also, how sensitive are outcomes to $\\rho$ and top-K beyond the reported settings?\n- Real-world assistants:\n  - Were interactions compliant with the platforms’ terms of service? Could you release the semi-automated scripts and detailed instructions to reproduce Table 4 results (with redactions if needed)?\n- LLM-as-judge reliability:\n  - For RedCode-Gen, did you perform any human verification on a stratified sample to estimate judge accuracy? If so, please report agreement rates and common failure modes.\n- Safety release plan:\n  - What is the exact policy for releasing prompts/memories/tools to avoid enabling misuse? Will you gate high-risk artifacts (e.g., reverse shell prompts) for registered researchers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OyJWJtZVKm", "forum": "IyIaAOihmZ", "replyto": "IyIaAOihmZ", "signatures": ["ICLR.cc/2026/Conference/Submission20844/Reviewer_NYrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20844/Reviewer_NYrX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852561242, "cdate": 1761852561242, "tmdate": 1762936335603, "mdate": 1762936335603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RedCodeAgent, an automated red-teaming framework for LLM-based code agents. The system integrates an adaptive memory module, a toolbox of existing jailbreak methods, and an evaluation module that uses sandbox execution to verify whether the generated code truly performs risky operations. Through extensive experiments on multiple benchmarks, the paper shows that RedCodeAgent achieves higher attack success rate and lower rejection rate compared to these baseline jailbreak methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments cover multiple agents, programming languages, and benchmarks, offering a broad empirical view of code-agent vulnerabilities.\n2. The sandbox-based evaluation and execution-level validation go beyond prior “LLM-as-a-judge” settings, improving measurement reliability."}, "weaknesses": {"value": "1. The design of RedCodeAgent demonstrates limited novelty.\n2. The paper does not include recent state-of-the-art jailbreak methods."}, "questions": {"value": "The paper is well-written and includes comprehensive experiments. However, the design of RedCodeAgent shows limited novelty, as it mainly combines well-known components such as a memory module, jailbreak tools, and sandbox-based testing. The contribution lies in integrating and empirically evaluating these elements within a single framework for code agents.\n\nThe selected baselines (GCG, AutoDAN, AdvPrompter, and AmpleGCG) are relatively basic and outdated, while more recent and stronger methods are not considered. If existing jailbreak techniques already perform well, what is the necessity of developing RedCodeAgent?\n\nAdditionally, it remains unclear how gradient-based methods like GCG are applied to test a black-box code agent, since gradients are typically inaccessible in such settings.\n\nFinally, the paper focuses solely on attack performance and does not evaluate or discuss potential defensive measures, which would be essential for providing a balanced view of code agent safety."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LnwPNqiO8z", "forum": "IyIaAOihmZ", "replyto": "IyIaAOihmZ", "signatures": ["ICLR.cc/2026/Conference/Submission20844/Reviewer_ibSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20844/Reviewer_ibSA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980748277, "cdate": 1761980748277, "tmdate": 1762936334800, "mdate": 1762936334800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A framework called RedCodeAgent is proposed to automatically red-team LLM-based code agents and find security flaws. It has three parts: a memory module that retrieves past successful attacks, a toolbox of jailbreak and code-specific tools to craft prompts, and an evaluation module that runs generated code in a sandbox to check whether the risky action actually occurs. The agent iteratively optimizes prompts, queries the target agent, and stores successful trajectories for future attacks. Experiments show higher attack success rates and lower rejection rates than prior jailbreak methods in various benchmarks, languages, and real-world code assistants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The model proposed can continuously learn from past attacks, which makes it more scalable and practical than static benchmarks.\n\n2. The model proposed does not require additional knowledge other then API calls to the code agent.\n\n3. The experiments are done using various benchmarks, programming languages, and real-world code agents."}, "weaknesses": {"value": "1. The discussion and evaluations seems to be based on a pre-existing memory; It would be great to see how performance degrades when some tools are removed or when memory is empty.\n\n2. The paper’s memory and embedding setup relies on general-purpose sentence embeddings and natural-language similarity, which weren’t specifically designed for attack semantics. That means the system may retrieve examples that are superficially similar but not actually useful for crafting successful exploits, reducing effectiveness on nuanced or codespecific vulnerabilities. It could be better if the authors adapt embeddings and memory structure for attack-relevance (e.g., code-aware or action-aware representations) to ensure the memory truly helps the inference process.\n\n3. Most of the tools in the toolbox are pre-existing methods taken from prior work. Also, the tools appears to be largely hand-picked rather than systematically derived. The paper provided little justification for why these particular tools were chosen or how they complement each other.\n\n4. The evaluation primarily focuses on attack success rate and rejection rate, with limited analysis of real-world impact or severity of the discovered vulnerabilities."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lRTa5aLxy9", "forum": "IyIaAOihmZ", "replyto": "IyIaAOihmZ", "signatures": ["ICLR.cc/2026/Conference/Submission20844/Reviewer_LEqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20844/Reviewer_LEqV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014299119, "cdate": 1762014299119, "tmdate": 1762936334140, "mdate": 1762936334140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}