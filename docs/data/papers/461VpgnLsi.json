{"id": "461VpgnLsi", "number": 7000, "cdate": 1758004579675, "mdate": 1759897878608, "content": {"title": "ReSplat: Degradation-agnostic Feed-forward Gaussian Splatting via Self-guided Residual Diffusion", "abstract": "Recent advances in novel view synthesis (NVS) have predominantly focused on ideal, clear input settings, limiting their applicability in real-world environments with common degradations such as blur, low-light, haze, rain, and snow. While some approaches address NVS under specific degradation types, they are often tailored to narrow cases, lacking the generalizability needed for broader scenarios. To address this issue, we propose Restoration-based feed-forward Gaussian Splatting, named ReSplat, a novel framework capable of handling degraded multi-view inputs. Our model jointly estimates restored images and gaussians to represent the clear scene for NVS. We enable multi-view consistent universal image restoration by utilizing the 3d gaussians generated during the diffusion sampling process as self-guidance. This results in sharper and more reliable novel views. Notably, our framework adapts to various degradations without prior knowledge of their specific types. Extensive experiments demonstrate that ReSplat significantly outperforms existing methods across challenging conditions, including blur, low-light, haze, rain, and snow, delivering superior visual quality and robust NVS performance.", "tldr": "", "keywords": ["Universal Image Restoration"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19e0f5a169334d0d5e1ebea58a7bcedcb1ba8785.pdf", "supplementary_material": "/attachment/3db1bc95b69fc2fa8700abe2803356acbc1ea2e5.zip"}, "replies": [{"content": {"summary": {"value": "1. **Originality-wise**: The core idea of creating a synergistic loop between a universal image restoration model and a feed-forward Gaussian Splatting model is highly novel. Specifically, using the intermediate 3D geometry from the GS model to enforce multi-view consistency in a diffusion-based restoration process is a clever and previously unexplored mechanism for this problem.\n2. **Quality-wise**: The claims are strongly supported by comprehensive experiments across a wide array of synthetic, mixed, and real-world degradations. The method consistently achieves state-of-the-art results in both novel view synthesis and image restoration tasks, demonstrating the framework's robustness and effectiveness. \n3. **Clarity-wise**: The paper is well-structured and clearly articulates a complex problem and its solution. The overall framework is well-illustrated with diagrams that effectively convey the interplay between the restoration and synthesis modules. The motivation, methodology, and results are presented logically and are easy to follow."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Solves a Novel and Practical Problem**: The work addresses degradation-agnostic novel view synthesis, a topic of high practical importance for real-world applications that has been underexplored compared to synthesis from clean images.\n\n2. **Innovative Synergistic Framework**: The core contribution is a novel framework where an image restoration model and a Gaussian Splatting model work in tandem. The use of 3D geometry from the GS model to guide the UIR model and ensure multi-view consistency is a key innovation.\n\n3. **SOTA Performance and Thorough Validation**: The method demonstrates superior performance on both novel view synthesis and image restoration tasks across a comprehensive set of experiments, including single, mixed, and real-world degradations."}, "weaknesses": {"value": "1. Ambiguous Mechanism and Potential for Detail Suppression in the Pre-filtering Module: The paper proposes a pre-filtering module to suppress artifacts but provides insufficient insight into its inner workings. The mechanism, which uses self-attention on both the corrupted and restored images, is a black box. It is unclear whether the module learns to identify specific \"restoration artifacts\" or if it simply learns to penalize any high-frequency regions that differ significantly from the degraded input.\n\n2. the complete absence of failure cases is a significant omission for a paper claiming such robust, \"agnostic\" capabilities. A rigorous scientific contribution requires a transparent discussion of a method's limitations. \n---\nI have listed my concerns, and the score will be adjusted based on the author's response."}, "questions": {"value": "Please refer to Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qwio5ctb2Y", "forum": "461VpgnLsi", "replyto": "461VpgnLsi", "signatures": ["ICLR.cc/2026/Conference/Submission7000/Reviewer_26dS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7000/Reviewer_26dS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760601250202, "cdate": 1760601250202, "tmdate": 1762919215690, "mdate": 1762919215690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReSplat, a novel framework for degradation-agnostic NVS. Unlike prior methods that either assume clean inputs or focus on specific degradation types, ReSplat integrates 3D Gaussian Splatting with a residual diffusion-based universal image restoration module. The approach jointly estimates restored multi-view images and explicit scene geometry through Gaussian splats, enabling multi-view consistent restoration and sharper novel view generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Combining Gaussian splatting with a residual diffusion model is conceptually new and addresses degradation-agnostic NVS in a unified way. And the proposed models have strong performance than competitive baselines (DiffUIR, GAURA) across both synthetic and real-world degradations, with consistent improvements in PSNR/SSIM/LPIPS.\n2. Evaluation covers multiple degradation types (blur, haze, low-light, snow, rain) and mixed-degradation scenarios, showcasing robustness.\n3. The method maintains practical inference speed (under one second for three views), which is important for deployment.\nAblation study: Clearly shows the effect of multi-view alignment and pre-filtering, validating each module’s importance."}, "weaknesses": {"value": "1. Real-world evaluations are restricted to blur, haze, and low-light datasets. Claims of degradation-agnostic performance would be stronger with more diverse real-world tests (e.g., snow/rain in the wild).\n\n2. While inference is efficient, the paper does not provide sufficient detail about training cost (e.g., GPU hours, memory usage). For diffusion-based models, this is important.\n\n3. Although the model claims to be degradation-agnostic, it is unclear how well it generalizes to unseen or compound degradations not present in training.\n\n4. The core innovation lies more in integration than in fundamentally new algorithms. Some may find the contribution incremental.\n\n5. The ablation studies are weak, the model are constructed by the SOTA restoration network and NVS network, how the framework performs when changing these parts with other weaker restoration and NVS networks?\n\n6. Some important NVS in low-quality scene [1,2] and image unified image restoration [3,4,5,6] papers are lacking.\n\n[1] HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes. \n[2] Robustgs: Unified boosting of feedforward 3d gaussian splatting under low-quality conditions.\n[3] Adair: Adaptive all-in-one image restoration via frequency mining and modulation. \n[4] Perceive-ir: Learning to perceive degradation better for all-in-one image restoration\n[5] Multi-task image restoration guided by robust DINO features.\n[5] Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration."}, "questions": {"value": "1. The real-world evaluations are limited to blur, haze, and low-light datasets. How would the method perform under more diverse real-world degradations such as snow or rain in the wild?\n2. The paper reports efficient inference but does not discuss training cost. What are the training resources (e.g., GPU hours, memory usage) required, and how do they compare with baselines like DiffUIR?\n3. The method claims to be degradation-agnostic. How well does ReSplat generalize to unseen or compound degradations that were not included in training?\n4. The contribution seems more about integration than proposing fundamentally new algorithms. Can the authors clarify which parts of the framework they view as the key novel technical contributions, beyond combining restoration and Gaussian splatting?\n5. The ablation studies only vary internal modules but still rely on strong SOTA backbones for restoration and NVS. How would the framework perform if weaker restoration networks or weaker NVS models were used in place of the chosen SOTA baselines?\n6. Several important recent works on NVS under degraded scenes [1,2] and unified image restoration [3–6] are not discussed. \n[1] HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes.\n[2] RobustGS: Unified boosting of feedforward 3D Gaussian Splatting under low-quality conditions.\n[3] Adair: Adaptive all-in-one image restoration via frequency mining and modulation.\n[4] Perceive-IR: Learning to perceive degradation better for all-in-one image restoration.\n[5] Multi-task image restoration guided by robust DINO features.\n[6] Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VyYfeCDv5V", "forum": "461VpgnLsi", "replyto": "461VpgnLsi", "signatures": ["ICLR.cc/2026/Conference/Submission7000/Reviewer_AE9N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7000/Reviewer_AE9N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760942933488, "cdate": 1760942933488, "tmdate": 1762919215102, "mdate": 1762919215102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes novel framework that can synthesize clean novel-view images given by the corrupted multi-view images. The model can construct clean 3D representation from images with diverse types of degradation by combining diffusion-based UIR model and feed-forward 3DGS."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation of this paper is explained clearly in the introduction part. Most of the previous literatures in the *Novel View Synthesis with Degradations* are limited to certain types of degradation and can't be generalized to corruptions that are net seen in the training dataset. GAURA attempted to mitigate this limitation by constructing degradation-aware generalizable NeRF but didn't leverage the prior knowledge of pretrained 2D UIR model. ReSplat points out those limitations which are important enough for the practicality of 3DGS.\n* ReSplat can be regarded as combination of DiffUIR and MVSGaussian. However, paper also proposes two novel modules in Sec. 3.3 and Sec. 3.4 to improve the aggregation of information from multi-view images. I believe Sec. 3.3 is more notable contribution where they explicitly fuse the features from matching points across multi-view images, thereby aggregating the multi-view information effectively. This is common practice in feed-forward 3DGS [c], but there is novelty to use this technique for 3D-aware denoising process.\n* Experiments are conducted on both synthetic and real-world datasets with multiple types of degradation. ReSplat consistently shows the performance improvement compared to the baselines.\n\n---\n\n### References\n\n[c] Charatan, David, et al. \"pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "weaknesses": {"value": "### Weaknesses\n\nThere are some parts that technical details are missing.\n\n* In the preliminary, the author explain about the original 3DGS. However, I believe author should explain the brief summary for feed-forward 3DGS (like MVSGaussian) instead of standard 3DGS since they used the feed-forward 3D model in this work. This explanation of the feed-forward model will also aid understanding in the later sections, such as Sec. 3.4. It seems that the writing assumes readers are already familiar with generalizable 3DGS models, such as MVSGaussian and skips a lot of technical details in the main paper.\n\n* In the Algorithm. 1, the novel view image $I_{nv}$ is used in line 7 but there is no explanation about this part in the main paper. Is this rendering loss conducted between rendered image and ground-truth clean image? Furthermore, detailed explanations about training with objective terms are missing in the main text where all of them are briefly summarized into Algorithm. 1.\n\n* It is unclear that how the *Pre-filtering with warped features* operates. How does the outputted $[W^i_{pre}]^N_{I=1}$ contribute to the final weights map? Furthermore, the motivation of this module is also hard to understand. In the L85-86, the paper explained that this module can assist to achieve *artifact-free* novel view synthesis. However, it is hard to grasp the relationship between the terms of 'artifact-free' and the operations of this module. \n\n* What does the operation 'IR → NV' in Tab. 1 mean? How are the single-image IR methods such as DiffUIR evaluated on novel view synthesis? According to L372-374, it seems that author adopted additional adapter to transfer single-image IR models into multi-view settings. Is it correct?\n\n* Author can diversify the baselines used in the comparison. For example, the most naive solution of achieving 3D reconstruction with UIR is: 1) Restore the corrupted multi-view images with pretrained 2D UIR method. 2) Reconstruct the clean 3DGS by using the restored multi-view images and pretrained feed-forward 3DGS. Author should compare the performance of ReSplat with this naive two-stage framework.\n\n* How many input views are used during the evaluation? How many views can ReSplat handle? Please specify the evaluation details.\n\n* There are previous literatures [a, b] that tried to use diffusion prior to construct clean 3D representation from the degraded images. However, all of them are limited to certain types of degradation (low-resolution or motion blur). It is better to cite those papers in the *Related works* since they are closely related with this paper in terms of using diffusion model for the 3D-IR task.\n\n---\n\n### References\n\n[a] Lee, Seungjun, and Gim Hee Lee. \"DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[b] Lee, Jie Long, Chen Li, and Gim Hee Lee. \"Disr-nerf: Diffusion-guided view-consistent super-resolution nerf.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "Refer to the *Weaknesses* section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jeMFUusS5Q", "forum": "461VpgnLsi", "replyto": "461VpgnLsi", "signatures": ["ICLR.cc/2026/Conference/Submission7000/Reviewer_FMUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7000/Reviewer_FMUU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394138031, "cdate": 1761394138031, "tmdate": 1762919214571, "mdate": 1762919214571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method, ReSplat, for feedforward NVS when input images are degraded in various ways, ReSplat leverages the feedforward 3D Gaussian Splatting method MVSplat[1] and an universal residual diffusion model DiffUIR[2] to solve the problem. The authors propose using Gaussian points information in the Diffusion encoder to promote 3D consistency and a pre-filtering technique to eliminate residual artifacts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of NVS with degraded inputs the paper focuses on is important and interesting.\n2. ReSplat outperforms other methods in both quantitative and qualitative results.\n3. The experiments are comprehensive, evaluating on both synthetic and real-world image degradations."}, "weaknesses": {"value": "1. My main concern is the effectiveness of the proposed 3D alignment module and pre-filtering technique, as the improvements in Table 4 ablation study are moderate.\n2. No qualitative figure in ablation study is provided. \n3. A limitations section should be discussed and added."}, "questions": {"value": "1. I don't understand how exactly Gaussian points information are used in the diffusion model. After per-point features obtained in Sec 3.3, are they using as conditional features in the diffusion model? I suggest outlining the attention and diffusion encoder equations. \n2. What are the memory consumption and training time of ReSplat?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tjl1QohFDC", "forum": "461VpgnLsi", "replyto": "461VpgnLsi", "signatures": ["ICLR.cc/2026/Conference/Submission7000/Reviewer_FesE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7000/Reviewer_FesE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882609598, "cdate": 1761882609598, "tmdate": 1762919214222, "mdate": 1762919214222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}