{"id": "unZhwukf0T", "number": 19699, "cdate": 1758298523914, "mdate": 1759897024821, "content": {"title": "CAT-VIDEO: CORRUPTION-AWARE TRAINING FOR ROBUST VIDEO DIFFUSION MODELS", "abstract": "Latent Video Diffusion Models (LVDMs) have achieved state-of-the-art generative quality for image and video generation; however, they remain brittle under noisy conditioning, where small perturbations in text or multimodal embeddings can cascade over timesteps and cause semantic drift. Existing corruption strategies from image diffusion (Gaussian, Uniform) fail in video settings because static noise disrupts temporal fidelity. In this paper, we propose **CAT-Video**, a corruption-aware training framework with structured, data-aligned noise injection tailored for video diffusion. Our two operators—*Batch-Centered Noise Injection (BCNI)* and *Spectrum-Aware Contextual Noise (SACN)*—align perturbations with batch semantics or spectral dynamics to preserve coherence. CAT-Video yields substantial gains: BCNI reduces FVD by **31.9%** on WebVid-2M, MSR-VTT, and MSVD, while SACN improves UCF-101 by **12.3%**, outperforming Gaussian, Uniform, and even large diffusion baselines like DEMO (2.3B) and Lavie (3B) despite training on $\\mathbf{5}\\times$ less data. Ablations confirm the unique value of low-rank, data-aligned noise, and theory establishes why these operators tighten robustness and generalization bounds. CAT-Video thus sets a new framework for robust video diffusion, and our experiments show that it can also be extended to autoregressive generation and multimodal video understanding LLMs.", "tldr": "We introduce CAT-Video, a corruption-aware training framework that improves robustness and temporal coherence in video diffusion models through structured, data-aligned noise injection.", "keywords": ["video diffusion", "corruption-aware training", "robust video generation", "structured noise injection", "multimodal robustness", "temporal coherence"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36b779e3109b1503084b84e3c1c30d2bc4985918.pdf", "supplementary_material": "/attachment/c99cdbeea034fc1e74ad38310569a2906228cc13.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CAT-LVDM, a corruption-aware training framework for Latent Video Diffusion Models designed to improve robustness against imperfect or noisy text prompts. The core of the framework consists of two structured noise injection strategies:\n\n1. BCNI: Perturbs text embeddings along intra-batch semantic directions to increase entropy while maintaining semantic alignment.\n\n2. SACN: Injects noise along dominant spectral modes (via SVD) to enhance low-frequency smoothness and temporal coherence.\n\nThe authors provide theoretical justification for both methods, analyzing conditional entropy and Wasserstein distance bounds. Experiments are conducted on several datasets (WebVid-2M, MSR-VTT, MSVD, UCF-101), where the proposed methods demonstrate quantitative improvements (e.g., reduced FVD) compared to uncorrupted baselines and simpler noise injection techniques."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a theoretical analysis for its proposed methods (BCNI and SACN)\n\n2. The paper addresses the practical problem of training video models on large-scale, noisy web data. The idea of corruption-aware training for video diffusion is considered interesting and worthwhile.\n\n3. The method demonstrates clear quantitative performance gains (e.g., FVD, SSIM, PSNR) over uncorrupted baselines and naive (Gaussian/Uniform) noise baselines on the tested datasets.\n\n4. The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "1. A major concern is that the method was only validated on an older LVDM architecture (DEMO). Its effectiveness and scalability on modern, state-of-the-art models (e.g., DiT-based) are unproven\n\n2. Despite quantitative gains, the practical impact is undermined by the generated videos. the visual results not unsatisfactory. \n\n3. he paper lacks a clear justification for applying BCNI primarily to caption-rich datasets (WebVid-2M, MSR-VTT) and SACN only to a class-labeled dataset (UCF-101). \n\n4. The evaluation is missing key comparisons. It fails to compare against stronger, modern LVDM baselines and does not empirically validate the necessity of \"video-specific\" corruption. \n\n5. The core idea of perturbing conditions to improve robustness has been explored in the image domain, limiting the paper's technical novelty."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6DfOOCwEmC", "forum": "unZhwukf0T", "replyto": "unZhwukf0T", "signatures": ["ICLR.cc/2026/Conference/Submission19699/Reviewer_wNJ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19699/Reviewer_wNJ3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840203927, "cdate": 1761840203927, "tmdate": 1762931543011, "mdate": 1762931543011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAT-Video, a training framework that improves video generation models' resilience to imperfect inputs. The authors address a key limitation of standard noise-injection techniques, which often disrupt video coherence. Their solution involves two novel methods: Batch-Centered Noise Injection (BCNI) to maintain semantic consistency within a batch, and Spectrum-Aware Contextual Noise (SACN) to preserve smooth temporal dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "​​+  a New Problem​​: It is one of the first to systematically address how to make video AI models robust to imperfect instructions, focusing on a key weakness in existing methods that break video coherence.\n\n​​+  Its two new techniques, BCNI and SACN, are simple and add almost no extra cost, yet outperform much larger and more expensive models.\n\n​​+ Extensive testing across different datasets and metrics proves the methods reliably enhance video quality and motion."}, "weaknesses": {"value": "- Is this problem meaningful in the research field?  In what kind of scenerios, noise will introduced into the input? Does the model robust for  adversarial attack? \n- Does the proposed model work for large video models?"}, "questions": {"value": "Please refer to the strengths and weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7n5Y6heSR", "forum": "unZhwukf0T", "replyto": "unZhwukf0T", "signatures": ["ICLR.cc/2026/Conference/Submission19699/Reviewer_5D4F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19699/Reviewer_5D4F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910789362, "cdate": 1761910789362, "tmdate": 1762931542070, "mdate": 1762931542070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAT-Video, a corruption-aware training framework that improves robustness in latent video diffusion models (LVDMs) under noisy or imperfect conditioning.\nThe core contribution lies in two structured corruption strategies: Batch-Centered Noise Injection (BCNI), which injects noise along the deviation from the batch mean, and Spectrum-Aware Contextual Noise (SACN), which perturbs only the low-frequency spectral components of conditioning embeddings. Theoretical analysis supports tighter generalization bounds and improved temporal fidelity, and extensive experiments across multiple benchmarks demonstrate empirical gains over conventional Gaussian and Uniform corruption techniques as well as large-scale diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper presents a clear exploration of corruption-aware training for video diffusion, supported by solid theoretical derivations that extend existing analyses to the temporal setting.\n2.\tExtensive experiments across four standard benchmarks and additional evaluations on autoregressive and multimodal models confirm broad empirical robustness.\n3.\tAblation studies, sensitivity analyses, and detailed appendices ensure reproducibility; released code and metrics further strengthen the work’s transparency."}, "weaknesses": {"value": "1.\tAlthough the authors claim model-agnostic applicability, most analyses are conducted on the DEMO backbone with OpenCLIP-based encoders. This limits the universality claim, while evaluating BCNI/SACN on one or two additional diffusion backbones would strengthen the evidence.\n2.\tWhile CAT-Video demonstrates strong robustness on standard datasets, further evaluation under more realistic, noisy, or weakly aligned conditions would better reflect its practical robustness.\n3.\tRelated work on corruption-aware methods in image diffusion and other noisy-input domains is not sufficiently discussed. This section primarily centers on LVDMs, which are not the main focus of this work.\n4.\tAdditional qualitative visualizations would help clarify how CAT-Video improves visual coherence compared with other corruption strategies."}, "questions": {"value": "1.\tWould combining multiple corruption techniques during training (e.g., applying BCNI followed by SACN) be beneficial, or would such composite perturbations destabilize the training process? Some insight into this interaction could be valuable for readers.\n2.\tHow are subsets of training data (e.g., 2 M vs. 10 M in the original DEMO paper) selected? Are they random samples or curated based on specific criteria?\n3.\tFor very long or high-resolution videos, does computing BCNI or SACN introduce significant additional computational overhead during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gx0765T5kL", "forum": "unZhwukf0T", "replyto": "unZhwukf0T", "signatures": ["ICLR.cc/2026/Conference/Submission19699/Reviewer_S25s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19699/Reviewer_S25s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055994447, "cdate": 1762055994447, "tmdate": 1762931540896, "mdate": 1762931540896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAT-Video, a corruption-aware training framework for latent video diffusion models (LVDMs). It introduces two structured, low-rank embedding perturbations: Batch-Centered Noise Injection (BCNI) and Spectrum-Aware Contextual Noise (SACN). The idea is to inject data-aligned noise during training to improve robustness to noisy text/multimodal conditioning and reduce semantic drift across timesteps. The method is supported by a theoretical sketch (entropy/Wasserstein/score-drift bounds) and experiments on WebVid-2M, MSR-VTT, MSVD, and UCF-101, with reported FVD gains over Gaussian/Uniform corruption and some large diffusion baselines; there are also brief extensions to autoregressive models and multimodal video understanding"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Robustness of T2V models to noisy/ambiguous conditioning is important and under-tested. The paper articulates temporal error accumulation in diffusion and motivates structured corruption specifically for video.\n\nBCNI perturbs along deviations from batch mean; SACN perturbs along principal spectral modes with exponentially decayed variances. Both add minimal overhead and have a single scale hyper-parameter."}, "weaknesses": {"value": "1. Novelty is limited relative to prior “noisy/structured conditioning” literature; positioning is dated.\nInjecting noise into conditioning embeddings (Gaussian/Uniform, token-level swaps/replacements) and exploiting structure/low-rank directions have been explored extensively in image diffusion and multimodal finetuning (e.g., corruption-aware pretraining, token-level perturbations, NEFTune-style noisy embeddings). The paper cites several such works but doesn’t clearly differentiate BCNI/SACN as more than “reasonable engineering variants” adapted to video. A crisper technical delta vs. structured conditioning noise in recent video works is missing. Suggest: provide head-to-head against stronger structured baselines, not only isotropic Gaussian/Uniform or simple temporal gradients (TANI/HSCAN as defined). For instance, compare to learned/adaptive corruption schedules, prompt-space adversarial perturbations with temporal regularizers, or curriculum-style noise aligned to caption syntax/scene cuts\n\n2. Experimental scope feels behind current 2025-26 standards; datasets/architectures and metrics are narrow.\nCore results rely on WebVid-2M/MSR-VTT/MSVD/UCF-101 with FVD-centric reporting. Modern T2V evaluation has moved toward longer videos, higher resolutions, compositional control, and stronger perception-aligned metrics (e.g., VBench subsets with motion/physics consistency, human studies with calibrated protocols). The paper mentions VBench/EvalCrafter in passing, but full tables are pushed to the appendix; the main text should surface these with stronger analysis and significance tests. Also, many baselines listed are from earlier generations of models; direct comparisons to current-gen LVDMs/DiT-style video transformers trained at scale are missing. Actionable:\nAdd long-horizon (≥10–16 s) and high-res evaluations;\nInclude modern SOTA baselines trained on 2024–2026 corpora (and/or reproduce baseline training with matched compute);\nReport human preference and motion/consistency metrics prominently in main text.\n\n3. Claims vs. baselines are hard to validate due to scale mismatch and unclear fairness.\nSeveral “beats larger models with 5× less data” statements are made, but the compared methods differ in architecture, parameterization, pretraining recipes, and dataset curation. Without matched compute / strong-to-strong comparisons (e.g., same backbone with/without CAT; or retrained SOTA with the authors’ code), it’s difficult to attribute gains to BCNI/SACN rather than other confounders. Please (i) include paired-control runs on the same modern backbone at matched data/compute; (ii) show scaling curves (data, steps, guidance, steps vs. FVD) for clean vs. CAT; (iii) report statistical significance (mean±std is in the appendix, but main-text needs tests across seeds).\n\n4. Theory is mostly appendix-level and not tightly coupled to practice.\nThe paper sketches entropy/Wasserstein/score-drift bounds and a D/d “complexity gap,” but practical instantiations (how d is chosen/estimated online, how SVD in SACN scales with sequence length, and why the assumed low-frequency dominance universally holds) are not convincingly validated. The main text should tie specific theorems to measurable proxies (e.g., Lipschitz estimates, score-norm smoothness, sensitivity slopes) and show correlations across datasets/noise levels. Provide ablations on rank d, spectral weighting schedules, and wall-clock overhead broken down by operator.\n\n5. The multimodal video understanding experiment (AVSD) is conducted at 0.5B scale only (LLaVA-OV-0.5B-FT and PAVE-0.5B). By ICLR-26 standards, this falls short of contemporary MLLM practice (7B/13B and stronger video-language backbones). For a credible “model-agnostic” claim, please include ≥7B variants (e.g., PAVE-7B/13B or comparable video-LLaVA baselines\n\n6. The paper claims “we also validate scalability by extending CAT to autoregressive video generation (NOVA)…,” but NOVA is not tabulated in the main paper. Table 3(a) discusses scalability to AR using MAGVIT/CogVideo as references, while NOVA-specific results are not shown; the authors point to Appendix Tables 14–16, yet Table 14 aggregates AR results by corruption type (BCNI/SACN/Gaussian/Uniform) without a clear NOVA row, making the claim hard to verify from the main text. Please provide an explicit NOVA baseline line with matched settings (params, data, steps) and report AR scaling curves (data/compute vs. FVD) to substantiate “model-agnostic” robustness."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xyjAauAItA", "forum": "unZhwukf0T", "replyto": "unZhwukf0T", "signatures": ["ICLR.cc/2026/Conference/Submission19699/Reviewer_Nqx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19699/Reviewer_Nqx2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762423717259, "cdate": 1762423717259, "tmdate": 1762931540294, "mdate": 1762931540294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}