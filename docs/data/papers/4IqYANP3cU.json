{"id": "4IqYANP3cU", "number": 6082, "cdate": 1757952389223, "mdate": 1763116744216, "content": {"title": "Conceptual Archetype Decomposition for Interpretable and Generalizable Model Decisions", "abstract": "Traditional concept decomposition methods have made significant progress in improving the interpretability of deep learning models, but they still face many challenges. A key issue is that they often lack traceable explanations for concepts, making it difficult to understand and verify how models make decisions and provide explanations based on specific concepts. To overcome this limitation, this paper proposes a new method—Conceptual Archetype Decomposition (CAD)—which aims to provide more interpretable concept learning and decision-making process. Unlike existing methods, our approach ensures that each concept can be represented as a linear combination of training samples, with its total activation value equal to 1. This constraint limits the learning space of the concepts and enhances their interpretability. Therefore, the advantage of our method lies in its fine-grained concept activation decomposition, which directly constructs the explanatory space between training samples and concepts. Through a dual-index decision mechanism, we deeply analyze the relationship between test samples and training samples. Extensive experiments on the CUB and ImageNet datasets demonstrate that our model not only improves decision transparency but also exhibits stronger generalization ability in multi-class classification tasks. Our code is available at: https://anonymous.4open.science/r/CAD-4510/", "tldr": "", "keywords": ["Archetype Decomposition", "Model Transparency"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ddaef414375d50c2e43dd33ec6ecacd6a00160b5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Conceptual Archetype Decomposition (CAD), a novel inherent framework designed to improve the interpretability and generalization of concept-based models. Existing decomposition approaches, such as CRAFT, utilize Non-Negative Matrix Factorization (NMF) to extract latent concepts from neural activations; however, they often fail to provide traceable or clear semantically meaningful explanations, as their concepts are not explicitly linked to real training samples. CAD learns the concept from samples and ensures that the decomposition concept can be used to reconstruct the original features, as evidenced by the experiment, which shows a lower disparity between the reconstructed errors of training and testing samples, and a lesser sacrifice of classification capability compared to CRAFT."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The CAD provides an inherent concept decomposition method that aims to improve and offer an intuitive interpretation.\n2. The CAD provides a traceable intrinsic explanation for the concept decomposition and bridges the relation between test and training samples."}, "weaknesses": {"value": "1. Although the paper claims that CAD achieves better interpretability, it does not provide experimental evidence to support this assertion.  \n\n2. The formulation of $A \\approx A^T C^T B$  is confusing, as the dimensions between the left-hand side and the right-hand side appear to be inconsistent. Based on the description, the dimension of left-hand side formulation is $A \\in \\mathbb{R}^{nwh\\times c}$. However, the right-hand side formulation doesn’t match. Besides, how $A^TC^T \\in \\mathbb{R}^{c \\times k}$ dot product with $B \\in \\mathbb{B}^{nwh \\times k}$?\n\n3. The presented concept in Figures 2 and 3 are hardly able to convey the clear semantics, which is also hard to convince that the CAD can provide clearer concept semantics than the CRAFT.  \n\n4. It would be helpful if the authors could provide qualitative results demonstrating that CAD supports more interpretable and more clearly semantical concepts compared to CRAFT.  \n\n5. The experiment in Table 1 shows the lower concept reconstruction error between the training and testing sets, but the purpose of the experiment isn’t clear."}, "questions": {"value": "1. The formulation in Equation 3 ($A-A^T C^T B$) does not seem to match the symbol in Figure 4 ($ACB$).  \n2. How is the performance in Experiment 2 calculated? The reported results reach nearly 100% across different models and datasets, which raises curiosity about the evaluation procedure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zaZqkJLFrm", "forum": "4IqYANP3cU", "replyto": "4IqYANP3cU", "signatures": ["ICLR.cc/2026/Conference/Submission6082/Reviewer_HYzh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6082/Reviewer_HYzh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540594596, "cdate": 1761540594596, "tmdate": 1762918453611, "mdate": 1762918453611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gwSw0bs5JA", "forum": "4IqYANP3cU", "replyto": "4IqYANP3cU", "signatures": ["ICLR.cc/2026/Conference/Submission6082/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6082/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116743086, "cdate": 1763116743086, "tmdate": 1763116743086, "mdate": 1763116743086, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Conceptual Archetype Decomposition (CAD), a concept-based factorization method defined as \n$\\hat{A} = A^\\top C^\\top B$. \nHere, $A$ is the activation matrix, $C$ (concept index) ensures each concept $z = A^\\top C^\\top$ is a convex combination of training activations via per-column softmax normalization, and $B$ (concept reconstruction) expresses samples as convex combinations of concepts. \nThis nested convex-hull structure allows test samples to be decomposed into training-defined concepts by fixing $C$ and optimizing $B$."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper clearly identifies two limitations of prior work, CRAFT:\n(i) extracted concepts lack semantic interpretability, and\n(ii) test samples are not explicitly connected to training samples.\nCAD’s convex-hull formulation directly targets these issues.\n+ The geometric idea of keeping reconstructions inside the convex hull of training activations is elegant and provides a theoretically consistent interpretation space."}, "weaknesses": {"value": "Overall, the paper suffers from serious presentation and formatting issues:\nfigures are blurry, equations are poorly typeset, page size and headers do not follow the official ICLR template, and the writing quality is well below conference standards.\n\n1. Limited datasets.\nOnly CUB and ImageNet are used, which are not the typical benchmarks for concept-based interpretability (e.g., Broden, CelebA, AwA).\nMore fine-grained or domain-diverse datasets are needed.\n\n2. Inadequate baselines.\nThe paper only compares with PCA and CRAFT.\nPCA is a dimensionality-reduction method, not a concept decomposition approach.\nProper baselines should include TCAV, ACE.\nIt is unclear why CRAFT’s original comparisons (e.g., with ACE) were not reproduced.\n\n3. Mischaracterization of CBMs.\nThe authors claim that inherently interpretable models necessarily trade off accuracy, which is not universally true, many CBM variants achieve both interpretability and strong performance.\n\n4. Missing comparison with attribution methods.\nNo qualitative or quantitative comparison with Grad-CAM, Integrated Gradients, or other post-hoc visualization baselines is provided.\n\n5. Poor visualization quality.\nThe presented decompositions are hard to interpret visually; it is unclear whether the extracted components have meaningful semantic content."}, "questions": {"value": "1. Concept definition on ImageNet:\nHow are the ImageNet “concepts’’ obtained, and how is the number of concepts (10–50) selected?\n\n2. Use of the same weight matrix C (lines 62–63):\nTechnically, this forces every test representation to be reconstructed as a convex combination of training samples.\nWhile this prevents OOD extrapolation, it completely removes the ability to represent novel concepts outside the training distribution.\n\n3. Unclear statement at lines 134–135 (“subregion cropping (?)”):\nWhat method is referenced here? Please provide a citation or clarification.\n\n4. Dataset choice:\nWhy are only two datasets evaluated?\nMore diverse or fine-grained datasets (e.g., Flowers, Aircraft, or medical concept benchmarks) would better validate the claimed interpretability and robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pbA1yEmDqp", "forum": "4IqYANP3cU", "replyto": "4IqYANP3cU", "signatures": ["ICLR.cc/2026/Conference/Submission6082/Reviewer_3Xub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6082/Reviewer_3Xub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797293545, "cdate": 1761797293545, "tmdate": 1762918453025, "mdate": 1762918453025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Conceptual Archetype Decomposition (CAD), a method that improves model interpretability by representing concepts as fixed-sum linear combinations of training samples. This approach enhances decision transparency and shows stronger generalization of reconstructed classification performance on two image classification tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The intention to make concepts traceable back to its original training sample is good."}, "weaknesses": {"value": "1. Unsupported Claim of Interpretability vs. Evidence: The paper's central thesis rests on achieving a \"more interpretable\" process. However, this claim is asserted rather than empirically demonstrated. The authors equate the existence of a \"traceable\" method (concepts as linear combinations of samples) with a \"more interpretable\" outcome for the user. To be convincing, the paper must provide direct evidence of this interpretability. This could include, for example, qualitative case studies that walk through a test sample's decision, trace it back to the specific training \"archetypes,\" and demonstrate how this \"dual-index\" explanation provides a more lucid or actionable insight than existing methods. Without this, the primary claim remains unsubstantiated. Fig 3 and 4 do not suffice.\n\n2. Lacking Formal Interpretability Benchmarks: The field of XAI has moved toward formally evaluating interpretability claims. The paper would be significantly strengthened by benchmarking its method against established interpretability metrics. While the method is new, its utility as an explanation is not measured. For instance, the authors could adopt benchmarks similar to those in CRAFT or related works, evaluating concept utility, completeness, or faithfulness. How effective are these concepts at explaining model failures? Can users leverage these \"archetypes\" to debug the model? By omitting such analyses, the paper misses a crucial opportunity to quantify how and how much more interpretable its method truly is.\n\n3. Lacking contextualization (Automatic Concept Methods): The paper's experimental validation and related work sections are notably missing a key class of competitors: automatic concept extraction methods. The authors should discuss and quantitatively compare CAD against prominent baselines (e.g. ACE). This is a critical omission. It is unclear whether CAD (which seems to decompose into pre-defined or sample-based archetypes) offers a practical advantage over methods that discover concepts automatically from the model's latent space. This comparison is essential for contextualizing the paper's contribution and understanding the trade-offs of the proposed \"conceptual archetype\" approach. Sparse autoencoders are also highly relevant in reconstructing representations in a more interpretable manner.\n\n4. Lacking Ablation Study: The proposed CAD method involves several key design choices that are presented without justification or analysis. A robust ablation study is necessary to understand the method's sensitivity and to validate these choices. For example:\n* Layer Sensitivity: From which layer are the \"conceptual archetypes\" extracted? How does this choice (e.g., early, middle, vs. final convolutional layers) impact the resulting concepts' quality and the model's downstream performance?\n* Architecture Dependence: The experiments are conducted on CUB and ImageNet, but the robustness of CAD across different backbone architectures (e.g., ResNets, Vision Transformers) is not explored. Does the method's effectiveness depend on a specific architectural family? Without these ablations, it is difficult to assess the method's generalizability or understand which components of the framework are most critical to its success."}, "questions": {"value": "See weaknesses. The most significant question is how CAD is more interpretable besides hand-wavey arguments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5hsIt20Fv", "forum": "4IqYANP3cU", "replyto": "4IqYANP3cU", "signatures": ["ICLR.cc/2026/Conference/Submission6082/Reviewer_ob9W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6082/Reviewer_ob9W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032441266, "cdate": 1762032441266, "tmdate": 1762918452171, "mdate": 1762918452171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed Conceptual Archetype Decomposition (CAD), a post-hoc framework for improving model interpretability through concept-based analysis. CAD represents each concept as a convex combination of training activations, constrained such that the combination weights sum to one, forming what the authors call a nested bilayer convex structure. \nThis formulation introduces two matrices: a concept index matrix $C$ that links each concept to its contributing samples, and a concept reconstruction matrix $B$ that reconstructs each sample from the learned concepts. \nThe authors claim that this convex-hull design naturally induces low-entropy sparsity and improves both interpretability and generalization. Experiments on the CUB and ImageNet datasets reportedly show that CAD achieves lower reconstruction error and higher classification accuracy on reconstructed activations than baseline methods such as CRAFT and PCA."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **S1. Geometrically Consistent and Intuitively Structured Design**\n\nThe convex-hull formulation in CAD provides a straightforward way to keep concept representations grounded in the training data. \nBy requiring that each concept be expressed as a convex combination of existing activations, the method maintains internal consistency and avoids producing out-of-distribution features. \nWhile this idea is conceptually simple, it offers a reasonable geometric intuition for linking concepts to data without requiring architectural changes or complex supervision. The formulation thus gives CAD a clear structure that is easy to interpret, even if its novelty and practical advantages remain modest."}, "weaknesses": {"value": "- **W1. Overstated Theoretical Guarantees about Sparsity and Archetypal Convergence**\n\nCAD’s theoretical section (Sec. 3.3.2) claims that its convex-combination design naturally produces sparse matrices without a regularized loss function, citing Caratheodory’s theorem as justification. \nHowever, this interpretation misuses the theorem’s scope. As established in classical convex analysis [1, 2], the theorem guarantees only that any point in $\\mathbb{R}^d$ can be expressed as a convex combination of at most $d + 1$ points from a set. \nIt is purely existential and does not imply that an optimization procedure will converge to such sparse representations or that learned coefficients will exhibit low entropy. \nSince CAD’s $B$ and $C$ matrices are parameterized by softmax activations, their weights are strictly positive and dense unless explicitly regularized. \nWithout an entropy or $l_1$ penalty, the model cannot ensure the sparsity or low-entropy structure claimed as an inherent property.\n\nThe authors also claim that “archetypes converge to extreme points of the convex hull,” referencing Cutler & Breiman (1994). \nYet both this work and later analyses (e.g., [3]) emphasize that archetypes are constrained to lie within the convex hull and typically tend toward its boundary, not the vertices themselves. \nConvergence to extreme points occurs only under restrictive geometric conditions (e.g., noiseless or perfectly separable data). \nBecause CAD’s interpretability depends on equating each concept with an atomic exemplar anchored at a hull vertex, the absence of a theoretical or algorithmic guarantee undermines this key claim. \nTo strengthen the argument, the authors could explicitly acknowledge the existential--not constructive--nature of Carathéodory’s theorem, and introduce mechanisms (e.g., entropy regularization or simplex-sharpening) that encourage sparse, vertex-proximal archetypes in practice.\n\n- **W2. Weak Semantic Grounding and Foreground–Background Confusion in Concept Visualization**\n\nWhile CAD claims to produce “conceptual archetypes” that capture human-interpretable object features, the visualizations in Figs. 2 and 3 reveal that many top-ranked crops correspond to background regions or peripheral textures rather than the foreground object (e.g., bird body parts). \nThis pattern indicates that CAD’s convex-combination mechanism does not distinguish between semantically meaningful regions and incidental correlations in feature space. Because the model operates purely on activation similarity without spatial or saliency constraints, it tends to select high-activation but non-diagnostic features--branches, skies, or color patches--that co-occur with classes but are not intrinsic concepts. Consequently, the resulting archetypes appear statistically coherent yet semantically shallow, weakening the claim that they offer transparent, object-centric explanations.\n\nThis limitation contrasts with prior post-hoc concept-based XAI methods, such as [4], [5], and [6], which explicitly assess concept existence, localization, and faithfulness to model decisions. \nThese methods incorporate tests to verify that discovered concepts align with human-perceptible regions and influence predictions, and they often include cleanup or hierarchy mechanisms to mitigate background bias. \nThe proposed method, in contrast, reports no localization or intervention analysis and lacks mechanisms to ensure that archetypes reflect genuine object semantics rather than background co-variation. Integrating object-centric priors--such as attention weighting, saliency-guided masking, or class-conditional filtering—would help align CAD’s archetypal decomposition with the interpretability standards established in the concept-based XAI literature.\n\n- **W3. Limited Experimental Design and Incomplete Evaluation Metrics**\n\nThe experimental validation of CAD is too narrow to substantiate its claims of interpretability and generalization. \nThe authors evaluated only on CUB and ImageNet, using two backbones (NF-ResNet50, ViT-B/32) and just two baselines--[5] and PCA. While CRAFT is a fair comparison, PCA is not a concept-based interpretability model and offers little insight into semantic or causal fidelity. \nMore recent post-hoc concept-based frameworks, including [4], [6], and [7], provide complementary criteria such as concept localization, completeness, faithfulness, and human alignment. Omitting these baselines and metrics makes CAD’s reported gains difficult to interpret within the current explainability landscape.\n\nMoreover, CAD’s evaluation relies solely on reconstruction MSE and classification accuracy of reconstructed features, which assess geometric fidelity but not whether the learned concepts actually drive predictions or align with human-interpretable semantics. \nTo make its experimental design more rigorous, the authors should adopt faithfulness and intervention metrics such as concept insertion/deletion or TCAV sensitivity tests to validate causal relevance, and report richer diagnostics like concept sparsity, activation entropy, and overlap statistics to empirically verify the claimed “low-entropy” structure. \n\n\n- **Reference**\n- [1] Rockafellar RT. Convex analysis:(pms-28).; Ch.17-(3)\n- [2] Boyd S, Vandenberghe L. Convex optimization. Cambridge university press; 2004 Mar 8.; Sec. 2.3.4 explicitly notes that the theorem “provides an upper bound on the number of atoms in a convex combination” and does not guarantee that algorithms yield sparse or vertex-anchored representations.\n- [3] Thurau C, Kersting K, Bauckhage C. Yes we can: simplex volume maximization for descriptive web-scale matrix factorization. InProceedings of the 19th ACM international conference on information and knowledge management 2010 Oct 26 (pp. 1785-1788).; Reinforces that archetypes’ positions depend on data geometry and optimization bias; convergence to vertices is not guaranteed without additional constraints.\n- [4] Ghorbani A, Wexler J, Zou JY, Kim B. Towards automatic concept-based explanations. Advances in neural information processing systems. 2019;32.\n- [5] Fel T, Picard A, Bethune L, Boissin T, Vigouroux D, Colin J, Cadène R, Serre T. Craft: Concept recursive activation factorization for explainability. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 2711-2721).\n- [6] Vielhaben J, Bluecher S, Strodthoff N. Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees. Trans. Mach. Learn. Res.. 2023 Jan 1.\n- [7] Kondapaneni N, Mac Aodha O, Perona P. Representational similarity via interpretable visual concepts. InThe Thirteenth International Conference on Learning Representations 2025 Mar 30."}, "questions": {"value": "Most of my main concerns or questions have been outlined in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dBBgFjfatW", "forum": "4IqYANP3cU", "replyto": "4IqYANP3cU", "signatures": ["ICLR.cc/2026/Conference/Submission6082/Reviewer_aYsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6082/Reviewer_aYsc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117546209, "cdate": 1762117546209, "tmdate": 1762918451614, "mdate": 1762918451614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}