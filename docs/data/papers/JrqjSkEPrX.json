{"id": "JrqjSkEPrX", "number": 13486, "cdate": 1758218508166, "mdate": 1759897433705, "content": {"title": "ChemEval: A Multi-level and Fine-grained Chemical Capability Evaluation for Large Language Models", "abstract": "The emergence of Large Language Models (LLMs) in chemistry marks a significant advancement in applying artificial intelligence to chemical sciences. While these models show promising potential, their effective application in chemistry demands sophisticated evaluation protocols that address the field's inherent complexities. To bridge this critical gap, we introduce ChemEval, an innovative hierarchical assessment framework specifically designed to evaluate LLMs' capabilities across chemical domains. Our methodology incorporates a distinctive four-tier progression system, spanning from basic chemical concepts to advanced theoretical principles. Sixty-two textual and multimodal tasks are designed to enable researchers to conduct fine-grained analysis of model capabilities and achieve comprehensive evaluation via carefully crafted assessment protocols. The framework integrates carefully curated open-source datasets with expert-validated materials, ensuring both practical relevance and scientific rigor. In our experiments, we evaluated the performance of most main-stream LLMs using both zero-shot and few-shot approaches, with carefully designed examples and prompts. Results indicate that general-purpose LLMs, while proficient in understanding chemical literature and following instructions, struggle with tasks requiring deep chemical expertise. In contrast, chemical LLMs perform better in technical tasks but show limitations in general language processing. These findings highlight both the current limitations and future opportunities for LLMs in chemistry. Our research provides a systematic framework for advancing the application of artificial intelligence in chemical research, potentially facilitating new discoveries in the field.", "tldr": "", "keywords": ["Large Language Models", "Benchmark", "Chemical Knowledge Inference"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce8949db8e9d4324faefd16c0cec53c394599493.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a comprehensive benchmark to evaluate the chemical capabilities of LLMs from multiple dimensions. Their benchmark, ChemEval, includes 4 progressive levels, evaluates 13 dimensions of LLMs’ capabilities, and features 62 distinct chemical tasks. The paper provides a thorough description of the data collection process and the task construction methodology. Finally, multiple advanced LLMs, including general LLMs and Chemical LLMs, are evaluated using ChemEval, and detailed analyses are conducted."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates substantial effort in data collection and task construction. I highly recognize its important contribution to establishing a standardized benchmark for evaluating LLMs’ chemical capabilities and to enriching the diversity of evaluation tasks in this area."}, "weaknesses": {"value": "1. The position of the figures should be organized more properly.\n2. After carefully examining the authors’ anonymously released dataset, I found that the quality of some tasks is concerning. Certain tasks appear to lack chemical validity and, therefore, fail to reflect the model’s true chemical capability accurately. For example:\n    * In the reaction time recommendation task (TimeRec), the reaction time should depend on factors such as catalyst type, reaction temperature, reactant concentration, and target yield. However, the task only provides the reactants and products, making the question essentially unanswerable.\n    * The Synthetic Difficulty Evaluation task (SynDE) asks the model to assign a score to represent the synthetic difficulty of a molecule. The concept of “synthetic difficulty score” itself is human-defined, but the task neither specifies the range or meaning of the score nor clarifies whether a higher score indicates easier or harder synthesis. As a result, the question again becomes ill-posed and unanswerable.\n\n    Issues like these raise serious concerns about the overall quality and validity of the dataset’s tasks. While expanding and diversifying the evaluation of chemical capabilities is indeed very important, the added tasks must at least ensure that they meaningfully and correctly assess a model’s chemical understanding. (In addition, the released dataset is difficult to inspect: the task categorizations are inconsistent with those described in the paper, many entries contain Chinese characters, and the number of samples differs from what is reported. These inconsistencies made it quite challenging to locate and examine data for specific tasks. I hope the authors will address these issues in future updates, as doing so would greatly improve the usability of the dataset.)\n3. It seems that the number of entries in ChemEval is relatively small. For most individual tasks, the dataset contains fewer than 50 samples. It is unclear how such a limited number of instances can provide sufficient representativeness to reliably assess the model’s performance and capabilities on each task.\n4. The conclusion regarding catastrophic forgetting in the paper is not properly reached. To properly evaluate whether catastrophic forgetting occurs, the comparison should at least be made between each chemistry LLM and its corresponding general LLM base model, such as ChemDFM vs. LLaMA, ChemLLM vs. InternLM, and LLasMol vs. Mistral. The fact that these chemical LLMs exhibit weaker general language performance than more advanced general LLMs does not provide valid evidence for the presence or absence of catastrophic forgetting during training.\n5. A completely new model named ChemSpark appears in the paper, but there is no corresponding citation, paper, or webpage reference. I was unable to find any information about this model, so it is unclear how it was trained, whether there was any data leakage, or whether dedicated multi-task finetuning was involved. Without further clarification, the conclusions drawn from comparisons involving this model seem unreliable."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qlLEc9bk2O", "forum": "JrqjSkEPrX", "replyto": "JrqjSkEPrX", "signatures": ["ICLR.cc/2026/Conference/Submission13486/Reviewer_Rxp7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13486/Reviewer_Rxp7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639706890, "cdate": 1761639706890, "tmdate": 1762924103291, "mdate": 1762924103291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authers introduce ChemEval, a hierarchical benchmark for evaluating LLMs on chemical tasks. The framework comprises four progressive levels, 13 capability dimensions and 62 tasks cover a comprehensive chemistry domain task.\nEvaluation of 16 general LLMs (GPT-4o, Claude-3.7, Gemini-2.5, Qwen, LLaMA, DeepSeek) and 4 chemistry-specific LLMs (ChemDFM, ChemLLM, LlaSMol, ChemSpark) reveals that general models excel at literature understanding and instruction-following but struggle with molecular reasoning, while specialized models show the opposite pattern, exhibiting catastrophic forgetting in general capabilities but superior performance in domain-specific tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "ChemEval's four-tier progression across 62 tasks with 37 novel dimensions (CharME, CatTE, multimodal spectroscopy/structure recognition) addresses gaps in prior benchmarks while covering real chemical workflows (retrosynthesis, reaction prediction, property estimation).\n\nA comprehensive coverage of data: three-stage pipeline combining 25 open-source datasets with expert-curated materials, explicit deduplication against model training sets (ChemDFM, ChemLLM), and professional annotation validation ensures quality and reduces data leakage risks.\n\nIn this work, 20 models with domain-appropriate metrics (Tanimoto, NRMSE, validity ratios) was evaluated, quantifying the specialization-generalization tradeoff and providing actionable guidance for model development while open-source release enhances reproducibility."}, "weaknesses": {"value": "Insufficient Justification for Task Selection and Missing Critical Tasks: The paper claims comprehensive coverage but still missing several core chemical AI tasks: Spectroscopy-to-structure elucidation, Safety/toxicity prediction. The 37 custom tasks lack clear selection criteria.\n\nSingle-domain focus: Evaluation is confined to chemistry; claims about LLM capabilities don't transfer to biology, materials science, or physics without cross-domain validation.\n\nMinor:\nSection 3.2.4 mentions \"continuously adjusted instructions based on GPT-4o feedback\" GPT-4o-generated prompts may bias evaluation toward GPT-4o's strengths. Were prompts validated by human experts?\n\nThe paper claims deduplication against ChemDFM/ChemLLM training sets, but what about GPT-4o's training data (cutoff Oct 2023)?"}, "questions": {"value": "What percentage of samples were rejected during filtering? What constituted \"irrelevant\" data (Figure 2 step b)? For LLM-as-judge metrics (LLM Score), which model was used—GPT-4o? This introduces potential bias where GPT-4o evaluates its own outputs.\nWhy does scaling fail? Table 3 shows Qwen2.5-7B→72B increases MCTask accuracy 59.6%→67.2% (+7.6pp) but decreases MolPC 64.04%→48.13% (-15.9pp). This is bizarre and unexplained. Is this overfitting? Requires error analysis.\nWhy do specialized models collapse on general tasks? The catastrophic forgetting observation (ChemLLM 0.00 on many tasks) is reported but not investigated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6zwlZEAlTU", "forum": "JrqjSkEPrX", "replyto": "JrqjSkEPrX", "signatures": ["ICLR.cc/2026/Conference/Submission13486/Reviewer_9BLh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13486/Reviewer_9BLh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887682045, "cdate": 1761887682045, "tmdate": 1762924102547, "mdate": 1762924102547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChemEval, a hierarchical benchmark to evaluate LLMs’ chemical capabilities. It organizes 62 tasks into four progressive levels (from basic concepts and literature-style QA → extraction and structured understanding → molecular-level reasoning → high-level synthesis/mechanism tasks). The benchmark is mixed-source: some tasks are adapted from existing open datasets (Mol-Instructions, ChemLLM-Bench, SMolInstruct, Llasmol), and 37+ tasks are newly authored/curated by chemistry experts from collaborating universities, including proprietary laboratory records and multimodal materials (tables, spectra, reaction schemes)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Compared with prior suites (ChemLLM-Bench, ChemCrow eval, ChemBench-MM, Mol-Instructions), this work (i) unifies text-only and multimodal chemistry tasks in one place, (ii) explicitly stages difficulty into four levels, and (iii) evaluates many models under a comparable protocol. \n\n2. Good task–metric alignment. Tasks that are structure-like or chemistry-like use appropriate metrics (Tanimoto, L2, exact-format scoring), while the knowledge / reading tasks use accuracy-style metrics."}, "weaknesses": {"value": "1. The core test size is 3,120 items across 62 tasks → that’s ~50 items per task on average, sometimes less. For high-variance LLM outputs (esp. with many format-sensitive IE tasks), this is thin, and you can’t draw very strong conclusions about model scaling, few-shot gains, or thinking-vs-non-thinking from 30–80 examples. Yet the result section does exactly that. ICLR will push back: “you are positioning this as a comprehensive benchmark but the statistical mass is closer to a curated diagnostic suite.” The paper acknowledges the cost of expert construction, but the conclusion section still speaks as if we have a definitive benchmark. That’s overstated relative to the data volume\n\n2. The paper does not adequately describe the annotation pipeline, especially for the scientific knowledge deduction tasks, which appear to be the most challenging and high-value part of ChemEval. In contrast to the relatively straightforward literature-understanding and molecule-level tasks, deduction requires multi-step reasoning, selection of relevant premises, and often domain-specific normalization. Right now, the description of how these instances were constructed, validated, and standardized is too brief to assess reliability. The authors are encouraged to spell out: (i) the source of the raw material (textbook, lab record, expert-written), (ii) how the gold answer was derived from it, (iii) whether multiple annotators or chemistry experts were involved, and (iv) how disagreements were resolved. Without this, it is hard to judge the difficulty and reproducibility of the most important part of the benchmark.\n\n Section 3.2.2–3.2.4 describes a 3-step pipeline (collect → filter → construct Q/A) but when we get to retrosynthesis, condition recommendation, reaction outcome prediction, mechanism analysis (the 13 tasks in Level 4), the paper stays at the narrative level. \n\n3. Although the paper frames ChemEval as addressing scenarios where models must read heterogeneous chemical inputs (text, tables, schemes) and then reason, the experiments only evaluate plain LLM inference (mostly greedy decoding) without any agent-style or chemistry-tool–augmented setups. For this domain, that’s a real gap: in practice, chemistry LLMs are often run with RDKit/OPSIN/reaction-database calls or ChemCrow-style tool chains to normalize structures, check validity, and retrieve reaction conditions. Without at least a small ablation (e.g. LLM-only vs. LLM + RDKit name-to-structure vs. LLM + reaction lookup) on 2–3 tasks, it is hard to tell whether ChemEval can distinguish different modeling paradigms (language-only vs. tool-augmented) or whether it is mainly measuring prompt-following in a chemistry setting. As written, the results mostly re-confirm a known pattern (“frontier general models > small chemistry-tuned models”) but do not show that the benchmark is sensitive to the way models are used."}, "questions": {"value": "Since many chemistry LLM applications are tool-augmented, can you report results for at least one LLM+tool pipeline (e.g., LLM → RDKit/OPSIN → answer) on a subset of tasks (name↔structure, condition extraction, reaction-type identification)?\n\nid you consider evaluating a standard chemistry agent (e.g., ChemCrow-style tool-calling) as a reference system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kl5yjGUSEC", "forum": "JrqjSkEPrX", "replyto": "JrqjSkEPrX", "signatures": ["ICLR.cc/2026/Conference/Submission13486/Reviewer_wAna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13486/Reviewer_wAna"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982715066, "cdate": 1761982715066, "tmdate": 1762924102267, "mdate": 1762924102267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper introduces ChemEval, a chemistry-specific evaluation suite designed with 4 progressive levels across 13 dimensions of LLM capabilities. The authors claim to innovatively introduce test sets related to information extraction. multimodal tasks are also included. Finally, tasks have been sourced from the web in openly accessible sites, as well as domain experts in chemistry."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and goes in depth into details of analyzing the data, evaluation metrics, and organizing the test-set into tasks, dimensions and levels, which makes the paper and the benchmark very valuable for the field.\nThe paper also gives a very detailed evaluation of several relevant llms such as closed-source state of the art models, as well as chemistry-specific models. The analysis is also in depth and discusses differences between e.g. reasoning and non-reasoning models, open and closed, and chemistry specific and non-specific."}, "weaknesses": {"value": "Some of the tasks, admittedly due to their complexity, require evaluation with an LLM Score. This score in itself was not evaluated, benchmarked against human baselines, or revised thoroughly. Additionally the LLM used to score was not disclosed, which further hinders transparency and reproducibility.\nGiven these points, it is not clear how the authors plan to make this benchmark endure the test of time. If the model is an open-weights model it would be good to mention it and release the weights as an asset together with the benchmark, so that the community can run these evaluations. If the model is a proprietary one, it is even less clear how to do this, as the continued availability of api-based models is not guaranteed.\nFurthermore, the paper is missing a discussion on why the specific llm used was used. For instance, what is the variance of the results if the LLM is changed? are there any specific differences in behaviour patterns between LLMs as evaluators?\n\nIt would also be great to provide a human baseline, to assess how good are expert chemists, and average humans, at the tasks in this evaluation set."}, "questions": {"value": "Please provide more details and ablations on the LLM used for evaluation for some of the tasks. See weaknesses section for more details and elaborate on how do you plan to move about this. If this is to be truly open and sustainable over time, the evaluations should rely on a specific snapshot of an open-weights model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KOV3fsUPXm", "forum": "JrqjSkEPrX", "replyto": "JrqjSkEPrX", "signatures": ["ICLR.cc/2026/Conference/Submission13486/Reviewer_DKh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13486/Reviewer_DKh7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111705885, "cdate": 1762111705885, "tmdate": 1762924101960, "mdate": 1762924101960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}