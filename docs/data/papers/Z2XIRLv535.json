{"id": "Z2XIRLv535", "number": 5792, "cdate": 1757935397696, "mdate": 1759897953248, "content": {"title": "MedGMAE: Gaussian Masked Autoencoders for Medical Volumetric Representation Learning", "abstract": "Self-supervised pre-training has emerged as a critical paradigm for learning transferable representations from unlabeled medical volumetric data. Masked autoencoder based methods have garnered significant attention, yet their application to volumetric medical image faces fundamental limitations from the discrete voxel-level reconstruction objective, which neglects comprehensive anatomical structure continuity. To address this challenge, We propose MedGMAE, a novel framework that replaces traditional voxel reconstruction with 3D Gaussian primitives reconstruction as new perspectives on representation learning. Our approach learns to predict complete sets of 3D Gaussian parameters as semantic abstractions to represent the entire 3D volume, from sparse visible image patches. MedGMAE demonstrates dual utility across medical imaging applications. For representation learning, sparse Gaussian prediction produces superior encoder representations that outperform traditional MAE baselines on downstream segmentation, classification, and registration tasks. For volumetric reconstruction, the Gaussian decoder leverages pretrained anatomical priors to accelerate 3D CT volume reconstruction convergence. Extensive experiments across multiple medical imaging datasets demonstrate that our approach achieves superior performance, establishing a new paradigm for medical image pre-training. Code will be released soon.", "tldr": "MedGMAE replaces traditional voxel-level reconstruction in medical volumetric data pre-training with 3D Gaussian primitives prediction, enabling more effective anatomical continuous representation and faster CT reconstruction convergence.", "keywords": ["3D Gaussian Representation", "Medical Imaging analysis", "Volumetric Representation Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de26d273773048de391ef9d821d9cc8d9835c763.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new representation learning framework based on learning 3D Gaussian primitives, which are then used to render volumes through Gaussian splatting. They argue that this intermediate representation enables the decoder to capture anatomical priors, improving performance on downstream tasks such as CT volume reconstruction, medical image classification, segmentation, and registration. The results are shown on multiple datasets and compared with relevant baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Strengths of the paper:\n\n1. The study includes a comprehensive evaluation across multiple medical downstream tasks, including CT volume reconstruction, image segmentation, disease classification, and deformable image registration, demonstrating the versatility and generalizability of the proposed representation learning framework.\n\n2. To the best of our knowledge, the use of 3D Gaussian primitives for representation learning in medical volumetric data has not been explored before. This work introduces a novel perspective by leveraging Gaussian primitives as an intermediate representation to capture anatomical structure and spatial context, enabling more effective learning for a range of medical imaging tasks."}, "weaknesses": {"value": "Weakness of the paper:\n\n1. One major weakness of the paper is the lack of methodological novelty. The proposed approach is essentially identical to that of [1], with substantial portions(equations, query tokens, masking ratio) of the methodology directly adapted from it, the only difference being its application to 3D medical volumes instead of 2D images. Therefore, I assign a score of zero for the novelty of the proposed method.\n\n(a) The reasons mentioned in the introduction line 048, 052 are exactly the same reasons for [1]. \n\n(b) Regarding the term “sparse representation,” it is unclear how the authors justify calling their representation sparse. Even in [1], the learned representation is not sparse by definition. It may be described as disentangled, but not sparse. The use of this term is confusing and should be clarified or corrected by the authors.\n\n\n2. If i consider this a benchmark paper and not a methodology paper there are several things missing:\n\n(a) No where in the paper i found how many Gaussian's are used for representation learning ? What is the impact of number of gaussians? \n\n(b) What is the impact of different masking ratios ? As the authors mentioned in the paper that medical images have less textural changes compared to natural images, then why use the same masking ratio ? Feels illogical and uninformative.\n\n(c) MAE are only compared in Table 1, missing from other tables.\n\nIn my opinion, as a benchmark, the work is not comprehensive and requires further development to provide more meaningful or complete evaluations.\n\n(3) The code could have been released through an anonymized GitHub repository; however, this is a minor issue.\n\n\n[1] Rajasegaran, Jathushan, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, and Shiry Ginosar. \"Gaussian masked autoencoders.\" arXiv preprint arXiv:2501.03229 (2025)."}, "questions": {"value": "The main question i have are :\n\n1. Why do the authors refer to the proposed approach as a “sparse representation”? Please clarify what is meant by “sparse” in this context.\n\n2. How does this method differ from “Gaussian Masked Autoencoders”? From my understanding, the approach appears to be largely identical, except for its application to 3D medical images. If there are substantive differences, the authors should provide a detailed comparison highlighting which components are new and which are retained from the original method.\n\n3. Since the code for “Gaussian Masked Autoencoders” has not been publicly released, did the authors collaborate with the original developers or otherwise verify that their implementation and assumptions are consistent with the original work?\n\n4. Please clarify the rationale behind using the same hyperparameters (e.g., masking ratio = 0.75) and other modeling choices. Were these parameters empirically validated for 3D medical data, or simply adopted from the original paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hSLrzfuJqC", "forum": "Z2XIRLv535", "replyto": "Z2XIRLv535", "signatures": ["ICLR.cc/2026/Conference/Submission5792/Reviewer_fEUr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5792/Reviewer_fEUr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761163894049, "cdate": 1761163894049, "tmdate": 1762918265250, "mdate": 1762918265250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedGMAE (Medical Gaussian Masked Autoencoder), a novel self-supervised learning framework for volumetric medical image representation that replaces conventional voxel-based masked autoencoding with reconstruction via 3D Gaussian primitives. Traditional 3D masked autoencoders reconstruct missing voxel intensities directly, which the authors argue fails to capture continuous anatomical geometry, produces non-transferable decoders, and wastes parameters due to sparse anatomical occupancy. MedGMAE instead predicts a set of continuous 3D Gaussian parameters—position, scale, rotation, and intensity—to represent anatomical structures compactly and coherently. The decoder serves a dual purpose: it not only supports self-supervised pretraining but also provides a zero-shot, geometry-aware initialization for 3D Gaussian Splatting (3DGS)–based CT reconstruction. The framework achieves 99% parameter reduction relative to voxel-based MAEs and enables a 1.39× speed-up in CT reconstruction convergence. Experimental results on multiple medical imaging benchmarks—AMOS, FLARE’22, BTCV, SegTHOR (segmentation); CT-RATE (classification); IXI/OASIS (registration); and AAPM-Mayo (reconstruction)—show that MedGMAE consistently outperforms state-of-the-art voxel-based and self-supervised baselines (e.g., MAE, HySparK, VoCo, SUP) in both data-efficient and full-data regimes"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- MedGMAE is the first to apply 3D Gaussian parameter prediction within a masked autoencoder framework for medical volumetric pretraining. This fundamentally shifts the pretraining objective from discrete voxel regression to continuous geometric modeling, aligning better with the smooth, anatomical nature of medical volumes.\n- The model architecture (Figure 2, p. 3) elegantly integrates a ViT encoder with a lightweight Gaussian decoder and differentiable volumetric renderer. The authors detail parameterization (position, scale, rotation, intensity), initialization schemes, and differentiable rendering—showing a mature engineering of the 3D Gaussian field within a self-supervised context.\n- Unlike typical masked autoencoders whose decoders are discarded post-pretraining, MedGMAE’s Gaussian decoder is explicitly transferable. It serves as a zero-shot initializer for downstream 3D Gaussian Splatting CT reconstruction, bridging pretraining and reconstruction in a unified formulation."}, "weaknesses": {"value": "- While intuitive, the paper provides little formal reasoning or analysis explaining why Gaussian parameterization yields better anatomical representation. A discussion relating spatial frequency content or anatomical topology to Gaussian smoothness would strengthen the conceptual core.\n- The custom CUDA Gaussian renderer adds substantial implementation complexity, and while Appendix §7.5 details optimizations, performance and stability under different resolutions or anisotropic voxel spacing are not benchmarked.\n- The model produces geometric primitives, yet the paper stops short of analyzing their spatial or anatomical semantics"}, "questions": {"value": "- How does the number of Gaussian primitives (k) affect the trade-off between geometric fidelity and representation compactness? Have you explored adaptive Gaussian selection based on anatomical density?\n- Cite RAPTOR which does some work on volumetric scans using foundation models (https://arxiv.org/abs/2507.08254)\n- Can you provide quantitative evidence linking Gaussian coherence (e.g., overlap or smoothness metrics) to segmentation or registration performance?\n- How much computational overhead does the Gaussian rendering add compared to standard voxel decoders, particularly during pretraining?\n- Did you observe any degenerate Gaussian cases (e.g., extreme scales or vanishing densities), and how are these handled during training?\n- Could the method support partially observed 3D data (sparse-view, partial slices) as a more direct input?\n-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kQM8yGZTGi", "forum": "Z2XIRLv535", "replyto": "Z2XIRLv535", "signatures": ["ICLR.cc/2026/Conference/Submission5792/Reviewer_Nwsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5792/Reviewer_Nwsv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515948762, "cdate": 1761515948762, "tmdate": 1762918264749, "mdate": 1762918264749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for self-supervised pretraining for medical volumes, based on 3D Gaussian primitives (instead of traditional volumetric representations). The idea is that the Gaussian primitives can capture long-range dependencies better than local voxel-based representations. Results are given for multiple datasets and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Good clear paper, explaining the context and method well.\n\n- The motivation for the method seems reasonable.\n\n- Extensive experiments are included for various datasets and tasks.\n\n- Computational requirements are explicitly discussed."}, "weaknesses": {"value": "- The authors use 96x96x96 voxel cubes, which is relatively low-resolution. It remains unclear to me how the method would perform on higher resolution data. For example, medical CT data can be 512^3 up to 2048^3 (see e.g. [1]). Non-medical CT datasets can be even larger. One concern is whether, for higher resolutions, the 3D gaussians are still able to capture long-range correlations well, and required computation time does not become prohibitively large.\n\n- It is not clear how specific hyperparameters settings are chosen by the authors, and how results are affected by different choices for the hyperparameters. This is also try for comparison methods -- how are the hyperparameters chosen for these?\n\n- Results are not always presented clearly -- for example, in Table 3 the caption states that the best result is shown in bold, but TransMorph achieves the best Dice result but isn't bold.\n\n[1] Oostveen, L. J., Boedeker, K. L., Brink, M., Prokop, M., de Lange, F., & Sechopoulos, I. (2020). Physical evaluation of an ultra-high-resolution CT scanner. European radiology, 30(5), 2552-2560."}, "questions": {"value": "- How does the method perform (both in terms of accuracy, and computation time) for larger volumes that can be common in practice?\n\n- How were hyperparameters chosen, and how do hyperparameters affect results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9CkOQsMHXP", "forum": "Z2XIRLv535", "replyto": "Z2XIRLv535", "signatures": ["ICLR.cc/2026/Conference/Submission5792/Reviewer_w2RZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5792/Reviewer_w2RZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852447025, "cdate": 1761852447025, "tmdate": 1762918264355, "mdate": 1762918264355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MedGMAE, a self-supervised pre-training framework for 3D medical images that replaces voxel-level masked reconstruction with prediction of sets of 3D Gaussian primitives followed by differentiable volumetric rendering. The encoder is ViT-based; a lightweight decoder emits k Gaussians which are rendered and compared to masked ground truth. An extended variant (MedGMAE\\*) adds hierarchical residual blocks for coarse-to-fine Gaussian refinement. The authors claim that Gaussian reconstruction better matches anatomical continuity, yields a transferable decoder that can zero-shot initialize 3D Gaussian–based CT reconstruction, and is parameter efficient versus voxel MIM. Experiments show gains on segmentation, classification, registration, and faster convergence for low-dose CT reconstruction when using the decoder as an initializer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper precisely formulates predicting Gaussian sets, detailing tokenization, query tokens, parameter heads, activations/normalization, and rendering, including initialization choices for stability.\n2. The motivation is well-argued, and the limitations of voxel-level objectives for continuous anatomy and the sparsity/efficiency argument for Gaussians are articulated, with a schematic overview (Fig. 1) that connects sparsity to parameter reduction and speedup. \n3. On segmentation with 1% labels, MedGMAE outperforms prior best notably on AMOS and FLARE’22 ; classification and registration show competitive/better results as well.  The ablation studies further support the claim."}, "weaknesses": {"value": "1. The related work briefly contrasts with 2D Gaussian MAE (Rajasegaran et al., 2025) and several medical 3DGS papers, but head-to-head empirical comparisons versus the most relevant medical Gaussian pretext tasks (if any) or Gaussian-rendered proxy tasks are missing; the comparison set is largely voxel-MIM or contrastive SSL. This makes it hard to attribute the transfer gains specifically to Gaussian reconstruction rather than dataset/implementation choices.\n2. The paper states UNETR fine-tuning with consistent preprocessing, but various baselines load official pre-trained weights. Different pre-training corpora/compute may advantage some methods. I suggest the authors to add an explicit data-scale or compute-matched comparison.\n3. The method fixes yet there is no systematic study of k on transfer and reconstruction. The claim of 99.25% parameter reduction relies on a specific setting; a sensitivity sweep would help validate efficiency-generalization tradeoffs.\n4. The main text lists very large pretraining settings (e.g., “batch size 192, 400K steps” vs. Appendix Table 9 “batch size 8, max steps 100K”), making it unclear which configuration corresponds to the reported results; this needs consolidation."}, "questions": {"value": "Please refer to the weaknesses part, especially weakness 2&3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVBLGvc2QZ", "forum": "Z2XIRLv535", "replyto": "Z2XIRLv535", "signatures": ["ICLR.cc/2026/Conference/Submission5792/Reviewer_TUT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5792/Reviewer_TUT3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955166182, "cdate": 1761955166182, "tmdate": 1762918264021, "mdate": 1762918264021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}