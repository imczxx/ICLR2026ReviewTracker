{"id": "AlJzfGWcFB", "number": 6218, "cdate": 1757958931674, "mdate": 1760503524202, "content": {"title": "Learning Representations as Resistance States: Structure-Aware Neuromorphic Sequence Modeling", "abstract": "We propose NeuSpell, a neuromorphic framework that learns representations as resistance states in a memristor crossbar array, offering a non-von Neumann alternative to token-based sequence modeling. Instead of digital tokenization and GPU-accelerated pipelines, structural components of input sequences are directly encoded as conductance dynamics, enabling massively parallel, in-memory inference with ultra-low latency (0.07 ms) and near-zero energy cost. Applied to Tibetan syllable recognition, a task where conventional models struggle due to morphological complexity and data scarcity, NeuSpell achieves 98.2\\% F1 on the SSC-TiM corpus and TUSA benchmark, outperforming state-of-the-art neural and large language models. Beyond this application, our results suggest that resistance-state dynamics can serve as a new foundation for structure-aware, token-free representation learning, opening a path toward efficient neuromorphic architectures that move beyond von Neumann computation.", "tldr": "This paper presents NeuSpell, a neuromorphic framework that learns token-free resistance-state representations in memristor crossbars, achieving state-of-the-art performance on Tibetan spelling correction with new structure-aware datasets.", "keywords": ["Neuromorphic computing; Memristor crossbar; Resistance-state representation; Token-free sequence modeling; Tibetan NLP; Low-resource languages"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/be3fa9ed7b964919f1fa337cfe30a03900fa8985.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}