{"id": "mKgUAO41zf", "number": 9701, "cdate": 1758135421707, "mdate": 1763710287085, "content": {"title": "Binomial Gradient-Based Meta-Learning for Enhanced Meta-Gradient Estimation", "abstract": "Meta-learning offers a principled framework leveraging *task-invariant* priors from related tasks, with which *task-specific* models can be fine-tuned on downstream tasks, even with limited data records. Gradient-based meta-learning (GBML) relies on gradient descent (GD) to adapt the prior to a new task. Albeit effective, these methods incur high computational overhead that scales linearly with the number of GD steps. To enhance efficiency and scalability, existing methods approximate the gradient of prior parameters (meta-gradient) via truncated backpropagation, yet suffer large approximation errors. Targeting accurate approximation, this work puts forth binomial GBML (BinomGBML), which relies on a truncated binomial expansion for meta-gradient estimation. This novel expansion endows more information in the meta-gradient estimation via efficient parallel computation. As a running paradigm applied to model-agnostic meta-learning (MAML), the resultant BinomMAML provably enjoys error bounds that not only improve upon existing approaches, but also decay super-exponentially under mild conditions. Numerical tests corroborate the theoretical analysis and showcase boosted performance with slightly increased computational overhead.", "tldr": "", "keywords": ["Meta-learning", "meta-gradient estimation", "bilevel optimization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09373b1a86bfc2c96888e2a75f804936ccd63b43.pdf", "supplementary_material": "/attachment/4bdd27faa56667408d3658c3372670542a958427.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a meta-learning method BinomMAML, based on MAML, that uses the binomial expansion of a repeated (I - H_k) application in the meta-gradient to derive a parallelizable execution method and low-error approximation by truncating higher order terms in the expansion.  In contrast to truncating on iteration depth, all iterations' hessians can still be used in the approximation, with truncation happening on the degree of their combinatorial interactions.  This is efficiently computed by parallelizing the computation of the second-order values given the first, etc., so that computing the L'th order approximation requires L serialized sets of parallel computations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This is an insightful method whose basic derivation is explained well with clear motivation.\n* Extensive theoretical derivations show its error decreases rapidly (possibly above exponential) and compare to similar derivations for MAML and Truncated MAML, showing excellent improvements.  These are verified on the (standard) sinusoid synthetic problem.\n* The method also realizes gains are in the more real-world data settings of mini-Imagenet and tiered-Imagenet (albeit possibly less than one might hope for based on the theory), but still adequate empirical validation"}, "weaknesses": {"value": "* While the theoretical results are excellent, the method's results on the two real-data tasks is more limited and with relatively small gains. (though, the paper touches on this in the discussion and points out its gains are larger in the 1-shot case with reasonable explanation)\n\n* Although I found the explanation of the method's ideas and the core derivation of the combinatorial algorithms and B operators clear, some of the formulas, and Alg. 1 in particular, are very busy and can be difficult to align with the derivation in Eq 8 and Prop 3.1.  Prop. 3.1 also has some indexing I found difficult to interpret in its use of $k_{L-i} = k_{L-1-i}+1$ as a sum subscript (though, I can see it corresponds to the combinatorial index variables 0<= k_1 < ...< k_l < K)"}, "questions": {"value": "Alg. 1 pseudocode I think could be a little better aligned to the notation in the text, in particular making use of the B operators and showing how the computations get these values.  The indexing is very busy in Alg. 1 and difficult to disentangle.\n\nFigures and tables often appear far from their descriptions in the text and sometimes out of order.  It would be easier if this could be organized so the figures are closer to their main discussion.  For example, sec 4.1 on p8 uses Fig 3 on p.9, even though the next section 4.2 looks at the earlier Fig 2.  Fig 3 is also referenced for the first time earlier on p.4, which is OK, but I felt I had to jump around a little too much to find these several times."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IQJQqnFMGv", "forum": "mKgUAO41zf", "replyto": "mKgUAO41zf", "signatures": ["ICLR.cc/2026/Conference/Submission9701/Reviewer_4xo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9701/Reviewer_4xo1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522680951, "cdate": 1761522680951, "tmdate": 1762921209136, "mdate": 1762921209136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors aim to reduce the computational cost of meta-learning while maintaining a low approximation error. Existing approaches typically achieve efficiency gains at the expense of increased approximation bias. To address this trade-off, the authors propose a new framework called BinomGBML. They observe that conventional gradient-based meta-learning methods suffer from limited parallelism in the Hessian–vector product (HVP) computations. To overcome this limitation, the authors develop a meta-gradient estimator based on the binomial expansion, which expresses the meta-gradient as a truncated binomial series. This expansion is further reformulated into a cascade of vector operators, each of which can be executed in parallel. Theoretically, they show that the proposed estimator achieves a lower error bound compared to existing meta-learning methods. Empirically, they demonstrate the efficiency and effectiveness of BinomGBML through numerical experiments on multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1: The authors carefully investigate why existing variants of gradient-based meta-learning methods are inefficient. Their theoretical analysis provides valuable insights into the underlying causes of computational bottlenecks, helping the community better understand the limitations of prior approaches.\n\n2: The proposed parallelized meta-gradient estimator is an interesting and original contribution. It introduces a new perspective on improving the efficiency of meta-learning by leveraging the binomial expansion to enable parallel computation of Hessian–vector products.\n\n3: The method demonstrates a well-balanced trade-off between computational efficiency and gradient approximation accuracy, achieving significant speedups without sacrificing performance."}, "weaknesses": {"value": "1: The method section is somewhat difficult to follow due to the dense mathematical formulation. Including high-level diagrams or flowcharts to illustrate the overall framework of BinomMAML would greatly enhance clarity and accessibility for readers.\n\n2: As the proposed estimator introduces a new way to compute meta-gradients, the paper would benefit from a more detailed discussion on practical implementation — for instance, how to integrate the estimator into common frameworks like PyTorch and how to manage computational overhead in practice.\n\n3:Although the paper emphasizes efficiency as a major advantage, it does not provide clear experimental results or ablation studies quantifying the computational gains. Presenting concrete benchmarks (e.g., runtime, memory usage, GPU utilization) would strengthen the empirical support for the proposed method."}, "questions": {"value": "1: How can the proposed binomial-expansion based meta-gradient estimator be implemented in practice using PyTorch? A detailed explanation or example would help clarify how the estimator operates and how its components interact in a real training pipeline.\n\n2: How does the runtime performance of the proposed estimator compare with existing baselines such as MAML, iMAML, and other meta-learning methods? Providing empirical comparisons or complexity analysis would help illustrate the computational advantages of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sDZfKJ1frs", "forum": "mKgUAO41zf", "replyto": "mKgUAO41zf", "signatures": ["ICLR.cc/2026/Conference/Submission9701/Reviewer_jYLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9701/Reviewer_jYLW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790808691, "cdate": 1761790808691, "tmdate": 1762921208791, "mdate": 1762921208791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to obtain accurate estimates of the multi-step inner-optimization gradient in gradient-based meta-learning in both fast and memory efficient manner. In MAML, a representative approach to meta-learning, estimating the posterior over task-invariant parameters requires multiple steps of gradient computation, which becomes a major computational bottleneck. To mitigate this, prior work has used truncation of the number of steps that contribute to the gradient approximation or implicit differentiation, yet these approaches suffer from gradient estimation error and numerical instability. This paper shows that, if one replaces a naive decomposition of the multi-step gradient with a decomposition inspired by the binomial expansion when truncating the number of steps, the truncation error decays exponentially with respect to the inner step size. The proposed method can be algebraically expressed as an operator acting on the gradient vector, which yields a parallelizable algorithm. Across several settings including sufficiently realistic ones, the authors demonstrate both theoretically and empirically that the proposed method attains smaller gradient estimation error than existing methods. In addition, as a secondary contribution, they show in theory and experiments that the proposed method reduces space complexity even relative to vanilla MAML."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The problem is very clearly formulated and is important to the community. Gradient-based meta-learning exemplified by MAML is widely studied, and obtaining accurate gradients while controlling computation is obviously one of the central concerns.\n- The paper is well written and easy to follow. In particular, it organizes gradient-based meta-learning from a consistent perspective. The target problem is framed via hierarchical Bayesian modeling with MAML positioned as one solution, and the paper provides a comprehensive review of representative methods for addressing computation time, including FO-MAML, Reptile, Truncated MAML, and implicit MAML. This gives readers a coherent viewpoint and effectively motivates the proposed method.\n- The idea inspired by the binomial expansion solves the problem elegantly with a solid construction. While it does not aim to remove the fundamental growth in computation with the number of steps as implicit MAML does, it is highly practical. The result is a gradient truncation scheme whose error decreases exponentially with the inner step size and that admits parallel computation, which is a noteworthy contribution.\n- The paper provides consistent proofs of upper bounds on gradient estimation error under multiple scenarios, from sufficiently realistic assumptions up to the stronger assumption of convex losses. These theoretical results strongly support the intuition of the method.\n- The theoretical results appear sound, although I could not verify every proofs in full detail.\n- The authors provide numerical validation under realistic settings for gradient estimation accuracy, time complexity, and space complexity. The experiments show that BinomMAML consistently outperforms other methods in gradient accuracy, while the time overhead remains acceptable even compared with TruncMAML. Moreover, although secondary, improved memory efficiency compared with vanilla MAML is also a significant advantage."}, "weaknesses": {"value": "- Although implicit MAML is discussed as prior work, the paper did not provide any of theoretical or empirical comparisons to it. Since iMAML is undoubtedly one of the representative approaches to addressing MAML’s computational burden, would it be possible to include performance comparisons with such methods, either theoretically or experimentally, to validate the effectiveness of the proposal?\n- Theoretical claims from Proposition 3.1 through Theorem 3.10 are central and valuable contributions. However, there is concern that the proofs of these statements are not discussed in the main text at all. For core results such as Proposition 3.1, Theorem 3.2, and Theorem 3.6, even a high-level proof sketch in the main body would substantially aid reader understanding."}, "questions": {"value": "- The proposal appears to conceptually distinguish BinomGBML from BinomMAML, but the practical difference is not clear to me. Chapter 3 seems to assume the MAML problem setting described in Chapter 2. Is this understanding correct?\n- What is the Hybrid Binom-Trunc Estimator section in Appendix D for?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sLvz9AYp5r", "forum": "mKgUAO41zf", "replyto": "mKgUAO41zf", "signatures": ["ICLR.cc/2026/Conference/Submission9701/Reviewer_UEik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9701/Reviewer_UEik"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917923297, "cdate": 1761917923297, "tmdate": 1762921208448, "mdate": 1762921208448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Gradient based meta-learning (GBML) suffers from high computational overhead. While approximations have been proposed to make GBML more tractable, these approximations suffer from large errors. This work proposes BinomMAML via targeted approximations achieved via truncated binomial expansion yields better meta-gradient estimation, in a efficient and scalable manner. The authors present theoretical analysis of their approximation and empirically validate its efficacy in comparisons with a range of competitive baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) The truncated binomial expansion along with the  regrouping terms into a few parallelizable computations is quite appealing\n2) The regrouping of terms enables parallel computation, increasing GPU usage and overall efficiency.\n3) They show quantitive improvements over standard approximation methods"}, "weaknesses": {"value": "1) Concerns about benchmark choice. Prior work (e.g., ANIL/NIL [1]) shows that on ImageNet-style few-shot benchmarks, removing or nearly removing the inner loop can match MAML’s performance—likely because train and test tasks are sampled from the same ImageNet distribution, so distribution shift is limited. If inner-loop adaptation offers little measurable benefit in this regime, then demonstrating a “better” meta-gradient approximation there is a weak proxy for approximation quality: the task may not actually require accurate meta-gradients. Consequently, results on ImageNet/TieredImageNet alone are hard to interpret in terms of the value of improved meta-gradient estimation.\n\n2) Lacks discussion of [1].\n\n[1]https://arxiv.org/abs/1909.09157"}, "questions": {"value": "1) I would suggest comparing the current approach to ANIL or NIL on cross domain meta learning settings or other challenging meta learning settings where there is a significant distribution shift between training and testing task distributions [1], table 3 in [2]. \n\n\n[1] https://openaccess.thecvf.com/content/CVPR2023W/VISION/papers/Lee_XDNet_A_Few-Shot_Meta-Learning_Approach_for_Cross-Domain_Visual_Inspection_CVPRW_2023_paper.pdf\n\n[2] https://arxiv.org/pdf/1904.04232"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zqbPN5z6XW", "forum": "mKgUAO41zf", "replyto": "mKgUAO41zf", "signatures": ["ICLR.cc/2026/Conference/Submission9701/Reviewer_r31s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9701/Reviewer_r31s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291396114, "cdate": 1762291396114, "tmdate": 1762921208189, "mdate": 1762921208189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revision"}, "comment": {"value": "First, we sincerely thank the reviewers for their thoughtful questions and important points raised about the paper. The following changes have been made in the revision:\n\n- One of the main concerns seems to be the dense mathematical formulation, and in particular, the interpretability of Alg. 1. To this end, we rewrite Alg. 1 using a formulation previously found in Appendix A that more directly relates to the core operator $B_t^{g,i}$, and we include a new figure (fig. 1 in the revision) illustrating the $(L-l+1)$-th step operator $B_t^{g_t^{K},L-l}$.\n- Another concern raised was about a more complete comparison to iMAML. To address this, we briefly elaborate the theoretical time and space complexity of iMAML in lines 187-189 in the revision, and we expand Fig. 4 to include empirical complexity results of iMAML.\n- To more completely compare the compute cost of each algorithm, a new plot detailing the GPU compute core utilization of each algorithm is added to Fig. 4.\n- The caption on Table 1 has been updated to say ``standard deviation'' rather than ``95\\% confidence interval'' as was erroneously stated in the original draft.\n- A proof sketch of the core Proposition 3.1 has been added.\n- Appendix D has been added, including a short subsection detailing a hybrid Binom-Trunc estimate, and another subsection detailing the relation of BinomGBML to ANIL.\n- Minor wording fixes.\n- Lastly, figures have been enlarged for clarity.\n\nAgain, we are grateful for the insight of the reviewers and welcome conversation during the discussion period."}}, "id": "VBOHwUMczw", "forum": "mKgUAO41zf", "replyto": "mKgUAO41zf", "signatures": ["ICLR.cc/2026/Conference/Submission9701/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9701/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9701/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763710067331, "cdate": 1763710067331, "tmdate": 1763710342090, "mdate": 1763710342090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}