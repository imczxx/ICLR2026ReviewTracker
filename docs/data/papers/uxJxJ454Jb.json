{"id": "uxJxJ454Jb", "number": 24285, "cdate": 1758354951636, "mdate": 1759896772866, "content": {"title": "Weakly Supervised Forgery Localization and Detection Method for Face Manipulations", "abstract": "The striking proficiency of generative models in producing and manipulating images with an unprecedented level of realism has elicited concerns regarding to malicious applications like face manipulation techniques. However, the majority of existing face forgery detection models are developed to provide only the real or fake binary label for a given image, which is not sufficient to identify the location of the manipulated area. In this paper, we propose a weakly supervised method for face manipulation localization and detection based on the Vision Transformer architecture, which only makes use of image-level labels and can realize forgery localization without extra pixel-level annotations. Unlike other weakly-supervised localization methods which conduct prediction directly depending on the feature of the single image, we design a novel weakly-supervised localization method (MVG-FL) that leverages statistical distribution characteristics of the entire dataset. MVG-FL estimates multivariate Gaussian (MVG) distributions for real and fake samples, and further uses the learned distributions to predict the location of the manipulated area. Additionally, based on the predicted mask, we propose a Distribution Centrality Learning to improve the compactness of patch embeddings around the distribution centers to further promote forgery localization. Additionally, we develop a new large-scale face manipulated image dataset, named DiffFMD, which is composed of various state-of-the-art diffusion-based generators and multiple sizes of facial manipulation regions.The experimental results demonstrate that the proposed method can achieve high detection and localization performance for face manipulation images.", "tldr": "", "keywords": ["Deepfake Detection", "weakly-supervised deepfake localization", "Diffusion-generated image detection and localization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e52956e06d02cff5cbf44560a3a01b32af266d0a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a weakly supervised forgery localization framework that adaptively models feature distributions using multivariate Gaussian representations and introduces a distribution centrality learning scheme. In addition, the authors construct a new dataset, DiffFMD, to advance research in face forgery localization. Experimental results demonstrate that the proposed method achieves strong detection and localization performance on both the DiffFMD and FF++ datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes a MVG-based weakly supervised forgery localization framework that integrates MVG distribution estimation, adaptive sampling, and distribution centrality learning. The proposed adaptive learning and MVG-based distribution estimation schemes are well designed and conceptually interesting, and the experimental results are encouraging. \n\n- The authors also construct a new face forgery dataset, DiffFMD, containing 279k images generated by several diffusion-based models, which will be valuable for advancing future research on face forgery localization."}, "weaknesses": {"value": "- The motivation for the task of face forgery localization should be further clarified. What are the practical applications of face forgery localization? While I understand the rationale behind weak supervision in semantic segmentation due to the high cost and time of pixel-level annotation, the situation here is different. Since ground-truth masks of forged faces can be readily obtained during the fake generation process, the need for weak supervision should be explicitly justified to enhance clarity.\n\n- Although MVG estimation is widely used in weakly supervised learning, it remains unclear why this particular statistical model is suitable for the task of face forgery localization. A more intuitive explanation or theoretical rationale would strengthen the paper.\n\n- The details of L_PCL are suggested to be moved to the appendix for improved readability and flow of the main text.\n\n- The proposed method does not compare against the SOTA techniques. The most recent baseline reported is from 2023. While this may be the first work on weakly supervised face forgery localization, the authors should at least include comparisons with contemporary weakly supervised semantic segmentation methods to contextualize performance.\n\n- The authors use layer L for iterative distribution estimation and forgery localization, but the rationale behind this specific layer selection is not well explained. Clarifying this design choice would help readers understand the effectiveness of the approach.\n\n- The contribution of the proposed dataset is somewhat limited. The dataset only incorporates diffusion-based forgery techniques. The authors should clarify how they ensure that the pixel values of real regions remain unchanged when using diffusion models to generate manipulated faces.\n\n- The paper contains multiple typos and grammatical errors (e.g., “Sec. 4.1 Eq.equation 2, Eq.equation 3”). These should be carefully corrected throughout the manuscript.\n\n- The method employs pix-AUC, pix-F1, and img-AUC to evaluate model performance. It is also recommended to include pix-IoU, which would offer a more intuitive measure for assessing localization accuracy."}, "questions": {"value": "- The work lacks task-specific insight and design considerations for Deepfake detection and localization. The proposed framework appears to be an extension from natural image manipulation detection rather than one specifically tailored for facial forgery localization.\n\n- It may be beneficial to explore introducing a new margin term in Eq. (5) to potentially further enhance the model’s discriminative capability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O9Q8NHbSs3", "forum": "uxJxJ454Jb", "replyto": "uxJxJ454Jb", "signatures": ["ICLR.cc/2026/Conference/Submission24285/Reviewer_WyMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24285/Reviewer_WyMB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713052501, "cdate": 1761713052501, "tmdate": 1762943029299, "mdate": 1762943029299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of localizing manipulated regions in forged facial images using only image-level labels (i.e., \"real\" or \"fake\", without extra pixel-level annotations). The authors propose a novel weakly-supervised framework, i.e., Multivariate Gaussian-based Forgery Localization (MVG-FL) and Distribution Centrality Learning (DCL) based on a ViT model. The authors also propose a a new large-scale dataset DiffFMD which generated by various state-of-the-art diffusion-based inpainting models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.This paper leverages dataset-wide statistical distributions (MVG) for localization instead of single-image features. The adaptive sampling strategy improves upon fixed sampling with added flexibility and theoretical support  \n\n2.The proposed  DiffFMD dataset fills a gap by focusing on diffusion-based inpainting manipulations of facial images, offering multiple manipulation sizes and generators as a valuable benchmark for real-world generalization  \n\n3.Extensive experiments on diverse test sets and thorough ablation studies clearly demonstrate the effectiveness of adaptive sampling and DCL"}, "weaknesses": {"value": "1.While adaptive sampling and DCL improve the method, the core MVG framework largely builds on UIA-ViT, making the contributions feel like incremental refinements rather than a major new direction  \n\n2.The method’s iterative distribution estimation, buffer updating, and multiple losses add complexity, but the paper lacks discussion on training time, resource use, or feasibility compared to simpler baselines  \n\n3.The ablation study shows only modest gains from DCL, and more detailed explanation or visualization of how DCL improves feature compactness is needed to clarify its value"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aLzI7RZMxz", "forum": "uxJxJ454Jb", "replyto": "uxJxJ454Jb", "signatures": ["ICLR.cc/2026/Conference/Submission24285/Reviewer_miqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24285/Reviewer_miqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917859421, "cdate": 1761917859421, "tmdate": 1762943029030, "mdate": 1762943029030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a weakly supervised method for face forgery localization and detection based on a Vision Transformer (ViT) backbone. Unlike most prior works that rely on pixel-level annotations, the proposed MVG-FL approach leverages multivariate Gaussian (MVG) distribution estimation to model real and fake patch features, combined with an adaptive sampling strategy and Distribution Centrality Learning (DCL) to enhance embedding compactness. Furthermore, the authors construct a new dataset, DiffFMD, including manipulated images generated by multiple diffusion-based inpainting models with varying manipulation sizes. Extensive experiments demonstrate superior performance on both DiffFMD and FaceForensics++ datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a weakly supervised localization framework based on multivariate Gaussian distributions, which is both novel and inspiring. It achieves superior performance in both forgery detection and localization tasks. Moreover, the paper introduces a newly constructed dataset, which can serve as a new standard benchmark for research on diffusion-model-based forgery detection."}, "weaknesses": {"value": "1, In terms of the experimental setup, the results on the FF++ dataset appear to be obtained from a model trained on the full training set. However, it remains unclear whether a model trained on the DiffFMD dataset can directly transfer and generalize to the FF++ dataset. This aspect is crucial for evaluating the model’s practical applicability and robustness in real-world cross-domain forgery detection scenarios. It is recommended that the authors provide further clarification or additional experimental evidence on this point.\n2, The paper does not appear to specify the release plan or privacy compliance of the DiffFMD dataset, which may affect the credibility of the work. Is there any plan to make the DiffFMD dataset publicly available?"}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zSYrKv2v2C", "forum": "uxJxJ454Jb", "replyto": "uxJxJ454Jb", "signatures": ["ICLR.cc/2026/Conference/Submission24285/Reviewer_mmEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24285/Reviewer_mmEW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986859502, "cdate": 1761986859502, "tmdate": 1762943028733, "mdate": 1762943028733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}