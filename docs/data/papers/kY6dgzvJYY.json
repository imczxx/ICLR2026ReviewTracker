{"id": "kY6dgzvJYY", "number": 11867, "cdate": 1758204337541, "mdate": 1759897549973, "content": {"title": "LeanGeo: Formalizing Competitional Geometry problems in Lean", "abstract": "Geometry problems are a crucial testbed for AI reasoning capabilities. Most existing geometry solving systems cannot express problems within a unified framework, thus are difficult to integrate with other mathematical fields. Besides, since most geometric proofs rely on intuitive diagrams, verifying geometry problems is particularly challenging. To address these gaps, we introduce LeanGeo, a unified\nformal system for formalizing and solving competition-level geometry problems within the Lean 4 theorem prover. LeanGeo features a comprehensive library of high-level geometric theorems with Lean’s foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo, comprising problems from the International Mathematical Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the capabilities and limitations of state-of-the-art Large Language Models on this benchmark, highlighting the need for further advancements in automated geometric reasoning. We open source the theorem library and the benchmark of LeanGeo at https://anonymous.4open.science/r/LeanGeo-9CE9", "tldr": "We introduce LeanGeo, a novel framework for formalizing and solving competition-level geometry problems in Lean 4, along with a benchmark of 122 problems to evaluate the geometric reasoning capabilities of large language models.", "keywords": ["Automated Theorem Proving", "Formal Geometry", "Large Language Models", "Reinforcement Learning", "Lean 4"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b36a851bc69bf6876573aca544cc4eaccc8906a.pdf", "supplementary_material": "/attachment/12f1a2183268ea71040393685b7ea80c58965353.zip"}, "replies": [{"content": {"summary": {"value": "A unified framework in the Lean 4 theorem prover for formalizing competition-level geometry problems. It features a comprehensive library of 260 theorems and LeanGeo-Bench, a benchmark of 122 problems including 43 IMO geometry tasks, to evaluate large language models' reasoning capabilities, highlighting current limitations and the need for further advancements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "LeanGeo provides a novel framework that integrates competition-level geometry into Lean 4, complemented by LeanGeo-Bench, offering a valuable resource for AI reasoning research. The seamless integration with Mathlib enables LeanGeo to leverage algebraic and inequality tools, enhancing its applicability to complex, interdisciplinary mathematical problems."}, "weaknesses": {"value": "1. LeanGeo heavily relies on existing frameworks such as LeanEuclid (Murphy et al., 2024) and SystemE (Avigad et al., 2009), with its primary contributions being an expanded theorem library and 52 new abbreviations (syntactic sugar). This incremental approach lacks substantial novelty. Compared to AlphaGeometry (Trinh et al., 2024), which introduces innovative neurosymbolic search, LeanGeo’s “human-like” proofs show limited differentiation. Table 1 highlights qualitative differences but fails to provide quantitative metrics, such as proof length or success rates on shared problems, to substantiate its advantages.\n\n2. The statement ‘We present the first framework in the Lean theorem prover capable of expressing and reasoning about competition-level geometry problems in a human-like manner’ (Page 2, Lines 100-104) may require further clarification. Myers (2024) has demonstrated progress in formalizing planar geometry in Lean, including a solution to a 2019 IMO problem, as noted in community discussions (e.g., ‘Lean in 2024’ blog). While this suggests prior efforts in handling competition-level geometry, the scope and capabilities of Myers’ work compared to LeanGeo remain unclear.\n\n3. The RL experiments in Section 5 are vague and lack depth. No ablation studies (e.g., assessing the impact of theorem library size or prompt strategies), hyperparameter details, or error analyses (e.g., identifying failed problems and their reasons) are provided. Additionally, the absence of comparisons with state-of-the-art methods, such as DeepSeek-Prover or Seed-Prover, undermines the validity of the reported “promising initial results.\n\n4. LeanGeo-Bench, comprising 122 problems including 43 IMO geometry problems, appears limited in scale compared to existing benchmarks like MATP-BENCH (1,056 problems). This raises concerns about whether the dataset is sufficiently representative of the diversity and complexity of competition-level geometry problems."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "R1gRwpc2yd", "forum": "kY6dgzvJYY", "replyto": "kY6dgzvJYY", "signatures": ["ICLR.cc/2026/Conference/Submission11867/Reviewer_iGd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11867/Reviewer_iGd7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555718686, "cdate": 1761555718686, "tmdate": 1762922886539, "mdate": 1762922886539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LeanGeo, a Lean-4–native framework and benchmark for competition-level Euclidean geometry. It offers a high-level theorem/tactic library, SMT integration, and a curated benchmark intended to mix geometry with general math (e.g., trigonometry, inequalities) via Mathlib."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear structure and provides readable examples; it is well-written, although it has minor wording issues.\n- The system design is plausible and well-motivated.\n- It is a valuable step toward Lean-native, competition-level geometry with cross-domain reasoning."}, "weaknesses": {"value": "- The experiments are thin and largely non-diagnostic. There are no core ablations—no SMT off/partial settings, no scaling with library size or lemma granularity, no comparisons of tactic schedules, prompts, or decoding—so it’s unclear why models fail.\n- It’s also unclear how this benchmark compares to others. There’s no side-by-side test on the same problems against existing Lean or geometry benchmarks, so claims about being better or different are mostly qualitative.\n- Scalability and complexity are uncharacterized (no curves vs. points/constraints/branching), and the RL component is preliminary without stronger curricula or knowledge-tracing.\n\nOverall, the experiments are too shallow to support firm claims. Adding these basic comparisons and reports would make the paper much stronger."}, "questions": {"value": "- How does performance change with SMT off or with restricted solver capabilities?\n- What are scaling curves for success/time vs. theorem-library size and lemma granularity?\n- Can you provide aligned comparisons with existing Lean/geometry benchmarks on a shared subset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JFu6b0YluF", "forum": "kY6dgzvJYY", "replyto": "kY6dgzvJYY", "signatures": ["ICLR.cc/2026/Conference/Submission11867/Reviewer_JjNZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11867/Reviewer_JjNZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897792466, "cdate": 1761897792466, "tmdate": 1762922886083, "mdate": 1762922886083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LeoGeo, a formal system for formulating and solving competition-level geometry problem, and LeoGeo-Bench, a collection of benchmark geometry problems, both built using Lean 4. Benchmark results for state-of-the-art LLMs are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The LeanGeo library allows formalizing and solving competition-level geometry problems in Lean 4. It includes an extensive library of high-level definitions and tactics, which makes formal proofs more intuitive and understandable. LeanGeo's integration with Mathlib allows it to leverage powerful tools from other areas of maths.\n* The LeanGeo-Bench benchmark is useful for evaluating advances in the field.\n* The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "I find some of the claims insufficiently supported and some more details would be helpful, as explained below.\n* The main limitation of LeanEuclid compared to LeanGeo seems to be a limited set of formalized geometry facts, as stated at line 52. Is it difficult to expand LeanEuclid's library? If not, what additional advantage does LeanGeo has?\n* The paper claims that LeanGeo allows expressing and reasoning about geometry problems in a human-like manner. This may be debatable as the examples presented in the paper are still highly formal. It is also not clear what exactly is done to make the proofs more \"human-like\". Can you provide more details?\n* At some places the writing could be improved. The RL experiments seem to be a significant part of the paper, but this is not motivated in the introduction, and not mentioned in the abstract and the list of contributions.\n\nMinor\n* Line 234: Line 1 should pass the point B, but this is not mentioned. \n* Line 820: empty bullet point."}, "questions": {"value": "See questions in Weaknesses and the questions below.\n\n* Is mathlib always used for generating proofs using the theorem prover? If yes, is it possible to evaluate the theorem prover's performance without using it?\n* Only 43 problems in the benchmark have proofs. Is it because the automatic theorem prover fails to provide proofs for other problems? If yes, on which problems the theorem prover fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iWZDVG4EfA", "forum": "kY6dgzvJYY", "replyto": "kY6dgzvJYY", "signatures": ["ICLR.cc/2026/Conference/Submission11867/Reviewer_hrZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11867/Reviewer_hrZo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990193379, "cdate": 1761990193379, "tmdate": 1762922885563, "mdate": 1762922885563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}