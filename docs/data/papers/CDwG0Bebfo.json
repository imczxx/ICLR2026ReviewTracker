{"id": "CDwG0Bebfo", "number": 1297, "cdate": 1756869399632, "mdate": 1763474806681, "content": {"title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context", "abstract": "Physically-based rendering (PBR) provides a principled standard for realistic material–lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic–roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods. Project page: [Anonymous Link](https://lumitex-pbr.github.io/).", "tldr": "", "keywords": ["Texture Generation", "3D Generation", "Diffusion Models", "Physically Based Rendering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e765701bc8b37a53c9bae0db02ca476397a0e185.pdf", "supplementary_material": "/attachment/b382639d3004c6c7c1fec9ac7f558fb1af13e4f6.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LumiTex, a PBR texture generation model that achieves SoTA quality for PBR texture painting for 3D assets. LumiTex strengthens the connection between illumination shading and PBR material decomposition in the context of 3D generative models. The texture completion via LVSM has also proved to be very effective."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The integration of illumination context implicitly through attention layers is a wise design. Additional illumination prior can effectively reduce shading ambiguity in PBR tasks.\n* A joint model for multi-view, multi-channel PBR materials is a good attempt. The results demonstrate its effectiveness in reducing accumulation errors compared to prior multi-stage approaches.\n* LVSM for texture completion performs very well in ensuring global consistency and seamlessness of the final texture maps."}, "weaknesses": {"value": "I don’t find any major weakness point. Below are a few minor weaknesses points. \n* It is unknown how MR is represented and decoded. Is MR in the orm convention or modeled separately? Is MR latents decoded with the same VAE used for regular RGB images?\n* For the LVSM texture inpainting part, authors seem not to mention how decomposed PBR textures get inpainted. The example (Fig. 4) is on the rendered images."}, "questions": {"value": "The LumiTex DiT is a native multi-view model, implying it should be capable of performing texture inpainting for additional views. I am curious why the authors opted for LVSM instead of leveraging the LumiTex DiT for this task and would appreciate some insights into this decision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i3ZNrQ7VaJ", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Reviewer_EhkN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Reviewer_EhkN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779379600, "cdate": 1761779379600, "tmdate": 1762915729504, "mdate": 1762915729504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Defending the \"Accept\" rating"}, "comment": {"value": "Given that I think that this paper is a strong contribution, I would like to defend my \"Accept\" rating, specifically by highlighting the critical flaws in \"Reject\" reviews below.\n\n## Reviewer t6H4\n> and limited to generating textures at 768×768 resolution\n\nThe method is not limited to generating textures at this resolution - it generated rendered shaded and materials images at this resolution. After subsequent LVSM module they unproject it into textures of higher resolution (theoretically unlimited). This is not a limitation of this method.\n\n> Real-time applications (such as interactive design in AR/VR) would be severely constrained by the current training time of 106 GPU days. \n\n106 GPU-days for a foundational materials generation model is not a lot (13 days in single-node 8-GPU training time) by today's industry standards. Also\n> real-time applications (such as interactive design in AR/VR)\n\nit is clear that the model will not be trained in \"real-time\" and certainly not on the devices common AR/VR industry folks use. It's trained once but then inferred on demand on consumer GPUs. So I do not agree these applications should be \"severely constrained\" by this training time.\n\n> Transparent materials, such as glass, water, and liquids, are commonly required in real-world 3D rendering, especially in architectural visualization and product design\n\nI would argue that transparent materials support is a niche task, rather than being \"commonly required\". Albedo + MR is much more commonly used and covers most use cases. Authors correctly list transparency as a limitation, so I do not consider this a critical weakness.\n\n> the lack of alpha channel modeling for transparent materials is acknowledged, but the paper does not offer a roadmap for incorporating transparency\n\nI think it's an additional line of work and not a core of this paper.\n\n> The originality of LumiTex comes into question because the combination of multi-view consistency and lighting-guided material attention don't significantly advance the state of the art in a groundbreaking way.\n\nThe metrics (e.g. FID) in tables 1 and 2 shows strong evidence that it does advance the state-of-the art.\n\n## Reviewer yPWH\n> the pipeline flow is confusing (e.g., where does the input mesh go, where do the reference images come into play)\n> how does the multi-view shaded image generator work is somewhat omitted\n\nWith respect, I would argue that you might have not understood the paper. I would advise lowering the confidence threshold in this case.\n\n> the majority of visuals does not show the actual PBR results but the shaded versions\n\nPlease see Figure 10 and Figure 11.\n\n> I’m confused about the term multi-modal DIT\n\nI think authors mean this:\n> The MM-T is designed to integrate geometry information, reference appearance, and material semantics (albedo or MR) for each view.\n\nDINO features is not an RGB image, also camera view dirs are not an RGB image (although a tensor with dimensionality of 3). This is what authors likely mean by the term multi-modal.\n\n## Conclusion\nI think the paper contributions are clear and sufficient for an acceptance, and above mentioned reviews contain significant flaws. I further ask the reviewers to lower their confidence rating, given these issues were highlighted."}}, "id": "2Fwh7yqpec", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Reviewer_RwEh"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Reviewer_RwEh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762974606715, "cdate": 1762974606715, "tmdate": 1762974648027, "mdate": 1762974648027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose an end-to-end framework that, given a reference image and a mesh, generates textures with PBR materials.\n\nMain contributions:\n- Multi-branch design (one for generation of shaded images - to capture illumination context).\n- 2-stage training of multi-view illumination context branch and material branch\n- Lightning-aware material attention mechanism (directly attending to shaded tokens instead of using explicit intermediate images or optimization techniques)"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "To my knowledge, this is the first approach that is able to generate close-to-true PBR materials, without noticeable baked reflections or highlights. Given strong quantitative and qualitative evaluation, and the importance and complexity of the texturing task, I consider this work to be significant to the field.\n\nThe main contributions and strengths of the paper are clearly demonstrated and ablated in section 4.5, namely:\n- Separate branch for shaded images prediction\n- Single-stage generation (no use of explicit intermediate shaded images)\n- (!) Multi-branch design (instead of multi-channel generation of albedo and MR)\nThis was very insightful to learn, and well-supported by visuals in Fig 9b.\n\nIt was also demonstrated that the pipeline generalizes well to real-world scenes which is helpful in practical applications."}, "weaknesses": {"value": "- Impact and novelty of geometry-guided inpainting module is limited, although this is not claimed as a main contribution of the paper."}, "questions": {"value": "1. Please clarify the novelty of the inpainting module. Is the main contribution that you added geometry guidance to LVSM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pJN3slcqLl", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Reviewer_RwEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Reviewer_RwEh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798454438, "cdate": 1761798454438, "tmdate": 1762915729370, "mdate": 1762915729370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to generate PBR textures for given objects. The core idea are two stages: from an input mesh + reference image, the goal is to generate N view-consistent PBR images. Here, the core idea is essentially a multi-view image generator with several conditions (they authors refer to this as an illumination-consistent base model); this is then frozen and a material branch is trained for the PBR part. The second stage is a geometry-guided LVSM to synthesize more viewpoints. This is a texture in-painting strategy based on LVSM which generates the extended views, hence, more complete textures. Training is done from 92K objects from Objaverse and Objaverse-XL – for each object, 30 views are rendered (albedo, metallic, roughness, and HDR images). The base model is FLUX.1-dev.\n\nHowever, to be honest, the main technical exposition is quite confusing and it’s not easy to follow the exact details of the multi-view PBR generator (more details below)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors tackle an important problem.\n\n- The renderings of the shaded outputs look nice.\n\n- I appreciate the re-lighting results in the video."}, "weaknesses": {"value": "The presentation is confusing, and I'm having trouble understanding several of the technical details:\n\n- The introduction mostly pitches the features but a coherent description of the core idea; e.g., how does the multi-view shaded image generator work is somewhat omitted – this makes it hard to read (e.g., first need to read the whole main section and even some of the results to understand which base models they were using)\n\n- Fig 3 is a pipeline but the description of the multi-view illumination-consistent base model is missing, and the pipeline flow is confusing (e.g., where does the input mesh go, where do the reference images come into play).\n\n- I’m confused about the term multi-modal DIT. Seems all input here are images… what are the modes you are referring to here?\n\n- In the video, while the re-lighting looks great, I would’ve loved to see the actual PBR materials rather than the shaded versions. Also the shading in the video seems exhibit some temporal unstable artifacts which is confusing given that the underlying rendering should be just a mesh + PBR texture = this should be visualized.\n\n- The main results are in Fig 6 in the main paper (the majority of visuals does not show the actual PBR results but the shaded versions). Unfortunately, this looks not that impressive. E.g., I would’ve loved to see the PBR textures of the objects in Fig 1 or Fig 5 instead of the shaded outputs.\n\nThere is a general confusing claim to  PBR textures but then consider environment lighting which would still mean that scene specific context is baked in. This is contrary to the definition of a PBR texture.\n\nFigure 2 albedo looks poor. These results look shaded as there is lighting baked in – this is unfortunately not a PBR image / texture. Am I missing something here?"}, "questions": {"value": "See above in the weakness sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b11ecr8oTn", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Reviewer_yPWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Reviewer_yPWH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874752171, "cdate": 1761874752171, "tmdate": 1762915729187, "mdate": 1762915729187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LumiTex, a PBR texture generation framework that addresses challenges like material decomposition with limited illumination cues and seamless texture completion. LumiTex combines a multi-branch generation scheme, a lighting-aware material attention mechanism, and a geometry-guided inpainting model to enhance texture quality, realism, and consistency across views. Extensive evaluations are delivered."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- First of all, the work proposes the multi-branch generation design and the lighting-aware attention mechanism offers a novel way of disentangling albedo and metallic-roughness (MR) while integrating illumination context. \n\n\n- Quantitative results (FID, CMMD, LPIPS) and qualitative evaluations indicate that LumiTex achieves competitive or superior performance compared to existing methods, particularly in terms of texture quality and relighting fidelity.\n\n- The authors conducted a wide range of experiments, including comparisons to both open-source and commercial systems, as well as a user study on texture quality, demonstrating LumiTex’s practical advantages in real-world applications."}, "weaknesses": {"value": "- While the framework employs multi-branch generation and illumination context, similar ideas have already been explored in other recent works. For example, the idea of using lighting priors for material generation is not very novel. The originality of LumiTex comes into question because the combination of multi-view consistency and lighting-guided material attention don't significantly advance the state of the art in a groundbreaking way.\n\n- The method is computationally intensive and limited to generating textures at 768×768 resolution, which severely restricts its application in high-end industries that require 4K or 8K resolution for detailed textures (IMO this is more useful for texture generation for gaming applications or AAA filming). Although the authors propose potential avenues for scaling (e.g., multi-resolution models), scalability remains an unresolved bottleneck. Real-time applications (such as interactive design in AR/VR) would be severely constrained by the current training time of 106 GPU days. The paper does not adequately explore solutions to this scalability issue.\n\n\n- The lack of support for transparent materials. Transparent materials, such as glass, water, and liquids, are commonly required in real-world 3D rendering, especially in architectural visualization and product design. The authors don’t provide a solid argument for why these materials are left out or when this limitation might be addressed. There already exist many works that investigate transparent image / video generation.\n\n- LumiTex doesn’t show sufficient results in handling complex reflective materials or subsurface scattering. These properties are common in materials such as skin, water, and polished metals, which are common in visual effects and games. \n\n- The generalization to novel inputs seems to be limited. While the paper demonstrates robustness with real-world scanned meshes, the model still relies heavily on the type of training data (from Objaverse and Objaverse-XL). The authors did not provide convincing results showing how well LumiTex generalizes to extremely diverse or out-of-distribution inputs. This is a key issue for any system claiming real-world applicability in fields such as gaming or film, where content varies vastly.\n\n- The paper presents failure cases (such as small printed text or transparent materials), but it does not delve deeply into why these failures occur or how they might be addressed in future work. For instance, the lack of alpha channel modeling for transparent materials is acknowledged, but the paper does not offer a roadmap for incorporating transparency modeling or refraction effects, making it seem like a major limitation without any plans for resolution.\n\n- Despite claims of robustness under various illuminations, the framework’s real-world reliability under extreme lighting scenarios (e.g., highly reflective surfaces, strong backlighting) is not well-documented. The existing results are focused on typical lighting conditions, and there is a risk that the model might fail in edge cases involving extreme or uncontrolled lighting, common in cinematic and interactive applications. Without those, the contribution of this paper is doubtful."}, "questions": {"value": "I would be grateful if the authors could address the above mentioned issues in weakness section and other reviewers' concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jfwuSU5H8u", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Reviewer_t6H4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Reviewer_t6H4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994975809, "cdate": 1761994975809, "tmdate": 1762915728600, "mdate": 1762915728600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear ACs and Reviewers,\n\nThank you very much for your dedication, support, and insightful feedback.\n\nPBR texture generation is a complex task, and achieving state-of-the-art performance against both open-source methods (trained on private datasets) and leading commercial methods is quite challenging. We are grateful for the reviewers' recognition of our work's strengths:\n\n- **Novel pipeline design:** Reviewers noted that our multi-branch design \"offers a novel way\" for PBR texture generation and is \"very insightful to learn\" (t6H4, RwEh, EhkN), and our light-aware material attention that directly attends to shaded tokens \"is a wise design\" (RwEh, EhkN).\n- **Effective Demonstration:** Reviewer highlighted that our single-stage design effectively avoids error accumulation inherent to previous multi-stage pipelines, and that the integration of illumination context in one branch can \"effectively reduce shading ambiguity\" (RwEh, EhkN). They also recognized that \"LVSM for texture completion performs very well\" (EhkN).\n- **Strong Experimental Results:** Reviewers praised that our approach is \"the first approach that is able to generate close-to-true PBR materials\" (RwEh), and our \"wide range of experiments\" provides \"strong evidence\" compared to previous approaches (t6H4, RwEh, EhkN). They also commended the model's ability to generalize well to real-world scenarios (t6H4, RwEh).\n\nWe have reviewed all the comments, addressed all questions, and provided additional experimental results. All revisions are highlighted in **red** in the updated version, and we summarize the revisions we made:\n\n- **Additional Experimental Results (yPWH; Line 834):** Added PBR decomposition results of 38 assets for real and AI-generated meshes and references in Fig. 15 and Appendix A.1.\n- **Robustness under Extreme Conditions (t6H4; Line 837):** Added results under extreme conditions in Fig. 16, Appendix A.1.\n- **Texture Resolution (t6H4; Line 1052, 1060):** Clarified the texture and per-view image resolution.\n- **Figure Clarity (yPWH):** Added more detailed annotations of our pipeline in Fig. 3.\n\n**Request for Feedback**\n\nWe respectfully invite the reviewers to carefully evaluate our revisions and the individual responses provided. We are more than willing to address any remaining questions or concerns. If our responses and the additional results sufficiently address your feedback, we would appreciate your consideration of increasing your scores. We sincerely appreciate your thoughtful engagement and constructive suggestions, which have been instrumental in enhancing the quality of this work.\n\nBest regards,\n\nAuthors of Submission 1297"}}, "id": "wHEScDbcAs", "forum": "CDwG0Bebfo", "replyto": "CDwG0Bebfo", "signatures": ["ICLR.cc/2026/Conference/Submission1297/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1297/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission1297/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763472388140, "cdate": 1763472388140, "tmdate": 1763472428343, "mdate": 1763472428343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}