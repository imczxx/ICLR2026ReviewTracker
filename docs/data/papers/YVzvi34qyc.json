{"id": "YVzvi34qyc", "number": 3532, "cdate": 1757466142797, "mdate": 1759898082665, "content": {"title": "Making and Evaluating Calibrated Forecasts", "abstract": "Calibrated predictions can be reliably interpreted as probabilities. An important step towards achieving better calibration is to design an appropriate calibration measure to meaningfully assess the miscalibration level of a predictor. A recent line of work initiated by Haghtalab et al. (2024) studies the design of truthful calibration measures: a truthful measure is minimized when a predictor outputs the true probabilities, whereas a non-truthful measure incentivizes the predictor to lie so as to appear more calibrated. All previous calibration measures were non-truthful until Hartline et al. (2025) introduced the first perfectly truthful calibration measures for binary prediction tasks in the batch setting. \n\nWe introduce a perfectly truthful calibration measure for multi-class prediction tasks, generalizing the work of Hartline et al. (2025) beyond binary prediction. We study common methods of extending calibration measures from binary to multi-class prediction and identify ones that do or do not preserve truthfulness. In addition to truthfulness, we mathematically prove and empirically verify that our calibration measure exhibits superior robustness: it robustly preserves the ordering between dominant and dominated predictors, regardless of the choice of hyperparameters (bin sizes). This result addresses the non-robustness issue of binned ECE, which has been observed repeatedly in prior work.", "tldr": "Truthfulness of a calibration measure improves the robustness to hyperparameter choice (i.e., the choice of bin size).", "keywords": ["Calibration", "uncertainty quantification", "game theory", "truthfulness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ffecccc55db031a43bc4b5fa8d99061db74828e3.pdf", "supplementary_material": "/attachment/5efbf632d39ed8a7e6b3cb343fc720f4509dc35c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel, theoretically grounded truthful calibration measure for multi-class prediction by extending existing binary truthful calibration measures. Unlike common approaches that aggregate binary subproblem errors using confidence-based methods, which are not generally truthful, the proposed class-wise aggregation preserves truthfulness when moving from binary to multi-class settings. The resulting measure is theoretically justified for robustness, as it maintains decision-task dominance between calibrated predictors regardless of hyperparameter choices. Finally, the authors provide an empirical evaluation across a wide range of models of different sizes for the image classification task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured, with a clear motivation and several examples throughout the text that enhance its overall clarity.\n2. Each step of the paper is guided by a clear theoretical motivation, which strengthens the overall work.\n3. The experimental evaluation clearly demonstrates the issues with non-truthful classifiers and calibration measures, while also highlighting the superiority of the proposed measure."}, "weaknesses": {"value": "1. The paper lacks a conclusion section that summarizes the insights and contributions. Adding one would significantly improve readability.\n2. The paper refers to Theorem 1.1 on line 51, but this theorem does not exist.\n3. The notations $l_1$ and $l_2$ appear in Figure 1 without proper introduction or explanation.\n4. The experimental evaluation could be strengthened by including additional tasks and domains.\n5. Providing a practical algorithm for evaluating the proposed truthful calibration measures would further enhance the paper."}, "questions": {"value": "1. Does the same observation hold in other domains, such as text classification tasks? Moreover, when dealing with LLMs, predictions are typically made over vocabularies of more than 60k classes. It would be interesting to explore whether the proposed approach can be applied to LLMs and what results can be expected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kPJyRnlhoV", "forum": "YVzvi34qyc", "replyto": "YVzvi34qyc", "signatures": ["ICLR.cc/2026/Conference/Submission3532/Reviewer_1hMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3532/Reviewer_1hMP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932131804, "cdate": 1761932131804, "tmdate": 1762916790129, "mdate": 1762916790129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new truthful calibration measure for multi-class prediction tasks, extending recent work on truthful calibration from binary settings. Prior measures like Expected Calibration Error (ECE) are non-truthful, meaning they can incentivize predictors to misreport probabilities to appear more calibrated. Building on Hartline et al. (2025), who proposed a perfectly truthful binary calibration measure – l2-qECE(classwise), this paper generalizes it to the multi-class case using class-wise aggregation instead of confidence aggregation, proving that only the former preserves truthfulness. The authors further establish that this l2 classwise measure is robust, in contrast to other metrics considered.  \nTheoretically, the paper proves that truthful calibration errors preserve dominance among predictors. Empirically, experiments on CIFAR-100 with various neural networks confirm that this truthful measure preserves model rankings across different binning sizes, unlike non-truthful metrics whose rankings can flip."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extending truthful metrics to multi-class domain\n- Theoretical contribution: calculated the estimates for some trivial predictors and proved the dominance preservation theorem, as well as showed that classwise aggregation preserve truthfulness\n- The paper is well written"}, "weaknesses": {"value": "- Results are incremental. Novelty builds almost directly on Hartline et al. (2025) with a straightforward extension."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "S4Cqiq35Ak", "forum": "YVzvi34qyc", "replyto": "YVzvi34qyc", "signatures": ["ICLR.cc/2026/Conference/Submission3532/Reviewer_5AnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3532/Reviewer_5AnH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105938106, "cdate": 1762105938106, "tmdate": 1762916789475, "mdate": 1762916789475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a truthful calibration measure for multi-class prediction. This generalizes the measure proposed in Hartline et al for binary prediction. The method is empirically validated and shown to preserve relative performance of methods better than prior non-truthful approaches."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Calibration is an important subfield of machine learning, and having better metrics to measure calibration can improve our ability to study it empirically. The calibration metric proposed in this paper does seem to be better than prior metrics, at least in terms of the empirical evidence in the last section. It is also relatively straightforward to compute."}, "weaknesses": {"value": "This paper overall seems like a very straightforward generalization of \"A Perfectly Truthful Calibration Measure\" by Hartline et al. (2025). I'm not sure that there's anything in here that is novel that would not occur to an expert on calibration reading that paper. I don't even think an expert would need to read that paper to prove Theorem 3.1; it just follows (as the paper says) from linearity of expectation.\n\nIt's also not clear to me how much interest this sort of truthful calibration is to the community. I don't think any paper on this subject has been published at ICLR (granted, it is a very new formalization)."}, "questions": {"value": "What do the authors see as the novel part of their contribution in this paper, relative to Hartline et al. (2025)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z97JBWzXSS", "forum": "YVzvi34qyc", "replyto": "YVzvi34qyc", "signatures": ["ICLR.cc/2026/Conference/Submission3532/Reviewer_NXvi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3532/Reviewer_NXvi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133531219, "cdate": 1762133531219, "tmdate": 1762916788849, "mdate": 1762916788849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification of our main contribution"}, "comment": {"value": "We thank the reviewers for their thoughtful feedback. \n\nWe believe there may have been a misunderstanding about the main contribution of our paper, perhaps due to the way we present the paper. The paper is not mainly about introducing a truthful calibration error for multi-class prediction. \n\nOur **main result** is that truthful calibration errors are important for **empirical work** aiming to understand calibration. In particular, we show that non-truthfulness helps explain well-known issues where earlier calibration errors give different model rankings when hyperparameters (bin sizes) are changed. This empirical issue can thus be fixed by the truthful calibration error of Hartline et al (see Note * below). Before our work, it was not clear that this ranking inconsistency was caused by non-truthfulness. Prior work proposes non-constructive explanations, such as estimation bias or convergence rate to the limit [1, 2]. Our result demonstrates how important truthfulness is when evaluating the calibration of models. Since our main empirical result did not feature strongly in the reviews, we understand that we need to improve the paper's narrative so that this main message is clearer.\n\nNote * : we had to generalize Hartline et al to multi-class setting, which is a **smaller contribution** in our paper but necessary. While the classwise extension may be a direct step from earlier work, the fact that standard practice (confidence aggregation) is not truthful is not obvious from prior work. \n\n\n[1] Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D. (2019, June). Measuring calibration in deep learning. In CVPR workshops (Vol. 2, No. 7).\n\n[2] Roelofs, R., Cain, N., Shlens, J., & Mozer, M. C. (2022, May). Mitigating bias in calibration error estimation. In International Conference on Artificial Intelligence and Statistics (pp. 4036-4054). PMLR."}}, "id": "NQXTYvvLhf", "forum": "YVzvi34qyc", "replyto": "YVzvi34qyc", "signatures": ["ICLR.cc/2026/Conference/Submission3532/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3532/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3532/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763424998068, "cdate": 1763424998068, "tmdate": 1763425011790, "mdate": 1763425011790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies truthful calibration metrics as introduced by Haghtalab et al. (2024). A truthful calibration metric is one for which the ground truth $p^*$ always receives the best error out of all predictors, on IID samples from any distribution. The recent work of Hartline et al. (2025) gives a few truthful calibration measures for the setting of binary prediction. \n\nThis paper points out that via linearity of expectation, any truthful calibration measure for binary prediction can be extended to one for multiclass prediction. In particular, define a new calibration metric “classwise” as the average calibration error of each of the $k$ binary prediction tasks. Then, if a calibration measure is truthful in the binary case, it will be truthful for this new aggregated multiclass calibration measure. The paper points out that this is not the case for confidence calibration (i.e., the “top predicted probability class” calibration error).\n\nThe paper also defines a notion of a predictor “dominating” another predictor in terms of proper losses. In particular, $f$ dominates $g$ if $f$ has lower average loss than $g$ over all proper losses.\n\nFirst, in section 3, the paper uses the fact that the multiclass “classwise” calibration error is truthful for any truthful binary calibration error in order to propose a new truthful multiclass calibration error: $\\ell_2-QECE^{\\text{classwise}}$. This is simply the $\\ell_2$ calibration error over equal mass bins chosen via sorting the predictions by predicted probability. \n\nIn section 4, the paper demonstrates that if $f$ dominates $g$, then $f$ will have lower  $\\ell_2-QECE^{\\text{classwise}}$ calibration error than $g$. This follows simply from the fact that Brier score is a proper loss (and hence, $f$ has lower Brier score than $g$ by definition of domination), and that the proposed calibration error is essentially the Brier score. Section 4 also includes a simple counter-example which demonstrates that the proposed calibration measure has nice behavior even when other discussed measures fail.\n\nLastly, Section 5 has simple experiments on CIFAR-100 demonstrating that the truthfulness properties and robustness to bin size hold in practice. In particular, other calibration measures will flip the ranking of models with the same loss at different bin sizes, whereas the proposed truthful multiclass measure does not. Furthermore, the same holds even when comparing different models or model training checkpoints."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very clearly written. Although I am familiar with calibration, I had not yet got around to reading the truthful calibration line of work. I was easily able to understand the motivation and setup behind truthful calibration measures.\n\nThe main results are also very simple to understand. In fact, most follow as a corollary of the work Hartline et al. (2025). The experimental evidence / plots given are also convincing."}, "weaknesses": {"value": "I am a bit unsure if the paper meets the content bar for ICLR. I would appreciate the opinion of other reviewers as well. In particular, from a theory perspective the results are extremely simple (almost all are 1-line proofs). In fact, the main contribution here could exist as essentially a page or two in Hartline et al. (2025), which, I will note, is a very recent paper (arxived August 18th 2025). Furthermore, the experiments, while covering many different models, are only evaluated on CIFAR-100.\n\nI believe that the paper either needs 1) more theoretical results, perhaps relating dominance and truthful calibration (see below discussion for some potential ideas / discussions); or 2) more comprehensive empirical results, over multiple model types (classical, deep learning, language classification, LLM, etc.) or multiple datasets. I believe both would be welcome additions and improve the contribution content of the paper. As it stands, I don’t see this paper as more than a short note that the previous result from Hartline can extend in a simple manner. 2) may be especially useful to gain broader buy-in from the practical / empirical community, who may stand to benefit from a new simple calibration metric with clear benefits.\n\nNonetheless, I am open to discussion on this fact. I don’t believe that something being simple should be the reason to _not_ accept the paper. I just am unable to frame the scope of the contribution in the canon of truthful calibration literature.\n\nI think the dominance vs. truthful calibration discussion can probably be fleshed out far more than it currently is. In particular, there are many works which point to correlation between calibration measures and accuracy in both classical ML (Chidambaram et al., 2024, Tao et al., 2024), multicalibration (Hansen et al., 2024) and more modern settings like LLMs (Mei et al., 2025). How does this discussion relate to the experiments conducted, and dominance vs. truthfulness more broadly? In particular, if we use calibration measures that correlate perfectly with accuracy / loss, are we even measuring anything interesting? Should we just be looking at the (proper) loss instead, and call it a day?\n\nThis all being said, I am still borderline because a simple idea does not imply that the paper should be rejected.\n\nMei et al., 2025. Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?\nHansen et al., 2024. When is multicalibration post-processing necessary?\nChidambaram et al., 2024. Reassessing how to compare and improve the calibration of machine learning models.\nTao et al., 2024. A benchmark study on calibration.\n\nSuggestions\n1. I would at least mention the completeness and soundness requirements for calibration metrics before defining truthfulness. I was confused until I read the appendix A.1 which pointed out that proper losses do not satisfy these criteria. It would also be helpful, in A.1, to point out other calibration metric papers which have similar criteria / desiderata (for example, Błasiok et al. 2023, Hartline et al.).\n2. I also have a bit of a bone to pick with the title. The title is extremely general, but the main result is really quite narrow. I believe a more suitable title would be something like “Multiclass Truthful Calibration Measures”, or something more along this line.\n\nBłasiok et al. 2023. A Unifying Theory of Distance from Calibration.\n\nTypos:\n1. 691: “Deferref\" -> Deferred\n2. 308: “dominance-reserving” -> “dominance-preserving”"}, "questions": {"value": "To my understanding, the multiclass classwise aggregation scheme must recompute bins for each class in order to retain truthfulness guarantees. Is this the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vfJWS6hAqr", "forum": "YVzvi34qyc", "replyto": "YVzvi34qyc", "signatures": ["ICLR.cc/2026/Conference/Submission3532/Reviewer_kYmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3532/Reviewer_kYmS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145303339, "cdate": 1762145303339, "tmdate": 1762916788562, "mdate": 1762916788562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}