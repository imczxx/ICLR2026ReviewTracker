{"id": "UJ2UUjT2ko", "number": 21902, "cdate": 1758323365944, "mdate": 1763705948136, "content": {"title": "Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context", "abstract": "A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval.\nFor example, an LM might represent *Ann loves pie* by binding *Ann* to *pie*, allowing it to later retrieve *Ann* when asked *Who loves pie?* \nPrior research on short lists of bound entities found strong evidence that LMs implement such retrieval \nvia a **positional mechanism**, where *Ann* is retrieved based on its position in context.\nIn this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions.\nTo compensate for this, we find that LMs supplement the positional mechanism with a **lexical mechanism** (retrieving *Ann* using its bound counterpart *pie*) and a **reflexive mechanism** (retrieving *Ann* through a direct pointer). \nThrough extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior.\nWe leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95\\% agreement.\nFinally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings.\nOverall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.", "tldr": "Entity binding in LMs is crucial for reasoning. Prior work established a positional mechanism underlying binding, yet it breaks down in complex settings. We find two additional mechanisms, lexical and reflexive, that drive model behavior.", "keywords": ["Interpretability", "Entity Binding", "Causal Abstraction", "Mechanistic Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5cb44cdc2cbc63c8edba57181ed42ce2ee97534.pdf", "supplementary_material": "/attachment/06d340bdb22d4e8dee5357d4486d481ff4e2159f.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates how LLMs bind and retrieve entities in context. Prior work has suggested that LLMs rely mainly on a positional mechanism, retrieving entities by their relative position in a list of contextually bound entities. However, the authors show that this mechanism becomes unreliable when the context grows and entities appear in middle positions, reminiscent of the lost-in-the-middle effect. To explain this, the paper proposes lexical and reflexive mechanisms that complement the positional one. Through systematic interchange interventions, they find that LLMs dynamically mix these three mechanisms depending on position and entity type. They further build a causal model that combines all three mechanisms and achieves up to 95% Jensen–Shannon similarity with true model predictions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The authors propose the lexical and reflexive mechanisms as natural extensions of how binding might operate when positional cues become unreliable. They support these hypotheses with well-designed counterfactual interventions that yield clear empirical evidence. The experimental setup, including controlled manipulations of entity positions and roles, provides high causal interpretability rather than correlational evidence.\n* The findings are replicated across nine model families and ten distinct tasks,  demonstrating robustness. The mixture model achieves 95% agreement with LLM token predictions, further demonstrating the faithfulness of the proposed framework.\n* The parallels drawn to primacy and recency biases in human memory, and the connection to the lost-in-the-middle effect, make the results conceptually relatable."}, "weaknesses": {"value": "* The three mechanisms are inferred largely through counterfactual patching, but the causal independence between them is assumed rather than rigorously established. For example, lexical and reflexive mechanisms often co-occur. It remains unclear if they are distinct causal variables or correlated manifestations of shared attention dynamics.\n* Although filler text is introduced later, most analyses still rely on templated X likes Y style prompts. These may not capture linguistic variability or discourse-level entity binding."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZCXuTmteQ3", "forum": "UJ2UUjT2ko", "replyto": "UJ2UUjT2ko", "signatures": ["ICLR.cc/2026/Conference/Submission21902/Reviewer_4tWV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21902/Reviewer_4tWV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273391661, "cdate": 1761273391661, "tmdate": 1762941973957, "mdate": 1762941973957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors explore multiple mechanisms by which transformers LMs track entities in context to retrieve factual information. It's already known that models will track the positions of entities and use this mechanism to answer factual questions. The positional mechanism, however, breaks down when there are many entities in context. The authors find that transformers have two other mechanisms: a lexical mechanism where the model looks up the queried entity and a reflexive mechanism where a model looks up information about a promoted token. These mechanisms can all disagree about the answer to a query, which the authors show with interventional experiments. They also create an effective model of this three-mechanism behavior that closely matches the output distributions of the LMs they test on."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- It seems intuitive that something like the lexical mechanism or the reflexive mechanism would exist. It's somewhat surprising that these are separate.\n- The interventions show that the different retrieval mechanisms can disagree in their predictions and behavior on the output distribution, which is compelling.\n- The authors create an effective model of the three retrieval mechanisms which closely matches the output distribution of the actual model.\n- Polluting the context with additional free-form text is a reasonable robustness check"}, "weaknesses": {"value": "- I'm looking for more details about how exactly the interchange interventions were performed and how the authors localized where to do interchange interventions. From the appendix, it seems like this is based on performing interchange interventions on the attention at specific layers on the last token position? And the authors use attention knockout to figure out which layers are passing entity information to the final token position? Can you clarify?\n- The main body of the paper would benefit from a bit more explanation of what exactly is going on here (I'm aware of space constraints, but I think this is important). And the appendix would benefit from a clear, high-level description of what the experiments are actually doing.\n\nMinor suggestions and feedback:\n- Putting the counterfactual input on top in Figure 1 feels confusing, seems more natural to have the original on top and the counterfactual below?\n- I found the text description of the reflexive mechanism (line 231) to be fairly hard to parse. This seems like something that is much easier to show than to say, so a figure could be helpful.\n- Line 159: use \\citet instead of \\citep"}, "questions": {"value": "- It seems like the model uses the reflexive pathway as a \"verification\" step. Does the reflexive pathway need to match the entity from the lexical pathway (as it does in Figure 1) to work? Is my understanding of this correct?\n- See \"weaknesses\" above re: questions about how the interchange interventions were performed"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Q49BJvB42", "forum": "UJ2UUjT2ko", "replyto": "UJ2UUjT2ko", "signatures": ["ICLR.cc/2026/Conference/Submission21902/Reviewer_U696"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21902/Reviewer_U696"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842660190, "cdate": 1761842660190, "tmdate": 1762941973682, "mdate": 1762941973682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how language models (LMs) retrieve bound entities in context, e.g., “Pete loves jam, Ann loves pie. Who loves pie?”. The authors identify that retrieval arises from a mixture of three mechanisms: positional, lexical, and reflexive.\n- Positional: retrieves the target based on the position of the entity group corresponding to the query.\n- Lexical: retrieves the entity bound to the queried token itself (e.g., “pie” → “Ann”).\n- Reflexive: uses a direct pointer to the target entity token.\nThey construct a controlled dataset with paired _original_ and _counterfactual_ examples (with entities shuffled), to identify each mechanism via patching experiments. This setup allows them to separate the contribution of each mechanism to the LM’s predictions. They further show that mixing these three mechanisms explains model behavior in longer contexts where the positional mechanism alone fails.  \nFinally, they build a simple causal model incorporating these mechanisms, which closely reproduces the LM’s next-token distribution (≈0.95 Jensen–Shannon similarity)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- **Clarity:** The paper is clearly written and well-organized. The motivation, hypotheses, and experimental setup are clearly presented with concrete examples.\n- **Originality:** The discovery of two previously undescribed retrieval mechanisms (lexical and reflexive) extends the mechanistic interpretability literature beyond the known positional mechanism.\n- **Quality:** The experimental methodology, which uses counterfactual patching and causal modeling, is rigorous and carefully justified."}, "weaknesses": {"value": "- **Figures:** Some key figures (e.g., Fig. 2) are difficult to interpret. The axes, metrics, and what constitutes “mixed” effects are not clearly explained in the captions or main text, making it hard for readers to connect the visualization to the described mechanisms.\n- **Presentation of reflexive mechanism:** The explanation and evidence for the reflexive mechanism remain somewhat unclear. Because the patched context may already contain the predicted token, it is hard to disentangle whether the observed effect truly demonstrates a “pointer” or simply reflects ongoing retrieval from other mechanisms."}, "questions": {"value": "Dataset\n- Could you specify the range of sizes for the entity set $\\mathcal{E}$ in the dataset ? \n\nFigures\n- In Figure 2, what exactly does the _y_-axis represent? Is it the proportion of examples where that mechanism predicts the correct answer? The term “index” on the axis is confusing.\n- What does “mixed” mean in this context?\n- Are cases excluded where the patching does not predict a valid entity (i.e., none of the entities in the list)? Or does this never happen?\n\nReflexive mechanism\n- In the main text, you mention patching into a context where the target entity is not present, to test the reflexive mechanism. However, it still seems possible that the patched representation already contains the target token’s activation, and that in the new context other mechanisms simply suppress it because the answer is implausible. Could you clarify why this result indicates the presence of a direct pointer, rather than the target token being carried over as a side effect of other retrieval processes already in progress?\n\nCausal Model\n- When training the causal model variants that exclude one mechanism, are the weights of the remaining mechanisms retrained independently, or are they frozen from the full model? Clarifying this would help interpret the ablation results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cIHoYTwrDF", "forum": "UJ2UUjT2ko", "replyto": "UJ2UUjT2ko", "signatures": ["ICLR.cc/2026/Conference/Submission21902/Reviewer_zu1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21902/Reviewer_zu1T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21902/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906698355, "cdate": 1761906698355, "tmdate": 1762941973433, "mdate": 1762941973433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}