{"id": "rD6nZS86c2", "number": 17237, "cdate": 1758273757635, "mdate": 1759897189106, "content": {"title": "Boundary-Aware Tokenization for Event-Driven Time-Series Forecasting", "abstract": "Transformer-based large sequence models have recently been extended from language to time-series to capture long-range dependencies and heterogeneous dynamics. However, unlike language, time-series lack a natural dictionary for principled tokenization: existing large sequence models often resort to fixed-length tokens or patches for computational efficiency. This design can obscure regime changes, expend attention on low-information tokens, and restrict the effective context length. We address this limitation with Boundary-aware tokenization, which initiates new tokens only at predicted regime changes in the time-series, analogous to how spaces delimit words in language. At its core, the model integrates an unsupervised boundary detector to form variable-length chunks, an intra-chunk fusion module to derive chunk-level token embeddings, and a smoothing module to stabilize training, before passing the resulting tokens to Transformer-based modules. We further add a gating refinement that fuses fixed- and variable-length representations before the forecasting decoder, enabling adaptive selection during pre-training based on data patterns. This design directly addresses event-driven regime changes, while remaining robust in stationary regimes. Across diverse benchmarks, our method reduces forecasting error by 10.5\\% on average, with learned chunks aligned with true regime boundaries. We also show that the model adaptively reverts to fixed-length tokenization in stationary time-series.", "tldr": "Dynamic Chunking Network for Time Series Forecasting", "keywords": ["Time Series", "Dynamic Chunking", "LLM"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3db505cdce47bbaea072710f86502cc2bd30fa30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Boundary-Aware Tokenization, a Transformer-based framework that dynamically segments time-series into variable-length chunks aligned with event boundaries. Unlike fixed-length tokenization used in PatchTST, BT-LSM detects regime changes using an unsupervised boundary detector based on first- and second-order embedding dynamics. Within each chunk, a mixture-of-experts fusion module combines multiple pooling statistics to produce chunk-level tokens, followed by a chunk-level Transformer and a causal smoothing module for stability. A gating refinement further combines fixed- and variable-length tokenization to ensure robustness on both stationary and event-driven data. Experiments across datasets show ~10.5% average forecasting error reduction compared to strong baselines. Ablation and visualization support the claim that learned boundaries align with real regime changes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Dynamic tokenization is a crucial problem for time-series models. The paper insightfully draws an analogy between language tokenization and event segmentation in time-series, and proposes a boundary-based dynamic tokenization approach that effectively addresses the inefficiencies of fixed patching.\n\n2. The proposed pipeline (boundary detection → MoE fusion → chunk Transformer → smoothing → gating → decoder) is well-motivated and internally coherent, with each component designed to tackle a specific modeling challenge.\n\n3. Theorem 1 formally establishes invariance to intra-chunk resampling—an elegant theoretical property that enhances the model’s robustness to irregular sampling."}, "weaknesses": {"value": "1. If the original multivariate time series exhibit strong temporal lags or misalignments across variables, the proposed method would still produce a single set of shared boundaries for all variables. This may limit its ability to capture variable-specific regime changes.\n\n2. In the test datasets, what proportion of cases have g₍var₎ > g₍fix₎? This statistic would help clarify whether the performance improvement mainly stems from the proposed dynamic tokenization mechanism.\n\n3. The paper lacks visualization of the gating weights among the different embedding experts (attention pooling, mean, max, etc.). Such visualization could provide insights into which features contribute most to the model’s representation.\n\n4. The experiments involve a relatively small set of datasets and baselines, which somewhat weakens the empirical evidence supporting the paper’s claims.\n\n5. Section 1 title: “Introductionn” → should be corrected to “Introduction.”"}, "questions": {"value": "1. Does the decoder-only Transformer incorporate positional embeddings to preserve temporal ordering? If so, please clarify what type of positional encoding is adopted.\n\n2. How are the variable-length and fixed-length representations (z₍var₎ and z₍fix₎) aligned before being fused by the gating module? It appears that they are aligned sequentially, which may imply that the two tokens being fused could correspond to quite different original temporal positions.\n\n3. How is the target boundary rate chosen in practice? Is it tuned as a hyperparameter or derived from data statistics? It would be helpful to clarify whether the model’s performance is sensitive to this setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vtN7z9EYg3", "forum": "rD6nZS86c2", "replyto": "rD6nZS86c2", "signatures": ["ICLR.cc/2026/Conference/Submission17237/Reviewer_2dpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17237/Reviewer_2dpd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548154783, "cdate": 1761548154783, "tmdate": 1762927194646, "mdate": 1762927194646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Boundary-aware Tokenization Large Signal Model (BT-LSM) for time-series, introducing a lightweight unsupervised boundary detector and mixture-of-experts chunk embeddings. The proposed model allocates tokens adaptively to event-driven transitions, avoiding uni\u0002form waste. Experimental results demonstrate that BT-LSM achieves over 10.5% lower forecasting error at matched compute budgets across diverse benchmarks, including energy, power, and traffic data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated and addresses a significant limitation in the literature.\n- The proposed method is evaluated on multiple benchmark datasets, demonstrating superior performance compared to baselines."}, "weaknesses": {"value": "- The paper omits evaluations on widely used benchmarks in the time series forecasting community, such as the ECL and Weather datasets. Similarly, the baseline does not include several key state-of-the-art models, such as DLinear and TimesNet, which are commonly used for benchmarking and would provide a more robust comparative analysis. \n- The description of the experimental setup is vague and lacks sufficient details. For instance, the length of the input sequence appears to be 144 based on visualization, but there is no explicit mention of this in the paper. This lack of clarity makes it difficult for readers to fully understand the conditions under which the experiments were conducted.\n- The experimental evaluation is primarily focused on the ETTh1 and ETTh2 datasets for multi-horizon forecasting. This narrow scope raises concerns about the generalizability of the proposed method to other datasets and forecasting tasks.\n- The paper lacks the ablation analysis on the input length. This is a critical omission, as input length is a significant parameter in time series forecasting models.\n- The paper does not analyze whether the proposed model can maintain its effectiveness under different conditions, such as shorter input sequences or varying levels of data sparsity. These scenarios are common in practical forecasting tasks, and further analysis is needed to ensure the method's applicability in such settings."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RrqUQR9r3l", "forum": "rD6nZS86c2", "replyto": "rD6nZS86c2", "signatures": ["ICLR.cc/2026/Conference/Submission17237/Reviewer_GaVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17237/Reviewer_GaVY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817315312, "cdate": 1761817315312, "tmdate": 1762927194162, "mdate": 1762927194162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a boundary-aware tokenization scheme for time series forecasting, which starts new tokens at event changes. The model contains an unsupervised boundary detection module, an intra-chunk fusion module, a smoothing module and a gating refinement module that dynamically selects fixed-length and boundary-aware tokenization. Experiments across multiple datasets indicate consistent improvements from the proposed tokenization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed tokenization scheme has clear motivation and the design is new. The unsupervised boundary detector based on velocity and acceleration does not involve additional training overhead.\n\n2. Experiments on multiple datasets show the effectiveness of the proposed tokenization scheme. The paper also presents many case studies to show the benefits."}, "weaknesses": {"value": "1. The paper does not compare with some recent time series tokenization schemes [1,2,3]. For example, [1] also moves beyond fixed encodings via pattern-based tokenization.\n\n2. The proposed tokenization scheme is not lossless. One cannot deterministically reconstruct the original series from tokens, which may limit certain applications.\n\n3. How robust is the hard boundary of 0.5?\n\n4. It would help to show a controlled comparison where only the tokenization differs (fixed vs proposed boundary-aware vs other existing tokenization schemes) under different forecasting backbones, to demonstrate model-agnostic gains and isolate the contribution of tokenization.\n\n[1] Byte Pair Encoding for Efficient Time Series Forecasting\n\n[2] Enhancing foundation models for time series forecasting via Wavelet-based tokenization\n\n[3] TOTEM: Tokenized Time Series Embeddings for General Time Series Analysis"}, "questions": {"value": "1. Are boundaries/tokens shared across variables or detected per channel in multivariate time series?\n\n2. Minor typo: accross -> across at Line 399"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zUHqPLVFGJ", "forum": "rD6nZS86c2", "replyto": "rD6nZS86c2", "signatures": ["ICLR.cc/2026/Conference/Submission17237/Reviewer_TTSi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17237/Reviewer_TTSi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889748185, "cdate": 1761889748185, "tmdate": 1762927193825, "mdate": 1762927193825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **BT-LSM (Boundary-aware Tokenization Large Signal Model)** for event-driven time-series forecasting, addressing the limitation of fixed-length tokenization by adaptively forming variable-length tokens aligned with regime changes. It integrates an unsupervised boundary detector, mixture-of-experts (MoE) chunk embedding, chunk smoothing, and a gating refinement that fuses fixed- and variable-length representations. BT-LSM concentrates model capacity on event transitions while remaining robust in stationary regimes, reducing forecasting error by 10.5% on average across diverse benchmarks and aligning learned chunks with true event boundaries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical flaw of fixed-length tokenization in time-series forecasting—its inability to align with event-driven regime changes—by introducing an unsupervised boundary detector. This detector leverages embedding dynamics (velocity, acceleration, energy change) to identify natural chunk boundaries, ensuring tokens concentrate on critical transitions (e.g., spikes, inflections) rather than wasting capacity on redundant stationary spans. Unlike supervised segmentation methods (e.g., SIMTSeg, U-Time) that require labels, this unsupervised design is broadly applicable across time-series domains.\n2. The proposed gating refinement module fuses variable-length (event-aligned) and fixed-length (stationary-optimized) representations, enabling BT-LSM to adapt dynamically. In stationary time-series (e.g., smooth cycles), the gate prioritizes fixed-length tokens to preserve short-range statistics; in bursty/irregular data, it shifts to variable-length tokens to capture events. This design eliminates the trade-off between event sensitivity and stationary robustness, a limitation of purely fixed or variable tokenization methods.\n3. The paper proves a resampling invariance theorem (Theorem 1), ensuring BT-LSM’s chunk embeddings and forecasts remain unchanged under intra-chunk resampling (e.g., varying sensor sampling rates)."}, "weaknesses": {"value": "1. The related work focuses on temporal-domain tokenization methods but omits direct comparisons to frequency-domain forecasting models (e.g., FEDformer, TimesNet) that excel at capturing periodic patterns. This leaves uncertainty about BT-LSM’s performance relative to frequency-aware approaches, especially for time series with strong periodicity but weak event signals.\n2. BT-LSM uses padding-and-masking to handle variable-length chunks in batch processing, which becomes inefficient for extremely long sequences (e.g., multi-year high-frequency data). The paper does not explore alternative batching strategies (e.g., chunk-level bucketing) to mitigate this, restricting its application to moderate-length time series.\n3. While BT-LSM performs well on datasets with clear event patterns (e.g., solar spikes, traffic peaks), it lacks evaluation on low signal-to-noise ratio (SNR) or highly irregular time-series (e.g., sparse medical sensors, non-periodic industrial anomalies). No experiments demonstrate its robustness to such edge cases, limiting generalizability to real-world \"messy\" data."}, "questions": {"value": "1. The paper mentions that the boundary detector relies on parameters like boundary probability threshold and minimum chunk length, but lacks a systematic optimization method. What specific hyperparameters of the boundary detector have the most significant impact on BT-LSM’s forecasting performance? Is there a potential adaptive adjustment strategy (e.g., learning hyperparameters via data-driven methods) that can reduce manual tuning efforts across different datasets?\n2. Since BT-LSM has not been tested on low SNR or highly irregular time-series (such as sparse medical sensors), what modifications to the boundary detector or chunk embedding module might help the model better filter noise and capture valid event boundaries in such challenging data scenarios?\n3. The padding-and-masking strategy used by BT-LSM becomes inefficient for extremely long time series. Are there alternative batch processing strategies (e.g., chunk-level bucketing, hierarchical chunking) that the authors have considered to improve the model’s scalability, and what preliminary results or feasibility analyses exist for these strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JV8FHx3Ag4", "forum": "rD6nZS86c2", "replyto": "rD6nZS86c2", "signatures": ["ICLR.cc/2026/Conference/Submission17237/Reviewer_v2PH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17237/Reviewer_v2PH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998097728, "cdate": 1761998097728, "tmdate": 1762927193492, "mdate": 1762927193492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}