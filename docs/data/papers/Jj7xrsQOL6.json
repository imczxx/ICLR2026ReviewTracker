{"id": "Jj7xrsQOL6", "number": 18280, "cdate": 1758285967975, "mdate": 1759897114529, "content": {"title": "AsseslyAI: AI–Powered Assessment Framework for Skill-Oriented Engineering Lab Education", "abstract": "Practical lab education in computer science often faces challenges like plagiarism, lack of proper lab records, inadequate execution and assessment, limited student engagement, and absence of progress tracking for both students and faculties causing graduates with insufficient hands-on skills. In this paper, we introduce **AsseslyAI**, it tackles these challenges through online lab allocation, unique lab problem for each student, integrates AI-proctored viva evaluations, and gamified simulators to enhance engagement and conceptual mastery. While existing platform are generating questions on the basis of topics. Our framework fine-tunes on a **10k+ Question-Answer dataset** built from AIML lab questions to dynamically generate diverse, code-rich assessments. Validation metrics show high Question-Answer similarity, ensuring accurate answers and non-repetitive questions. By unifying dataset-driven question generation, adaptive difficulty, plagiarism resistance, and assessment in a single pipeline, our framework advances beyond traditional automated grading tools and offers scalable path to produce genuinely skilled graduates.", "tldr": "AsseslyAI leverages AI-driven personalized assessments, proctored vivas, and gamified simulators to enhance hands-on learning and skill development in computer science education.", "keywords": ["Ai in education", "Dynamic Question Generation", "Gamified Simulators", "Lab Record Management"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/533fe8a997c3697e0c4ce3bde4352fe73cf0566e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work focuses on building a framework for automatic assessment and feedback generation for students. Specifically, the authors built a dashboard for students and teachers to view the progress of students. They synthesized a set of 10K problems to avoid potential cheating issues as well."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The system if implemented correctly, should make meaningful contributions to the domain of intelligent tutoring systems."}, "weaknesses": {"value": "- The framing of the work does not make sense -- what exactly is the motivation? It's just not coherent in the motivation framing of the paper. The lack of engagement in labs may not be directly associated with cheating -- it's another issue. In addition, why would these new assessment tools help address the issues mentioned before? It's just not clear in the introduction.\n- Multiple validation processes or labeling processes require further details. For example, the difficulty levels are manually labeled -- it's a large set of 10K questions. How many people were involved? How about their level of expertise, and did they have a consistent guideline to label difficulty? These points are unclear.\n- There's no evaluation on the actual usage of the system. What is the population of students? Did they have learning gains from these systems, and how did the experiment work? These points are critical for educational AI work."}, "questions": {"value": "- Line 46: How would this be ensured for unique questions for students?\n- Section 2.1: These two things are just different -- they can be separate sessions.\n- Section 2.2: Difficulty calibration is not introduced, and I also don't think this is needed.\n- Line 250: How would an XGBoost generate these? How exactly do you generate feedback?\n- Figure 4: It's from January to August -- do you have the system across semesters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yZ1ikfIjzn", "forum": "Jj7xrsQOL6", "replyto": "Jj7xrsQOL6", "signatures": ["ICLR.cc/2026/Conference/Submission18280/Reviewer_PVGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18280/Reviewer_PVGm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608093979, "cdate": 1761608093979, "tmdate": 1762928001857, "mdate": 1762928001857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve lab work in computer science education by automatically creating lab problems based on teacher input and difficult levels. These question can then be assigned to each student. Towards this goal, they create 10K question-answer pairs. The question/answer pairs are evaluated based on their similarity and automated grading compared against manual grading."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The strength of the paper is that is tackles an important problem: AI-supported, individual learning. The authors seem to have implemented a working prototype including web interface for their system including a student/teacher dashboard. The system also seems to feature a Speech Interface for viva questions."}, "weaknesses": {"value": "The paper reads more like an implementation report of an AI-Lab Management system (e.g., authentication and access control). The challenges it wants to address are very wide:\nplagiarism, lack of proper lab records,unstructured lab conduction,inadequate execution and assessment,lack of practical learning,limited student engagement, and absence of progress tracking.\nThese challenges seem to have little overlap with what is then presented and evaluated later in the paper."}, "questions": {"value": "I think this paper is not a good fit for ICLR. I suggest the authors to revise the paper, i.e., better defining the scope and designing experiments and then submit it to a more applied conference/venue, maybe in the educational science domain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x5RhFpDOf6", "forum": "Jj7xrsQOL6", "replyto": "Jj7xrsQOL6", "signatures": ["ICLR.cc/2026/Conference/Submission18280/Reviewer_sVvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18280/Reviewer_sVvs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662426632, "cdate": 1761662426632, "tmdate": 1762928001442, "mdate": 1762928001442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a platform for delivering personalised practical exercises for computer science students. The pair describes the overall system architecture and key features and a dataset of 10,000 LLM-generated questions that have been created. A model to generate more questions is then fine-tuned using this dataset. Some evaluations are presented to validate the questions in the synthetic dataset and those generated by the fine tuned model."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The main strengths of the paper are:\n\n— The paper addresses the use of LLMs to support computer science education through the generation of personalised learning materials, which is a promising research area. \n— The generated set of 10,000 questions could be a useful contribution to the research community."}, "weaknesses": {"value": "The main weaknesses of the paper are:\n\n— Large parts of the paper are descriptions of system, platform, and interface design which have very little relevance to ICLR. \n— The paper needs careful review and revision as it has many typographical and language issues. \n— It is not clear what is actually being tested in the evaluation experiments described by the authors. Evaluations compare the marksAI and marksFaculty but it is not clear what these actually measure. \n— The contributions of the paper are not made explicit."}, "questions": {"value": "What exactly do marksAI and marksFaculty measure and why are they needed in the synthetic question database? \n\nHow is the following claim supported: \"Unlike competitive programming datasets such as CodeNet Puri et al. (2021), our synthetic dataset is explicitly aligned with computer science lab curricula. It emphasizes implementation skills, conceptual understanding, and explainability-oriented evaluation rather than correctness alone, better reflecting the learning objectives of practical coursework.\"?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This paper proposes a system that will automatically grade student work. This raises significant ethical issues that should be discussed."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qEmJMgIera", "forum": "Jj7xrsQOL6", "replyto": "Jj7xrsQOL6", "signatures": ["ICLR.cc/2026/Conference/Submission18280/Reviewer_g1gh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18280/Reviewer_g1gh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870421596, "cdate": 1761870421596, "tmdate": 1762928001087, "mdate": 1762928001087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims to use a model fine-tuned on 10k (or 8k) programming lab questions to\ngenerate different questions for individual students, of varying difficulty levels, with automatic grading."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Using AI to generate questions for individual students may be useful.\n\n2. Adapting question difficulty to individual students would be useful."}, "weaknesses": {"value": "1. Unfortunately, this works lacks any human validation of its claim.\n\n2. It is unclear if parts of the paper or reproducibility checklist are written by a human or revised/hallucinated by an LLM to meet submission criteria.\n\n3. The paper includes inconsistencies about implementation details.\n\n4. Missing data and code.\n\n5. Missing evidence for claims made in the abstract."}, "questions": {"value": "Were parts of the paper written or revised by an LLM?\nWere the figures or code used for generating the figures generated by an LLM? Was there human oversight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s1xo7zBSHH", "forum": "Jj7xrsQOL6", "replyto": "Jj7xrsQOL6", "signatures": ["ICLR.cc/2026/Conference/Submission18280/Reviewer_4BNn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18280/Reviewer_4BNn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488848379, "cdate": 1762488848379, "tmdate": 1762928000762, "mdate": 1762928000762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}