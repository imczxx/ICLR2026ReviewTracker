{"id": "yB09CcjoII", "number": 6402, "cdate": 1757980434256, "mdate": 1759897916815, "content": {"title": "Concept or Skills? Rethinking Instruction Selection for Multi-modal Models", "abstract": "Most existing instruction selection methods in vision-language learning rely on sample embeddings to guide data choice. These embeddings are typically derived from pure vision encoders or small multimodal models and they primarily capture \\emph{visual concepts} while under-representing \\emph{visual skills} such as counting, spatial reasoning, or commonsense inference. This imbalance overlooks a key distinction: multimodal benchmarks vary widely in whether they emphasize conceptual grounding or skill-based reasoning. We show that this concept--skill axis provides a systematic lens for characterizing benchmark demands, and that prioritizing one dimension often comes at the expense of the other. To address this, we introduce a simple benchmark-aware data selection framework that adapts training data to the dominant alignment factor of each benchmark. Across twelve diverse benchmarks, our approach yields consistent improvements, especially in low-data regimes (+0.9\\% over the best existing baseline on average and +1.5\\% on the skill-focused subset). More broadly, our findings highlight that advancing multimodal learning requires explicit recognition of the dual role of concepts and skills in shaping benchmark behavior.", "tldr": "", "keywords": ["instruction selection", "multimodality", "vision-language model", "data selection", "instruction-tuning", "SFT"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2177ae98098359cfb5eeed288ee32faf66947d25.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical limitation in existing vision-language instruction tuning: the overreliance on visual concept-centric embeddings in instruction selection, which fails to adequately capture skill-based reasoning (e.g., counting, spatial inference). It introduces a benchmark-aware framework that decouples \"concepts\" (visual entities/attributes) and \"skills\" (reasoning operations) , using nearest-neighbor retrieval in separate embedding spaces to select training instructions aligned with a benchmark's dominant axis (concept- vs. skill-heavy). The authors validate the framework across 12 diverse benchmarks, demonstrating consistent performance gains (especially in low-data regimes: +0.9% average over baselines, +1.5% on skill-focused tasks). Overall, the work offers valuable insights into benchmark characterization and task-adaptive data selection for multi-modal models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.\tThe clear distinction between concepts (what is depicted) and skills (how to reason about it) provides a systematic lens for analyzing benchmark demands and exposes biases in prior embedding-based methods, which often favor concept-heavy tasks.\n\n2.\tThe dual-space nearest-neighbor retrieval (using CLIP for concepts and LLM-prompted sentence embeddings for skills) is elegant, lightweight, and easy to implement, requiring no new training—ideal for low-resource settings and enabling controlled ablation of concept vs. skill prioritization.\n\n3.\tTesting across 12 benchmarks spanning VQA, OCR, and reasoning tasks, with multiple data budgets and datasets (LLaVA-1.5, ALLaVA-4V), demonstrates robustness; results show targeted gains especially in skill-heavy benchmarks (e.g., +7.6% on OK-VQA at 5% budget), backed by statistical significance via multiple seeds.\n\n4.\tThe paper’s analysis of hybrid selection strategies (e.g., summing concept/skill scores) reveals that naive combination dilutes dominant signals, and its mutual ranking method for benchmark classification offers a lightweight alternative to iterative fine-tuning—both findings guide future work on task-adaptive learning."}, "weaknesses": {"value": "1.\tThe framework assumes prior access to benchmark details (e.g., validation sets for determining \"concept/skill dominance\"), which limits its applicability to novel or unseen tasks where such information is unavailable. The mutual ranking method partially mitigates this but still requires precomputed concept/skill embeddings of benchmark samples.\n\n2.\tThe paper evaluates tasks in isolation but does not explore how the framework performs in multi-task tuning (e.g., combining concept-focused and skill-focused benchmarks). This is a key oversight, as real-world multi-modal systems often need to generalize across multiple task types, where concept/skill biases might interfere.\n\n3.\tRelying on GPT-4o for isolating skills introduces potential costs, biases, or inconsistencies (e.g., varying prompt sensitivity)\n\n4.\tThe evaluation focuses on a single model architecture (LLaVA-1.5 with Vicuna-7B and CLIP-ViT) and does not test the framework’s transferability across different model scales (e.g., larger LLMs) or architectures (e.g., non-ViT visual encoders). It remains unclear if the findings hold for more diverse multi-modal models. An ablation on alternative/open-source LLMs or manual validation would strengthen claims about the pipeline's reliability."}, "questions": {"value": "The skill embedding pipeline relies on GPT-4o to generate skill descriptions (e.g., \"interpreting graph trends\" for SQA-I tasks) and MiniLM-L6-v2 for encoding. However, the paper does not report LLM output consistency for these skill descriptions: (1) How consistent are GPT-4o’s skill descriptions across multiple generations for the same instruction? (2) Did you validate these descriptions against human annotations (e.g., having annotators label skills for a subset of instructions) to ensure they align with \"true\" reasoning demands of the tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5m05SenKJE", "forum": "yB09CcjoII", "replyto": "yB09CcjoII", "signatures": ["ICLR.cc/2026/Conference/Submission6402/Reviewer_prYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6402/Reviewer_prYi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761142400214, "cdate": 1761142400214, "tmdate": 1762918805126, "mdate": 1762918805126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark-aware instruction selection framework for vision-language models, emphasizing the distinction between concepts (what an image depicts) and skills (how to reason about it). The authors build dual embedding spaces—concept and skill—and select training data aligned with each benchmark’s dominant type. Experiments on several benchmarks show consistent improvements, especially on reasoning-oriented tasks, and a mutual-ranking analysis predicts whether a benchmark is concept- or skill-driven."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The distinction between concept and skill provides an interpretable lens to analyze multimodal benchmarks and instruction data. This perspective is intuitive grounded.\n2. The retrieval-based dual embedding framework is methodologically clear and computationally efficient.\n3. The use of mutual ranking and qualitative studies adds interpretability and provides a diagnostic tool for understanding benchmark biases."}, "weaknesses": {"value": "1. Skill extraction is a central component of the proposed method, yet it relies heavily on prompting large proprietary LLMs such as GPT-4o. This dependency may introduces several risks, such as uncontrolled variability, potential hallucination, and dependency on closed-source systems. It is suggested that the authors analyze how noisy or inconsistent skill descriptions affect downstream performance.\n2. The method presupposes knowledge of whether a benchmark is concept- or skill-dominant. In real-world or unseen tasks, this assumption may be impractical. Although the mutual-ranking heuristic partially addresses this, it still requires benchmark data access, which limits generalization.\n3. The reported improvements are relatively modest and could stem from the aforementioned unrealistic assumptions. The authors do not conduct rigorous significance testing or ablation to isolate the contribution of each component (e.g., concept vs. skill embeddings).\n4. While the authors claim efficiency, the method requires embedding all samples in two spaces and prompting LLMs for every instruction, which could be expensive at scale. It is suggested that the authors provide runtime or cost analysis.\n5. While the paper presents an appealing conceptual distinction between “concepts” and “skills,” it does not offer a formal theoretical framework to justify or to explain how these dimensions interact within multimodal learning dynamics. The absence of theoretical modeling limits the interpretability and generalizability of the proposed approach beyond empirical observations."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7NbgT1XXzT", "forum": "yB09CcjoII", "replyto": "yB09CcjoII", "signatures": ["ICLR.cc/2026/Conference/Submission6402/Reviewer_CQNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6402/Reviewer_CQNM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200989230, "cdate": 1761200989230, "tmdate": 1762918804683, "mdate": 1762918804683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dual-space instruction selection framework for multimodal (vision–language) models, distinguishing between concept-based and skill-based learning. The key insight is that existing data selection methods primarily capture conceptual similarity (e.g., object identity, visual categories) while under-representing skills such as counting, reasoning, or spatial understanding.\nThe authors introduce separate embeddings for concepts (via CLIP image features) and skills (via LLM-generated skill descriptions embedded by a sentence transformer), and use nearest-neighbor retrieval to curate instruction subsets in each space. They then fine-tune vision–language models (based on LLaVA-1.5 and ALLaVA-4V) on these subsets and evaluate across 12 benchmarks, showing that concept- and skill-targeted selection improve performance on concept- and skill-dominant tasks respectively. They further propose a mutual rank heuristic to predict benchmark alignment (concept vs skill) without exhaustive fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The conceptual separation of concept and skill spaces for data selection is both novel and intuitive, offering a new lens to interpret multimodal benchmarks. The proposed mutual-rank analysis provides a simple yet elegant diagnostic tool for predicting task alignment.\n\n- The experimental setup is thorough, including comparisons with baselines such as Coincide and PreSel under various data budgets. The consistency of improvements supports the central claim.\n\n- The paper is generally well organized, and the visual–linguistic dichotomy is clearly motivated. The methods (Sections 3.1–3.4) are presented in a reproducible way, with solid quantitative and qualitative evidence (Tables 1–5).\n\n- The framework highlights a fundamental bias in multimodal instruction tuning — the dominance of concept alignment — and provides a principled way to address it. This could inspire future benchmark design and adaptive data selection methods in large-scale multimodal learning."}, "weaknesses": {"value": "- Quantification of concept/skill mixture: Many real-world tasks span both conceptual and skill dimensions. The paper does not propose a quantitative metric or visualization (e.g., cluster plots) to show how tasks distribute along this spectrum. Including such an analysis would clarify how “concept-dominant” vs “skill-dominant” labels were determined.\n\n- Lack of interpretability in skill embeddings: Since skill embeddings rely on GPT-4o-generated textual descriptions, their reliability across LLMs or domains remains uncertain. Some justification or ablation on the LLM dependency would strengthen confidence in the approach.\n\n- The fixed data budget may bias results toward in-distribution tasks; as seen in Table 2, when the budget increases, the gap between concept- and skill-targeted models narrows, suggesting possible data sufficiency effects.\n\n- The ALLaVA 2.5 % experiment omits some baselines (e.g., PreSel, Coincide), making the comparison incomplete.\n\n- Missing examples: Section 3.2 would benefit from a concrete illustration of a concept representation (e.g., an image–CLIP embedding) and a corresponding skill representation (e.g., “counting objects” → textual embedding).\n\n- No visualization of benchmark clusters: Without a figure demonstrating benchmark grouping or task-space distribution, the distinction between concept- and skill-dominant benchmarks feels qualitative."}, "questions": {"value": "- Can the authors visualize the benchmark distribution in concept/skill space to justify their classification?\n\n- How sensitive are the results to the choice of the LLM used for generating skill descriptions? Would a smaller or different model yield consistent embeddings?\n\n- When relaxing the data-budget constraint, how does the relative gain of skill-targeted selection evolve? Does the method still improve over untargeted baselines when more data are available?\n\n- Could the authors provide examples of instructions where concept- and skill-based retrieval select notably different neighbors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ebZkrnfeEx", "forum": "yB09CcjoII", "replyto": "yB09CcjoII", "signatures": ["ICLR.cc/2026/Conference/Submission6402/Reviewer_5ywy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6402/Reviewer_5ywy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786411466, "cdate": 1761786411466, "tmdate": 1762918804375, "mdate": 1762918804375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Instruction tuning for vision–language models usually picks training examples by embedding similarity, which mostly captures what’s in an image (concepts) but not what the task is asking you to do (skills like counting, spatial reasoning, or reading text). That makes current selection pipelines biased toward concept-heavy benchmarks such as VQAv2 or VizWiz and less useful for skill-heavy ones like AI2D, ScienceQA-image, or OK-VQA. The paper asks whether selection should explicitly match the benchmark on concepts or on skills, because different benchmarks emphasize different things.\n\nTo test this, the authors build two parallel selection spaces over the same instruction pool. One space uses CLIP-style image embeddings to measure concept similarity. The other space first asks an LLM which visual skills an example needs and embeds that skill description, giving a skill similarity space. For any benchmark, they can retrieve nearest neighbors in either space and fine-tune the same VL model twice, making it clear which alignment is better. A mutual-ranking heuristic then predicts whether a benchmark is concept- or skill-dominant, so you can pick the right space without always training twice.\n\nAcross 12 multimodal benchmarks in low-data regimes, matching the benchmark’s dominant dimension gives consistent improvements: on average about 0.9 percentage points better than strong selection baselines, and about 1.5 points better on the skill-heavy tasks, where prior methods struggle. Simple mixtures of concept and skill data don’t consistently beat the targeted choice, so the main lesson is that instruction selection should be benchmark-aware instead of one-size-fits-all."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Explores the problem of data selection in multi-modal instruction tuning, which has significant practical benefits\n\n- The idea to build two parallel retrieval spaces — one for concepts (CLIP-like visual embeddings) and one for skills (LLM derived skill descriptions) is simple but powerful.\n\n- The cross-ranking trick — compare how concept-based and skill-based rankings agree or disagree — gives a way to predict whether a task is concept- or skill-dominant without fully training two models."}, "weaknesses": {"value": "- Relies on LLM-based skill extraction: the skill space quality depends on how well a general LLM can label “what skill is needed,” so noise or prompt sensitivity there can propagate through the whole method.\n\n- Gains are modest in absolute terms: +0.9% avg / +1.5% on skill-heavy is solid but not great\n\n- Assumes access to the target benchmark’s distribution: their benchmark-aware strategy needs to “peek” at the benchmark to decide concept vs. skill"}, "questions": {"value": "- Your skill space comes from LLM-generated skill tags. How sensitive are your results to that step ?\n- Your mutual-ranking diagnostic is supposed to tell us whether a benchmark is concept- or skill-dominant. How robust is that in practice: how much target data do you need, and does it still work under distribution shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PYl3lxXIpy", "forum": "yB09CcjoII", "replyto": "yB09CcjoII", "signatures": ["ICLR.cc/2026/Conference/Submission6402/Reviewer_EqNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6402/Reviewer_EqNf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982146100, "cdate": 1761982146100, "tmdate": 1762918803903, "mdate": 1762918803903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}