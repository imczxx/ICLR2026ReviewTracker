{"id": "b2tBRLic4V", "number": 12868, "cdate": 1758211051769, "mdate": 1763725856061, "content": {"title": "Bayesian Post Training Enhancement of Regression Models with Calibrated Rankings", "abstract": "Accurate regression models are essential for scientific discovery, yet high-quality numeric labels are scarce and expensive. In contrast, rankings (especially pairwise) are easier to obtain from domain experts or artificial intelligence (AI) judges. We introduce Bayesian Enhancement with Calibrated Ranking (BAYES-ECR), a novel plug-and-play method that improves a base regressor’s prediction for a\nquery by leveraging pairwise rankings between the query and reference items with known labels. BAYES-ECR performs a Bayesian update that combines a Gaussian likelihood from the regressor and the Bradley-Terry likelihood from the ranker. This yields a strictly log-concave posterior with a unique maximum likelihood solution and fast Newton updates. We show that prior state-of-the-art is a special case of our framework, and we identify a fundamental failure mode: Bradley-Terry likelihoods suffer from scale mismatch and curvature dominance when the number of reference items is large, which can degrade performance. From this analysis, we derive a calibration method to adjust the information originating from the expert rankings. BAYES-ECR shows a stunning 97.65% median improvement across 12 datasets over previous state-of-the-art method using a realistically-accurate ranker, and runs efficiently on a consumer-grade CPU.", "tldr": "", "keywords": ["post training", "regression enhancement", "calibrated rankings"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac84aa53dbe320ac89be903e27ab4c38bb378ccd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a plug-and-play Bayesian enhancement to regression models. Specifically, pairwise ranking information complements scarce absolute labels as in RankRefine, but with Bayesian aggregation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Performance gain: Similar ideas as RankRefine but shows performance\n* Efficiency: Plug-and-play avoids costly model retraining. \n* Data Efficiency: Complementary pairwise signal helps data-scarce fields such as science."}, "weaknesses": {"value": "Some questions left unanswered.\n* Motivation behind pairwise: Would listwise ranking be a stronger signals?\n* Generalization: While we understand science is a data-scarece domain, evaluating only on this scenario makes us wonder whether it would generalize to general regression problems."}, "questions": {"value": "Any additional analysis to answer questions in the weakness section would help rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iF21FkLqzd", "forum": "b2tBRLic4V", "replyto": "b2tBRLic4V", "signatures": ["ICLR.cc/2026/Conference/Submission12868/Reviewer_aXK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12868/Reviewer_aXK6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461440481, "cdate": 1761461440481, "tmdate": 1762923659516, "mdate": 1762923659516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of improving regression models in data-scarce domains, where high-quality numeric labels are expensive, but pairwise rankings are relatively cheap to obtain. The authors propose Bayesian Enhancement with Calibrated Ranking (BAYES-ECR), a novel and efficient \"plug-and-play\" method to enhance a pre-trained regressor's predictions post-training, without requiring any retraining. The core idea is to perform a per-query Bayesian update. For a given query, BAYES-ECR combines two sources of information: (1) A Gaussian likelihood from the pre-trained regressor, centered on its prediction $\\hat{y}_{0}^{re}$ with variance $\\sigma_{re}^{2}$. (2) A ranker likelihood derived from pairwise comparisons between the query and a small set of $k$ reference items with known labels. This likelihood is modeled using the Bradley-Terry model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main contribution of the work is its analysis of where the Bradley–Terry model breaks down. Although using BT models for ranking is fairly common, the authors offer a fresh insight: the “soft-count” sigmoid mechanism interacts with the number of reference points, $k$, in a way that creates a kind of curvature dominance—an effect that amplifies bias. This observation is both new and important. This insight moves the field from heuristic fusion (like inverse-variance weighting) to a more principled understanding."}, "weaknesses": {"value": "1. The framework builds on combining a regressor likelihood, $\\mathcal{N}(\\hat{y}{0}^{re}; y_0, \\sigma{re}^2)$, with a ranker likelihood. The Newton step works as an information-weighted average, where the regressor’s contribution is scaled by $I_{reg} = 1/\\sigma_{re}^2$. This makes $\\sigma_{re}^2$ a key parameter. The paper notes that many regressors don’t naturally provide this value and suggests using ensembling or dropout to approximate it—but it never clearly explains how $\\sigma_{re}^2$ was actually estimated for the Random Forest and MLP models in the experiments. Was it treated as a single hyperparameter tuned on a validation set?\n\n2. The accuracy of the ranker, $a$, is necessary to operate the accuracy-aware soft gate, $\\tau(a)$. According to the authors, a \"small validation set\" is used to gauge this accuracy. However, rather than focusing on how sensitive the results are to the accuracy estimate itself, the robustness checks in the appendix examine how well $\\hat{\\tau}_{cal}$ is calibrated (see Figures 5 and 6). The estimate of $a$ may not be accurate if that validation set is very small, which could render $\\tau$ less than ideal. The robustness claims would be much stronger if a brief sensitivity analysis of how $\\tau$ varies with various estimates of $a$ were included."}, "questions": {"value": "1. Could the authors please clarify precisely how the regressor variance $\\sigma_{re}^2$ was estimated for the Random Forest and MLP experiments? Given its critical role as $1/I_{reg}$ in the information-weighted update step, this detail is essential for reproducibility and for understanding the method's behavior. Did you, for example, estimate $\\sigma_{re}^2$ as the Mean Squared Error of the base regressor on a hold-out set?\n\n2. According to Section 3.3, labeled pairs from the reference set $\\mathbb{D}$ are fitted with $s(\\hat{\\omega}(y_a - y_b))$ to estimate $\\hat{\\tau}_{cal}$. Is the true rankings (i.e., $r_i = 1$ if $y_a > y_b$) obtained from the ground-truth labels in $\\mathbb{D}$ used in this fitting process? Or does it make use of the external ranker's noisy rankings $r_i = R(x_a, x_b)$?\n\n3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2QzMQYnsAJ", "forum": "b2tBRLic4V", "replyto": "b2tBRLic4V", "signatures": ["ICLR.cc/2026/Conference/Submission12868/Reviewer_JneQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12868/Reviewer_JneQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886643030, "cdate": 1761886643030, "tmdate": 1762923659018, "mdate": 1762923659018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, authors propose BAYES-ECR, a novel plug-and-play post-training method designed to improve the predictions of a base regression model. The key idea is to leverage pairwise rankings between a query item and a set of reference items (with known labels), often easier and cheaper to obtain than high-quality numeric labels. BAYES-ECR is based on a Bayesian update using Gaussian likelihood (from base regressor) and another likelihood from external pairwise ranker. Authors find that the resulting posterior is log-concave thereby ensuring a unique maximum likelihood solution which can be recovered using efficient optimization procedures like Newton updates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the following strengths in this paper:\n- I find combining regressor predictions with pairwise ranking information is a novel way as the form of Bayesian update, a cool idea and which I believe is this work’s significant conceptual strength, moving beyond other heuristic approaches.\n- The core problem, i.e. scarcity of high-quality numeric labels versus the availability of easier-to-obtain pairwise rankings, is highly relevant in many scientific and engineering domains (not just LLMs), for e.g. such as materials science and drug discovery, therefore this method has a wide applicability.\n- I find the proposed temperature calibration and accuracy-aware soft gating mechanisms to be well-motivated by the theoretical analysis.\n- Based on the empirical findings, the proposed method runs very efficiently and from what I find, inference is unlikely to be a bottleneck for this method.\n- The study by including off-the-shelf LLMs as imperfect rankers highlights the method's applicability to real-world scenarios."}, "weaknesses": {"value": "Few weaknesses that I would like the authors to address:\n- The core Bayesian update relies on the assumption of conditional independence between the regressor and ranker - while I think this assumption may make some sense in general, but I also think thai may not always perfectly hold in practice, especially if the ranker's \"expert\" knowledge is implicitly derived from data similar to the regressor's training data. Is this something the authors have considered or have any view on? How will the method change if this is the case?\n- The paper mentions that regression improvements may depend on the composition of the reference set and suggests future work on sophisticated sampling strategies. A brief discussion of potential biases / optimal strategies for reference set selection, even if speculative, could enrich the overall study in this paper.\n- One of the commonly discussed limitations of the Bradley-Terry model is that it inherently assumes transitive relationship among the pairwise comparisons, i.e. if A > B and B > C, then A > C. While this is usually fine in many cases, some real-world \"expert\" rankings might be locally inconsistent or reflect non-transitive preferences in certain contexts, which could challenge the model. Can the authors comment on such a setup? Or what impact it can have on such applications? Finally, any thoughts on how the current framework can be extended to handle it?"}, "questions": {"value": "Some additions questions to the authors:\n- How sensitive is BAYES-ECR's performance to inaccuracies or miscalibration of the base regressor's uncertainty estimate?\n- How much is the computational burden of the Newton update steps in real-world applications? Can the authors provide some clear numbers to understand the overhead for using the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "spJOXv6Jdt", "forum": "b2tBRLic4V", "replyto": "b2tBRLic4V", "signatures": ["ICLR.cc/2026/Conference/Submission12868/Reviewer_zBQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12868/Reviewer_zBQs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938908395, "cdate": 1761938908395, "tmdate": 1762923658470, "mdate": 1762923658470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BAYES-ECR (Bayesian Enhancement with Calibrated Rankings), a novel post-training Bayesian refinement framework for regression models using pairwise ranking information. The method integrates the Gaussian likelihood from a pretrained regressor with a Bradley-Terry ranking likelihood, forming a strictly log-concave posterior that enables efficient MAP optimization.\nThe authors identify a key failure mode in prior ranking-based refinements (e.g., RankRefine): when the number of reference items increases, the Bradley-Terry likelihood can dominate due to curvature mismatch, causing bias. BAYES-ECR corrects this through temperature calibration and accuracy-aware soft gating, controlling the relative contribution of ranking information.\nEmpirical results across 12 datasets (including molecular ADMET and tabular tasks) show that BAYES-ECR significantly improves predictive accuracy and robustness"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is with fair conceptual clarity and novelty. It cleanly reformulates post-training regression enhancement as a Bayesian inference problem rather than heuristic fusion, providing a unified probabilistic interpretation that subsumes RankRefine as a special case.\n\n- The theoretical grounding is solid. The analysis of rank-likelihood dominance and curvature scaling (Lemmas 3.4–3.7) is rigorous and insightful, pinpointing why Bradley–Terry models can fail at large k and how temperature calibration mitigates this.\n\n- The overall evaluation is comprehensive. The paper evaluates multiple variants (MAP, MLE, MLE-Temp, MLE-GatedTemp) across molecular and non-molecular datasets, using both oracle and LLM rankers. Ablations, runtime, and robustness studies are thorough."}, "weaknesses": {"value": "I'm not the expert of reco.sys, but I tried my best to understand the paper. Here are some general concerns based on my understanding:\n\n1. Despite the clean Bayesian framing, much of the improvement stems from calibration heuristics rather than a fundamentally new probabilistic model. It seems that the method extends, rather than transforms, RankRefine’s core idea.\n\n2. The proofs dominate the narrative. Figures are dense and sometimes too mathematical (Fig. 1–3). The paper could benefit from a small worked example demonstrating Bayesian updates visually. While technically sound, the exposition can feel overloaded: long derivations, dense notation reuse, and sometimes unclear transitions (e.g., from Lemma 3.7 to Corollary 3.8). Some results could move to Appendix without loss. Clearer clarification on the background will be appreciated."}, "questions": {"value": "- How sensitive is BAYES-ECR to the regressor variance σ²_re? Does under- or overestimated uncertainty systematically bias the refinement?\n\n- The analysis assumes symmetric flip noise. What happens if the ranker has systematic bias (e.g., consistently underestimates large values)? Could the Bayesian posterior incorporate a bias term instead of gating?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "smNMWwIrIb", "forum": "b2tBRLic4V", "replyto": "b2tBRLic4V", "signatures": ["ICLR.cc/2026/Conference/Submission12868/Reviewer_rqoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12868/Reviewer_rqoX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002067798, "cdate": 1762002067798, "tmdate": 1762923658190, "mdate": 1762923658190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}