{"id": "BdeUsYlyIf", "number": 21493, "cdate": 1758318244778, "mdate": 1763636906829, "content": {"title": "Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX", "abstract": "Reinforcement learning (RL) research requires diverse, challenging environments that are both tractable and scalable. \nWhile modern video games may offer rich dynamics, they are computationally expensive and poorly suited for large-scale experimentation due to their CPU-bound execution. \nWe introduce Octax, a high-performance suite of classic arcade game environments implemented in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely adopted as a benchmark in RL research.\nOctax provides the JAX community with a long-awaited end-to-end GPU alternative to Atari games, offering image-based environments, spanning puzzle, action, and strategy genres, all executable at\nmassive scale on modern GPUs. \nOur JAX-based implementation achieves orders-of-magnitude speedups over traditional CPU emulators.\nWe demonstrate Octax's capabilities by\ntraining RL agents across multiple games, showing significant improvements in \ntraining speed and scalability compared to existing solutions. The environment's modular design enables researchers to easily extend the suite with new games or generate novel environments using large language models, making it an ideal platform for large-scale RL experimentation.", "tldr": "", "keywords": ["Reinforcement Learning", "Benchmarking", "CHIP-8", "JAX", "Environments", "Simulation", "Acceleration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b752db3720a25ed414f6090881e8bfc26798ca96.pdf", "supplementary_material": "/attachment/ad0e866a896cff866733ba7d46d6b85fcd524a9d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OCTAX, a suite of classic arcade-style RL environments implemented as a fully vectorized CHIP-8 emulator in JAX. The goal is to provide an Atari-like benchmark that (a) runs end-to-end on GPU, (b) scales to thousands of parallel environments, and (c) maintains authentic game dynamics. The authors show throughput up to ~350k steps/sec (≈1.4M frames/sec) with 8,192 parallel environments on a single consumer GPU (RTX 3090), which they report is roughly 14× faster than CPU-bound baselines like EnvPool on Atari Pong. \n\nThe benchmark covers 20+ CHIP-8 games across genres (puzzle, action, navigation, resource management, etc.), and the paper provides PPO training curves for 16 of them. These curves exhibit meaningful diversity in difficulty and learning dynamics (fast plateau vs. gradual improvement vs. failure-to-learn cases like Tetris/Worm), suggesting this is not just “16 clones of Breakout,” but a range of cognitive burdens. \n\nFinally, the paper explores automatic environment generation: they use an LLM to synthesize new CHIP-8 games (e.g. a “Target Shooter” with increasing difficulty levels), plus score/termination logic, and then successfully train PPO on these generated tasks with clean difficulty gradients. This is pitched as a path toward scalable curriculum / rapid task creation. \n\nOverall, the paper proposes OCTAX as “the missing GPU-native Atari for JAX RL,” arguing that this makes statistically reliable RL experiments cheaper and more reproducible for smaller labs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**S1. Clear problem, clear value to the community.**\nThe paper correctly identifies a real bottleneck in RL: environments are still mostly CPU-bound, even though policy learning is GPU-bound. This prevents large-scale sweeps, high seed counts, and rigorous statistics, especially for vision-based tasks like Atari. OCTAX directly targets that gap with an image-based, arcade-style benchmark that runs entirely in JAX on GPU. \n\n**S2. Strong engineering contribution.**\nThe authors implement a full CHIP-8 fetch/decode/execute loop in JAX using functional state passing, vectorized dispatch (lax.switch), and batched framebuffers, and wrap it as Gym/Gymnax-style RL envs with reward extraction, termination logic, action pruning, frame stacking, etc. They argue that fidelity to original game mechanics is preserved. \nThis is nontrivial and, as far as I know, not previously available in the JAX ecosystem for Atari-like games.\n\n**S3. Throughput + scaling results are compelling.**\nReported numbers (350k steps/sec, linear-ish scaling up to 8k envs on a single 3090; 14× over EnvPool Pong at high parallelism) are impressive and very relevant to labs that don't have multi-node clusters. The memory footprint (2 MB per env, linear in env count) is also concretely discussed. \nThis is an unusually thorough systems evaluation for an RL benchmark paper.\n\n**S4. Diversity of tasks and empirical characterization.**\nThey don’t just dump environments; they also measure PPO learning curves across 16 games and group them into qualitative regimes (fast plateau vs. gradual improvement vs. hard/sparse). This suggests these games could be used for algorithm diagnostics, curriculum, etc."}, "weaknesses": {"value": "**W1. How “Atari-like” is it, really? (External validity.)**\nCHIP-8 games are dramatically simpler than Atari 2600 in terms of resolution (64×32 monochrome), action semantics, and world complexity. The paper asserts “Atari-like cognitive demands,” but the qualitative gap (e.g., long-horizon exploration, partial observability, rich object interactions) is not deeply quantified. We see PPO struggling on Tetris/Worm, but I'd like more systematic evidence that success on OCTAX predicts anything on Atari, NetHack, Procgen, etc. \nRight now OCTAX looks great for intra-JAX benchmarking and ablations, but it's less clear if it’s meaningful as a drop-in Atari replacement for algorithm claims.\n\n**W2. Reward shaping / termination extraction feels artisanal.**\nFor each game, they manually or semi-automatically infer score registers, life counters, game-over flags, menu skips, etc. (e.g. “Brix stores score in V5, Pong encodes BCD in V14, etc.”). This is powerful, but also fragile and slightly underspecified. \nIf a new contributor adds a weird ROM, how confident are we that OCTAX's automatic heuristics won't silently produce a broken reward function (e.g. rewarding losing health)? The paper mentions static+dynamic analysis plus some LLM help, but I’d like stronger guarantees or validation.\n\n**W3. No baselines beyond PPO.**\nAll learning curves are PPO only. There's no DQN-style baseline, no lightweight world-model baseline, no offline RL baseline, etc. PPO is reasonable (and popular), but I'd like to know if these environments produce meaningful rankings across algorithms, or whether they’re PPO-biased (e.g. continuous control style tuning, frame stacking assumptions, frame-skip assumptions). \nRight now we mainly learn “PPO can learn some of them.”"}, "questions": {"value": "**Generalization / external validity.**\nDo you have any evidence that OCTAX performance correlates with Atari performance, Procgen performance, or other visual RL benchmarks? Even a tiny pilot (e.g. rank-correlation of seed-averaged scores across algorithms) would strengthen the “Atari alternative” positioning. \n\n**EnvPool/CuLE comparison.**\nCan you report CuLE numbers (GPU Atari) on the same GPU you used for OCTAX, or at least discuss why that wasn’t feasible? That would make the “14× faster” claim feel less like apples vs oranges. Also, have you tried running OCTAX on CPU only to show the CPU→GPU delta cleanly? \n\n**Automatic reward/termination inference.**\nFor a new arbitrary CHIP-8 ROM (unseen by you), how robust is score/termination extraction? Do you have quantitative success rates for the static+dynamic heuristics or the LLM-generated wrappers? For RL practitioners, “plug in a ROM and it just works” is a killer feature—please convince us it's realistic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pWfpa8yfvK", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Reviewer_E8PB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Reviewer_E8PB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524546738, "cdate": 1761524546738, "tmdate": 1762941804409, "mdate": 1762941804409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Octax, a high-performance arcade-style benchmark for Reinforcement Learning in Jax. The paper utilizes end-to-end GPU training, allowing extremely high throughput. Primarily, this work looks to provide a viable alternative to the long-standing Atari benchmark, but using considerably less computational resources. Furthermore, the paper presents a way to quickly use LLMs to generate new environments, on top of the current set of games presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper aims to achieve the important goal of reducing the cost of running Reinforcement Learning experiments, clearly succeeding, achieving an impressive 1.4 million frames per second on consumer-grade hardware. The code already being open-source is also a nice positive. LLM-generated environments using the provided prompts provide a nice way for developers to test environments with specific properties if they wish."}, "weaknesses": {"value": "While I appreciate the attempt to reduce the computational burden of the Atari, I feel that the authors missed many key reasons why the Atari benchmark became so popular, and many of the features that still make it so popular today. In its current form, I don’t believe this work is capable of replacing the Atari benchmark without major reforms for a few key reasons:\n\n- **A clear, fixed way to evaluate** - One reason Atari is useful is that there is a relatively clear and fixed protocol for evaluation [1]. This means researchers know what environments they need to run, the settings they need to use, and know that their work will be comparable to others. Furthermore, there are precise guidelines on how scores should be reported [2]. The paper would benefit from being very clear about what environments are included in the benchmark, how many frames/timesteps algorithms should be trained for, and how results should be reported, with examples. Currently Figure 3 does not meet the same standards as many papers which use the Atari benchmark.\n- **Aggregate Scores** - Atari is useful as a benchmark, as the performance of an algorithm can be boiled down to a single number or single graph. In Atari, this has become human-normalized IQM performance with 95\\% confidence intervals, which provides a score for the given algorithm. This prevents games with different score magnitudes from dominating the final score. Currently, this appears to be missing, despite games having scores of different magnitudes.\n- **No-op Starts, Sticky Actions and Random Actions** - In the Atari benchmark, features are provided that prevent the agent from exploiting determinism (preventing brute-force approaches). Specifically, no-op starts randomly uses up to 30 no-op steps at the start of episodes; sticky actions give a 25% chance for actions to be repeated, and agents are forced to take random actions 1% of the time. While Octax appears to have some environments that have randomness, it's unclear whether brute-force approaches could work in some environments. \n- **JAX only** - While JAX has its advantages in speed, for a benchmark, I see it as a significant weakness if this benchmark is exclusive only to JAX users. A significant portion of RL researchers and users use other frameworks, such as PyTorch, which appear to be excluded from using this benchmark.\n- **Historic data** - One major advantage of Atari is that a huge number of algorithms have been evaluated, making it useful to compare against new algorithms, while Octax only has PPO. Please consider adding more algorithms such as DQN [3], SAC [4], PQN [5], and also more state-of-the-art algorithms would be appreciated.\n- **Game categorizations** - Currently, in Octax games are categorized into groups such as Puzzle, Action, etc. I think it would be more beneficial for the research community if environments were grouped by the aspects of the algorithm they challenge. In Atari, there are well known groups such as hard exploration (Montezuma’s Revenge), long term-credit assignment (Skiing) and many more. For a good example of this, please look at BSuite [6]. While Table 1 somewhat provides this, I still don’t think it is up to the standards of recent work. Furthermore, I’d appreciate a more detailed description of the tasks, including the frequency and magnitude of rewards. It is currently unclear if  Octax provides environments that are as challenging as Atari - for example, even after years of research, environments such as Pitfall and Montezuma’s Revenge are still extremely challenging.\n\n\n[1] Machado, Marlos C., et al. \"Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents.\" Journal of Artificial Intelligence Research 61 (2018): 523-562.\n\n[2] Agarwal, Rishabh, et al. \"Deep reinforcement learning at the edge of the statistical precipice.\" Advances in neural information processing systems 34 (2021): 29304-29320.\n\n[3] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n\n[4] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[5] Gallici, Matteo, et al. \"Simplifying Deep Temporal Difference Learning.\" The Thirteenth International Conference on Learning Representations.\n\n[6] Osband, Ian, et al. \"Behaviour Suite for Reinforcement Learning.\" International Conference on Learning Representations."}, "questions": {"value": "- Is this benchmark only usable to those who have algorithms written in JAX?\n- Does this benchmark have a set of rigorous set of specifications (number of frames, number of games, way to compare different overall performance) to ensure that researchers can easily compare their work?\n- How challenging are the existing environments in this benchmark? Can you benchmark some different algorithms and provide code for these in the repository?\n- Do all environments have a source of stochasticity? Or can they be solved by brute-force style algorithms?\n\nAlso, it appears Line 79 has a mistake. While I'm strongly in favor of benchmarks which make research easier, I feel this work still has a long way to go before it could replace something like Atari, thus I cannot yet recommend acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LcpS3rVCNE", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Reviewer_VWtY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Reviewer_VWtY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569876037, "cdate": 1761569876037, "tmdate": 1762941804068, "mdate": 1762941804068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Octax, a high-performance arcade-style benchmark for Reinforcement Learning in Jax. The paper utilizes end-to-end GPU training, allowing extremely high throughput. Primarily, this work looks to provide a viable alternative to the long-standing Atari benchmark, but using considerably less computational resources. Furthermore, the paper presents a way to quickly use LLMs to generate new environments, on top of the current set of games presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper aims to achieve the important goal of reducing the cost of running Reinforcement Learning experiments, clearly succeeding, achieving an impressive 1.4 million frames per second on consumer-grade hardware. The code already being open-source is also a nice positive. LLM-generated environments using the provided prompts provide a nice way for developers to test environments with specific properties if they wish."}, "weaknesses": {"value": "While I appreciate the attempt to reduce the computational burden of the Atari, I feel that the authors missed many key reasons why the Atari benchmark became so popular, and many of the features that still make it so popular today. In its current form, I don’t believe this work is capable of replacing the Atari benchmark without major reforms for a few key reasons:\n\n- **A clear, fixed way to evaluate** - One reason Atari is useful is that there is a relatively clear and fixed protocol for evaluation [1]. This means researchers know what environments they need to run, the settings they need to use, and know that their work will be comparable to others. Furthermore, there are precise guidelines on how scores should be reported [2]. The paper would benefit from being very clear about what environments are included in the benchmark, how many frames/timesteps algorithms should be trained for, and how results should be reported, with examples. Currently Figure 3 does not meet the same standards as many papers which use the Atari benchmark.\n- **Aggregate Scores** - Atari is useful as a benchmark, as the performance of an algorithm can be boiled down to a single number or single graph. In Atari, this has become human-normalized IQM performance with 95\\% confidence intervals, which provides a score for the given algorithm. This prevents games with different score magnitudes from dominating the final score. Currently, this appears to be missing, despite games having scores of different magnitudes.\n- **No-op Starts, Sticky Actions and Random Actions** - In the Atari benchmark, features are provided that prevent the agent from exploiting determinism (preventing brute-force approaches). Specifically, no-op starts randomly uses up to 30 no-op steps at the start of episodes; sticky actions give a 25% chance for actions to be repeated, and agents are forced to take random actions 1% of the time. While Octax appears to have some environments that have randomness, it's unclear whether brute-force approaches could work in some environments. \n- **JAX only** - While JAX has its advantages in speed, for a benchmark, I see it as a significant weakness if this benchmark is exclusive only to JAX users. A significant portion of RL researchers and users use other frameworks, such as PyTorch, which appear to be excluded from using this benchmark.\n- **Historic data** - One major advantage of Atari is that a huge number of algorithms have been evaluated, making it useful to compare against new algorithms, while Octax only has PPO. Please consider adding more algorithms such as DQN [3], SAC [4], PQN [5], and also more state-of-the-art algorithms would be appreciated.\n- **Game categorizations** - Currently, in Octax games are categorized into groups such as Puzzle, Action, etc. I think it would be more beneficial for the research community if environments were grouped by the aspects of the algorithm they challenge. In Atari, there are well known groups such as hard exploration (Montezuma’s Revenge), long term-credit assignment (Skiing) and many more. For a good example of this, please look at BSuite [6]. While Table 1 somewhat provides this, I still don’t think it is up to the standards of recent work. Furthermore, I’d appreciate a more detailed description of the tasks, including the frequency and magnitude of rewards. It is currently unclear if  Octax provides environments that are as challenging as Atari - for example, even after years of research, environments such as Pitfall and Montezuma’s Revenge are still extremely challenging.\n\n\n[1] Machado, Marlos C., et al. \"Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents.\" Journal of Artificial Intelligence Research 61 (2018): 523-562.\n\n[2] Agarwal, Rishabh, et al. \"Deep reinforcement learning at the edge of the statistical precipice.\" Advances in neural information processing systems 34 (2021): 29304-29320.\n\n[3] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n\n[4] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[5] Gallici, Matteo, et al. \"Simplifying Deep Temporal Difference Learning.\" The Thirteenth International Conference on Learning Representations.\n\n[6] Osband, Ian, et al. \"Behaviour Suite for Reinforcement Learning.\" International Conference on Learning Representations."}, "questions": {"value": "- Is this benchmark only usable to those who have algorithms written in JAX?\n- Does this benchmark have a set of rigorous set of specifications (number of frames, number of games, way to compare different overall performance) to ensure that researchers can easily compare their work?\n- How challenging are the existing environments in this benchmark? Can you benchmark some different algorithms and provide code for these in the repository?\n- Do all environments have a source of stochasticity? Or can they be solved by brute-force style algorithms?\n\nAlso, it appears Line 79 has a mistake. While I'm strongly in favor of benchmarks which make research easier, I feel this work still has a long way to go before it could replace something like Atari, thus I cannot yet recommend acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LcpS3rVCNE", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Reviewer_VWtY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Reviewer_VWtY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569876037, "cdate": 1761569876037, "tmdate": 1763639838888, "mdate": 1763639838888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents OCTAX, a JAX-native, vectorized CHIP-8 emulator and RL environment suite that runs many GPU-accelerated game instances. OCTAX exposes (21) CHIP-8 titles as Gym/Gymnax-compatible environments, includes wrappers to extract score and termination signals, reports high throughput (claims up to hundreds of thousands env steps/s / millions of frames/s), provides PPO training experiments across multiple games, and demonstrates an LLM-assisted pipeline to generate CHIP-8 games and corresponding reward/termination wrappers. An anonymized code repository is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a clear practical gap in the JAX ecosystem: image-based, GPU-native environments for RL. It thereby comes with significant engineering effort: a fully vectorized CHIP-8 emulator in JAX that can execute many parallel instances and integrates with standard RL training loops (Gym/Gymnax). Furthermore, this work directly empowers the community to work on more complex JAX-accelerated environments, which wasn't directly possible beyond Craftax before.\n\nIt introduces a broad catalog of 21 CHIP-8 games across multiple genres, useful for rapid prototyping and curriculum experiments. The reported throughput and scaling is promising, and if validated could substantially reduce wall-clock time for many experiments allowing for low resource algorithmic developments.\n\nFinally, the authors for provide evidence for a novel auxiliary idea: an LLM-assisted pipeline to generate lightweight CHIP-8 games and wrappers, enabling rapid environment prototyping. I find this especially exciting since it may enable open-ended training and novel autocurricula approaches."}, "weaknesses": {"value": "Emulation fidelity claims ('perfect fidelity') may be partially unsubstantiated: no instruction-trace equivalence, frame-by-frame comparison, or unit tests against a trusted CHIP-8 interpreter are reported. Most of the functional correctness assertions come from the successful training of agents.\n\nArguably, there is no really fair CPU baseline for CHIP-8 measured on the same machine, and comparisons to EnvPool/ALE conflate environment complexity differences. I understand that this is not trivial to accomplish, but I think it would make sense to maybe show runtimes of CPU/GPU-enabled environments just for contextualization. Even if they are not the same as the ones implemented in OCTAX.\n\nThe GPU memory/accounting claims (~2 MB per environment) lack a principled breakdown and appear implausible without explanation of XLA/JAX buffer and compiled executable overheads.\n\nThe LLM-assisted game generation is presented as a single case study with no statistics on compile/run success rates, human edit frequency, or failure modes.\n\nSome broader claims (energy savings, enabling small labs) are asserted without corresponding measured energy or cost data. This could and should be better substantiated.\n\nAll experiments only consider PPO as the single RL algorithm tested."}, "questions": {"value": "Re Emulator fidelity: Do you have systematic validation tests? E.g. for something like Pong, can you provide instruction-by-instruction equivalence tests, frame-by-frame rendering comparisons, and unit tests across representative ROMs against a trusted CHIP-8 interpreter.\n\nBaselines and fairness: If you retain EnvPool/ALE comparisons, justify differences in environment complexity or compare EnvPool on a matched low-resolution workload. Additionally, consider adding DQN-style results.\n\nProfiling and memory breakdown: Provide GPU profiler traces (kernel times, GPU utilization) and a detailed memory breakdown per environment (state arrays, framebuffers, intermediate buffers, compiled executable memory). Explain how you measured ~2 MB/env and the causes of scaling limits.\n\nPPO protocol: Were hyperparameters tuned per game or held fixed? Provide ablations showing sensitivity to frame-skip and observation stacking and report whether reported learning curves use per-game tuning.\n\nLLM pipeline evaluation: What fraction of LLM-generated ROMs compiled and ran without manual edits? How often did generated score_fn/terminated_fn require human correction? Provide statistics and typical failure cases and describe any automated validation used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CV7R75C55G", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Reviewer_iK9J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Reviewer_iK9J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996348099, "cdate": 1761996348099, "tmdate": 1762941803625, "mdate": 1762941803625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present a JAX implementation, known as OCTAX, of the CHIP-8 platform amenable to GPU acceleration. Octax converts CHIP-8 ROMs into RL environments that can take advantage of vectorized JAX functions. Octax provides a wide selection of games of interest to RL researchers spanning a number of different genres and difficulty levels. The performance of Octax is shown to be substantially higher than traditional methods for environment simulation using parallel instances of games with EnvPool. The authors note that some CHIP-8 ROMs had difficult reward functions to implement and understand, but this process was made easier with the help of LLMs to study the machine code. By reversing this process, they were able to train LLMs to produce new games that could be added to the RL training environment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The performance improvement of Octax over vectorized environments is impressive and will decrease the training time for many researchers using JAX to engage in RL experiments. With a shorter training time on simpler games, many new experiments to understand the training dynamics and tradeoffs between the large number of RL hyperparameters may be investigated more effectively.\n- On top of improved acceleration, the authors demonstrate the ability of LLMs to aid in the understanding of existing ROMs and the generation of new environments based on properly constructed prompts. I believe this may be a simplified and interesting sandbox to explore LLM coding in a way that is faster than traditional code generation procedures.\n- Providing this code as a testbed will facilitate the construction of more complex environments, such as Super-CHIP8, to add further complexity and interest."}, "weaknesses": {"value": "- The utility of arcade environments for SOTA RL has passed, so the real value is to use the environment to accelerate understanding of RL training dynamics and other metrics. Although the added challenge of generating new environments may mitigate this point.\n- Supporting yet another training environment may have a limited impact and lower the contributions.\n- There's no discussion regarding the success rate of generating ROMs using LLMs. I assume this is for the sake of space and to focus the discussion of the paper on the acceleration of arcade environments."}, "questions": {"value": "- How many attempts were required to generate Target Shooter using Claude? Was the success rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "osjVF486zo", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Reviewer_6XnA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Reviewer_6XnA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762205059212, "cdate": 1762205059212, "tmdate": 1762941803316, "mdate": 1762941803316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback, which has strengthened our manuscript. In response to your comments, we have made the following key revisions:\n\n- Added GPU profiler traces (NVIDIA Nsight Systems) and detailed memory analysis to the repository\n- Included PQN as an additional baseline in Figure 3 to complement our PPO results\n- Added a feasibility study (Appendix E) systematically evaluating LLM-based reward and termination extraction from CHIP-8 assembly\n- Added detailed environment specifications for all games, including reward ranges, action spaces, and task descriptions\n- Provided wrappers for no-op starts and sticky actions\n- Revised the introduction to better position Octax as an extensible toolkit for creating GPU-accelerated RL environments from CHIP-8 ROMs, rather than a fully standardized benchmark\n\nWe believe these revisions address the core concerns raised."}}, "id": "mkJqqqzjIu", "forum": "BdeUsYlyIf", "replyto": "BdeUsYlyIf", "signatures": ["ICLR.cc/2026/Conference/Submission21493/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21493/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21493/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763637302664, "cdate": 1763637302664, "tmdate": 1763637302664, "mdate": 1763637302664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}