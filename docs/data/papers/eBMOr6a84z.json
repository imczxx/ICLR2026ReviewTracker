{"id": "eBMOr6a84z", "number": 3519, "cdate": 1757457189034, "mdate": 1759898083498, "content": {"title": "TIPS: Turn-level Information-Potential Reward Shaping for Search-Augmented LLMs", "abstract": "Search-augmented large language models (LLMs) trained with reinforcement learning (RL) have achieved strong results on open-domain question answering (QA), but training still remains a significant challenge. The optimization is often unstable due to sparse rewards and difficult credit assignments across reasoning and tool calls. To address this, we introduce Turn-Level Information Potential Reward Shaping (TIPS), a simple framework that assigns dense, turn-level rewards to each reasoning + tool-call segment based on the increased likelihood of the correct answer under a teacher model. By leveraging the potential-based reward shaping, TIPS offers fine-grained and policy-invariant guidance that overcomes the limitations of outcome-only optimization. Evaluated on seven QA benchmarks, TIPS consistently outperforms GRPO/PPO baselines and substantially improves training stability. For instance, with a Qwen-2.5 7B Instruct model, TIPS improves the average Exact Match score by 11.8% and F1 by 13.6% relative to PPO. Our results demonstrate that turn-level information-potential reward shaping provides an effective and general solution to sparse-reward credit assignment for multi-turn LLM reasoning.", "tldr": "TIPS introduces turn-level reward shaping for search-augmented LLMs, yielding more stable training and higher QA performance than RLVR baselines.", "keywords": ["Agentic LLM", "Reinforcement Learning", "Question Answering"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/025641ef90d52b8afe0b18cb7edf2aeb84538f1a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TIPS, a framework designed to address the significant challenges of training search-augmented Large Language Models (LLMs) with reinforcement learning (RL). The authors identify that optimization is often unstable due to sparse rewards and difficult credit assignment across multiple reasoning steps and tool calls.TIPS mitigates this by assigning dense, turn-level rewards to each \"reasoning + tool-call segment\". This reward is calculated based on the increased likelihood of the correct answer, as computed by a teacher model, following the addition of that turn's information. By leveraging potential-based reward shaping (PBRS), the TIPS framework provides fine-grained guidance that is policy-invariant, overcoming the limitations of standard outcome-only optimization. The method was evaluated on seven open-domain question answering (QA) benchmarks. The results show that TIPS consistently outperforms PPO and GRPO baselines and substantially improves training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles one of the most significant obstacles in training LLM agents: the sparse-reward and credit-assignment problem. Providing a dense, principled reward signal for intermediate reasoning steps (i.e., tool calls) is a crucial area of research.\n\n2. The core contribution—defining the reward as the change in log-likelihood of the gold answer under a teacher model—is an elegant and well-motivated approach. It directly measures the \"information potential\"  of each tool-use turn.\n\n\n3. The method is not merely a heuristic. It is formally grounded in potential-based reward shaping (PBRS) , which guarantees that the optimal policy remains unchanged (policy invariance) under standard PBRS conditions.\n\n4. TIPS consistently outperforms all PPO and GRPO-based baselines across seven diverse QA benchmarks, using both 3B and 7B models. The performance gains are particularly pronounced on difficult multi-hop and out-of-domain (OOD) tasks, which is precisely where one would expect a better credit assignment method to excel."}, "weaknesses": {"value": "1. The primary weakness, acknowledged by the authors, is the computational cost.\n\n2. The method introduces a new critical hyperparameter, $\\alpha$ (the shaping scale). The ablation in Figure 6 shows that the method is highly sensitive to this value. \n\n3. The entire framework's success hinges on the availability and quality of a \"teacher model\". The paper shows that this teacher must be \"refreshed\" periodically to prevent the potential function from becoming stale, which itself causes instability. This introduces new complexities, such as determining the optimal refresh rate.\n\n4. As noted by the authors, the method is currently demonstrated only within the PPO framework. It is unclear how this segment-level PBRS approach would be integrated with other, potentially more sample-efficient, RL algorithms (e.g., off-policy methods)."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JiKjJmW5YE", "forum": "eBMOr6a84z", "replyto": "eBMOr6a84z", "signatures": ["ICLR.cc/2026/Conference/Submission3519/Reviewer_jPXU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3519/Reviewer_jPXU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557436059, "cdate": 1761557436059, "tmdate": 1762916779998, "mdate": 1762916779998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Turn-Level Information-Potential Reward Shaping (TIPS), a novel reinforcement learning framework designed to address the challenge of sparse rewards and difficult credit assignment in multi-turn, search-augmented LLMs. The proposed approach exploits the notion of \"information gain\" as an increase of the teacher model's log-likelihood of the correct answer. The gain in the form of log-likelihood ratio serves as Potential-Based Reward Shape (PBRS), which maintains policy invariance. The proposed approach follows a standard PPO training procedure. Experimental results on various QA tasks show that the proposed approach outperforms the comparison approach based on accuracy and training stability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tModeling the \"information gain\" as a turn-level reward is intuitive, well-designed. It effectively addresses the critical problem of sparse credit assignment in long-horizon reasoning tasks.\n2.\tThe authors provide detailed proof of policy invariance in Appendix C, which lends strong theoretical credentials to the statement that the approach used by them has not affected the original policy.\n3.\tExtensive experiments on several QA benchmarks demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1.\tLack of efficiency analysis. TIPS requires two forward passes of the teacher model per turn (to compute the likelihood before and after the observation) for every rollout. This introduces a non-trivial computational cost compared to outcome-only methods.\n2.\tHave you considered using a distilled, smaller model as the teacher to minimize the computational overhead?  Besides, could the \"information gain\" signal be approximated in a more computationally efficient manner?\n3.\tThe reward shaping heavily relies on a set of acceptable answers A to calculate the teacher's likelihood L(S;A). The performance might be sensitive to the completeness and quality of this answer set, especially for questions with many valid paraphrases. The robustness of TIPS on imperfect answer set hasn’t been investigated in the paper.\n4.\tThe teacher model is the key component of TIPS, but its selection and properties are not deeply explored. The paper uses the same LLM as the backbones of  both the policy model and teacher model. It is unclear how the method could perform with a teacher of different capabilities (e.g., larger, smaller, or from a different family) or if the policy and teacher drift too far apart even with periodic refreshes.\n5.\tThe authors only compare TIPS against naive RL baselines (PPO, GRPO) and their multi-turn extensions. Several key and highly relevant baselines [1-10] are missing from the main results table (Table 1), which makes it difficult to assess the true contribution of TIPS.\n\n[1] Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning.\n[2] ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning.\n[3] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs.\n[4] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL.\n[5] Stop summation: Min-form credit assignment is all process reward model needs for reasoning.\n[6] DeepRAG: Thinking to Retrieve Step by Step for Large Language Models.\n[7] Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl.\n[8] O-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering.\n[9] Search-o1: Agentic Search-Enhanced Large Reasoning Models.\n[10] R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\n[11] From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization.\n[12] Openprm: Building open-domain process-based reward models with preference trees.\n\n6.\tExperiments are restricted to the Qwen2.5 family. The performance and stability of TIPS haven’t been unverified on other model architectures (e.g., Llama, Deepseek) and a wider range of model scales, raising concerns about its general applicability.\n7.\tThe author claims that they used the same experiment setting as used in Search-R1. However, the reported results in Table 1 are inconsistent with those in the cited Search-R1 paper. \n8.\tMarginal Gains vs. High Cost: The performance gain is rather marginal, particularly in the case of the 3B model where it occasionally performs worse than baselines. With the added complexity and computational cost of the teacher model so high, the cost-benefit ratio does not seem favorable."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CdDdh4nPiQ", "forum": "eBMOr6a84z", "replyto": "eBMOr6a84z", "signatures": ["ICLR.cc/2026/Conference/Submission3519/Reviewer_ajMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3519/Reviewer_ajMR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749483698, "cdate": 1761749483698, "tmdate": 1762916779450, "mdate": 1762916779450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TIPS (Turn-Level Information-Potential Reward Shaping), a novel method to address the sparse reward and credit assignment problem in training multi-turn, search-augmented Large Language Models (LLMs). The core idea is to provide dense, turn-level rewards by measuring the increase in the likelihood of a correct answer, as evaluated by a \"teacher\" model, after each reasoning-and-retrieval turn. This information-gain signal is framed as a Potential-Based Reward Shape (PBRS), which theoretically preserves policy invariance. The authors integrate TIPS into the PPO framework and evaluate it on seven open-domain QA benchmarks. Results show that TIPS consistently outperforms strong baselines like PPO and GRPO, achieving significant improvements in Exact Match and F1 scores (e.g., +11.8% EM and +13.6% F1 for a 7B model) while demonstrating superior training stability and more effective credit assignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The core idea of using a teacher model's likelihood of the correct answer to compute information-gain rewards is highly original. It provides a principled and automated way to generate dense supervision, distinct from heuristic rules or learned reward models.\n\nQuality: The work is of very high quality. The combination of a solid theoretical grounding (PBRS) with extensive and carefully designed empirical validation is commendable. The ablations and analysis sections are particularly thorough.\n\nClarity: The paper is a model of clarity. The writing is precise, the concepts are well-motivated, and the flow from problem statement to solution to validation is logical and easy to follow.\n\nSignificance: This work addresses a fundamental bottleneck in RLHF/RLVR for complex LLM tasks. The demonstrated improvements in performance and, crucially, in training stability, represent a significant advance. The method has the potential to become a standard component in the toolkit for training reasoning agents."}, "weaknesses": {"value": "The weaknesses are minor and do not detract from the overall excellent contribution.\n\n1. Computational Overhead: While not explicitly quantified, using a teacher model (especially a 7B model) to compute log-likelihoods for every turn during training introduces non-trivial computational overhead compared to outcome-only rewards. A brief discussion of this cost (e.g., estimated % increase in training time or FLOPs) would be helpful for practitioners.\n\n2. Teacher-Student Capacity: The method assumes the teacher model is sufficiently powerful to make reliable judgments about information gain. The paper uses the agent's own initial checkpoint as the teacher. It is unclear how the method would perform if the agent significantly diverges from the teacher's knowledge distribution, or if a less capable teacher were used. An ablation with teachers of different capacities could further strengthen the work.\n\n3. Comparison to PRMs: The introduction contrasts TIPS with Process Reward Models (PRMs) by citing their need for \"high-quality supervisory labels.\" However, recent work has explored training PRMs on synthetic or LLM-generated step-level feedback. A more direct comparison or discussion on how TIPS's \"information gain\" reward compares in quality and efficiency to a learned PRM would be insightful.\n\nIt is worth noting that I am not an expert in this field, and I will adjust my score based on the scores of other reviewers."}, "questions": {"value": "1. Computational Cost: Could the authors provide an estimate of the computational overhead introduced by the teacher model's forward passes for calculating $I_k$? How does the wall-clock time or FLOPs for a TIPS training run compare to a standard PPO run?\n\n2. Teacher Model Selection: The paper uses the agent's initial checkpoint as the teacher. Did you experiment with using a larger, more powerful frozen model as the teacher? If so, what were the results? Conversely, what happens if the teacher is significantly weaker than the agent being trained?\n\n3. Theoretical Guarantees and Teacher Refresh: The policy invariance proof holds for a fixed potential function $\\Phi$. Your refresh@200 strategy, while empirically beneficial, technically breaks this strict invariance between refresh windows. Can you comment on the theoretical implications of this? Did you observe any instability or \"jumps\" in the return values immediately after a refresh, and how does this relate to the \"constant shift\" mentioned in the text?\n\n4. Failure Modes: The advantage distribution (Fig. 5) for TIPS is much cleaner than PPO's. However, are there specific types of tasks or reasoning steps where the information-gain reward might provide misleading guidance (e.g., when new evidence is subtly distracting but doesn't immediately lower the teacher's likelihood)? Can you provide an example of a failure case for TIPS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "I4l3TcOFDQ", "forum": "eBMOr6a84z", "replyto": "eBMOr6a84z", "signatures": ["ICLR.cc/2026/Conference/Submission3519/Reviewer_VPap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3519/Reviewer_VPap"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923691770, "cdate": 1761923691770, "tmdate": 1762916779231, "mdate": 1762916779231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TIPS (Turn-level Information-Potential Shaping), a reinforcement learning framework for optimizing search-augmented large language models in open-domain question answering. The authors identify that existing reinforcement learning methods such as PPO and GRPO depend heavily on sparse, outcome-only rewards that are only provided at the end of an episode. This design makes it difficult to assign credit accurately across multiple reasoning and retrieval steps, leading to unstable learning dynamics.\n\nTIPS addresses this limitation by introducing a turn-level dense reward mechanism. Instead of waiting for a final correctness signal, the framework measures how much each reasoning-and-retrieval step increases the teacher model’s confidence in the correct answer. This reward, based on the change in the teacher’s confidence, follows the Potential-Based Reward Shaping (PBRS) principle, which guarantees that the underlying task objective remains unchanged while providing richer learning feedback. The system therefore offers fine-grained credit assignment across reasoning turns without compromising theoretical soundness.\n\nThe authors evaluate TIPS on seven open-domain QA benchmarks (including NQ, TriviaQA, 2Wiki, MuSiQue, and HotpotQA) using Qwen-2.5 models of 3B and 7B parameters. Across tasks, TIPS consistently surpasses PPO and GRPO in both accuracy and stability, with particularly strong improvements on multi-hop reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Built on potential-based reward shaping, TIPS ensures that policy invariance is maintained while providing denser feedback signals, addressing a fundamental limitation in sparse-reward reinforcement learning for language models.\n- The paper presents a well-structured pipeline—multi-turn reasoning, retrieval, teacher evaluation, and potential-based shaping—supported by consistent mathematical logic and implementation clarity.\n- Evaluations across seven QA benchmarks and two model scales demonstrate reproducible and significant improvements in EM/F1 metrics, convergence speed, and training stability.\n- The inclusion of advantage-distribution analysis and ablation studies on shaping coefficients and teacher refresh strategies provides concrete evidence of behavioral differences and system robustness.\n- The framework is directly applicable to retrieval-augmented and reasoning-intensive LLMs, making it relevant for real-world agentic settings that require iterative tool use."}, "weaknesses": {"value": "- The reward signal is fully determined by the teacher model’s likelihood estimates. If the teacher is miscalibrated or biased, the shaping signal may misrepresent information gain. No calibration analysis or correction mechanism is discussed.\n- All experiments use the same teacher model (Qwen-2.5), differing only in whether it is fixed or periodically refreshed. The paper does not evaluate how the reward behaves with different teachers, leaving the robustness of TIPS to teacher variation untested.\n- The need to query the teacher model for every reasoning turn increases training cost compared to standard PPO/GRPO. The paper does not quantify the additional compute or discuss trade-offs between stability and efficiency.\n- All results are reported using EM/F1 metrics. There is no qualitative or human study to determine whether TIPS improves reasoning coherence, interpretability, or factual correctness in practice."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sw5H21H2ll", "forum": "eBMOr6a84z", "replyto": "eBMOr6a84z", "signatures": ["ICLR.cc/2026/Conference/Submission3519/Reviewer_vPWX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3519/Reviewer_vPWX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924557717, "cdate": 1761924557717, "tmdate": 1762916779012, "mdate": 1762916779012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Turn-Level Information Potential Reward Shaping (TIPS), a simple framework that assigns dense, turn-level rewards to each reasoning + tool-call segment based on the increased likelihood of the correct answer under a teacher model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this paper is sound: it aims to introduce denser reward signals to improve RL training.\n2. The experimental results show notable improvements."}, "weaknesses": {"value": "1. The writing quality needs improvement: Section 3 is hard to follow, and I could not find any mention of what exact model is as the teacher model (if I did not miss anything).\n\n2. The experimental setup seems outdated. Why not evaluate on GAIA or BrowseComp for search LLM? Likewise, why stick with the Qwen2.5 series, which is barely capable of search, instead of building upon the latest Qwen3 or other up-to-date models?\n\n3. The proposed method is heavily tied to a teacher model. A fairer baseline would be direct distillation from that same teacher."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khNAqnKkLY", "forum": "eBMOr6a84z", "replyto": "eBMOr6a84z", "signatures": ["ICLR.cc/2026/Conference/Submission3519/Reviewer_EXdd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3519/Reviewer_EXdd"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3519/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012681781, "cdate": 1762012681781, "tmdate": 1762916778580, "mdate": 1762916778580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}