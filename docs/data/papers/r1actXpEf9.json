{"id": "r1actXpEf9", "number": 22942, "cdate": 1758337315126, "mdate": 1759896839285, "content": {"title": "Probing the Limits of Embodied Spatial Planning in LLMs", "abstract": "Can the symbolic reasoning of Large Language Models (LLMs) extend to the physical world, or do they lack a fundamental \"mind's eye\" for grounded physical reasoning? This paper investigates this question by probing the ability of LLMs to reason about a dynamic physically-grounded environment. We introduce a novel methodology centered on indoor bouldering, a task that demands spatial imagination to (1) construct a mental environment from coordinates, (2) simulate an embodied agent's movement within that environment, and (3) adhere to physical constraints from the agent. Using our purpose-built dataset, EmbodiedPlan, which incorporates multiple agent profiles to test embodied reasoning, we challenge state-of-the-art LLMs (e.g., GPT-4o, Gemini Pro) to generate plans for different embodied agents. Our experiments reveal a consistent gap between syntactic fluency and physical plausibility: models can generate plans that are syntactically correct yet physically naive and poorly adapted to the agent's body. The results suggest that current LLMs possess a \"brittle\" mind's eye, capable of manipulating spatial symbols but lacking the grounded imagination required for true physical reasoning.", "tldr": "", "keywords": ["LLM-based planning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/acf11b530220a8728f98301c0109fc86df58ae26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EmbodiedPlan, a benchmark designed to evaluate the embodied spatial planning capabilities of Large Language Models (LLMs) in physically grounded tasks. Using indoor bouldering as a testbed, the authors assess LLMs’ ability to generate symbolic action plans that respect physical constraints and agent-specific embodiment. The benchmark includes multiple agent profiles and a suite of evaluation metrics, including syntactic validity, semantic alignment, and center-of-gravity (CoG) trajectory simulation. The empirical results reveal a significant gap between syntactic fluency and physical plausibility in current LLMs, highlighting limitations in spatial imagination and embodied reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-Motivated Problem**: The paper is well-motivated and addresses a critical gap in current LLM capabilities—namely, their lack of grounded physical reasoning. This is highly relevant for deploying AI agents in real-world environments.\n\n2. **Comprehensive Evaluation**: The benchmark includes a variety of metrics and agent profiles (e.g., height of the agents), revealing clear limitations in current LLMs' ability to adapt plans to physical constraints.\n\n3. **Clear Writing and Presentation**: The paper is well-organized and clearly written, with informative figures and tables that support the claims."}, "weaknesses": {"value": "1. **Oversimplified Physical Modeling**:\n\n    - The benchmark treats the human body as a point mass, ignoring rotational dynamics and gravity distribution. This simplification undermines the realism of the task and diverges from the paper’s stated motivation (physical-aware reasoning). \n    - For example, the rotational angle of the body significantly affects force distribution between limbs. Hands typically bear less weight than legs, and this biomechanical reality is not modeled.\n    - Consequently, the task reduces to a symbolic path-finding problem rather than a physically grounded planning challenge.\n\n2. **Evaluation Criteria Ambiguity**:\n    \n    - Precision and Recall: These metrics are typically suited for offline datasets. It’s unclear how they are computed in an online decision-making context.\n    - Center-of-Gravity (CoG): CoG is defined using only hand positions. This neglects the role of foot placement and body orientation, which are critical in climbing and embodied movement.\n    - Lack of Online Evaluation Metric: Existing metrics (e.g., LCS) measure similarity to ground truth but fail to capture online performance or feasibility. There is no mechanism to evaluate whether a generated plan is executable or stable in real-time. This problem should be considered since there could have multiple solutions for a single task.\n\n3. **Divergence from Motivation**:\n    \n    - The benchmark’s simplifications result in a mismatch between the paper’s motivation (testing embodied reasoning under physical contraints). This weakens the contribution and raises concerns about the benchmark’s validity."}, "questions": {"value": "**Questions**:  See Weakness\n\n**Suggestions**:  \nThe motivation behind this work is strong and timely. However, the current benchmark design oversimplifies the physical modeling, which limits its effectiveness. If this problem is addressed, this work has the potential to become a valuable and impactful benchmark for embodied reasoning in LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sc0mueab0X", "forum": "r1actXpEf9", "replyto": "r1actXpEf9", "signatures": ["ICLR.cc/2026/Conference/Submission22942/Reviewer_SaLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22942/Reviewer_SaLW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761010326897, "cdate": 1761010326897, "tmdate": 1762942447602, "mdate": 1762942447602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EmbodiedPlan is a new benchmark that tests LLMs on indoor-climbing plans. Given route coordinates and agents' heights, an LLM outputs a sequence of discrete actions (grip, match, move foot). Performance is scored on (1) symbolic correctness, (2) semantic alignment to expert plans, and (3) physical plausibility via a simplified center-of-gravity simulator. GPT-4o, Gemini Pro, etc. write valid-looking moves but often ignore real-world physics, especially for short climbers—showing current LLMs still lack true physical reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  An interesting real world task: indoor bouldering as a discrete symbolic planning task, extending evaluation of LLM beyond text-based games. \n2.  Evaluation from multiple views (symbolic, semantic, physical) that captures nuances of embodied reasoning.\n3.  Take personalization and embodiment as a condition, which is a rarely explored dimension."}, "weaknesses": {"value": "1.  Only 2-D MoonBoard walls are considered; no evidence the findings generalize to richer 3-D or non-climbing tasks.\n2.  Datasets are not very easy to get since it is constructed from real videos.\n3. Several SOTA LLMs such as  GPT-5 are missing in some experiments."}, "questions": {"value": "1.  What is the exact size and diversity (number of routes, difficulty grades, agent profiles) of EmbodiedPlan?\n2.  Can this benchmark extend to 3-D to capture more realistic scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jqfcMizq4z", "forum": "r1actXpEf9", "replyto": "r1actXpEf9", "signatures": ["ICLR.cc/2026/Conference/Submission22942/Reviewer_Bssf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22942/Reviewer_Bssf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814510722, "cdate": 1761814510722, "tmdate": 1762942447353, "mdate": 1762942447353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark to evaluate LLMs' ability to generate an actionable bouldering plan that respect geometric, physical and bodily constraints. Three variations of agent profiles are included to test personalized planning. However, there are serious flaws in the benchmark design and evaluation method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The choice of the bouldering planning task to benchmark LLMs' planning ability is indeed intriguing."}, "weaknesses": {"value": "1. While bouldering is a good sport that gains popularity in modern life, I am not convinced it is a good choice to test the planning abilities of LLMs. As an amateur who tried indoor bouldering a few times, the challenge of bouldering does not lie in route planning. Due to the large variety of possible routes and difference in climber's skill levels, there is no such a thing as \"optimal plan\" for bouldering. That is why in bouldering competition the points are gained as long as the top is reached, regardless of the plan taken. Therefore, I found it questionable to use Gemini-generated plans as ground-truth or any \"ground-truth\" plans to evaluate the embodied planning ability of other LLMs for this specific task. If only validity of plans is considered, the benchmark becomes trivial as even small open-sourced model such as Qwen3-4B can produce flawless plans.\n2. The metrics for LLM evaluations rely heavily on comparison with the ground-truth plans labelled by Gemini and reviewed by humans, which do not really reflect the spatial imagination and reasoning ability of LLMs. For example, in Table 1, Qwen3-4B has better LCS compared to GPT-4o-mini, which is far from my personal experience with these two models."}, "questions": {"value": "1. What are the results of the most powerful LLMs like GPT-4o, Gemini, Claude and Grok in the main benchmark as shown in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SbL4ua4DQb", "forum": "r1actXpEf9", "replyto": "r1actXpEf9", "signatures": ["ICLR.cc/2026/Conference/Submission22942/Reviewer_EHWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22942/Reviewer_EHWH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879438049, "cdate": 1761879438049, "tmdate": 1762942447152, "mdate": 1762942447152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting question: can the symbolic reasoning capabilities of Large Language Models (LLMs) be extended to the physical world? By introducing a novel indoor bouldering task and constructing a dedicated EmbodiedPlan dataset, this paper conducts multi-dimensional experimental evaluations. The results demonstrate that current SOTA LLMs lack grounded physical imagination."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe problem explored in this paper is interesting, and the task setup is easy to follow.\n2.\tThis paper defines agents with different heights and introduces gravity as a physical constraint to simulate a real-world physical environment.\n3.\tThis paper evaluates the performance of LLMs on this task across five different dimensions, providing a relatively comprehensive analysis."}, "weaknesses": {"value": "1.\tThe physical environment in this paper is highly simplified. For example, the Center-of-Gravity of the agent is approximated by averaging the coordinates of its two hands. Moreover, in real-world scenarios, considering only a 2D physical environment is clearly insufficient.\n2.\tThe definition of agent attributes is relatively limited, as it only considers height and gender, without taking into account other fine-grained characteristics. For example, arm span is a crucial physical attribute in bouldering, and different arm spans may lead to entirely different route-planning strategies. Therefore, future work could consider evaluating the spatial planning capabilities of LLMs using multi-dimensional physical characteristics\n3.\tAlthough this paper explores the problem in depth, it does not further investigate the underlying causes, nor does it discuss whether existing techniques can effectively address this limitation. Whether this issue can be alleviated by incorporating an external physics engine or by constructing similar embodied sequence data."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ofFrmicrtM", "forum": "r1actXpEf9", "replyto": "r1actXpEf9", "signatures": ["ICLR.cc/2026/Conference/Submission22942/Reviewer_BGpb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22942/Reviewer_BGpb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762442040058, "cdate": 1762442040058, "tmdate": 1762942446946, "mdate": 1762942446946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}