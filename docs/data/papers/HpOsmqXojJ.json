{"id": "HpOsmqXojJ", "number": 23525, "cdate": 1758344949404, "mdate": 1759896810213, "content": {"title": "Benchmarking Long-Term Memory with Continuous dialogue Lifelogs", "abstract": "Memory system in the real world holds considerable promise, especially in the potential continuous dialogue lifelogs scenarios, where wearable devices with microphone always-on can keep recording the surrounding dialogue. \nExisting benchmarks mostly focus on Person-AI interaction or Person-Person conversations, neglecting the continuous dialogue lifelogs scenarios, integrating multi-person interaction, causal and temporal event threads and so on.\nIn this paper, we propose two benchmark, named \\textbf{EgoMemBench} and \\textbf{LifeMemBench}, with a hierarchical life simulation framework.\nEgoMemBench is built in a bottom-up manner from a real-world lifelogging video dataset EgoLife over a seven-day period, while LifeMemBench is simulated by LLMs with a top-down elaboration to generate year-long personal lifelogs.\nBased on the hierarchical data with different temporal granularities, we design an automatic question-answering construction pipeline to generate four types with high-quality.\nRegarding the evaluation mode, employing both online and offline approaches--with the online mode prioritized, as it better aligns with the continuous dialogue lifelogs scenario.\nExperiments across four representative memory systems show that MemOS consistently outperforms others, achieving overall accuracies of 67.59\\% and 66.16\\% on the benchmarks. \nThis highlights the value of fine-grained memory management and the effectiveness of our benchmarks.\nMoreover, we show that event-level semantic segmentation of continuous dialogues yields superior results compared to naive chunking, pointing to more effective ways of structuring lifelog memories.\nIn conclusion, we define a continuous dialogue lifelogs scenario, positioning it as a potential cornerstone for next-generation terminal AI assistants.", "tldr": "We build two benchmarks for real world memory agent evaluation in lifelog scenarios", "keywords": ["memory agent", "lifelog", "memory dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95e7f49f795a9a77e481d3c276e8f785bb746a3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes two benchmarks, EgoMemBench and LifeMenBench, for evaluating long-term memory systems in continuous dialogue lifelog scenarios. It also introduces a QA construction pipeline with four question types, and a novel online evaluation mode that better reflects real-world memory usage. However, there are some concerns regarding data generation and the experimental interpretation, detailed below."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The focus on continuous dialogue lifelogs represents a promising and underexplored scenarios. The problem is clearly defined. \n2. The proposed “online” evaluation mode effectively mirrors real-world deployment, where queries are answered using current memories, while new information is continuously processed."}, "weaknesses": {"value": "1. The experiments mainly compare the performance on four memory systems,  but do the observed results reflect genuine challenges for memory systems, or do they arise from noise or ambiguity within benchmark itself?  A comparison with existing benchmarks would strength the claims. \n2. The authenticity and generalisability of the synthetic dataset are questionable, potential LLM-induced stereotypes or biases during data generation may compromise realism. \n3. It would be helpful to include sample examples of the four question types for better transparency.\n4. The experimental setup is limited to the Qwen3 model series. Validation on other mainstream LLMs (e.g., Claude, Llama3, and GPT-4) is missing, raising doubts about whether MemOS’s advantages is due to its architecture or its compatibility with Qwen3. \n5. Table 2 shows that Qwen-3-max-preview achieves nearly 100% accuracy, suggesting that benchmark questions themselves may not be difficult. The benchmark’s ability to distinguish between advanced memory systems with different reasoning capacities is therefore uncertain."}, "questions": {"value": "1. How are the noise and irregular data handles in the two datasets? Are the unstructured and incoherent characteristics of real-world lifelogs modeled to enhance robustness ?\n2. In Table 4, the performance of different segmentation settings are very small (less than 5% for MemOS-V).  Could you explain the reason behind this?\n3. Figure 5 shows that MemOS-G performs similarly under both stream and batch settings in EgoMenBench.  Why does this occur? \n4. In the “online evaluation mode”, how is the query stream generated? Are these queries predefined or dynamically produced during the simulation?\n5. How are queries temporally aligned in the streaming setting? What is the relationship between the query timestamp and the current memory time window?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1XEVHJFVDI", "forum": "HpOsmqXojJ", "replyto": "HpOsmqXojJ", "signatures": ["ICLR.cc/2026/Conference/Submission23525/Reviewer_W12f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23525/Reviewer_W12f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814415373, "cdate": 1761814415373, "tmdate": 1762942698852, "mdate": 1762942698852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose benchmarks for evaluating long-term memory capabilities in continuous dialogue lifelog settings. They motivate their work by pointing out the lack of benchmarks that effectively model ongoing, everyday (always-on) conversational experiences. Building on a hierarchical life simulation framework, the authors introduce two complementary benchmarks: (1) EgoMemBench, which is based on the real-world EgoLife dataset, (2) LifeMemBench, a synthetic dataset generated using LLMs. The two datasets are constructed using bottom-up and top-down approaches respectively, and include question–answer pairs designed to capture temporal changes in information over time. Using these benchmarks, the authors conduct experiments to evaluate the performance of several memory methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines a realistic continuous dialogue lifelog setting that reflects how people interact in daily life. Because the data include real timestamps and continuous updates, the benchmark helps evaluate how well a memory system can track and update information over time.\n2. The benchmarks use a hierarchical life simulation framework that organizes data by time. This design allows the study of how memory systems handle information at different time scales, from short-term to long-term.\n3. The authors test several well-known memory systems, such as RAG, A-Mem, Mem0, and MemOS, under both offline and online settings. These experiments show that the benchmarks are useful for comparing systems and reveal the challenges of keeping memories accurate and consistent in continuous dialogue situations."}, "weaknesses": {"value": "1. The experiments are conducted only with the Qwen models. To strengthen the main claims, it would be better to include additional baseline models. This would help determine whether the results and analyses generalize beyond the Qwen model.\n2. The paper provides no illustrative examples from the proposed datasets. Since the benchmark is a major contribution, including sample data or qualitative cases would increase clarity and help readers assess the reliability of the dataset.\n3. Compared with existing long-term memory benchmarks such as LongMemEval or LoCoMo, the proposed datasets cover a shorter temporal range. While the continuous dialogue lifelog setting is meaningful, extending the duration would make the benchmark more competitive and realistic.\n4. The evaluation relies solely on multiple choice questions. While this format easy to scoring, it may provide cues to the model and fail to reflect the open-ended reasoning expected from real-world memory systems. Adding an open-ended QA setting could provide a more complete and challenging evaluation.\n5. The experimental section feels somewhat narrow in scope. Including ablation studies or deeper analyses, for example, testing the influence of retrieval parameters, etc., would make the results more convincing and informative.\n6. Details about the annotators and the review guidelines used in the data review process (Section 3.2) are completely missing from the paper. The authors are strongly encouraged to clarify these aspects and fully address this concern during the rebuttal phase."}, "questions": {"value": "1. Although EgoMemBench is based on the EgoLife dataset, which contains multimodal data (video), the paper uses only text. Incorporating visual information could make the benchmark more comprehensive and novel? (just question)\n2. Could you provide some examples of the benchmark? It would be very helpful for me to assess the quality of the dataset.\n3. As there is no human evaluation of the benchmark, the paper would better from a detailed description of the human verification pipeline. Could you provide a specific, step-by-step account of this process? Providing more details about the annotators' background, evaluation criteria, and quality control procedure would improve transparency and help assess the reliability of the human review process.\n\nPlease check the weaknesses for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMEHkwEa75", "forum": "HpOsmqXojJ", "replyto": "HpOsmqXojJ", "signatures": ["ICLR.cc/2026/Conference/Submission23525/Reviewer_FJsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23525/Reviewer_FJsV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921251101, "cdate": 1761921251101, "tmdate": 1762942698526, "mdate": 1762942698526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of long-term memory for AI agents in continuous dialogue lifelogs, a scenario where an always-on wearable device records daily multi-person multimodal conversations. It introduces two new benchmarks: EgoMemBench and LifeMemBench. EgoMemBench is built from a real-world egocentric dataset, covering seven days of audio and video lifelogs, totaling 300 hours. LifeMemBench is a year-long simulated lifelog generated by LLMs. From these rich lifelogs, it generates closed-ended multiple-choice QAs, and open-ended QAs. It also proposes a novel online evaluation protocol, which assesses a system’s performance incrementally as new dialogues stream in. \nThe benchmarks are used to evaluate four representative long-term memory systems integrated with Qwen3-8B. It also extends the model size to 32B to investigate the stronger LLMs as the backbone of the memory system."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper identifies a critical unexplored scenario, continuous multi-person lifelogs, for long-term memory in LLMs. This setting is significant and innocuous, and the data sources from real-world for EgoMemBench, which demonstrates high quality.\n2.\tThis paper contributes two benchmarks. EgoMemBench grounds the evaluation in real-world data, while LifeMemBench provides scale and diverse data. The data generation pipeline is well-designed. After generating conversational data, the evaluation QAs also cover various aspects. The online evaluation mode is also insightful.\n3.\tThe evaluation on Qwen3-8B is comprehensive enough. It covers four memory systems and evaluates their performance thoroughly. It also investigates the stronger LLMs as the backbone of the memory system using the Qwen family.\n4.\tThe paper is generally well-organized and clear. The benchmark construction is clear."}, "weaknesses": {"value": "1.\tA concern is that LifeMemBench is entirely simulated. It is acknowledged that the simulation is well-designed, and the paper has said that human evaluation is in the loop. However, a clear discussion on how humans evaluate the simulation process with a clear evaluation metric is expected.\n2.\tAll evaluations use one LLM family (Qwen3). It is admitted that these LLMs are powerful; however,  to conduct experiments on one LLM as the main result is still not satisfactory. Moreover, recent studies have reported the performance gap between the thinking mode and non-thinking mode of the Qwen3-8B and suggest that this base model may not be stable enough. Studies on new models are necessary, and it is NOT a criticism of the base model setting as this paper is a work on benchmark; however, more experiments on the instruct models, for example, Qwen2.5-7B-instruct, may be expected."}, "questions": {"value": "1.\tIf time permits, more studies on the Instruct models can help readers better understand the benchmark.\n2.\tLifeMemBench is OK to be entirely simulated by LLMs. However, the human-in-the-loop review should be reported clearly. Also, if using real-world data as seed data and then following your simulation process, will this better reflect the real-world lifelogs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGzucUnGjs", "forum": "HpOsmqXojJ", "replyto": "HpOsmqXojJ", "signatures": ["ICLR.cc/2026/Conference/Submission23525/Reviewer_vWTR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23525/Reviewer_vWTR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977670779, "cdate": 1761977670779, "tmdate": 1762942698250, "mdate": 1762942698250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical and overlooked gap in the evaluation of memory systems for Large Language Models (LLMs): **continuous dialogue lifelogs**.\nTo fill this gap, the authors propose **two complementary benchmarks: EgoMemBench and LifeMemBench**.\n1.  **EgoMemBench** is built \"bottom-up\" from a real-world, seven-day first-person video dataset (EgoLife), generating plausible dialogues from video summaries.\n2.  **LifeMemBench** is built \"top-down\" using LLMs to simulate a year-long personal lifelog, enabling a much longer temporal span and richer simulated social interactions.\n\nBoth benchmarks utilize a **hierarchical data structure** (e.g., from seconds-to-days or years-to-days) and feature an automatic QA construction pipeline. This pipeline generates four types of multiple-choice questions (e.g., event detail, multi-hop reasoning, temporal info).\n\nA key contribution is the introduction of an **\"online\" evaluation mode**, which more realistically simulates the real-world scenario of streaming data and on-the-fly querying, contrasting it with the traditional \"offline\" mode.\n\nExperiments evaluate four representative memory systems (RAG, A-Mem, Mem0, MemOS):\n* The **MemOS** system (particularly the vector-based MemOS-V) consistently achieves the best performance.\n* A surprising and crucial finding is that **memory systems based on aggressive summarization (A-Mem and Mem0) perform worse than a simple RAG baseline**，because of summarization discarding critical details (like timestamps) necessary for answering lifelog questions.\n* **Temporal Information QA (QT4)** is shown to be the most challenging task for all systems.\n* **Event-level semantic segmentation** is superior to naive fixed-length chunking strategies.\n\nOverall, this paper defines an important new direction and evaluation suite for memory systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Complementary Benchmark Design:** The two-benchmark approach (EgoMemBench and LifeMemBench) is very strong. EgoMemBench provides real-world grounding, while LifeMemBench provides the scale and controlled complexity (via simulation) needed to evaluate long-horizon (1-year) dependencies.\n2.  **Innovative Evaluation Methodology:** The proposed \"online\" evaluation mode is a significant contribution. It better reflects the dynamic nature of the lifelog scenario (continuous data ingest, queries at any time) than the conventional \"store-then-evaluate\" offline mode.\n3.  **Insightful Experimental Findings:** The results are clear and illuminating. The finding that \"**summarization is harmful**\" (i.e., A-Mem and Mem0 being worse than RAG) makes a powerful statement about *what* should be remembered. It highlights the critical importance of **preserving raw textual evidence** (especially timestamps and details) in the lifelog scenario.\n4.  **Focus on Data Quality:** The authors detail their data curation and QA generation pipeline, including the use of LLM-assistance and human-in-the-loop review, as well as dedicated steps to filter information leakage and check for answerability. This inspires confidence in the quality of the benchmarks."}, "weaknesses": {"value": "1.  **Synthetic Nature of LifeMemBench:** As a year-long simulated dataset, LifeMemBench is entirely LLM-generated. Despite the hierarchical design, this synthetic data may lack the organic complexity, irrationality, and \"messiness\" of a real human life. It risks a \"model-in-the-loop\" bias, where the benchmark primarily tests a model's ability to retrieve information generated by a similar model, rather than genuine human memory patterns.\n2.  **Indirectness of EgoMemBench:** The dialogue data in EgoMemBench is not derived from *direct* audio transcription from EgoLife. Instead, it is *generated* from **10-minute summaries** from Ego-R1. This is a \"summary-of-a-summary\" approach. While the authors claim this preserves \"semantic fidelity\", this layer of indirection may introduce artifacts and fails to leverage the actual dialogues that may have been present in the original data.\n3.  **Evaluation Format (Multiple-Choice):** The authors chose multiple-choice questions (MCQ) for \"stability and usability\". While practical for automated evaluation, the authors themselves concede that open-ended QA is more realistic. The MCQ format may not fully test the reasoning and generation capabilities of the systems, which might \"cheat\" by matching keywords from the options rather than truly \"understanding\" the memory.\n4.  **\"Online\" Mode Finding:** The finding that performance is generally *higher* in the online mode is interesting, but the explanation (smaller, less noisy memory pool at each step) may also suggest that the query difficulty is not sufficiently scaled to the size of the memory bank at that point. Does the difficulty truly increase dynamically, or are the systems just getting better at retrieving (more relevant) recent memories?"}, "questions": {"value": "1.  **Re: EgoMemBench:** Could you elaborate on the decision to *generate* dialogue from Ego-R1 summaries instead of using *actual* transcribed dialogues from the EgoLife audio? Was this due to a lack of dialogue in the original audio, poor ASR (automatic speech recognition) quality, privacy concerns, or other considerations? This indirection seems to be a key design choice.\n2.  **Re: LifeMemBench:** Thank you for the detailed \"top-down\" simulation process. How do you reason about the \"ecological validity\" of a year-long lifelog generated entirely by an LLM? Were measures taken to prevent the simulation from falling into an LLM's \"comfort zone\" (e.g., avoid generating overly repetitive or overly \"logical\" life events) and to ensure it is truly challenging?\n3.  **Re: Evaluation Format:** Did you experiment with an open-ended \"LLM-as-Gudge\" version of the QA? I am curious: Mem0 and A-Mem failed on the detail-oriented MCQs, but would they perform better if the questions were higher-level, open-ended \"summary\" reasoning tasks (e.g., \"Summarize my work priorities from last week\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "diELQiKoHJ", "forum": "HpOsmqXojJ", "replyto": "HpOsmqXojJ", "signatures": ["ICLR.cc/2026/Conference/Submission23525/Reviewer_QMmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23525/Reviewer_QMmg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180294669, "cdate": 1762180294669, "tmdate": 1762942697992, "mdate": 1762942697992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}