{"id": "lQxSGO2kBa", "number": 12293, "cdate": 1758206886114, "mdate": 1763717782448, "content": {"title": "Learnable Kernel Density Estimation for Graphs", "abstract": "This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and generalization. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.", "tldr": "We propose a theoretically grounded Learnable Kernel Density Estimation framework (LGKDE) under deep graph MMD-based metric space that shows superior performance on graph-level anomaly detection.", "keywords": ["Graph Anomaly detection", "Graph Neural Networks", "Density Estimation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f37d087a98cc09775b5cf4d7730f93e47dc7fc7.pdf", "supplementary_material": "/attachment/7edbc01720268d66985784e4e2ace1ab1c5e50a3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LGKDE, a learnable kernel density estimation framework for graphs that integrates GNN-based representations, a deep MMD metric, and a multi-scale KDE with learnable mixture weights. The method contrasts normal graphs with their structure-aware perturbed counterparts to learn a density function capable of modeling complex graph distributions. The authors provide theoretical guarantees on consistency, convergence, and robustness, and demonstrate empirical superiority on twelve benchmark graph anomaly detection datasets. The study aims to unify deep representation learning and nonparametric density estimation within a principled framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem setting and motivation. The paper tackles a fundamental yet underexplored problem of graph density estimation, framing it in a principled manner that naturally connects to graph-level anomaly detection.\n\n2. Integration of theoretical and empirical perspectives. The authors not only design a learnable KDE model but also provide non-trivial theoretical analyses (consistency, convergence rate, and robustness bounds), lending rigor to the framework.\n\n\n3. Conceptually coherent design. The use of a deep MMD metric space and structure-aware perturbations offers a reasonable and interpretable approach to contrastive density learning, aligning well with the paper’s stated objectives."}, "weaknesses": {"value": "1. Motivation and Empirical Support\n\nThe paper claims that existing graph learning methods trained with standard supervised objectives tend to capture spurious signals, making them fragile under distributional shifts. However, this statement lacks systematic evidence or quantitative validation. The only supporting illustration is a qualitative t-SNE visualization, which is insufficient to substantiate the general claim. A dedicated experiment or section demonstrating the degradation of standard methods under controlled distributional shifts or noise perturbations would make the motivation more convincing.\n\n2. Methodological Assumptions and Theoretical Validity\n\n   The theoretical derivation of LGKDE relies on a strong assumption that the intrinsic dimensionality ( d_{\\text{int}} = 1 ), simplifying the density estimation formulation and constants. Furthermore, the convergence and robustness proofs implicitly assume that the graph space admits a smooth Riemannian structure that allows Taylor expansion, which may not hold for heterogeneous or discrete graph distributions. These assumptions weaken the generality of the claimed theoretical guarantees.\n\n3. Incomplete Baseline Coverage\n\n   Although the paper compares LGKDE with various graph anomaly detection (GAD) models and a two-stage GAE+KDE pipeline, it omits several directly related density-based approaches. In particular, there is no comparison with Sun & Fan (2024), which also employs an MMD-based metric learning framework, or with graph normalizing flow and energy-based models that perform explicit density estimation. Including these baselines would clarify whether LGKDE provides genuine methodological advancement beyond existing learnable density estimators.\n\n4. Insufficient Ablation on Perturbation Module\n\n   The proposed structure-aware perturbation mechanism is central to the model’s objective, yet its role is not rigorously dissected. The ablation studies only vary the number of KDE bandwidths and mixture weights, but do not test variants using (a) feature-swap only, (b) spectral perturbation only, or (c) random edge perturbation. Since the perturbation strategy directly drives the density contrastive learning, such ablations are crucial to justify its necessity and effectiveness."}, "questions": {"value": "1. Could authors provide quantitative experiments showing how existing baselines (e.g., OCGIN, SIGNET) fail under controlled distribution shifts, to empirically support your motivation?\n2. How realistic is the assumption ( d_{\\text{int}} = 1 )? Have authors tested whether relaxing this assumption (e.g., learned or dataset-dependent intrinsic dimension) affects performance or theoretical consistency?\n3. Why were Sun & Fan (2024) or other explicit density estimation methods (graph flows, EBMs) excluded from the comparison? Are there implementation or conceptual reasons?\n4. Can authors report additional ablations on the perturbation module—particularly feature-only, spectral-only, and random perturbations—to show which component contributes most to the performance gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MAg5V9MIdw", "forum": "lQxSGO2kBa", "replyto": "lQxSGO2kBa", "signatures": ["ICLR.cc/2026/Conference/Submission12293/Reviewer_xSCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12293/Reviewer_xSCo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888471645, "cdate": 1761888471645, "tmdate": 1762923224880, "mdate": 1762923224880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers"}, "comment": {"value": "**We are deeply grateful to all reviewers for your invaluable time, meticulous attention, and the exceptionally constructive feedback you have provided.** The recognition of our theoretical rigor, the breadth of our experiments, and the significance of the problem we tackle is both humbling and profoundly encouraging.\n\nIn addition to our **detailed point-by-point responses**, we **provide the general response here to summarize the rebuttal**.  We acknowledge that our original submission included extensive supplementary materials (a 28-page Appendix), which may have made certain pre-existing theoretical analysis and empirical results difficult to locate quickly and potentially easy to overlook. Therefore, in addition to conducting further experiments and providing clarifications in the rebuttal, we have revised the manuscript accordingly. **All revised and updated content is highlighted in blue in the updated PDF.**\n\n---\n\n| **Question&Concerns** | **Key Evidence in Original Submission** | **Rebuttal Action & Revisions** |\n|:---|:---|:---|\n| **Novelty** (3Fbp, MSxu, xSCo) | (1) **First end-to-end learnable graph density estimation framework**. (2) **Novel energy-based spectra perturbation** (Eq 3-4) is not random augmentation, **combined with density-based loss** (3) **Complete theoritical framwork** prove consistency, convergence, robustness, and generalization bound (4) **Extensive empirical validation** and achieve **superior performance** on density estimation and graph anomaly detection tasks  | **Apart from detailed response, we enhanced related work discussion in Appendix C.2**: Explicit comparison with Normalizing Flows, EBMs, VAEs showing LGKDE's advantages (variable graph sizes, no MCMC, exact guarantees); **Clarification our design choice with existed and extend experiment results**; **Moving generalization bound (Theorem 4.6) and its analysis from Appendix F.5 to Sec 4**. |\n| **Scalability** (Y81R, MSxu) | (1) **Theoretical complexity analysis** (Sec 3.4, Appendix E.4.1, Table 6), Memory $O(NLnd)$ matches standard GNNs; (2) **Empirical running time comparison** (Appendix E.4.2, Table 12): Complete runtime on COLLAB (5,000 graphs), LGKDE 1.5× faster than SIGNET, competitive with advanced baselines; (3) **Scalability** (Sec 3.4, Appendix E.4.3, Table 13): Sampling strategies achieve speedup with negligible performance loss | we **give further detailed respose and clarification**, improved appendix navigation. |\n| **Bandwidth** (3Fbp, MSxu) | **Ablation study has covered these points** (Sec 5.3, Appendix B.4, Figure 6, Table 6) | (1) **Clarified learnable mixture weights approach** (no manual tuning). (2) **Provided theoretical and practical justification** (3) Add revision in Sec 5.3. |\n| **Perturbation Module Ablation** (3Fbp, xSCo) | **Appendix D.2**, Figures 10-12: case studies on different $(\\tau_1, \\tau_2)$ combinations with edge change ratio distribution; **Density-contrast motivation (Sec 3.3)**: Soft density targets avoid hard anomaly labels, naturally paired with structure-aware perturbations | We have **conducted addtional experiments, (1) Appendix B.4**, Table 7: **Systematic comparison of feature-only, spectral-only, random edge, and combined perturbations; (2) Appendix B.4, D.2** Table 8 & 15: **Comprehensive $\\tau_1, \\tau_2$ grid-search with performance and edge modification statistics report** (3) Add revision in Sec 5.3.|\n| **$d_{int}$ analysis** (xSCo, MSxu) | (1) **Appendix F.3 has discussed it** and it's a design choice for density estimation, not a graph space restriction.| We **provide further detailed responses and clarifications, and improved appendix navigation and content**. |\n| **Distribution Shift Validation** (xSCo) | (1) **Existing robustness and training contamination study in Appendix B.1, B5.4.** (2) **Density recovery on synthetic graphs in Section 5.1 and Appendix B5.1-5.2.** | We have **conducted additional experiments, Add Appendix B.6**, Table 14, **further validate it**; we also add corresponding revision in Sec 5.3. |\n| **LGKDE design validation** (3Fbp) | **Ablation study has covered these points (Sec 5.3, Appendix B.4, Figure 6, Table 6)** | We have **conducted additional experiments Appendix B.4**, Table 9: Parameter-controlled experiments **isolating KDE contribution from increased GNN backbone params**; we also add corresponding revision in Sec 5.3. |\n\n---\n**Thanks again, hope our comprehensive rebuttal response addresses all your questions and concerns, and we look forward to your kind and insightful feedback.**"}}, "id": "oBYs4qmY7w", "forum": "lQxSGO2kBa", "replyto": "lQxSGO2kBa", "signatures": ["ICLR.cc/2026/Conference/Submission12293/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12293/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12293/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763702531899, "cdate": 1763702531899, "tmdate": 1763702531899, "mdate": 1763702531899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LGKDE, a graph-level density estimation framework that learns graph embeddings with a GNN, defines a multi-scale KDE in an MMD-based metric space, and trains by contrasting each graph’s density against carefully designed structure- and spectrum-aware perturbations. The authors claim consistency, convergence, robustness, and generalization guarantees, and evaluate LGKDE mainly on graph anomaly detection benchmarks, reporting improvements over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formulates graph-level density estimation through a theoretically grounded framework that integrates graph neural network embeddings with learnable multi-scale kernel density estimation in an MMD-based space.\n2. The authors provide L1-consistency and convergence rate results (Theorems 4.1 and 4.2), establishing statistical soundness and connecting the method to nonparametric theory under intrinsic dimension assumptions.\n3. The framework includes structure- and spectrum-aware perturbations for density contrast, a clear complexity analysis with sub-sampling for scalability, and empirical validation on graph anomaly detection benchmarks showing steady gains over prior methods."}, "weaknesses": {"value": "1. The distinction between LGKDE and deep density estimation methods is not sharply articulated, leaving unclear where LGKDE provides a fundamental advantage.\n2. It is not clear how the learned MMD metric is constrained to prevent overfitting of the density landscape (e.g., through regularization of kernel parameters or Rademacher-style control), and how this constraint is reflected in the stated generalization bound.\n3. More remarks or further insights are needed to help the readers better understand the theorems."}, "questions": {"value": "1. What practical procedure do you recommend for bandwidth selection at multiple scales, and how sensitive are results to bandwidth mis-specification?\n2. Can you quantify how spectral perturbations change graphs in spectral energy vs. topological terms, and correlate that with performance gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1pUZsjEBkm", "forum": "lQxSGO2kBa", "replyto": "lQxSGO2kBa", "signatures": ["ICLR.cc/2026/Conference/Submission12293/Reviewer_MSxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12293/Reviewer_MSxu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895000813, "cdate": 1761895000813, "tmdate": 1762923224277, "mdate": 1762923224277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LGKDE, a principled framework for graph-level density estimation, aiming to unify deep graph representation learning with adaptive kernel density estimation. LGKDE represents each graph as a distribution over learned node embeddings and measures pairwise similarity via a deep Maximum Mean Discrepancy (MMD) metric. Density is then estimated with a multi-scale kernel density estimator whose weights are learned. A novel density contrasting loss maximizes densities of normal graphs relative to structure-aware perturbed counterparts, with perturbations applied to both node features and graph spectra."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, with a clear problem statement and a logically organized presentation.\n2. The paper focuses on a relatively unexplored but important subarea of nonparametric graph density estimation, which directly underpins graph-level anomaly detection.\n3. Synthetic validations recover known distributions, and broad benchmarks for anomaly detection show consistent AUROC, AUPRC, and FPR95 improvements over competitive baselines."}, "weaknesses": {"value": "1. While complexity analysis is given, the framework requires pairwise deep MMD computation for KDE and generation of multiple perturbed samples, which might be costly for very large datasets. Empirical runtime/memory profiles for large-scale sparse graphs are lacking.\n2. Perturbed samples are not true anomalies. The performance gain depends on the quality of the perturbations, and it is unclear how LGKDE would perform when the perturbations poorly reflect anomalous structures."}, "questions": {"value": "1. How does LGKDE scale in practice when N is very large or graphs have high average degree? Could you provide empirical runtime and memory usage breakdowns for large-scale settings?\n2. Why were $\\tau_{1}$ and $\\tau_{2}$ fixed at 0.5 and 0.75, respectively? Did you explore alternative threshold values, and how sensitive are the results to changes in $\\tau_{1}$​ and $\\tau_{2}$​?\n3. The deep MMD metric involves taking the supremum over $\\Gamma_{\\mathrm{emb}}$. For large $S = \\|\\Gamma_{\\mathrm{emb}}\\|$, this can be computationally expensive. Do you have an efficient approximation or kernel selection strategy to handle large $\\Gamma_{\\mathrm{emb}}$ without significantly increasing runtime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BWfedUxaOT", "forum": "lQxSGO2kBa", "replyto": "lQxSGO2kBa", "signatures": ["ICLR.cc/2026/Conference/Submission12293/Reviewer_Y81R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12293/Reviewer_Y81R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901431889, "cdate": 1761901431889, "tmdate": 1762923223903, "mdate": 1762923223903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a learnable kernel density estimation framework for graphs that integrates GNNs with maximum mean discrepancy based kernel learning. The method jointly learns graph representations and adaptive kernel bandwidths by contrasting densities of normal graphs with perturbed counterparts, where perturbations are applied to both node features and graph spectra. The paper provides theoretical guarantees for consistency, convergence, robustness, and generalization, and reports superior performance in graph-level anomaly detection across multiple benchmark datasets compared with existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and underexplored problem which is relevant to applications such as anomaly detection and molecular graph analysis. The proposed approach is described clearly and supported by theoretical derivations. The experimental section is comprehensive, covering a wide range of datasets and showing that LGKDE consistently improves upon baseline methods. The presentation is coherent, and the proposed framework bridges traditional kernel-based methods and modern deep learning–based graph modeling."}, "weaknesses": {"value": "The novelty of the contribution is limited, as the method mainly combines known components (GNN embeddings, MMD distances, and KDE) rather than introducing a fundamentally new concept. The proposed perturbation strategy and contrastive density objective are incremental variations on existing ideas in self-supervised learning and graph anomaly detection. The related work discussion is incomplete; several recent graph density and contrastive learning approaches are not adequately compared or discussed. The experimental comparison omits some strong baselines such as flow-based and diffusion-based graph generative models. Moreover, the analysis of the results is largely descriptive and does not provide convincing evidence explaining why the proposed model outperforms others beyond parameter tuning."}, "questions": {"value": "How sensitive is LGKDE to the choice of bandwidth scales and perturbation strength? Can the authors provide ablation results showing whether the observed gains come from the learnable KDE component or simply from additional parameters in the GNN backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mbnI3AKrTx", "forum": "lQxSGO2kBa", "replyto": "lQxSGO2kBa", "signatures": ["ICLR.cc/2026/Conference/Submission12293/Reviewer_3Fbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12293/Reviewer_3Fbp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012654008, "cdate": 1762012654008, "tmdate": 1762923223515, "mdate": 1762923223515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}