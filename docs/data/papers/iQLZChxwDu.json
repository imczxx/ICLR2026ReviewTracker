{"id": "iQLZChxwDu", "number": 5668, "cdate": 1757926550109, "mdate": 1759897962040, "content": {"title": "Probing CLIP's Comprehension of 360-Degree Textual and Visual Semantics", "abstract": "The dream of instantly creating rich 360-degree panoramic worlds from text is rapidly becoming a reality, yet a crucial gap exists in our ability to reliably evaluate their semantic alignment. Contrastive Language-Image Pre-training (CLIP) models, standard AI evaluators, predominantly trained on perspective image-text pairs, face an open question regarding their understanding of the unique characteristics of 360-degree panoramic image-text pairs. This paper addresses this gap by first introducing two concepts: \\emph{360-degree textual semantics}, semantic information conveyed by explicit format identifiers, and \\emph{360-degree visual semantics}, invariant semantics under horizontal circular shifts. To probe CLIP's comprehension of these semantics, we then propose novel evaluation methodologies using keyword manipulation and horizontal circular shifts of varying magnitudes. Rigorous statistical analyses across popular CLIP configurations reveal that: (1) CLIP models effectively leverage explicit textual identifiers, demonstrating an understanding of 360-degree textual semantics; and (2) CLIP models fail to robustly preserve semantic alignment under horizontal circular shifts, indicating limited comprehension of 360-degree visual semantics. To address this limitation, we propose a LoRA-based fine-tuning framework that explicitly instills invariance to circular shifts. Our fine-tuned models exhibit improved comprehension of 360-degree visual semantics, though with a slight degradation in original semantic evaluation performance, highlighting a fundamental trade-off in adapting CLIP to 360-degree panoramic images.", "tldr": "", "keywords": ["CLIP", "360-degree panorama", "360-degree textual semantics", "360-degree visual semantics", "evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74a3168f111e1b0647e2bd53136b44d7a4695ec5.pdf", "supplementary_material": "/attachment/74512135aee78acaad4c15fa3d4286070a66bf40.zip"}, "replies": [{"content": {"summary": {"value": "The paper's title is \"probing CLIP's comprehension of 350-degree textual and visual semantics\". The paper presents several themes, including 360-dgeree panoramic image generation (040), accurate evaluation of semantic alignment between generated 360-degree panoramic images (Line 050), and CLIP models' comprehension ability in 360-degree panoramic image-text pairs (Line 072). These themes make the paper less focused. Nevertheless, the paper studies CLIP models in term of their ability to understand panoramic images. It adopts various statistical tools to investigate this matter. It horizontally shifts panoramic images and measures how consistent CLIP models output scores. It concludes that CLIP models can understand keywords such as \"a 360 degree view of\" and \"panorama\", but does not process robustness in producing consistent scores on horizontally shifted panoramic images. It adopts LoRA to adapt the visual encoder, enhancing this robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Below are notable strengths of this paper.\n\n- The variety of statistical tools are interesting.\n- Technical design choices are generally well thought-out, e.g., the null hypotheses, dataset construction, etc.\n- Using LoRA to empower CLIP models' robustness to horizontal-shift panorama is interesting."}, "weaknesses": {"value": "Below are notable weaknesses of this paper.\n\n- The focus of this paper is not clear. Line 011 and the first paragraph of Introduction leave an impression that this paper focuses on generating 360-degree panoramic images. However, Line 049 motivates evaluating semantic alignments between generated 360-degree panoramic images, leaving an impression that the work focuses on developing methods to evaluate generative models' performance in terms of semantic alignments of their generated images. Quite confusingly, Line 051 and Line 072 state that the the work particularly focuses on CLIP models and studies their understanding ability of panoramic images.\n\n- As the paper is motivated for accurate evaluation of semantic alignment between generated 360-degree panoramic images, it is not convincing to only focus on CLIP (Line 073). Given the existing of powerful Multimodal Large Language Models (MLLMs), it is natural to ask whether these MLLMs can serve as the evaluator. \n\n- Line 064 states \"raises questions about their applicability to evaluating 360-degree panoramic image-text pairs, which present fundamentally different characteristics\". Did the authors justify that 360-degree panoramic image-text pairs \"present fundamentally different characteristics\" from CLIP's pretraining data? Do the CLIP's pretraining data contain panoramic images? This is an important point, as the analysis of CLIP's understanding ability on panoramic images can be greatly determined by whether the CLIP's pretraining dataset contains panoramic images and the amounts / portion of such images\n\n- Line 035 and 068 mention \"360x180\". What does 180 mean?\n\n- In Equation (1), it is unclear why multiplying the constant 100. It is unclear either why using a max operation. The paper does not explain these\n\n- Table 1 presents some statistical testing results but does not explain what data are used in the test. Specifically, it states that it uses \"two paired image-text datasets (360_real and 360_syn) but does not explain how they are constructed and what data they contain. (Okay, the paper talks about them in later text, Section 4.1. But the current presentation causes confusions, i.e., using the acronyms without defining them.)\n\n- It is questionable whether the current design of beta (Line 240) is a good choice. It is based on the difference of CLIP scores between images and their left-right flips. To align with horizontal shifts in the study of 360-degree visual semantic, isn't it a better choice to properly horizontally shift images to derive beta?\n\n- The paper uses LoRA to adapt CLIP models for gaining a robust understanding of understanding of 360-degree visual semantics. But the paper does not explore how it helps evaluate generated panoramic images, which is the theme of the paper (Line 049)."}, "questions": {"value": "The reviewer asks the authors to address each point in weaknesses listed above and does not repeat them in this Questions box."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2kAehkkxU4", "forum": "iQLZChxwDu", "replyto": "iQLZChxwDu", "signatures": ["ICLR.cc/2026/Conference/Submission5668/Reviewer_3QfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5668/Reviewer_3QfH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537931522, "cdate": 1761537931522, "tmdate": 1762918184309, "mdate": 1762918184309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates the semantic understanding capability of the pre-trained CLIP model regarding 360° panoramic images and related textual descriptions, particularly its limitations in circular/360-degree visual semantics. The authors find that horizontal circular shifts of panoramic images cause significant fluctuations in the CLIP score, indicating the model's lack of intrinsic understanding of the geometric property of such images. To address this issue, the authors propose a lightweight fine-tuning framework based on Low-Rank Adaptation (LoRA), designed to enhance the CLIP image encoder's perception of 360° visual semantics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well-defined and the motivation is clear: The research addresses a practical limitation of CLIP in panoramic vision tasks, which is highly relevant. Through a series of methods and designs, the authors measure the model's sensitivity to both semantics (e.g., 360°-related cues in prompts) and visual features in panoramic images. The conclusion—that CLIP is more sensitive to semantic information but less capable in capturing and extracting panoramic vision features—is reasonable and logically sound.\n\n2. The method design is clever: Using image augmentation and incorporating constraints effectively enhances CLIP's feature extraction capability for 360° panoramic images and improves semantic-visual alignment."}, "weaknesses": {"value": "The main weakness of this work is the lack of demonstrated applicability in downstream scenarios and insufficient experimental results, which primarily rely on inspection. Firstly, providing more quantitative results, rather than the binary 0/1 outcomes from the inspection, would be more persuasive. Secondly, it would be significantly better to show positive results on downstream tasks, such as 360° image retrieval/generation or visual question answering. If show promising results, I will consider raise my score."}, "questions": {"value": "1. The current experiments focus on indoor and cityscape panoramas (e.g., Laval). It is recommended to test the model's generalization on more diverse scenes, such as natural landscapes or dynamic environments.\n\n2. The current method only fine-tunes the image encoder. Since 360° semantics also involve textual understanding, future work could explore jointly fine-tuning the text encoder or designing more fine-grained text prompts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ohi5rLj8cR", "forum": "iQLZChxwDu", "replyto": "iQLZChxwDu", "signatures": ["ICLR.cc/2026/Conference/Submission5668/Reviewer_3aJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5668/Reviewer_3aJi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570669336, "cdate": 1761570669336, "tmdate": 1762918183952, "mdate": 1762918183952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the CLIP models' comprehension abilities of 360-degree image's visual and textual semantics. To probe CLIP's such visual and textual semantics, the paper set up an evaluation method in two ways: keyword manipulation for textual cues, and horizontal circular shift of 360-degree images for visual cues. Based on these evaluations, first CLIP is able to exploit the textual cue such as \"360-degree image\" for a better alignment with the corresponding image. However, second, The invariance does not robustly hold with respect to the image's circular horizontal shift, indicating CLIP's limited comprehension of 360 degree visual semantics. To remedy this, authors propose to fine-tune the CLIP models by introducing LoRA adapters on visual encoders to introduce invariance to such horizontal shifts in the images. Experiments show the improve performance of understanding visual cues of 360-degree circular shifts across different CLIP models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper investigates the CLIP's understanding capabilities of textual and visual semantics, which is underexplored in literature and derived a meaningful observations and solution. Specifically, the findings that (1) CLIP can exploit \"360-degree image-specific\" cues in the text prompt, rather than a generic prompt like \"a photo of\" for a better alignment to its image, and (2) CLIP's alignment especially lacks invariance to 360-degree image's circular horizontal shift would be valuable. All these presentation are clear in the draft and writing is well-organized."}, "weaknesses": {"value": "While the motivation and observation presented in the paper are quite strong, but the major concern lies in the technical side. The devised solution to improve the CLIP's understanding of visual semantics appears too straightforward and leads to expected results; fine-tuning CLIP on shifted images can naturally enhances its robustness to such shifts during inference. In addition, although the method adopts fine-tuning with LoRA, it is important to include different fine-tuning methodologies as well, such as full fine-tuning of both encoders or each encoder individually, to measure the respective effectiveness. Further reasoning and analysis for the fine-tuning part are expected to strengthen its technical solidity of this work."}, "questions": {"value": "Based on the weaknesses stated above, further analyses on the fine-tuning methodology and the fine-tuned models are expected. First, how the different fine-tuning strategies affect the image and text understanding capabilities in 360-degree images? Second, regarding robustness in visual semantics, can the fine-tuned models generalize to shift magnitudes unseen during training (e.g., when the horizontal circular shift applied at test time exc eeds the range observed during training)? In addition, one natural question is that does the scale of pre-training data of CLIP models affect the robustness of visual and textual semantics understanding after fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mXdAO4GvoZ", "forum": "iQLZChxwDu", "replyto": "iQLZChxwDu", "signatures": ["ICLR.cc/2026/Conference/Submission5668/Reviewer_W71t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5668/Reviewer_W71t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984442851, "cdate": 1761984442851, "tmdate": 1762918183569, "mdate": 1762918183569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper probes whether CLIP understands 360° panoramic semantics.\n\nIt defines two new notions:  textual semantics (explicit cues like “360 photo”) and visual semantics (invariance under circular shifts) — and tests them via statistical analysis.\n\nResults show CLIP models rely on textual cuse but fail to maintain shift invariance.\n\nA LoRA-based fine-tuning improves robustness but slightly degrades baseline performance\n."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work clearly defines 360-degree textual semantics and 360-degree visual semantics, addressing a **novel and underexplored problem**.\n\n- The presentation is clear and concise, making it easy to grasp the main ideas quickly.\n\n- The proposed LoRA-based fine-tuning effectively instills shift invariance for 360-degree panoramic scenes.\n\n- The paper provides comprehensive experiments and analyses to support its claims."}, "weaknesses": {"value": "1. The overall method is complete and well studied. But the main concern is that the explored 360-degree visual setting represents a **relatively narrow scenario** and can be viewed as a special case of standard 2D images.\n\n2. The proposed LoRA-based tuning is a commonly used technique, and thus the methodological novelty appears limited.\n\n3. The paper relies on the original CLIP model, which is **somewhat outdated**; incorporating comparisons with more recent models such as SigLIP-V2 or Qwen-VL would strengthen the analysis.\n\n4. While the experimental results are solid, they primarily serve as confirmatory findings rather than revealing deeper insights or unexpected behaviors."}, "questions": {"value": "1. Would the same findings hold for SigLIP or multimodal LLMs (e.g., CLIP-based vision towers in LLaVA or Kosmos-2)?\n\n2. Any visualization or interpretability on why CLIP fails under shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZV615DQZG0", "forum": "iQLZChxwDu", "replyto": "iQLZChxwDu", "signatures": ["ICLR.cc/2026/Conference/Submission5668/Reviewer_UWJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5668/Reviewer_UWJ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137417748, "cdate": 1762137417748, "tmdate": 1762918183251, "mdate": 1762918183251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}