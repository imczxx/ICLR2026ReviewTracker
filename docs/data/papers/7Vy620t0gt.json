{"id": "7Vy620t0gt", "number": 18922, "cdate": 1758292016440, "mdate": 1763110283988, "content": {"title": "Vision Remember: Recovering Visual Information in Efficient LVLM with Vision Feature Resampling", "abstract": "The computational expense of redundant vision tokens in Large Vision-Language Models (LVLMs) has led many existing methods to compress them via a vision projector.\nHowever, this compression may lose visual information that is crucial for tasks relying on fine-grained spatial relationships, such as OCR and Chart \\& Table Understanding.\nIn this paper, we propose to resample original vision features across the LLM decoder layers to recover visual information and attain efficiency. \nFollowing this principle, we introduce Vision Remember, which includes two key modules: (1) Token-Feature Cross-Attention Layer and (2) Token Bidirectional Self-Attention Layer.\nIn the Token bidirectional attention, we employ self-attention mechanism to maintain the bidirectional interaction between vision tokens and the text-guided token.\nIn the Token-Feature interaction attention, we introduce local cross-attention to resample the visual feature and utilize the multi-level fusion to enrich the visual representation.\nWe conduct comprehensive experiments on multiple visual understanding benchmarks and the results with the LLaVA-NeXT baseline show that Vision Remember outperforms TokenPacker by 2.7 and FastV by 5.7 across nearly all the settings.\nThe experimental results also validate the generalization capability of the proposed method when combined with various efficient vision projectors and LVLMs.", "tldr": "", "keywords": ["Multi-Modality Model", "Efficient Large Vision-Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3ca2e1219d691e884862ca0f6106cdcee35b5f3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper discusses the accuracy-efficiency challenge in vision-language models, where feeding too many visual tokens to the large decoder model sacrifices efficiency, and using existing token pruning and merging methods sacrifices accuracy, particularly for tasks requiring fine-grained details such as OCR. The paper proposes a module called “Vision Remember,” which is added to a few layers of the decoder architecture to help the model access fine-grained visual features without increasing the number of tokens. Using a combination of open-weight vision encoders, language models, and public datasets, the paper shows several results supporting the empirical benefits of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The research direction discussed in the paper (efficiency consequences of augmenting a large decoder model with visual tokens) is valuable and important for practical use cases.\n- Paper presentation is generally good and easy to follow.\n- The proposed method is intuitive: it allows visual tokens to access original visual features to tackle the information bottleneck caused by visual token compression.\n- The preliminary linear probing results shown in Figure 1 are interesting.\n- The empirical results support the efficacy of the proposed method on commonly used VLM benchmarks."}, "weaknesses": {"value": "- One major shortcoming of all presented results is that accuracy and cost are not discussed together. The proposed method reduces the number of visual tokens but adds extra compute and parameters. Therefore, to fairly compare it with other methods, results should show that for the same cost (for example, TTFT), it achieves better accuracy. For instance, in Table 5a it is shown that adding the proposed VR module on top of Qwen2.5-VL improves performance by 1.7. What happens to TTFT and TPS in this setup? Similar information is needed for Tables 3, 2, and 1.\n- Missing prior work: In Sections 1 and 2, it is mentioned that existing approaches to tackle the efficiency problem in VLMs involve token pruning or merging, either at the projection layer or in the decoder. However, the authors miss a third category: using vision encoders designed for VLMs that produce fewer visual tokens. For example, the FastViTHD architecture in [1] produces 16× fewer visual tokens than a regular ViT while maintaining accuracy. [1] also discusses multi-level feature utilization. Including a discussion and comparison with this work would clarify the empirical contribution of the proposed method relative to existing approaches.\n- In the ablation studies in Section 4: The explanation of why global attention performs worse than the proposed local attention is not very clear. From an efficiency standpoint, local attention is a better choice. However, global attention still has access to local area information, i.e., what can be attended to in global attention is a superset of local attention. Why does it perform worse?\n- In Section 3.3 on self-attention: This goal seems orthogonal to the main motivation of efficiency. For example, a simple baseline would be to allow full attention mask for visual tokens. Have you considered comparison with such a baseline?\n- The second paragraph in Section 3.3 argues that visual tokens cannot attend to query text tokens. This argument depends on the setup where visual tokens precede text tokens, but this need not be the case. The order could simply be reversed (i.e., text query tokens first, then image tokens). This is common when using interleaved image-text data and does not require an intrusive change to the decoder. Have you considered a baseline where visual tokens come after query text tokens?\n- Augmenting visual tokens with the max pooling of text tokens (the text guided token) could potentially break causality: a text token at position k can know some information about text token at position k+1 through attending augmented visual tokens (the output of Vision Remember module). Can you clarify this point?\n\nMinor points:\n- Typo in the sentence on lines 242–243.\n- Lines 225–226: This is not necessarily an “expansion,” depending on the LLM and vision encoder dimension sizes.\n\n[1] Vasu, Pavan Kumar Anasosalu, et al. “FastVLM: Efficient Vision Encoding for Vision Language Models.” Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "- In phase 2, is the image encoder trainable, or is it kept frozen?\n- What is the LLM size for the results in this table?\n- Section 3.3 on text-guided tokens: What are these text tokens? Are they the original query text tokens, or during inference, when new text tokens are generated (as part of the model’s response), are those also included? From line 470, it appears to be the former. \n- Section 1 (the preliminary analysis): Based on the observations, the authors identify text alignment as one of the reasons original visual feature information is lost. But why is this a problem? Forgetting visual features that are not relevant to the query at hand could, in fact, be beneficial.\n- In line 360, it is mentioned that for a fair comparison of results in Table 3, the same compression ratio is used. Can the authors provide additional details? The proposed method requires additional parameters as well as extra compute. When comparing with other methods, what compute budget is matched to them?\n- When considering global attention in section 4, do you augment visual features with any positional encoding?\n- In Figure 2, three dimensions are shown: $D_v$, $D_e$, and $D$. But they are not clearly defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FOhm2BOnYx", "forum": "7Vy620t0gt", "replyto": "7Vy620t0gt", "signatures": ["ICLR.cc/2026/Conference/Submission18922/Reviewer_TWmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18922/Reviewer_TWmZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439356553, "cdate": 1761439356553, "tmdate": 1762930908945, "mdate": 1762930908945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "OMPo9jrW2b", "forum": "7Vy620t0gt", "replyto": "7Vy620t0gt", "signatures": ["ICLR.cc/2026/Conference/Submission18922/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18922/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763110282770, "cdate": 1763110282770, "tmdate": 1763110282770, "mdate": 1763110282770, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies efficiency in Large Vision-Language Models. Current efficient LVLMs compress or prune vision tokens to reduce cost, but this destroys fine-grained visual cues (text regions, small objects, local layout) and also causes the model to gradually \"forget\" visual details as text dominates later layers. The paper proposes Vision Remember: instead of only compressing vision tokens once at the projector and hoping the model keeps that info, it repeatedly re-injects (resamples) the original high-resolution visual features back into the LLM decoder layers. It does this with (1) a Token-Feature Cross-Attention layer that performs local cross-attention between current vision tokens and multi-level vision features from early/mid/deep vision encoder layers, and (2) a Token Bidirectional Self-Attention layer that lets vision tokens attend bidirectionally to each other and to a pooled \"text-guided token,\" bypassing causal masks and letting vision attend to the user’s textual reference. The method is inserted between decoder layers, is lightweight, and is meant to be plug-compatible with different projectors and backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Identified two concrete failure modes in efficient LVLMs (i) information bottleneck from projector compression and (ii) visual cue forgetting across decoder layers. This is a useful diagnosis.\n2. Resampling vision features mid-decoder is architecturally simple, does not require retraining the whole LVLM from scratch, and can be attached to different visual projectors and different backbones.\n3. Ablations are thorough."}, "weaknesses": {"value": "1. Training cost is not fully discussed. They retrain with CC-558K + 779K instruction tuning. It’s unclear whether Vision Remember needs full two-phase tuning each time you attach it to a new backbone or projector, or whether it can be added with light finetuning on a smaller set. This matters for “plug-and-play” claims.\n2. Some methods are re-trained on additional data instead of original released recipes. This might not be fair to training free methods. \n3. In the empirical study, the proposed method should also be compared against baselines under multiple compression ratios to further validate its effectiveness."}, "questions": {"value": "1. Does the method require full two-phase retraining every time it is applied to a new backbone or projector?\n2. Is it fair to compare Vision Remember with other methods that are retrained on additional data, especially when some baselines are designed to be training-free?\n3. Has the proposed method been systematically evaluated against baselines under multiple compression ratios to fully validate its effectiveness across different efficiency settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i9NNXEDXZD", "forum": "7Vy620t0gt", "replyto": "7Vy620t0gt", "signatures": ["ICLR.cc/2026/Conference/Submission18922/Reviewer_Vwgw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18922/Reviewer_Vwgw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462427061, "cdate": 1761462427061, "tmdate": 1762930908271, "mdate": 1762930908271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the causes of visual information loss in large vision–language models (LVLMs) and proposes Vision Remember, a method that re-injects original visual features into decoder layers to mitigate information bottlenecks and visual forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The writing is clear, and the overall presentation is well-structured."}, "weaknesses": {"value": "1. The proposed Vision Remember framework appears to be an incremental improvement over existing LVLM architectures, as it primarily involves a relatively straightforward modification of the self-attention and cross-attention mechanisms. The methodological novelty seems limited without deeper architectural innovation.\n\n2. It is unclear whether Table 1 compares Vision Remember and the baseline under identical training conditions. If both are trained on the same dataset, the practical significance of the improvement is questionable. Given that current multimodal large models, trained on massive image–text corpora, already achieve strong performance on fine-grained tasks such as OCR and Chart & Table Understanding, it remains uncertain whether the proposed method can provide meaningful gains at scale. The authors should clarify the training setup and justify the necessity of this approach under large-scale pretraining.\n\n3. To demonstrate the generality and robustness of Vision Remember, it would be important to evaluate the approach across different MLLMs and model scales, such as Qwen2.5-VL-7B, InternVL. \n\n4. The proposed modules introduce additional cross-attention and self-attention layers, which likely increase both computational and memory costs. However, the paper does not provide any quantitative analysis of this overhead. A detailed comparison of model complexity (e.g., FLOPs, inference time, or parameter count) would help justify the practical trade-offs between performance gains and computational cost."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbNob0ehkh", "forum": "7Vy620t0gt", "replyto": "7Vy620t0gt", "signatures": ["ICLR.cc/2026/Conference/Submission18922/Reviewer_3iZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18922/Reviewer_3iZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665364729, "cdate": 1761665364729, "tmdate": 1762930907756, "mdate": 1762930907756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Vision Remember, a module to recover visual information lost in efficient Large Vision-Language Models (LVLMs). Traditional LVLMs compress vision tokens for efficiency but lose fine-grained spatial cues. Vision Remember resamples original visual features across LLM decoder layers through two components: a Token-Feature Cross-Attention Layer for local feature resampling and multi-level fusion, and a Token Bidirectional Self-Attention Layer for bidirectional visual-text interaction. Experiments on 11 benchmarks show consistent improvements (up to +5.7 over baselines) with better efficiency, demonstrating its generality across various LVLMs and vision projectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall writing is relatively clear.\n\n2. Experiments show consistent improvements with better efficiency, demonstrating the effectiveness of the module."}, "weaknesses": {"value": "1. The phenomenon shown in Fig. 1b is not particularly new, it has been reported repeatedly (e.g., FastV, PyramidDrop).\n\n2. The performance of “LLaVA-NeXT” is too low; validating an algorithm on it is therefore not very convincing, you can build stronger baselines with more data and new LLM.\n\n3. Are the FastV and PDrop configurations presented in Table 3 their default settings? Please also show the results with no compression applied."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JRGrksC5IU", "forum": "7Vy620t0gt", "replyto": "7Vy620t0gt", "signatures": ["ICLR.cc/2026/Conference/Submission18922/Reviewer_KgXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18922/Reviewer_KgXE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933749327, "cdate": 1761933749327, "tmdate": 1762930906956, "mdate": 1762930906956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}