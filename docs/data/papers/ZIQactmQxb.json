{"id": "ZIQactmQxb", "number": 3772, "cdate": 1757516900489, "mdate": 1763666663780, "content": {"title": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally Constrained Predictions", "abstract": "Artificial Neural Networks (ANNs), including fully-connected networks and transformers, are highly flexible and powerful function approximators, widely applied in fields like computer vision and natural language processing. However, their inability to inherently respect causal structures can limit their robustness, making them vulnerable to covariate shift and difficult to interpret/explain. This poses significant challenges for their reliability in real-world applications. In this paper, we introduce Causal Transformers (CaTs), a general model class designed to operate under predefined causal constraints, as specified by a Directed Acyclic Graph (DAG). CaTs retain the powerful function approximation abilities of traditional neural networks while adhering to the underlying structural constraints, improving robustness, reliability, and interpretability at inference time. This approach opens new avenues for deploying neural networks in more demanding, real-world scenarios where robustness and explainability is critical.", "tldr": "Causal Transformers (CaTs) are neural networks constrained by a causal DAG, combining the power of standard ANNs with improved robustness to covariate shift, greater reliability, and interpretability for real-world applications.", "keywords": ["transformers", "causal inference", "causality", "inductive bias", "DAGs"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61914cb554218df999b8b2458d29b0dacb1454ac.pdf", "supplementary_material": "/attachment/c3ccbc8fa0ed521d4c65176c850c35349c64c41f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces causal transformers (CaTs) for causal prediction. CaTs encode the information of a given causal graph in the architecture, offering a strong approximation power for a given causal model. Detailed experiments show the superior performance of CaTs in term of ATE estimation."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A key advantage highlighted is that by constraining the model to a (correct) causal DAG, it learns to ignore spurious correlations in the training data. This makes the model more robust and stable when faced with distributional shifts (i.e., covariate shift), a common failure point for traditional machine learning models.\n\n2. The experiments show superior performance over the traditional random forest models even if the DAG is misspecified. \n\n3. Unlike many causal inference methods designed for single-value, tabular data, the Causal Transformer (CaT) architecture is built to handle high-dimensional inputs. It can accept a sequence of embeddings, making it applicable to complex, multimodal data like text, images, or multi-item questionnaires."}, "weaknesses": {"value": "1. The literature review part is not complete. There have been many works in the literature that consider using neural networks for counterfactual estimation. For instance, [1] establishes the connection between neural nets and causal models and constructs feed-forward networks for discrete causal models. [2] establishes similar results for causal models with mixed variables (continuous and discrete) and uses it for partial identification. Similar to this work, [3] proposes causal transformers for counterfactual estimation. There are many other works on this topic as well. Please include those works in the related work part. \n\n2. Some details are unclear in the paper. Please see the question part. \n\n3. In the experiment, the author compared the ATE given by different models. However, it is well-known that the Double Robust (DR) estimator provides a more robust way for estimation. It will provide more practical implications if the author can provide a comparison (using the same propensity model).  \n\n[1] Xia, Kevin, et al. \"The causal-neural connection: Expressiveness, learnability, and inference.\" Advances in Neural Information Processing Systems 34 (2021): 10823-10836.\n\n[2] Tan, Jiyuan, Jose Blanchet, and Vasilis Syrgkanis. \"Consistency of neural causal partial identification.\" Advances in Neural Information Processing Systems 37 (2024): 68956-68999.\n\n[3] Melnychuk, Valentyn, Dennis Frauen, and Stefan Feuerriegel. \"Causal transformer for estimating counterfactual outcomes.\" International conference on machine learning. PMLR, 2022."}, "questions": {"value": "Clarify questions:\n\n1. Does CaTs apply to all kinds of causal graphs or just the graph in Figure 2? If it applies to all causal graphs, the identification formula (1) seems to be incorrect since $L_{\\text{set}}$ may contain colliders. \n\n2. How do you model hidden confounders in Figure 3 (b)?\n\n3. Can the authors explain the use of learnable embedding $\\gamma$? In particular, why does $Q$ depend on $\\gamma$ but $K,V$ do not? \n\n4. The authors mention that they do not include normalization in CaTs because it may impact the capacity for precise estimation. In a standard transformer, LayerNorm is applied to each variable (token) independently across its feature dimension. It does not typically \"contaminate\" information between variables. Can the authors provide a more detailed explanation?\n\nExperiment questions:\n\n5. The authors include a standard transformer in Table 1, but do not include it Table 2. Can you include the results too?\n\n6. In Table 1, can you explain why misspecified DAGs can yield low MSE? This is quite counterintuitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DD3SLjh7Bi", "forum": "ZIQactmQxb", "replyto": "ZIQactmQxb", "signatures": ["ICLR.cc/2026/Conference/Submission3772/Reviewer_5yaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3772/Reviewer_5yaS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760846864134, "cdate": 1760846864134, "tmdate": 1762916982211, "mdate": 1762916982211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article focuses on embedding causal knowledge in the form of a Directed Acyclic Graph (DAG) into a transformer architecture. The resulting model is designed to perform causal inference tasks. To achieve this, the authors introduce a causal transformer that enables this integration. They also implement a fully connected neural network that respect DAG structure as an additional baseline. Using a toy example, the authors demonstrate the importance of having a causal structure and a model that respects this causal structure. The work is further validated through real-world experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The authors provide clear motivation and demonstrate it with a compelling example that compares predictive power and treatment effect estimation.\n* They propose a novel modification of the attention mechanism for embedding DAGs into the model.\n* They introduce a formulation for incorporating DAG structure into neural networks.\n* They benchmark their approach against other causal inference methods."}, "weaknesses": {"value": "* The architecture needs to be trained for each DAG on each dataset, which can limit the method from being applied to larger datasets.\n\n* While the authors describe the ability to handle multidimensional embeddings as an advantage, the experiments are conducted on tabular datasets. It would be valuable to see whether the transformer architecture can excel in such settings.\n\n* The authors quantitatively justify not using normalization; however, it is not clear whether the pros would outweigh the cons."}, "questions": {"value": "* Is there a way to amortize this approach across different DAGs or datasets?\n* Are you confident that the differences between CAT and CCFCN are not due to insufficient hyperparameter tuning?\n* Is it possible to create synthetic datasets for these models that include high-dimensional data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "I6sPIR381w", "forum": "ZIQactmQxb", "replyto": "ZIQactmQxb", "signatures": ["ICLR.cc/2026/Conference/Submission3772/Reviewer_Ac6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3772/Reviewer_Ac6L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926386678, "cdate": 1761926386678, "tmdate": 1762916981792, "mdate": 1762916981792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Causal Transformers (CaTs), which incorporates causal graphs as masks inside the attention mechanism of transformers, which improves their performance in terms of robustness and interpretability. CaTs are capable of estimating causal queries such as treatment effects, and they are shown experimentally to outperform methods that do not take causality into account, while being able to compete with methods that do."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Incorporating graphs into the attention mechanism ensures that the causal constraints encoded in the graphs (and therefore the resulting causal inferences) are guaranteed to hold. This is a strong advantage over methods that attempt to achieve causality through regularization. The adjustments such as the omission of layer norm are done carefully to preserve this causal integrity.\n\n2. Assumptions are stated clearly.\n\n3. The provided experimental results seem promising."}, "weaknesses": {"value": "4. The contributions of the paper are largely empirical and leave much to be desired from the theoretical aspects of the models. Most notably, I believe it would be a core contribution to include a theorem that guarantees that causal constraints (such as those from Causal Bayesian Networks) are correctly enforced in the CaT architecture, and therefore, the estimated queries will be correct. Another important contribution would be to support the claim that causal models like CaT can resolve issues related to covariate shift, so it would be fundamental to include a theorem that shows what kinds of guarantees CaT can provide across domains (perhaps using causal transportability theory).\n\n5. The training details of CaT are somewhat unclear. In particular, it would be great to see the exact details of how the loss function is implemented, and how causality plays a role in it.\n\n6. It is unclear what kinds of interventional queries CaTs are capable of estimating, and how each of those queries are particularly calculated through the CaT architecture. It seems that one weakness is that CaTs are not capable of estimating ALL interventional queries (perhaps only the causal effect of treatment on target).\n\n7. Notation is not introduced properly. It is not clear what $B$, $Z$ and $C$ are."}, "questions": {"value": "8. What makes CFCN an interesting baseline, as opposed to just another method developed by the paper? It seems like an oddly specific inclusion to the paper.\n\n9. The paper claims in Sec. 4.2 that it is not expected for CaT and CFCN to excel compared to models specifically tuned for causal inference tasks. I would have listed this as a weakness, but I believe that perhaps one strength of these models is their generative capabilities across many different queries. Could the authors confirm if this is the case (e.g., could CaTs produce novel samples of both the observational and interventional data distributions)? If so, how does this model compare to common approaches used in the causal generative modeling literature such as [1][2][3]?\n\n[1] Kocaoglu, et al., “CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training” (2017).\n\n[2] Xia, et al., “The Causal-Neural Connection: Expressiveness, Learnability, and Inference” (2021).\n\n[3] Saremi, “Neural Network Parameter-optimization of Gaussian Pre-marginalized Directed Acyclic Graphs” (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fehIX7jQrx", "forum": "ZIQactmQxb", "replyto": "ZIQactmQxb", "signatures": ["ICLR.cc/2026/Conference/Submission3772/Reviewer_E5kE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3772/Reviewer_E5kE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762297900465, "cdate": 1762297900465, "tmdate": 1762916980984, "mdate": 1762916980984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}