{"id": "cnrhmiw1VG", "number": 39, "cdate": 1756728205707, "mdate": 1763345568637, "content": {"title": "GLEAM: Learning to Match and Explain in Cross-View Geo-Localization", "abstract": "Cross-View Geo-Localization (CVGL) focuses on identifying correspondences between images captured from distinct perspectives of the same geographical location. However, existing CVGL approaches are typically restricted to a single view or modality, and their direct visual matching strategy lacks interpretability: they only determine whether two images correspond, without explaining the rationale behind the match. In this paper, we present GLEAM-C, a foundational CVGL model that unifies multiple views and modalities—including UAV imagery, street maps, panoramic views, and ground photographs—by aligning them exclusively with satellite imagery. Our framework enhances training efficiency through optimized implementation while achieving accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy. Moreover, to address the lack of interpretability in traditional CVGL methods, we leverage the reasoning capabilities of multimodal large language models (MLLMs) to propose a new task, GLEAM-X, which combines cross-view correspondence prediction with explainable reasoning. To support this task, we construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro to generate training and testing data. The test set is further refined through detailed human revision, enabling systematic evaluation of explainable cross-view reasoning and advancing transparency and scalability in geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing **G**eo-**L**ocalization by enabling models to better **E**xplain **A**nd **M**atch. Code and datasets used in this work will be made publicly accessible.", "tldr": "This work presents GLEAM-C and GLEAM-X, a unified pipeline that advances cross-view geo-localization by integrating multi-view alignment with interpretable, explainable reasoning.", "keywords": ["Remote Sensing", "Cross-View Geo-Localization", "Multimodal Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1068d666bcb65c28ebeebc2ff1da8d2cacf97f9f.pdf", "supplementary_material": "/attachment/3649d00816711f2efb443d6c95c2566a816df980.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a unified and interpretable framework for cross-view geo-localization. The first module, GLEAM-C, aligns multiple modalities (UAV, map, panorama, ground) with satellite images through contrastive learning and a two-phase training strategy, achieving strong accuracy and 5× faster training via distributed data parallelism. The second module, GLEAM-X, introduces an explainable reasoning benchmark where multimodal large language models (e.g., Qwen2.5-VL) generate bilingual (English–Chinese) natural-language explanations for image correspondences, using data annotated by GPT-4o and Doubao-1.5. Together, they form an integrated system that enhances both performance and interpretability in geo-localization tasks"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. GLEAM-C integrates diverse modalities—UAV, street maps, panoramas, and ground images—into a single contrastive learning framework aligned with satellite imagery. This unified design eliminates the need for modality-specific architectures and enhances generalization across heterogeneous views.\n\n2.The model adopts a two-phase training procedure (VIGOR pretraining followed by multi-dataset fusion) combined with a Distributed Data Parallel implementation that aggregates cross-GPU features. This innovation accelerates training by more than fivefold while maintaining accuracy, enabling large-scale foundational CVGL learning.\n\n3. GLEAM-X extends CVGL beyond retrieval to interpretable reasoning, coupling cross-view matching with bilingual (English–Chinese) explanation generation using multimodal large language models. Fine-tuning Qwen2.5-VL on high-quality annotated data improves both matching accuracy and transparency over commercial LLMs."}, "weaknesses": {"value": "There are several concerns that I would like the authors to address.\n\nFirst, the paper does not clearly justify why all modalities were aligned exclusively with satellite imagery. It would be important to provide reasoning or an ablation to show why other modalities were not used as reference anchors.\n\nSecond, the work appears largely as an engineered combination of existing ideas rather than a conceptually novel framework. For instance, the core methodology draws heavily from established approaches such as InfoNCE-based contrastive learning and OpenCLIP-style distributed training, without introducing a distinct algorithmic innovation.\n\nThird, the evaluation of explanation quality raises validity concerns — since the reference explanations are GPT-generated and only the test set is human-edited, the model is effectively rewarded for mimicking its teacher rather than for producing truly accurate or human-intelligible reasoning.\n\nAdditionally, Section 3.3.1 lacks justification for choosing VIGOR as the initial dataset in the two-phase training strategy.\nFinally, no information is provided on inter-annotator agreement during the human revision of the explanations — including how many samples were discarded, or qualitative feedback from annotators on the outputs of the multimodal LLMs."}, "questions": {"value": "I would request the authors to comment on the points mentioned in the weaknesses part. Specifically,\n\n\nWhat is the rationale for aligning all modalities exclusively with satellite imagery? Have the authors explored or analyzed alternative anchor modalities, or can they provide evidence (empirical or conceptual) supporting this design choice?\n\n\nBeyond combining existing components such as InfoNCE loss and OpenCLIP-style DDP, what specific algorithmic or architectural innovation does GLEAM introduce? How does it advance the state of the art beyond prior CVGL frameworks?\n\n\nSince GPT-generated explanations serve as the primary supervision and evaluation reference, how do the authors ensure that the model learns genuine interpretive reasoning rather than merely imitating GPT-style phrasing? Were any human-centric or factual quality metrics used?\n\n\nWhy was VIGOR chosen as the initial dataset in the two-phase training process? Have the authors verified that starting with VIGOR (instead of other datasets) leads to measurable benefits in generalization or convergence?\n\n\nCould the authors report inter-annotator agreement scores or qualitative statistics (e.g., number of samples revised or rejected) to substantiate the reliability of the human-refined explanation dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "76RbDf9KUv", "forum": "cnrhmiw1VG", "replyto": "cnrhmiw1VG", "signatures": ["ICLR.cc/2026/Conference/Submission39/Reviewer_kFbq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission39/Reviewer_kFbq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission39/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799732783, "cdate": 1761799732783, "tmdate": 1762915441883, "mdate": 1762915441883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all reviewers for their thorough and constructive reviews.  We have carefully addressed all comments with detailed responses and made corresponding revisions to the manuscript. Below, we summarize the major changes in the paper in sequential order (highlighted in blue) and their correspondence to the reviewers' comments.\n\n1. Line 039:\n    - Highlighting \"satellite observations\" (kFbq)\n    - Expanding GNSS acronym to full form (hWJr)\n2. Line 269:\n    - Adding analysis of linguistic fairness (bDAZ)\n3. Line 300:\n    - Adding a dedicated section for the GLEAM-C/X pipeline (gY16)\n4. Line 317 and 810:\n    - Adding real-world deployment performance (gY16)\n5. Line 394 and 829:\n    - Adding cross-domain generalization experiments of GLEAM-C (bDAZ)\n6. Line 394 and 877:\n    - Explaining why we chose VIGOR as the initial dataset (kFbq)\n7. Line 424:\n    - Adding analysis of trade-off between training with explanations and solely matching labels (hWJr)\n8. Line 477:\n    - Adding analysis of negative bias of original MLLMs (gY16)\n9. Line 501 and 922:\n    - Adding LLM-as-a-judge and human test (hWJr, bDAZ, kFbq)\n10. Line 511:\n    - Analyzing potential bias of teacher models (hWJr, bDAZ, kFbq)\n11. Line 781:\n    - Adding more details of human annotation (kFbq)"}}, "id": "0IZgyf1XY4", "forum": "cnrhmiw1VG", "replyto": "cnrhmiw1VG", "signatures": ["ICLR.cc/2026/Conference/Submission39/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission39/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission39/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763260939564, "cdate": 1763260939564, "tmdate": 1763260939564, "mdate": 1763260939564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GLEAM, a two-component framework that addresses the challenges of multi-view/modal integration and interpretability in CVGL. The first component aligns UAV imagery, street maps, panoramic views, and ground photographs with satellite images through contrastive learning with an efficient two-phase training strategy. The second component establishes a bilingual benchmark combining image matching with explainable reasoning, where a fine-tuned vision-language model outperforms existing systems and generates human-aligned explanations. Overall, GLEAM integrates accurate cross-view matching with interpretable reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths：\n\n1. **Well-motivated problem.** The paper addresses two important and existing challenges in the CVGL field: limited view/modality coverage and lack of interpretability, emphasizing their practical significance.\n\n2. **Sound technical design.** The proposed two-phase training strategy effectively balances data complexity across datasets, and distributed optimization improves training efficiency.\n\n3. **High-quality benchmark.** By combining large-scale LLM-generated explanations with selected expert refinement, the benchmark ensures both data quality and bilingual generality.\n\n4. **Comprehensive experimental analysis.** Experiments cover multiple evaluation aspects and provide valuable insights into model behavior under different supervision settings."}, "weaknesses": {"value": "## Weaknesses：\n\nI believe the paper's methodology and experiments are thorough and well-detailed. However, the following points could be addressed to further strengthen the work:\n\n1. **Paper structure.** The notion of an \"integrated pipeline\" for GLEAM-C and GLEAM-X is highlighted as a key contribution, but its operational workflow is not described in a dedicated section. Providing a clear explanation of this operational workflow would help substantiate its significance.\n\n2. **Experimental analysis.** While the paper identifies a strong bias toward negative samples in the original MLLM, the root cause is not investigated. Understanding the origin of the performance discrepancy between positive and negative pairs would clarify this behavior.\n\n3. **Practical usage.** Although the paper discusses the unified model's deployment advantages, it lacks analysis of practical performance metrics. Reporting the model's memory footprint and power consumption would be valuable for real-world deployment."}, "questions": {"value": "## Questions：\n\n1. Could the authors consider adding a dedicated section for the GLEAM-C/X pipeline in the main text to clarify its operational significance?\n\n2. What is the root cause of Qwen2.5-VL-3B's bias, and has its origin been explored?\n\n3. Could the authors provide information on the model's memory footprint and power consumption under real deployment scenarios?\n\n4. Algorithm 1, detailing the DDP optimization, is important for understanding the methodology. Would the authors consider moving it from the Appendix to the main text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hazAVvr294", "forum": "cnrhmiw1VG", "replyto": "cnrhmiw1VG", "signatures": ["ICLR.cc/2026/Conference/Submission39/Reviewer_gY16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission39/Reviewer_gY16"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission39/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904780742, "cdate": 1761904780742, "tmdate": 1762915441636, "mdate": 1762915441636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified framework for cross-view geo-localization (CVGL) that integrates both matching and explainability. The proposed system comprises two complementary components: GLEAM-C, a foundational CVGL model aligning multiple modalities with satellite imagery using a two-phase training strategy and distributed data parallel optimization; and GLEAM-X, an explainable benchmark that employs multimodal large language models to produce interpretable reasoning for image correspondence. By constructing a bilingual dataset with human-refined annotations and leveraging models like GPT-4o and Doubao-1.5, GLEAM-X enables systematic evaluation of explainable matching."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a novel contribution by combining multi-modal alignment with explainable reasoning in CVGL. The integration of MLLM-based explanation into visual matching tasks (GLEAM-X) represents a creative and timely advancement, addressing interpretability, a known limitation in existing CVGL models.\n2. Experimental results across multiple datasets (VIGOR, University-1652, SetVL-480K, MAP) are comprehensive, and the reported efficiency gains are well-quantified.\n3. The paper is well-structured. Figures and tables effectively illustrate architecture and results."}, "weaknesses": {"value": "1. Limited novelty in the core CVGL model: While GLEAM-C unifies modalities effectively, its architecture largely builds on prior contrastive learning paradigms without introducing new architectural elements beyond training optimizations.\n2. GLEAM-X depends heavily on GPT-4o and Doubao-generated labels, which may introduce linguistic bias or superficial reasoning patterns. The evaluation of “explanation quality” via semantic similarity does not fully capture human interpretability or factual grounding.\n3. Main figures not show enough information of the work (dataset catories, detailed data distribution), and are not informative enough."}, "questions": {"value": "1. Human Evaluation: Have human judges been used to assess the fidelity and usefulness of generated explanations, beyond Sentence-BERT similarity? How consistent are these with human rationales?\n2. Generalization: How well does GLEAM-C perform when trained on one modality and tested on unseen modalities? A cross-domain generalization study could highlight robustness.\n3. Bias Mitigation: Since GPT-based models generated training explanations, what measures were taken to ensure linguistic fairness and minimize cultural/geographical bias in the bilingual dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pzMzrrdvSP", "forum": "cnrhmiw1VG", "replyto": "cnrhmiw1VG", "signatures": ["ICLR.cc/2026/Conference/Submission39/Reviewer_bDAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission39/Reviewer_bDAZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission39/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101883626, "cdate": 1762101883626, "tmdate": 1762915441423, "mdate": 1762915441423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GLEAM, a comprehensive framework for CVGL. It introduces GLEAM-C, a unified foundational model that aligns multiple modalities with satellite imagery using contrastive learning. Additionally, it proposes GLEAM-X, a novel benchmark for explainable CVGL, leveraging MLLMs to provide interpretable reasoning for matching predictions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The integration of diverse input modalities—UAV imagery, street maps, panoramic views, and ground-level photos—into a single, satellite-aligned CVGL framework. The definition is clear and has important potential social impacts in real-world scenarios.\n* Comprehensive experimental and implementation considerations."}, "weaknesses": {"value": "* Need to add extra experiments for assessing the quality of the generated explanations. Currently, the paper calculates explanations’ semantic similarity to the annotated references with Sentence-BERT. However, Sentence-BERT may fail to capture the key reasoning step. The score would be high when the logic seems to be true, but false in key steps. Adding a user study or LLM-as-a-judge is important.\n* As shown in Table 4, the model trained with label-only supervision (+label only) achieves a high matching accuracy of 92.76%, whereas the model trained with explanation supervision (+GPT-4o) drops to 90.38%. Discussion on this is expected. Does this suggest that incorporating explainability may come at the cost of reduced matching performance? However, the paper does not provide an in-depth analysis of this trade-off.\n* Insufficient analysis of failure cases. The paper lacks an in-depth analysis of model failure cases and does not discuss the specific scenarios in which the model underperforms or the underlying reasons for such failures. \n* The teacher model (GPT4o and Doubao-1.5) may introduce bias during training data curation. Discussing this issue will enhance the rigor of the data curation workflow."}, "questions": {"value": "see above weakness \n\nOthers:\n- Write the full name of GNSS in the first paragraph that firstly mention it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YqOwnXFkVw", "forum": "cnrhmiw1VG", "replyto": "cnrhmiw1VG", "signatures": ["ICLR.cc/2026/Conference/Submission39/Reviewer_hWJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission39/Reviewer_hWJr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission39/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762441445540, "cdate": 1762441445540, "tmdate": 1762915441199, "mdate": 1762915441199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}