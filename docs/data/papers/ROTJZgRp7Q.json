{"id": "ROTJZgRp7Q", "number": 20134, "cdate": 1758302900805, "mdate": 1759896999885, "content": {"title": "Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation", "abstract": "To deploy large language models (LLMs) in high-stakes application domains that require substantively accurate responses to open-ended prompts, we need reliable, computationally inexpensive methods that assess the trustworthiness of long-form responses generated by LLMs. However, existing approaches often rely on claim-by-claim fact-checking, which is computationally expensive and brittle in long-form responses to open-ended prompts. In this work, we introduce semantic isotropy—the degree of uniformity across normalized text embeddings on the unit sphere—and use it to assess the trustworthiness of long-form responses generated by LLMs. To do so, we generate several long-form responses, embed them, and estimate the level of semantic isotropy of these responses as the angular dispersion of the embeddings on the unit sphere. We find that higher semantic isotropy—that is, greater embedding dispersion—reliably signals lower factual consistency across samples. Our approach requires no labeled data, no fine-tuning, and no hyperparameter selection, and can be used with open- or closed-weight embedding models. Across multiple domains, our method consistently outperforms existing approaches in predicting nonfactuality in long-form responses using only a handful of samples—offering a practical, low-cost approach for integrating trust assessment into real-world LLM workflows.", "tldr": "We introduce Semantic Isotropy, a geometry-inspired metric for assessing the trustworthiness of long-form language model outputs, and demonstrate its effectiveness and robustness across diverse models and evaluation settings, achieving new SOTA.", "keywords": ["Language Modeling", "Trustworthiness", "Semantic Uncertainty", "Long-Form Natural Language Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63f10aa70f95252a72cd468d213f4ec54e120901.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a computationally efficient, embedding-based approach to approximating the factuality of LLM responses. The Semantic Isotropy approach samples multiple answers, runs them through embedding models, and then estimates the von Neumann entropy of the cosine kernel under the set of embeddings. This approximator is evaluated via its correlation with SegmentScore, a coarse-grained version of FactScore that this paper also introduces. The approximator outperforms numerous baselines under the correlation metric using numerous models for generating and embedding answers. The paper contributes the estimator, the new metric, and a new dataset of LLM-generated long form answers over subsets of FS-BIO and TriviaQA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The method's computational efficiency would be highly beneficial if the experimental evidence gives strong support for using it in a downstream task.\n\nS2. The authors comprehensively evaluate multiple baseline approximators, answer generators, and embedding models. \n\nS3. The paper is well-written with no obvious methodological details missing."}, "weaknesses": {"value": "W1. The approach is evaluated using a new metric, Segment-Score, that is not validated for its coherence. FactScore, on which the new metric is based, was evaluated against ground-truth human annotations to verify its correlation. Especially since Segment-Score is less granular than FactScore (evaluating at the level of segment rather than atomic fact) and has higher variance, it is important to verify that it still has a low ground truth error rate. The authors show a reasonable ($R^2 = .63$) correlation, but this is not perfect and merits further validation.\n\nW2. The approach requires overgenerating many responses (8+) to get a nontrivial signal of embedding isotropy, so cannot be used off-the-shelf to evaluate one given response.\n\nW3. The overall $R^2$ numbers for certain configurations, e.g. Meta Llama on TriviaQA, are quite low; while the approach outperforms the considered baselines as an approximation for the factuality score, it raises the question of if any of these approaches are at all useable if none of them achieve a certain threshold of calibration. \n\nW4. In general, the paper treats $R^2$ as an end goal for an approximator, but that doesn't tell us if the method will be practically useful."}, "questions": {"value": "Q1. L306 \"we exclude all entities that correspond to days, dates or numerical values and only select those that match the title of the underlying Wikipedia page.\" -- the motivation for this is unclear, why did you perform this subselection?\n\nQ2. Does the approach work for cases in which the LLM response is more sparse in its factual statements? e.g. One response might contain [Fact1, Fact2] while another might contain [Fact2, <elaboration of fact2>, Fact1, <elaboration of fact1>] -- i would imagine the embedding of these two responses might be further from each other than [Fact1, IncorrectFact3]. \n\nQ3. Why is SegScore higher on average for >750 length responses than it is on <650? \n\nQ4. How might this approach be put into practice? Does the method actually work for detecting factual problems?  It would be helpful to perform task-specific validation of some sort -- using the method as a verifier with a task-specific success criterion (e.g. how about using the FactScore itself to evaluate responses and then compute an ROC curve across various thresholds of Semantic Isotropy?)\n\nQ5. The approach violates my intuitions about LFQA. Multiple 500+ token responses that are both factual but, perhaps, say different things about a subject (e.g. one response \"tell me about Marie Curie\" is about where Marie Curie lived and another is about her research; -- semantically very different, so likely different embeddings). Wouldn't this approximator penalize this sort of behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "py4ilAx02J", "forum": "ROTJZgRp7Q", "replyto": "ROTJZgRp7Q", "signatures": ["ICLR.cc/2026/Conference/Submission20134/Reviewer_esvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20134/Reviewer_esvS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942535720, "cdate": 1761942535720, "tmdate": 1762933165039, "mdate": 1762933165039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses semantic isotropy to assess the level of trustworthiness of long-form documents, to address the issue of the computational complexity of claim-by-claim fact checking. The authors also introduce a Segment-Score protocol to generate and score datasets, which they use to produce datasets for this type of problem. They then empirically demonstrate the efficacy of the proposed method for this task, exploring multiple models and experimental parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors identify an important problem: We need fact-checking systems more than ever, and existing claim-by-claim verifiers are extremely computationally costly, given that the models that do well across varied domains tend to be exorbitantly expensive when run over entire documents.\n\n- The approach does not require fine-tuning or any training data, and can be used with closed-weight models.\n\n- The approach is evaluated across multiple domains.\n\n- The evaluation results of this system across multiple benchmarks is strong, and strengthens the claim that this approach is a worthwhile method to investigate further. It would be interesting to explore this method used in conjunction with a traditional claim-by-claim verifier, applied to only specific portions of text, for example.\n\n- The method description and math segments are well-written and clear.\n\n- The \"sensitivity studies\" are very useful for better understanding the nature of the approach and areas for future work and experimentation."}, "weaknesses": {"value": "- How does the approach handle explicit disinformation, where the incorrect claim under consideration is intentionally hidden? Claim-by-claim verifiers are generally able to detect these, but this seems to not be the use case of this method. More detail regarding when this approach should be used over claim-by-claim verification and potential pitfalls would be helpful.\n\n- The authors claim that \"[existing claim verification systems] struggle with open-ended, multi-sentence answers where relevant facts are implicit rather than explicit\", but it isn't entirely clear in the text how their proposed approach solves this specific problem. Further elaboration here would be helpful.\n\n- Ideally, the authors would evaluate on more base datasets beyond FactScore-Bio and TriviaQA. Evaluating on more varied domains/genres would greatly strengthen the experiment results."}, "questions": {"value": "See weaknesses. In addition to these points, another question is: The authors state that \"intuitively, if a prompt admits a single, factually grounded explanation, independently sampled responses should cluster tightly in embedding space\". Are there any cases in which this would not be the case? How should these be handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OTEuuHBGvI", "forum": "ROTJZgRp7Q", "replyto": "ROTJZgRp7Q", "signatures": ["ICLR.cc/2026/Conference/Submission20134/Reviewer_4mCe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20134/Reviewer_4mCe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963892653, "cdate": 1761963892653, "tmdate": 1762933163753, "mdate": 1762933163753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a factuality scoring mechanism for long-form QA tasks. The score is based on the isotropy of semantic embeddings of sampled generated long-form responses from LLMs. The score is compared with another proposed ground truth score - Segment score, which uses an oracle LLM-as-a-judge."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I think that the problem is well-motivated and important to solve. I agree that existing approaches are not sufficient.\n2. The experiments are pretty comprehensive, with multiple models and baselines. \n3. The work contributes a dataset for long-form answer factuality check."}, "weaknesses": {"value": "1. My main critique of the work is about the hypotheses. Specifically,\n    1. The work assumes that \"certainty\" and \"factuality\" are the same and uses them interchangeably. However, even if an LLM is certain of a fact and regenerates similar text when resampled, it may not be factually correct. Clarifying the definition of factuality assumed by the work would be useful here.\n    2. The semantic isotropy score depends on the quality of the embeddings. While the authors have an extensive ablation study on the embedding models, the fundamental assumption that embeddings correctly capture semantic meaning and group semantically similar text together, is debatable.\n2. I think that the isotropy definition is incorrect and actually for orthornormal vectors. The intuition provided in the text doesn't look consistent with the math.\n3. Segment-score is paper's own method to establish ground truth, but why not use existing ground truth? Also, why can't one just use segment-score (the ground truth) directly? What are the advantages of semantic isotropy score? How does this ground truth correlate with other ground truth like FactScore?"}, "questions": {"value": "1. L185-186: \"on average, the transformation represented by X preserves directions and spreads uniformly across all directions in R^D\" is not clear to me.\n2. Does the isotropy definition require N = D for the matrix multiplication to be valid?\n3. Why have you used different kinds of embedding aggregations, e.g., last token embeddings for all but Nomic v1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5VQg4Prfb1", "forum": "ROTJZgRp7Q", "replyto": "ROTJZgRp7Q", "signatures": ["ICLR.cc/2026/Conference/Submission20134/Reviewer_D6SR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20134/Reviewer_D6SR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965237606, "cdate": 1761965237606, "tmdate": 1762933160895, "mdate": 1762933160895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel idea semantic isotropy for measuring factuality of long-form responses. This is based on the idea that if a model resposne is factual, independently sampled responses should cluster tightly in the embedding space, while hallucinated responses are dispersed in the embedding space. Based on this idea, this work develops Segment-Score protocol for factuality scoring. Via experiments, the results show that the proposed approach achieved SOTA performance on factuality checking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed approach achieves SOTA performance on factuality checking.\n2. The proposed approach is lightweight and computationally efficient.\n3. The approach is robust to the embedding models."}, "weaknesses": {"value": "1. This work is developed for long-form text generation. However, the experiments focuse on resonse lengths up to 1000 words, which seems not long enough considering that a lot of LLMs can generate up to 100K tokens.\n2. The benchmarking datasets include TriviaQA and FS-BIO, which is limited. It should include more datasets for experiments to ensure the generlizabilit of the proposed approach."}, "questions": {"value": "See Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jo6QS3dJBX", "forum": "ROTJZgRp7Q", "replyto": "ROTJZgRp7Q", "signatures": ["ICLR.cc/2026/Conference/Submission20134/Reviewer_2c9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20134/Reviewer_2c9h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967111361, "cdate": 1761967111361, "tmdate": 1762933149450, "mdate": 1762933149450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}