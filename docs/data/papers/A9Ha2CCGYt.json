{"id": "A9Ha2CCGYt", "number": 16643, "cdate": 1758267181938, "mdate": 1759897227789, "content": {"title": "DocReward: A Document Reward Model for Structuring and Stylizing", "abstract": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality- agnostic way. DOCREWARD is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5’s 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents", "tldr": "document reward model focusing on structure and style", "keywords": ["reward model", "document structure and style"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a3d397de1e05cb79f937db5011c304eb77e77de.pdf", "supplementary_material": "/attachment/1717023c2d9c65022ab84b6f83b6398f17277407.zip"}, "replies": [{"content": {"summary": {"value": "The paper describes work on building a reward model for optimizing professional aesthetics of documents for document-generation/document-assessment agentic workflows. The authors define the notion of “professionalism” based on two factors 1) structure pertaining to proper use of spacing, indention, alignment, breaks, etc and 2) style pertaining proper use of fonts, headings, emphasis, numbering, formatting, etc. The authors propose a three-step agent-based data construction pipeline to augment existing professional document datasets (GovDocs, NapierOne, collected docs) and rank based on GPT-5 judgments of professionalism. The final dataset result contains 117K paired documents, covering 32 domains and 267 document types. For the experiments, the author use Qwen-2.5-VL as the model of choice to be optimized and compared its performance exclusively with commercial models (GPT,-4o and 5 and Claude Sonnet) across pairwise and pointwise setups. Results show that DocReward-7B outperforms all other models by some margin but no tests were conducted whether the performance advantage is significant. The authors also miss comparing with strong baselines (vanilla Qwen) as well as strong openweight VL models that could have increased the paper’s empirical findings and technical rigor. There are also concerns on proper capturing of rewards from human preferences conducted by the authors. At this current stage, I’m not confident in accepting the paper at its current state. More information on issues and points for improvement are discussed below."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is fairly well written and easy to read. The problem being tackled on notions of professionalism in documents is important but requires proper modelling of desired outcomes or aesthetics from diverse human preferences. The augmented dataset from the paper DocPairs may be of use for the community provided that proper licensing and terms of agreements are clearly-defined."}, "weaknesses": {"value": "The experimented models are restrictive and exclusive to commercial LLMs and lack strong baseline comparisons. Why did you not include vanilla unoptimized Qwen-2.5-VL (3B, 7B, and 32B) for pairwise and pointwise settings? Since you’re using this as your baseline model, it’s perfectly fair to also compare it without the additional task-based optimization for document rewarding. For additional rigor and empirical evaluations to strengthen the paper’s findings, the authors can also add more updated or openweight VLMs such as DeepSeek-VL, CogVLM, etc.\n\nThere is very limited information in the human-related preference ranking which makes the paper confusing. How many (skilled) human annotators were asked to rank preferred documents? How are the pairs distributed? If only one person is employed, the reward model might be shortsighted and only favors one person’s notion of professionality. Please clarity/justify this part.\n\nSimilar to the previous question, what are the domain backgrounds of the annotators? Human annotators have inherent biases such as an annotator working on education domain might prefer formats from education documents and treat it as the gold-standard professional document which still qualifies for the “professional” definition provided by the authors. How does the study handle this possible bias? What is the specific instruction provided to the annotators when ranking? \n\nI strongly suggest using a more robust and recognized metric like Cohen’s kappa for calculating annotator reliability across all experiments than percentage scoring as this is misleading.\n\nThe paper lacks error analysis of the reward models. Instead of the case study, I would prefer learning about cases of instances where say a human rates a certain document layout as high score but the reward model rates it with a low score (and vice versa). These misjudgments are important to diagnoses certain weaknesses of the reward model which can be attributed to factors such as limited diversity of preference ranking, limited variation in document sources (since the domain distribution is not balanced), etc. \n\nThere are parts using the phrase “well-educated human evaluators” - Please a more specific wording such as “qualified” or “skilled” (with n number of domain-specific experience if applicable) instead of “well-educated” as this is vague."}, "questions": {"value": "Did you measure for data overlap from the existing public collections of document datasets (GovDocs1 and NapierOne) against the ones you collective via Common Crawl? Both are from public sources so there may be duplicated documents.\n\nWhat would be the license and terms of agreement that the authors will associate with the new DocPairs dataset? Please mention this explicitly in the paper.\n\nHow many (skilled) human annotators were asked to rank preferred documents? How are the pairs distributed?\n\nAre the improvements for models optimized DocReward statistically significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h3Y9Bl9Shu", "forum": "A9Ha2CCGYt", "replyto": "A9Ha2CCGYt", "signatures": ["ICLR.cc/2026/Conference/Submission16643/Reviewer_XQhC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16643/Reviewer_XQhC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760907521315, "cdate": 1760907521315, "tmdate": 1762926705382, "mdate": 1762926705382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a large-scale document preference dataset and proposes DocReward to evaluate the professionalism of documents. Experiments show that the trained model outperforms existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a large-scale document preference dataset.\n2. Experiments demonstrate the effectiveness of the proposed dataset and model."}, "weaknesses": {"value": "1. The proposed dataset and model are limited to Microsoft Word only.\n\nThe reason I give a score of 4 and a confidence of 3 is that I have many questions (refer to the questions). Once these questions are addressed, I’m willing to raise my score."}, "questions": {"value": "1.  In Figure 7, why is (c) scored higher than (b)? (I’m not entirely sure about the definition of professionalism. After checking the rules mentioned in Section 4.5, I still prefer (b).)\n2. The ranking and prompt do not consider semantic accuracy. How can you ensure that the generated document does not contain incorrect content yet still achieves a high professionalism score due to its layout?\n3. The ranking suggests that human-generated documents are always better. However, shouldn’t model-generated documents sometimes outperform human ones? In lines 195–198, the filtering process shows that some low-quality documents are also human-generated. In this case, the reward model may simply learn human style rather than true quality of style. (This is also reflected in the accuracy gap between Synth vs. Synth and Real & Synth.)\n4. What if the versions of Microsoft Word are different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cd9CphXLC7", "forum": "A9Ha2CCGYt", "replyto": "A9Ha2CCGYt", "signatures": ["ICLR.cc/2026/Conference/Submission16643/Reviewer_UQBH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16643/Reviewer_UQBH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813456321, "cdate": 1761813456321, "tmdate": 1762926705015, "mdate": 1762926705015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses automatic assessment of document professionalism, emphasizing structural and stylistic quality beyond textual content. The authors propose DocReward, a pointwise document reward model that operates on rendered page images and is trained with a Bradley–Terry  preference-learning objective over pairwise comparisons. To enforce textual-quality agnosticism, they construct the DOCPAIR dataset, in which each pair shares identical textual content but differs in structure and style. The model is built on Qwen-2.5-VL with multi-image inputs; a regression head outputs a single scalar score, optimized with the BT loss to separate preferred from non-preferred samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem specification: The textual content is held fixed and only structure/style are evaluated, thereby avoiding contamination from writing quality or factual correctness; the formal objective is consistent with the annotation protocol.\n2. Training data scale and diversity: DOCPAIR spans 32 domains and 267 document types, comprising 117K paired samples, and includes both Real-vs-Synth and Synth-vs-Synth comparisons.\n3. Alignment with preference learning: Pairwise supervision with the Bradley–Terry (BT) loss aligns with human-preference data and is consistent with the preference-learning paradigm underlying RLHF/DPO."}, "weaknesses": {"value": "1. The work reads primarily as an engineering integration: dataset construction and a reward-modeling pipeline built on existing multimodal backbones dominate the contribution, while methodological innovations and fundamental advances over existing paradigms (preference learning/layout understanding/aesthetic evaluation) remain unclear.\n2. Although the training set covers many document types, it cannot exhaust the long-tail of real-world distributions; the paper does not provide consistent evaluation on unseen types or out-of-domain settings. It is recommended to conduct explicit out-of-domain experiments (ensuring certain types/domains are entirely unseen during training) and report performance on the held-out sets; additionally, include robustness tests and error analyses for cross-lingual cases and extreme layouts (e.g., scanned documents, multi-column pages, complex tables).\n3. The data pipeline relies on GPT-5 for heuristic filtering and ternary comparisons (Synth-vs-Synth). Even with a small-scale human audit, this may distill upstream model preferences in layout/style into the training signal. Moreover, the paper does not disclose expert grading rubrics/annotation guidelines or quantify inter-annotator consistency.\n4. The current “extrinsic evaluation” is primarily an offline candidate generation → reward-based reranking setup and does not demonstrate that the reward can serve as an effective training signal to substantively improve generation quality. It is recommended to add downstream tasks that optimize layout/page generation with this reward (e.g., as a signal for fine-tuning or RL), and report the resulting gains."}, "questions": {"value": "1. Are the DOCPAIR dataset and its construction methodology open-sourced? If so, under what license?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nndu11irtZ", "forum": "A9Ha2CCGYt", "replyto": "A9Ha2CCGYt", "signatures": ["ICLR.cc/2026/Conference/Submission16643/Reviewer_Knxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16643/Reviewer_Knxt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896927957, "cdate": 1761896927957, "tmdate": 1762926704553, "mdate": 1762926704553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DocReward, a multimodal reward model based on Qwen2.5-VL for evaluating document professionalism in structure (e.g., spacing, alignment) and style (e.g., fonts, headings), ignoring textual quality. It builds DocPair, a 117K paired dataset across 32 domains/267 types with identical content but varying visuals, using agentic expansion and GPT-5 ranking. Trained with Bradley-Terry loss on rendered images. Overall, advances agentic workflows for visually polished documents.\n\nStrength:\n- Rigorous multi-phase dataset pipeline with diverse domains provides a scalable benchmark for multimodal reward modeling.\n\nWeaknesses:\n- Heavy dependence on closed-source LLMs for agentic expansion and ranking risks propagating their stylistic biases, potentially undermining the model's independence.\n- Human evaluation emphasis on win-rates overlooks potential over-optimization to annotator preferences, which may not generalize to broader cultural or accessibility standards.\n\nOverall, the paper contributes a new dataset and a new model that may benefit the community. The method is not technically flawed per se. However, the paper lacks technical depth and novelty. Therefore, I recommend weak acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Rigorous multi-phase dataset pipeline with diverse domains provides a scalable benchmark for multimodal reward modeling."}, "weaknesses": {"value": "- Heavy dependence on closed-source LLMs for agentic expansion and ranking risks propagating their stylistic biases, potentially undermining the model's independence.\n- Human evaluation emphasis on win-rates overlooks potential over-optimization to annotator preferences, which may not generalize to broader cultural or accessibility standards."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6dlLQthLgx", "forum": "A9Ha2CCGYt", "replyto": "A9Ha2CCGYt", "signatures": ["ICLR.cc/2026/Conference/Submission16643/Reviewer_qN6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16643/Reviewer_qN6s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16643/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145292650, "cdate": 1762145292650, "tmdate": 1762926704006, "mdate": 1762926704006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}