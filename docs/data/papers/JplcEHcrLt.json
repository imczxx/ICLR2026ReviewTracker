{"id": "JplcEHcrLt", "number": 11237, "cdate": 1758194092807, "mdate": 1759897599325, "content": {"title": "Inductive Bias and Spectral Properties of Single-Head Attention in High Dimensions", "abstract": "We study empirical risk minimization in a single-head tied-attention layer trained on synthetic high-dimensional sequence tasks, given by the recently introduced attention-indexed model. Using tools from random matrix theory, spin-glass physics, and approximate message passing, we derive sharp asymptotics for training and test errors, locate interpolation and recovery thresholds, and characterize the limiting spectral distribution of the learned weights. Weight decay induces an implicit nuclear-norm regularization, favoring low-rank query and key matrices. Leveraging this, we compare the standard factorized training of query and key matrices with a direct parameterization in which their product is trained element-wise, revealing the inductive bias introduced by the factorized form. Remarkably, the predicted spectral distribution echoes empirical trends reported in large-scale transformers, offering a theoretical perspective consistent with these phenomena.", "tldr": "We analyze ERM in high-dimensional attention, deriving learning curves and spectral laws that explain empirical transformer behavior.", "keywords": ["High-dimensional statistics; Overparameterization; Attention mechanisms; Inductive bias; Random matrix theory; Approximate message passing; Generalization; Spectral analysis; Compressed Sensing"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8348ade97b2cd668e9a5c2f8f0aedc7a36d00462.pdf", "supplementary_material": "/attachment/5a084a07f90df90495c3253c110b2c980ebddc77.zip"}, "replies": [{"content": {"summary": {"value": "This work studies the fundamental inductive bias topic of the (single-head) attention mechanism when applied to attention-indexed targets. Authors investigate this specific learning problem, derive exact formulas for optimal solutions of corresponding risk minimization, and quantify spectral laws of learned weights, under certain high-dimensional statistical asymptotics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work provides fine-grained mathematical analysis of a concrete learning problem of approximating attention-indexed targets with single-head attention. \n2. The theoretical part is quite detailed, with interesting and insightful characterizations on optimal learning errors and model parameters‘ (spectrum) distributions. \n3. There are numerical verifications aligned with derived theoretical results and implications."}, "weaknesses": {"value": "1. For model space: In Eq. (1), authors only focus on a special architectural form, where for weight parameters, key and query matrices are equal (tied), and value matrices are set as identities. In addition, a centering operation and an extra $d$-scaling are required, which is not the case in practice. \n2. For target space: As defined in Eq. (4, 5), the attention-indexed target also appears a special form that resembles the model. This is acceptable for approximation purposes in theory, but how does this particular form relate to practical applications? \n- More questions on Eq. (5): \n    - Why do we need a special scaling in input dimensions ($\\sqrt{d}$) differing from that of the model ($d$)?  \n    - Why can we model higher-order dependencies in target input-output relationships as (Gaussian) noises? \n    - It is not clear of the definition of $\\Delta$ and square roots of matrices.  \n3. For input space: Only Gaussian input sequences are investigated. Even if this is acceptable in theory, it would be better to include numerical verifications on whether similar patterns hold in general for more types of data distributions, particularly on real-world datasets. \n4. Although it is reasonable to assume the limit conditions in Eq. (6) for the purpose of exact formulas of e.g. ERM asymptotics, I wonder why such conditions are necessary. Since increasing either the model dimension $m$ or number of samples $n$ should be beneficial (particularly for the convergence of ERM), one should not restrict both of them as in Eq. (6) ($O(d)$ and $O(d^2)$, respectively). \n5. More questions on Claim 1: \n- It seems that there are no formal proofs of Claim 1, except the sketch of logical steps. \n- The presentation is too long and can be improved for better readability. For example, authors can hide all intermediate quantities and directly state ultimate results, or use an informal version in the main text and leave the rigorous long version in appendices. \n6. Again, this work can be further strengthened by both stressing theoretical insights and discussing practical implications. How can we apply the derived theory here to guide applications?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XNI9WtIOjR", "forum": "JplcEHcrLt", "replyto": "JplcEHcrLt", "signatures": ["ICLR.cc/2026/Conference/Submission11237/Reviewer_bdfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11237/Reviewer_bdfj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854436040, "cdate": 1761854436040, "tmdate": 1762922399388, "mdate": 1762922399388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyze ERM for single-head tied attention under both factorized (tied query and key matrices) and unfactorized settings (their product), deriving sharp asymptotics for training and test errors, interpolation thresholds, and the spectral distribution of learned weights, with emphasis on the inductive bias from weight decay."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The results are strong. The authors derive sharp asymptotics for test and training error of the global minimizer (depending on sample ratio $\\alpha$ and rank ratios $\\kappa$, $\\kappa_0$) with experimental verification. The theoretical predictions align well with experimental results across various settings."}, "weaknesses": {"value": "1. The neural network architecture is simplified (one-layer, tied query and key weights, no value matrix), which limits direct applicability to practical transformers.\n2. Claim 1 is difficult to fully understand for reviewers outside the field. The components ($\\mathcal{M}$, $J$ etc.) lack intuitive interpretation, making the main result somewhat opaque.\n3. The paper analyzes only the global minimum of ERM, not the optimization dynamics of gradient methods or whether they converge to the global optimum (though experiments suggest they do)."}, "questions": {"value": "1. Can the authors explain why tied attention was chosen instead of the original untied formulation? What are the main technical difficulties in analyzing untied attention?\n2. Can the authors provide a brief intuitive interpretation of Claim 1? What do the order parameters M and J represent, and what is their role in the optimization objective?\n3. What is the main practical takeaway from this work? Does the analysis provide guidance for practitioners, such as how to choose the weight decay parameter $\\lambda$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cVIeTGb533", "forum": "JplcEHcrLt", "replyto": "JplcEHcrLt", "signatures": ["ICLR.cc/2026/Conference/Submission11237/Reviewer_LLsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11237/Reviewer_LLsC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896294666, "cdate": 1761896294666, "tmdate": 1762922398845, "mdate": 1762922398845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies empirical risk minimization ERM in a single-head tied-attention layer in the high-dimensional regime, trained on synthetic sequence tasks modeled using the recently proposed attention-indexed model. Leveraging random matrix theory tools, spin-glass methods, and approximate message passing technique, the authors derive sharp asymptotics for training and test errors, and thresholds for interpolation and perfect recovery. They show how weight decay induces an implicit nuclear-norm regularization promoting low-rank structure, and characterize the spectral properties of the learned weights. They compare standard factorized key-query vs. merged parameterizations, showing how the former generalizes better."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper is highly theoretical and adds to the line of work on training dynamics of attention models. The main strength is extending the line of work on Gaussian multi-index models to study neural networks to better understand attention models, by leveraging the recently developed attention index models. This allows considering both classification and next-token prediction under similar frameworks."}, "weaknesses": {"value": "### 1. Missing references to several related works:\n\nA major weakness is that the paper is missing references to some highly relevant works [1-9] on optimization dynamics and inductive bias of (self-)attention-based models (except [6] which considers fixed attention weights), for classification and next-token prediction settings. The authors should add these and contextualize their contributions taking them into account. \n\nSpecifically, the connection of GD optimization of single-head attention with implicit nuclear-norm regularization is not novel; [2] shows this connection when optimizing the key-query matrices separately with a fixed linear decoder, which explains the low-rank bias seen in practice (see section 2.1 in [2]). These works [1,2,4,5] also explain practical observations about trained attention maps being sparse due to softmax saturation. \n\nAdditionally, while the question of gradient-based methods to reach global minima for attention optimization for the setting of the paper is new, as discussed in line 378, it should be contextualized with respect to other works, e.g., [5], which study global convergence for attention optimization in a different setting which is still non-convex.\n\nAdditionally, there are also several works that study benign overfitting and training dynamics of in-context learning (ICL) in attention-ba models, which I encourage the authors to check and appropriately cite and discuss, perhaps in a related work section in the appendix. In particular, note [10], which studies ICL of linear regression in a simplified multi-head linear self-attention model, comparing the dynamics of optimizing merged key-query matrices versus optimizing them separately. \n\n### 2. The proof of the main claim seems incomplete.\n\nCondition 10 in the paper is not shown theoretically, and only verified empirically, as stated in lines 333-337. Can the authors clarify what the technical difficulty to prove this is, and whether any prior works have made similar simplifications? \n\n**References:**\n\n[1] Tarzanagh et al., “Max-Margin Token Selection in Attention Mechanism”, NeurIPS 2023.\n\n[2] Tarzanagh et al., “Transformers as Support Vector Machines”, arxiv 2023.\n\n[3] Tian et al., “Scan and snap: Understanding training dynamics and token composition in 1-layer transformer”, NeurIPS 2023.\n\n[4] Tian et al., “Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention”, ICLR 2024.\n\n[5] Vasudeva et al., “Implicit Bias and Fast Convergence Rates for Self-attention”, TMLR 2025.\n\n[6] Thrampoulidis, “Implicit Optimization Bias of Next-token Prediction in Linear Models”, NeurIPS 2024.\n\n[7] Huang et al, “Non-asymptotic Convergence of Training Transformers for Next-token Prediction”, NeurIPS 2024.\n\n[8] Sheen et al., “Implicit Regularization of Gradient Flow on One-Layer Softmax Attention”, arxiv 2024.\n\n[9] Li et al, “On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent”, ICLR 2025.\n\n[10] Zhang et al., “Training Dynamics of In-Context Learning in Linear Attention”, ICML 2025."}, "questions": {"value": "Please see the weaknesses section above. Some other minor suggestions for improvement:\n\n1. In some instances, \\citep and \\citet are not used properly, e.g., line 62.\n\n2. In some places, ERM is used directly to abbreviate empirical risk minimization, whereas sometimes, the full form is used later on."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XcJGKdKZ1g", "forum": "JplcEHcrLt", "replyto": "JplcEHcrLt", "signatures": ["ICLR.cc/2026/Conference/Submission11237/Reviewer_gcWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11237/Reviewer_gcWx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041204494, "cdate": 1762041204494, "tmdate": 1762922398439, "mdate": 1762922398439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study the low-rank  bias of weight matrices in attention layer, in a controlled experimental settings: a single-head tied-attention trained on synthetic high-dimensional sequence-to-sequence and sequence-to-label tasks.  Authors used the RMT-tools (Marchenko-Pastur laws) to demonstrate the  bulk-outlier dynamics, and  discovered spectral laws of learned weights (in attention layers). \n\nThe findings of this papers complements the observations made in the prior work demonstrating the natural  preference for low-dimensional learning, in the context of factorized vs non-factorized $QK$-parameterization. Overall, the theoretical analysis seems aligned with the empirical evidence on the spectral behavior of attention mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Studying the spectral dynamics of weight matrices in Transformer architectures, particularly the attention layers, is an interesting and emerging research area, which is crucial for understanding the impact of architectural choices on the representation dynamics. \n\n2. This work adopts a theory-first approach, which offers a more clearer understanding of how architectural design choices affect representation learning, and complements the recent empirical studies on low-rank preference and inductive biases in Transformer-based architectures. \n\n3. The authors have provided solid theoretical justifications for the spectral laws of learned weights and inductive biased from learned (factorized vs non-factorized) $QK$-weights."}, "weaknesses": {"value": "1. This paper does not provide any significant conceptual insights into the representation dynamics of attention weight matrix.   Prior work has already discussed the low-dimensional preference of learned representation, and the spectral dynamics of  $QK$ weight matrices in a practical multi-layer LLMs [1,2,3]. \n\n\n2. The practical utility of the demonstrated inductive bias of un-factorized $QK$-projections is not clear, as almost all the Transformer architectures use factorized $QK$-projection. \n\n3. The lack of empirical power-law relationship, which is typically expected in the tails of the learned weight spectrum, limits the completeness of the spectral analysis and their utility for introducing suitable inductive bias in the existing Transformer Architecture.  \n\n\n[1] Dandi et al., A random matrix theory perspective on the spectrum of learned features and asymptotic generalization\ncapabilities,  AISTATS 2025\n\n[2] Bao et al., self-attention networks localize when QK-eigenspectrum concentrates, ICML 2024\n\n[3] Jha et al., A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention, HiLD workshop ICML 2025"}, "questions": {"value": "1. What sort of power-laws (mentioned in the L#483) can author demonstrate  form the $QK$-weight spectrum? And how does it helps practitioners and  researchers to make informed architectural choices? \n\n2. Can the authors discuss whether a power-law relationship is expected to emerge in the spectrum of attention weight matrices, similar to the scaling behavior observed in the FFN intermediate representations reported in [1]?\n\n\n3. What types of inductive-biased or learning constraints can improve the tail-behavior of attention weight matrices? For example, learning weights on some particular geometrical shape ?\n\n\n\n[1] Jha et al., Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?, EMNLP 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PzOPCG6gat", "forum": "JplcEHcrLt", "replyto": "JplcEHcrLt", "signatures": ["ICLR.cc/2026/Conference/Submission11237/Reviewer_V8P3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11237/Reviewer_V8P3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329409051, "cdate": 1762329409051, "tmdate": 1762922397897, "mdate": 1762922397897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}