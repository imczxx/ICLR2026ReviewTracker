{"id": "LQ1qYDOk6o", "number": 22685, "cdate": 1758334443961, "mdate": 1759896852361, "content": {"title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration", "abstract": "Large language models (LLMs) have demonstrated remarkable performance, but their long-context reasoning remains constrained by the excessive memory required for the Key-Value (KV) cache. This makes KV cache compression a critical step toward efficient long-context inference. Recent methods have explored low-rank techniques to reduce the hidden size of the KV cache. However, they neglect the distinct roles and varying importance of Keys and Values, leading to significant performance drops under high compression. To address this, we propose ReCalKV, a post-training low-rank KV cache compression approach with tailored strategies for Keys and Values. For Keys, we propose Head-wise Similarity–aware Reordering (HSR), which clusters structurally similar heads into groups, enabling more accurate low-rank approximation via grouped SVD. For Values, we propose Offline Value Calibration (OVC), which efficiently calibrates the value projection matrix using calibration data without training, ensuring an accurate representation of contextual information. Extensive experiments show that ReCalKV consistently outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. We will release all the code and models.", "tldr": "ReCalKV applies post-training low-rank compression with separate strategies for Keys and Values, enabling accurate long-context inference under high KV cache compression.", "keywords": ["LLM", "KV Cache Compression", "low-rank approximation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d54282eeae75a89127f76700f5251c6c83cd6abe.pdf", "supplementary_material": "/attachment/4ee7aea063aa749c7764f348695f20fe934fbdac.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of high memory and bandwidth overhead caused by the KV cache in LLM during long-context during. This paper proposes ReCalKV, a post-training low-rank KV cache compression method that introduces differentiated strategies. it applies Head-wise Similarity-aware Reordering to cluster structurally similar attention heads before grouped SVD for Key compression, and Offline Value Calibration to recalibrate Value projection matrices using small calibration datasets. \nThe experimental results demonstrate that ReCalKV achieves consistent improvements over prior low-rank compression baselines （Palu and LoRC). It maintains competitive perplexity and zero-shot accuracy under 50–70% KV-cache compression ratios on models including LLaMA-2-7B, Mistral-7B, and LongChat-7B. However, all experiments are conducted on relatively older architectures, and no evaluation is reported on Llama-3 or Qwen-3."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and practical problem—reducing KV-cache memory overhead for long-context LLM inference, which remains a key bottleneck for efficient deployment.\n2. The proposed approach is model-agnostic and can be readily applied to various Transformer architectures without retraining, showing potential for integration into large-scale serving systems."}, "weaknesses": {"value": "1. The reported experimental performance, while better than earlier SVD-based baselines, remains clearly inferior to recent quantization-based methods such as KVQuant and AnTKV, which achieve much lower perplexity under similar even higher compression ratios.\n2. ReCalKV still introduces additional computations for restruct KV using low rank kv (compute with R_k and R_v) during each decoding step. I recommend the authors evaluate latency and accuracy on end-to-end tasks such as AIME.\n3. The evaluation focuses mainly on outdated models (e.g., LLaMA-2, Mistral-7B) and lacks results on modern architectures like LLaMA-3 or Qwen-3, making it difficult to assess real-world relevance."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XJAgf5GTTZ", "forum": "LQ1qYDOk6o", "replyto": "LQ1qYDOk6o", "signatures": ["ICLR.cc/2026/Conference/Submission22685/Reviewer_yTbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22685/Reviewer_yTbf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378112568, "cdate": 1761378112568, "tmdate": 1762942335919, "mdate": 1762942335919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReCalKV, a post-training framework for compressing the Key-Value (KV) cache in large language models (LLMs) by reducing the hidden dimension via low-rank approximations. It proposes asymmetric strategies: Head-wise Similarity-aware Reordering (HSR) for Keys, which reorders and groups attention heads based on Centered Kernel Alignment (CKA) similarity to enable more accurate grouped Singular Value Decomposition (SVD); and Offline Value Calibration (OVC) for Values, which calibrates the decomposed SVD matrices using a small dataset and fuses the right factor into the output projection to eliminate runtime reconstruction overhead. Extensive experiments on LLaMA and Mistral models demonstrate superior perplexity, zero-shot QA accuracy, and long-context performance compared to baselines like Palu, with minimal degradation (e.g., ~2% relative accuracy drop at 50% compression) and compatibility with quantization for higher ratios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* K-side uses CKA-guided head reordering + grouped SVD to share low-rank factors among similar heads (greedy pairing; fixed group size), and V-side uses closed-form offline calibration to minimize projection error on a small calibration set.  \n\n* Matrix fusion folds (R_v) into (W_o), eliminating online reconstruction and avoiding extra inference ops; the end-to-end procedure is fully post-training/offline (Algorithm 1).  \n\n* Strong ablations isolate HSR and OVC and show they are complementary at fixed compression (Table 3).  \n\n* Evaluations span multiple model families and tasks, plus quantization compatibility (3–4-bit) demonstrating orthogonality to per-token KV quantization (Table 4).  \n\n* Figures 2–3 make the reordering/grouped-SVD mechanism concrete; Algorithm 1 spells out the pipeline; equations (9–11) specify the fused-inference path.  \n\n* Targets a real deployment bottleneck (KV memory/latency) with demonstrable inference efficiency improvements on long contexts, while remaining compatible with common compression stacks."}, "weaknesses": {"value": "- Code not provided, therefore it's not reproducible as is.\n- While the method is effective, baselines are limited primarily to Palu (G-LRD), lacking comparisons with recent variants like CommonKV or FDC, which could better substantiate SOTA claims (section 4).\n- Experiments do not quantify runtime overhead from Key reconstruction post-HSR (Figure 3), despite claims of low cost; real-world latency measurements on diverse hardware would strengthen efficiency arguments (Figure 4). \n- Equations (7) and (8) for OVC appear to have typos in transposes and do not explicitly state assumptions (e.g., whitening) needed for the closed forms."}, "questions": {"value": "- Please address the items mentioned under Weaknesses. For example, lack of reproducibility.\n- In section 3.3 (lines 216-269), the OVC calibration uses equations (7) and (8) with a small dataset X (256 WikiText2 samples; section 4.1). How sensitive is performance to the size and domain of X? Could you provide perplexity results on WikiText2 for LLaMA-2-7B at 50% compression using 128 vs. 512 samples, or a different domain like C4?\n- Section 3.2 describes HSR as greedy grouping based on the CKA similarity matrix S (Eq. 5) with a fixed group size (e.g., 4 heads per group when  h=32). Table 3 shows that at 80% compression, HSR+OVC attains 8.48 perplexity on WikiText-2. What is the effect of the HSR group size s on performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "duG3AKGrTD", "forum": "LQ1qYDOk6o", "replyto": "LQ1qYDOk6o", "signatures": ["ICLR.cc/2026/Conference/Submission22685/Reviewer_5Foc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22685/Reviewer_5Foc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878271654, "cdate": 1761878271654, "tmdate": 1762942335691, "mdate": 1762942335691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReCalKV, a post-training framework for low-rank KV cache compression that treats Keys and Values separately. It enhances Key approximation  through similarity-based head grouping and decomposition and refines Value through lightweight calibration and fusion. Experiments demonstrate that ReCalKV consistently outperforms prior methods under high compression rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies and analyzes the asymmetric roles of Keys and Values, particularly emphasizing that individual attention heads differ in information content. Using CKA-based head reordering before SVD to minimize approximation error is a well-motivated and conceptually sound idea.\n\n2. Across multiple model families and compression ratios, ReCalKV demonstrates competitive or superior results compared with the main low-rank baseline (Palu). The method maintains high accuracy even under aggressive compression and shows compatibility with quantization."}, "weaknesses": {"value": "1. The paper mainly compares with low-rank SVD-based approaches such as Palu, but lacks comparisons with other classes of KV cache compression methods (e.g., KIVI, KVQuant, or token eviction approaches).\nAs a result, the reader cannot fully assess how ReCalKV performs in a broader landscape of KV compression techniques — especially when low-rank compression is not necessarily the only or best strategy.\n\n2. Experiments focus on older LLaMA/Mistral models, with limited evaluation on recent architectures or larger scales. Since the method relies on specific structural properties of attention heads (CKA similarity patterns), it's unclear whether these properties generalize across diverse modern architectures and model scales beyond the tested family.\n\n3. While latency speedups are reported, the computational cost of online head reordering during inference is not quantified separately. The reliance on custom Triton kernels also raises questions about achievability with standard inference frameworks, limiting practical deployment insights."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jrt9sYWkMX", "forum": "LQ1qYDOk6o", "replyto": "LQ1qYDOk6o", "signatures": ["ICLR.cc/2026/Conference/Submission22685/Reviewer_akiT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22685/Reviewer_akiT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927244569, "cdate": 1761927244569, "tmdate": 1762942335484, "mdate": 1762942335484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel low-rank KV cache compression method, building on PaLU, a prior work that decomposes the KV projection matrix using SVD to reduce the dimension of the KV cache. The paper makes the following contributions on top of PaLU:\n- Reordering the key projection matrix to achieve a better SVD decomposition.\n- Refining the low-rank approximation of the value projection matrix using a calibration dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Strong improvements over PaLU on the evaluated models.\n- Comprehensive ablation studies demonstrating the effectiveness of each contribution for both key and value projections."}, "weaknesses": {"value": "**[W1]** The evaluated models are outdated. I suggest moving the results on the Llama-3.1 model to the main body. This is important because many modern LLM architectures employ GQA, while only the Mistral model from the main results section does so. Validation on multiple models with GQA would strengthen the paper.\n\n**[W2]** Comments on writing:\n- *L55–60:* This is not something revealed by your analysis.\n- *L60–63:* I cannot find a section describing your analysis of Fisher information.\n- The fact that whitening is applied before SVD should be discussed earlier in the Methods section, with more detail.\n\nMinor comments that did not affect the score:\n- *L69:* “Offline Calibration Value” --> “Offline Value Calibration”"}, "questions": {"value": "**[Q1]** Is there a reason why offline calibration is applied only to the value projection matrices? Could this also be applied to the key projection matrices?\n\n**[Q2]** How effective is the proposed method in terms of the memory–accuracy trade-off compared to other KV cache compression methods beyond those based on SVD of the projection matrices?\n\n**[Q3]** How would the method perform for reasoning models such as the Qwen3 model family on long generation tasks like AIME or LiveCodeBench? Demonstrating this would highlight the method’s robustness under long-generation scenarios, which are not captured by perplexity or long-context retrieval tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S7HISFRC1h", "forum": "LQ1qYDOk6o", "replyto": "LQ1qYDOk6o", "signatures": ["ICLR.cc/2026/Conference/Submission22685/Reviewer_rgUM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22685/Reviewer_rgUM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966147833, "cdate": 1761966147833, "tmdate": 1762942335198, "mdate": 1762942335198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}