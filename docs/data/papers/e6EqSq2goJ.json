{"id": "e6EqSq2goJ", "number": 19538, "cdate": 1758297049824, "mdate": 1759897033953, "content": {"title": "MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Human Elicitation", "abstract": "Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g.,  objects involved, human goals/intents, and their impacts on planning -- thus leading to knowledge gaps in joint planning. In this paper, we consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent’s elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT’s reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.", "tldr": "", "keywords": ["human-in-the-loop", "human-AI", "LLM planning", "decision tree", "neurosymbolic", "reinforcement learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2b60a74304105d747e4d276be03c7dc990da0c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies human-AI joint planning when the agent lacks key task facts. It formalizes a “knowledge gap” as a neat set of feasible task descriptors that induce a family of MDPs. The method, MINT, builds a small tree over these gaps, scores each node by estimated impact on the optimal action, and periodically asks the human a single yes/no question that prunes the tree. Questions are curated by an LLM to be concise and action-relevant. There is a local continuity bound that links smaller gaps to smaller value differences. Experiments on MiniGrid, a modified Atari setting, and an Isaac-Gym rescue scenario show strong returns with very few queries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and elegant problem framing for planning-time human queries rather than training-time feedback. Binary questions keep human effort minimal while still moving the plan.\n- Concrete gap representation and a simple tree procedure that is easy to follow and implement.\n- Sensible query trigger + theoretical support: only ask when the current uncertainty can change the action and a pseudo-metric on MDPs and a continuity style bound that justifies stopping early.\n- Empirics show near-expert performance with tiny query budgets on diverse tasks. And ablations around tree growth and stop rules are interpretable for practitioners."}, "weaknesses": {"value": "- The gap parameterization seems hand-crafted. It is unclear how to scale the type-subtype-value scheme when unknowns are high dimensional or compositional.\n- The uncertainty signal relies on Q-value variance from a bootstrapped policy. There is limited analysis of calibration, failure modes, or alternatives.\n- LLM involvement in merging and question wording may add prompt sensitivity and non-determinism. Limited study of how this affects later plans. Human factors are not deeply evaluated. There is no user study on cognitive load, latency, or error rates when humans answer quickly or incorrectly (or in other words, what if human made mistakes).\n- Detection of “uncertain objects” and the mapping from raw observations to gap descriptors is under-specified for real robotics or vision-heavy tasks."}, "questions": {"value": "- Why tree structures are chosen for this work? Can we extend it to more generalizable representations like graphs?\n- How robust is performance if the human sometimes answers incorrectly or says “I do not know”? Can MINT detect and recover from inconsistent answers?\n- Can you replace the LLM merging and question curation with a rule-based variant and report deltas, so we know how much the LLM contributes? In other words, I personally think it would be better if the usage of LLMs is justified and tested.\n- How do you construct gap descriptors from raw observations in more complex domains without oracle metadata? Can the descriptor be learned rather than hand-typed?\n- What happens when there are multiple interacting unknowns that affect the same action choice? Does the tree branch order bias the questions and outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QXvyxsh1Vf", "forum": "e6EqSq2goJ", "replyto": "e6EqSq2goJ", "signatures": ["ICLR.cc/2026/Conference/Submission19538/Reviewer_fY71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19538/Reviewer_fY71"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761335899078, "cdate": 1761335899078, "tmdate": 1762931425439, "mdate": 1762931425439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work focuses on human-AI collaboration, where incomplete information is presented in planning in the real world. To address this knowledge gap, the work first trains a DQN to quantify the uncertainty of selecting consistent actions under different level of knowledge gaps. When the uncertainty exceeds certain threshold, the authors propose to construct a tree of knowledge gap, where each child node is a valid subset of knowledge space. The branching is controlled via LLM, with pruning and generate human query. Human will answer the question to narrow the space of knowledge gap and the search is stopped when the action uncertainty is acceptable. Through experiment, demosntrate strong performance over baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Strong Motivation: incomplete information, long-standing challenge\n\n* Principled Elicitation Trigger: The strength lies in the formalization of when to ask for human help. Using the Q-value variance over the knowledge gap ($\\sigma_u^2(s)$) is a sound way to quantify the policy's uncertainty. It allows the agent to trigger human interaction only when the uncertainty is high enough to impact decision-making, rather than querying at every step or based on simple heuristics.\n\n* Novel Neuro-Symbolic Combination: The idea of combining a neural policy (for value estimation) with a symbolic tree (for knowledge representation) is interesting. Using an LLM to bridge the gap between this symbolic tree and a natural language query for the human is a novel pipeline."}, "weaknesses": {"value": "* Lack of Baselines: The baselines of Pure LLM fails to serve as a planner, which totally makes sense. The baseline of \"Query-A\" queries for an expert action but not for information, which can be viewed as an ablation of MINT module. The core of MINT is its ability to actively elicit information to resolve \"knowledge gaps\". Therefore, the evaluation is missing a crucial comparison to the most relevant body of work: agents that actively seek information (not expert actions) to resolve model uncertainty, particularly regarding unknown rewards or transitions. Works on active preference elicitation (e.g., Handa et al., 2024) and active queries in RLHF (e.g., Ji et al., 2024) represent a more appropriate set of baselines. Without these, it is difficult to assess the novelty and efficiency of MINT's specific query-generation strategy.\n\n  * Handa, Kunal, et al. \"Bayesian preference elicitation with language models.\" arXiv preprint arXiv:2403.05534 (2024).\n  * Ji, Kaixuan, Jiafan He, and Quanquan Gu. \"Reinforcement learning from human feedback with active queries.\" arXiv preprint arXiv:2402.09401 (2024).\n\n* Simulated Human: For a paper on \"human-AI teaming,\" the experimental setup excludes the human entirely. The paper states, \"In our experiments, we use another LLM with full knowledge to automatically generate the yes/no answers\" (line 321). This totally makes sense for simple cases, but requires more emphasis, as it invalidates many of the core challenges of this field (human vagueness, incorrect answers, cognitive load, query phrasing) are all ignored.\n\n* The \"Grounding\" Problem: The paper misses a critical link in its pipeline and experiment setup: grounding a sensory input (e.g., \"raw pixels\" in Atari) to a symbolic knowledge gap $u$. The MINT process is supposedly triggered by \"an uncertain object detected by the perception module\" (line 240), which implies a separate, unexplained perception system. This system must identify the sole source of the policy's high variance and map it to the abstract root node $u_0 = \\langle \\text{any, any, 0, 1} \\rangle$."}, "questions": {"value": "* The Grounding Problem: Following Weakness, can the authors please explain the mechanism that connects a specific sensory input (e.g., a \"blue block\" in MiniGrid) to the triggering of the MINT process? Specifically,how does the system: (1) map high Q-value variance back to a specific sensory input (2) instantiate the abstract root node $u_0$ to correspond to that specific object? How does the system handle cases with multiple unknown objects, and how is variance attributed?\n\n* Definition of $u$: Following the grounding problem, how is the space of $u$ cosntructed, is it predefined for each experiment? The paper's expansion logic (line 294-305) seems to assume these dimensions are orthogonal. What is the justification for this assumption? How would MINT handle a single unknown object where, for example, the impact on reward and transition is intercorrelated?\n\n* Lack of Experiment Details: what is the convergence of bootstrapped DQN, as it learns across a wide distribution of different knowledge gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LhOlkMJ5Jg", "forum": "e6EqSq2goJ", "replyto": "e6EqSq2goJ", "signatures": ["ICLR.cc/2026/Conference/Submission19538/Reviewer_6Xak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19538/Reviewer_6Xak"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865289235, "cdate": 1761865289235, "tmdate": 1762931425082, "mdate": 1762931425082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MINT, which is a method for human-in-the-loop planning in environments with unknowns and incomplete information. The method aims to discover strategies for optimally eliciting human inputs to aid in planning. It learns a neural planning policy to estimate uncertainty and uses an LLM to create queries that elicit human input to optimize performance. The method performs very well in a set of benchmark environments with unseen and unknown objects with a limited number of human queries."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper frames the problem of human-in-the-loop very clearly. It addresses the practical challenge of determining what to ask and when to ask in human-AI planning, and solves this practical challenge in a novel way.\n- The formalization with a knowledge gap and a descriptor space is elegant and makes a lot of sense. Comparing the estimated Q-gap to the variance at the node to determine when to query for more information is a nice idea.\n- Using an LLM to craft binary questions that maximize information gain is a sound way to involve an LLM in the planning procedure.\n- The results on the Isaac Gym benchmark are impressive, and the proposed MINT* clearly outperforms the LLM baselines used."}, "weaknesses": {"value": "- Could you please clarify the inequality $\\max_a \\Delta_{s_0,a} (M_{\\phi*}, M_{\\phi_1}) \\le \\max_a \\Delta_{s_0,a} (M_{\\phi_1}, M_{\\phi_2})$ in the proof of Theorem A.4?  For instance, in the case of $\\gamma = 0$, this seems only to hold if the interpolation $\\phi^* = \\lambda\\phi_1 + (1-\\lambda)\\phi_2$ also implies an interpolation in the MDP space (or reward function, to be precise). Wouldn't you need additional assumptions about the behavior of the reward functions to make this claim in general? For $\\gamma > 0$, the symmetrization of two asymmetric divergences with the minimum does not, in general, satisfy the triangle inequality, which could be an additional problem.\n- The discussion surrounding the aleatoric and epistemic uncertainty could be clarified. Typically, aleatoric uncertainty refers to uncertainty that cannot be reduced by collecting more data. Hence, when we ask for information from humans, we're trying to reduce the epistemic uncertainty. However, the authors state (L259) that the paper focuses on aleatoric uncertainty. Then, the aleatoric uncertainty estimation from UA-DQN is used (L267). However, $\\phi$ (or $\\Phi_u$) is used as input to $Q$. Hence, the aleatoric estimate from UA-DQN might actually incorporate variance from the uncertainty associated with the knowledge gap and some model-weight variation, i.e., it correlates with epistemic uncertainty, which most likely enables the method to work. If you examine Table 6 in the appendix, the uncertainty declines as you ask questions, which, to me, seems to be epistemic uncertainty reduction by definition.\n- The method assumes full observability of the environment during training, which gives it an advantage over the baseline RL algorithms. On the other hand, MINT relies on RL training to outperform the zero-shot LLM baselines. Table 4 reveals that this method uses 5e7 training episodes, which could make the comparison unfair. Even then, this training only allows us to solve one specific domain. The paper would benefit from an LLM baseline that can either query a human or act, to help understand the benefits of the explicit planning and the UA-DQN formulation. To make his baseline more competitive, it could even be RL-trained (think GRPO or some variant). However, this RL training is likely outside the scope of the rebuttal and would be a valuable contribution on its own.\n- While the MINT* algorithm used for the NVIDIA Isaac task arguably fixes the RL-related weakness of MINT, it is severely underdescribed in the paper. It feels like a conceptually very big leap from the UA-DQN algorithm that the paper discusses.\n- The method assumes that the different types of uncertainty are pre-specified, whereas an LLM could perhaps detect uncertainty on the fly. Furthermore, the method assumes that humans always give correct and complete answers when queried.\n- There's related work the authors might want to acknowledge. Even though they don't involve direct human interaction, they let agents query external sources. [1] learns a cost-aware policy for when to query an LLM for high-level instructions. Additionally, there's work on LLMs that query for additional information to improve reasoning and decision-making [2, 3].\n\n[1] Hu, B., Zhao, C., Zhang, P., Zhou, Z., Yang, Y., Xu, Z., & Liu, B. (2023). Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach. RLC.\n\n[2] Kim, M., Kim, J., Kim, J., & Hwang, S. W. (2024, November). QuBE: Question-based belief enhancement for agentic LLM reasoning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 21403-21423).\n\n[3] Chen, X., Zhang, S., Zhang, P., Zhao, L., & Chen, J. (2023). Asking before acting: Gather information in embodied decision making with language models. arXiv preprint arXiv:2305.15695."}, "questions": {"value": "See above. In general, the method is exciting, and the problem being tackled is very important. However, the authors should clarify the assumptions underlying the proof of Theorem A.4 and the discussion surrounding the types of uncertainty. They should also consider including a naive LLM baseline with the capability to query for information and properly describe the MINT* algorithm. If the authors can clarify/address these issues, I'm willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "W98RlT1ix1", "forum": "e6EqSq2goJ", "replyto": "e6EqSq2goJ", "signatures": ["ICLR.cc/2026/Conference/Submission19538/Reviewer_McDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19538/Reviewer_McDF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951815251, "cdate": 1761951815251, "tmdate": 1762931424627, "mdate": 1762931424627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}