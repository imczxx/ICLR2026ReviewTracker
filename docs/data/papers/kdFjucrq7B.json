{"id": "kdFjucrq7B", "number": 14769, "cdate": 1758243340732, "mdate": 1759897350295, "content": {"title": "LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning", "abstract": "While Large Language Models (LLMs) excel in general domains, their reliability often falls short in scientific problem-solving. The advancement of scientific AI depends on large-scale, high-quality corpora. However, existing scientific question-answering (QA) datasets suffer from high error rates, frequently resulting from logical leaps and implicit reasoning within the answers. To address this issue, we introduce LOCA (Logical Chain Augmentation), a novel framework for automatically cleaning scientific corpora, implemented through an augment-and-review loop. At its core, LOCA enhances raw answers by completing missing logical steps and explicitly separating the underlying scientific principle from its subsequent derivation. By applying LOCA to challenging scientific corpora, we demonstrate that it can automatically filter noisy datasets, typically reducing the error rate from as high as 20\\% to below 2\\%. LOCA provides a scalable and effective methodology for creating high-quality scientific corpora, paving the way for more reliable training and evaluation of scientific AI.", "tldr": "", "keywords": ["scientific corpus cleaning", "logical chain", "AI for science", "LLMs"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6adf0ad32849f9012988103f077cd7de25b8d589.pdf", "supplementary_material": "/attachment/9bd9e56c7096711b6b2d9ada26ff81d426ddc305.zip"}, "replies": [{"content": {"summary": {"value": "LOCA proposes an agent-based workflow for quality filtering of scientific QA data (in practice only physics QA is used). The method consists of three main components:\n1. Logic-chain enhancement: Complete the implicit reasoning steps in the original answer and decompose each step into a pair: (Principle, Derivation).\n2. Iterative review: Two reviewers (agents) independently verify the principle and derivation of each step. A QA pair is accepted if it passes three consecutive review rounds; conversely, if it accumulates five failures, it is discarded.\n3. Final check: The enhanced final answer must remain identical to the original answer.\nExperiments demonstrate a significant reduction in residual error rate on multiple physics QA datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of augmenting hidden reasoning chains is novel and interesting.\n- On three physics QA datasets the method shows strong empirical improvement.\n- The (Principle, Derivation) pairing naturally matches reasoning patterns in scientific problems and provides a clear structure for human auditing."}, "weaknesses": {"value": "- Although the title claims to target scientific corpora, the evaluation is limited to physics, leaving generalization to other domains unverified.\n- Using a single LLM for both enhancement and reviewing may introduce self-bias and may partially explain the advantage observed under evaluation with Gemini 2.5 Pro.\n- The contribution is more of a engineering design rather than algorithmic innovation."}, "questions": {"value": "- Consider validating whether the cleaned dataset further improves downstream model training quality.\n- Consider evaluating variants that incorporate multiple distinct models into the enhancement/review loop to avoid single-model bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uXkCp1SaVN", "forum": "kdFjucrq7B", "replyto": "kdFjucrq7B", "signatures": ["ICLR.cc/2026/Conference/Submission14769/Reviewer_dSxn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14769/Reviewer_dSxn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289049442, "cdate": 1761289049442, "tmdate": 1762925122693, "mdate": 1762925122693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LOCA (Logical Chain Augmentation), a framework for automatically cleaning scientific corpora by addressing logical incompleteness in reasoning chains. The method employs an augment-and-review loop that (1) completes missing logical steps through chain completion, (2) decomposes each step into orthogonal principle and derivation components, and (3) uses specialized review agents to iteratively refine solutions. Applied to physics QA datasets (PHYBench, PHYSICS, ABench-Physics), LOCA reportedly reduces error rates from ~20% to below 2% while maintaining substantial dataset size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a genuine problem—high error rates in scientific reasoning datasets—with a structured approach. The principle-derivation decomposition provides interpretable intermediate representations that could facilitate both automated review and human verification.\n\n2. The evaluation compares against diverse baselines spanning reasoning methods (CoT, ToT, GoT), review methods (Review-SC), and iterative refinement (Self-Reflection) across multiple LLMs. The detailed example in Appendix A.2 illustrates how the method processes a physics problem through multiple iterations."}, "weaknesses": {"value": "1. Ground truth is created by experts using LOCA's structured outputs to identify errors, then LOCA is evaluated against these same errors. This makes the <2% error rate claim unverifiable.\n\n2. Only 300 questions evaluated across three datasets, far too limited for a method claiming to enable \"large-scale, high-quality scientific corpora.\" No computational cost analysis provided despite requiring up to 8 LLM calls per question.\n\n3. Please provide Cohen's kappa across multiple independent experts for error identification to validate ground truth reliability.\n\n4. Bottom margins appear excessively wide throughout, resulting in significantly less content per page than standard ICLR submissions. Authors must verify compliance with official ICLR 2026 template."}, "questions": {"value": "1. What are computational costs? Report: (a) average LLM calls per question, (b) total tokens processed, (c) wall-clock time, (d) cost per 1000 questions, (e) comparison with baselines.\n\n2. What are hyperparameter sensitivities? Show how error rate and accepted set size vary across N_{corr}^{(max}} ∈ {1,2,3,4,5} and N_{wrg}^{(max)} ∈ {1,3,5,7,10}.\n\n3. How does the system determine when a step is \"non-atomic\"? How are principles identified from axiom space P? How is semantic equivalence determined for external consistency checks?\n\n4. Can you confirm template compliance? The bottom margins appear non-standard. Verify the manuscript uses the official ICLR 2026 LaTeX template without modifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aR190QpKo2", "forum": "kdFjucrq7B", "replyto": "kdFjucrq7B", "signatures": ["ICLR.cc/2026/Conference/Submission14769/Reviewer_ro15"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14769/Reviewer_ro15"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856168985, "cdate": 1761856168985, "tmdate": 1762925122381, "mdate": 1762925122381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LOCA, a framework for automatically cleaning scientific QA corpora through logical chain augmentation. LOCA reconstructs reasoning by inserting missing logical steps and separating each step into principles and derivations. Experiments on three physics benchmarks show that LOCA reduces corpus error rates from around 20% to below 2%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "LOCA identifies an  issue (logical incompleteness of solution) in problem-solving benchmarks and proposes a solution to tackle this. \n\nAuthors show empirical results across PHYBench, PHYSICS, and ABench-Physics. It shows that LOCA can reduce the residual error rate compared to baselines."}, "weaknesses": {"value": "- The framework mainly applies to problem-solving questions in derivation-heavy domains like physics or mathematics, while the title and introduction claim a broader scope across scientific domains. This seems inaccurate since corpora in other domains can contain other issues such as factual errors, unclear writing, or formatting problems. Clarifying the scope would make the paper more accurate.  \n- The definition of “error” appears to focus on logical incompleteness rather than final-answer correctness. In my opinion, it is somewhat unclear whether a solution with a correct final answer but missing intermediate steps should be considered erroneous. The paper would benefit from clarifying what kinds of reasoning flaws LOCA is designed to detect, and why such augmentation matters if a knowledgeable reader could easily fill in those steps.  \n- Evaluation benchmarks are limited to physics. Including results from other scientific fields, such as math, would strengthen the generalizability of LOCA.  \n- The iterative review loop depends on LLM judgment. However the paper lacks analysis of the reliability of these LLM-based reviewers. The paper would benefit from some human evaluation of review quality.\n- The dataset used for experiments is relatively small: only 100 questions per benchmark are sampled from larger datasets (e.g., PHYBench with 500 problems, PHYSICS with 1297).  \n- Hyperparameter choices (Ncorr, Nwrg) are not justified or analyzed for sensitivity."}, "questions": {"value": "1. How sensitive are results to the number of review iterations (Ncorr, Nwrg)? How they chosen in the paper?\n2. In lines 38–39, the authors state that “our own expert analysis reveals that error rates in major benchmarks’ QA pairs can exceed 20%.” How is this “error” defined? What criteria or guidelines were provided to the experts? Does it include both incorrect answers (as shown in the appendix) and logically incomplete solutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OnDPE9OcqM", "forum": "kdFjucrq7B", "replyto": "kdFjucrq7B", "signatures": ["ICLR.cc/2026/Conference/Submission14769/Reviewer_Vcy6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14769/Reviewer_Vcy6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971642094, "cdate": 1761971642094, "tmdate": 1762925121859, "mdate": 1762925121859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "(I am relatively confused by the contribution of this paper, but I will try my best to summarize the work given my understanding) \n\nThis paper proposes an agentic pipeline, LOCA, that can clean language model responses for solving some physics problems: given a generated reasoning paragraph, LOCA breaks it down or rewrites the original generation into atomic reasoning units, conducts logical reasoning, and then rejects examples that fail to satisfy the logical constraints. The authors conduct some evaluation of the proposed agent pipeline, and claim that it can be used to clean datasets (without any experimental results supporting that)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Unfortunately I cannot say I am excited about specific aspects of this paper, since I am very confused about the paper’s presentation (see my comments in the weakness section below)."}, "weaknesses": {"value": "I think my confusion comes from the mismatch between the experiments as well as the goal/claim of this paper. \n1. If I understand correctly, the authors aim to develop a pipeline to clean existing (training) corpus for scientific use cases. Therefore, one way to validate this idea seems to be (1) build a method (2) validate the correctness of the cleaning method and (3) apply the method to clean some corpus, and show the usefulness (in terms of the improved accuracy when applying the trained models). However, the paper only has (1) and (2), but not (3). The author only uses this agentic framework to clean answers for a small-scale evaluation dataset, and uses the residual error rate to demonstrate its effectiveness. There’s nothing about (3) in the paper. \n2. And even for (2), I don’t think the current evaluation is sufficient: It uses the residual error rate and retention corpus size as the evaluation metrics. The authors mentioned that they’ve already used a LOCA filtered subset (line 278) for this experiment – as such, there’s significant bias in the reported numbers."}, "questions": {"value": "How is “residual error rate” defined? Does it mean the error rate in the filter examples? Please define it properly in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "quhl5nR60q", "forum": "kdFjucrq7B", "replyto": "kdFjucrq7B", "signatures": ["ICLR.cc/2026/Conference/Submission14769/Reviewer_V6rY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14769/Reviewer_V6rY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131942038, "cdate": 1762131942038, "tmdate": 1762925121481, "mdate": 1762925121481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}