{"id": "xa3oLRQG55", "number": 4485, "cdate": 1757687826089, "mdate": 1759898030292, "content": {"title": "Solvable Gradient Flow Through Diagram Expansions", "abstract": "We propose a general diagram-based approach to analyze scaling regimes and obtain explicit analytic solutions for gradient descent evolution in large learning problems.\nWe propose a general diagram-based approach to analyze scaling regimes and obtain explicit analytic solutions for gradient descent evolution in large learning problems.\nWe focus on a class of problems in which an identity tensor is learned by gradient descent starting from a sum of rank-one tensors with random normal weights.\nA central element of our approach is to expand the loss evolution in a formal power series over time. The coefficients of this expansion can be described in terms of suitable diagrams akin to Feynman diagrams. \nDepending on the scaling of the initial weight magnitude and the number of parameters, we find several extreme learning regimes, such as NTK, mean-field, under-parameterized learning, and free evolution. \nThese regimes include lazy training as well as strong feature learning. We identify these regimes with extreme points and sides of a hyperparameter polygon. \nWe then show that in some of these regimes, the loss power series satisfies a formal partial differential equation. For certain scenarios, this equation is first order and can be solved by the method of characteristics, producing explicit loss evolution formulas that agree very well with experiment. \nWe give a series of specific examples where this methodology is fully implemented.", "tldr": "We propose a general diagram-based approach to analyze scaling regimes and obtain explicit analytic solutions for gradient descent evolution in large learning problems.", "keywords": ["gradient flow", "tensor decomposition", "feature learning", "analytic solutions", "generating functions", "diagram expansion", "wide networks"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07814c303c6c4cdf31b225c566743b5870d5ef27.pdf", "supplementary_material": "/attachment/f2f67738409338c0ea4a6bf5f7523817d5a094e9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a diagrammatic expansion for analyzing gradient flow learning of order ν identity tensor, from a sum of rank one factors with Gaussian initialization, that yields a formal time power series for the expected loss, whose coefficients are computed by diagram pairings and contractions. This allows to classify scaling regimes, such as NTK, mean field, and no learning, by a hyperparameter polygon, that organizes how the target size p, the model size H, and the initial \\sigma, scale. The series are turned into generating functions, that satisfy first order PDEs, which are solved by characteristics, producing closed form learning curves, such as an explicit loss decay in free evolution and solvable expressions in under and over parameterized cases.  These predictions are compared with certain numerical gradient descent experiments and match closely. Summary: the overall contributions are:  (i) a diagrammatic calculus for gradient flow dynamics with explicit coefficient counting, (ii) the hyperparameter polygon taxonomy linking canonical limits, such as NTK and mean field, and (iii) closed form solutions for loss dynamics that align with experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(i) Analytic framework that combines a formal loss time series with a diagrammatic calculus to get an explicit dynamics of the expected loss for the identity tensor factorization task. In some regimes, the power series can be extended to a multivariate generating function, that obeys a first order PDE, which can be solved in certain cases. (ii) A hyperparameter polygon, that allows for a classification of large scale limits that recover known regimes, such as the NTK and mean field. (iii) Numerical experiments align well with the theory across regimes."}, "weaknesses": {"value": "(i) A narrow task: the theory is demonstrated on learning the identity tensor, (ii) The analysis requires large p,H, while finite width corrections are not characterized,(iii) The theory is for gradient flow, and the numerics use Euler discretization to mimic it, which differs from practical SGD, (iv) The method is not rigorous, the PDEs for the generating functions are derived from formal series, while convergence and error bounds are not proven."}, "questions": {"value": "Suggestions for improvement of the paper: (i) Make the formal method more rigorous by addining the remainder, when going from the formal series to PDE in one regime, which would elevate the method from formal to controlled, (ii) Quantify the effect of dropping non Pareto optimal terms, by providing bounds, or empirical checks, for the size of discarded terms vs. kept terms across polygon regions, (iii) Add an estimate of the leading finite size corrections to the large p and H formulas, e.g. via next to leading order contractions, and compare to numerics. This will narrow the gap between asymptotics and practice.  (iv) Bridge gradient flow and the discrete training by a discretization error analysis.\n(v)  Add, using theory or numerics, an example of a simple non identity structured target."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MZOYjo1HRC", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Reviewer_Cfis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Reviewer_Cfis"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393587288, "cdate": 1761393587288, "tmdate": 1762917392215, "mdate": 1762917392215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider a tensorial generalization of the matrix factorization problem in linear deep learning. \nIn this case, they are able to compute the expected value of the loss during the learning evolution as modeled by gradient flow using Feynman diagram expansion techniques.\nUsing this expansion, they classify the learning regime of the dynamics (lazy, rich, etc.) depending on the variance used to initialize the model parameters as well as the values of other parameters playing the role of model complexity and target size."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The author are able to provide an explicit formula for the expected value expansion for the loss under gradient flow\n* The classification of previously identified learning regimes (lazy, rich, etc.) depending on the model hyper-parameters is very satisfying\n* The use of Feynman expansions in this way, and the general approach is novel to deep learning and mathematically interesting"}, "weaknesses": {"value": "* The highly mathematical technicality of the paper made it hard for me to assess its correctness. (Part of me thinks this paper may be more appropriate for an applied mathematical journal submission.) The exposition is in general could be improved by focusing in the main paper on a simpler case (for instance matrix factorization), while deferring the more general case of tensor factorization to the appendix. \n\n* The possibility of the Feynman expansion technique seems to rely heavily on the non-linearity of the network. It would help if there were some comments on how to extend the method beyond this linear toy model. Is this approach a dead-end beyond linear networks, or can it be applied to more useful settings? Is it possible to comment on this briefly?\n\n* The learning-regime classification depends on the assumption of gradient flow. However, in practice, optimization is done in discrete steps whose size influences the learning regime (higher learning rates have been show to bias the model trajectories to flatter, more generalizable solutions for example). Can the author comment on the possibility of extending their method to practical discrete optimization?"}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wEmj0z1mNW", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Reviewer_GNcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Reviewer_GNcE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428170893, "cdate": 1761428170893, "tmdate": 1762917391876, "mdate": 1762917391876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyze from an asymptotic perspective the gradient flow training dynamics of a tensor approximation problem. Specifically, they consider the problem of approximating in the least squares sense a diagonal identity tensor of order \\nu and dimension p by a tensor of rank at most $H$, and focus on the gradient flow dynamics with i.i.d. random Gaussian initialization of variance $\\sigma$. \nBy exploiting the polynomial nature the loss, they express its time derivatives of any order as polynomials and use so-called \"diagrams\" (that take the form of certain multi-graphs) to provide \"formal expressions\" of the involved polynomials using a so-called \"diagram merging\" operations. They then invoke Wick's theorem and the diagram viewpoint to decompose the expectation of these polynomials (with respect to the Gaussian initialization) into terms expressed as $p^q H^n \\sigma^{2\\ell}$. Invoking diagram arguments that I was not able to follow, they conduct a Pareto-optimal analysis to identify which exponents $q,n,\\ell$ can dominate the other, depending on the relative scaling of $p$, $H$ and $\\sigma$. This Pareto-optimal viewpoint is then exploited to identify scaling regimes, which in turn allow to express an asymptotic partial differential equation leading to explicit \"asymptotic solutions\" in certain regimes."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "It is very welcome to analyze the gradient flow dynamics of large-scale learning problems, and the idea that this can be achieved with certain polynomial loss functions is very attractive. The exploitation of the fact that the time derivatives are also polynomials and that their dominant terms can be identified is also very appealing and interesting.\nIf the results are correct, they do provide some interesting insight on the behavior expected in different scaling regimes (relating the problem dimension $p$, its rank $H$, the initialization scale $\\sigma$, and the learning rate $1/T$)."}, "weaknesses": {"value": "The paper heavily relies on concepts, terminology and viewpoints that seems to require a (statistical) physics background, making it very hard to access for a general reader with a standard machine learning or mathematics background.\nThe use of so-called \"diagrams\", which are certainly not well-known to the average ICLR audience and are very tersely described, makes it particularly difficult to follow the reasoning. I was not able to check whether the results are correct, due to the heavy use of such informal arguments, combined with \"formal expansions\" (where actual convergence analysis of series would also seem necessary) and  \"non-rigorous\" aspects of the proofs in particular when deriving asymptotic PDEs.  These are the main reasons for my grade on \"soundness\" and \"presentation\".\n\nWhile I reckon that the general approach (cf my summary above) *seems* sensible (hence my grade 3 on \"contribution\"),  it would need to be much more clearly and convincingly conveyed by postponing as much as possible the \"diagram\" viewpoint and making it much more explicit via polynomials and lemmas where appropriate. \n\nWhile the title and beginning of the abstract suggest general results on gradient flows for large learning problems, the setting consider in the paper is very specific, focused on very particular tensor optimisation problems with identity target tensor. Despite claims below (3), this seems very far from being representative of general learning problems.\n \nThe paper lacks references to the vast literature on the considered tensor optimization problem, which is known under various names such as Canonical Polyadic Decomposition or PARAFAC decomposition, and is known to lead in general to a number of topological difficulties (for example, a minimizer of the loss does not necessarily exist - see e.g. De Silva & Lim \"Tensor Rank And The Ill-Posedness Of The Best Low-Rank Approximation Problem\"). There is a significant gap between the specific model considered here (in particular, using the identity tensor as a target) and the evoked richness of the \"class of problems\"  below (3).\n\nThe learning rate $1/T$ appearing the gradient flow equation (1) seems superfluous at first sight, since the solution to (1) with $T=1$ yields the solution for general T via a simple time rescaling. It would be helpful to mention this fact while explaining that  $T$ will nevertheless be useful to highlight how certain \"time scales\" appear in the asymptotic analysis."}, "questions": {"value": "Can the approach be expressed without reliance on \"diagrams\" but simply on polynomials, using a notion of \"polynomial merging\" in (7) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "C9HBnwVya4", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Reviewer_aX8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Reviewer_aX8t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645975682, "cdate": 1761645975682, "tmdate": 1762917391457, "mdate": 1762917391457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors use novel analytical techniques to study the dynamics of a gradient flow algorithm on a specific learning task, which can be understood as learning the identity tensor using a sum of rank one components. They show that the loss for this\ntask, when the tensor is order 3, is similar to the population loss obtained for a modular arithmetic task in the Fourier basis, thus arguing that it captures real learning phenomena.\nUsing a diagrammatic approach that mixes tensor networks and Feynman diagrams, they are able to derive closed form expressions for the time evolution of the loss function averaged over the initialisation in some scaling regimes. Furthermore, they systematically\ncatalog different learning phases as a function of the scalings between the number of model parameters, the dimensionality of the target to be learned and the variance of the initialisation of the weights."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The analytical approach used in this paper is, as far as I can tell, very original and unique and it makes the model analytically solvable in some cases.\nThe expansion in time, and the subsequent mix of Feynman diagrams and tensor networks is a hybrid approach which combines techniques that come from different fields of computer science and physics. I particularly like that with this single approach, various scaling regimes for the hyper parameters of the model emerge naturally by looking at the dominating diagrams, establishing connections to well known topics such as NTK, Lazy Regime and Feature Learning, and derive natural scalings for the initialisation variance and learning rate.\nThis is a nice alternative to Dynamical Field Theory for the calculation of the dynamics of gradient descent, and it would be interesting to see if it can be applied for other learning tasks."}, "weaknesses": {"value": "The main weakness is the relevance of the model studied. \nThe connection to modular arithmetic for $\\nu=3$ is interesting, however the link seems vague and very specific. Furthermore, the main interest of such modular arithmetic tasks is\ntypically the phenomenon of Grokking, which is not studied in the paper. \nIt would have been nice to see the time evolution of the learning phases for $\\nu=3$, and perhaps hope to observe the Grokking phenomenon analytically. \nThe time evolution is only obtained for the $\\nu=2$ case, which I can only connect to two layer linear networks, but with the unusual property that no dataset is present in the learning. Even disregarding this, no particular insights on the dynamics are derived. Ultimately it seems that the main feat of this paper is to derive the various scaling regimes, NTK, Mean Field etc..., with a single approach. I\nwonder however if this technique can be used to study other learning models, and perhaps obtain more intuition in the dynamics of learning.\nHowever, the relevance of the studied model seems limited, and no new interesting phenomena is explained using the analytical\nsolution. \nThe importance of the paper seems to be more technical than anything else."}, "questions": {"value": "For the presentation:\nIt would be nice if the notation were more explicitly stated, and quantities listed as vectors, matrices tensors, etc... \nFor example in the first page we read ${\\bf u}=\\\\{u\\\\}$, what does this mean? \nIt seems like a pretty ambiguous notation. \nAlso on page 2, it is stated that the case $\\nu=2$ is equivalent to learning the identity matrix with matrices $UU^T$. What are the dimensions of the matrix $U$? Obviously this can all be derived by the reader, but if you explicitly give the dimensions of the matrix and state how this is connected to a deep linear network it makes it easier to read.\nThe literature review in the appendix is very long, and touches many topics which are only mildly related to the paper. I would suggest shortening it, and maybe inserting some of the most important references in the main text.\nFor the relevance for the paper:\nI wonder if the case $\\nu=2$, for which most of the results are derived, can also be connected to other known learning tasks, as in the case $\\nu=3$. This would give more relevance to the results of this paper. The symmetric case for example seems similar to an a linear autoencoder, can the authors comment on this?\nCan you find some interesting phenomena in your solved trajectories which can be maybe observed in more realistic learning tasks? \nA numerics section would give the paper more concreteness, and show that this diagrammatic method is really explaining some phenomena."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "amp1rBWUQQ", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Reviewer_gZLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Reviewer_gZLv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819409612, "cdate": 1761819409612, "tmdate": 1762917390566, "mdate": 1762917390566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers (Part IV)"}, "comment": {"value": "### Generalization\n\nOur method naturally applies to optimization problems on **finite datasets** and allows one to track both the empirical (**train**) and the distribution (**test**) losses. We list two classes of such problems below.\n\n1. **Modular arithmetic.** For such problems, we could think of one-hot vectors as model inputs, so the training dataset $S$ is nothing but a subset of entries of the target tensor. The $\\\\nu$ corresponding $p$-vertices in the diagrams associated with the training loss are tied together with this training subset; now they sum to $|S|$ together instead of $p^\\\\nu$ when there were no ties. One merges diagrams as before, but keeps the ties induced by the training dataset. Note that the diagrams associated with the test loss do not impose any constraints.\n\n2. **Supervised learning with Gaussian data.**  \n   The simplest example is linear regression with\n   \n   $$L_{\\\\text{train}}(u) = \\\\frac{1}{2N} \\\\left\\\\| X^\\\\top u - X^\\\\top w \\\\right\\\\|_2^2,$$\n   \n   and\n   \n   $$\n   L_{\\\\text{test}}(u)\n   = \\\\frac{1}{2} \\\\mathbb{E}_{x \\\\sim \\\\mathcal{N}(0,I_p)}\\\\left[\\\\left( x^\\\\top u - x^\\\\top w \\\\right)^2\\\\right]\n   = \\frac{1}{2} \\\\left\\\\| u - w \\\\right\\\\|_2^2,\n   $$\n   where $X \\\\in \\\\mathbb{R}^{p \\times N}$ is the training dataset of size $N$ and $u,w \\\\in \\\\mathbb{R}^p$ are model and teacher parameter vectors, respectively. We do not train $X$ and $w$, while sampling their entries independently from $\\\\mathcal{N}(0,1)$. The above losses result in diagrams with three types of nodes, $p$-, $N$-, and output, and three types of edges, $X$-, $u$-, and $w$-. Since we train only the vector $u$, we are allowed to merge diagrams only by $u$-edges, while we pair all same-type edges as before. The same approach generalizes naturally to **linear nets of any depth**.\n\n### Nonlinear models\n\nOur approach is **not restricted to linear models** either: it naturally covers supervised learning problems with models with **polynomial activation functions**. One of the simplest examples of such problems would be learning random Gaussian labels on a random Gaussian dataset with a two-layer network with quadratic activations:\n$$\nL(U,v)\n= \\\\frac{1}{2N} \\\\left\\\\| v^\\\\top (U X)^{\\\\odot 2} - y^\\\\top \\\\right\\\\|_2^2,\n$$\nwhere $X \\\\in \\\\mathbb{R}^{p \\\\times N}$ is a Gaussian dataset of size $N$, $U \\\\in \\\\mathbb{R}^{H \\\\times p}$ and $v \\\\in \\\\mathbb{R}^H$ are model parameters to learn, and $y \\\\in \\\\mathbb{R}^N$ is a vector of random Gaussian targets. We could as well introduce a teacher instead of random targets."}}, "id": "9OWm0YxIwn", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763753960930, "cdate": 1763753960930, "tmdate": 1763753960930, "mdate": 1763753960930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the gradient flow dynamics of learning a generic tensor under squared loss, aiming to provide insights on the gradient descent dynamics in large learning problems. To be specific, the authors consider a standard gradient flow with learning rate $1/T$: $$\\frac{du}{dt} = - \\frac{1}{T} \\partial_u L(u),$$ where the weights of the model $u$ is a tensor. The target is assumed to be the identity tensor, and the loss $L$ is the standard quandratic loss. The main contributions of this paper are as follows: (i) The authors use a technique, called \"diagram calculus\", to computed the expected loss function at time $t$, in order to analyze the gradient flow dynamics. The exact formula derived turns out to be connected to the Feynman diagram; (ii) The contribution of each diagrammatic term in the expansion scales with the problem's parameters. By identifying the corresponding hyperparameter polygon, the paper identifies several scalings of the learning regime, e.g., NTK, mean-field and free evolutions; (iii) The paper also explores several special cases, where the gradient flow has a formal solution gives by power series, or an explicit solution given by the method of characteristics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The use of \"diagram calculus\" to study gradient flow dynamics is interesting and novel, especially it is able to provide explicit, analytical expressions for $\\mathbb{E} [L(t)]$ the expected loss of gradient flow at time $t$. It is also interesting to see that different learning regimes correspond to different \"hyperparameter polygons\" in the diagram."}, "weaknesses": {"value": "I have a few concerns regarding the rigor and generality about the main results of this paper: (i) Lack of rigor. The main theorem (Theorem 1) is a mathematically rigourous derivation of power series expansion of $\\mathbb{E} [L(t)]$. However, the proof of Theorem 2 is only a formal power series derivation rather than a rigorous proof. (ii) Limited generality. The entire theory, although elegent, is only developed for learning a specific tensor model with quadratic loss. It would be better to discuss more examples from deep learning for which the abstract theory from this paper can be applied to investigate the learning behavior. Otherwise, it might be clear why it is interesting to develop this \"diagram calculus\"-based method for studying gradient flow dynamics."}, "questions": {"value": "Below are some specific comments and questions for the authors:\n(i) On the bottom of page 6, partial differential equation is abbreviated as PDF. This also occurs in several other places in the paper, please correct.\n(ii) Some appendix numbers are not properly referenced, e.g., in the statement of Theorem 1.\n(iii) The main results are stated for expected loss, namely $\\mathbb{E} [L(t)]$. Since the randomness here comes from the initialization, which is Gaussian, I was wondering whether there is a high-probability statement for $L(t)$? For example, some Gaussian concentration inequalities might help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "50POSflJbO", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Reviewer_No34"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Reviewer_No34"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044803386, "cdate": 1762044803386, "tmdate": 1762917390280, "mdate": 1762917390280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers (Part III)"}, "comment": {"value": "## Extensions of our theory\n\nWhile in the present manuscript we study only the problem of identity tensor decomposition, the method of diagram expansions we use could be well applied to a much wider class of problems. Below we list several examples of such problems that might be interesting to the Machine Learning community.\n\n### Alternative targets\n\n1. **Our method is by no means restricted to the identity target.** We could as well consider any target that is a Kronecker delta of a system of linear conditions on tensor indices modulo $p$. For example, the problem of summing $\\\\nu - 1$ integers modulo $p$ gives an order-$\\\\nu$ target tensor with indices $i_1,\\ldots,i_\\nu$, with ones at positions satisfying $\\\\sum_{k=1}^{\\\\nu-1} i_k = i_\\\\nu \\\\mod p$ and zeros elsewhere.\n\n   The corresponding loss function still induces two diagrams: one, denoted by $D_{2\\\\nu}$, is “free” — it encodes interactions of the model with itself but not with the target. The other one, denoted by $R_\\\\nu$, encodes interactions of the model with the target. The free diagram stays the same as for the identity target, while the interacting one changes: now it consists of $\\\\nu$ distinct $p$-vertices tied with a condition.\n\n   We proceed as before: we merge subsequent diagrams by an edge so that endpoints match. We then pair the remaining edges. The conditions remain and define how the remaining $p$-nodes should be summed: they sum into their number minus the rank of the system of conditions.\n\n2. Apart from zero-one target tensors, our method complies with target tensors taken from a sequence of deterministic **trace-class operators indexed with $p$**: $\\\\{F_p\\\\}_{p=1}^\\\\infty$. An example of such sequence is a sequence of diagonal operators with **eigenvalues following a power-law**: such power-laws are ubiquitous in real-life data, giving a common assumption for target spectrum. We have to supply each $p$-node with a power $q$ of the target tensor associated with this node. Though all $p$-nodes have $q = 1$ before contractions, when two nodes contract, their powers add. Each contracted node induces $\\\\mathrm{tr}[F_p^q]$ instead of $p$ in the sum.\n\n3. Our method is also compatible with **random** targets that can be expressed as **products of Gaussian matrices**.  \n   As an example, consider **Wishart ensemble decomposition**:\n   \n   $$L(U) = \\\\frac{1}{2} \\\\left\\\\| U^\\\\top U - W^\\\\top W \\\\right\\\\| \\\\to \\\\min_U,$$\n  \n   where we sample the entries of $W$ independently from a Gaussian and do not optimize them. The above problem statement generalizes naturally to higher-order symmetric and asymmetric tensors. When both dimensions of $W$ grow proportionally, the limit spectrum of $W^\\\\top W$ does not concentrate at any finite set of points.\n\n4. The three classes of target tensors mentioned above do not cover the whole class of targets compatible with our method. We expect any **linear combination** of compatible target tensors to be also compatible. The resulting “mixed” target tensors might induce rich spectral features, combining continuous parts with multiple discrete eigenvalues. As observed for linear networks initialized close to the origin, the target principal components are learned in a strictly sequential manner starting from the strongest. We therefore expect the loss evolution, which our method explicitly gives, to decompose into distinct phases according to the current eigenvalue to learn."}}, "id": "8gFmXqQiMf", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754136438, "cdate": 1763754136438, "tmdate": 1763754136438, "mdate": 1763754136438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers (Part II)"}, "comment": {"value": "**Proofs of convergence.** Several reviewers indicate that it is desirable to provide rigorous proofs of convergence of our asymptotic power series and/or estimate the associated remainders.\n\n- We fully agree that this would be very desirable, but we believe that this objective is quite difficult and most likely not even generally feasible. The limiting asymptotic expansion that we propose is generally well-defined term-wise, but need not be convergent in the usual sense. In particular, we have preliminary results for some models and regimes where the limiting asymptotic power series has a zero convergence radius; accordingly, the series does not converge in the usual sense (but can be summed for $t$ in a suitable domain $D \\\\subset \\\\mathbb{C}$ by, for example, our summation method).\n- We can reproduce some of our PDE-based explicit formulas for $\\\\mathbb{E}[L(t)]$ using random matrix theory. This can be seen as a validation of our approach. However, we can do this only on a case-by-case basis, not by some general theory. Because of this and a lack of space, we have not included these derivations in the paper.\n- These difficulties closely parallel the well-known difficulties associated with Feynman diagram expansions in quantum field theory and statistical physics. While present in most textbooks, these expansions are notoriously hard to rigorously justify. Usually these expansions are only considered term-wise, as in our case. Rigorous constructions of interacting theories are normally performed by other methods such as cluster expansions; only after the theory is rigorously constructed it can sometimes be connected to a Feynman diagram expansion [1]. Some theories have never been rigorously constructed: for example, this question for the Yang–Mills theory is one of the six unsolved Millennium problems [2]. Feynman power series typically have zero convergence radius and require Borel or other special summation methods. Nevertheless, the nonrigorous Feynman diagram expansions are much more common than rigorous cluster expansions in physics textbooks, since they are conceptually and computationally most natural and simple, and streamline the analysis of various phenomena (in particular scaling-related).\n- Similarly, in our ML setting we do not think that the difficulties of classical summation should prevent us from analyzing the limiting asymptotic expansion. We demonstrate that this expansion is a natural, well-defined and useful constructive object. Whether or not it converges in the classical sense, it allows us to systematically derive non-obvious results on the phase structure and in some cases even on the large-$t$ behavior of the model.\n\n[1] V. Rivasseau. Constructive field theory in zero dimension. *Advances in Mathematical Physics*, 2009(1), 2009.\n\n[2] <https://www.claymath.org/millennium/yang-mills-the-maths-gap/>"}}, "id": "ycWagbC11i", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755340757, "cdate": 1763755340757, "tmdate": 1763755340757, "mdate": 1763755340757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers (Part I)"}, "comment": {"value": "We thank the reviewers for a careful reading of our paper. We are impressed by the high quality of all the reviews and sincerely appreciate the feedback. We address the common concerns of the reviewers below.\n\n## Mathematical clarity and rigor\n\nSeveral reviewers express concerns regarding mathematical aspects of the exposition (clarity and rigor). We accept some parts of this criticism (particularly with regard to clarity).\n\n**General exposition.** Several reviewers point out that the exposition is too terse and relies on informal arguments in the diagram language foreign to ML audience; the focus can be shifted towards standard mathematical language. We tend to agree with this point and are now revising the paper to make it more easily readable.\n\n**Mathematical status of the results.** We also admit that the mathematical status of specific results presented in the paper can be described more clearly. We are revising the paper to this end. Below we emphasize some key points.\n\n1. The expansion of the expected loss $\\\\mathbb{E} [L(t)]$ studied in the paper is mathematically well-defined for finite models as an *asymptotic series* at $t \\\\to 0$.\n2. We then study the large-model/target limit of this asymptotic series; it is mathematically well-defined *term-wise* (under appropriate time rescaling).\n3. We show that this term-wise limit of the loss expansion *contains important information* about the evolution of large-size models. In particular, we show that there is a *mathematically consistent and systematic way* to identify *different large-scale learning regimes* by associating them with different subsets of leading monomials in the loss expansion coefficients.\n4. Accordingly, we give a *complete and rigorous classification* of these regimes for two scenarios (asymmetric and symmetric with even $\\\\nu$). The resulting sets of regimes are *different* (no NTK regime in the symmetric, even $\\\\nu$ scenario). We expect that the set of regimes can be even richer in more complex models.\n5. We also show that, at least in certain scenarios and regimes, the limiting loss expansion admits a *formal summation*. We propose a general summation method based on connecting the coefficients by recurrences and solving a first-order PDE. This method is *not universally applicable*, but we show that it is applicable in several scenarios and produces *nontrivial quantitative predictions* (analytic solutions) that *agree very well* with the experiment.\n\nIn summation, we propose a *new and mathematically consistent methodology* that produces *nontrivial new results*. Within this methodology — i.e., accepting the limiting power series expansion and our PDE-based summation method — we provide *rigorous proofs* of the large-scale phase structure for several specific models and *analytic derivations* of explicit solutions for several learning regimes."}}, "id": "sh5zHoVQn2", "forum": "xa3oLRQG55", "replyto": "xa3oLRQG55", "signatures": ["ICLR.cc/2026/Conference/Submission4485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4485/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission4485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763755400022, "cdate": 1763755400022, "tmdate": 1763755400022, "mdate": 1763755400022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}