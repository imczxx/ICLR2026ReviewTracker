{"id": "W8bKDPf1Ko", "number": 10814, "cdate": 1758182498423, "mdate": 1763521469627, "content": {"title": "Graph-Theoretic Intrinsic Reward: Guiding RL with Effective Resistance", "abstract": "Exploration of dynamic environments with sparse rewards is a significant challenge in Reinforcement Learning, often leading to inefficient exploration and brittle policies. To address this, we introduce a novel graph-based intrinsic reward using Effective Resistance, a metric from spectral graph theory. This reward formulation guides the agent to seek configurations that are directly correlated to successful goal reaching states. We provide theoretical guarantees, proving that our method not only learns a robust policy but also achieves faster convergence by serving as a variance reduction baseline to the standard discounted reward formulation. We perform extensive empirical analysis across several challenging environments to demonstrate that our approach significantly outperforms state-of-the-art baselines, demonstrating improvements of up to 59% in success rate, 56% in timesteps taken to reach the goal, and 4 times more accumulated reward. We augment all of the supporting lemmas and theoretically motivated hyperparameter choices with corresponding experiments.", "tldr": "We propose an intrinsic reward formulation using the notion of Effective Resistance based on spectral graph theory, for learning robust policies in sparse environments.", "keywords": ["Reinforcement Learning", "Intrinsic Motivation", "Goal Conditioned RL", "Effective Resistance"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/940d0ae7bc1f6baa9d2f7d13b4dfd2d127342dfd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new metric termed effective resistance that can be used as an intrinsic motivation reward for goal-conditioned RL tasks. The paper hypothesizes that this reward is better than using one proportional to Euclidean distance to the goal. The paper also performs some analysis to show that the proposed intrinsic reward is almost unbiased with respect to the extrinsic reward, and that using the proposed intrinsic reward leads to improved sample complexity.\n\nThe empirical evaluation on a suite of environments called Safety-Gymnasium seek to show that the above proposed theoretical guarantees hold, and that the proposed technique outperforms some reasonable baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Overall the idea is somewhat novel and might be a useful addition to the literature.\n* The proposed technique is interesting. Using ideas that consider graph flows to estimate how easy it is to navigate from one point to another has been seen to be useful in the past [1], and this modern revival of the technique could present some benefits.\n* Dynamic graph updates are very useful, allowing an environment that changes over time.\n* Theoretical analysis seems to be sound and gives some confidence that the proposed approach will not converge to a suboptimal solution and will help with some local exploration.\n* The evaluation methodology seems robust and statistically sound, and I especially appreciate the 1000 episode evaluations (5 training seeds and 200 episode evals per seed), which should capture variance in the policy and variance in the training.\n* The baselines that are used in the evaluation seem mostly reasonable. There are caveats here that I expand on in weaknesses.\n\n\n\n## References\n[1] Şimşek, Ö., Wolfe, A.P. and Barto, A.G., 2005, August. Identifying useful subgoals in reinforcement learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning (pp. 816-823)."}, "weaknesses": {"value": "There are some issues that keep this paper from being of a quality that I can confidently recommend for acceptance:\n* This paper is trying to suggest a new intrinsic motivation based on effective resistance in a graph. But seems to be specific to robotics type problems with objects present in the environment causing navigational or manipulational difficulties. If specific to robotics problems, the setup and writing should clarify and try to position itself accordingly so that it will attract the same community of researchers. It also does not compare to other GCRL methods for exploration in literature [1, 2, 3], or more up to date GCRL benchmarks like OGBench [4].\n* Part of the issue here is that the problem setup is specific to continuous state spaces and a 2 dimensional action space (Section 3.1).\n* My understanding is that the intrinsic reward is only calculated when the reward enters the agent's field of view. This seems like it will help mostly with local exploration instead of more generally.\n* The baselines proposed do not seem to be exploiting the structure of GCRL based problems from what I can tell. One of [1] or [3] would be great additions to show how effective the proposed intrinsic reward is in GCRL problems specifically.\n\n\n## References\n[1] Grace Liu, Michael Tang, and Benjamin Eysenbach. A single goal is all you need: Skills and exploration emerge from contrastive RL without rewards, demonstrations, or subgoals. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=xCkgX4Xfu0\n\n[2] Ma, Y.J., Yan, J., Jayaraman, D. and Bastani, O., 2022. How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via $ f $-Advantage Regression. arXiv preprint arXiv:2206.03023.\n\n[3] Durugkar, I., Tec, M., Niekum, S. and Stone, P., 2021. Adversarial intrinsic motivation for reinforcement learning. Advances in Neural Information Processing Systems, 34, pp.8622-8636.\n\n[4] Park, S., Frans, K., Eysenbach, B. and Levine, S., 2024. Ogbench: Benchmarking offline goal-conditioned rl. arXiv preprint arXiv:2410.20092."}, "questions": {"value": "* Could the authors clarify how more general exploration will be handled under the given scheme? Perhaps contrast with [3] from the weakness section, since that is an approach that handles more general exploration?\n* Could approaches like Quasimetric learning [1] also learn some metric like effective resistance?\n\n## References\n[1] Wang, T., Torralba, A., Isola, P. and Zhang, A., 2023, July. Optimal goal-reaching reinforcement learning via quasimetric learning. In International Conference on Machine Learning (pp. 36411-36430). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uQXtf0gQ2v", "forum": "W8bKDPf1Ko", "replyto": "W8bKDPf1Ko", "signatures": ["ICLR.cc/2026/Conference/Submission10814/Reviewer_nmWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10814/Reviewer_nmWg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761134738106, "cdate": 1761134738106, "tmdate": 1762922021703, "mdate": 1762922021703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel intrinsic reward mechanism for reinforcement learning in sparse reward environments based on effective resistance from spectral graph theory. The key idea is to construct a time-evolving graph from the agent's observations (specifically LiDAR data) where nodes represent the agent, goal, and environmental objects, and edges encode proximity relationships. The intrinsic reward is defined as the negative change in effective resistance between the agent and goal nodes, encouraging the agent to seek configurations that improve structural accessibility to the goal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The application of effective resistance from spectral graph theory to RL is creative and theoretically grounded. \n2. The paper provides a comprehensive theoretical analysis with multiple lemmas and a main theorem."}, "weaknesses": {"value": "1. The method is specifically designed for environments where meaningful graph construction from observations is possible. The reliance on LiDAR data limits applicability to certain domains, and it's unclear how this would extend to other observation modalities or higher-dimensional state spaces.\n2. Algorithm 1 involves many design choices (clustering threshold τ, connectivity patterns, central node selection) that appear to require careful tuning. The sensitivity analysis (Section A.9) shows some robustness to τ, but the overall complexity raises concerns about generalizability.\n3. While the paper compares against several baselines, most are relatively older methods. More recent state-of-the-art intrinsic motivation methods could strengthen the comparison.\n4. While Section A.10 provides some runtime analysis, the computational cost of repeated graph construction and effective resistance computation could be prohibitive in real-time applications or larger graphs."}, "questions": {"value": "1. How does the method scale to environments with many more objects or higher-dimensional observation spaces? What is the computational complexity as a function of graph size?\n2. How does the method scale to environments with many more objects or higher-dimensional observation spaces? What is the computational complexity as a function of graph size?\n3. How does the method perform in environments with dense rewards? Does the intrinsic reward provide benefits or potentially interfere with learning in such settings?\n4. How does the method perform in environments with dense rewards? Does the intrinsic reward provide benefits or potentially interfere with learning in such settings?\n5. Beyond τ, how sensitive is the method to other hyperparameters like α and β? The theoretical guidelines (Corollary 1) provide bounds, but practical selection seems to require empirical validation.\n6. How does this approach compare to more recent intrinsic motivation methods like NGU, RND, or ICM on the same environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "T0kMV7acov", "forum": "W8bKDPf1Ko", "replyto": "W8bKDPf1Ko", "signatures": ["ICLR.cc/2026/Conference/Submission10814/Reviewer_Pwps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10814/Reviewer_Pwps"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154760107, "cdate": 1762154760107, "tmdate": 1762922021143, "mdate": 1762922021143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reward shaping methods orignated from spectral graph theory to tackle reward sparsity in reinforcement learning setting. By modifying its instrinsic reward, the agent needs to maintain a graph of its surrounding environment first through its sensors (e.g. LIDARs) every timestep, then calculates its effective resistance between itself and the goal, which will be used as part of its reward construction.  They provide theoretical guarantees to prove they can learn a robust policy and also achieves faster convergence. Through experiments, they show their methods can beat state of the art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good originality: abstract objects into nodes in graph, and design rewards based on the constructed graph. An novel reward shaping methods that encourage the agent to get closer to goal.\n- Quality: theoretically sound. Assumptions setup with proper citation and completely. Proved decreasing the effective resistance can also maintain connectivity on the graph. This paper also show the advantage of its algorithms emprically with extensive experiments."}, "weaknesses": {"value": "- This methods can only be applied to specific domains, for example, robotics navigation tasks in the paper, in which the robots have a suite of sensors that are assumed not having noise and the robots having a good localization capability and can categorizes or recognize objects as nodes in the map. \n- I would appreciate more explanation on how the graph is constructed and what reducing effective resistance would bring on the main text, but overall the paper is easy to follow."}, "questions": {"value": "- To increase the impact of this paper, can this method apply to a more general MDP setting, e.g. continuous MDP or tabular MDP, how would you construct the graph in such MDPs? specifically, what are nodes/edges/weights in these settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "21sJGJg7pw", "forum": "W8bKDPf1Ko", "replyto": "W8bKDPf1Ko", "signatures": ["ICLR.cc/2026/Conference/Submission10814/Reviewer_FfQG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10814/Reviewer_FfQG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762464790256, "cdate": 1762464790256, "tmdate": 1762922020471, "mdate": 1762922020471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "# Common Points and Revisions\n\nWe sincerely thank all reviewers for their constructive feedback, which has helped us substantially strengthen our paper. We have incorporated all the revisions, including new SOTA baselines and new appendix sections, to directly address the common points. We actively invite the reviewers to engage in further discussion during the rebuttal period, should any points require further clarification. If you find that our detailed responses, new theoretical insights, and the substantial new empirical evidence provided have satisfactorily addressed your concerns, we respectfully request that you consider increasing your score to reflect the strengthened contribution of this paper. (**Please note that all revisions and additions made to the manuscript PDF are highlighted in blue for easy tracking.**)\n\nTo address the comparison, we have now fully integrated two new SOTA methods, NGU and AIM, into our study. Their implementation details are defined in **Section 5 (Page 6, lines 296-304)**. More importantly, we have re-run our experiments and updated all main results to include a direct comparison across all 9 environments. These new results are presented in the updated **Figures 2, 3, and 4 (Pages 7-8)**, where our method's significant performance gains are shown to hold, and in many cases widen, against these strong, recent baselines. For instance, the success rates are over **3x** and **4x** for the highest difficulty level for navigation and building environments respectively. Similar observations hold for the other metrics.\n\nWe also appreciate the reviewers' questions on the flexibility and generalization of our framework. To explicitly address this, we have added two new sections to the appendix. **Appendix A.15 (Page 41)** discusses generalization to arbitrary inputs like vision **(lines 2187-2199)**. In more generality, **Appendix A.16 (Pages 41-42)** provides concrete formulations for applying our $\\mathcal{R}_{\\text{eff}}$ reward to new domains, where we provide a general graph construction and computation of the effective resistance to construct the intrinsic reward. We then specify details on how to construct graphs for maze environments like Antmaze **(lines 2221-2227)** and for locomotion tasks like Half-Cheetah **(lines 2230-2247)**, showing how $\\mathcal{R}_{\\text{eff}}$ can serve as a holistic pose descriptor for learning stable gaits.\n\nFinally, we respectfully and strongly argue that our experimental setup, far from being niche, is grounded in real-world practicality. As we have now detailed in the expanded **Appendix A.2.2 (Page 23, lines 1197-1209)**, our use of LiDAR mirrors its primary role ranging from production robotics systems (e.g., Waymo, Boston Dynamics) to most widely used academic benchmarks (e.g., CALVIN [1], LIBERO[2]).\n\nThis setup also allows us to clarify a core design choice. Our $\\mathcal{R}_{\\text{eff}}$ reward is deliberately formulated as a dense, goal-directed local planning signal that activates once the goal is perceived. This complements the global exploration (i.e., finding the goal) driven by the base PPO algorithm and the large $\\pm\\beta$ reward **(Equation 3, Page 4)**. The success of this decoupled mechanism is empirically proven in **Table 5 (Page 36)**, which shows our final trained policies maintain >98.9% goal visibility across all environments, confirming they successfully learn to both find the goal and robustly navigate toward it.\n\n**References**\n[1] Mees, O., Hermann, L., Rosete-Beas, E., Burgard, W. “CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks,” 2022.\n\n[2] Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y.,  Stone, P. “LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning,” 2023."}}, "id": "U69fEhexH5", "forum": "W8bKDPf1Ko", "replyto": "W8bKDPf1Ko", "signatures": ["ICLR.cc/2026/Conference/Submission10814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10814/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10814/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763518976048, "cdate": 1763518976048, "tmdate": 1763521757462, "mdate": 1763521757462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "# Common Points and Revisions\n\nWe sincerely thank all reviewers for their constructive feedback, which has helped us substantially strengthen our paper. We have incorporated all the revisions, including new SOTA baselines and new appendix sections, to directly address the common points. We actively invite the reviewers to engage in further discussion during the rebuttal period, should any points require further clarification. If you find that our detailed responses, new theoretical insights, and the substantial new empirical evidence provided have satisfactorily addressed your concerns, we respectfully request that you consider increasing your score to reflect the strengthened contribution of this paper. (**Please note that all revisions and additions made to the manuscript PDF are highlighted in blue for easy tracking.**)\n\nTo address the comparison, we have now fully integrated two new SOTA methods, NGU and AIM, into our study. Their implementation details are defined in **Section 5 (Page 6, lines 296-304)**. More importantly, we have re-run our experiments and updated all main results to include a direct comparison across all 9 environments. These new results are presented in the updated **Figures 2, 3, and 4 (Pages 7-8)**, where our method's significant performance gains are shown to hold, and in many cases widen, against these strong, recent baselines. For instance, the success rates are over **3x** and **4x** for the highest difficulty level for navigation and building environments respectively. Similar observations hold for the other metrics.\n\nWe also appreciate the reviewers' questions on the flexibility and generalization of our framework. To explicitly address this, we have added two new sections to the appendix. **Appendix A.15 (Page 41)** discusses generalization to arbitrary inputs like vision **(lines 2187-2199)**. In more generality, **Appendix A.16 (Pages 41-42)** provides concrete formulations for applying our $R_{eff}$ reward to new domains, where we provide a general graph construction and computation of the effective resistance to construct the intrinsic reward. We then specify details on how to construct graphs for maze environments like Antmaze **(lines 2221-2227)** and for locomotion tasks like Half-Cheetah **(lines 2230-2247)**, showing how $\\mathcal{R}_{\\text{eff}}$ can serve as a holistic pose descriptor for learning stable gaits.\n\nFinally, we respectfully and strongly argue that our experimental setup, far from being niche, is grounded in real-world practicality. As we have now detailed in the expanded **Appendix A.2.2 (Page 23, lines 1197-1209)**, our use of LiDAR mirrors its primary role ranging from production robotics systems (e.g., Waymo, Boston Dynamics) to most widely used academic benchmarks (e.g., CALVIN [1], LIBERO[2]).\n\nThis setup also allows us to clarify a core design choice. Our $\\mathcal{R}_{\\text{eff}}$ reward is deliberately formulated as a dense, goal-directed local planning signal that activates once the goal is perceived. This complements the global exploration (i.e., finding the goal) driven by the base PPO algorithm and the large $\\pm\\beta$ reward **(Equation 3, Page 4)**. The success of this decoupled mechanism is empirically proven in **Table 5 (Page 36)**, which shows our final trained policies maintain >98.9% goal visibility across all environments, confirming they successfully learn to both find the goal and robustly navigate toward it.\n\n**References**\n[1] Mees, O., Hermann, L., Rosete-Beas, E., Burgard, W. “CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks,” 2022.\n\n[2] Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y.,  Stone, P. “LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning,” 2023."}}, "id": "U69fEhexH5", "forum": "W8bKDPf1Ko", "replyto": "W8bKDPf1Ko", "signatures": ["ICLR.cc/2026/Conference/Submission10814/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10814/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10814/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763518976048, "cdate": 1763518976048, "tmdate": 1763525096615, "mdate": 1763525096615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}