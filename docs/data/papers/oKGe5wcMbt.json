{"id": "oKGe5wcMbt", "number": 12511, "cdate": 1758208313366, "mdate": 1759897504841, "content": {"title": "Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts", "abstract": "Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical Dual-RObust Cross-domain Offline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.", "tldr": "This paper aims to enhance the train-time and test-time robustness against dynamics shifts for cross-domain offline RL.", "keywords": ["Offline Reinforcement Learning; Cross-domain Policy Adaptation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61ce4b1853945b53cb0845e7398c02645ecff0b3.pdf", "supplementary_material": "/attachment/e9af7daa4a526334bfd8ac5b81d60460eb184a8b.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates dual robustness in cross-domain offline RL, addressing both train-time robustness against source-target dynamics mismatch and test-time robustness against deployment-time dynamics perturbations. The authors introduce a novel Robust Cross-domain Bellman (RCB) operator with theoretical guarantees, and develop the practical DROCO algorithm using ensemble dynamics models, dynamic value penalty, and Huber loss. The experimental results across 32 scenarios demonstrate that DROCO achieves good performance.\n\nThe authors employ dynamics perturbations of large magnitude, which may diverge from realistic scenarios. More critically, to put the theory into practice, the algorithm adopts a practical scheme that deviates from its core theoretical assumptions: using an ensemble of dynamics models trained on limited target data to approximate uncertainty. To compensate for the potential value estimation errors caused by this deviation, DROCO further introduces a dynamic value penalty and the Huber loss.\n \nAlthough the method demonstrates performance surpassing baselines and enhanced robustness on specific Mujoco tasks, its effectiveness is highly dependent on a set of sensitive hyperparameters that are difficult to tune effectively in a purely offline setting. Furthermore, the experimental scope is confined to locomotion tasks and has not been validated on more challenging benchmarks, which leaves the generalizability and practical value of its claimed \"dual robustness\" open to question."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem formulation is both novel and practically important. While existing cross-domain offline RL methods focus exclusively on train-time robustness, this work is the first to systematically study both train-time and test-time robustness together. The motivation is compelling, with Figure 1 clearly demonstrating that policies trained with limited target domain data are highly vulnerable to test-time dynamics perturbations. This observation reveals a critical gap in current approaches that assume deployment environments will match the target domain exactly.\n\nThe theoretical contributions are well-developed and rigorous. The RCB operator elegantly handles dual robustness by applying robust Bellman updates only to source domain data while using standard updates for target data."}, "weaknesses": {"value": "My primary concern is the insufficient analysis of generalization. Moreover, the experiments are confined entirely to MuJoCo tasks. Maybe authors can consider more experiment for validation. \n\nThe paper's own sensitivity analysis (Section 5.3) showed that the optimal values for β and δ vary significantly across different tasks and datasets. In a real-world offline scenario, it is nearly impossible to tune these parameters to their optimal values for a new task due to the inability to validate against the target environment. The method's claim to practicality is diminished if its strong performance relies on meticulous, per-task tuning that cannot be replicated in practice."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kCslrlnJvA", "forum": "oKGe5wcMbt", "replyto": "oKGe5wcMbt", "signatures": ["ICLR.cc/2026/Conference/Submission12511/Reviewer_k93k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12511/Reviewer_k93k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548671371, "cdate": 1761548671371, "tmdate": 1762923380509, "mdate": 1762923380509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper enhances offline RL by introducing two types of robustness: train-time robustness and test-time robustness. Train-time robustness addresses the dynamic shift between the source-domain dataset and the target-domain dataset, while test-time robustness focuses on the shift from the target dynamics to the deployment dynamics. Considering that (1) deployment dynamics may change in real-world settings and (2) existing methods often fail to generalize well under such changes, I believe this paper makes a valuable contribution.\n\nThe proposed method centers around a novel Robust Cross-domain Bellman (RCB) operator, which integrates two types of Bellman operations: (1) the standard Bellman operator for training on the target-domain dataset and (2) a “robust” Bellman operator for policy evaluation on the source-domain data. The theoretical analysis, in my view, can be derived from classic results in robust reinforcement learning.\n\nThe theoretical justification mainly concerns the convergence (contraction) property of the proposed RCB operator. I believe that the conclusions for both the idealized and practical cases can be obtained with relatively minor modifications to existing robust RL proofs.\n\nThe empirical results (Table 1, on MuJoCo tasks) adequately demonstrate the advantages of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The theoretical justification is solid, covering both the idealized case (Proposition 4.1) and the practical case (Proposition 4.3).\n\n- The motivation and analysis for both train-time and test-time robustness  (Proposition 4.4 and 4.5) are meaningful and potentially impactful, although their direct relevance to practitioners might be limited.\n\n- The empirical evaluation is convincing. The chosen baselines are sota methods for offline RL and cross-domain offline RL, yet the proposed algorithm (DROCO) achieves significant improvements over them."}, "weaknesses": {"value": "I do not see any major weaknesses worth highlighting."}, "questions": {"value": "Q1. Lipschitz Q-function assumption:\nWhy do you cite recent studies to justify the Lipschitz continuity assumption? If I am not mistaken, this assumption is a standard one in Q-learning and can be found in many textbooks.\n\nQ2. Test-time robustness:\nIt is somewhat difficult for me to see the benefits of DROCO regarding test-time robustness. Could you please elaborate on this aspect explicitly in the Experiments section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CbITPMfjBN", "forum": "oKGe5wcMbt", "replyto": "oKGe5wcMbt", "signatures": ["ICLR.cc/2026/Conference/Submission12511/Reviewer_XtEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12511/Reviewer_XtEu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586988530, "cdate": 1761586988530, "tmdate": 1762923380195, "mdate": 1762923380195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies dual robustness in cross-domain offline RL. It considers train time robustness to distribution shift between source and target, and test-time robustness to perturbations around the target dynamics. It proposes a Robust Cross-domain Bellman (RCB) operator that performs robust backups (min over uncertainty set) on source data and standard backups on target data. Experiments are performed on D4RL MuJoCo tasks to show performance gains and test time stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The goal of achieving both train time and test time robustness in offline RL is well motivated.\n+ The RCB operator that separates robust and standard updates is simple especially with the duality result which simplifies the uncertainty set of distributions to one over states.\n+ The paper gives good empirical results with RL benchmarks showing that the approach outperforms baselines under moderate dynamics shifts."}, "weaknesses": {"value": "- The robust Bellman backups and the derived contraction properties are standard. As far as I understand, the main idea is to split robust and standard updates, which is conceptually incremental. The paper seems conceptually incremental for ICLR.\n- The setup is restrictive as only dynamics shift is modeled. Typically, there is shift in reward, observation, state/action spaces, etc. \n- The theoretical results largely follow directly from known robust RL results. For example Prop 4.1 showing the contraction is immediate for discounted robust Bellman operators. The train time conservatism and lower bound properties (e.g., Prop 4.4) are classic robust RL analyses. Even the test time guarantee of Prop 4.5 arguing that the performance is better than the worst case value when the true perturbation lies inside the set is by construction and standard in DRO. \n- The framework is restricted to Wasserstein distance without extensions to TV/MMD or other divergences.\n- It is unclear how to choose or tune the uncertainty radius. Also if the domain shift is large, the approach is likely going to be over conservative given the requirement that the target lies in the uncertainty set. \n- I did not quite understand the value penalty and the Huber loss parts as they are treated superficially without sufficient depth."}, "questions": {"value": "What is the precise theoretical and/or algorithmic novelty beyond the split backup? Any coverage aware or calibration guarantees that are new? The paper needs better positioning in relation to vast literature on robust offline RL/DRO. \n\nCan your approach extend to TV/MMD balls (even approximately)? What breaks or becomes intractable?\n\nAny results for reward or state shift to demonstrate generality?\n\nHow does performance scale with the amount of target data and epsilon? Where does RCB become too conservative?\n\nThe source and target behavior policies are often different. Can you analyze this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAYhSwThTL", "forum": "oKGe5wcMbt", "replyto": "oKGe5wcMbt", "signatures": ["ICLR.cc/2026/Conference/Submission12511/Reviewer_z4fr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12511/Reviewer_z4fr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679756610, "cdate": 1761679756610, "tmdate": 1762923379905, "mdate": 1762923379905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}