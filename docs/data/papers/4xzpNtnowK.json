{"id": "4xzpNtnowK", "number": 22249, "cdate": 1758328381851, "mdate": 1759896877782, "content": {"title": "Sketched Gaussian Mechanism on Matrix for Private Federated LoRA", "abstract": "Low-Rank Adaptation (LoRA), which modifies frozen pre-trained parameters via the product of two trainable low-rank factors, has been widely adopted for communication-efficient fine-tuning of language models, including extensions to federated learning (FL). Nevertheless, two challenges arise at scale: (i) for very large models, the adapter factors can remain high-dimensional, leading to nontrivial communication costs between clients and the server; and (ii) transmitting local adapters between clients and the server risks privacy leakage. Incorporating differential privacy (DP) by additive mechanisms, e.g., the Gaussian mechanism (GM), often leads to substantial noise amplification, particularly in algorithms that must perturb both low-rank components.\n\nIn this paper, we propose the Sketched Gaussian Mechanism on Matrix (SGMM), which couples random sketching with the Gaussian mechanism at the matrix level. Using tools from Rényi differential privacy (RDP), we provide a unified analysis of SGMM’s privacy guarantee and show that, for a fixed privacy level, the required noise magnitude scales as $1/\\sqrt{b}$ for sketch dimension $b$. Consequently, for moderate $b$, SGMM attains the same privacy with markedly less noise than GM. We instantiate SGMM within federated LoRA algorithms, including FFA-LoRA and FlexLoRA, where sketching further reduces adapter dimensionality and, in turn, the noise needed to meet a given privacy target, addressing both communication overhead and noise amplification. Experiments demonstrate that, at matched privacy budgets, SGMM-based federated LoRA is at least competitive with and in some settings outperforms non-sketched private baselines.", "tldr": "", "keywords": ["Federated Learning", "Differential Privacy", "Sketching", "Communication Efficiency", "LoRA"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18c6771b37c089af68a45c0d45dacc75e2d6e5d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a mechanism named Sketched Gaussian Mechanism on Matrix (SGMM) for private federated LoRA. By combining random sketching with the Gaussian mechanism at the matrix level , this method aims to simultaneously address two challenges in federated LoRA: the high communication overhead from large adapter factors and the significant noise amplification caused by standard DP mechanisms. The authors provide a theoretical RDP analysis of SGMM and integrate it into the FFA-LORA and FlexLoRA algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is the first to integrate matrix sketching methods with the federated LoRA, supported by a rigorous privacy guarantee using the Rényi Differential Privacy framework."}, "weaknesses": {"value": "The paper's novelty is constrained, as the core matrix sketching-plus-noise mechanism exists in prior works such as [1], and recent research offers independent RDP analyses of this technique [2]. Furthermore, the empirical support presented is limited, focusing narrowly on a single dataset with fixed privacy and sketching parameters. Broader experiments covering diverse datasets, sensitivity analyses across key parameters, and evaluations of efficiency metrics are needed to convincingly establish practical benefits.\n\n[1] Yuchang Sun, Jiawei Shao, Songze Li, Yuyi Mao, and Jun Zhang, \"Stochastic Coded Federated Learning with Convergence and Privacy Guarantees.\" 2022 IEEE International Symposium on Information Theory (ISIT), pages 2028-2033, 2022.\n[2] Omri Lev, Vishwak Srinivasan, Katrina Ligett, Ayush Sekhari, and Ashia C Wilson, \"The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches.\" Accepted to the 38th Advances in Neural Information Processing Systems (NeurIPS 2025), 2025."}, "questions": {"value": "1. How does the theoretical RDP bound derived in Theorem 2.3 compare in tightness to privacy analyses of similar client-level sketching-plus-noise mechanisms, such as the MI-DP analysis in work [1] ?\n2. The derived privacy bound suggests noise variance scales approximately as $\\sqrt{r}/\\sqrt{b}$. Achieving a small privacy loss $\\epsilon_p$ thus seems to require $b$ to be comparable to $r$, implying substantial noise if the rank $r$ is large while a strong privacy guarantee (small $\\epsilon_p$) is desired. Does this indicate a potential practicality issue for SGMM when applied to high-rank LoRA adaptations under strict privacy constraints?\n3. How does model performance, including accuracy and convergence, trade off across varying privacy budgets $\\epsilon_p$, sketch dimensions $b$, and LoRA ranks $r$? \n\n[1] Yuchang Sun, Jiawei Shao, Songze Li, Yuyi Mao, and Jun Zhang, \"Stochastic Coded Federated Learning with Convergence and Privacy Guarantees.\" 2022 IEEE International Symposium on Information Theory (ISIT), pages 2028-2033, 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz0mBgNor6", "forum": "4xzpNtnowK", "replyto": "4xzpNtnowK", "signatures": ["ICLR.cc/2026/Conference/Submission22249/Reviewer_yPFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22249/Reviewer_yPFM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386882799, "cdate": 1761386882799, "tmdate": 1762942136253, "mdate": 1762942136253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Sketched Gaussian Mechanism on Matrix (SGMM), an adapted approach of the earlier proposed SGMV method, to improve communication efficiency and privacy in federated Low-Rank Adaptation (LoRA) for large language models. It is shown that the proposed algorithm achieves the same privacy protection strength with noise magnitude of $1/\\sqrt{b}$ ($b$ is the dimension of sketch), which can be better than the vanilla Gaussian mechanism. The authors combine SGMM with the existing federated learning LoRA algorithms and demonstrate some empirical results as well."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposed an improved version of sketching algorithms for matrices, SGMM. The algorithm is insightful and well-motivated.\n2. The paper presents a detailed analysis of the privacy proofs of the proposed algorithm.\n2. The paper provides some necessary empirical analysis to show the effectiveness of the proposed algorithm."}, "weaknesses": {"value": "1. The proposed algorithm relies on SVD of a matrix, which will introduce additional computation and could be hard to scale to large model.\n2. Although the authors show the benefit of the proposed algorithm has a certain level of theoretical benefit, their empirical results show that such a benefit might be very limited when such an algorithm is applied to model training.\n3. The writing of the paper can be further improved. The readers may find it easy to get lost in what the main contributions are in the paper.\n4. There is no utility analysis of the proposed algorithm. While the privacy proof is presented, it is not clear how the magnitude of noise can theoretically affect the (reconstructed) matrices, e.g., the $\\tilde{B}^{t, k}_c$."}, "questions": {"value": "1. Is it possible to provide some theoretical analysis about how the noise and sketch dimension can affect the utility?\n2. What will be the computation overhead (considering SVD) when the SGMM is applied to model training?\n3. What can be the potential reason that SGMM-FFA-LoRA and SGMM-FlexLoRA do not show significant performance gain? Is it related to the sketch/matrix dimension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j7poxmR4OE", "forum": "4xzpNtnowK", "replyto": "4xzpNtnowK", "signatures": ["ICLR.cc/2026/Conference/Submission22249/Reviewer_9JDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22249/Reviewer_9JDd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448573445, "cdate": 1761448573445, "tmdate": 1762942135932, "mdate": 1762942135932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Sketched Gaussian Mechanism on Matrix (SGMM), which couples random sketching with the Gaussian mechanism at the matrix level. It also provides a unified privacy analysis of the proposed sketching mechanism, which shows that, for a fixed privacy level, the required noise variance scales inversely proportional to the sketch dimension. Finally, they apply the idea to the setting of federated low-rank adaptation, motivated by reducing the communication overhead and achieving client-level privacy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies the trilemma between communication overhead, privacy and utility in federated low-rank adaptation, which is an interesting and important problem. \n2. The work proposes a sketching approach natively designed for matrix statistics, which is an important problem and improves the SoTA of sketching approaches.\n3. The authors also provide the theoretical privacy analysis of the proposed approach, showing its difference with that of the existing vectorization-based sketching approaches."}, "weaknesses": {"value": "1. The theoretical results in this work suggest the following main messages:\n\n-  For a fixed privacy level, the order of Gaussian noise magnitude is: GM > SGMM > SGMV (with $h=br$)\n- SGMM has less computational complexity than SGMV. \n- Using SGMM and SGMV can reduce the communication overhead in federated low-rank adaptation, but potentially at the cost of model utility.\n\nHowever, the experimental results are very limited and do not fully support the above claims/findings. Further questions are asked about this below.\n\n2. An important limitation of the proposed method when combined with FFA-LoRA is that all the participating clients in federated low-rank adaptation need to use the same sketch matrices $R_B^t$ (as well as $R_A^t$ for Flex-LoRA) in each round $t$. This is a strong limitation, as clients in FL cannot communicate to synchronize their matrices. Also, even the server cannot be aware of the matrices, as from the client-level privacy considered in the paper, it seems that the server is not trusted. Even in algorithm 1 and 2, it is not clear where the sketch matrices in each round $t$ come from.\n\n3. While the work has some contributions, its main achievements seem unclear.\n\nOverall, the work needs to get improved, especially the experimental results."}, "questions": {"value": "Following the weaknesses mentioned above, I have the following questions:\n\n1. It has been shown that for a fixed privacy level, SGMM adds more Gaussian noise, and has less computational complexity than SGMV. However, in Fig. 1 (a), SGMM seems to get a better utility than SGMV (on average). Is this result inconsistent with the findings mentioned above?\n\n2. Similarly, in Fig. 1 (b), GM clearly performs better than SGMM. SGMM adds less noise than GM, and has some utility loss due to incorporating the random sketching. Considering Fig. 1 (a) and (b), it seems that we cannot make a clear conclusion when comparing the utility of GM with that of SGMV and SGMM. In other words, they get comparable utility. So, at a fixed privacy level, what is gained from using sketching (either SGMV or SGMM)? Only reducing the communication overhead? \n\n3. In algorithms 1 and 2, the sketch matrices of participating clients in each round $t$, should be synchronized. Have the authors considered this important point?\n\nminor comments:\n\ntypo in line 315: analogously\n\ntypo in line 348: $R_A^t$ should be $R_A^{t^T}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "27pB1T9faG", "forum": "4xzpNtnowK", "replyto": "4xzpNtnowK", "signatures": ["ICLR.cc/2026/Conference/Submission22249/Reviewer_8FnX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22249/Reviewer_8FnX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143563763, "cdate": 1762143563763, "tmdate": 1762942135651, "mdate": 1762942135651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Sketched Gaussian Matrix Mechanism (SGMM) for federated LoRA. The proposed SGMM is specifically designed to do DP for matrix data instead of the conventional vector data. Theoretical analysis shows the condition under which the proposed SGMM is better than the classical GM, which basically translates to a sufficiently small rank $r$. Empirical results on CIFAR 100 and ViT demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is based on a clean and important motivation: DP in federated LoRA. Given that classical DP uses Gaussian mechanisms for vectors, the proposed sketched Gaussian mechanism for matrix is a reasonable step. The theoretical analysis justifies the advantages of the proposed method under low rank adaptation."}, "weaknesses": {"value": "* Although theoretical analysis seems sound, it is an incremental step from the existing knowledge (e.g., Theorem 1 and standard sketched Gaussian Mechanism). \n* Given the limited theoretical contribution, a strong empirical contribution is expected. However, this paper only evaluate the proposed method on CIFAR100 under a single privacy setting. The experiments only use ViT, while the paper motivates itself using LLMs. Moreover, the empirical results do not show advantage of the proposed method comparing to standard Gaussian mechanisms, in contrast to what the theory might suggest. \n* Minor issue in Line 110: citation Dwork et al. (2006) should use another format."}, "questions": {"value": "* Remark 2.1 states that \"Comparing to classical GM, SGMM attains the same privacy level with smaller noise whenever the sketch dimension satisfies $b\\geq \\Omega (\\frac{r\\epsilon_p^2}{\\ln(1/\\delta_p)})$.\" However, if we have $r=1$ and the matrix reduce to a vector, how the classical GM is better than the SGMM? I may misunderstand this point, and an explanation is appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tTCNtCdbCE", "forum": "4xzpNtnowK", "replyto": "4xzpNtnowK", "signatures": ["ICLR.cc/2026/Conference/Submission22249/Reviewer_YSJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22249/Reviewer_YSJc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22249/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157117525, "cdate": 1762157117525, "tmdate": 1762942135100, "mdate": 1762942135100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}