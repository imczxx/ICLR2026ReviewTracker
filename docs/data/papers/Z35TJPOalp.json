{"id": "Z35TJPOalp", "number": 9623, "cdate": 1758130880473, "mdate": 1763145115724, "content": {"title": "Enhance-A-Video: Better Generated Video for Free", "abstract": "DiT-based video generation has achieved remarkable results, but research into enhancing existing models remains relatively unexplored. In this work, we introduce a training-free approach to enhance the coherence and quality of DiT-based generated videos, named Enhance-A-Video. The core idea is to enhance the cross-frame correlations based on non-diagonal temporal attention distributions. Thanks to its simple design, our approach can be easily applied to most DiT-based video generation frameworks without any retraining or fine-tuning. Across various DiT-based video generation models, our approach demonstrates promising improvements in both temporal consistency and visual quality. We hope this research can inspire future explorations in video generation enhancement.", "tldr": "", "keywords": ["Video Generation Enhancement", "Diffusion Transformer", "Training-Free"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3161de44bee69c3528eee6ea9c3d5ea70120c613.pdf", "supplementary_material": "/attachment/61c6c055855df2f8865304ac4dbd065ffa9e3b22.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Enhance-A-Video, a training-free and plug-and-play method designed to improve the quality of videos generated by pre-trained DiT models. The authors identify that existing DiT models often suffer from temporal inconsistencies and poor visual details because their temporal attention mechanisms focus excessively on intra-frame information rather than cross-frame correlations. The proposed Enhance Block is a simple module that calculates the Cross-Frame Intensity (CFI) from the temporal attention map and uses it to adaptively scale the attention block's output within the residual connection. This approach aims to enhance temporal consistency and visual fidelity without requiring any model retraining or fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is training-free and functions as a plug-and-play module. This allows it to be integrated into various existing, pre-trained DiT-based video generation models without any costly retraining or fine-tuning.\n\n2. The Enhance Block introduces negligible computational overhead during inference."}, "weaknesses": {"value": "1. Conceptual Contradiction of Temperature\n\nA primary weakness is the conceptual contradiction in the definition and application of temperature (tau).\n\n-  In Section 3.2, the paper introduces 'temperature' in its classical sense: a parameter applied inside the softmax function to modulate the probabilistic distribution of attention weights without altering the feature scale (Eq. (4)) \n-  However, the proposed enhance temperature in Section 3.3 is used in a completely different mechanism. As shown in Eq. (7), it is summed with the frame count F and used to scale the CFI value, an operation external to the softmax function.\n\nAlthough the authors intentionally dismiss the classical method (Eq. (4)) as a naive strategy, their co-opting of the same term for two mechanically disparate operations creates significant confusion and undermines the paper's methodological clarity.\n\n2. Lack of Theoretical Justification for Heuristic Formula\n\nThe core of the proposed method, Eq. (7), appears to be an empirical heuristic lacking clear theoretical justification.\n\n- The operation of summing the temperature parameter tau with the number of frames F is not derived from any stated principle.\n- The paper provides no deep analysis or ablation to justify why this specific formulation is an optimal or principled way to enhance cross-frame correlations.\n- This suggests the method may be an engineering trick discovered to work in a specific experimental setup, rather than a generalizable principle.\n\n3. Risks of Altering Feature Scales in a Training-Free Method\n\nThe most significant potential flaw is the method's direct alteration of intermediate feature scales within a training-free framework.\n\n- Eq. (8) explicitly modifies the norm of the attention output by multiplying O_attn with the scalar CFI_enhanced.\n- Pre-trained generative models like DiT are highly sensitive to the statistical distributions of the features they were trained on. Introducing features with an altered, out-of-distribution scale risks destabilizing subsequent layers and the entire denoising process, which could lead to severe artifacts.\n- The authors' defense for this in Appendix D (Fig. 11(b))  is that the norm of the attention output is relatively small compared to the norm of the hidden state.\n- This defense is tenuous. It relies on a single empirical observation on the CogVideoX model and does not guarantee that this property holds universally across all DiT architectures. The method's stability precariously relies on the assumption that O__attn's contribution to the residual stream is always negligible, which may not be a safe or generalizable assumption."}, "questions": {"value": "- Beyond the high-level effect of smoothing the distribution, the proposed enhance temperature shares no mechanistic commonality with the classical temperature parameter defined in Section 3.2. Regarding the conceptual contradiction of temperature, could the authors elaborate on the theoretical link, if any, between the classical softmax modulator defined in Section 3.2 (Eq. 4) and the mechanically disparate scaling factor in Section 3.3 (Eq. 7), which is summed with the frame count F? \n- Given this terminological confusion, would introducing a distinct term, such as enhancement strength, not have been clearer for the proposed mechanism in Eq. (7)?\n- The core formula in Eq. (7) appears to be an empirical heuristic. What is the theoretical justification for summing the tau parameter with the frame count F?\n- Could the authors detail the derivation process for Eq. (7)?\n- How can the method's direct alteration of intermediate feature scales (Eq. 8) be considered stable within a training-free framework, given that it introduces out-of-distribution feature scales to a pre-trained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zG8lyt66wW", "forum": "Z35TJPOalp", "replyto": "Z35TJPOalp", "signatures": ["ICLR.cc/2026/Conference/Submission9623/Reviewer_kFrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9623/Reviewer_kFrD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570672176, "cdate": 1761570672176, "tmdate": 1762921160897, "mdate": 1762921160897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "at75Gm9r2x", "forum": "Z35TJPOalp", "replyto": "Z35TJPOalp", "signatures": ["ICLR.cc/2026/Conference/Submission9623/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9623/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763145114431, "cdate": 1763145114431, "tmdate": 1763145114431, "mdate": 1763145114431, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Enhance-A-Video, which is a training-free and plug-and-play algorithm for DiT-based text-to-video models. The key idea is to compute a Cross-Frame Intensity (CFI) from the non-diagonal entries of the temporal attention map, adjust it with a manually tuned temperature, and inject the result into the residual connection of the temporal attention block. The method is tested with several DiT-based video generation models (Wan2.1, HunyuanVideo, CogVideoX, LTX-Video, Open-Sora) and demonstrates superior performance on VBench against the baselines with negligible overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Enhance Block is conceptually clean, allowing it to be easily integrated into temporal attention modules in existing DiT-based video generation models without re-training or fine-tuning.\n2. The proposed method improves the performance of the existing models with <3% runtime increase on A100.\n3. Enhance-A-Video demonstrates its superiority across 3D-full and spatial-temporal attention video generation models (Wan2.1, HunyuanVideo, CogVideoX, LTX-Video, Open-Sora), suggesting decent robustness and broad applicability."}, "weaknesses": {"value": "1. The 110-participant preference study is promising, yet drawn from only 15 prompts with an uneven per-model sample count. More prompts and counterbalancing would strengthen claims.\n2. Why can’t model training learn the calibrated attention pattern? Your observation is that learned temporal attention concentrates on diagonals, under-exploiting cross-frame cues. Couldn’t one add a regularizer encouraging non-diagonals so that the model learns similar CFI-like balancing during training?\n3. The temperature must be manually tuned per model. Will this value also depend on different seeds or even prompts?\n4. The paper does not provide any video examples to support its claims of improved temporal consistency. This omission makes it difficult to judge whether the method truly enhances motion coherence or inadvertently over-constrains frame-to-frame dynamics. Intuitively, amplifying cross-frame attention might suppress desirable temporal variation/dynamics.\n5. The paper claims that the proposed method improves many aspects of video generation and fixes various artifacts, but such broad benefits seem unjustified and overclaimed. The technique is simple, and it is unclear why it could consistently enhance so many dimensions. The shown examples are few; more systematic experiments and diverse cases are needed to support the claims."}, "questions": {"value": "How does the proposed method work on other types of video generation models? e.g., image-to-video models, sound-to-video models, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zMPG192HKQ", "forum": "Z35TJPOalp", "replyto": "Z35TJPOalp", "signatures": ["ICLR.cc/2026/Conference/Submission9623/Reviewer_Fgea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9623/Reviewer_Fgea"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913193256, "cdate": 1761913193256, "tmdate": 1762921159665, "mdate": 1762921159665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Enhance-A-Viseo, it is a training-free, plug-and-play enhancement for DiT-based video generation. By softly amplifying cross-frame attention via a residual “cross-frame intensity” and temperature scaling, it boosts temporal coherence and visual fidelity without extra parameters or noticeable overhead, yielding consistent improvements across multiple leading DiT-based video generation models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured and easy to follow, making it accessible for readers at different levels of expertise.\n2. The proposed Enhance-A-Video is a training-free and plug-and-play method, it is easy to integrate it with existing DiT-based T2V models, including SOTA models like WAN and HunyuanVideo.\n3. The authors have conducted extensive experiments to demonstrate the effectiveness of Enhance-A-Video."}, "weaknesses": {"value": "1. **The visual improvement is not significant**. The visual comparison depicted in Fig. 1, Fig. 6 (b) and Fig. 8 (left) does not present a significant performance gain compared to the original results. Moreover, I doubt that the results in Fig. 1 is actually a cherry-picked one, since current T2V models possess limited capability (or domain knowledge) in generating coherent limb features, but the authors claimed that their method can tackle these issues in a training-free manner without modifying model weights. Similar concerns also come from Fig. 18; I believe the original performance of the Wan2.1-14B model would not produce the blurry and distorted baseline shown by the authors, suggesting this baseline may have been carefully selected to exaggerate the improvement. \n2. **The relationship between the proposed temporal attention operations and the improvement in key visual details is not well clarified**. As far as I concerned, such operations mainly contribute to temporal coherency. How can Enhance-A-Video facilitate the synthesis of image details like the human hand and the dog face? The authors can elaborate on the explanation of such phenomena.\n3. **The paper’s presentation of results is inappropriate**. Most generated videos comprising dozens of frames are represented by only one to four frames, which is insufficient to judge the method’s true efficacy. Because the approach focuses on modifying temporal attention, potential impairments to long-range consistency cannot be ruled out; releasing longer frame sequences is therefore essential for a fair assessment."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rj71YHn5to", "forum": "Z35TJPOalp", "replyto": "Z35TJPOalp", "signatures": ["ICLR.cc/2026/Conference/Submission9623/Reviewer_bnzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9623/Reviewer_bnzY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916413596, "cdate": 1761916413596, "tmdate": 1762921159259, "mdate": 1762921159259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes a training-free framework for improving visual quality and prompt coherence of the video generation model"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the paper presents convincing visualizations and clearly justifies its motivation. the idea of using non-diagonal temporal attention with temperature for better temporal consistency seems to novel for video generation tasks. the module design is simple and intuitive\n- the paper conducted a user study to validate the effectiveness of the proposed method and extensive experiments demonstrate the effectiveness of the proposed method\n- the proposed method achieves reasonable performance improvements with negligible overhead\n- the paper provided source codes in the supplementary material"}, "weaknesses": {"value": "- the paper adopts VBench for quantitative comparisons. however, the reliability of the VBench metrics is still not well justified. based on the reviewer's experience, some scores might favor specific aspects of videos while ignoring the actual visual quality. we can see some of the reported improvements are quite marginal \n- the number of samples used in user study seems to be limited and how those samples are selected is not mentioned (randomly selected or not?)\n- the experimental settings of inference efficiency seem to be unclear: how was the inference time measured? did the authors report a single run time or the averaged time of multiple inference runs\n- in terms of long video generation, what if the proposed method is tested on HunyuanVideo+RIFLEx?\n\n## other comments (not weaknesses)\n\n- it might be slightly better to avoid using wrapfig"}, "questions": {"value": "please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8ddpRe0zfl", "forum": "Z35TJPOalp", "replyto": "Z35TJPOalp", "signatures": ["ICLR.cc/2026/Conference/Submission9623/Reviewer_gH7t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9623/Reviewer_gH7t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255651622, "cdate": 1762255651622, "tmdate": 1762921158751, "mdate": 1762921158751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}