{"id": "Frha3JV1T1", "number": 20759, "cdate": 1758309817799, "mdate": 1759896960061, "content": {"title": "Less is More: Adaptive Coverage Sampling for Synthetic Training Data", "abstract": "Synthetic training data generation with Large Language Models (LLMs) offer a promising solution to the challenge of obtaining large, labeled datasets for training classifiers. \nWhen rapid model deployment is critical, such as in classifying emerging social media trends or combating new forms of online abuse tied to current events, the ability to generate training data is invaluable.\nWhile prior research has examined the comparability of synthetic data to human-labeled data, this study introduces a novel sampling algorithm, based on the maximum coverage problem, to select a representative subset from a synthetically generated dataset. \nOur results demonstrate that training a classifier on this contextually sampled subset achieves superior performance compared to training on the entire dataset. \nThis ``less is more'' approach not only improves model accuracy but also reduces the volume of data required, leading to potentially more efficient model fine-tuning.", "tldr": "", "keywords": ["Large Language Models", "Synthetic Data Generation", "Sampling Algorithms", "Maximum Coverage Problem", "Data Efficiency"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cd865fb7487ea83eb667d4430acb3d07fcee767.pdf", "supplementary_material": "/attachment/1c3781052d479accfae9bda110eeb539968ae84b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Adaptive Coverage Sampling (ACS), a novel and theoretically grounded reformulation of the data selection problem as a maximum coverage problem on a similarity graph. The key idea is to represent synthetic data samples as nodes connected by semantic similarity edges and to identify a compact subset of samples that maximizes representational coverage. The authors establish a monotonicity theorem showing that coverage increases as the similarity threshold decreases, enabling an efficient binary search to find the optimal threshold. They further demonstrate empirically that this monotonicity holds in practice even under greedy approximation. Extensive experiments across multiple NLP tasks show that ACS consistently outperforms heuristic and LLM-based baselines, achieving comparable or superior performance using only a small fraction of the synthetic data. Interestingly, the results also suggest that smaller, carefully selected subsets can sometimes improve model performance compared to training on the entire synthetic dataset, highlighting the effectiveness of principled data reduction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The paper presents a novel and theoretically grounded formulation of the data selection problem as a graph-based maximum coverage optimisation. This framing is both elegant and insightful, connecting efficient data selection with submodular optimisation and thereby enabling the use of greedy algorithms with established approximation guarantees.\n\n* The authors provide a sound theoretical analysis for the ideal (exact) case and an empirical validation showing that the monotonicity property also holds under the greedy approximation. This property justifies the use of an efficient binary search to automatically determine the optimal similarity threshold for graph construction.\n\n* The paper addresses a timely and practically important challenge: how to effectively sample and curate synthetic data to improve downstream model performance, a problem that is becoming increasingly relevant as synthetic data becomes more widespread.\n\n* The paper includes comprehensive experimental evaluations across multiple NLP tasks, demonstrating that ACS achieves significant efficiency gains over existing methods."}, "weaknesses": {"value": "* Although the formulation of the paper is compelling, the writing sometimes makes it difficult to follow the motivation behind searching for the maximum threshold that achieves sufficient coverage. In particular, the connection between threshold selection and downstream performance is not always clearly motivated or illustrated.\n\n* The proposed ACS method implicitly assumes that the synthetic-data distribution and the target downstream distribution are closely aligned, meaning that nearly all relevant points are coverable in the constructed graph. However, in synthetic-data settings (as the paper emphasises), this assumption may not hold: synthetic data can include hallucinations, label inconsistencies, or missing regions of the target space. Recent works [1], [2] argue that distribution alignment and data quality are as important as diversity or coverage. The paper would be strengthened by explicitly discussing this assumption and clarifying under what conditions ACS remains effective when the synthetic and target distributions diverge.\n\n* The theoretical connection between the maximum-coverage solution (based on the maximum threshold) and other established objectives, such as maximising information gain under identical training and testing distributions, is not explored. Establishing such a connection could strengthen the theoretical impact and situate the contribution more firmly within the broader literature.\n\n* The computation of pairwise similarities can be costly, particularly for large datasets. In addition, obtaining high-quality embeddings often requires using larger and more expressive models, which independently increases the computational and memory cost of the overall method.\n\n[1] Kuo et al., Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification, ICLR 2025\n\n[2] Li et al., Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations, EMNLP 2023"}, "questions": {"value": "* The citation formatting could be improved. Many in-text citations use `\\citet`, while `\\citep` would be more appropriate for parenthetical references in formal academic writing.\n\n* The ACS method builds on the submodular property of the coverage function, but the similarity threshold is determined through binary search rather than being incorporated directly into the objective. Could the authors explain why they did not consider alternative monotone submodular objectives, such as information gain [3] or a weighted coverage formulation [3], defined as  \n  $$f(S) = \\sum_{(i,j)\\in E} s(i,j) \\mathbf{1}(i \\in S \\text{ or } j \\in S),$$  \n  where $s(i,j)$ denotes the similarity between nodes $i$ and $j$, $E$ is the set of edges, and $\\mathbf{1}(\\cdot)$ is an indicator function that counts each edge once if either endpoint is selected. This weighted formulation remains monotone and submodular, retains the same $(1 - 1/e)$ approximation guarantee of the greedy algorithm, and removes the need to tune a similarity threshold.\n\n\n* The ACS framework depends on two key hyperparameters: the coverage target $c$ and the budget $k$. How sensitive are the results to these parameters, and how should they be selected in practice?\n\n[3] Andreas Krause and Daniel Golovin. Submodular Function Maximization. In Tractability: Practical Approaches to Hard Problems, Cambridge University Press, 2014."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SRhIIow5VT", "forum": "Frha3JV1T1", "replyto": "Frha3JV1T1", "signatures": ["ICLR.cc/2026/Conference/Submission20759/Reviewer_Rg75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20759/Reviewer_Rg75"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582214319, "cdate": 1761582214319, "tmdate": 1762934183463, "mdate": 1762934183463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes selecting a representative subset from the generated dataset by framing it as a maximum-coverage problem. The algorithm allows using only 10% of the data to achieve similar performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The algorithm is intuitive.\n2. The experiments demonstrate improvement over several baselines, including random selection and AlphaGasus."}, "weaknesses": {"value": "1. The main experiment is based on data generated by GPT-3.5. I am doubtful we can achieve similar results with more recent models like GPT-4o-mini, which should be much cheaper and more intelligent.\n2. While the description of the used baseline is limited, I can see that a lot of data selection baselines are missing: (I) k-center / k-medoids / coverage and other classic algorithms. (I) Deduplication approach used in pretraining [1][2] and finetuning [3]\n\n[1] D4: Improving LLM Pretraining via Document De-Duplication and Diversification\n\n[2] SemDeDup: Data-efficient learning at web-scale through semantic deduplication\n\n[3] NLU on DataDiets: Dynamic Data Subset Selection for NLP Classification Tasks"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmyGhjzX1Y", "forum": "Frha3JV1T1", "replyto": "Frha3JV1T1", "signatures": ["ICLR.cc/2026/Conference/Submission20759/Reviewer_GnUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20759/Reviewer_GnUP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867561399, "cdate": 1761867561399, "tmdate": 1762934182359, "mdate": 1762934182359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formulates downsampling and de-redundancy as a graph-based maximum k-coverage problem and uses a greedy approximation with threshold binary search; experiments on SST-2, FewRel, and CrossNER-AI validate its advantages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is the first to model de-redundancy of LLM-synthesized datasets as a graph based maximum k coverage problem; it provides a formal definition of coverage and proves that, under a similarity thresholded graph, the optimal maximum coverage solution is monotone in the threshold.\n2. Across SST-2, FewRel, and CrossNER-AI, the method often surpasses baselines\n3. Scalability is handled by tuning the threshold on a small subsample and transferring it to the full set; a Hoeffding-type bound supports this, and thresholds learned from under 20% of the data reliably meet the target coverage, with or without a degree cap."}, "weaknesses": {"value": "1. When the de-redundancy of LLM-synthesized datasets is reduced to a graph-based maximum k-coverage problem, the employed approach using **greedy approximation** and **threshold binary search** is standard practice rather than an innovation. For example:\n[1] Lin, Hui, Jeff Bilmes, and Shasha Xie. \"Graph-based submodular selection for extractive summarization.\" *2009 IEEE Workshop on Automatic Speech Recognition & Understanding*. IEEE, 2009.\n[2] Biabani, Leyla, et al. \"Faster Query Times for Fully Dynamic $ k $-Center Clustering with Outliers.\" *Advances in Neural Information Processing Systems* 36 (2023): 9226-9247.\n2. The experimental evaluation remains insufficient.\n    - It does not compare against alternative algorithms for the maximum k-coverage problem, including submodular maximization and k-center-based solvers.\n    - It omits head-to-head comparisons with coverage-centric coreset methods that target representativeness under a fixed budget.\n    - It lacks baselines from standard dataset de-duplication pipelines, such as near-duplicate detection, leaving the advantages over de-redundancy untested."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "52yTAaGwuu", "forum": "Frha3JV1T1", "replyto": "Frha3JV1T1", "signatures": ["ICLR.cc/2026/Conference/Submission20759/Reviewer_8cs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20759/Reviewer_8cs9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915249436, "cdate": 1761915249436, "tmdate": 1762934181383, "mdate": 1762934181383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to select representative subsets (subsets) of data points from a synthetically generated corpus. It relies in building a similarity graph over the data points, i.e., data points are nodes and edges similarities. The with a greedy graph traversal a subset of nodes is extracted. The hyper-parameters are the threshold on edge similarities and number of extracted nodes. Varying these two it is possible to obtain different samples with different diversity and coverage of the underlying entire corpus. The motivation for such proposal is the usually observed data imbalance and bias amplification in synthetic datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- A method to select varied samples for data imbalance."}, "weaknesses": {"value": "- Weak motivation. That LLMs serve to generate training data for downstream tasks and then fine-tune small models. If the LLM model can generate data, then can be used directly to solve the task.\n\n- Differences between methods seem small (Figure 3/4/5). Bigger differences seem in regions where F1 scores are not the best.\n\n- Experiments are carried out on old BERT models. It is unclear how the sampling method would help newer LLMs.\n\n- Scalability of the approach. A method for efficient and high quality data sampling would be very useful for very large corpora (i.e., LLM pre-training and fine-tuning). The graph similarity construction seems to be rather expensive in these cases."}, "questions": {"value": "- Sample variation is assessed via SelfBLEU, is it possible to also show number of instances per class in the target samples (in comparison also with the exiting reported methods)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zTj3bBD75C", "forum": "Frha3JV1T1", "replyto": "Frha3JV1T1", "signatures": ["ICLR.cc/2026/Conference/Submission20759/Reviewer_GKGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20759/Reviewer_GKGn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184367353, "cdate": 1762184367353, "tmdate": 1762934180633, "mdate": 1762934180633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}