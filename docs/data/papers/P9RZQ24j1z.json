{"id": "P9RZQ24j1z", "number": 23340, "cdate": 1758342432275, "mdate": 1759896819988, "content": {"title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models", "abstract": "DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement—detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.", "tldr": "", "keywords": ["Code Generation", "Telemetry-Guided", "LLM Evaluation", "Software Development", "Ecological Validity", "Functional Correctness", "Similarity Metrics", "Contamination-Free Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5010a1df177efdc2709ebd89066cd0218b31aa34.pdf", "supplementary_material": "/attachment/3cb541897639d5f8a77ca2e88c124c278057f3a5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DevBench, a telemetry-driven benchmark aimed at evaluating code generation models in realistic coding scenarios. The benchmark is constructed from over one billion developer code completion **telemetry records**, **based on which** 1,800 evaluation tasks spanning six major programming languages and six developer-task categories are **synthesized by GPT-4o**. DevBench is designed to reflect genuine developer behaviors and challenges, to be contamination-resilient, and to support fine-grained diagnostics. The authors evaluate nine state-of-the-art LLMs using multiple metrics, including functional correctness, similarity-based measures, and LLM-judge (AI-assistant) evaluation, revealing nuanced model strengths and weaknesses, especially in challenging categories like code-to-natural language translation and syntax completion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Ecological validity & methodology**: DevBench uniquely leverages a massive corpus of real developer telemetry to guide the selection of task categories, scenario design, and dataset construction, a substantial step forward versus synthetic or challenge-style code benchmarks. The benchmark explicitly reflects actual developer needs.\n\n**Broad language and task coverage**: The benchmark encompasses six major languages and is categorized across multiple realistic yet distinct development tasks (API Usage, Code Purpose Understanding, Code2NL/NL2Code, Low Context, Pattern Matching, Syntax Completion). \n\n**Multi-metric, nuanced evaluation**: The use of functional correctness, two similarity measures, and LLM-judging allows for a nuanced multi-perspective understanding of model capabilities, strengths, and weaknesses.\n\n**Transparency and reproducibility**: The authors provide comprehensive details of experimental setup, infrastructure, prompting strategies (Appendix E), failure handling, and plan to release the benchmark and generation infrastructure."}, "weaknesses": {"value": "**Insufficient Data Difficulty and Quality Justification**: The manuscript does not convincingly demonstrate that the synthetic instances are hard enough to differentiate state-of-the-art models. Pass@1 scores above 80 % on almost every tested model suggest a ceiling effect, which limits the benchmark’s discriminative power. No ablation or sensitivity analysis is provided to show how task difficulty was tuned or validated against real-world complexity distributions.\n\n**About Data Diversity**: The code completion data is synthesized using the same prompt within the same language and the same task category. Although the prompts try to make the generated code diverse, the internal diversity under the results generated by the same prompt is not proved in the paper.\n\n**Generator-Induced Bias Not Ruled Out**: Relying on GPT-4o to synthesise the evaluation set creates an obvious source of stylistic and distributional bias. The claim that “GPT-family models do not win everywhere, therefore no bias exists” is logically flawed; disparate model rankings can coexist with systematic generator bias. The paper lacks statistical tests (e.g., distributional overlap, pattern frequency analysis) to quantify or exclude this bias.\n\n\n**Potential Training-Data Contamination via Synthesized data**: Although the instances are synthetic, they are produced by a model (GPT-4o) trained on vast public code corpora. The training data of GPT-4o already contains a large amount of GitHub code, and using it to \"synthesize\" new questions is essentially a disguised reproduction of the patterns in the training data. The assertion that synthesis automatically guarantees contamination-free data is therefore unsubstantiated.\n\n**Ambiguous Human Review Protocol**: The human-validation pipeline is described only qualitatively. Inter-annotator agreement, detailed scoring rubrics, complexity thresholds, and realism criteria are not reported, making the review stage non-reproducible. Without these controls, the claim that realism was “prioritised” cannot be verified or replicated by other researchers."}, "questions": {"value": "With only 1,800 tasks distilled from >1 billion telemetry events, what sampling scheme and representativeness metrics guarantee that the final set mirrors real-world usage distributions?\n\nThe six task categories were derived from telemetry and then “refined” by the authors without releasing the clustering algorithm or inter-rater overlap scores; how can we be sure the taxonomy is exhaustive and mutually exclusive?\n\nWhy does “GPT-4o does not always win” equal “no bias” without statistical tests?\n\nWhat exact rules kept human reviewers from accepting too-simple or textbook tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LmHuMp1R5p", "forum": "P9RZQ24j1z", "replyto": "P9RZQ24j1z", "signatures": ["ICLR.cc/2026/Conference/Submission23340/Reviewer_rLDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23340/Reviewer_rLDn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907357174, "cdate": 1761907357174, "tmdate": 1762942613287, "mdate": 1762942613287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DevBench, a new benchmark for evaluating code generation models, specifically on code completion tasks. The benchmark's primary contribution is its \"telemetry-driven\" design, where the task categories are derived from an analysis of over one billion real-world developer interactions, aiming for high \"ecological validity\". To avoid data contamination and protect privacy, the benchmark consists of 1,800 synthetically generated instances that were subsequently validated by human experts. DevBench spans six programming languages (Python, JS, TS, Java, C++, C#) and six task categories (e.g., API Usage, Code Purpose Understanding, Pattern Matching). The paper evaluates 9 different models using a combination of functional correctness (Pass@1), similarity metrics, and LLM-judge scoring."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Focus on Ecological Validity and Contamination Resistance: The paper's core motivation—to create a benchmark grounded in \"observed developer behavior\" rather than arbitrary open-source scrapes—is a significant strength. The use of a synthetic generation pipeline based on telemetry-derived patterns, rather than using the telemetry data directly, is a clever approach to avoiding privacy issues and, crucially, training data contamination.\n\n\n\n* Human-in-the-Loop Validation: The inclusion of a rigorous, multi-annotator human review process to validate the realism, usefulness, and complexity of each generated instance is a high-quality step that many benchmarks lack .\n\n\n\n* Detailed Diagnostics: The benchmark's structure, spanning 6 languages and 6 categories, provides a fine-grained framework for diagnosing model strengths and weaknesses (e.g., the case study on DeepSeek-V3) beyond a single aggregate score."}, "weaknesses": {"value": "1. Contradiction in Benchmark Difficulty: The primary weakness is the conflict between the paper's claims of high complexity (high cyclomatic complexity in Table 3) and the high Pass@1 scores in Table 5. A benchmark with an 84.8% Pass@1 for the best model (and 90.3% in one category ) is not a challenging benchmark. This high pass rate suggests the benchmark fails in its primary goal of rigorously evaluating and differentiating SOTA models.\n\n\n\n2. Lack of Transparency in Data Curation: The process of moving from \"one billion developer interactions\" to 1,800 instances is a black box. The paper uses vague terms like \"sampling\" and \"discussions\" without providing a systematic, reproducible methodology. This opaqueness undermines the central claim of being \"telemetry-driven,\" as the selection of \"common challenging scenarios\" appears subjective and unverifiable.\n\n\n\n3. Insufficient Model Evaluation: The paper fails to evaluate a sufficient range of models to demonstrate its discriminative power. The evaluation primarily includes a few top-tier proprietary models and one small 3B model, with a large gap in between. The lack of evaluation on widely-used open-source models (e.g., Code Llama 7B/13B/34B, or other intermediate-sized models) makes it impossible to assess the benchmark's utility for the broader research community. A benchmark should be able to show a clearer performance gradient across the spectrum of model sizes and families."}, "questions": {"value": "1. On the Pass Rate Contradiction: Could the authors please address the apparent contradiction between the high cyclomatic complexity (Table 3) and the very high Pass@1 rates (Table 5)? Does this imply that the benchmark, while syntactically complex, is testing semantically simple or trivial problems? Given the 85-90% scores, how do you justify this benchmark's utility for measuring future SOTA progress?\n\n2. On Telemetry Sampling: Could the authors provide a more detailed, systematic methodology for how the \"over one billion\" telemetry interactions were sampled and analyzed to derive the six categories and inform the 1,800 instances? What sampling strategy (e.g., stratified, random) was used? What were the quantitative criteria for a \"common challenging completion scenario\"?\n\n3. On Model Suite: Why were popular and strong open-source models, such as those from the Code Llama family or other models in the 7B-70B range, excluded from the evaluation? Evaluating these models would be crucial to demonstrate that the benchmark can actually differentiate between the models that most researchers actively use and develop."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RfaAyIzzWM", "forum": "P9RZQ24j1z", "replyto": "P9RZQ24j1z", "signatures": ["ICLR.cc/2026/Conference/Submission23340/Reviewer_dxVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23340/Reviewer_dxVV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988366795, "cdate": 1761988366795, "tmdate": 1762942613068, "mdate": 1762942613068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DevBench is a code-understanding benchmark that evaluates LLMs on functional correctness, similarity-based metrics, and LLM-judge assessments of usefulness and contextual relevance. The benchmark is large, comprising 1800 instances.\nHowever, as I am not particularly familiar with code benchmarks, I cannot reliably assess the novelty and contributions of this paper relative to existing code-related benchmarks. I therefore recommend that the AC omit my review and prioritize those from reviewers with higher confidence scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "no grammar error"}, "weaknesses": {"value": "1. Transparency and Reproducibility: The generation pipeline, prompts, and full dataset are open-sourced. LLM-judge is validated against human annotations (strong correlation), and confidence intervals are reported for robustness.\n2. Figure 1 (the end-to-end pipeline) is overly simplistic and misunderstanding.\n3. LLM-judge may be biased. In what case would a response be assigned with low score is unknown in the paper."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "Ve4SxetyrD", "forum": "P9RZQ24j1z", "replyto": "P9RZQ24j1z", "signatures": ["ICLR.cc/2026/Conference/Submission23340/Reviewer_cuxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23340/Reviewer_cuxu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992540331, "cdate": 1761992540331, "tmdate": 1762942612750, "mdate": 1762942612750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DevBench is a code generation benchmark built on real developer telemetry data, featuring 1,800 test cases across six programming languages (Python, JavaScript, TypeScript, Java, C++, and C#) and six categories: API usage, code purpose understanding, code-to-natural language translation, low-context completion, pattern matching, and syntax completion. These categories come from analyzing over 1 billion actual developer code completion interactions. Unlike existing benchmarks, DevBench uses synthetic generation to avoid training data contamination while keeping things realistic through expert human review, and it evaluates models in three ways: functional correctness (Pass@1), similarity metrics, and LLM-based judging for relevance and usefulness. Testing 9 state-of-the-art models revealed some interesting findings: Claude 4 Sonnet leads with 84.80% Pass@1, but GPT-4o tops the LLM-judge rankings, showing that reasoning capabilities boost functional correctness but don't necessarily align with practical usefulness. Low-context completion proved easiest (top models hit 87-90%), while code-to-language translation was toughest (even Claude 4 Sonnet only managed 78.90%), and TypeScript consistently performed worst across all languages. The benchmark's diagnostic value shines through in the DeepSeek-V3 case study: the model shows high similarity but lower correctness in pattern matching, suggesting it relies too much on memorization rather than semantic understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper grounds its benchmark in real-world developer workflows by analyzing over 1 billion actual code completion interactions, rather than just scraping public repositories. This means the evaluation is based on scenarios that developers actually encounter in practice, not hypothetical tasks. \n\n2. The contamination-resistant design using synthetic generation is quite timely and addresses a growing concern in the field. The multi-dimensional evaluation framework is particularly insightful, for example, it reveals that DeepSeek-V3 achieves high syntactic similarity but lower functional correctness in pattern matching, something you'd completely miss with single-metric approaches.\n\n3. The paper is well-written and clearly organized. It walks you through the entire pipeline, from motivation to design, construction, evaluation, and results, in a logical way that's easy to follow. The extensive doc and appendices are helpful for understanding the details."}, "weaknesses": {"value": "1. While the categories come from massive-scale telemetry, the actual evaluation instances are still synthetic. To comply with privacy requirements, the authors explicitly avoid using raw user code and instead have GPT-4o generate instances based on templates, which are then validated through automatic and manual checks. This approach reduces the risk of \"dirty data,\" but it might also miss the messy context of real code, things like traces of multi-person collaboration, cross-file dependencies, and the general complexity that comes with actual codebases.\n\n2. The LLM-judge evaluation lacks rigorous validation. Although the paper claims \"strong correlation\" with human ratings, there are no quantitative correlation coefficients reported. The validation set of just 150 completions (25 per language) seems quite limited when you're using it to validate judgments on over 9,000 completions. There's no inter-annotator agreement statistics (Cohen's kappa, ICC), and the significant gap between LLM-judge rankings (GPT-4o leads) and Pass@1 (GPT-4o ranks 5th) isn't adequately explained.\n\n3. Category orthogonality hasn't been validated, there's no correlation analysis between category performances, no factor analysis of underlying capability dimensions, and potential overlaps (like \"Code Purpose Understanding\" vs. \"Pattern Matching,\" both requiring semantic reasoning) remain unexplored.\n\n4. More seriously, the benchmark shows an obvious ceiling effect problem. Claude and GPT-series models are already achieving pretty high scores across multiple tasks, Claude 4 Sonnet hits 84.80% Pass@1, and the Low Context category even reaches 87-90% success rates. This suggests the benchmark isn't challenging enough for frontier models. Given how rapidly model performance is improving, this benchmark could saturate pretty quickly and lose its ability to discriminate between top-tier models. That's a significant limitation for a benchmark that claims to \"support future research.\""}, "questions": {"value": "See the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQe2DG2e5X", "forum": "P9RZQ24j1z", "replyto": "P9RZQ24j1z", "signatures": ["ICLR.cc/2026/Conference/Submission23340/Reviewer_Toow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23340/Reviewer_Toow"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213944039, "cdate": 1762213944039, "tmdate": 1762942612312, "mdate": 1762942612312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}