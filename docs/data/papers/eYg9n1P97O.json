{"id": "eYg9n1P97O", "number": 10351, "cdate": 1758167921084, "mdate": 1759897656695, "content": {"title": "Towards Efficient SNNs: Sensitivity-Guided Pruning for Deep Spiking Architectures", "abstract": "Spiking Neural Networks (SNNs) offer compelling advantages in energy efficiency and biological plausibility but face performance and deployment challenges due to redundant structural units in suboptimal architectures. Existing compression techniques predominantly rely on unstructured connection-level pruning, which often necessitates specialized hardware for efficient execution. To overcome these limitations, we propose STE (Sensitivity-guided pruning by Taylor Expansion), a structured pruning framework that leverages Taylor expansion to estimate each convolutional kernel's sensitivity to the loss function during training. This enables the iterative removal of less critical components. Extensive experiments across four benchmark datasets demonstrate the effectiveness of STE. Remarkably, STE achieves 78.09\\% connectivity sparsity on CIFAR10 with a +1.49\\% accuracy gain, outperforming previous state-of-the-art methods in both performance and model compactness.", "tldr": "A pruning method for spiking neuron network about sentivity.", "keywords": ["spiking neural networks"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0200e38ef42c4b9d7566b694ad318eb9664ccb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a Taylor-expansion-based structured pruning method for SNNs, and validates its performance on static and neuromorphic datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. In addition to the inference accuracy after pruning, this work also conducted statistical analysis on inference speed."}, "weaknesses": {"value": "1. As shown in Tab.2, the performance comparison for this work is too simple:\n- Firstly, some works that maintain superior performance on SNNs with extremely high sparsity have not been included. \n- Secondly, this work did not include Synaptic Operations (SOPs), which is an important indicator for evaluating SNN power consumption. \n- Thirdly, the results presented in this work are all under conditions of low sparsity (Connectivity > 50%), without verifying the performance of the method under conditions of extremely high sparsity. Some cases even experienced accuracy loss (Acc. Loss < 0%) under the conditions of Connectivity > 50%. \n- In addition, this work only conducted performance validation on convolutional architectures and did not conduct performance validation on large-scale datasets (e.g. ImageNet-1k). \n- The authors should consider using a network structure consistent with the comparative works for clear and detailed performance comparison. Meanwhile, the inference accuracies achieved in this work is clearly unsatisfactory in the current SNN community.\n\n2. Pruning towards the synaptic layers is just one solution to reduce the power consumption of SNNs. Other techniques such as pruning for neuron layers and lightweight quantization for SNNs have also been proposed. Therefore, I tend to think the contribution of this work to the SNN community is relatively limited.\n\n3. The layout of figures, tables and formulas in this paper still needs further polishing.\n\n[1] Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework. ICLR 2024.\n\n[2] QP-SNN: Quantized and Pruned Spiking Neural Networks. ICLR 2025."}, "questions": {"value": "See Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tV6NQpLLDE", "forum": "eYg9n1P97O", "replyto": "eYg9n1P97O", "signatures": ["ICLR.cc/2026/Conference/Submission10351/Reviewer_cwzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10351/Reviewer_cwzv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470444481, "cdate": 1761470444481, "tmdate": 1762921682251, "mdate": 1762921682251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents STE, an innovative structured pruning framework for SNNs, which effectively tackles the challenge of network compression. Extensive experiments highlight its promising capability to significantly reduce model size and FLOPs across various benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovation Method: Hardware-Friendly Structured Pruning Framework. By using first-order Taylor expansion for sensitivity evaluation, STE performs channel- or kernel-level pruning that aligns with the spatiotemporal characteristics of SNNs.\n2. Superior Compression–Accuracy Trade-off: STE achieves remarkable model compression while maintaining or even improving accuracy across multiple benchmarks."}, "weaknesses": {"value": "1. The citation style is inconsistent and does not conform to the official ICLR formatting guidelines.\n2. Related Work:\nThis section lacks depth and persuasiveness due to an insufficient number of relevant citations (which I think is incomprehensible). A qualified study needs to be supported and corroborated by previous papers. Expanding the discussion to include more recent and influential SNN pruning and optimization works would strengthen the context of this research.\n3. The claim of “ensuring compatibility and performance retention across diverse architectures” (lines 069–070) appears overstated, given that experiments are restricted to VGG- and ResNet-style networks.\n4. The results in Table 2 raise concerns: under identical architectures, the proposed STE method occasionally performs worse than competing pruning techniques."}, "questions": {"value": "Please refer to “Weaknesses”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yt4BGI4qcB", "forum": "eYg9n1P97O", "replyto": "eYg9n1P97O", "signatures": ["ICLR.cc/2026/Conference/Submission10351/Reviewer_Zx95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10351/Reviewer_Zx95"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821984303, "cdate": 1761821984303, "tmdate": 1762921681550, "mdate": 1762921681550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ​​STE (Sensitivity-guided pruning by Taylor Expansion)​​, a structured pruning framework for Spiking Neural Networks (SNNs). STE uses Taylor expansion to estimate the sensitivity of each convolutional kernel to the loss function, guiding the pruning process to preserve important structures for efficient execution on general-purpose hardware."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The focus on ​​structured pruning​​ is a benefit, as it produces hardware-friendly models that can run efficiently on standard GPUs and CPUs without the need for specialized sparse acceleration hardware.\n\nThe method appears to be straightforward and simple, building on the established concept of Taylor expansion for importance estimation. This makes it potentially easy to understand and implement.\n\nBy estimating sensitivity during the training process, the method can dynamically adapt the pruning strategy, potentially leading to better preservation of accuracy compared to one-shot pruning methods."}, "weaknesses": {"value": "A major concern is that the core framework of the proposed method may not be sufficiently novel, potentially being an incremental application of existing ideas to ANNs. The use of channel/kernel sensitivity evaluation via Taylor expansion is a known technique in the ANN literature, and the paper may not demonstrate enough adaptation or innovation to make it compelling for the SNN domain.\n\nTable 2 compares the method against too few existing state-of-the-art benchmarks, making it difficult to assess its true competitive standing.\n\nThe citation format is problematic (e.g., \"Recent studies Lietal. (2024a;b)\"). Please use \\citep appropriately. \nSections 2.1 and 2.2 lack citations entirely, and 2.3 has too few, weakening the literature review.\n\nThe use of \"So\" at the beginning of a sentence (Line 191) is inappropriate for academic writing.\n\nThe acronym \"STE\" is already widely used in the SNN field for \"Straight-Through Estimator,\" which is the standard method for training SNNs. This creates immediate and significant confusion."}, "questions": {"value": "What is the specific novel contribution of this work that differentiates it from simply applying existing ANN structured pruning techniques to SNNs?\n\nThe method is demonstrated on convolutional architectures. Can the proposed framework be effectively applied to more modern, Transformer-like SNN architectures?\n\nThe paper reports inference speed on a GPU. However, a key motivation for using SNNs is their efficiency on neuromorphic hardware. What are the expected benefits or performance of the pruned models on neuromorphic processors, and why was this not evaluated?\n\nGiven the critical name conflict with \"Straight-Through Estimator (STE),\" would the authors consider changing the name of their method to avoid confusion and improve the identity of their work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DNaPAkfhRZ", "forum": "eYg9n1P97O", "replyto": "eYg9n1P97O", "signatures": ["ICLR.cc/2026/Conference/Submission10351/Reviewer_wA6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10351/Reviewer_wA6y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892903733, "cdate": 1761892903733, "tmdate": 1762921680846, "mdate": 1762921680846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel structured pruning framework for SNNs named STE (Sensitivity-guided pruning by Taylor Expansion). The method leverages first-order Taylor expansion to estimate the sensitivity of convolutional kernels to the loss function, enabling the iterative removal of less critical components in a structured manner. The pruned models show significant reductions in FLOPs and improvements in inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully adapts a principled, Taylor-expansion-based sensitivity analysis from ANNs to the SNN domain, addressing a significant gap in structured pruning for spiking architectures. The approach is well-motivated by the limitations of existing unstructured pruning methods.\n\n2. The paper provides valuable insights beyond mere accuracy and sparsity numbers. The analysis of sensitivity distribution across layers (Figure 3) offers an intuitive explanation for why deeper layers can be pruned more aggressively, strengthening the methodological foundation."}, "weaknesses": {"value": "1. While the paper compares favorably against other SNN pruning methods, it would be strengthened by a brief discussion or comparison with state-of-the-art structured pruning techniques applied to ANNs on the same tasks. This would help contextualize STE's performance within the broader model compression field.\n\n2. The description of the iterative pruning process could be more detailed. Specifically, the total number of pruning iterations, the schedule for fine-tuning (e.g., number of epochs per iteration), and the associated computational cost are not explicitly stated."}, "questions": {"value": "1. The method calculates sensitivity over the entire temporal dimension (T). How does the performance of STE change with a different number of timesteps? Is the sensitivity metric consistently reliable across varying temporal sequence lengths?\n\n2. Can this approach be extended to other mainstream neural network frameworks like spiking transformers? \n\n3. Can this approach be extended to more large-scale datasets/tasks?\n\n4. Can this approach be extended to more non-vision tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wu74K0OoVL", "forum": "eYg9n1P97O", "replyto": "eYg9n1P97O", "signatures": ["ICLR.cc/2026/Conference/Submission10351/Reviewer_JBuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10351/Reviewer_JBuL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974696970, "cdate": 1761974696970, "tmdate": 1762921680177, "mdate": 1762921680177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}