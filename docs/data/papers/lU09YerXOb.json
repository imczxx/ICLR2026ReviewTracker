{"id": "lU09YerXOb", "number": 7785, "cdate": 1758036237139, "mdate": 1759897832524, "content": {"title": "Arboreal Neural Network", "abstract": "Recent advancements in deep learning and Large Language Models (LLMs) have significantly influenced fields such as Natural Language Processing (NLP), Computer Vision (CV), and audio analysis. However, in the domain of structured tabular data, tree-based models like Gradient Boosted Decision Trees (GBDTs) remain dominant. In this paper, we propose Arboreal Neural Network (ArbNN), a novel model for tabular data that combines the end-to-end learnability of neural networks with the inductive bias and interpretability of decision trees. We achieve this by recasting the discrete logic of a decision tree into a fully differentiable neural structure (dubbed ArborCell), which serves as the basic unit of ArbNN. Moreover, we theoretically prove that the computations in ArborCell are equivalent to a specific self-attention mechanism, providing a new perspective on tree-based models. In addition, we present TabCredit, a large-scale industrial credit-risk dataset designed to enable realistic evaluation under conditions of temporal drift and extensive feature engineering. These two factors are typically lacking in existing benchmarks yet critical for tabular data tasks in real-world scenarios. Empirical evaluations on both public benchmarks and the TabCredit dataset demonstrate ArbNN’s superior performance compared to State-of-the-Art (SOTA) neural network architectures and traditional GBDT methods. ArbNN has been successfully deployed in real-world credit risk management systems, processing millions of loan applications and supporting monthly transactions in billions of U.S. dollars, demonstrating its robustness and industrial value. The code and TabCredit dataset will be released upon acceptance of this paper.", "tldr": "A novel model for tabular data-based predictive modeling that combines the benefits of end-to-end learnability with the interpretability of decision trees.", "keywords": ["Tabular data analysis", "Decision trees", "Neural networks", "Neural-tree models", "Credit-risk dataset", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5768c507bbd35131fd0181eea78399ad627f2582.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Arboreal Neural Networks (ArbNN), a differentiable architecture that bridges gradient-boosted decision trees and neural networks. The key idea is to encode a pretrained XGBoost model into a neural form by translating its structure—feature splits, thresholds, and leaf values—into matrix operations that can be optimized end-to-end. This allows the model to retain the interpretability and inductive bias of trees while gaining the flexibility of gradient-based learning. Experiments on eight public tabular datasets and one large industrial credit dataset (TabCredit) show that ArbNN consistently matches or outperforms strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a novel and well-structured idea that combines the structural bias of decision trees with the flexibility of neural networks. The concept is intuitive yet original, and the formulation is clearly presented.\n\nThe writing is clean and logically organized, making the technical details easy to follow. The experiments are thorough within the chosen scope and demonstrate consistent improvements over strong baselines such as XGBoost."}, "weaknesses": {"value": "- **Limited Benchmark Coverage**\n\nThe evaluation includes only eight public datasets, which is considerably below the current standard in the tabular learning community. This narrow benchmark scope limits the credibility of the claimed generalization. Given the model’s conceptual promise, it would be valuable to test ArbNN on a broader set of heterogeneous tabular tasks.\n\n- **Unclear Motivation and Overemphasis on Industrial Data**\n\nThe paper’s motivation is not fully convincing. Although the central idea—learning the structural bias of trees—is conceptually interesting, the claimed interpretability advantage remains unsubstantiated, as XGBoost provides only limited transparency. It appears that the work may be driven by a specific industrial objective, possibly related to the proprietary dataset used. If so, this motivation can be stated explicitly and the framing adjusted accordingly. Clarifying how the industrial requirements connect to the model’s broader scientific contribution and analyze why previous dl models perform worse, would significantly strengthen the paper’s coherence and impact."}, "questions": {"value": "See Above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BGPOfTTjHn", "forum": "lU09YerXOb", "replyto": "lU09YerXOb", "signatures": ["ICLR.cc/2026/Conference/Submission7785/Reviewer_M5dz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7785/Reviewer_M5dz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760932252, "cdate": 1761760932252, "tmdate": 1762919828347, "mdate": 1762919828347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Arboreal Neural Networks (ArbNNs), a framework that converts trained decision trees into differentiable neural operators called ArborCells. Each ArborCell encodes a tree’s split features, thresholds, structure, and leaf values into four matrices/vectors, enabling end-to-end optimization while preserving the original tree semantics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A differentiable “tree-as-layer” formulation (ArborCell) with explicit feature–node selection matrix $W$, split-threshold vector $f$, tree-structure / routing matrix $P$, and leaf-value vector $v$ that avoids path-probability products via one-shot matrix aggregation\n2. An algorithm to parse trees into ArborCells and the ability to decompile trained ArborCells back to refined trees, maintaining symbolic interpretability\n3. Competitive performance on public tabular tasks and consistent vintage-curve improvements over XGBoost on TabCredit under temporal drift\n4. Introduction of TabCredit, an industrial credit-risk dataset with temporal splits to benchmark robustness and interpretability in realistic settings"}, "weaknesses": {"value": "1. The experimental section does not include comparisons with strong, modern baselines, especially tabular foundation models.\n\n2. Limited gains over XGBoost in Table 2 relative to method complexity. On the reported datasets, the improvement over a well-tuned XGBoost baseline is small.\n\n3. The paper evaluates on a relatively small set of benchmarks\n\n4. Dependence on pretrained tree models for initialization. The core recipe assumes the availability of a strong GBDT (XGBoost/LightGBM) to parse into ArborCells. This limits applicability in settings where (i) trees are hard to train well, or (ii) one would like to learn the structure jointly with the downstream objective. The paper does not show a convincing “from-scratch ArbNN” alternative."}, "questions": {"value": "1. Can the authors add comparisons with recent tabular foundation models (e.g., TabPFNv2 [1], TabICL [2])?\n\n2. Can the authors clarify the necessity of GBDT-based initialization? The current version treats “compiling from a strong GBDT” as a given prerequisite, but there is no experiment demonstrating whether ArbNN can still achieve comparable performance.\n\n3. Can the authors provide more detail on scalability and serving? Since each ArborCell does a one-shot aggregation over all leaves, how does inference time and memory compare to the original XGBoost model. A brief complexity analysis or inference time comparison would make the method more practical.\n\n[1] Hollmann, Noah, et al. \"Accurate predictions on small data with a tabular foundation model.\" Nature 637.8045 (2025): 319-326.  \n[2] Qu, Jingang, et al. \"Tabicl: A tabular foundation model for in-context learning on large data.\" ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Jxp05cWrc", "forum": "lU09YerXOb", "replyto": "lU09YerXOb", "signatures": ["ICLR.cc/2026/Conference/Submission7785/Reviewer_2UEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7785/Reviewer_2UEA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936437148, "cdate": 1761936437148, "tmdate": 1762919827928, "mdate": 1762919827928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the lack of tree-structured inductive bias in deep neural networks for tabular data. To this end, the authors propose ArbNN, a novel architecture that reformulates decision trees into differentiable neural modules, enabling end-to-end gradient optimization while preserving interpretability. Extensive experimental results on multiple public benchmarks and a large-scale industrial credit risk dataset demonstrate that ArbNN consistently outperforms both traditional tree-based models and neural baselines, achieving superior accuracy and interpretability in tabular learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* This paper proposes the ArborCell structure to introduce the inductive bias of decision trees, and I am happy to see that the authors also provide visual comparisons to demonstrate the interpretability of the proposed method.\n* The authors discuss the related literature in considerable detail.\n* The paper is well written and easy to follow."}, "weaknesses": {"value": "1. I am not an expert in tabular data, but I am curious about the convergence behavior of the proposed ArbNN. Could the authors provide training curves and compare them with other networks to illustrate convergence stability?\n2. How does the training cost of the proposed method compare to other baselines? In addition, please evaluate the computational efficiency during inference, e.g., in terms of FLOPs, memory usage, and inference time.\n3. The figures contain text that is too small to read clearly. It is recommended to increase the font size, use vector graphics for better clarity, and include a complete schematic diagram of the model architecture.\n5. The authors do not provide code for reproducibility checks."}, "questions": {"value": "My questions are in Weakness Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "D3u3zuBQxX", "forum": "lU09YerXOb", "replyto": "lU09YerXOb", "signatures": ["ICLR.cc/2026/Conference/Submission7785/Reviewer_THMx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7785/Reviewer_THMx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067914648, "cdate": 1762067914648, "tmdate": 1762919827595, "mdate": 1762919827595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new architecture for tabular data that is based on the idea of converting decision trees to a particular variation of two matrix multiplications with a non-linearity. It proposes to initialize such trees with XGBoost and then finetune the thresholds and values. The paper also proposes a new credit scoring dataset TabCredit. The method is tested on the new dataset and a simple benchmark constructed from pytorch-frame, claiming state-of-the-art performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think that looking into tree-structured models and combining their inner workings with DL models is an interesting pursuit. I had a great time digging through related work on the topic and think that there is something in this line of work that could lead to strong and interpretable models and this direction is currently underexplored.\n\nThe dataset contribution also seems very timely and important as there are not a lot of realistic testbeds for tabular machine learning methods readily available in academia. When done right this is a major contribution, so I encourage authors to go through with it regardless of this review period decision."}, "weaknesses": {"value": "At times the writing is very hard to make sense of. In the related work section, for example, I still can't make sense of how challenging instances in datasets are related to the pre-tuned default hyperparameter configurations (lines 91-93). The overall algorithm for constructing an \"ArborCell\" may also be improved I believe (see the next point for examples).\n\nI believe the paper does not fully cover the relevant related work. It packages an idea of decision tree inference in matrix form into an \"ArborCell\", but this idea seemed not novel, and there are very similar existing approaches indeed:\n- https://arxiv.org/abs/1604.07143 - Neural Random Forests. Which seems to do exactly what authors propose here\n- https://blog.dailydoseofds.com/p/transform-decision-tree-into-matrix a blog post example, which does a better job in explaining the same procedure which is used in the paper\n\nFinally, I do not believe the results are solid as there are some indications of poorly tuned baselines. Like TabM (recent SoTA model) performing on par with or sometimes worse than an MLP, or some large performance gains over XGBoost just from tuning the thresholds and leaf values (may indicate poorly tuned XGBoost in the first place). I also had trouble understanding some of the results like which datasets exactly were used (e.g. what dataset is CH?, why JA - Jannis is seemingly binclass and not multiclass as it is in the pytorch-frame benchmark). Without code being available this is impossible to check further. I suggest the authors compare to an established and well tuned set of baselines, you can take TabArena benchmark which publishes reference model scores in a csv on github:\n\n```python\nimport pandas as pd\npd.read_parquet(\"https://tabarena.s3.us-west-2.amazonaws.com/results/df_results_leaderboard.parquet\")\n```\n\nComparing the method to a correct set of baselines would increase reliability of the results very much."}, "questions": {"value": "See suggestions in weaknesses.\n\nRegarding the newly introduced dataset. Does it have a dedicated train/val/test split which is time-based? Or is it different? Can you provide more details regarding the evaluation and tuning setup on the new dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kRctp48JMz", "forum": "lU09YerXOb", "replyto": "lU09YerXOb", "signatures": ["ICLR.cc/2026/Conference/Submission7785/Reviewer_oszf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7785/Reviewer_oszf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762303031954, "cdate": 1762303031954, "tmdate": 1762919826921, "mdate": 1762919826921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}