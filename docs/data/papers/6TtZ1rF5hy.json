{"id": "6TtZ1rF5hy", "number": 1185, "cdate": 1756861304898, "mdate": 1759898223008, "content": {"title": "Input-Time Scaling", "abstract": "Current Large Language Models (LLMs) have achieved impressive performance in math and other reasoning tasks. They are usually  post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling. It can effectively boost reasoning abilities, and it complements previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. During the exploration of this paradigm, we also discover a new phenomenon,  train-test co-design.  It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. \nThrough our experiments, we are surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant and out-of-distribution information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the previous widely held inductive bias,  \"garbage in, garbage out\". It further raises \nconcerns about the inductive biases we use during data curation. We show that curating datasets with seemingly high-quality data can potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability, only replacing the quality requirement potentially with diversity.\nWith comprehensive experiments on Qwen2.5-32B-Instruct, we show using the simplest form of Input-Time Scaling is able to be comparable or reach SOTA performance among 32B open-source models with AIME24(76.7%) and AIME25(76.7%) pass@1, greedy decoding. A majority vote of three models can further achieve AIME24(76.7%) and AIME25(80%). If we start from DeepSeek-R1-Distill-Qwen-32B, the best result would be the among the SOTA performance of open-source models, 90.0% on AIME24 and 80.0% on AIME25. Our method is extremely simple and clear, without using tedious data\\&training pipelines and with little human labor. Such transparency and efficiency are our core contributions. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.", "tldr": "We introduce the Input-Time Scaling paradigm and the train-test co-design phenomenon. Quality and quantity intuitives may lower the performance. SFT with 1k randomly selected and augmented data can get 90.0% on AIME24 and 80.0% on AIME25 (32B model).", "keywords": ["Input-Time Scaling", "train-test co-design", "reasong", "less is more phenomenon", "training with random selection and augmentation", "exceptional and unexpected math reasoning performance"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c2befe5afbaf5b818286a9b2e7af625759d048c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel paradigm called Input-Time Scaling, which enhances large language model (LLM) reasoning capabilities by refining input queries through meta-cognitive methods. The approach achieves state-of-the-art (SOTA) performance on challenging mathematical reasoning benchmarks while using a simple, transparent, and highly efficient training pipeline. More importantly, the work challenges long-standing assumptions about data “quality” and “quantity” in LLM training and uncovers an intriguing train–test co-design phenomenon, where consistent strategies applied during both training and inference are crucial for performance gains.\n\nOverall, this paper makes a significant contribution. The proposed method is simple, effective, and well supported by empirical evidence. It opens a new direction for enhancing LLM reasoning and provokes deep reflection on data management practices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **High originality:** The concept of Input-Time Scaling is genuinely novel, extending the traditional trichotomy of Data / Training / Inference Scaling by introducing the idea of allocating computational resources at the input level. The use of meta-cognitive methods to introduce various personas—similar, dissimilar, or random—during both training and testing is an innovative way to enhance reasoning diversity and robustness.\n- **Strong empirical performance:** With only 1k supervised fine-tuning samples and no reinforcement learning (RL) stage, the method achieves SOTA results on challenging math benchmarks such as AIME24 and AIME25 with 32B-scale models. The authors conduct extensive ablations across multiple persona strategies during training and testing, showing the effectiveness and stability of the approach.\n- **Challenging conventional wisdom:** The surprising finding that adding irrelevant or seemingly low-quality information (e.g., dissimilar personas) can improve performance directly contradicts the commonly held “garbage in, garbage out” assumption. This challenges the community’s bias toward data purity and suggests that diversity may play a more crucial role than quality alone."}, "weaknesses": {"value": "- **Limited generalization evidence:** The experiments focus solely on four mathematical benchmarks (AIME24, AIME25, MATH, and GPAQ). It remains unclear whether Input-Time Scaling generalizes to other reasoning tasks such as code generation, logical reasoning, or commonsense QA. Since the proposed pipeline is simple and data-agnostic, it would be valuable to test it on different reasoning domains. In addition, all experiments use 32B models; it would be informative to evaluate whether the method is equally effective for smaller models (e.g., 7B).\n- **Lack of mechanistic understanding:** Section 5.2 shows which combinations of training–testing persona strategies perform best, but the paper provides little insight into why they work. In several cases, even random or mismatched personas lead to large improvements. A deeper discussion of the underlying mechanism would significantly strengthen the paper’s depth and credibility.\n- **Details of persona generation:** Although the appendix provides prompts, the main text lacks clarity on implementation details—such as which model is used for persona generation, the degree of randomness, or how diversity is controlled. Since persona quality itself could be a confounding variable, these details should be made explicit. It would also be useful to include qualitative case studies showing how persona-based input refinement changes the reasoning process.\n- **Data quality and validity claims:** The paper claims that “lower-quality” data (OT-1k) outperform the more carefully curated LIMO dataset. However, it is not clearly demonstrated that LIMO is indeed of higher intrinsic quality. A more rigorous comparison or justification of this assumption would make the argument more convincing."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ycOXxsSWLy", "forum": "6TtZ1rF5hy", "replyto": "6TtZ1rF5hy", "signatures": ["ICLR.cc/2026/Conference/Submission1185/Reviewer_bJws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1185/Reviewer_bJws"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221086798, "cdate": 1761221086798, "tmdate": 1762915700036, "mdate": 1762915700036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"Input-Time Scaling\", where the authors concatenate different personas during the training and testing phase for performance gains. The personas are divided into four types depending on the relevance to the original prompt. The strategy is tested on Qwen2.5-32B.\n\nThe paper evaluates on four datasets, AIME2024/2025, Math, and GPQA, and claims to outperform heavily trained models of similar size. \n\nThe paper aims to highlight that their proposed method differs from \"test-time scaling\" in that train-test co-training is necessary to benefit from it fully."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a new scaling axis. Past works usually either scale train-time or test-time. This paper proposes scaling the inputs."}, "weaknesses": {"value": "1) Limited Novelty: Unlike how the paper claims \"input time scaling\" to be novel, I can easily think of different papers that try a similar thing. For instance, (https://arxiv.org/abs/2502.11027) shows that adding diversity into prompts for best-of-n boosts performance. While the two papers differ in that this paper requires training, I don't think it is a significant difference.\n\n2) Limited Evaluation: In section 5.6, the paper mentions Math and GPQA lack \"discriminative effects\"; accordingly, it mostly concentrates on evaluation results from AIME 2024 and 2025. Additionally, the paper uses pass@1 due to resource constraints. However, both datasets contain only 30 samples; accordingly, the main claims of this paper are based on a single inference over 60 questions. This severely lacks statistical credibility. Either more datasets should be added or multiple runs on AIME should be performed to demonstrate that the trained models consistently outperform baselines.\n\n3) Limited Models: The method is only verified by Qwen2.5-32B. While training bigger models might be unaffordable, it's not understandable for the paper to lack results from smaller models. I suggest the authors to try the same method on a larger diversity of models, Qwen2.5-1.5B or Llama-3.1-8B .\n\n4) Limited Ablations: If the author is trying to argue this as a new axis of \"scaling,\" I think it is necessary for them to show how compute-efficient it is. By comparing to past scaling methods. \n\n5) Typo in tables 1~3 should be GPQA not GPAQ. \n\n6) The authors try out diverse combinations of prompting (e.g., N-S, S-D ...) and only AFTER they have seen the results on the test set they choose the optimal prompting method. I would say this is a form of test-contamination. They should have had a validation set to choose the best method and see if it generalize to an unseen test set."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wfbI317YwR", "forum": "6TtZ1rF5hy", "replyto": "6TtZ1rF5hy", "signatures": ["ICLR.cc/2026/Conference/Submission1185/Reviewer_zHqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1185/Reviewer_zHqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797053027, "cdate": 1761797053027, "tmdate": 1762915699648, "mdate": 1762915699648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a \"Input-Time Scaling\" method, which augments an SFT dataset (OpenThoughts) using personas (prompting a language model to rewrite a training example using a persona). They fine-tune a Qwen2.5-32B Instruct model on this dataset and evaluate it on standard reasoning benchmarks (AIME, MATH, GPQA), showing some improvements."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The high-level idea of augmenting SFT data is a good, reasonable one.\nThe results seem strong."}, "weaknesses": {"value": "This paper has a number of problems. First, the proposed ideas (using personas to perform data augmentation) is not novel (see PersonaMath from [Luo et al., 2025]).\nSecond, the 3 types of modification (S, D, R) are not particularly well-motivated nor explained clearly; looking at Appendix 3 doesn't really help.\nThird, I would have liked to see a broader evaluation on a larger number of datasets, especially since the AIME datasets are quite small.\nIt would be good to evaluate the method on other non-Qwen models like Llama since Qwen is regarded to have quite a bit of reasoning already baked inside.\nFinally, the writing in the paper could be greatly improved. For example, the abstract is quite verbose, the core method should be explained (section 2.1), the results tables could use more description (and it's hard to tell what the key takeaway is).\nCalling it input-scaling a new paradigm is a bit exaggerated."}, "questions": {"value": "Why does the method work?  Improving diversity is good, but why personas as opposed to other types of diversity (e.g., different problems, reasoning patterns).\nHow many examples does 1K examples get augmented into?\nWhat are the hyperparameters and how were they tuned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tfEiJjVUmL", "forum": "6TtZ1rF5hy", "replyto": "6TtZ1rF5hy", "signatures": ["ICLR.cc/2026/Conference/Submission1185/Reviewer_7VGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1185/Reviewer_7VGZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129197900, "cdate": 1762129197900, "tmdate": 1762915699469, "mdate": 1762915699469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Input-Time Scaling, a new paradigm that improves LLM reasoning by allocating computation and diversity to query modification rather than model or training scaling. The key finding is the train–test co-design phenomenon: applying the same persona-based query strategies at both training and inference time is crucial for performance. Using small datasets (as few as 1k examples) and simple meta-cognitive persona generation, the method achieves strong reasoning performance on AIME24 and AIME25 benchmarks (up to 90%/80% pass@1 with DeepSeek-R1-Distill-Qwen-32B), surpassing prior open-source 32B models. Surprisingly, lower-quality or more diverse data (random or dissimilar personas) outperform curated datasets, challenging common assumptions about data quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a novel scaling axis that complements data, model, and inference scaling. It focuses instead on the input level via persona augmentation. \n- Findings challenge existing inductive biases about data quality (“garbage in, garbage out”) and show benefits of diversity. I think this is surprising and also interesting."}, "weaknesses": {"value": "- The writing and presentation should be improved (e.g., the abstract is a bit too long, the method section could need more clarity) \n- I would want to understand more why the simple persona change could diversify the training distribution and make the training results better. This augmentation didn't change the question distribution, just how the prompt is seeded. It would be great to look at some qualitative samples and see whether the improved skill correlation with certain persona?"}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rfPbfJz7sr", "forum": "6TtZ1rF5hy", "replyto": "6TtZ1rF5hy", "signatures": ["ICLR.cc/2026/Conference/Submission1185/Reviewer_Ej8f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1185/Reviewer_Ej8f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1185/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141449533, "cdate": 1762141449533, "tmdate": 1762915699298, "mdate": 1762915699298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}