{"id": "ZyLQVFDgfr", "number": 10614, "cdate": 1758177459739, "mdate": 1759897640544, "content": {"title": "Consistency Flow Model Achieves One-step Denoising Error Correction Codes", "abstract": "Error Correction Codes (ECC) are fundamental to reliable digital communication, yet designing neural decoders that are both accurate and computationally efficient remains challenging. Recent denoising diffusion decoders with transformer backbones achieve state-of-the-art performance, but their iterative sampling limits practicality in low-latency settings. We introduce the Error Correction Consistency Flow Model (ECCFM)} an architecture-agnostic training framework for high-fidelity one-step decoding. By casting the reverse denoising process as a Probability Flow Ordinary Differential Equation (PF-ODE) and enforcing smoothness through a differential time regularization, ECCFM learns to map noisy signals along the decoding trajectory directly to the original codeword in a single inference step. Across multiple decoding benchmarks, ECCFM attains lower bit-error rates (BER) than autoregressive and diffusion-based baselines, with notable improvements on longer codes, while delivering inference speeds up from 30x to 100x faster than denoising diffusion decoders.", "tldr": "", "keywords": ["Error Correction Codes", "Consistency Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3239bce1b50e7d681db40a7d92af1e6c4133210.pdf", "supplementary_material": "/attachment/ef7f8ee33b2081af1e129eb2d02f2bf681972e92.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores the use of diffusion models for the task of error correction code (ECC). While prior works have demonstrated promising results using diffusion models for ECC, they suffer from high computational costs. To improve efficiency, the authors adopt the consistency model, which enables one-step denoising and significantly reduces computational overhead. Experimental results show that the proposed framework achieves a lower bit-error rate compared to previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is easy to follow.\n* The work focuses on an interesting and important application.\n* Experimental results indicate that the proposed method achieves promising performance with improved efficiency."}, "weaknesses": {"value": "* Notation\n    * The definition of the function f at Line 223 and in Equation (3) appears problematic. The output of f should be a prediction of the codeword rather than a probability distribution. The correct formulation should align with Algorithm 1, e.g., $d(f_\\theta(x_r, r), x_0)$ instead of $d(f_\\theta(x_r, r), \\delta(x - x_0))$.\n    * The notation $L_{\\text{Consistency}}$ is inconsistent with the previous line at Algorithm 1—it should be written as $L_{\\text{EC-CM}}$.\n* Claims Without Sufficient Support\n    * An ablation study on the use of soft syndrome is missing. Since this is a key contribution of the paper, a comparison between using soft and hard syndromes should be included to substantiate the claimed advantage. Additionally, it would be valuable to report results when incorporating an explicit soft-syndrome loss in the overall objective to validate the design choice.\n    * The paper claims that the proposed objective in Equation (3) provides a stronger learning signal than the conventional objective in Equation (2). However, no experimental evidence is provided to support this claim. It remains unclear whether the proposed objective improves performance, convergence speed, or both.\n* Limited Insight\n    * While I typically avoid judging contributions solely based on novelty, this paper seems to offer limited conceptual insight. The motivation—to enhance the efficiency of diffusion-based denoisers for ECC—is clear, but the proposed solution (adopting the consistency model) is rather straightforward, as the efficiency benefit of consistency models is already well established. Beyond combining existing techniques such as the consistency model and soft-syndrome mechanism to achieve better empirical results, the paper provides limited generalizable insights for broader applications.\n* Inaccurate statement\n    * The description from Lines 211–214 appears inaccurate. The Boundary Condition and Self-Consistency are not “naturally inherent” properties of ECC data. Instead, they are intrinsic properties of the consistency model framework, imposed on the function $f_\\theta$ through its parameterization and training objective. These properties are data-agnostic, whereas the text incorrectly attributes them to ECC itself."}, "questions": {"value": "* During inference, how is the value of $\\sigma$ in Equation (7) determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7JLoPNaIM", "forum": "ZyLQVFDgfr", "replyto": "ZyLQVFDgfr", "signatures": ["ICLR.cc/2026/Conference/Submission10614/Reviewer_z34F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10614/Reviewer_z34F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772071313, "cdate": 1761772071313, "tmdate": 1762921877550, "mdate": 1762921877550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the long-latency problem of DDECC by introducing a consistency flow model. In addition, they employ a soft-syndrome formulation to replace the hard-syndrome approach. The results are meaningful, achieving better performance than the baseline, CrossMPT, while maintaining similar inference time."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of the paper is clear, and the methodology is well adapted from the machine learning field. The modification of the model for the channel coding context is also well derived. The simulation results overall appear to be accurate and convincing."}, "weaknesses": {"value": "Most parts of the paper focus on the training method. However, I am curious about the decoding architecture. The authors mention that CrossMPT is used — does this mean the architecture is exactly the same as CrossMPT? How does e_T (the second parameter of f_theta) affect the decoding process? Do we need multiple models depending on the value of this second parameter to perform decoding? If so, this could be a drawback of the proposed approach. Please clarify this point.\n\nIn the main body of the paper, it would be valuable to include simulation results for ECCFM with the ECCT architecture, not only with the CrossMPT architecture. For a fair comparison with DDECC, both ECCFM and DDECC should share the same underlying architecture (ECCT). Although I noticed that the authors included results with the ECCT-based architecture in the Appendix, it would be better to include them in the main text.\n\nThe graphs in Figure 4 (particularly the FER graph for Polar code) appear somewhat abnormal. Increasing the SNR step size resolution from 1 dB to 0.5 dB and raising the maximum number of testing trials from 10^7 to 10^8 would improve the reliability of the results."}, "questions": {"value": "The sentence above Eq. (6) mentions the “soft-syndrome error condition for each row j,” but the formulation of the soft-syndrome error condition sums over all j, losing the dependency on j. Could the authors clarify this inconsistency?\n\nIn Eq. (8), what is the specific reason for including the soft-syndrome loss in the total loss term? Is it primarily for stabilizing training, or does it also contribute to performance improvement?\n\nMany com fair comparison, the number of training epochs should be aligned.parative works use 1000 epochs, while this paper uses 1500. For a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "izGD4pPFkw", "forum": "ZyLQVFDgfr", "replyto": "ZyLQVFDgfr", "signatures": ["ICLR.cc/2026/Conference/Submission10614/Reviewer_F3Pq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10614/Reviewer_F3Pq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804551526, "cdate": 1761804551526, "tmdate": 1762921877098, "mdate": 1762921877098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a new architecture-agnostic training framework for high-fidelity one-step decoding. It seems that this work integrates the consistency model framework to transformer-based decoders well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is the first to apply the consistency model framework to error-correcting codes (ECC), achieving state-of-the-art performance.\n- The proposed approach replaces the reverse process of the diffusion model with consistency model framework, effectively improving inference efficiency and reducing overall latency compared to DDECC.\n- For the noise condition, this paper employs soft-syndromes, whereas DDECC uses hard syndromes, resulting in smoother trajectories and more stable training."}, "weaknesses": {"value": "- In the modifying the loss function, the authors applied triangle inequality. However, since binary cross entropy (BCE) is not a distance metric, it is unclear whether applying the triangle inequality is theoretically valid in this context.\n- As the experiments were conducted only on the ECCT architecture, it may be inappropriate to claim the model-agnostic properties. Additional results using other architectures, such as CNN would strengthen this claim.\n- When adopting the consistency model (CM) framework, it would be helpful to quantify how much the training cost was reduced compared to other models. A more detailed analysis—such as reporting FLOPs or the number of parameters—would also be beneficial.\n- The texts in Figure 5,7,8,9, and 10 are too small and difficult to read.\n- Equation (6) should add a minus sign at the front, since it represents the binary cross-entropy between the estimated syndrome and the all-zero syndrome.\n- In Equation (7), the “+” following the 1/2 should be a “–”. This is because, under BPSK modulation, a valid codeword satisfies the parity condition with an even number of ones, resulting in a soft syndrome value of 0. If we follow equation (7), the soft syndrome becomes 1 when the codeword is valid, which contradicts the statement in the paper."}, "questions": {"value": "- The paper states that ECCFM employs a Transformer architecture with cross-attention. However, it is unclear which specific model was used. Is this architecture distinct from CrossMPT or a variant of it? If it differs from CrossMPT, please include the performance of the neural decoder used in ECCFM for comparison.\n- In Equation (7), the “+” following the 1/2 should be a “–”. Under BPSK modulation, a valid codeword satisfies the parity condition with an even number of ones, leading to a soft syndrome value of 0. According to the current formulation in Equation (7), a valid codeword yields a soft syndrome of 1, which contradicts the intended parity-check behavior.\n- In Table I, the results are presented only as numerical values. It would be beneficial to include corresponding figures for representative cases to enhance interpretability and comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4f6OXPTevw", "forum": "ZyLQVFDgfr", "replyto": "ZyLQVFDgfr", "signatures": ["ICLR.cc/2026/Conference/Submission10614/Reviewer_LFFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10614/Reviewer_LFFA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960990285, "cdate": 1761960990285, "tmdate": 1762921876326, "mdate": 1762921876326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Error Correction Consistency Flow Model (ECCFM), a framework that enables single-step decoding for error correction codes. By using an \"soft syndrome\" condition, it matches the state-of-the-art accuracy of slow diffusion-based decoders while delivering massive 30-100x speedups, making high-performance neural decoding practical for low-latency applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method innovatively introduces a 'soft syndrome' to solve the critical problem of non-smooth trajectories when applying consistency models to ECC decoding.\n- It uniquely achieves both state-of-the-art decoding accuracy and massive inference speedups of 30-100x over diffusion-based methods.\n- The framework has high practical value for low-latency applications and is model-agnostic, making it widely applicable to different network architectures.\n- Its claims are substantiated by rigorous and comprehensive experiments across various standard codes with fair comparisons to strong baselines."}, "weaknesses": {"value": "- The study's evaluation is confined to the ideal AWGN channel, leaving its effectiveness in more realistic fading channels unexplored.\n- The training process is potentially sensitive and highly dependent on the construction of a smooth trajectory, which may require careful tuning for different codes.\n- Its core idea is a clever adaptation of consistency models from another field, rather than a fundamentally new theoretical invention. I'm more familiar with consistency models than with error-correcting codes, so it's hard to assess the novelty."}, "questions": {"value": "- How robust is the core 'soft syndrome' method under extremely low SNR conditions, where unreliable Log-Likelihood Ratios could disrupt the smoothness of the decoding trajectory and lead to performance degradation?\n- Have the authors experimented with multi-step sampling for ECCFM, and if so, does it provide a meaningful improvement in Bit Error Rate, offering a flexible trade-off between decoding latency and accuracy?\n- What are the primary technical challenges in extending the ECCFM framework to more complex channel models like Rayleigh fading, and would it require a fundamental redefinition of the 'soft syndrome' to maintain a learnable trajectory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FNjvTf0Kca", "forum": "ZyLQVFDgfr", "replyto": "ZyLQVFDgfr", "signatures": ["ICLR.cc/2026/Conference/Submission10614/Reviewer_teDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10614/Reviewer_teDY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762703780307, "cdate": 1762703780307, "tmdate": 1762921875856, "mdate": 1762921875856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}