{"id": "cUYmiV7wvP", "number": 22521, "cdate": 1758332234374, "mdate": 1759896861667, "content": {"title": "Principal component analysis for very heavy-tailed data", "abstract": "Principal component analysis (PCA) is a ubiquitous tool for dimensionality reduction and exploratory data analysis. However, most theoretical and empirical studies implicitly assume that noise is light-tailed. When data are corrupted by heavy-tailed noise, as is increasingly common (e.g. in omics or brain connectivity data), standard PCA techniques can fail dramatically. While recent work in robust statistics has addressed this problem in certain contexts, many existing methods remain sensitive to extreme outliers, performing poorly under truly heavy-tailed distributions. Furthermore, many of the methods which have been designed for heavy-tailed distributions do not scale well to large data sizes. In this work, we propose a novel algorithm for PCA that is designed for extremely heavy-tailed noise and which is computable for even very large data matrices. Our approach is designed to reduce sensitivity to such deviations while recovering informative low-rank structure. In the case of very heavy-tailed data with a large number of observations, we demonstrate significant improvements over classical PCA and existing robust PCA variants.", "tldr": "We introduce a simple, scalable PCA algorithm tailored for extremely heavy-tailed data (including infinite-variance cases), which outperforms existing robust PCA methods on synthetic, transcriptomic, and connectomic benchmarks.", "keywords": ["Robust PCA; Heavy-tailed data; Robust statistics; Random matrix theory; Dimensionality reduction; Scalable algorithms; Unsupervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f775607a5097fec2e746b28cd522f2129fc1eb68.pdf", "supplementary_material": "/attachment/fe287bd9708f4722a79b1e49a56a28fc8697499d.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of performing principal component analysis (PCA) under extremely heavy-tailed noise, where classical PCA and many existing robust PCA approaches fail. The authors observe that for heavy-tailed data, the true principal component may lie in the span of several leading sample eigenvectors rather than aligning with the top one. Based on this observation, they propose a heuristic algorithm that repeatedly subsamples columns of the data matrix, computes leading eigenvectors of each subsample, and aggregates these subspaces to recover the principal direction. The method incorporates a weighted sampling strategy to avoid repeatedly selecting outlier-dominated columns. Empirically, the approach demonstrates improved robustness over classical PCA and several robust PCA variants on synthetic data and two biological datasets. The paper claims scalability and practical advantages, especially in high-dimensional scenarios with heavy-tailed noise."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important and challenging problem: PCA for extremely heavy-tailed data, where standard methods are known to break down.\n- The proposed method is simple, scalable, and easy to implement using basic linear algebra routines, making it potentially useful for large-scale applications.\n- The authors provide extensive experimental evaluation on synthetic data and two real-world datasets, demonstrating that the approach can achieve improved empirical robustness compared to many baselines.\n- The observation that the signal may lie in the span of multiple sample principal components under heavy-tailed noise is interesting and worth further theoretical investigation."}, "weaknesses": {"value": "The primary concern is that the algorithm is entirely heuristic and lacks any theoretical guarantees. While heuristics are valuable in practice, the paper does not provide sufficient justification for the proposed approach beyond empirical observation. In particular:\n\n- The method does not come with formal guarantees regarding recovery accuracy, convergence, or robustness, unlike prior work in robust PCA and heavy-tailed estimation.\n- The motivation relies heavily on a qualitative empirical observation, but no theoretical explanation or analysis is offered to support the key claim.\n- The experiments, although extensive, are not sufficiently diverse to fully establish the reliability of the heuristic. For such a method, more varied real-world benchmarks and stronger empirical gains are necessary to justify its contribution.\n- In the absence of theory, the paper risks lacking generality; it remains unclear under which regimes or distributional assumptions the algorithm can be expected to perform well."}, "questions": {"value": "- Can the authors provide theoretical insights, even partial, into why aggregating PCA subspaces from subsampled data approximates the true principal direction under heavy-tailed noise?\n- How sensitive is the algorithm to the choice of hyperparameters in practice, particularly for datasets with different scales and tail behaviors?\n- Would the method still hold up if evaluated on a broader set of real-world datasets beyond transcriptomics and neural connectivity data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "orYU9v2LyW", "forum": "cUYmiV7wvP", "replyto": "cUYmiV7wvP", "signatures": ["ICLR.cc/2026/Conference/Submission22521/Reviewer_dhEA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22521/Reviewer_dhEA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982629393, "cdate": 1761982629393, "tmdate": 1762942257053, "mdate": 1762942257053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a simple and scalable method for performing principal component analysis when data exhibit heavy-tailed noise, a setting where classical PCA and many robust variants often fail because the sample covariance is dominated by a few extreme observations. The proposed approach repeatedly draws random subsamples of the data, computes the top-$P$ principal components for each subsample, and aggregates the resulting subspaces by averaging their projection matrices. The final estimate of the leading principal direction is obtained as the top eigenvector of this aggregated projection matrix, which stabilizes the estimate by diluting the influence of heavy-tailed outliers across many subsamples. The method uses only standard linear algebra operations and empirically outperforms classical PCA, geometric-median PCA, and convex robust PCA on synthetic and biological datasets, particularly in regimes with infinite-variance noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Introduces a simple yet effective geometric aggregation approach for PCA under heavy-tailed noise, based on repeated subsampling and subspace averaging. The method is conceptually simple yet addresses a challenging regime (including infinite-variance data) rarely handled by existing algorithms.\n\n2. Demonstrates substantial gains over classical PCA, Minsker’s geometric-median PCA, and convex robust PCA across synthetic and real-world datasets, particularly under extreme heavy-tailed noise.\n\n3. Achieves scalability by relying only on standard linear-algebraic primitives (SVD, eigen-decomposition), avoiding convex optimization and achieving near-linear time in data size.\n\n4. The algorithm is transparent and easy to implement, offering intuitive insight into why subspace aggregation stabilizes principal directions.\n\n5. Includes empirical sensitivity analyses over hyperparameters $(P, R, N)$ and tests on diverse biological datasets (transcriptomic and synaptic connectivity), highlighting robustness and generality.\n\n6. Effectively connects the BBP phase transition and random-matrix theory results to the degradation of PCA under heavy-tailed noise, grounding the algorithm’s rationale."}, "weaknesses": {"value": "The paper lacks formal theoretical guarantees, deeper analysis of hyperparameter sensitivity, and a comprehensive comparison to established robust-scatter PCA frameworks (e.g., Tyler's (1987), ROBPCA (Hubert et al. 2012) etc.). These omissions limit the perceived depth of contribution. Here are some of the key weaknesses want to outline:\n\n1. The method is supported primarily by geometric intuition and empirical evidence, but lacks formal theoretical guarantees. In particular, the paper does not provide asymptotic analysis, finite-sample error bounds, perturbation-theoretic results, or convergence and robustness guarantees.\n\n2. The algorithm’s dependence on $(N, P, R)$ is discussed qualitatively, but tuning strategies are heuristic and not systematically analyzed, particularly for real-world data.\n\n3. The relationship to established robust-scatter and shape-based PCA approaches (e.g., Tyler's M-estimator, ROBPCA, spatial-sign PCA) is underdeveloped, limiting clarity on conceptual novelty.\n\n4. Baselines focus on convex robust PCA and geometric-median PCA, omitting newer high-dimensional or probabilistic heavy-tail estimators (e.g., Catoni-type covariance, truncation-based PCA, Lerman \\& Maunu 2018). \n\n5. While the method performs well across the presented experiments, the paper offers little guidance on when the proposed approach may fail or be inappropriate. In particular, there is no analysis of how performance depends on the data regime (e.g., high-dimensional settings with $p\\gg n$, low sample size scenarios, or weak signal-to-noise conditions), nor any diagnostic tools for practitioners to assess whether HT-PCA is likely to provide a reliable estimate on a given dataset. Given that robustness methods can degrade sharply outside their intended regimes, a clearer discussion of failure cases, limitations, and practical checks (e.g., subsample stability tests) would strengthen the utility and transparency of the proposed approach."}, "questions": {"value": "In relation to the weaknesses outlined  above, here are the questions for the authors:\n\n1. Instead of taking the mean of projection matrices, have you considered alternative aggregation measures (e.g., geometric or median subspace averaging)?\n\n2. Can you provide any theoretical intuition or formal result about convergence of the leading eigenvector of $W$?\n\n3. How should practitioners choose the hyperparameters $(N,P,R)$ in different regimes?\n\n4. Could your approach be extended to recover multiple principal components simultaneously?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0DLOEMtv1I", "forum": "cUYmiV7wvP", "replyto": "cUYmiV7wvP", "signatures": ["ICLR.cc/2026/Conference/Submission22521/Reviewer_mB9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22521/Reviewer_mB9s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762287384070, "cdate": 1762287384070, "tmdate": 1762942256672, "mdate": 1762942256672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a different idea for what a principal component for data should be when dealing with am emphasis on heavy tailed data.\n\nThey suggest the following procedure for defining the principal component of data:\n- Given a set of vectors $\\vec x_1, \\ldots, \\vec x_n \\in \\mathbb R^p$\n- Take a random subsample of those vectors, say $\\vec{x}_{s_1}, \\ldots, \\vec{x}_{s_N}$ for $N \\ll n$\n- Let $\\mathbf V \\in \\mathbb R^{p \\times P}$ contain the top $P$ left singular vectors of $\\vec{x}_{s_1}  \\ldots  \\vec{x}_{s_N}$\n- The principal component is then the top eigenvector of $\\mathbb E[VV^\\top]$\n\nThis principal component is then estimate by simple monte carlo: Generate many such $V$ matrices, form an empirical estimation of $\\tilde W = \\mathbb E[VV^\\top]$, and return the top eigenvector of $W$.\n\n$V$ is not sampled uniformly at random. It uses importance sampling which is inversely proportional to the norms of the vectors.\n\n\nThe paper gives evidence suggesting that for heavy-tailed data, this recovers a more natural notion of a principal component when compared to classical PCA (i.e. returning the top eigenvector of the sample covariance matrix).\n\nEvidence is empirical throughout the paper; theory is not provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think this is a really interesting text. Overall, I'm inclined to accept, pending some adjustments to the text.\n\nThe intuition underlying the proposed estimator makes sense, and is a simple linear algebraic notion. I've never seen it used for this purpose, but I've certainly see people analyze the expected projection $\\mathbb E[VV^\\top]$ in theoretical problems (see eg Thm 3.1 of [this paper](https://arxiv.org/pdf/2208.09585); no need to cite this or anything just being thorough about the connection).\n\nThe paper provides convincing evidence that this notion of a principal component is meaningful, and that it is empirically useful.\n\nThe paper is well written and was even kinda fun to read!\n\nI'm no expert on the statistical side of PCA, so I can't really comment effectively on the originality of this work relative to prior work. Taking their long prior work section at face value, this seems like a valuable contribution!"}, "weaknesses": {"value": "The paper suffers four core weaknesses, not all equal:\n1. The experiments lack confidence intervals (plus other smaller issues on the figures)\n2. The experiments fail to consistently and effectively compare the proposed PCA method to alternative PCA methods on\n3. There is not a very crisp formalization of what makes a PCA method \"good\" for heavy tailed data\n4. There is not a clear notion of how to produce more than one principal component\n\n\nLet's start with experiments.\n\nThe figures in this paper are slightly disastrous.\n- Despite the proposed method being a randomized algorithm, and the variance of randomized methods for PCA often having non-trivial confidence intervals, none of the experiments seem to have any confidence intervals. In my view, EVERY plot should contain confidence intervals for work like this. I personally prefer seeing the median error with 10/90 quantiles or 25/75 quantile; though mean +- standard deviation is okay. Line 242 says the code was run 20 times, so this should be an easy fix.\n- Figure 2 has no real caption (page 5)\n- Figures throughout the paper have unexplained parameters. The notion of \"ERROR\" and \"alpha\" isn't defined until after figure 2.\n- Printed on paper, it's very hard to read the axes of many figures. The text should be larger on the axes (and in some legends)\n\nNext, the paper lacks some baseline comparisons and confuses me at some points.\n- \"Sample Cov w/ Del.\" is absent from Fig 3 for some reason\n- Section 4.1 (page 8) studies the \"self-consistency\" metric on real data, but only reports the error achieved by the proposed PCA method, and does not show the error achieved by the other methods considered on synthetic data. No confidence interval on the error is given.\n- Section 4.2 (page 9) studies the same error metric on a different real data source, but only reports the error achieved by two PCA methods. The metric here used is confusing as the authors refer to both \"self-consistency\" and \"mean cosine similarity\" but only define the former, and perhaps only report the latter? Either way, the metric used here is confusing, and not enough estimators are compared.\n- Section 4.2 has a high standard deviation of their error metric, nearly as large as the average value of the metric. Some further discussion about runtime to lower that standard deviation would be good (I know it's discussed elsewhere in the paper; but it needs acknowledgement here as well)\n\n\nNext, let's get more conceptual. There's not a clear notion of what a good PCA method is.\nThe paper proposes two tests that a good PCA method should achieve on heavy-tailed data\n1. Have good \"self-consistency\": if you split a dataset in half and run your PCA method on each half, it should return nearly identical vectors\n2. Work on data from a specific generative model that has noise distributed as a heavy-tailed Student distribution\n\nThese are both... good things we want from PCA, but neither one really is a fundamental notion of what good PCA should be defined as. I'd like to see a more fundamental model for what the authors consider good PCA to be. The authors have this interesting ansatz that with heavy-tailed noise, the fundamental principal components should be distributed amongst the first few eigenvectors of the sample covariance matrix. I'd love to see this pushed a step further, into a potential guess for what a good formalization of PCA would be.\n\nI'll acknowledge that my question here is somewhat underspecified; I'm not sure the authors have a good super formal notion of what PCA should be defined as, and I don't want to give them an undue burden to do such a thing. But if they have a more formal idea, I'd love to see that written out more. (to clarify, not an algorithm, but a more statistical notion of what a principal component should be)\n\n\n\nThis bleeds into my final topic -- the fact that this paper only considers producing a single principal component.\nIt's a very obvious question to ask: How should I generate a second principal component?\nAnd how about the $k^{th}$?\nExplicit iterative deflation may be needed as in LazySVD; or maybe just returning the top $k$ eigenvectors of the monte carlo projection suffices?\nI think this should be acknowledged within this paper, at least to some minimal extent."}, "questions": {"value": "## List of typos & recommended edits\n\n_ Feel free to ignore anything in here you disagree with, without any need for further discussion _\n\n1. [60] Specify what community these real-world datasets come from. Neuroscience?\n2. [68] Last sentence here is phrased too strong. Maybe \"Unless many other PCA methods for heavy-tailed data...\"\n3. [79] \"reasonably small matrices\"? at ICLR, a 100 by 500 matrix is small but already shows what you want it to show\n4. [144] Usually, to me, Pi is a projection not a subspace\n5. [throughout] P and p both being symbols in this paper is kinda annoying... maybe swap p for d, or swap P for k?\n6. [154] Usually, to me, V is a tall matrix so that V'V is the identity and VV' is a projection. Transpose the definition?\n7. [Fig 1] Specify the data used to generate table 1\n8. [Throughout] Actually formalize the method used to importance sample. IID sampling with/without replacement? Wrt squared col L2 norms, or non-squared norms? Any smoothness/regularization used?\n9. [189] I think this is just Courant-Fisher. Would be good to name-drop that here.\n10. [Sec 2.3] This is very long. Shorten this a bunch. Will help you with the page limit.\n11. [314] You're using kappa for both kurtosis and signal strength. Split these two different things up."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gnOa9myiBa", "forum": "cUYmiV7wvP", "replyto": "cUYmiV7wvP", "signatures": ["ICLR.cc/2026/Conference/Submission22521/Reviewer_ibbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22521/Reviewer_ibbZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762302070137, "cdate": 1762302070137, "tmdate": 1762942255580, "mdate": 1762942255580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to compute the leading singular vector(s) of a matrix, motivated by scenarios where the matrix data is heavy-tailed (which is known to degrade the performance of other methods)\n\nThe method involves iterative taking random columns (ie datapoints) and finding the principal components of this, then adding up these found principal components and finally fining the principal component of the agglomerate. \n\nThere is no rigorous guarantee of when or if the method works, but it is evaluated on synthetic and real datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Method has been clearly described."}, "weaknesses": {"value": "The main weakness of this paper is that there is no rigorous guarantee that this method will work (or even, a characterization of specific scenarios of when it will work). Most existing methods for Robust PCA have such guarantees, and it is important to establish at least a basic rigorous guarantee of when such an algorithm will work (and, will work better than simple PCA)\n\nAlso, the paper is missing many works on robust PCA as comparison baselines, e.g.\n\nhttps://arxiv.org/abs/1010.4237\n\nhttps://arxiv.org/abs/2305.02544 (and references therein)\n\nEtc."}, "questions": {"value": "Not currently"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7WpxpQiH4A", "forum": "cUYmiV7wvP", "replyto": "cUYmiV7wvP", "signatures": ["ICLR.cc/2026/Conference/Submission22521/Reviewer_Jcbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22521/Reviewer_Jcbn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762375366112, "cdate": 1762375366112, "tmdate": 1762942255010, "mdate": 1762942255010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}