{"id": "ErvDQ6tacJ", "number": 16803, "cdate": 1758268855281, "mdate": 1759897218687, "content": {"title": "PureCover: Bridging the Gap in Re-ranking for Retrieval-Augmented Generation via Balancing Coverage and Noise", "abstract": "Re-ranking, originating from  Information Retrieval (IR), has become a critical technique for filtering retrieved documents in Retrieval-Augmented Generation (RAG). Current RAG systems often directly apply re-rankers from traditional IR, which were originally designed to provide relevant and diverse documents to human users. However, this adoption overlooks a fundamental gap: unlike humans can use selective attention to filter noise and focus on key evidence, LLMs lack this ability. This gap causes traditional re-rankers to fail in covering essential evidence and minimizing noise for LLMs, significantly hurting RAG performance, especially in complex question-answering tasks. To address this, we argue that RAG re-rankers should serve a distinct objective: not only ensuring the coverage of key information but also minimizing noise in the selected document set. To achieve this objective, we propose PureCover, a document selection framework tailored for RAG. Instead of relying on traditional Top-K re-ranking, we reformulate the document selection process as a multi-objective optimization problem and solve it by exploiting LLM attention patterns during goal-oriented reasoning. To improve efficiency, we distill the selection capability into an LLM selector via a set-wise strategy. Experiments on four multi-hop QA benchmarks demonstrate that PureCover consistently outperforms state-of-the-art baselines, achieving a better balance between coverage and noise for RAG.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "Re-ranking", "Multi-objective Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df725fe06a4a6d2730ef6ec3fc8a3b712d8b7c90.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "PureCover reframes RAG re-ranking as selecting a set that jointly maximizes evidence coverage while minimizing noise, rather than ranking by per-doc relevance or diversity; the system estimates per-query information needs from CoT reasoning attention, calibrates away position bias, solves a submodular 0–1 objective with a greedy procedure, then distills the selection into a lightweight LLM selector that filters by first-token logits at inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LLMs lack human selective attention, so RAG needs a list-level objective balancing coverage and noise rather than pure relevance. The paper formalizes this and ties it to CoT-based estimation.\n\n2. PureCover edges strong cross-encoder and LLM rankers on all four datasets."}, "weaknesses": {"value": "1. end-to-end tables fix K=5 for all datasets, but several baselines are K-sensitive; the paper only shows K-sweep on HotpotQA. Best-K comparisons and Pareto fronts (EM/F1 vs #docs) across datasets would be more convincing. \n\n\n2. Information Coverage/Purity partly rely on “contains answer string” heuristics; multi-hop bridging evidence may be under-counted. Please validate these proxies with human-labeled supports on a sample.\n\n3. experiments use one retriever (e5-base-v2) and one generator setup; results may depend on the retrieval pool and generator size. Cross-retriever/generator tests would strengthen claims."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BFgAWilWub", "forum": "ErvDQ6tacJ", "replyto": "ErvDQ6tacJ", "signatures": ["ICLR.cc/2026/Conference/Submission16803/Reviewer_waZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16803/Reviewer_waZd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472684197, "cdate": 1761472684197, "tmdate": 1762926835931, "mdate": 1762926835931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PureCover, a novel document re-ranking framework for Retrieval-Augmented Generation (RAG) that reframes document selection as a multi-objective optimization problem to balance information coverage and noise minimization. Unlike traditional re-rankers designed for human users, PureCover leverages LLMs’ internal attention patterns during goal-oriented reasoning to identify key evidence and employs a greedy algorithm with set-wise distillation for efficient inference. Experiments on multiple multi-hop QA benchmarks show that PureCover consistently outperforms state-of-the-art baselines by better balancing coverage and noise, leading to significant improvements in RAG performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Formulation: It identifies a specific, often-overlooked flaw in applying traditional re-rankers to RAG systems.\n\n2. Strong Empirical Results: It demonstrates consistent and significant performance improvements over many state-of-the-art baselines."}, "weaknesses": {"value": "Limited Justification for Core Assumptions: The entire method hinges on using LLM attention during Chain-of-Thought (CoT) reasoning as a proxy for \"information requirements\" and document utility. While the results are positive, the paper provides no validation that these attention patterns are a reliable and generalizable ground truth for what constitutes essential evidence versus noise. This is a significant assumption. A stronger justification, such as a human annotation study correlating high-attention tokens with key facts, or an ablation showing that the content of the CoT steps (rather than just the attention signal) is crucial, would make the foundational premise more convincing."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xXMB83P9uL", "forum": "ErvDQ6tacJ", "replyto": "ErvDQ6tacJ", "signatures": ["ICLR.cc/2026/Conference/Submission16803/Reviewer_rmWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16803/Reviewer_rmWK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745449673, "cdate": 1761745449673, "tmdate": 1762926835564, "mdate": 1762926835564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues IR re-rankers assume human selective attention while LLMs don’t, so RAG needs re-ranking that balances evidence coverage with noise suppression. \nIt defines coverage and noise for complex QA, then reframes document selection as a multi-objective problem solved with calibrated reasoning-attention signals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The goal is RAG-native and practical: cover what’s needed and keep junk out of the context.\n\nThe position-bias calibration acknowledges “lost-in-the-middle” issues and corrects attention before using it.\n\nThe set-wise distillation and first-token scoring make deployment feel realistic instead of research-only."}, "weaknesses": {"value": "The method leans hard on CoT prompting and attention introspection, which can drift across models and prompts. \n\nThe noise term uses a max over requirements, which might miss partial usefulness or cross-requirement interactions. \n\nSubmodularity and monotonicity hinge on attention-based estimates that could be noisy in practice. \n\nThe calibration relies on dummy CoTs/docs and may not transfer cleanly to other layout or packing strategies. \n\nThe distilled selector introduces thresholds and budgets that still need tuning in new domains."}, "questions": {"value": "How stable are P(e|q) and P(d|e) across different CoT prompts, temperatures, and model sizes.\n\nWhat’s a practical recipe to set lambda and the selection threshold without dataset-specific sweeps.\n\nCan the framework capture “two-docs-together” interactions rather than “at least one doc” coverage only."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKy91DI3Bh", "forum": "ErvDQ6tacJ", "replyto": "ErvDQ6tacJ", "signatures": ["ICLR.cc/2026/Conference/Submission16803/Reviewer_xVuM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16803/Reviewer_xVuM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762048405092, "cdate": 1762048405092, "tmdate": 1762926835004, "mdate": 1762926835004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}