{"id": "ccjukmExrB", "number": 1112, "cdate": 1756842289491, "mdate": 1763592892278, "content": {"title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning", "abstract": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple labels across different levels of granularity. To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback–Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code will be available on GitHub.", "tldr": "", "keywords": ["Multi-Granular Language Learning", "Medical Image Analysis", "Multimodal Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1b1dc07736c81471788af3503db007b695c4246.pdf", "supplementary_material": "/attachment/30abc1acd1f9b74dfcd67d2776a316e66e480b53.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MGLL (Multi-Granular Language Learning) which is a training method that extends CLIP to jointly handle multi-label supervision and text granularities in medical imaging. It initroduces three complementary losses: soft-CLIP, point-wise BCE, and smooth KL. The model trained with MGLL outperforms prior CLIP-style baselines across benchmarks and also boost downstream MLLM performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The study introduces a triple loss function to for training medical image-text to capture multi-granular level meaning, which standard CLIP does not handle.\n\n2) The study curates two multi-granular datasets, MGLL-Funds and MGLL-Xray.\n\n3) The paper explain the motivation for each term of the MGLL with empirical analyses.\n\n4) The paper provides strong, consistent grains across benchmarks in both linear-probe and full fine-tune settings.\n\n5) MGLL can be easily adopt in other medical domains that have hierarchical labels or reports."}, "weaknesses": {"value": "1) The study does not provide report on the overlap between MGLL-Fundus (aggregates from 49 sources) and target datasets. Possible risky for data leakage.\n\n2) The paper claims MGLL is computationally efficient due to no additional encoders, but provides no evidence such as training time, memory usage, compared to baselines like CLIP.\n\n3) The paper does not provide clinical relevant metrics such as F1, or sensitive, specificity."}, "questions": {"value": "1) Can the authors provide experimental results showing MGLL's performance on datasets with artificially introduced noise, such as 10%–30% missing granularity labels? \n\n2)  Can the authors provide experimental results showing MGLL's performance on datasets with mixing granularity levels? For example, some samples have 2 level of granularity level, while others have 1 level.\n\n3) Have clinical experts validated MGLL's alignments or its class activation maps for clinical relevance?\n\n4) Can the authors provide specific metrics (e.g., training time, GPU memory usage) comparing MGLL to CLIP or other baselines on the MGLL-Fundus dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MA1UYirn7p", "forum": "ccjukmExrB", "replyto": "ccjukmExrB", "signatures": ["ICLR.cc/2026/Conference/Submission1112/Reviewer_pAv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1112/Reviewer_pAv1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557040387, "cdate": 1761557040387, "tmdate": 1762915682384, "mdate": 1762915682384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel medical vision-language pre-training framework, Multi-Granular Language Learning (MGLL), designed to achieve both multi-label and cross-granularity image-text alignment. Building upon CLIP, MGLL introduces three key components: soft-label contrastive loss, point-wise binary supervision loss, and smooth KL divergence loss, to enhance cross-granularity consistency. The authors construct two large-scale medical image-text datasets, MGLL-Fundus (fundus images) and MGLL-Xray (X-ray images), for model pre-training. Experiments demonstrate that MGLL significantly outperforms existing state-of-the-art methods on 11 downstream tasks and effectively boosts the performance of multimodal large language models (MLLMs)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall contribution is clear and highly practical, offering significant utility for addressing the multi-level semantic complexity inherent in medical images.\n2. Experiments are comprehensive and results are significant:\n2.1. Covers multiple datasets (Fundus and X-ray) and 11 downstream tasks;\n2.2. Validates various application scenarios, including linear probing, full fine-tuning, and integration with MLLMs;\n2.3. Significantly outperforms existing CLIP variants on the majority of tasks, demonstrating strong experimental consistency."}, "weaknesses": {"value": "1. The details of dataset construction are insufficient. \n2. The innovation leans more towards a compositional approach, rather than proposing entirely new learning principles or optimization mechanisms.\n3. Lacks ablation studies on the temperature coefficient (τ)."}, "questions": {"value": "1. How is the multi-granular text supervision generated?\n2. Is there a potential risk of trivial solutions in the smooth KL divergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oJNFbvTDtw", "forum": "ccjukmExrB", "replyto": "ccjukmExrB", "signatures": ["ICLR.cc/2026/Conference/Submission1112/Reviewer_Bhm2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1112/Reviewer_Bhm2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727852704, "cdate": 1761727852704, "tmdate": 1762915682271, "mdate": 1762915682271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed MGLL, a novel multi-grandularity contrastive learning framework for medical visual language pre-training. Different from vanilla CLIP, MGLL conducts contrastive learning through both multi-label and multi-granular optimization. It proposed three new contrastive losses designed for data with multi-granular annotation, showing an improved performance on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has contributed two large-scale image-text pair datasets for fundus and X-ray images, providing multi-granular annotation, which will be very helpful to the field.\n\n2. According to the evaluation and derivation, the proposed losses (soft CLIP loss, point-wise BCE loss, and KL loss) help improve the model's performance on multiple downstream evaluations, showing a uniform improvement against baselines.\n\n3. The ablation experiment is especially detailed, providing strong support to the model design.\n\n4. Code is provided in the supplement."}, "weaknesses": {"value": "My major concern is the clear formatting issue. The paper has clearly adjusted the vertical space between the section and sub-section titles, gaining more space for their content. Table 1 and Figure 4 overlap with each other. The reviewer believes that this is a violation of the conference policy, which suggests \"Do not change any aspects of the formatting parameters in the style files. In particular, do not modify the width or length of the rectangle the text should fit into...\" Considering that, I am afraid I cannot recommend acceptance regardless of the content of the paper, unless there is clear evidence that suggests otherwise (modifying vertical space is fine, or proving the vertical space is not modified).\n\nBesides the format issue, the reviewer still has concerns about the proposed method. Listed below:\n\n1. In equation (2), the soft CLIP loss is defined based on both $l_\\{ik\\}$ and $l_\\{ki\\}$, where the second term (if I understand correctly) refers to text-to-image loss. But this is clearly wrong, since $i$ is for the $i$-th image/text in the batch, but $k$ is the $k$-th label across different granularities, and they are clearly un-interchangeable. According to Figure 2, the soft CLIP loss should be computed over 3 dimensions: image batch, text batch, and granularity. So, the reviewer guessed there might be a missing index in this equation.\n\n2. The point-wise BCE loss optimizes as a multi-label classification loss. However, it is weird that $M$ now serves as the number of **categories**, rather than the number of different **granularities**, and they can't be the same value. Since for the $i$-th image, it is always paired with all its $M_i$ annotations, as they just refer to different granularity. This might just be a typo, but it would still be better to clarify these two values.\n\n3. As for the KL loss, there is no explanation on how $z$ is obtained, which is the \"predicted logits\". The reviewer assumes this refers to the logits for each category/class of the image. But it still uses $m$ as an index, which makes it very confusing.\n\n4. According the section 3.2, the proposed method relies on category-wise prediction (both point-wise BCE loss and KL loss), which means it can only be applied to data with a known number of classes. And this is a fundamental weakness compared with CLIP, since it loses the capability and flexibility for unseen classes. One needs to carefully design the granularity and classification classes before applying them to new data. This also contradicts the claim of \"plug-and-play\" in the abstract; the proposed method only fits with specific datasets rather than general data.\n\n5. Additionally, the results in section 4.3 and table 2 are also confusing. To replace the vision encoder in the MLLM, one needs to fine-tune the MLLM to adapt to a new vision feature space. However, there is no discussion or clarification about this. If the MLLMs with a new vision encoder are fine-tuned while the original ones are not, then it will be an unfair comparison."}, "questions": {"value": "1. The reviewer is also curious about the training cost. How much extra training time does MGLL need compared with the normal CLIP?\n\n2. It seems that the X-ray data and Fundus data use different granularity; will this influence the final performance? How sensitive is the model to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "34KsImsTzU", "forum": "ccjukmExrB", "replyto": "ccjukmExrB", "signatures": ["ICLR.cc/2026/Conference/Submission1112/Reviewer_HXxg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1112/Reviewer_HXxg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864193765, "cdate": 1761864193765, "tmdate": 1762915682151, "mdate": 1762915682151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a vision-language pretraining method for fundus and chest X-ray images. The method considers the multi-granularity of medical disease labels and improves the contrastive learning framework with multi-label and cross-granularity alignment. Experiments are conducted on public fundus and chest X-ray images."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and clinically relevant topic, multi-granular vision-language representation learning for medical imaging, which has strong potential to improve medical AI systems by aligning visual features with diagnostic text at different levels of detail."}, "weaknesses": {"value": "- Although the paper mentions multi-label alignment as part of its motivation, it remains unclear how labels are defined within the framework, and what the exact relationship is between multi-label and multi-granularity. Does a “label” refer to a textual description at a specific granularity level?\n\n- Ambiguity in the Definition of Text Features in Point-wise Loss. It is not explicitly stated whether T_j refers to a single label at a given granularity level, or a concatenation of multiple labels from different granularity levels, similar to the definition used elsewhere in the paper (e.g., T_i). Moreover, it is unclear what M\\ specifically represents in Equation (4). These details are important for accurately understanding the methodology presented in the paper.\n\n- While the authors impose KL-based consistency across granularities, they simultaneously learn granularity-specific alignments using soft-CLIP. This duality is not reconciled theoretically or empirically. There is no investigation into whether enforcing uniformity across granularities harms the model’s ability to capture granularity-specific patterns.\n\n- The idea of improving the multi-granularity of vision-language alignment is not new and has been explored in previous works. It is unclear how the proposed method differs from or advances beyond previous approaches, such as [R1][R2].\n\n- The paper lacks comparison with closely related multi-granularity alignment methods such as MGCA [R1]\n\n- Both retinal fundus and chest X-ray image domains have seen many vision-language models proposed in recent years. However, the paper lacks a comprehensive comparison with these existing methods, such as [R1-R4].\n\n- The experiments focus solely on disease classification tasks, without evaluating the method on more challenging dense prediction tasks such as critical region segmentation or detection, which are also commonly used in the evaluation of pretraining methods.\n\nReferences:\n[R1] Wang F, Zhou Y, Wang S, Vardhanabhuti V, Yu L. Multi-granularity cross-modal alignment for generalized medical visual representation learning. NeurIPS, 2022.\n[R2] Wang M, Lin T, Lin A, Yu K, Peng Y, Wang L, Chen C, Zou K, Liang H, Chen M, Yao X. Enhancing diagnostic accuracy in rare and common fundus diseases with a knowledge-rich vision-language model. Nature Communications. 2025.\n[R3] Phan VM, Xie Y, Qi Y, Liu L, Liu L, Zhang B, Liao Z, Wu Q, To MS, Verjans JW. Decomposing disease descriptions for enhanced pathology detection: A multi-aspect vision-language pre-training framework. CVPR 2024.\n[R4] Ma D, Pang J, Gotway MB, Liang J. A fully open AI foundation model applied to chest radiography. Nature. 2025 Jun 11:1-1."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jO1gsLui7U", "forum": "ccjukmExrB", "replyto": "ccjukmExrB", "signatures": ["ICLR.cc/2026/Conference/Submission1112/Reviewer_o8AL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1112/Reviewer_o8AL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874975057, "cdate": 1761874975057, "tmdate": 1762915682001, "mdate": 1762915682001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}