{"id": "vko4DuhKbh", "number": 23495, "cdate": 1758344589833, "mdate": 1759896812001, "content": {"title": "Flow Caching for Autoregressive Video Generation", "abstract": "Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames—an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps.\nIn this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by an importance-based KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality.\nOur method achieves remarkable speedups of $\\textbf{2.38}\\times$ on MAGI-1 and $\\textbf{6.7}\\times$ on SkyReels-V2, with negligible quality degradation (VBench: $0.87\\uparrow$ and $0.79\\downarrow$ respectively). These results demonstrate that FlowCache, successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation—establishing a new benchmark for efficient video synthesis at scale. The code is available at  https://anonymous.4open.science/r/FlowCache-23495iclrAnonymous", "tldr": "", "keywords": ["Autoregressive video generation", "chunkwise caching", "KV cache compression", "ultra-long video synthesis", "video acceleration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/974925a0ff6a497c3bc1947e7a8eae84bc3202f6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a **cache compression method** designed to significantly reduce the KV cache size in autoregressive (AR) video generation. The proposed approach effectively improves generation speed while largely maintaining output quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is **well-organized**, **logically structured**, and clearly highlights its main contributions.  \n- The proposed method introduces an **adaptive criterion** for KV cache compression, enabling dynamic adjustment of the compression ratio.  \n- This adaptive mechanism effectively improves cache compression efficiency and leads to faster video generation without significant quality degradation."}, "weaknesses": {"value": "1. **Equation (8)** appears potentially ambiguous. It seems that the Softmax operation should be applied along the \\( L_k \\) dimension, but the current formulation places Softmax directly outside \\( q_i \\) and \\( k_j \\), which may be mathematically inconsistent.  \n2. It is unclear **when compression begins**—does it occur before reaching the cache budget, or only after exceeding it? If the method only keeps top-B tokens, then no compression happens before reaching the budget. The paper should clarify that the reported speedup comparisons are made **after the cache reaches the specified budget**.  \n3. Although the proposed approach achieves high compression rates, each compression step requires a **pre-computation of attention scores** to determine which tokens to keep, followed by another computation of the true attention scores for the retained tokens. This effectively adds an extra forward pass. The paper should report the **computational overhead ratio** introduced by this additional calculation."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGZBL1XeJF", "forum": "vko4DuhKbh", "replyto": "vko4DuhKbh", "signatures": ["ICLR.cc/2026/Conference/Submission23495/Reviewer_3Ldg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23495/Reviewer_3Ldg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890768868, "cdate": 1761890768868, "tmdate": 1762942682041, "mdate": 1762942682041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlowCache, a novel caching and compression framework for autoregressive video generation that addresses the unique challenge of heterogeneous chunk denoising states. The method is theoretically grounded, empirically strong, and significantly advances the state of the art in efficient long-video synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.  The paper's primary strength is its clear identification and empirical validation of *why* existing caching methods fail for autoregressive models: the \"heterogeneous denoising states.\" This is a sharp and important insight.\n2. The \"chunkwise caching\" policy is a direct, logical, and highly effective solution to the problem identified. The ablation study decisively proves that this chunk-specific strategy is the main reason for the method's success in preserving quality.\n3. Additionally, the KV cache compression method is also well-motivated. It correctly identifies the high-redundancy problem in video data and proposes a solution that intelligently balances both importance and redundancy, which is a clear improvement over importance-only methods from language modeling. From my perspective, this is an excellent work overall."}, "weaknesses": {"value": "1. There is an internal contradiction between the paper's theory and its empirical results. **Theorem 1** (and its proof in Appendix B) is used to establish that the relative L1 distance is a *monotonically decreasing* function of time (as $t$ goes from 0 to T). However, the paper's *own empirical plot* (Figure 2) and *main text* (e.g., \"relative L1 distance monotonically increases as denoising progresses\" in line 302) show the exact opposite. This contradiction undermines the stated theoretical foundation.\n2. The ablation in Table 2 suggests that the complex KV cache compression adds very little performance on top of the main chunkwise policy. On MAGI-1, the chunkwise policy *alone* achieves a 77.66% VBench score, while the *full* method with compression gets 77.93%. This small benefit may not justify the added complexity of the importance-redundancy scoring mechanism; more discussion is required."}, "questions": {"value": "How sensitive is the KV cache compression performance to the choice of λ and the granularity settings? Is there a recommended configuration for different video types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GUGMpcDI5v", "forum": "vko4DuhKbh", "replyto": "vko4DuhKbh", "signatures": ["ICLR.cc/2026/Conference/Submission23495/Reviewer_fkWp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23495/Reviewer_fkWp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978655104, "cdate": 1761978655104, "tmdate": 1762942681844, "mdate": 1762942681844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies training-free acceleration for autoregressive video diffusion. The authors empirically demonstrate that different chunks should have independent feature caching policies rather than a single global caching policy. A redundancy-aware KV-cache compression scheme is also adopted for long-video generation.\nExperiments on MAGI-1 and SkyReels-V2 demonstrate a noticeable speedup with a minor drop in VBench score."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposed training-free, plug-in acceleration for causal video diffusion. Treating each video chunk as its own, with an independent reuse policy, is well motivated by the observed heterogeneity across chunks at the same timesteps.\n- Experiment isolates the benefit of chunkwise feature reuse over full reuse and shows that kv-compression has a small impact."}, "weaknesses": {"value": "- Memory claims lack evidence. Memory usage is stated to be fixed, but no benchmark on memory usage is exhibited in the paper.\n- Lack of quality comparison. Only a few images are displayed in the paper. No video clip was provided, making it hard to evaluate the visual quality.\n- Lack of evaluation on long-video benchmark, e.g., VBench-long, since the method is claimed to be helpful for long-video generation.\n- MAGI-1 already applied window attention (8-second preceding video content). Weakening the motivation for KV-cache compression.\n- MAGI-1 has a shortcut step-distill version. The proposed method does not compare with it, nor apply the feature reuse to it.\n- Feature reuse necessarily increases memory consumption because the cache has to be retained. This seems to conflict with the stated motivation of KVcache compression, which is to reduce memory usage."}, "questions": {"value": "- line 99-101: Please clarify the experimental/testing setup for this result (e.g., GPU type, memory, batch size, video length). Without this, it’s hard to judge how generalizable the observation is.\n- The paper claims to “provide insights into memory–quality trade-offs,” but the experiments do not actually show memory vs. quality curves/tables (e.g., VBench vs. cache size/compression ratio). This weakens the contribution.\n- How is the chunkwise caching policy obtained in practice? Is it derived offline from a calibration set, or learned/heuristic? When generating videos of different lengths or motion patterns, does the policy need to be recomputed or adapted?\n- Do you have numerical benchmarks (peak memory, KV size per frame/chunk, vs. baseline) to substantiate the claim of reducing the memory with KV-cache compression?\n- misc:\n  - The citation of [1] is not about diffusion and seems unrelated to the context in which it is cited.\n  - VBench is a scaled score, not a percentage\n\n[1] Mengwei Xu, et la, Deepcache: Principled cache for mobile deep vision. 2018"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BiapuQ0QA6", "forum": "vko4DuhKbh", "replyto": "vko4DuhKbh", "signatures": ["ICLR.cc/2026/Conference/Submission23495/Reviewer_LS46"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23495/Reviewer_LS46"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979775040, "cdate": 1761979775040, "tmdate": 1762942681652, "mdate": 1762942681652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a caching framework for autoregressive video generation. The key idea is to change existing uniform caching strategies used in common video diffusion models. Firstly, it applies dynamic caching strategies to different noise levels. Higher noise level is more likely to reuse cached feature, while lower noise level is more likely to recompute. It also introduce a compression mechanism to kv cache due to large memory consumption based on importance and redundancy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dynamic assignment of calculate or reuse significant improves the efficiency of the model i.e., more than 2x comparing to TeaCache-fast.\n- The proposed KV cache compression method balances both past token visual importance and redundancy. It is specifically designed for video token characteristics."}, "weaknesses": {"value": "- The idea of reuse or recompute based on L1 similarity  of L1rel is clearly stated and proved, but the detail of how to decide to reuse or recompute the cache is not clear. Is there any threshold or decision making module for this part?\n- It is not clear why the proposed method both out perform the baseline method, TeaCache both in terms of speed and video quality. The reuse operation accelerates the speed, but why it also achieve better frame quality. It would better to have more detailed comparison and discussion between the baseline method."}, "questions": {"value": "- For the dynamic chunk caching and reuse mechanism, is it adaptively applied across different videos based on the degree of motion dynamics? As mentioned in Weakness 1, the decision-making process remains unclear — it would be helpful to provide a concrete inference example illustrating how this adaptation works.\n- Are there any known failure cases for the two proposed designs? For instance, how do they perform on videos with large or complex motion? In such cases, is the speed up improvement less significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V6e8BdbKPZ", "forum": "vko4DuhKbh", "replyto": "vko4DuhKbh", "signatures": ["ICLR.cc/2026/Conference/Submission23495/Reviewer_xZAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23495/Reviewer_xZAs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980886075, "cdate": 1761980886075, "tmdate": 1762942681451, "mdate": 1762942681451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}