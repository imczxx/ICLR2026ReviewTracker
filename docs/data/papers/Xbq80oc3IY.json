{"id": "Xbq80oc3IY", "number": 19475, "cdate": 1758296574475, "mdate": 1759897036935, "content": {"title": "Mind the Gap: Diagnosing Spatial Reasoning Failures in Vision-Language Models", "abstract": "Vision-Language Models (VLMs) have captivated the research community by effectively merging visual and textual information, implying a holistic comprehension of the environment. These models find applications in tasks such as Image Captioning and Visual Question Answering, fostering the assumption that they perceive reality in a way similar to human cognition. However, this apparent understanding may be misleading. We argue that a critical component of comprehension—spatial reasoning—has been insufficiently addressed, as current benchmarks primarily test models' ability to identify object positions rather than evaluate genuine spatial logic. In this study, we aim to address this limitation. Drawing from the fundamental elements of human cognition, we developed a diagnostic framework designed to isolate the essential components of spatial reasoning: relational understanding, orientation, mental rotation, and visualization. We evaluated 17 state-of-the-art VLMs within both controlled synthetic settings and the complex variability of images captured in the real world. Results indicate a substantial gap in performance: the apparent competence of these models decreases significantly under spatial reasoning tasks that require any dynamic transformation and manipulation of spatial information. On average, their performance parallels random guessing, which highlights a major systematic weakness in spatial reasoning in current VLMs. In addition to providing evidence for this limitation, this study also provides the research community with a foundational framework for developing models that can accurately understand and reason about spatial properties in their environment.", "tldr": "VLMs appear competent but struggle with true spatial reasoning, performing near random on our diagnostic tasks and revealing a critical weakness in current models.", "keywords": ["Vision-Language Models", "VLMs", "spatial reasoning", "relational understanding", "orientation", "mental rotation", "visualization", "synthetic tasks", "real-world images", "model evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bae12add01e2cf7282d493705320e851d6d1f94.pdf", "supplementary_material": "/attachment/fb5065a804a3a415ab1ea1ff536fbc2a6dabd876.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to look at the critical component of spatial reasoning based on inspirations from human cognition. It introduces a diagnostic framework designed to isolate components in spatial reasoning from relational, mental rotation to visualization. However, 17 state-of-the-art vision-language models (VLMs) show random chance results, suggesting that there's a major systematic weakness in spatial reasoning in current VLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Paper is easy to follow."}, "weaknesses": {"value": "1. The paper claimed that current benchmarks primarily test models’ ability to identify object positions rather than evaluate genuine spatial logic. However this is not true as many existing benchmarks has looked at spatial logic like object relations and mental rotation beyond positioning [1,2,3].\n2. The paper claimed that it draws from the fundamental elements of human cognition. However some of the core tasks are challenging / unsolvable even for average humans. For example, the mental rotation tasks are very hard to solve for humans so it is not surprising that vision language models (VLMs) cannot solve it. Human evaluation with **non-biased participants** is necessary in the paper.\n\nReferences\n\n[1] Ray, Arijit, et al. \"Sat: Spatial aptitude training for multimodal language models.\" arXiv e-prints (2024): arXiv-2412.\n\n[2] Ma, Wufei, et al. \"3dsrbench: A comprehensive 3d spatial reasoning benchmark.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n\n[3] Zhang, Weichen, et al. \"Open3dvqa: A benchmark for comprehensive spatial reasoning with multimodal large language model in open space.\" arXiv preprint arXiv:2503.11094 (2025)."}, "questions": {"value": "1. Figure 2 should add detailed descriptions of each tasks like the prompting questions. It's hard to understand what the tasks are for Paper Folding and Orientation from looking at the figure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5CfKRnksT", "forum": "Xbq80oc3IY", "replyto": "Xbq80oc3IY", "signatures": ["ICLR.cc/2026/Conference/Submission19475/Reviewer_PJzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19475/Reviewer_PJzY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767084015, "cdate": 1761767084015, "tmdate": 1762931386355, "mdate": 1762931386355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SRBench, a comprehensive diagnostic framework for evaluating spatial reasoning in Vision-Language Models (VLMs).\nBuilding upon paradigms from cognitive psychology, the benchmark decomposes spatial reasoning into four pillars:\n(1) Mental Rotation, (2) Spatial Visualization (Paper Folding), (3) Relational Understanding, and (4) Egocentric Navigation & Orientation.\n\nThe authors evaluate 17 state-of-the-art VLMs (including GPT-4o, QwenVL2.5, InternVL-3, LLaVA, Gemma, and MiniCPM) under synthetic and real-world conditions.\nFindings show that while these models perform well on static spatial tasks (relations, orientation), their accuracy drops to near random on tasks requiring dynamic transformations such as rotation or folding. Scaling up model parameters improves transparency and reasoning articulation but does not overcome fundamental deficits in spatial simulation.\n\nThe paper concludes that bridging this “static-dynamic gap” requires new architectural inductive biases that explicitly support simulation-style spatial reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic Framework：\nThe benchmark covers multiple cognitive dimensions, offering a holistic view of spatial reasoning rather than isolated tasks.\n\n2. Insightful Findings: \nThe identified “static competence → dynamic collapse” phenomenon highlights a fundamental limitation in current VLMs and provides valuable diagnostic insight.\n\n3. Clear, Engaging Presentation: \nThe writing is fluent and well-organized, with convincing examples and thoughtful analysis of scaling effects and failure modes."}, "weaknesses": {"value": "**Lack of conceptual coherence across sub-tasks:**  \n\nThe benchmark merges a diverse set of spatial reasoning tasks—mental rotation, paper folding, navigation, relational understanding—yet offers no clear theoretical or empirical justification for treating them as components of a single construct. Each of these capabilities has already been explored in specialized prior works, often with more controlled task design and deeper analysis. If the paper’s goal is diagnostic comprehensiveness, it should at least demonstrate how these tasks correlate or capture shared latent factors of spatial cognition, rather than serving as an unstructured “collection of puzzles.”\n\nWithout such analysis (e.g., inter-task correlation, factor analysis, or shared failure patterns), the paper reads as a broad but shallow aggregation—a “jack of all trades, master of none.” The integration would be more convincing if the authors could empirically show that performance across these tasks reflects a coherent spatial reasoning ability, rather than coincidental co-location of unrelated evaluations.\n\n\n[1] *DOES SPATIAL COGNITION EMERGE IN FRONTIER MODELS?*  \n[2] *11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis*  \n[3] *SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization*  \n[4] *OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models*  \n[5] *Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics*"}, "questions": {"value": "On task integration:\nYou argue for a unified diagnostic framework of spatial reasoning, yet most cross-task analyses are missing. The only explicit discussion of inter-task relations appears in Section 3.3.2 (“The Fragility of Abstract Spatial Transformation”), which links Paper Folding and MRT tasks.\nCould the authors clarify whether this relationship generalizes to the other sub-tasks (Relations, Orientation, Navigation)?\nIn other words, is there any empirical evidence that these tasks measure a shared latent spatial reasoning ability, or are they simply co-located evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Goh5yfQAYF", "forum": "Xbq80oc3IY", "replyto": "Xbq80oc3IY", "signatures": ["ICLR.cc/2026/Conference/Submission19475/Reviewer_YfVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19475/Reviewer_YfVR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992225676, "cdate": 1761992225676, "tmdate": 1762931385837, "mdate": 1762931385837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the spatial reasoning capabilities of vlms, presenting a novel diagnostic benchmark to evaluate foundational aspects of spatial cognition. The authors focus on four core aspects: mental rotation, spatial visualization, relational understanding, and egocentric navigation to construct a systematic framework, which is based on cognitive psychology. Empirical results reveal that while VLMs can reliably handle static spatial relations and orientations, their performance drops to near-random levels on tasks requiring internal simulation or dynamic spatial manipulation (such as mental rotation and sequential paper folding), highlighting a significant gap in current VLM capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a diverse spatial reasoning benchmark, rigorously formulated from cognitive psychology. The benchmark systematically covers dynamic and static spatial reasoning, extending well beyond existing datasets that often focus on localized or single-facet assessments.\n2. The paper identifies a key aspect of VLMs: they handle static spatial relations well but struggle with transformation-rich spatial reasoning, which is very insightful."}, "weaknesses": {"value": "1. The paper convincingly identifies the performance gap, but does not provide a formal mathematical framework for why VLM architectures fail in dynamic spatial reasoning. For example, Section 3 attributes the collapse to a “lack of inductive biases” and hypothesizes about causes, but provides no detailed model analysis (e.g., explicit examination of attention map, model arch, visual embedding/encoding, how vision tokens represent transformations in latent space, or quantitative ablation of positional encoding mechanisms). While empirical, the absence of targeted analysis on the exact architectural or representational bottlenecks means the diagnosis stops short of design insight.\n2. There are many spatial reasoning benchmarks for VLMs such as VSI-Bench, ViewSpatialBench, MindCube, and STARE and several settings (e.g., mental simulation and spatial understanding) are very similar to those in this paper. It would be helpful if the authors listed the differences from these benchmarks and discussed in detail how this work differs.\n3. The paper evaluates only GPT-4o and o1 （proprietary model), which is not sufficient. To my knowledge, GPT-5 is released before the ICLR submission deadline and shows substantial gains in spatial understanding. The authors should include GPT-5 in the evaluation; otherwise, the results on older models may not reflect the capabilities of current frontier VLMs. For completeness and fairness, it would also be important to evaluate the latest Gemini and Claude VLMs.\n4. One of the most valuable roles of a benchmark paper is to expose concrete failure modes and point to actionable paths for improvement. This paper concludes that “human-like reasoning will require not just greater scale, but new architectural paradigms and inductive biases” for dynamic object transformations. That insight is important, but it remains high-level. Could the authors substantiate it with targeted experiments that test specific, more concrete ideas for improving performance?"}, "questions": {"value": "1. Did the authors prepare different prompt templates and test VLM performance across prompt variants?\n2. What is the human performance on each SRBench subtask?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TIHNKGKb5e", "forum": "Xbq80oc3IY", "replyto": "Xbq80oc3IY", "signatures": ["ICLR.cc/2026/Conference/Submission19475/Reviewer_tUNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19475/Reviewer_tUNJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042629746, "cdate": 1762042629746, "tmdate": 1762931384839, "mdate": 1762931384839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SRBench, a diagnostic benchmark designed to assess dynamic spatial reasoning in VLMs. Building on paradigms from cognitive psychology (mental rotation, paper folding, spatial relations, egocentric orientation, navigation), the authors evaluate 17 SOTA models—including GPT-4o, InternVL-3, and Qwen-VL—across synthetic and naturalistic settings. Results show that while VLMs perform reasonably on static spatial tasks, they fail dramatically on dynamic transformations, often near random-chance accuracy. The paper argues that scaling and instruction tuning alone do not yield genuine spatial reasoning, calling for architectures with explicit inductive biases toward spatial simulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Timely and important problem. Spatial reasoning is a known blind spot in multimodal AI; the study provides systematic evidence of this deficiency.\n\n2. Readable, well-presented results and qualitative analyses. Clear tables and insightful failure analyses enhance interpretability."}, "weaknesses": {"value": "1. Although the suite spans four task families, each split is tiny (MRT-Easy=200, MRT-Hard=200, Paper-Folding=200, Relations=400, Orientation=400; Navigation size not specified in text). At ~1–2K total items, this is far below recent large-scale diagnostics and limits conclusions about generalization and fine-tuning. Report exact per-split counts (incl. Navigation), and either scale the corpus or narrow the paper’s claims.\n\n2. The model set omits several state-of-the-art commercial VLMs (e.g., GPT o3/5, Google Gemini 2.5 pro), which weakens the headline claim about “current VLMs.”  They have more robust and powerful ability.\n\n3. The paper provides excellent quantitative results but would benefit from a more in-depth qualitative error analysis. For instance, in the most difficult  task category, what are the specific failure modes? Do models fail at mental rotation differently than at geometric pattern recognition? Including a few examples of incorrect model outputs and analyzing the flawed reasoning (or lack thereof) would provide the community with a much richer understanding of the core challenges.\n\n4. The scope is explicitly limited to four abilities (mental rotation, spatial visualization, relations, egocentric navigation). That misses key facets of spatial reasoning (e.g., 3D metric localization, occlusion/containment dynamics, multi-object compositional reasoning, frame-of-reference switching).\n\n5. The paper fails to explicitly position itself relative to prior spatial reasoning benchmarks, such as SpatialVLM  SpatialRGPT and Omnispatial. These works already evaluate spatial understanding across text-only, multimodal, and cognitive-psychology-inspired paradigms.\nHowever, the authors provide no systematic comparison table, no shared task taxonomy, and no empirical or conceptual justification for how their benchmark probes fundamentally new dimensions (e.g., integration, transformation, or dynamic reasoning) beyond what prior datasets test."}, "questions": {"value": "I am curious about the true value of the dataset, as I strongly suspect that it may merely overfit to its own format.\n\nIf a base model (e.g., Qwen-VL) could be fine-tuned on this dataset and subsequently demonstrate performance gains on other benchmarks (such as VSI-Bench, OmniSpatial and SPACE), I would be much more inclined to recognize the dataset’s contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PwwA3NNtFY", "forum": "Xbq80oc3IY", "replyto": "Xbq80oc3IY", "signatures": ["ICLR.cc/2026/Conference/Submission19475/Reviewer_oEEL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19475/Reviewer_oEEL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762504370081, "cdate": 1762504370081, "tmdate": 1762931384019, "mdate": 1762931384019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}