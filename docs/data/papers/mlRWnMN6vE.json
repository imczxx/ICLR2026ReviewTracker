{"id": "mlRWnMN6vE", "number": 3160, "cdate": 1757344689618, "mdate": 1759898105085, "content": {"title": "Differentially Private Geodesic Regression", "abstract": "In statistical applications it has become increasingly common to encounter data structures that live on non-linear spaces such as manifolds. Classical linear regression, one of the most fundamental methodologies of statistical learning, captures the relationship between an independent variable and a response variable which both are assumed to live in Euclidean space. Thus, geodesic regression emerged as an extension where the response variable lives on a Riemannian manifold. The parameters of geodesic regression, as with linear regression, capture the relationship of sensitive data and hence one should consider the privacy protection practices of said parameters. We consider releasing Differentially Private (DP) parameters of geodesic regression via the K-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical bounds for the sensitivity of the parameters showing they are tied to their respective Jacobi fields and hence the curvature of the space. This corroborates recent findings of differential privacy for the Fr\\'echet mean. We demonstrate the efficacy of our methodology on the sphere, $\\mathbb{S}^2\\subset\\mathbb{R}^3$, the space of symmetric positive definite matrices, and Kendall's planar shape space. Our methodology is general to any Riemannian manifold and thus it is suitable for data in domains such as medical imaging and computer vision.", "tldr": "We develop methodology to release private estimates for geodesic regression, a generalization of linear regression parameterized by a footpoint and shooting vector.", "keywords": ["differential privacy", "Riemannian manifolds", "geodesic regression"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40ea1b1fd510ab9463834ae30ae4a5a56f6d2f94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies differentially private (DP) release of geodesic‐regression parameters on Riemannian manifolds. Geodesic regression is parameterized by a footpoint $p\\in\\mathcal M$ and shooting vector $v\\in T_p\\mathcal M$; the loss is the mean squared geodesic distance between observations ($y_i\\in\\mathcal M$) and predictions $\\mathrm{Exp}(p,x_i v)$. The authors instantiate the K-Norm Gradient (KNG) exponential mechanism on manifolds to privately sample $\\tilde p,\\tilde v$, and derive global sensitivity bounds for the gradients of the least-squares energy with respect to $p$ and $v$. Central to the analysis is that sensitivities are controlled by Jacobi fields along the prediction geodesic, hence by the sectional curvature bounds of $\\mathcal M$. The main bound (Theorem 3.3) yields $\\Delta_p\\le \\tfrac{2\\tau}{n}$ for nonnegative curvature and $\\Delta_p\\le \\tfrac{2\\tau}{n}\\cosh(2\\sqrt{-\\kappa_\\ell}(\\tau_m+\\tau))$ for $\\kappa_\\ell<0$; an analogous bound holds for $\\Delta_v$. Experiments on the sphere $S^2$, SPD(2), and Kendall’s planar shape space illustrate error–privacy trade-offs and effects of budget splits $(\\varepsilon_p,\\varepsilon_v)$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clean synthesis of KNG with geodesic regression, yielding curvature-explicit sensitivity bounds; the link through Jacobi fields is elegant and broadly useful. \n* Derivations for $\\nabla_p E,\\nabla_v E$ via adjoints and Jacobi fields, and bounds using Rauch and geodesic-length control are careful and instructive. \n* Preliminaries and appendices (sampling on $S^2$; proofs; geometry background) make the paper approachable to the manifold-ML audience. \n* A re-usable blueprint for DP mechanisms in manifold regression (beyond geodesics), especially where curvature varies across spaces (sphere, SPD, Kendall)."}, "weaknesses": {"value": "1. Using $\\tau=\\max_i|\\varepsilon_i|$ from data to set the mechanism scale breaks DP; results are therefore not privacy-valid as presented. Recommend public/DP bounds (e.g., clipping residuals at a public radius; or releasing a DP (\\tilde\\tau) with its own budget) and re-running experiments. \n2. Missing comparisons to manifold Laplace/Gaussian mechanisms, or to output perturbation of $(\\hat p,\\hat v)$ under $\\mu$-GDP/RDP accounting; also no advanced composition or tighter accounting beyond $\\varepsilon_p+\\varepsilon_v$. \n3. The Riemannian M-H sampler lacks guarantees that approximate sampling error preserves $\\varepsilon$-DP; proposal/acceptance choices could leak beyond intended $\\varepsilon$. Consider exact samplers where possible, proximal/noisy gradients with DP optimization on manifolds, or rigorous approximation error -> privacy bounds. \n4. Sphere experiments are synthetic; SPD(2) appears only in appendix; shape-space study is modest. Include higher-dimensional SPD tasks (e.g., covariance descriptors), larger $n$, and report runtime/acceptance rates vs. budgets. \n5. Assumption practicality. Guidance to verify or enforce curvature/radius assumptions in practice is limited; discuss public preprocessing to ensure bounded domains (e.g., projection/clipping under a known atlas)."}, "questions": {"value": "1.  Can you replace the data-dependent $\\tau=\\max_i|\\varepsilon_i|$ by either (i) a public bound (domain restriction + clipping) or (ii) a DP estimate $\\tilde\\tau$ (Laplace/Gaussian on a 1-sensitive statistic) and re-report results? How sensitive are utilities to over-clipping? \n2. Why not use $\\mu$-GDP/RDP accounting (or advanced composition) to budget $\\varepsilon_p,\\varepsilon_v$? Could tighter accounting materially change utility? \n3. Can you provide bounds on mixing / total variation error and show that sampling error does not compromise privacy? Any chance to adopt DP Riemannian optimization (perturbed gradients on $\\mathcal M)$ to avoid MCMC? \n4. Please compare against (a) manifold Laplace mechanism applied to $(p,v)$, (b) output perturbation of non-private $(\\hat p,\\hat v)$, and (c) DP Frechet regression/metric-space regression where applicable, under the same $\\varepsilon$. \n5. Could you empirically validate the curvature dependence predicted by the bounds (e.g., compare equal-radius synthetic tasks on $S^2$ vs. negatively curved models)? \n6. Public feasibility checks. How would practitioners choose public $r,\\tau_m$ and ensure $\\tau$-closeness without peeking at private data? Any model-selection strategy that is itself DP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NWGfvzqHCH", "forum": "mlRWnMN6vE", "replyto": "mlRWnMN6vE", "signatures": ["ICLR.cc/2026/Conference/Submission3160/Reviewer_Eaui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3160/Reviewer_Eaui"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431840111, "cdate": 1761431840111, "tmdate": 1762916578980, "mdate": 1762916578980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends differential privacy techniques to geodesic regression on Riemannian manifolds by adapting the K-Norm Gradient (KNG) mechanism. The authors derive theoretical sensitivity bounds for the regression parameters (footpoint and shooting vector) and demonstrate that these sensitivities depend on Jacobi fields and manifold curvature. The methodology is validated through experiments on three manifolds: the 2-sphere, the space of symmetric positive definite matrices, and Kendall's planar shape space. Results show that privacy-utility tradeoffs behave predictably, with sensitivity decreasing as sample size increases and balanced budget allocation generally outperforming extreme splits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper tackles differential privacy for geodesic regression, a fundamental statistical method for manifold-valued data that appears in sensitive domains like medical imaging (brain tensors, anatomical shapes) and spatial statistics. Given the increasing prevalence of non-Euclidean data in privacy-sensitive applications, providing formal privacy guarantees for this regression framework fills a significant gap in the privacy literature.\n* The paper rigorously derives sensitivity bounds (Theorem 3.3) for geodesic regression parameters by connecting them to Jacobi field equations and curvature bounds. This extends previous work on the Fréchet mean and provides a principled framework that generalizes to arbitrary Riemannian manifolds with bounded curvature, making it broadly applicable."}, "weaknesses": {"value": "* The authors acknowledge setting τ = max_i ||ε_i|| which requires examining the data and \"indeed violates privacy.\" This is a fundamental flaw that undermines the practical applicability of the method. While they claim \"the concept of our methodology still holds,\" differential privacy is compromised if data-dependent quantities are used to calibrate noise, and no alternative solution is proposed beyond experimenting with inflated values.\n\n* While the paper examines different budget allocations and sample sizes, it lacks important ablation studies. There is no analysis of MCMC mixing quality, no comparison with baseline methods (e.g., adding noise in ambient space). The number of MCMC samples (m=10 footpoints, 10 vectors each) seems small for reliable posterior characterization.\n\n* The literature review misses several relevant recent contributions that would benefit readers and position the paper more suitably (See [1, 2, 3])\n\n\nRefs\n\n\n1) Shape and structure preserving differential privacy.\n\n2) Differentially Private Fréchet Mean on the Manifold of Symmetric Positive Definite (SPD) Matrices with log-Euclidean Metric.\n\n3) Improved Differentially Private Riemannian Optimization: Fast Sampling and Variance Reduction."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lw3wg3In9M", "forum": "mlRWnMN6vE", "replyto": "mlRWnMN6vE", "signatures": ["ICLR.cc/2026/Conference/Submission3160/Reviewer_HY9b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3160/Reviewer_HY9b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826790780, "cdate": 1761826790780, "tmdate": 1762916578702, "mdate": 1762916578702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Again, we thank all the reviewers for all their diligent work. We wanted to address the concern with $\\tau$ and $\\tau_m$ as a whole as all the reviewers shared similar views.\n\nIn DP, a priori bounding data is a delicate procedure. For instance, consider a dataset $D$ of ages of humans, it might be reasonable to assume the ages are bounded between 0 to 122. This upper bound, however, can theoretically be exceeded/violated. Similarly, if one had more information, such as \"ages of humans with Alzheimer's,\" one might be inclined to use a lower bound of approximately 65, as this is reasonable. Still, this can (and has) been violated (early onset Alzheimer's can arise in one's 30s). Each of these settings requires some public knowledge. While we can use public information to guide how we bound the data, these bounds are still subject to possible violations. \n\nWe agree with the reviewers, our choice of $\\tau$ and $\\tau_m$ can be improved. As reviewer \\textbf{Eaui} rightly pointed out (or rather asked), we can replace our $\\tau$ and $\\tau_m$ with a sanitised version, say $\\tilde{\\tau}, \\tilde{\\tau}_m$. However, we would like to point out that since $\\sigma\\propto \\tau$, replacing $\\tau$ with $\\tilde{\\tau}>\\tau$ and $\\tau_m$ with $\\tilde{\\tau}_m>\\tau_m$  we'd simply get a slightly larger noise scale. Further, since $\\sigma\\propto \\tau/\\epsilon$ \"increasing $\\tau$\" behaves similarly to \"decreasing $\\epsilon$\", which is one reason we included experiments (Fig 1, Fig 2, Fig 5, Fig 6, and Fig 7) where we examined the behavior of our methodology under varying $\\epsilon$. Lastly, even in the case where we have a private $\\tau$, just as with the earlier Alzheimer's example, it may still be possible that $\\tilde{\\tau}$ is not large enough. This is a persistent problem in DP.\n\nSeeing as we generally agree with the reviewers, we will add a longer discussion on the choice of $\\tau$ and $\\tau_m$ and guidance on how it can be achieved in a private manner. As noted earlier, though, our goal of this paper is to extend geodesic regression into the DP landscape. We believe we have achieved this."}}, "id": "IayLlR97eV", "forum": "mlRWnMN6vE", "replyto": "mlRWnMN6vE", "signatures": ["ICLR.cc/2026/Conference/Submission3160/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3160/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3160/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763552223119, "cdate": 1763552223119, "tmdate": 1763552223119, "mdate": 1763552223119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes differentially private geodesic regression on Riemannian manifolds via the K-Norm Gradient mechanism, deriving sensitivity bounds that depend on curvature through Jacobi fields. The ICLR version broadens experiments (sphere, SPD, Kendall shape space) and clarifies notation, in comparisons to the previous NeurIPS submission."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper discusses privacy-preserving learning on manifolds. It is the first work to formulate and analyze geodesic regression under differential privacy, bridging a gap between classical Euclidean differential privacy mechanisms and the geometry of Riemannian manifolds. In our earlier NeurIPS review we identified three main issues: (i) an ill-posed difference in Lemma 3.3 involving adjoints and elements in different tangent spaces; (ii) an incorrect application/direction of the Rauch comparison; and (iii) the privacy implications of the $\\tau-$ closeness assumption. While some has improved: \n1. **Rauch comparison direction.** The sensitivity analysis now uses a lower curvature bound and obtains cos/cosh-style bounds, which fixes the directionality error we flagged.\n2. **Notation and preliminaries.** The manuscript defines the adjoint and states Jacobi initial conditions explicitly, reducing ambiguity.\n3. **Sampling/composition.** The sequential treatment of $(p,v)$ via the fiber-bundle view of $TM$ is articulated, clarifying how privacy composition is handled without assuming a global product structure.\n\nHowever, I still have other concerns as mentioned below."}, "weaknesses": {"value": "## Concerns that remain:\n1. The revised proof avoids directly subtracting vectors in different tangent spaces by appealing to operator norms and Jacobi comparisons, but the bound still implicitly depends on choices of parallel transport inside the supremum over adjacent datasets. The paper should add a formal lemma showing transport invariance (or independence from path choices), or otherwise bound uniformly over admissible transports.\n\n2. The assumption that the least-squares geodesic is $\\tau$-close (and related $\\tau_m$) appears to be determined relative to the confidential data. As written, this is not obviously compatible with DP unless $\\tau$ is fixed publicly in advance with projection/truncation, or selected via a DP pre-screening step. This core privacy concern remains unresolved.\n\n3. The NeurIPS version highlighted the Euclidean reduction and compared against standard DP libraries. If the reduction is claimed as a benefit, those Euclidean baselines (or equivalent) should be retained or replicated for completeness."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rtmgxUCQRS", "forum": "mlRWnMN6vE", "replyto": "mlRWnMN6vE", "signatures": ["ICLR.cc/2026/Conference/Submission3160/Reviewer_xHep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3160/Reviewer_xHep"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939061029, "cdate": 1761939061029, "tmdate": 1762916578471, "mdate": 1762916578471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies differentially private geodesic regression, which can be viewed as linear regression generalized to cases where the output lives on a curved space rather than in ordinary Euclidean space. It proposes releasing the fitted p and v with the K-Norm Gradient mechanism, adapted to work directly on a Riemannian manifold. The key technical step is to bound how much the loss gradient can change when one data point changes, using Jacobi fields so that the required noise depends on curvature and on a radius bound for the data. The method samples from the resulting target distribution with a Riemannian MCMC routine and reports results on the sphere and on Euclidean space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper is technically sound. I feel like the major contribution is to apply KNG with tight analysis on the geometry and sensitivity."}, "weaknesses": {"value": "The authors implement a Riemannian random-walk MH sampler to draw from the KNG density. Because DP is guaranteed for the target distribution, numerical sampling error can in principle weaken guarantees."}, "questions": {"value": "The experiments empirically choose 10 different epsilon_p and epsilon_v pairs. Is there any analytical guidance or asymptotic criterion suggesting an optimal split under curvature? My concern is that parameter tunning would also cost privacy budget in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hykPjoULTM", "forum": "mlRWnMN6vE", "replyto": "mlRWnMN6vE", "signatures": ["ICLR.cc/2026/Conference/Submission3160/Reviewer_KD4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3160/Reviewer_KD4m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951962109, "cdate": 1761951962109, "tmdate": 1762916578113, "mdate": 1762916578113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}