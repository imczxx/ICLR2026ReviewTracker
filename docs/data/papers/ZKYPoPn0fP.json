{"id": "ZKYPoPn0fP", "number": 9934, "cdate": 1758150268897, "mdate": 1759897684541, "content": {"title": "Differentiable JPEG-based Input Perturbation for Knowledge Distillation Amplification via Conditional Mutual Information Maximization", "abstract": "Maximizing conditional mutual information (CMI) has recently been shown to enhance the effectiveness of teacher networks in knowledge distillation (KD). Prior work achieves this by fine-tuning a pretrained teacher to maximize a proxy of its CMI. However, fine-tuning large-scale teachers is often impractical, and proxy-based optimization introduces inaccuracies.\n To overcome these limitations, we propose Differentiable JPEG-based Input Perturbation (DJIP), a plug-and-play framework that improves teacher–student knowledge transfer without modifying the teacher. DJIP employs a trainable differentiable JPEG layer inserted before the teacher to perturb teacher inputs in a way that directly increases CMI. We further introduce a novel alternating optimization algorithm to efficiently learn the coding parameters of the JPEG layer to maximize the perturbed CMI. Extensive experiments on CIFAR-100 and ImageNet, across diverse distillers and architectures, demonstrate that DJIP consistently improves student accuracy-achieving up to 4.11% gains-while remaining computationally lightweight and fully compatible with standard KD pipelines.", "tldr": "", "keywords": ["Knowledge Distillation", "JPEG", "Conditional Mutual Information"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5f7dcdc8dcf12501db365bd3f30482549e029d3.pdf", "supplementary_material": "/attachment/6b5c59dc71846c9b84c6d38fe639e0f34b06a7c6.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Differentiable JPEG-based Input Perturbation (DJIP), a plug-and-play framework that boosts knowledge distillation by maximizing a frozen teacher’s conditional mutual information (CMI) without altering its weights. DJIP inserts a trainable, differentiable JPEG module before the teacher to learn input-space perturbations, and pairs it with an alternating optimization routine that dynamically updates class centroids to maximize perturbed CMI. On CIFAR-100 and ImageNet—spanning CNNs and ViTs and multiple distillers (KD, DKD, AT)—DJIP delivers consistent student accuracy improvements (up to 4.11%), while composing cleanly with existing methods.\n\nContributions\n1. A lightweight, broadly compatible input-perturbation mechanism via differentiable JPEG compression.\n2. An alternating CMI-maximization algorithm that overcomes fixed-centroid limitations (e.g., MCMI).\n3. Extensive empirical validation demonstrating orthogonality to prevailing techniques and scalability across datasets, architectures, and distillers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality. DJIP reframes JPEG-based input perturbation as a continuous optimization problem, in contrast to prior discrete toggling or adversarial noise schemes. The proposed alternating routine offers a principled advance for CMI maximization/estimation, jointly adapting perturbations and class centroids to better capture teacher–input dependence.\n2. Quality. The evaluation is broad and careful: two large-scale datasets, 10+ architectures (CNNs and ViTs), and 15+ KD methods, with component-wise ablations (Table 5) that isolate the contribution of the differentiable JPEG layer and the centroid updates. A thorough hyperparameter sweep (Appendix A.5) tests sensitivity and demonstrates stable gains across settings, strengthening the empirical claims.\n3. Clarity. Exposition is accessible: the CMI objective is introduced with clear intuition and notation (Sec. 3.2), and the overall pipeline is modularized in Fig. 1, making it straightforward to slot DJIP before a frozen teacher. Algorithm boxes and training pseudocode further improve reproducibility.\n4. Significance. Because DJIP is plug-and-play and lightweight (≈5–12% parameter overhead), it is practical for resource-constrained deployments. Notably, it narrows or even surpasses larger baselines (e.g., a 3B DJIP student outperforming a 14B non-DJIP counterpart), and its benefits persist across diverse backbones and distillers—evidence of orthogonality rather than method-specific tuning. Collectively, these attributes position DJIP as a scalable, implementation-friendly upgrade path for KD pipelines."}, "weaknesses": {"value": "1. Computational Efficiency. Table 4 reports parameter overhead, but end-to-end latency, peak memory, and throughput under realistic batch sizes/hardware are not measured. These metrics are essential for edge/smartphone deployment. Suggest reporting wall-clock inference (ms/img), GPU/CPU memory footprints, and A/B latency deltas with and without DJIP across batch sizes.\n2. Baseline Diversity. Comparisons emphasize CKD and MCMI, but omit recent non-JPEG perturbation approaches (e.g., GAN/ diffusion-generated counterfactuals, learned augmentors). Including such baselines would clarify whether DJIP’s gains stem from JPEG structure or from perturbative training per se.\n3. Theoretical Limits. Convergence evidence (Fig. 5) is empirical; there is no formal rate or stationarity guarantee for the alternating updates. A brief analysis (e.g., monotonic ascent under bounded curvature, or conditions ensuring convergence to a critical point) would solidify the algorithmic contribution.\n4. Data Efficiency. Experiments rely on high-quality data (e.g., PixMo-Cap); low-data regimes (few-shot/long-tail KD) remain untested. Evaluating DJIP under subsampling, noisy labels, or class imbalance—and reporting data-efficiency curves—would demonstrate robustness where KD is most needed."}, "questions": {"value": "1. Computational Overhead & Scaling. How does DJIP’s end-to-end cost scale with input resolution and batch size relative to vanilla KD? Please report latency (ms/img), throughput (img/s), and peak memory across GPU/CPU settings. Have you explored hardware-aware optimizations of the JPEG layer (e.g., fused CUDA kernels, SIMD/Neon intrinsics, TensorRT plugins), and what is the measured speedup?\n2. Format Generalization. Beyond JPEG, have you evaluated differentiable alternatives (e.g., WebP/AVIF or learned compression) to test the mechanism’s generality? A head-to-head comparison controlling bitrate/PSNR would clarify whether gains stem from JPEG’s structure or from compression-induced perturbations more broadly.\n3. Algorithmic Stability. Does the alternating scheme exhibit instability when class centroids shift quickly (e.g., early training, long-tail classes)? Would momentum/EMA, trust-region or proximal updates, or line-search on the CMI objective improve stability and convergence consistency? Any failure cases or sensitivity analyses?\n4. Black-Box Teachers. How does DJIP perform when the teacher is accessible only via queries (API/closed-source), precluding gradient flow? Can zeroth-order/finite-difference estimates, score-matching proxies, or pseudo-label distillation approximate the CMI objective, and with what accuracy–cost trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S75skvkT2d", "forum": "ZKYPoPn0fP", "replyto": "ZKYPoPn0fP", "signatures": ["ICLR.cc/2026/Conference/Submission9934/Reviewer_gUa4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9934/Reviewer_gUa4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885587583, "cdate": 1761885587583, "tmdate": 1762921384607, "mdate": 1762921384607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Differentiable JPEG-based Input Perturbation (DJIP), a novel framework designed to enhance teacher-student knowledge transfer in knowledge distillation (KD) without modifying the teacher model. \nDJIP uses a differentiable JPEG layer that perturbs the teacher’s inputs to directly increase conditional mutual information (CMI), which improves distillation effectiveness. \nThe paper also proposes a new alternating optimization algorithm to efficiently learn the JPEG layer's coding parameters to maximize the perturbed CMI. \nExtensive experiments on CIFAR-100 and ImageNet demonstrate that DJIP consistently boosts student accuracy by up to 4.11% while being computationally efficient and compatible with standard KD pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel technique, Differentiable JPEG-based Input Perturbation (DJIP), which is a plug-and-play framework for improving knowledge distillation (KD) without requiring modifications to the teacher model. \n\n2. The method is extensively evaluated on CIFAR-100 and ImageNet datasets, demonstrating consistent improvements in student accuracy, with gains up to 4.11%. \n\n3. DJIP is computationally lightweight and integrates seamlessly with standard KD pipelines, making it an efficient solution for enhancing knowledge transfer. The method optimizes just the JPEG layer without modifying the teacher model, ensuring low overhead.\n\n4. The proposed method works well across both same-architecture and cross-architecture (CNN-to-ViT) settings, which shows that DJIP can be applied broadly in knowledge distillation tasks, including distilling between heterogeneous model types.\n\n5. DJIP is orthogonal to other state-of-the-art methods like MCMI, as demonstrated by the paper's results. This suggests that DJIP can be integrated with existing KD techniques to further enhance performance, providing flexibility in distillation pipelines."}, "weaknesses": {"value": "1. While the method is effective in practice, the paper provides limited theoretical analysis of why perturbing the input via the JPEG layer improves distillation beyond just the CMI maximization objective. \n\n2. The method heavily relies on a differentiable JPEG layer, which could limit its applicability to certain use cases or architectures where JPEG compression may not be optimal or desirable. \n\n3. The alternating optimization algorithm introduced for learning the JPEG coding parameters adds a layer of complexity. While efficient, the algorithm may require fine-tuning, and its performance could vary depending on the choice of hyperparameters, such as lambda.\n\n4. The usage of JPEG limits the method within the image domain. The method’s effectiveness might degrade on different types of data, especially those that are not image-based or have different structures."}, "questions": {"value": "1. How does DJIP compare to other input perturbation methods in terms of generalization to different types of data, such as non-image datasets or tasks beyond image classification? Are there any potential limitations or challenges when applying DJIP to these domains?\n\n2. The proposed alternating optimization algorithm is central to maximizing the perturbed CMI. Could you provide more details on how this algorithm scales with larger models or more complex datasets, and whether there are any concerns regarding its stability or convergence in these cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qi7EsOUn1p", "forum": "ZKYPoPn0fP", "replyto": "ZKYPoPn0fP", "signatures": ["ICLR.cc/2026/Conference/Submission9934/Reviewer_RUYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9934/Reviewer_RUYs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932590140, "cdate": 1761932590140, "tmdate": 1762921384263, "mdate": 1762921384263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Differentiable JPEG-based Input Perturbation (DJIP), a plug-and-play framework for improving knowledge distillation (KD) without modifying the teacher model.\nDJIP introduces a differentiable JPEG layer before the teacher network to perturb inputs in a way that maximizes conditional mutual information (CMI) between inputs and outputs.\nThe authors also design an alternating optimization algorithm that jointly optimizes cross-entropy and CMI losses, enabling efficient learning of JPEG quantization parameters.\nExtensive experiments on CIFAR-100 and ImageNet show consistent student accuracy improvements (up to 4.11%) across CNNs and ViTs, and compatibility with many KD baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tSolid theoretical foundation: The CMI-based alternating optimization is mathematically consistent and improves upon fixed-centroid MCMI.\n2.\tComprehensive empirical validation: Tested on multiple datasets and KD frameworks, with consistent improvements.\n3.\tOrthogonality: Demonstrated compatibility with both MCMI and CKD, suggesting general utility according to the paper.\n4.\tReproducibility: Implementation details and appendices are complete and transparent."}, "weaknesses": {"value": "1.\tLimited theoretical novelty: The alternating CMI formulation is an incremental improvement over prior MCMI, and the key novelty lies in engineering implementation (JPEG layer).\n2.\tInsufficient qualitative analysis: The paper lacks visualization or interpretation of how and why JPEG perturbations affect teacher responses.\n3.\tLimited performance improvements: As an additional, plug-and-play module, DJIP offers limited performance enhancements to networks, which focuses more on engineering optimizations than theoretical innovations."}, "questions": {"value": "1.\tCould you provide a runtime and GPU memory usage of training DJIP layer?\n2.\tHow sensitive is the method to the hyperparameter λ? Are there any cases where maximizing CMI harms student performance?\n3.\tDid the authors observe any instability during training due to learning rate choice? How robust is DJIP to different optimizer configurations (e.g., step size, momentum, or learning)\n4.\tHave you considered using other differentiable codecs (e.g., differentiable WebP or learned compression networks) as a generalization of DJIP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tMJlxdqRl5", "forum": "ZKYPoPn0fP", "replyto": "ZKYPoPn0fP", "signatures": ["ICLR.cc/2026/Conference/Submission9934/Reviewer_tbBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9934/Reviewer_tbBh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964639575, "cdate": 1761964639575, "tmdate": 1762921383849, "mdate": 1762921383849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DJIP, a plug-and-play framework to enhance KD without modifying teacher model weights. Experiments show that this method has certain effectiveness. Experiments show that this method has certain effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper improves the effectiveness of knowledge distillation without modifying the teacher model’s weights. The work is both interesting and effective.\n2. The paper is well written, and the proposed method is plug-and-play and easy to follow.\n3. The paper provides a detailed and thorough theoretical analysis."}, "weaknesses": {"value": "1. The application scope demonstrated in this paper is somewhat limited. Could the proposed method be extended to LLMs (e.g., LLaMA) or generative models (e.g., Stable Diffusion)? Discussing applications beyond classification tasks would help enhance the paper’s breadth and contribution.\n2. The JPEG-layer perturbation operates in the pixel space. It is unclear whether such pixel-level perturbations have a significant impact on the feature space. The rationality and reliability of the supervision signal constructed in this manner are therefore questionable.\n3. The paper lacks an explanation regarding the selection strategy for key parameters, such as the frequency of alternating updates.\n4. The paper does not sufficiently discuss several recent SOTA works[1,2,3,4,5,6] on knowledge distillation.\n\n[1] f-Divergence Minimization for Sequence-Level Knowledge Distillation. ACL 2023.\n\n[2] DistiLLM: Towards Streamlined Distillation for Large Language Models. ICML 2024.\n\n[3] MiniLLM: Knowledge Distillation of Large Language Models. ICLR 2024.\n\n[4] Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models. COLING 2025.\n\n[5] ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via alpha-beta-Divergence. ICML 2025.\n\n[6] DA-KD: Difficulty-Aware Knowledge Distillation for Efficient Large Language Models. ICML 2025."}, "questions": {"value": "In addition to the issues mentioned in the Weaknesses, I have a few more concerns:\n\n1. The paper claims orthogonality between DJIP and MCMI since DJIP explicitly optimizes the input space. Could the authors provide quantitative evidence (e.g., gradient cosine similarity between DJIP and MCMI objectives) to support this orthogonality claim?\n2. Does increasing the number of quantization parameters consistently improve performance, or is there a saturation point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TZ5wRYlCgI", "forum": "ZKYPoPn0fP", "replyto": "ZKYPoPn0fP", "signatures": ["ICLR.cc/2026/Conference/Submission9934/Reviewer_JMaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9934/Reviewer_JMaG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762352974249, "cdate": 1762352974249, "tmdate": 1762921383547, "mdate": 1762921383547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}