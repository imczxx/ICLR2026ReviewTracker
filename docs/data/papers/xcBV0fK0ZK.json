{"id": "xcBV0fK0ZK", "number": 8788, "cdate": 1758098253639, "mdate": 1763130999289, "content": {"title": "Adversarial Robustness of LLM-Based Multi-Agent Systems for Engineering Problems", "abstract": "Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS), often in new domains, including for solving engineering problems. Unlike purely linguistic tasks, engineering workflows demand formal rigor and numerical accuracy, meaning that adversarial perturbations can cause not just degraded performance but systematically incorrect or unsafe results. In this work, we present the first systematic study of adversarial robustness of LLM-based MAS in engineering contexts. Using representative problems-including pipe pressure loss (Darcy-Weisbach), beam deflection, mathematical modeling, and graph traversal-we investigate how misleading agents affect collaborative reasoning and quantify error propagation under controlled adversarial influence. Our results show that adversarial vulnerabilities in engineering differ from those observed in generic MAS evaluations in important aspects: system robustness is sensitive to task type, the subtlety of injected errors, and communication order among agents. In particular, engineering tasks with higher structural complexity or easily confusable numerical variations are especially prone to adversarial influence. We further identify design choices, such as prompt framing, agent role assignment, and discussion order, that significantly improve resilience. These findings highlight the need for domain-specific evaluation of adversarial robustness and provide actionable insights for designing MAS that are trustworthy and safe in engineering applications.", "tldr": "LLM Agents can be influenced by other misaligned Agents, which we research the impact on for engineering problems", "keywords": ["LLM", "MAS", "Adversarial robustness", "Engineering", "GPT-4o mini", "Misalignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a925b93f4a7a4621347d246dc2bfbff0956b2bc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the adversarial robustness of LLM-based Multi-Agent Systems in engineering contexts. The authors selected several representative tasks, such as pipe pressure loss calculation and mathematical modeling, to explore this topic. Through a series of controlled experiments simulating red-team attacks, they conclude that the vulnerabilities of MAS in engineering domains differ from those in generic contexts and that the design of prompts is critically important for system robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel and important problem: the adversarial robustness of MAS in real-world engineering applications. I believe this is indeed a critical issue that warrants investigation.\n\n- The findings on the impact of different prompts, names, and authority levels on the rejection rate are compelling. They provide a valuable reference for the design of MAS specifically for the engineering domain.\n\n- The conclusions of this paper have strong practical implications and contribute significantly to the optimization of MAS for engineering tasks."}, "weaknesses": {"value": "- While the problem the paper aims to solve is urgent, the motivation is not clearly articulated. It remains unclear what the significant differences are between the adversarial robustness of MAS for general purposes and for engineering applications. The authors claim that being misled in engineering workflows is more severe than in other tasks, but they do not experimentally demonstrate this severity; they only show the system's susceptibility to being misled. It is crucial to elaborate on the potential consequences if an engineering MAS is misled. Can these consequences be quantified?\n\n- Figure 1 is unclear and fails to clearly illustrate the entire workflow of the MAS. Furthermore, it contains excessive white space. The authors should optimize this figure to make it more aesthetically pleasing and easier to understand.\n\n- The presentation of the experimental conclusions is disorganized. I suggest that after analyzing each specific task, the authors systematically organize and present the misleading issues encountered by MAS in engineering environments. This could be effectively presented in the main body through a statistical table or detailed case studies.\n\n- This paper reads more like an empirical study, where the authors conduct experiments and present results without engaging in deep discussion or attempting to explain them. The authors need to evaluate more dimensions of the results and provide deeper insights; otherwise, I doubt this work meets the acceptance criteria for a top-tier conference.\n\n- The experiments predominantly use a single model, gpt-4o-mini, with only limited tests on gpt-4o and o3-mini in the appendix. It is unclear whether the conclusions drawn throughout the paper reflect a common flaw in current MAS or are merely limitations specific to the model used.\n\n- The paper is missing some relevant citations, such as G-Safeguard[1] and ARGUS[2], which also discuss the dangers and monitoring of misinformation propagation in MAS.\n\n[1] arxiv.org/abs/2502.11127\n\n[2] arxiv.org/abs/2506.00509"}, "questions": {"value": "- I admit that I do not have extensive expertise in traditional engineering. But I would like to know if the four representative problems introduced by the authors can genuinely cover the majority of engineering domains.\n\n- I am curious about the impact of certain hyperparameters on the experimental results, such as the number of interaction rounds, the number of advisors in the MAS, and different topological structures of the MAS. When these parameters change, do the conclusions presented in the paper still hold? I believe this is a very important point regarding the generalizability of the findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kCHBqFcyPz", "forum": "xcBV0fK0ZK", "replyto": "xcBV0fK0ZK", "signatures": ["ICLR.cc/2026/Conference/Submission8788/Reviewer_jLyp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8788/Reviewer_jLyp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760635224920, "cdate": 1760635224920, "tmdate": 1762920562742, "mdate": 1762920562742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "3huUjF2JvM", "forum": "xcBV0fK0ZK", "replyto": "xcBV0fK0ZK", "signatures": ["ICLR.cc/2026/Conference/Submission8788/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8788/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763130998347, "cdate": 1763130998347, "tmdate": 1763130998347, "mdate": 1763130998347, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how adversarial agents affect LLM-based multi-agent systems in engineering tasks. Through representative problems, the authors show that such systems are especially vulnerable to adversarial influence, depending on task type and agent communication. They also identify design strategies to improve robustness and emphasize the importance of domain-specific evaluation for safe MAS deployment in engineering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is easy to follow and is organized well.\n2. The findings of this paper may be useful for solving engineering problems using multi-agent systems.\n3. This paper has studied the robustness of multi-agent systems under various problems."}, "weaknesses": {"value": "1. The author’s research setting is solving engineering problems, so I believe the choice of model should lean towards more powerful models such as GPT-5 or Gemini-2.5-pro. However, in the main text, the author uses GPT-4o-mini. Furthermore, as shown in Figure 7 of the appendix, both GPT-4o and o3-mini exhibit significantly greater robustness than GPT-4o-mini. This greatly undermines the validity and credibility of the conclusions presented in the main text.\n\n2. The author investigates the robustness of multi-agent systems in the context of solving engineering problems and assumes the existence of malicious agents, but an important aspect is missing: in this setting, how are malicious agents generated? In other words, from a practical perspective, how does this threat model hold up?\n    - What are the attackers' intentions?\n    - How do attackers manage to poison an agent? Does this require assuming the multi-agent system accepts information from the external environment?\n    - What are the concrete harms that the multi-agent system might suffer after an attack?\n    - Compared to the threat model in language tasks, what are the fundamental differences in this setting?\n    \n    The lack of thorough discussion and analysis in these areas makes it difficult for me to grasp the substantive contribution of this paper relative to prior work."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a1uh0ttESy", "forum": "xcBV0fK0ZK", "replyto": "xcBV0fK0ZK", "signatures": ["ICLR.cc/2026/Conference/Submission8788/Reviewer_E51W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8788/Reviewer_E51W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141997982, "cdate": 1761141997982, "tmdate": 1762920562350, "mdate": 1762920562350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of adversarial robustness in large-language-model (LLM) based multi-agent systems (MAS) applied to engineering problems. The authors focus on four representative tasks—pipe pressure loss (Darcy-Weisbach), cantilever beam deflection, basic math problems, and Eulerian graph traversal—and inject controlled semantic or numerical errors via a “misleading advisor” agent. This paper evaluates how variations in leader and advisor system prompts, LLM choice and decoding parameters, task formulation and complexity, number, order, and naming of agents affect the MAS’s tendency to accept or reject adversarial suggestions and final accuracy. The findings are: (1) Explicit warnings and non-concise leader prompts dramatically improve rejection rates; (2) Tasks with subtle numeric confusions are most vulnerable; (3) the agent who speaks first exerts outsized influence; (4) Naming or framing advisors as “experts” amplifies their persuasive power.."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper is the first to study adversarial prompt-level attacks on LLM-MAS in formal, numerically rigorous domains.\n\n2. All the details are provided in the appendix."}, "weaknesses": {"value": "1. Experiments are confined to idealized, textbook problems (Darcy–Weisbach formula, cantilever beam, simple sums and small graphs). There is no evidence that these findings carry over to complex, real-world engineering workflows (e.g. multiphysics simulations, CAD integration, control-system design).\n\n2. The paper limits itself to a single style of adversarial error (e.g. replacing 64/Re with 25/Re). Modern adversarial-example techniques (gradient-based prompt generation, reinforcement-learning attackers) are omitted. The threat model is therefore narrow and unlikely to capture the true worst-case behaviors of a malicious agent.\n\n3. The authors test ~20 prompt variants without explaining how these were selected or whether they cover the most impactful dimensions of system-prompt design. There is no principled methodology (e.g. pilot-study clustering, mutual-information ranking) to justify prompt choices, raising concerns of cherry-picking.\n\n4. The MAS model is a fixed, synchronous, two-phase, turn-based “leader + advisor(s)” scheme with a hard 5-round limit. Real MAS employ asynchronous messaging, hierarchical committees, or dynamic role reassignment. It is unclear whether the first-mover and “expert” effects persist in more realistic protocols.\n\n5."}, "questions": {"value": "The study falls short in practical relevance, methodological rigor, and threat-model completeness. Before reconsideration, the authors should:\n\n1. Demonstrate transfer of findings to complex, real-world engineering workflows.\n\n2. Incorporate automated adversarial-prompt generation to cover a broader threat space.\n\n3. Apply principled selection of prompt perturbations and control for multiple statistical tests.\n\n4. Explore more realistic MAS protocols (asynchronous, hierarchical)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fs0XxD2SeQ", "forum": "xcBV0fK0ZK", "replyto": "xcBV0fK0ZK", "signatures": ["ICLR.cc/2026/Conference/Submission8788/Reviewer_NDu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8788/Reviewer_NDu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078613839, "cdate": 1762078613839, "tmdate": 1762920561948, "mdate": 1762920561948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the  adversarial robustness of LLM-based multi-agent systems (MAS) specifically within engineering contexts. The MAS turns out to be a conversation between an LLM solving the problem and another LLM provides adversarial misleading information. Authors argue that  different from generic tasks, robustness in engineering problems is highly sensitive to the task's structural complexity, the subtlety of injected numerical errors, and the communication order. The study also identifies actionable design choices, including prompt framing, agent role assignment, and discussion order, that can significantly improve the trustworthiness of these systems in engineering applications."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Originality: the studied topic, adversarial robustness in engineering problems seems to be a new domain, and the studied problems are not tested in prior work\n\n2. The paper is easy to understand."}, "weaknesses": {"value": "1. This paper investigates adversarial robustness in **engineering problems**. However, I have concerns about the \"engineering problem\" setting This paper only studies four problems, namely, pipe pressure loss (Darcy-Weisbach equations), beam deflection, fundamental mathematical modeling, and graph theory. Based on my understanding , the latter two problems should be categorized as mathematical and programming problems. The beam deflection problem has also been studied in previous benchmarks MMLU and GPQA. \n\nHow do you define a problem as one of the **engineering problems**, instead of math, code or other problems studied in prior work? And why the adversarial robustness of **engineering problems** has different properties as other MAS? Secondly, only four case studies are sufficient to draw the conclusions.\n\n2. In the method section, it seems the MAS is just a conversation between an LLM solving the problem and another LLM provides adversarial misleading information. Since the second agent does not aim to solve this problem, I think the studied setting is far from really world MAS settings.  In Section 4.3, more advisors setting is discussed. However, it is not clear how these advisors are interacting with other agents.\n\n3. Only OpenAI family LLMs are studied in this work. It is not clear whether the conclusion in this work can transfer to other LLMs, i.e., the robustness issue is GPT4-mini's weakness or a shared weakness of all LLMs."}, "questions": {"value": "1. In Section 3.2, what is Re?\n\n2. In Section 4.1, how are the percentages computed. Run the same system with the same initial prompt multiple times and average the final results?\n\n3. What is the base performance of the 4 tasks without adversary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qetTblP3Y4", "forum": "xcBV0fK0ZK", "replyto": "xcBV0fK0ZK", "signatures": ["ICLR.cc/2026/Conference/Submission8788/Reviewer_7Scj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8788/Reviewer_7Scj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231649163, "cdate": 1762231649163, "tmdate": 1762920561606, "mdate": 1762920561606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}