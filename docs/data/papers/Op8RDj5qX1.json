{"id": "Op8RDj5qX1", "number": 16641, "cdate": 1758267163923, "mdate": 1763293110540, "content": {"title": "Optimizing optimizers for fast gradient-based learning", "abstract": "We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics collected from training or validation sets. Furthermore, this optimization of optimization can be performed dynamically during training.", "tldr": "Gradient-based optimizers translates force into parameter motion, and we can use this analogy to design a problem-specific optimizer.", "keywords": ["optimizer", "gradient-based learning", "power allocation", "quadratic form"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a6064f101520664596269ead827a46898d4f06e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies preconditioner-based optimizers, and develops a general class of problems from which preconditioners for many standard machine learning optimizers can be derived as solutions. The problems are posed in terms of finding a preconditioner that maximizes a power budget subject to constraint over a class of PSD matrices. The class of problems can be extended by allowing convolutions over time in order to facilitate the formulation of preconditioners that are computed using EMAs (e.g., use of variance estimators such as in Adam), as well as momentum. The authors present a list of problem formulations from which each standard optimizer is derived."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed class of optimization problems allows for a very wide range of optimizers to be derived. Derivations are presented for many optimizers that have not been explored from this view before.\n- The authors present a novel way to derive momentum from the extension to the framework presented in Section 3."}, "weaknesses": {"value": "- The main result, which is the unifying view of a preconditioner as the solution of this kind of constrained minimization problem, is not new. See AdaReg [1].\n- Missing related work on AdaReg [1], Linear Minimization Oracles [2] which unify \"step-as-a-minimizer\" optimizers just like this paper unifies \"preconditioner-as-a-minimizer\" optimizers, and older Quasi-Newton methods like BFGS [3] which try to adaptively estimate the preconditioner $Q$.\n- The authors propose a new method of tuning hyperparameters for this class of optimizers by collecting gradient covariances (line 366), but this is completely infeasible on all but tiny low dimensional problems due to how much memory it would require, and would be an empty suggestion. Even the gradients themselves take a substantial amount of memory to store and communicate; there is no way the gradient covariance, which needs the square of that amount of memory, would fit. Not only that, but the gradient covariance will also converge incredibly slowly since it would need to be estimated as a large sum over rank-one components.\n- While the paper presents a theory to connect existing optimizers, there is not demonstration of how the theory is intended be useful. For example, no new optimizers were derived from this theory or tested. The authors make no statements about the properties or convergence rates of existing optimizers or new optimizers that emerge as a result from this framework; only results to prove that the known optimizers indeed emerge as derived from the framework.\n- While the theory may give rise to a new optimizer, and the optimal $Q$ and gradient may mathematically exist, difficulty in computing the parameter update (without materializing $Q$ due to memory constraints) may easily block the resulting optimizer from ever becoming feasible to implement for many choices of $Q$ that the user might pick, unless this problem is addressed somehow.\n\n[1] Gupta, V., Koren, T., Singer, Y. (2017). A Unified Approach to Adaptive Regularization in Online and Stochastic Optimization. arXiv preprint arXiv:1706.06569.\n\n[2] Garber, D., & Wolf, N. (2021, July). Frank-Wolfe with a nearest extreme point oracle. In Conference on Learning Theory (pp. 2103-2132). PMLR.\n\n[3] Fletcher, R. (1988). Practical Methods of Optimization."}, "questions": {"value": "Is there any case where this theory can comes into use, where pre-existing work does not suffice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b8cKrgunJQ", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Reviewer_H31M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Reviewer_H31M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760567937093, "cdate": 1760567937093, "tmdate": 1762926704257, "mdate": 1762926704257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Initial Response and the Authors' Promise of Commitment to Revision"}, "comment": {"value": "Dear Reviewers,\n\nWe deeply thank the reviewers for taking your precious time to review our work.\n\nIn the upcoming three-week discussion period, we will do our best to resolve all the concerns that have been raised, as well as any that may arise.\n\nAs soon as the reviews were released, we carefully read all your valuable comments line by line, multiple times, to make sure that we fully understand your concerns.\n\nWe have already started working on the first revision, which should be delivered **no later than this Saturday (AoE)**, so that we can begin the discussion before the weekend passes.\n\nFollowing that, we will keep updating this until every one of your concerns has been resolved.\n\nWe are very excited to take part in this discussion.\n\nThank you again for your time and feedback.\n\nPlease stay tuned.\n\nBest Regards,\n\nThe Authors."}}, "id": "7Dd0R0XjMC", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762948752517, "cdate": 1762948752517, "tmdate": 1762948752517, "mdate": 1762948752517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unifying theoretical framework that treats the design of optimization algorithms as a constrained maximization of instantaneous loss reduction. By formulating the update rule as the optimal solution to a convex problem over a “budget set” of positive semidefinite operators, the authors show that a wide range of optimizers can all be derived as special cases. The overall idea is elegant and offers a fresh geometric perspective on optimization design, but the paper lacks substantial empirical evidence to demonstrate practical effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper formulates optimizer design as a convex optimization problem that maximizes the instantaneous decrease of the training loss. This simple but powerful view connects many existing algorithms under a single mathematical framework and provides clear geometric intuition.\n- Modeling momentum and EMA-style updates as single-pole linear filters and showing their optimality under extended dynamic budgets is technically sound and conceptually appealing.\n- If empirically validated, the framework could unify theoretical understanding and provide a foundation for automatic optimizer design across architectures and modalities."}, "weaknesses": {"value": "- The paper presents only small-scale toy examples. There are no experiments on standard deep-learning benchmarks such as CIFAR, ImageNet, or language models. As a result, it is difficult to assess whether the proposed “optimal” updates translate to practical gains in convergence or generalization.\n- The framework relies on estimating gradient covariance and cross-moment statistics, which can be unstable or expensive in large-scale settings. The paper does not discuss how these quantities are maintained efficiently or how noise affects the theoretical guarantees."}, "questions": {"value": "- How does the proposed “instantaneous loss reduction” objective correlate with the final validation or test loss in large-scale training? Have you observed cases where it leads to over-aggressive or unstable updates?\n- Can you provide quantitative experiments on at least one deep-learning benchmark, comparing your analytic optimizer against AdamW, Shampoo, or K-FAC under equal training budgets?\n- How sensitive is the method to errors in these statistical estimates? Does the Lipschitz stability analysis in the appendix extend to stochastic updates with mini-batches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M0dGoTkmbz", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Reviewer_5QCh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Reviewer_5QCh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661433518, "cdate": 1761661433518, "tmdate": 1762926703895, "mdate": 1762926703895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Announcement of the First Revision and Invitation to Discussion"}, "comment": {"value": "Dear Reviewers,\n\nWe thank all the Reviewers for waiting for our response.\n\nAfter reading all the valuable reviews, we have noticed that the Reviewers share a common set of concerns, which are of most concern to them. Therefore, in this letter, we would like to first write a general response to this, and then summarize the changes we made in our first revision. We kindly ask the Reviewers to first refer to this letter to help resolve their concerns, and then proceed to the individual comments we have written in response to each review.\n\nWe find two major concerns that are shared among the Reviewers: the (1) **questionable practical value** (all the Reviewers) and the (2) **lack of convergence endpoint analysis** (ca7o, 5QCh). We find that these issues hid the central meaning and value of this work at the fundamental level, making our manuscript weak even though no reviewers have denied the correctness and soundness of the presented theory, and, thankfully, all the Reviewers find our theory interesting and thorough (ca7o), clear in presentation (nr4L), technically sound and conceptually appealing (5QCh), and exceptionally broad and novel (H31M). Therefore, we also greatly value the importance of these two concerns and revised our manuscript to address them properly with the utmost priority.\n\nThe following is our response to these concerns and the key changes made in this first revision.\n\n---\n\n### **Regarding the practical demonstration**\n\nFrankly speaking, our original intention behind this paper was to deliver actual practical value, by eliminating the need for intense hyperparameter tuning for a great proportion of machine learning applications. However, after carefully reading the reviews, we find that our presentation was definitely not successful in delivering this intent. We have undertaken a major revision.\n\nOne of the main results was the construction and proof of the optimal hyperparameters for existing optimizers in practice, such as SGD+Momentum (**Corollary 5**) and Adam (**Corollary 6**). We also prove the optimal hyperparameter claims for other types of classified optimizers in **Table 3** and **Appendix E of the revised manuscript**. We rearranged the writing to highlight this contribution.\n\n1. We conducted a real experiment on CIFAR-100 with ResNet-18 as a minimally sufficient demonstration of the optimality proved in the theory. The results are summarized in the dedicated **Section 3.2** with two tables (**Tables 1 and 2**) and a figure (**Figure 3**).\n2. We rephrased the theorems, especially **Corollary 5 and 6**, to highlight that the results of the corollaries are the analytically optimal hyperparameter representations of the existing optimizers.\n3. We revised **Section 5** and attached experimental results to demonstrate the validation-aware optimizer tuning in **Figure 4**.\n\nWith these revisions and attached experimental results, we connect our theoretical findings to actual practical applications. We hope that this revision now fully delivers our intention to aid machine learning practitioners by **eliminating their struggles with hyperparameter search**.\n\n---\n\n### **Regarding the convergence endpoint of the greedy optimal optimizers**\n\nWe have added **Section 4 in the revised manuscript**, dedicated for convergence endpoint analysis for the greedy optimal optimizers of our framework. Also, we adjusted the overall writing so that the reading flows naturally. This extends the previous version’s Proposition 7 for endpoint analysis of validation-aware optimizers.\n\nHere, we summarize the key findings. We find that our greedy optimization framework also simultaneously aligns the resulting optimizer to the gradient distribution. This alignment is manifested as a commutativity relationship (**Lemma 7**). In a simple case of least squares, training leads to the best solution: the canonical pseudoinverse solution (**Proposition 8 & Figure 2**). For more complex scenarios, the training process can be approximated with a kernel flow, as we typically do with the neural tangent kernel. And, this alignment fixes the convergence endpoint to the minimum norm solution in the RKHS norm, which leads to good long-horizon convergence endpoints (**Theorem 9 & Figure 3**).\n\nWe hope this revision now provides a sufficient amount of credibility to our greedy paradigm, and opens up a new avenue for studying optimizers."}}, "id": "2xZ91AAsXW", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763293197137, "cdate": 1763293197137, "tmdate": 1763293197137, "mdate": 1763293197137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors provide a framework to obtain the update rules of commonly used optimizers as the result of an optimization problem interpreted as greedily minimizing the loss in one step. They also obtain rules for hyperparameter choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper presentation is clear, except for the minor typos."}, "weaknesses": {"value": "The authors, in their words, \"reverse-engineer commonly used optimizers\" by defining the update rule as the result of an optimization problem. The authors point out that this optimization problem gives an unbounded result and proceed to add different constraints to it, and pointing out that many optimizers in use arise as the result from this optimization problem, where each optimizer correspond to a specific constraint. \n\nOne can always represent a (stateless or not) algebraic rule as the solution of an optimization problem (e.g. in other domain: a function value can be written as the Fenchel dual of its Fenchel dual, which is the result from an optimization problem) and given that this paper only cares about obtaining these optimizers without providing any way to obtain other effective algorithms or select among them, and it does not provide experiments either on the effectiveness on, for instance, their insights of hyperparameter tuning, it seems to me that this only provides a mapping one to one between algebraic rules and the solution of optimization problems, which is a weak result for ICLR. \n\nThe authors suggest, in Proposition 7, to design optimizers that are optimized to decrease the validation loss . This seems to be essentially equivalent to using the validation set as training set and although the authors make a brief comment in passing about the potentially controversy of this, the discussion should be expanded, since as it is right now I am just inclined to think that the authors are simply not using any validation set and using such data as training data.  \n\n\nA couple of minor typos: \n\nline 174 Eveidence\nline 247 the followings -> the following\nline 269 obtaind"}, "questions": {"value": "Suggestions: \nIt would be good that the authors add an extended explanation for their choice of a positive semidefinite operator Q as opposed to any other choice.\n\nDrori and Teboulle https://arxiv.org/abs/1206.3209 developed the widely used PEP framework (https://github.com/PerformanceEstimation/PEPit) that phrases the problem of finding the best optimization algorithm for a problem in an algorithmic class, into solving semidefinite programs. This literature is relevant and should be discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "01XYuDrCwc", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Reviewer_nr4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Reviewer_nr4L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664734631, "cdate": 1761664734631, "tmdate": 1762926703348, "mdate": 1762926703348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a unified framework for designing optimizers that are “optimal” in the sense of maximizing the instantaneous reduction in loss. They develop this framework and show that it can both recover common optimizers as well as find closed-form solutions for the optimal hyperparameters of these optimizers. The framework is developed both with and without memory for the optimizer (e.g. momentum variables)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The general approach is interesting, with potential applications to find better optimizers than the ones which are currently popular.\n* The theoretical aspects are very thoroughly analyzed, including a very wide array of common optimizers in the new framework."}, "weaknesses": {"value": "1. Formulating optimizer design as seeking to maximize the instantaneous decrease in loss seems like a greedy choice. A more natural definition would be to minimize the loss at the end of training, which is the common definition of optimization tasks as it is. Even though this point is addressed in the paper, I think it brings into question the motivation for this work. Can we really expect real-world benefits from this approach? Instead of analyzing so many theoretical aspects, it would have been better to take the next step and show that this approach can be extended to the more natural “minimize converged loss”.\n\n\n2. Even though the generality of the framework is a good thing, it still seems a bit contrived. There’s no good a-priori choice of optimizer budget, so being able to reverse-engineer existing optimizers doesn’t seem like a surprising result. By shaping the budget I can make every optimizer seem “optimal”, even so the optimality doesn’t really make any sense. This makes me think that the new framework is too broad to be useful as it is.\nEven calling the budget a “budget” is confusing, since there’s no real “cost” being spent here - it’s constraining the search space of the problem. The term optimizer family is more appropriate (also used in the paper).\n\n\n3. No real-world problem was shown to benefit from this approach experimentally.\n\n4. The paper is VERY dense with results, with no room for any of the proofs. I would have expected that at least from the reverse-engineering we could glean some insights into the choice of optimizer family, but the fact this was omitted strengthens my assertion that the “optimizer family” is too broad to be a useful concept. The density of results makes the paper hard to follow. Almost every paragraph introduces a new concept. The appendices are pretty much a required read, which shouldn’t be the case."}, "questions": {"value": "1. When working on a new optimization problem, what suggestions do you have for choosing the optimizer family?\n2. I would suggest making at least a few experiments with real-world problems, such as training even a small open-source LLM, comparing convergence rate and achieved performance. At least demonstrate you can find the optimal hyper parameters without expensive hyper opt search, and better yet - show you can choose an optimizer family that leads to faster convergence than e.g. Adam."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ye28W8hxs", "forum": "Op8RDj5qX1", "replyto": "Op8RDj5qX1", "signatures": ["ICLR.cc/2026/Conference/Submission16641/Reviewer_ca7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16641/Reviewer_ca7o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16641/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926744144, "cdate": 1761926744144, "tmdate": 1762926702723, "mdate": 1762926702723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}