{"id": "NpU7ZXafRi", "number": 11793, "cdate": 1758203853266, "mdate": 1763610582249, "content": {"title": "Dynamic Early Exit in Reasoning Models", "abstract": "Recent advances in large reasoning language models (LRMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and LiveCodeBench) show that the proposed method is consistently effective on 11 cutting-edge reasoning LLMs of varying series and sizes, reducing the length of CoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3% to 5.0%.", "tldr": "", "keywords": ["Large Language Models", "Efficient Reasoning", "Early Exit"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b969183337882fda66f9829a15aefe276baee70a.pdf", "supplementary_material": "/attachment/46c8dc092ec9d3e36c33d26f2f82b15738a67bdf.zip"}, "replies": [{"content": {"summary": {"value": "Summary\n\nThis paper proposes DEER (Dynamic Early Exit in Reasoning), a training-free framework that allows large reasoning models (LRMs) to dynamically terminate the chain-of-thought (CoT) generation when the model is already confident enough in its answer. The method monitors reasoning transition points through linguistic or entropy-based signals, induces trial answers at these points, and uses token-level confidence scores to decide whether to stop reasoning early. The authors also introduce DEER-Pro, a robust variant using multiple answer inducers and calibrated confidence aggregation.\nExperiments across 10 reasoning benchmarks (math, science, and programming) and 11 models (1.5B–671B parameters) show substantial token reduction (19–80%) and accuracy improvements (up to +5%). The method is efficient, plug-and-play, and demonstrates strong robustness to thresholds and model scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear Motivation & Insightful Observation:\nThe paper convincingly identifies the “overthinking” problem in modern reasoning LLMs and empirically verifies the existence of pearl reasoning — points where further CoT expansion is redundant or even harmful.\n\nSimple Yet Effective Approach:\nDEER is training-free, easy to integrate, and works well across different reasoning models. It leverages confidence signals during inference — a practical and theoretically grounded design.\n\nComprehensive Experiments & Strong Results:\nEvaluations cover diverse tasks (GSM8K, AIME, GPQA, BigCodeBench) and model families. DEER consistently outperforms baselines such as CoD, Dynasor-CoT, and SEAL in both accuracy and efficiency."}, "weaknesses": {"value": "The paper uses a limited number of backbone models, and it remains unclear how well the proposed method performs on closed-source reasoning models.\n\nMathematical reasoning benchmarks typically use pass@k metrics to reduce randomness and improve robustness, but this paper does not adopt that evaluation standard.\n\nThe authors do not include ablation studies to verify how each component or module of the proposed method contributes to the overall performance improvement."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "viBnQQtrx3", "forum": "NpU7ZXafRi", "replyto": "NpU7ZXafRi", "signatures": ["ICLR.cc/2026/Conference/Submission11793/Reviewer_Ax2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11793/Reviewer_Ax2Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608634960, "cdate": 1761608634960, "tmdate": 1762922816132, "mdate": 1762922816132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DEER, a training-free method for eliciting efficient reasoning from LRM by dynamically early-exiting CoT generation. The authors argue overthinking harms both latency and accuracy in long CoTs. Instead of using fixed heuristics, DEER watches for reasoning transitions (linguistic markers like “Wait” or entropy spikes), briefly induces a trial answer, and computes a geometric-mean confidence from token probabilities. If confidence exceeds a threshold, it stops thinking and outputs the answer. A robust variant, DEER-Pro, averages multiple inductions and MAD-calibrates confidence. Across 10 benchmarks and 11 models, DEER cuts reasoning length while achieving higher accuracy over the vanilla LRM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is a training-free and plug-and-play approach for eliciting efficient reasoning from LRMs, which has clear advantages over training-based methods.\n2. The experiments demonstrate that the method improves efficiency and accuracy simultaneously.\n3. The method is well-motivated and clearly presented."}, "weaknesses": {"value": "1. Confidence may not always serve as a reliable signal for early exit. Prior work [1] has shown that explicitly encouraging reflection (such as by appending tokens like “wait”) can help the model identify and correct earlier mistakes. In contrast, DEER’s reliance on confidence may lead the model to terminate reasoning prematurely: it could be highly confident yet incorrect, losing the opportunity to self-correct in later reasoning steps.\n2. It remains unclear how DEER and DEER-Pro achieve higher accuracy than the vanilla model in Table 1 despite substantial reductions in token length. Without a detailed explanation, it is difficult to justify the improvement.\n3. The fairness of the reported comparisons is uncertain. DEER-Pro employs a parallel decoding strategy, which inherently introduces additional computational overhead compared to single-path decoding methods. This difference complicates claims of efficiency and may limit the validity of direct, apples-to-apples comparisons with baseline models.\n\nReferences\n1. s1: Simple test-time scaling"}, "questions": {"value": "Can this training-free method for efficient reasoning be adapted to be a training-based method to further improve efficiency and accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M6rGYnUWE1", "forum": "NpU7ZXafRi", "replyto": "NpU7ZXafRi", "signatures": ["ICLR.cc/2026/Conference/Submission11793/Reviewer_gfWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11793/Reviewer_gfWw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042199589, "cdate": 1762042199589, "tmdate": 1762922815595, "mdate": 1762922815595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large reasoning models often produce excessively long reasoning sequences, leading to computational inefficiency. To address this issue, the paper proposes DEER, a method that detects transition points during model reasoning, based on linguistic markers or entropy, and prompts the model to generate trial answers at these points. A confidence evaluation mechanism is then applied to decide whether to stop further reasoning and produce a final answer or continue generating additional thoughts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe proposed method is simple and effectively reduces the number of output tokens in practice.\n-\tThe experiments are comprehensive, covering a wide range of models (DeepSeek-R1-Distill-Qwen series, Qwen3 series, QwQ-32B, and DeepSeek-R1) across 10 benchmarks."}, "weaknesses": {"value": "-\tMore details about the branch-parallel acceleration strategy (Section 3.3) are needed. The current description lacks clarity.\n-\tThe technical novelty is relatively limited. Using linguistic markers or entropy to identify reasoning transitions is not entirely new, as similar ideas have been well explored in prior and concurrent works [1-6]. It would be beneficial to include a discussion of these related approaches.\n-\tThe paper lacks an evaluation of the quality of reasoning steps. It remains unclear whether early exits lead to oversimplified reasoning or disrupt the reasoning flow. A more detailed quality analysis would strengthen the work.\n\n[1] Yan, Hang, et al. \"Mur: Momentum uncertainty guided reasoning for large language models.\" arXiv preprint arXiv:2507.14958 (2025).\n[2] Wang, Shenzhi, et al. \"Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.\" arXiv preprint arXiv:2506.01939 (2025).\n[3] Fu, Yichao, et al. \"Efficiently Scaling LLM Reasoning with Certaindex.\" arXiv preprint arXiv:2412.20993 (2024).\n[4] Agarwal, Shivam, et al. \"The unreasonable effectiveness of entropy minimization in llm reasoning.\" arXiv preprint arXiv:2505.15134 (2025).\n[5] Huang, Jiameng, et al. \"Efficient reasoning for large reasoning language models via certainty-guided reflection suppression.\" arXiv preprint arXiv:2508.05337 (2025).\n[6] Vanhoyweghen, A., Verbeken, B., Algaba, A., & Ginis, V. (2025). Lexical hints of accuracy in llm reasoning chains. arXiv preprint arXiv:2508.15842."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ViaEqs0fDk", "forum": "NpU7ZXafRi", "replyto": "NpU7ZXafRi", "signatures": ["ICLR.cc/2026/Conference/Submission11793/Reviewer_LEHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11793/Reviewer_LEHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210845216, "cdate": 1762210845216, "tmdate": 1762922814369, "mdate": 1762922814369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}