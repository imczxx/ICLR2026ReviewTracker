{"id": "KmAQFT5F3Y", "number": 13047, "cdate": 1758213064960, "mdate": 1759897469009, "content": {"title": "Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift", "abstract": "Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe$^2$L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, $\\bf H$ard Label for $\\bf A$lleviating $\\bf L$ocal Semantic $\\bf D$rift ($\\bf HALD$),  which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 41.8\\% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by $\\bf 8.1$ \\%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label–dominated training.", "tldr": "", "keywords": ["Efficient Learning", "Green AI"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b0688f752ce419c5023a5662caa37059e7c1d1f.pdf", "supplementary_material": "/attachment/aac814bea1ff3407480a09c82641145531cb79ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies \"Local Semantic Drift\" (LVSD), a problem in dataset distillation where using few crops (to save storage) causes soft labels to mismatch the image's global semantics. To address this, the authors propose HALD (Hard Label for Alleviating Local Semantic Drift), a \"Soft-Hard-Soft\" training schedule that uses hard labels as a \"content-agnostic anchor\" to correct this drift. The paper provides theoretical justification for the problem and the solution and demonstrates strong empirical results, such as an 8.1% improvement over LPLD on ImageNet-1K under a 285M storage budget."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Clear Problem Identification: The paper clearly identifies, names, and illustrates (Fig. 1) a practical and important problem (LVSD) that arises from the very real constraint of soft-label storage costs.\n\n2. Strong Empirical Results: When properly isolated (in the ablations), the HALD training schedule provides massive, consistent improvements over a Soft-Only baseline. Table 5 shows HALD boosts performance across all tested generation methods (e.g., +10.5% for FADRM on ImageNet-1K, IPC=10, SLC=100).\n\n3. High Practical Value: The method directly addresses a practical bottleneck (storage). The results in Table 4, showing HALD's robustness to soft-label compression, are highly compelling.\n\n4. Coherent Justification: The paper provides a complete narrative: it identifies a problem (LVSD), formalizes it (Thm 1-2), proposes a solution (HALD), and provides both theoretical (Thm 3, Cor 1) and empirical (Fig 3) justification for why the solution works (gradient alignment leads to variance reduction)."}, "weaknesses": {"value": "1. Missing Key Baseline: The paper's novelty rests on its \"Soft-Hard-Soft\" schedule. However, it fails to compare against the most obvious and simpler baseline: a static combined loss (e.g., $\\mathcal{L} = \\mathcal{L}_{soft} + \\lambda \\mathcal{L}_{hard}$), which is related to prior work cited by the authors (e.g., GIFT). This makes it difficult to assess if the complex schedule is truly necessary.\n\n2. The paper's true contribution is best isolated in Table 5, which clearly compares Soft-Only vs. HALD on the same generated data. This table is very strong but is presented as an ablation rather than a main result."}, "questions": {"value": "1. The novelty of the \"Soft-Hard-Soft\" schedule is a key point. Could the authors provide an ablation comparing HALD against a simpler combined loss (e.g., $\\mathcal{L} = \\mathcal{L}_{soft} + \\lambda \\mathcal{L}_{hard\\_w/\\_LS}$)? This would significantly strengthen the justification for the schedule-based design.\n\n2. The final \"Soft\" refinement stage (Stage C) appears critical, given the poor performance of \"Soft-Hard\" in Table 7 (14.2% vs 37.0% for Soft-Hard-Soft). Can you provide more intuition for why this stage is so important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HUFPnWSUg9", "forum": "KmAQFT5F3Y", "replyto": "KmAQFT5F3Y", "signatures": ["ICLR.cc/2026/Conference/Submission13047/Reviewer_x3At"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13047/Reviewer_x3At"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943989591, "cdate": 1761943989591, "tmdate": 1762923781465, "mdate": 1762923781465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the local semantic drift problem when applying soft labels in dataset distillation methods for downstream training, pointing out that when the number of image croppings is limited, soft labels may deviate from the original semantics, resulting in a mismatch between training and test distributions. To address this issue, this paper proposes HALD method, which employs a three-stage soft-hard-soft training mechanism. First, hard labels are introduced as content-agnostic anchors to calibrate semantic drift, building upon soft label training. Then, the training returns to soft labels for fine-tuning to maintain fine-grained semantic features. Also, the theoretical analysis in the paper explains that introducing hard labels into soft-label training can improve optimization stability and generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper points out the problem of local semantic drift in soft-label based dataset distillation with a clear motivation.\n2. The paper also theoretically analyzes the importance of introducing hard labels into soft-label based dataset distillation, demonstrating its ability to improve optimization stability and generalization ability. The paper is well-reasoned and supported.\n3. The performance of the proposed method compared with baseline under the same storage budget is good.\n4. The article has a good structure and is well-written."}, "weaknesses": {"value": "1. When SLC is set to a fixed value (e.g., 300), the performance of the original method (which requires augmentation and generation of corresponding soft labels in each epoch during downstream training) is degrade. However, SRe2L is improved compared to the original method. This phenomenon is strange (because according to Table 5, the performance actually decreases when SLC is reduced).\n2. One question is that, according to the original method, methods like RDED can directly store the teacher model for online soft label generation. For example, the ResNet-18 model only requires 44.7MB of storage, while IPC10 can achieve an accuracy of 42%.\n3. There are also some methods, e.g., GIFT[1], for combining soft labels and hard labels, but the paper does not make further comparisons, only briefly described in related work part. Other methods [2] explored the domain shift issue, but no further comparisons are made.\n\n[1] GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost\n\n[2] Large Scale Dataset Distillation with Domain Shift"}, "questions": {"value": "1. Based on weaknesses 1 and 2, it would be helpful if the author could provide further explanation and clarification regarding these questions. Furthermore, it would be appreciated if the author could clearly explain how to choose the timing (not the length) for hard-label training.\n2. Based on weakness 3, if the authors can provide further comparisons and explanations would be appreciated.\n3. The cross-architecture experiments mainly performed on residual-based models. To demonstrate a more comprehensive cross-architecture capability, comparisons can be made with models such as ViT and VGG."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pWMQdroRN8", "forum": "KmAQFT5F3Y", "replyto": "KmAQFT5F3Y", "signatures": ["ICLR.cc/2026/Conference/Submission13047/Reviewer_xHep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13047/Reviewer_xHep"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971373305, "cdate": 1761971373305, "tmdate": 1762923781075, "mdate": 1762923781075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies dataset distillation under low Soft Label per Class (SLC) and without further teacher access. It is observed that local crops of an image can obtain soft targets that are inconsistent with the global image label. In this paper, it is referred to as Local View Semantic Drift (LVSD). To address this, the authors propose a three-stage training schedule, named HALD, which consists of Soft, Hard, and Soft stages. On the specific protocol defined in the paper, HALD achieves strong empirical results, as shown in Table 4."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a concrete failure mode of low SLC distillation. Local crops do not always agree with the image-level target;\n- The proposed Soft Hard Soft schedule is simple and easy to add to existing dataset distillation pipelines;\n- Empirical results in the stated setting are clearly positive. The paper also attempts to give a theoretical view, which is not very common in this area."}, "weaknesses": {"value": "### The research problem depends on a specific setting\nThe whole story assumes that the image-level hard label is the only correct semantic anchor. Any crop level target that is different from it is treated as drift. This is one possible viewpoint, but not the only reasonable one. If we look from a local visual viewpoint, then many of the paper’s examples are no longer drift. A crop that shows only the ball in a human and ball image can reasonably receive the class ball from the teacher. In this viewpoint HALD is not correcting an error. HALD is enforcing a preference for global semantics over local visual fidelity. \n\nThe paper should explain why the global label dominant viewpoint is the right one for dataset distillation. Otherwise, it is more like you first chose a particular way of looking at the problem, and that viewpoint makes the issue look serious. But if someone does not adopt your viewpoint, the issue may not even arise. Therefore, you cannot present it as a universally occurring problem.\n\n### The reason for the performance improvement is unclear\nHALD is described as using image-level hard labels as a content-agnostic anchor to pull drifting soft labels back. In many realistic images, the global hard label is not an unbiased anchor for a local crop. It is another biased signal at a different scale. For example, in a multi-object scene or in a heavily occluded crop, the global label can be semantically farther from the crop than the original soft prediction.\n\nA very plausible interpretation is therefore the following. Stage B injects a deliberately mismatched but stable supervision signal, which prevents the student from overfitting any single view and thus acts as a strong regularizer. The current experiments do not distinguish between these two explanations. If the authors want to claim semantic calibration, they need to show crop-level consistency or similar observable quantities before and after Stage B. Otherwise, the safe reading is that HALD is a good regularized schedule rather than a semantic correction mechanism.\n\n### Alignment between theory and Algorithm\nSection 3.4 together with Appendix B.4 proves a control variate style variance reduction statement. In that setting, two highly correlated signals are used in the same update. The result is an effective sample size of at least s over $1- \\rho^2$. The implemented algorithm is different. HALD uses the two signals in different stages. First soft labels, then hard labels, then soft labels again. The paper does not provide a mathematical argument that this stage-wise or asynchronous use of correlated signals preserves the same effective sample size gain as the synchronous case analyzed in the theory.\n\nAnother thing. Theorem 3 on gradient alignment only shows that the switch is optimization coherent. It shows that the gradients of soft and hard become close, so the switch will not destroy training. It does not show that the switch can improve performance or reduce variance in the way claimed for the per-step setting. At present, the theory should be read as motivational for HALD rather than as a formal justification of the exact algorithm in Section 3.3.\n\n### Experiments\nAll main tables compare HALD, which uses soft and hard signals in stages, with a soft-only baseline under low SLC and without teacher access. This is an information source un-equal comparison. Two supervision sources are compared to one supervision source. A stronger test would keep storage and training budget the same and add a simultaneous objective $L = L_{soft} + \\lambda L_{hard}$ or allow existing baselines to attach the same hard label branch. Without this comparison, we cannot tell whether the gain comes from the schedule itself or simply from the fact that HALD uses one more supervision signal."}, "questions": {"value": "- Can you report a simultaneous soft plus hard baseline under the same settings?\n- The variance reduction derivation in Appendix B.4 also applies to the stage-wise HALD, or is it only an intuitive guide?\n- What happens if the teacher can produce crop-level or region-level predictions so that LVSD is small? If we adopt a relaxed setting where the teacher can be queried a few times during training, is HALD still the best use of the budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BQzwnPyD2Q", "forum": "KmAQFT5F3Y", "replyto": "KmAQFT5F3Y", "signatures": ["ICLR.cc/2026/Conference/Submission13047/Reviewer_6Y9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13047/Reviewer_6Y9U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128356075, "cdate": 1762128356075, "tmdate": 1762923780718, "mdate": 1762923780718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}