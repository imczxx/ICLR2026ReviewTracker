{"id": "dCm9bBrk5d", "number": 4234, "cdate": 1757644085700, "mdate": 1759898045120, "content": {"title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "abstract": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established response patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from the expert, which promotes on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on mathematical reasoning problems and practical tool-use tasks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We will release the source code to inspire further research.", "tldr": "", "keywords": ["Large Language Model", "Post-Training", "Reinforcement Learning", "Supervise Fine-Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd5e79864c85fd83370767b2d564381846d9e484.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the interplay between Supervised Fine-Tuning (SFT)  and RL. It proposes a simple yet effective method called Chord, where GRPO and SFT loss are dynamically combined, where the loss computation of SFT is further adjusted based on each token’s policy probability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The major focus — addressing the shift-readapt-overfit phenomenon — is well motivated, and the analysis is insightful. Overall the paper has conducted comprehensive experiments with detailed analysis to explain various design choice."}, "weaknesses": {"value": "The proposed weighting mechanism is not very novel, and the empirical effectiveness is mostly observed on Math."}, "questions": {"value": "1. I don’t see much difference btw GRPO (pure RL) v.s. proposed method in BFCL, even though 5k instances are used, any intuition why the improvement is very large on AIME but not in tool-use?\n\n2. Continue from Q1, since Math benefit more from longer reasoning chain, chord-$\\mu$ could mostly learn from expert data (Deepseek-R1)’s format and tendency to generate long reasoing trace, which greately improve the result. From Table 1, the result seems to confirm about this assumption (comparing SFT-best+RL v.s. CHORD). This makes me question if the model is well SFT-trained, is there still any need for combining RL + SFT. \n\n3. Is there any result on base model Qwen3-8B-base? As newer model has much better performance on math/tool-use domain, I am curious if the method still brings improvement across math/tool-use domain.\n\n4. the method of smoothly combining with SFT should be generalizable to non-verifable task as well (and arguably more useful since SFT data could help constrain exploration space to prefered style/format). Any thoughts on such setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pIlgCNYwn1", "forum": "dCm9bBrk5d", "replyto": "dCm9bBrk5d", "signatures": ["ICLR.cc/2026/Conference/Submission4234/Reviewer_pEZ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4234/Reviewer_pEZ4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018911674, "cdate": 1761018911674, "tmdate": 1762917242808, "mdate": 1762917242808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a method to unify supervised fine-tuning (SFT) and reinforcement learning (RL) into a composite objective that weights the two individual objectives with a dynamic weighting value. Furthermore, the paper proposes to weight individual tokens to avoid issues that can arise from over-reliance on off-policy data and to encourage on-policy exploration. The effectiveness of these two weighting factors are studied empirically on Math and Tool-use datasets. The paper uses datasets provided by an expert model to learn policy. Empirical results suggest that the proposed method exceeds several reasonable baselines that includes both SFT and RL variants as well as  recently proposed works (LUFFY, SASR)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clear description of the two objectives used in their method called CHORD. Furthermore, the paper clearly describes the experimental setup and results.\n- The analysis conducted to motivate the method is clear. \n- Empirical results suggest that the proposed method (CHORD-\\phi) improves over several reasonable baselines on Math and Tool-use cases. The ablations included in the main paper suggest that the transition from offline imitation to online-RL learning is effective as it allows for exploration by the policy."}, "weaknesses": {"value": "- The paper proposes one simple way to combine the two objectives. It's not clear why a convex combination of SFT and RL objectives is the right approach. Would it be possible to have generic weights for SFT and RL and let the model and data decide their optimal values? \n\n- The objective for Chord-\\phi uses a weight that looks like the variance of a Bernoulli random variable. Just like above, is this the optimal value for this weight? Are there any insights on what might happen if the base model is not as strong as the one considered in the experiments?\n\n- Related to above, the analysis is conducted on Qwen2.5 for Math and Llama-3.2 for Tool use. What are the reasons for using these models the way they were used? Would the findings translate to other/newer models released in the (near) future?"}, "questions": {"value": "(repeated from weaknesses)\n\n- Would it be possible to have generic weights for SFT and RL and let the model and data decide their optimal values? \n- Are there any insights on what might happen if the base model is not as strong as the one considered in the experiments for the Chord-\\phi's token weight?\n- Are the findings applicable to newer models, especially architectures like MoEs that have been released or will be released in the future.\n\nI would like to discuss the first two questions with the authors during rebuttal. The third question is asked to help the paper make generic but does not require a response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2FE1dpWktF", "forum": "dCm9bBrk5d", "replyto": "dCm9bBrk5d", "signatures": ["ICLR.cc/2026/Conference/Submission4234/Reviewer_UX1j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4234/Reviewer_UX1j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791471724, "cdate": 1761791471724, "tmdate": 1762917242364, "mdate": 1762917242364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose CHORD, a unified framework that integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) through a dynamically weighted objective. CHORD introduces two main components: a global coefficient μ that decays over time to balance imitation and exploration, and a token-wise weighting function ϕ(p)=p(1−p) to stabilize off-policy updates by emphasizing uncertain tokens. The method aims to mitigate instability when combining on-policy RL signals with off-policy expert data. Experiments on mathematical reasoning (OpenR1-Math) and tool-use tasks (ToolAce) demonstrate improved stability and modest gains over SFT→RL and recent hybrid methods such as LUFFY and SASR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets an important and timely problem in large language model post-training: how to combine supervised expert data with reinforcement learning in an effective way.\n\n\n- The proposed framework is simple, well-motivated, and easy to implement in existing RLHF pipelines. The dual-control design (μ and ϕ) provides both stage-level and token-level balance between on- and off-policy learning.\n\n\n- Experiments are extensive and include ablations (fixed vs. dynamic μ, with vs. without ϕ), entropy/reward analyses, and qualitative case studies that support the claimed stability improvements."}, "weaknesses": {"value": "The novelty of CHORD is limited. The method reweights two existing loss terms (SFT and RL) using a dynamic coefficient and a heuristic token-wise weighting. Similar annealing strategies and uncertainty-based regularization have been explored in LUFFY, SRFT, and PPO variants with KL or imitation penalties.\n\n- The token-level weighting ϕ(p)=p(1−p) is conceptually similar to entropy-based weighting and lacks theoretical justification for its specific form.\n\n- The improvement margins over baselines are modest, and the experiments do not cover diverse post-training domains such as instruction-following or dialogue, leaving generality uncertain.\n\n- The framework lacks a formal connection to off-policy correction theory or mixed-policy optimization, making it primarily heuristic rather than theoretically grounded."}, "questions": {"value": "- Can the authors clarify how CHORD differs algorithmically from LUFFY or SRFT, beyond changing the weighting coefficients?\n\n\n- Is there any theoretical interpretation (e.g., weighted policy gradient under mixed distributions) that supports the design of μ and ϕ?\n\n\n- Have the authors tried learning μ adaptively (e.g., via reward variance or gradient norms) rather than fixing a decay schedule?\n\n\n- Does the token-level ϕ weighting introduce significant computational overhead?\n\n\n- How sensitive is the model’s stability to the exact shape of ϕ(p)? Would other functions (e.g., entropy-based) work similarly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C6zRwV6Wli", "forum": "dCm9bBrk5d", "replyto": "dCm9bBrk5d", "signatures": ["ICLR.cc/2026/Conference/Submission4234/Reviewer_f4d2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4234/Reviewer_f4d2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854192319, "cdate": 1761854192319, "tmdate": 1762917242090, "mdate": 1762917242090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines why SFT+RL can underperform pure RL when expert data diverges from the policy, characterizing a “shift–readapt–overfit” dynamic. It proposes CHORD, which mixes GRPO with an auxiliary SFT loss: a global weight controls the overall expert influence; a token-wise weight emphasizes mid-probability tokens. Results on math and tool-use show improvements over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper gives a clear empirical documentation of SFT instability under off-policy expert trajectories.\n\n2. The hybrid objective (μ-weighted SFT + RL) is easy-to-implement. \n\n3. The token-wise weighting is a simple stability heuristic; and the ablations on μ and training dynamics are decent."}, "weaknesses": {"value": "1. The idea of combining supervised learning and RL during fine-tuning has been explored in prior works (e.g., SRFT, SimpleMix, LUFFY). CHORD uses a similar structure by optimizing a weighted sum of SFT loss and GRPO — with the addition of a global schedule μ and a token-level weight φ(y)=p(1–p).\n\n2. The heuristic p(1−p) is plausible but lacks theoretical backing or strong comparisons to alternative uncertainty weights (entropy/focal/margin).\n\n3. There is a heavy reliance on DeepSeek-R1 experts; and the analysis is limited for weaker/similar experts or different stylistic gaps."}, "questions": {"value": "1. Can you provide controlled comparisons to SRFT/SimpleMix with matched compute/data and identical rollout settings? What is fundamentally new beyond weighting choices?\n\n2. Can you compare the weighting funciton to alternatives (entropy, focal-style, margin/clipping) and report sensitivity?\n\n3. Test experts that are weaker or stylistically closer to the policy; does μ decay still help, or does CHORD harm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0weLf5Rro", "forum": "dCm9bBrk5d", "replyto": "dCm9bBrk5d", "signatures": ["ICLR.cc/2026/Conference/Submission4234/Reviewer_hxFy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4234/Reviewer_hxFy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100433425, "cdate": 1762100433425, "tmdate": 1762917241725, "mdate": 1762917241725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}