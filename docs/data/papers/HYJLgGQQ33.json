{"id": "HYJLgGQQ33", "number": 16841, "cdate": 1758269287417, "mdate": 1759897216375, "content": {"title": "NR-DARTS: Node Rewiring for Differentiable Architectures with Adaptive SE-Fusion", "abstract": "Efficient model design is critical for deployment on edge and embedded hardware where compute, latency, and energy budgets dominate feasibility, which has driven the adoption of Neural Architecture Search (NAS) to discover task specific backbones. Because multi objective search balances accuracy and efficiency under proxy evaluation, the resulting architectures can be suboptimal for deployment, and post search structured pruning is commonly applied to NAS discovered models to further reduce compute or latency while maintaining accuracy. However, conventional channel or operation level pruning is ill suited to NAS cells since local saliency proxies are unreliable under multi branch interactions and weight sharing, and fine grained removals break cell wise dimensional coupling and trigger cascading realignments. Thus, we propose NR-DARTS, Node Rewiring for Differentiable Architectures with Adaptive SE Fusion, which deletes low importance intermediate nodes scored by learnable gates. Then, the proposed method rewires their predecessors directly to each successor, and compensates at the successor input via a learned linear aggregation followed by channel wise SE recalibration. By preserving cell structure and feature dimensional consistency, our method avoids misalignment issues common in fine grained pruning and achieves reliable performance. On CIFAR-10 dataset, NR-DARTS reduces FLOPs by 27.3\\% from 338.94M to 246.41M while maintaining accuracy at 93.81\\% versus 94.07\\% for the DARTS baseline and it outperforms channel and operation level pruning under matched budgets. Ablation studies further show that adaptive SE fusion improves accuracy at similar FLOPs compared to fixed summation and explain the effectiveness of the compensation mechanism.", "tldr": "", "keywords": ["Neural Architecture Search", "Pruning", "Node Rewiring"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/946ec11b406056b4d94b919267845f88be2031ae.pdf", "supplementary_material": "/attachment/b55e351d2c812af2a291b4e320dc064c1ae1d211.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes NR-DARTS, a post-hoc pruning framework specifically designed for architectures discovered by Differentiable Architecture Search algorithm, i.e., DARTS. The key innovation lies in elevating the pruning granularity from channel/operation level to node level, addressing fundamental limitations of conventional pruning methods, when applied to NAS-discovered multi-branch cell structures. The proposed framework consists of three stages: 1) pruning search using learnable gates to estimate node importance; 2) node pruning and rewiring based on importance scores; 3) retraining with an Adaptive SE-Fusion module to compensate for information loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. In this paper, authors clearly articulate why conventional structured pruning fails for NAS. The issues of unreliable local importance proxies in multi-branch structures and cascading dimensional misalignments are compelling and well-illustrated in Figure 1. Additionally, the proposed NR-DARTS prunes at the node-level rather than channels or operations, which directly addresses the structural misalignment and cascading adjustment problems in multi-branch DARTS-style cells.\n\n2. Table 1 demonstrates that NR-DARTS achieves the best trade-off between accuracy, FLOPs and latency under equal efficiency constraints, outperforming operation/channel pruning and random/reduced-depth baselines. Additional ablation studies and visualisation provide further evidence of the effectiveness of proposed methods, and ability to preserve discriminative feature structure and spatial attention."}, "weaknesses": {"value": "1. The evaluation is restricted to CIFAR-10, which significantly undermines the generalisability claims. Modern NAS research typically validates their methods on CIFAR-10/100, ImageNet, and other challenging datasets. Although authors acknowledge this as a limitation in the conclusion, they did not attempt even modest extensions or discussion of anticipated bottlenecks for practical deployment.\n\n2. The proposed method is only tested on DARTS architectural space, the applicability to other search spaces remains uncertain. The claim of general applicability to 'NAS-discovered architectures' is not substantiated.\n\n3. While latency measurements are provided, the evaluation lacks comprehensive hardware profiling across broader devices, e.g., mobile GPUs. The experiments on a single RTX 4060 are insufficient for a method which targeting 'edge and embedded hardware'.\n\n4. The comparison in this paper is quite limited, lacks comparison with recent efficient NAS methods that jointly optimise for accuracy and efficiency. The conducted experiments are limited to post-hoc pruning methods, which may not represent the best accuracy-efficiency trade-offs.\n\n5. There are some ambiguities in the mathematical definition. For example, although the gate $\\gamma_{k}$ is introduced as a learnable scalar modulating each node’s output, there is no concrete discussion about how gates are trained or how the pruning threshold or hyper-parameters is chosen in practice. Regarding the $x_{\\text{fusion}}$ and $S_{\\text{fusion}}$, the derivation of linear fusion weights followed by the SE recalibration is intuitive, but the exact effect on gradient flow and its robustness is not explored theoretically."}, "questions": {"value": "1. Can authors clarify how the pruning ratio is set and whether any gate regularisation, annealing, or threshold schedules are used for stability? How sensitive is the proposed method to the number of nodes pruned?\n\n2. How does NR-DARTS perform on ImageNet with larger architectures? Does the node-level pruning strategy scale to deeper networks with more complex cell structures?\n\n3. Why do conventional pruning methods show such high latency variation (5.12-6.49 ms) at similar FLOPs?\n\n4. Can authors provide empirical evidence that gate-based importance scores are more reliable than magnitude/gradient-based metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uooexhcHYj", "forum": "HYJLgGQQ33", "replyto": "HYJLgGQQ33", "signatures": ["ICLR.cc/2026/Conference/Submission16841/Reviewer_QWyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16841/Reviewer_QWyX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761122255054, "cdate": 1761122255054, "tmdate": 1762926863024, "mdate": 1762926863024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pruning method to compress the DARTS-searched model. It consists of two components: (1) the node-level pruning approach specifically designed for the discovered architectures, and (2) the Adaptive SE-Fusion module is introduced to weight the contributions of predecessor feature maps and perform channel-wise recalibration, alleviating the accuracy drop."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written\n- The analysis is thorough.\n- The method part has a lot of information."}, "weaknesses": {"value": "- Experiments on large scale datasets (e.g., ImageNet) should be conducted.\n\n- Missing the comparison with other advanced pruning methods, such as [1,2,3,4,5].\n\n- Please further explain the generalization ability of the method. Can it be applied to compress other models?\n\n- Why use the SE module? Other advanced modules (e.g., SENetV2 [6]) may yield more significant benefits.\n\n- Insufficient explanation of symbols. For example, in Eq. (2), $W_1$ and $W_2$ are not fully defined.\n\n    [1] Fang G, Ma X, Mi M B, et al. Isomorphic pruning for vision models[C]. ECCV 2024. \\\n    [2] Fang G, Ma X, Song M, et al. Depgraph: Towards any structural pruning[C]. ICCV 2023. \\\n    [3] Gao S, Zhang Y, Huang F, et al. BilevelPruning: unified dynamic and static channel pruning for convolutional neural networks[C]. CVPR. 2024. \\\n    [4] Zhang H, Liu L, Zhou H, et al. Akecp: Adaptive knowledge extraction from feature maps for fast and efficient channel pruning[C]. ACMMM. 2021. \\\n    [5] Lin M, Ji R, Wang Y, et al. Hrank: Filter pruning using high-rank feature map[C]. CVPR 2020. \\\n    [6] Narayanan M. SENetV2: Aggregated dense layer for channelwise and global representations[J]. arXiv preprint arXiv:2311.10807, 2023."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jaVi0r7cgo", "forum": "HYJLgGQQ33", "replyto": "HYJLgGQQ33", "signatures": ["ICLR.cc/2026/Conference/Submission16841/Reviewer_wXqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16841/Reviewer_wXqD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879330116, "cdate": 1761879330116, "tmdate": 1762926862278, "mdate": 1762926862278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an approach to improve Neural Architecture Search. Rather than pruning naively the cells used in differentiable NAS which is sensitive to their connectivity it addresses this problem by deleting low importance intermediate nodes scored by learnable gates. Then it changes the cell wiring to account for the removal of nodes. By preserving cell structure and feature dimensional consistency, this method avoids misalignment issues common in fine grained pruning and shows reductions of FLOPs while maintaining accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important problem of cell-based NAS methods with regards to naïve pruning and also highlights the opportunity for model compression by focusing on the normal cells. \n- Uses the squeeze-excite approach to adaptively weight the contributions for predecessor channel-wise recalibration to compensate for node pruning.  This approach is simple yet effective.\n- Ablation studies are performed to disentangle the contributions of the two key components comparing against random pruning and alternative fusion."}, "weaknesses": {"value": "- Evaluations only on a single dataset. While CIFAR-10 is a common benchmark it limits the scope of the method and demonstration of its broad effectiveness. \n- Results are only slightly better compared to the state of the art. In some cases, the accuracy difference is less than 1% which may not be statistically significant."}, "questions": {"value": "- Does this mechanism impact the search time or adds to the memory demands? Usually methods also compare the GPU-days."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "flZxlm3OHb", "forum": "HYJLgGQQ33", "replyto": "HYJLgGQQ33", "signatures": ["ICLR.cc/2026/Conference/Submission16841/Reviewer_EHEq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16841/Reviewer_EHEq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907273608, "cdate": 1761907273608, "tmdate": 1762926861621, "mdate": 1762926861621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NR-DARTS is a post-hoc pruning framework for DARTS architectures. It addresses issues in channel- or operation-level pruning by performing node-level pruning with learnable gates, followed by rewiring predecessors to successors and compensating for lost information via an adaptive SE-fusion module. On CIFAR-10, it reduces FLOPs and outperforms several pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The shift to node-level pruning effectively mitigates issues like cascading realignments and unreliable local proxies in multi-branch cells.\n\n2. On CIFAR-10, NR-DARTS achieves a favorable accuracy-efficiency trade-off, reducing FLOPs while maintaining near-baseline accuracy. \n\n3. The paper provides intuitive figures and in-depth ablations, making the contributions easy to follow."}, "weaknesses": {"value": "1. Evaluations are restricted to CIFAR-10 with a small-scale setup (3 seeds). No results on larger datasets like ImageNet or more diverse tasks, limiting evidence of scalability and generalization.\n\n2. While compared to several pruning methods on the same DARTS backbone, it needs more benchmarks against state-of-the-art NAS-pruning hybrids or hardware-aware NAS methods.\n\n3. The pruning ratio is treated as a hyperparameter, but its impact on different architectures or budgets isn't deeply analyzed. Similarly, gate initialization and optimization details could be more robustly tested."}, "questions": {"value": "1. How does NR-DARTS perform on larger-scale datasets like ImageNet or in transfer learning scenarios? Would the node-level pruning and SE-fusion scale effectively to deeper architectures?\n\n2. Can you provide more details on the SE-fusion implementation, such as the reduction ratio in the SE block or how linear weights are initialized/optimized to handle single- vs. multi-branch dominance?\n\n3. How was the pruning ratio selected, and what sensitivity analysis was done for different ratios or node counts? Does it vary across normal vs. reduction cells?\n\n4. Why not integrate the pruning search into the NAS process rather than post-hoc? How does this compare computationally to end-to-end pruning-aware NAS methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dX5JwDs0wB", "forum": "HYJLgGQQ33", "replyto": "HYJLgGQQ33", "signatures": ["ICLR.cc/2026/Conference/Submission16841/Reviewer_vVf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16841/Reviewer_vVf6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921022894, "cdate": 1761921022894, "tmdate": 1762926861218, "mdate": 1762926861218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}