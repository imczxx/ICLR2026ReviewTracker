{"id": "Xn33bU71m4", "number": 18193, "cdate": 1758284926413, "mdate": 1759897120674, "content": {"title": "LLMs as Reverse Engineers? Not Yet on Types and Names", "abstract": "Large Language Models (LLMs) have shown promising potential in reverse engineering tasks such as function name recovery, owing to their ability to generate meaningful identifiers under input conditions. However, existing studies primarily emphasize fine-tuning LLMs for particular applications, often without providing a clear rationale for selecting a given model. To address this gap, we systematically evaluate and quantify the performance of widely used open-source mid-sized LLMs, including CodeLlama, Llama 2, and DeepSeek-R1, on two core reverse engineering tasks: name recovery and type inference. Our experimental results reveal that, without fine-tuning, none of these models achieves a high F1 score in either task. These findings enhance our understanding of the practical utility of LLMs in binary analysis and highlight critical avenues for improving their effectiveness in reverse engineering and related domains.", "tldr": "Systematically and extensively benchmarking LLMs for reverse engineering,especially in name recovery and type inference", "keywords": ["Reverse Engineering", "Name Recovery", "Type Inference", "LLM Benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baad533db43fa765a5db20c0a63e63faf3c52a58.pdf", "supplementary_material": "/attachment/42ea3817c620420c6024f687c4fa5201e1bc7be8.zip"}, "replies": [{"content": {"summary": {"value": "This paper systematically investigates the effectiveness of open-source mid-sized Large Language Models (LLMs) in reverse engineering tasks. While LLMs have shown promise in generating meaningful identifiers for functions, prior research has often focused narrowly on task-specific fine-tuning without justifying model choice. To address this, the authors evaluate CodeLlama, Llama 2, and DeepSeek-R1 across two key tasks—function name recovery and type inference. The results show that, without fine-tuning, these models achieve relatively low F1 scores, revealing their current limitations in handling binary analysis tasks. This study provides a clear benchmark for assessing the baseline capabilities of mid-sized LLMs in reverse engineering and underscores the need for targeted adaptation strategies to improve their performance in this domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Explore recent models."}, "weaknesses": {"value": "What is the distinct contribution of this work compared to prior studies [1,2], which already provide more comprehensive experiments covering both function name recovery and summarization tasks? \n\nThe organization of the paper is also unclear, with many figures and tables placed in the appendix, which negatively impacts readability and overall presentation. \n\nMoreover, the study focuses solely on general-purpose LLMs without examining the many existing fine-tuned LLM-based decompilers. Meanwhile, the findings offer limited insights and practical relevance for reverse engineering practitioners.\n\n[1] Shang, Xiuwei, et al. \"How Far Have We Gone in Binary Code Understanding Using Large Language Models.\" 2024 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2024.\n[2] Shang, Xiuwei, et al. \"An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding.\" arXiv preprint arXiv:2504.21803 (2025)."}, "questions": {"value": "Include additional baselines, datasets, and evaluation tasks to provide a more comprehensive assessments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K557H29HBT", "forum": "Xn33bU71m4", "replyto": "Xn33bU71m4", "signatures": ["ICLR.cc/2026/Conference/Submission18193/Reviewer_TNfc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18193/Reviewer_TNfc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363737234, "cdate": 1761363737234, "tmdate": 1762927941181, "mdate": 1762927941181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a benchmark that evaluates (mid-sized) LLMs' capabilities on type inference and name recovery from stripped binaries (of C code). The evaluation pipeline is designed to automatically collect LLM's response into structured format (using an auxiliary LLM to format the collected response) and then run corresponding metrics for name recovery (semantic-based similarity) and type inference (exact match). Results show that none of the 9 mid-sized coding LLMs tested are useful for these two tasks. In total, 8 findings were reported, discussing the effects of fine-tuning, model size, training data, llm output formatting, etc."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "S1: this work identifies mid-sized LLMs inability to solve the tasks of name recovery and type inference when given assembly and binaries. \n\nS2: experiments were large-scale, tested on 9 different mid-size LLMs.\n\nS3: this work provides some preliminary insights into how model size, fine-tuning, training data affect the performance of mid-sized LLMs on the two reverse engineering tasks."}, "weaknesses": {"value": "W1: generally, everything is over-claimed. The author should make the claims specific to the target domain, as well as draw valid conclusions that can be supported by the evaluation results.\n\nW1.1: overstated problem domain. This work only assessed mid-size LLMs on two tasks, however, both in the title and in various places in the writing it is mentioned that the reverse-engineering capability is tested. The authors should justify why and to what degree these two tasks are representative of all other reverse-engineering tasks.\n\nW1.2: mid-sized LLMs are not representative of SOTA LLMs. As the evaluations are only done on mid-sized LLMs, **ALL** claims should be stated with respect to mid-sized LLMs, not extending to production level LLMs such as the ones used in coding agents. Although one of the experiment shows scaling from 7B to 34B of codellama does not bring significant improvement, it is not convincing enough if not even one of the SOTA models was tested, especially when all tested models are performing so poorly, and the scale on Figure 2 is tiny in absolute value so there does not seem to have any significant difference between the models with different sizes.\n\nW2: motivation is not clear. The writing claims that there is a gap of under-justified choices of LLMs in the literature and that the proposed benchmark aims to improve this gap. However, it is not clear to the reader how this benchmark (which in an essence reveals LLMs' inability toward the two tasks) helps user choose an LLM for their coding task. In addition, it is not clear to the user why LLMs' ability in inferring types and variable names in assembly and binaries is helpful for end users. The author should motivate the readers a bit more on the significance of this evaluation.\n\nW3: in the design of the automated pipeline, an auxiliary LLM is involved to clean the output of testing LLMs into structured responses. This design choice can be very problematic. First of all, it is not mentioned which LLM is used to do this task (which needs justification of the choice as well). Second, the auxiliary LLM might hallucinate, or not following instructions, so its output can be highly unstable or untrustworthy. The quality of this benchmark will then heavily depend on the capability of this auxiliary LLM, which is not desirable.\n\nW4: poor writing qualities. Since these are minor, please refer to questions and suggestions for the details."}, "questions": {"value": "Detailed questions and suggestions are as follows:\n\nQ1: Line 73, any prior works that have tested llm type inference? Is there any reference on why types are measured by exact match? Any other metrics that can measure types (as the authors pointed out, the poor performance might be due to the exact match being too strict)?\n\nQ2: Line 102-105, these related works are mentioned but not discussed at all. How are they related to the current work? What differentiates this work from them?\n\nQ3: with a nearly zero result for all tested models, have the authors consider adjusting metrics (as mentioned before, maybe exact match is too strict) such that more insightful findings could be drawn to distinguish the capabilities?\n\n#------------------#\n\nSugg1: all claims should be made on mid-sized llms instead of \"current llms\" as there is no evaluation on current llms (that are used by coding agents).\n\nSugg2: Line 114-121 seem to be very repetitive as these points were already discussed in the introduction. More detailed information/discussion should be included to illustrate the motivation.\n\nSugg3: Figure 3 and Figure 4 were mentioned in main text, however they are both in the appendix. Either move them into main text, or mention that they are in the appendix in the main text.\n\nSugg4: the citation of Jiang et. al. is missing complete information (publication year) Line 522.\n\nSugg5: line 260 - 267, the reader does not find a list of resulting figures/tables to be helpful for understanding the results and finding. Instead, each figure/table should be mentioned and referred in text within the respective discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D1c1gB0nvi", "forum": "Xn33bU71m4", "replyto": "Xn33bU71m4", "signatures": ["ICLR.cc/2026/Conference/Submission18193/Reviewer_dQcM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18193/Reviewer_dQcM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694931444, "cdate": 1761694931444, "tmdate": 1762927940791, "mdate": 1762927940791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates several open-source mid-sized LLMs on reverse engineering tasks such as function name recovery and type inference from stripped binaries. The authors present an automated benchmarking framework that standardizes heterogeneous model outputs via an auxiliary LLM for post-processing. Experiments compare nine models under various architectures and compiler optimization levels, before and after LoRA fine-tuning. Results show consistently low F1 scores (typically < 0.1), with fine-tuning offering inconsistent or marginal improvements. The authors conclude that current LLMs remain too limited for reverse engineering."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors implement an automated pipeline that allows easy reproducibility and makes it easy to apply the setup to other models of interest.\n- The dataset spans multiple architectures and compiler optimizations.\n- The idea to provide a guideline on model selection for future experiments seems reasonable and useful. (But the paper does not sufficiently implement and show this.)"}, "weaknesses": {"value": "While the experimental procedure appears systematic, the study suffers from several methodological weaknesses. \n\nMany larger models are excluded solely due to hardware or cost constraints, limiting the generalizability and diminishes the practical utility of the paper as a guide for model selection on reverse engineering tasks. \n\nImportant methodological details are missing from the main text. Most notably, a precise description of how F1 is computed for function name recovery and type inference, despite F1 being the central metric in most experiments. A clearer debate on the metric under investigation would make it easier interpreting the results better. \n\nIn the appendix, the authors state that evaluation depends to some extend on the random behaviour of the models. However, the evaluation metrics, particularly the F1 scores, are reported without details regarding confidence intervals. Furthermore, the evaluation relies solely on F1 scores, without considering a greater variety of metrics that could provide a more nuanced picture of model behaviour and error types. However, they raise awareness of that shortcoming in finding 2.\n\nMoreover, several key design choices such as the prompting strategy, or the auxiliary LLM’s role in post-processing are insufficiently explained. For example, the paper mentions modifications to the prompt design without describing what was changed, which obscures the authors reasoning. \n\nAnother limitation lies in the narrow methodological scope. The authors only employ LoRA-based fine-tuning to improve model performance, without exploring alternative knowledge-editing techniques. Given the limited and inconsistent success of LoRA in their setup, the study would have benefited from comparing different methods. \n\nNegative results are over-interpreted (See e.g. the debate around finding 4, or finding 6), e.g. leading the authors to draw disproportionately strong conclusions that current models cannot perform reverse-engineering tasks, rather than acknowledging possible shortcomings in their setup. Given the strictness of evaluation (e.g., exact type matches and sematic distance in naming) it is unclear whether poor performance reflects model incapability or inadequate task framing. \n\nFurthermore, the findings come without offering actionable hypotheses for failure modes. (See e.g. the debate around finding 1, or finding 3.) \nThe manuscript is hard to follow and flows poorly; many sections read like disconnected bullet points rather than a cohesive narrative. \nThe related work section seems unbalanced. A few papers are discussed at some length, while many other relevant works are merely listed without sufficient depth. \n\nWhile several external methods are mentioned, their descriptions remain mostly vague. Further studies such as Function-to-Style Guidance of LLMs for Code Translation (Zhang et al., 2025) could have provided further insights into how stylistic and structural cues enhance code understanding, which could be relevant to reverse engineering.\n\nTables 2–6 are not referenced at the points in the text where their results are discussed, and helpful visual aids (e.g., color coding to highlight results discussed by the authors) are absent. Figures are also consistently too small to read comfortably, which makes it harder to verify the arguments made around them. \n\nFinally, from line 231 onwards citations stop abruptly mid-paper, and several referenced works are left without proper citation."}, "questions": {"value": "- How was the auxiliary LLM used for normalization trained and validated? Could it bias the evaluation?\n- Can the authors clarify how F1 scores were computed?\n- What are the main sources of failure? \n- Could you provide more qualitative examples or an error analysis to shed light on model limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "os8Bbk81YO", "forum": "Xn33bU71m4", "replyto": "Xn33bU71m4", "signatures": ["ICLR.cc/2026/Conference/Submission18193/Reviewer_LjgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18193/Reviewer_LjgB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909511012, "cdate": 1761909511012, "tmdate": 1762927940370, "mdate": 1762927940370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}