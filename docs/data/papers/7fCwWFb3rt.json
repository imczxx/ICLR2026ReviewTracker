{"id": "7fCwWFb3rt", "number": 5816, "cdate": 1757937095724, "mdate": 1759897951671, "content": {"title": "Search-T2I: Internet-Augmented Text-to-Image Generation", "abstract": "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation framework (Search-T2I) to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by ∼30\\% in human evaluation.", "tldr": "Search-T2I, the first framework that integrates reference images from the Internet into T2I models, effectively mitigating inaccurate image generation caused by uncertain knowledge in text prompt.", "keywords": ["Internet-augmented generation", "text-to-image generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ad2ceb3f0ce9fd23efed0a367a96ac147ff2bce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This summary introduces Search-T2I, a framework designed to improve text-to-image models to generate state-of-the-art images by retrieval augmentation. The system uses an internet search to find relevant reference images, actively decides when they are needed, selects the most suitable ones, and employs a self-reflection mechanism to refine the output. The framework is tested on a dedicated dataset with various uncertainty types, and significantly outperforms a model like GPT-4o in human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper introduces a internet-augmented framework that actively retrieves and utilizes reference images, moving beyond standard T2I models to dynamically resolve knowledge uncertainty.\n2. The framework demonstrates superior performance, significantly outperforming strong baselines like GPT-4o by a large margin in human evaluation.\n3. The paper is clearly structured, logically explaining the problem and systematically introducing each component of the solution, which makes the framework easy to understand."}, "weaknesses": {"value": "1. This work is based on prompting, and the focus should be on pipeline design. However, the overall pipeline of this paper is trivial, where all modules, including the query reformulation module, retrieval module, reflection module, etc., have been widely used in previous works such as [1]. This significantly diminishes the paper's innovation.  \n2. There is a lack of comparison with existing retrieval-augmented image generation works, such as [1][2][3][4], making it impossible to assess the effectiveness of the proposed framework.  \n\n[1]. Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng Dou. Finerag: Fine-grained retrieval-augmented text-to-image generation.  \n[2]. Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented text-to-image generator.  \n[3]. Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented diffusion models.  \n[4]. Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, Ohad Fried. ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation."}, "questions": {"value": "Same to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iS6bIyv4zI", "forum": "7fCwWFb3rt", "replyto": "7fCwWFb3rt", "signatures": ["ICLR.cc/2026/Conference/Submission5816/Reviewer_pE1r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5816/Reviewer_pE1r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661752579, "cdate": 1761661752579, "tmdate": 1762918279347, "mdate": 1762918279347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The papaer propose an Internet-Augmented text-to-image generation framework (Search-T2I) to compel T2I models clear about such uncertain knowledge by providing them with reference images. Experiments prove the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Pros:\n1. The idea is interesting, using internet to augment exisiting generation model.\n2. Compared against GPT-4o, the performance is good."}, "weaknesses": {"value": "1. I have come across a paper in Arxiv: IA-T2I: Internet-Augmented Text-to-Image Generation (Chuanhao Li, et, al.); which has some overlap with this submission, while different titles and some details. Is this the same paper? If not, I do not find any citations.\n\n2. The citations should be regulated. For instance, FLUX Labs (2024). What labs? You should specify BFL.\n\n3. There is no ethnic approvement for using human sources to evaluate. Does they work fairly? Is there any forced labor happens?\n\n4. The collected dataset Img-Ref-T2I is not public available.\n\n5. The proposed Search-T2I framework is a combination of existing works. No novel."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "1. The paper uses Internet data without specific web crawling policies\n2. The paper does not mention annotator compensation, which may cause forced labor.\n3. The paper has significant overlap with a paper on Arxiv with a different title."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fj9gNSN7eP", "forum": "7fCwWFb3rt", "replyto": "7fCwWFb3rt", "signatures": ["ICLR.cc/2026/Conference/Submission5816/Reviewer_LVS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5816/Reviewer_LVS4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720141941, "cdate": 1761720141941, "tmdate": 1762918279032, "mdate": 1762918279032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a training-free framework that actively retrieves and hierarchically filters reference images from the internet to augment text-to-image (T2I) and text-driven image editing (TI2I) models on-the-fly, mitigating generation errors caused by uncertain knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. From the perspective of design motivation, the paper's focus on a training-free framework, which  demonstrates notable advantages in both T2I and TI2I applications, offering valuable insights for future research.\n\n2. By extending the IAG techniques traditionally used in LLMs to the image-generation domain and providing a well-designed diversity-based selection strategy, the work represents an effective and welcome technological expansion."}, "weaknesses": {"value": "- Section 3.6: The section say “three key criteria” but lists four; criteria (2) and (3) appear to overlap. Table 4 further claims that criteria (1)(2)(3) were used, yet the correct set should be (1)(2)(4). Please verify whether (2) and (3) were originally intended as a single criteria.\n\n- Section 5.2: The evaluation protocol does not detail reliability or bias-mitigation steps. Clarify how personal preference differences of the (co-author) evaluators influent the evaluation and how the influence were controlled.\n\n- Table 2 in Section 5.2: Removing the Active Retrieval (AR) module still improves scores, even outperforming the full framework. Discuss why retrieved information (AR + SR) degrades performance versus self-reflection alone; a brief ablation or error analysis is expected.\n\n- Table 5: The distinctions between prompts are relegated to the appendix, leaving the table itself uninformative. Add a concise in-text summary of how the prompts differ.\n\n- Table 6: Google April data are missing, precluding a clear time-comparison; the text can only assert that multi-engine retrieval helps. Moreover, July data (later and should be more abundant) yield lower scores than April—an unexpected trend. Provide a brief analysis for why performance drops with more information or what difference between multi-engine retrieval and later retrieval."}, "questions": {"value": "Please see weakness for my questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tBU4jV47oV", "forum": "7fCwWFb3rt", "replyto": "7fCwWFb3rt", "signatures": ["ICLR.cc/2026/Conference/Submission5816/Reviewer_Mjnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5816/Reviewer_Mjnz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826610767, "cdate": 1761826610767, "tmdate": 1762918278811, "mdate": 1762918278811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents IA-T2I, an Internet-augmented text-to-image generation framework that addresses the limitation of T2I models when encountering text prompts containing uncertain knowledge (rare, unknown, or ambiguous information). The framework incorporates an active retrieval module to determine when reference images are needed, a hierarchical image selection process that first clusters retrieved images for diversity and then re-ranks candidates via large vision-language models, and a self-reflection mechanism that iteratively evaluates generated images against the prompt, reference image utility, and previous outputs to decide whether to accept or regenerate. To evaluate the framework, the authors construct the Img-Ref-T2I dataset comprising T2I and TI2I tasks. The author claims integrating IA-T2I into GPT-4o and Gemini can outperforming raw GPT-4o by ~30 % in human evaluation, while ablations confirm the complementary contributions of diversity selection and self-reflection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. IA-T2I innovatively introduces a proactive retrieval module to determine when a reference image is needed and designs a two-level selection strategy of \"diversity clustering + re-ranking\" to effectively filter out a large number of noisy images returned by the search engine, providing the T2I model with the most instructive single reference image and significantly improving generation accuracy in scenarios with uncertain knowledge.\n\n2. Both human and GPT-4o evaluation are integrated in this loop."}, "weaknesses": {"value": "1. The self-reflection loop is presented as a principal contribution, yet **iterative generate-evaluate-regenerate pipelines is common standard** in both LLM and AIGC literature since early 2024; the paper neither positions the proposed mechanism against prior reflection frameworks (e.g., Self-Refine, Iterative Prompting) nor quantifies its marginal gain over single-shot or alternative reflective baselines, rendering the claimed novelty unsubstantiated.\n\n2. The experimental evidence relies almost exclusively on manual ratings provided by humans; objective metrics commonly adopted in T2I evaluation—CLIP-Score, FID, DSG, etc.—are absent, and inter-annotator agreement is not reported, leaving the results vulnerable to confirmation bias and statistical insignificance.\n\n3. **The adoption of GPT-4o as an automatic preference evaluator is elevated to a core contribution; however, LLM-as-a-judge is already a widespread practice in generative vision research.** The authors offer no correlation analysis (Pearson/Spearman), significance tests, or comparisons with existing learned metrics (HPSv2, ImageReward, PickScore), overstating the methodological advance.\n\n4. Img-Ref-T2I comprises **only 240 human-curated instances**; this scale is insufficient to stabilise performance estimates, and no power analysis is supplied to justify that the dataset can reliably detect the hypothesised effect size.\n\n5. Prompts in the dataset average fewer than 20 words and exhibit limited compositional complexity; consequently, model scores cluster narrowly, failing to expose differences under the long-form, multi-attribute, or combinatorial instructions emphasised by contemporary benchmarks such as DrawBench or T2I-CompBench.\n\n6. The baseline pool omits several competitive T2I systems (e.g., Hunyuan, Doubao) that may exhibit distinct behaviours under the proposed retrieval-augmented regime, thereby constraining the external validity of the comparative findings.\n\n7. **The paper asserts “high consistency” between GPT-4o-based automatic ratings and human preferences, yet provides no quantitative corroboration**—neither correlation coefficients nor error breakdowns. Its strongly recommended to read some established protocols like **T2I-CompBench, T2V-CompBench or V-Bench** that mandate Pearson r ≥ 0.8 and detailed failure-mode analysis; without this verification, all subsequent claims of performance improvement rest on an unvalidated evaluator and remain scientifically fragile."}, "questions": {"value": "I believe this paper needs a complete overhaul, particularly regarding the weaknesses section. Compared to ImageReward and Pick-a-Pic, which are two years old, it offers little advantage, let alone compared to more recent papers. Furthermore, the dataset is far too small; no AIGC dataset has ever been this small. Additionally, the authors should have avoided exaggerating their contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8f8KH6zN3S", "forum": "7fCwWFb3rt", "replyto": "7fCwWFb3rt", "signatures": ["ICLR.cc/2026/Conference/Submission5816/Reviewer_DuHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5816/Reviewer_DuHG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989652262, "cdate": 1761989652262, "tmdate": 1762918278472, "mdate": 1762918278472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}