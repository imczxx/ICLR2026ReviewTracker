{"id": "7s68UdT65x", "number": 2295, "cdate": 1757052895989, "mdate": 1759898157351, "content": {"title": "Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "abstract": "Current auto-regressive models can generate high-quality, topologically precise meshes; however, they necessitate thousands—or even tens of thousands—of next-token predictions during inference, resulting in substantial latency. We introduce XSpecMesh, a quality-preserving acceleration method for auto-regressive mesh generation models. XSpecMesh employs a lightweight, multi-head speculative decoding scheme to predict multiple tokens in parallel within a single forward pass, thereby accelerating inference. We further propose a verification and resampling strategy: the backbone model verifies each predicted token and resamples any tokens that do not meet the quality criteria. In addition, we propose a distillation strategy that trains the lightweight decoding heads by distilling from the backbone model, encouraging their prediction distributions to align and improving the success rate of speculative predictions. Extensive experiments demonstrate that our method achieves a  speedup without sacrificing generation quality. Our code will be released.", "tldr": "Lossless Acceleration of Auto-Regressive Mesh Generation via Multi-Head Speculative Decoding", "keywords": ["Mesh Generation", "Acceleration", "Auto-Regressive Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14eee90760ad5968992166894009dbb9d125541d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces XSpecMesh, a method to accelerate the inference speed of auto-regressive mesh generation models. The core problem addressed is the high latency caused by the sequential, token-by-token generation process. The proposed solution adapts the concept of speculative decoding from large language models. Instead of a separate draft model, XSpecMesh attaches multiple lightweight, cross-attention decoding heads to the main transformer backbone. In a single forward pass, these heads predict a sequence of candidate tokens in parallel. The main model then verifies these tokens, accepting a prefix of correct predictions and resampling from the first point of failure. The method is applied to the BPT model and demonstrates a ~1.7x speedup while preserving the original model's generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper successfully achieves its primary goal of speeding up inference for auto-regressive mesh generation. The reported 1.7x speedup is significant from a practical standpoint, and the empirical results convincingly show that this is achieved with a negligible drop in geometric quality compared to the base model.\n2. The proposed multi-head architecture is a clever adaptation of speculative decoding that avoids the need to train and maintain a separate, smaller draft model. The technical execution, including the use of distillation, LoRA fine-tuning to align distributions, and a clear verification strategy, is sound and well-justified.\n3. The authors provide a comprehensive set of ablations that analyze the impact of key design choices, such as the number of decoding heads, the type of decoder, and the acceptance criteria. This adds credibility to their results and provides valuable insights for future work in this specific direction."}, "weaknesses": {"value": "1. The primary weakness of this work lies in its limited conceptual contribution. Speculative decoding is a well-established and widely known technique for accelerating inference in large language models. This paper presents a straightforward application of this existing \"trick\" to the domain of mesh generation. While the engineering adaptation is non-trivial, it does not introduce a new fundamental concept or paradigm. The contribution is more of an engineering speed-up than a core research advance, making it a questionable fit for a top-tier conference like ICLR that values foundational contributions.\n2. The most critical and challenging research problem in generative 3D modeling today is the quality of the output—achieving correct topology, plausible geometric detail, global coherence, and semantic consistency. This paper explicitly sidesteps this primary challenge. By design, its quality ceiling is strictly limited by the performance of the base model (BPT). The paper even acknowledges that it inherits all the failure modes of BPT (Appendix A.9, A.10). Investing research effort into accelerating a potentially flawed or quality-limited generator seems to misalign with the community's most pressing needs.\n3. While a 1.7x speedup is a welcome improvement, it is ultimately an incremental gain. It does not fundamentally alter the capabilities of auto-regressive mesh models or enable new applications that were previously out of reach. It makes a slow process slightly less slow, but does not represent a breakthrough that would significantly change the research landscape or practical workflows."}, "questions": {"value": "1. The core contribution is applying a known LLM acceleration technique to meshes. Could the authors elaborate on what makes this adaptation uniquely challenging or insightful in the 3D domain, beyond the architectural changes? What fundamental principle does this work teach us about 3D generation itself, rather than just about inference optimization?\n2. Given that generation quality is the main bottleneck for the field, how do the authors justify the importance of this work for ICLR? Why is accelerating a model of a certain quality level a more critical contribution at this stage than proposing a new method that could generate higher-quality meshes, even if slower?\n3. The performance of XSpecMesh is capped by the base BPT model. Have the authors considered applying their method to a different, potentially higher-quality (even if experimental or slower) auto-regressive backbone? Is the technique general enough to apply to any such model, and would the speedup gains be consistent?\n4. The introduction of D decoding heads and LoRA adds complexity and parameters during the training phase. Could you provide a more detailed analysis of the trade-offs, including training time, memory overhead, and implementation complexity, versus the achieved 1.7x inference speedup?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YWBCijNiGs", "forum": "7s68UdT65x", "replyto": "7s68UdT65x", "signatures": ["ICLR.cc/2026/Conference/Submission2295/Reviewer_HMRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2295/Reviewer_HMRD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760785435541, "cdate": 1760785435541, "tmdate": 1762916181004, "mdate": 1762916181004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents XSpecMesh, a framework designed to accelerate auto-regressive mesh generation models while maintaining the quality of the generated 3D meshes. It addresses the high inference latency of existing auto-regressive models, which require thousands of sequential next-token predictions. This is done by employing multiple cross-attention speculative decoding heads for multi-token prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core achievement is the reported 1.7x acceleration in generation speed without sacrificing output quality. The adaptation of using lora to reduce the performance gap is reasonable. \n\nThe paper is clear in defining the problem. The high latency of auto-regressive mesh generation, stemming from the thousands of required forward passes, is introduced as the primary motivation.\n\nparallel prediction, verification/resampling, and LoRA integration are easy to follow.\n\nThe paper has a sensitivity analysis for the critical hyperparameters: the number of speculative heads ($D$) and the verification threshold ($\\delta$)."}, "weaknesses": {"value": "While the method avoids truncated training, long sequences lead to massive KV cache usage. Instead, it pointed out that truncated training is a drawback because it loses the context. Then, the work could not achieve long token sequences that the truncated training is addressing. The mesh polycount might be limited.\n\n\nThe 1.7 times speed up actually might not be much."}, "questions": {"value": "What is your maximum polycount?\n\nCould you provide somewhere for testing the practical performance? thanks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cJ9Bw9rXmW", "forum": "7s68UdT65x", "replyto": "7s68UdT65x", "signatures": ["ICLR.cc/2026/Conference/Submission2295/Reviewer_QJVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2295/Reviewer_QJVU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931936950, "cdate": 1761931936950, "tmdate": 1762916180875, "mdate": 1762916180875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel method, XSpecMesh, aimed at accelerating the inference process of auto-regressive 3D mesh generation models while preserving generation quality. Diverging from traditional methods that rely on single next-token prediction, XSpecMesh employs a multi-head speculative decoding scheme. The core idea is to utilize multiple lightweight decoding heads to predict several future tokens in parallel within a single forward pass. Furthermore, the authors have designed a verification and resampling strategy that leverages the backbone model to validate these predicted candidate tokens and resample any that do not meet the quality criteria. To improve the success rate of speculative predictions, the method incorporates a distillation strategy and LoRA fine-tuning to train the decoding heads, aligning their prediction distributions with that of the backbone model. Experimental results demonstrate that this method achieves approximately a 1.7× speedup over the baseline model (BPT) without sacrificing generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1、Significant Speedup While Maintaining Quality:** The most significant contribution of this paper is the achievement of a ~1.7× inference speedup, which is a substantial improvement for time-consuming auto-regressive generation tasks. More importantly, this acceleration is accomplished with almost no loss in generation quality, as demonstrated by objective metrics like Chamfer Distance (CD) and Hausdorff Distance (HD), as well as the user study. This proves the method's effectiveness and practical value.\n\n**2、Efficient and Novel Framework Design:** Unlike common speculative decoding schemes in Large Language Models (LLMs) that require a separate \"draft model,\" the multi-head decoding approach proposed here is more lightweight and efficient. It avoids the complexity and overhead of training and maintaining an additional model. The clever integration of LoRA fine-tuning and knowledge distillation not only addresses the challenge of predicting multiple tokens directly from intermediate hidden states but also effectively aligns the decoding heads with the backbone model. This design is highly sophisticated.\n\n**3、Thorough Experimental Validation:** The authors have conducted comprehensive experiments to validate their method's effectiveness. The ablation studies (Table 3), in particular, are well-designed and clearly demonstrate the contributions of key components, such as the cross-attention decoders and LoRA fine-tuning, to the final performance (in terms of both quality and speedup). Additionally, the analysis of hyperparameters like the number of decoding heads (D) and the acceptance threshold (δ) provides valuable insights for the application and optimization of the method."}, "weaknesses": {"value": "**1、Limited Acceleration Factor:** Although a 1.7× speedup is valuable in the context of mesh generation, this figure appears conservative when compared to speculative decoding techniques in the LLM domain, where speedups of 3-4× or even higher are common. The paper would benefit from a deeper discussion on the bottlenecks that prevent a higher acceleration factor in this task. For instance, is it due to stronger dependencies in mesh sequences, lower entropy in the token distribution, or the computational overhead introduced by the multi-head architecture?\n\n**2、Limited Novelty:** The core ideas of the paper (multi-head prediction, verification, distillation) are heavily inspired by mature techniques from the LLM acceleration field. While successfully applying these concepts to 3D mesh generation is a valuable contribution, from an algorithmic perspective, the work appears more like a successful \"A+B\" application rather than a fundamentally new theoretical framework. The paper needs to better articulate the unique challenges of the mesh generation task and why these challenges necessitate specific adaptations to existing speculative decoding frameworks to highlight its originality.\n\n**3、Performance Ceiling is Limited by the Base Model:** The proposed method is essentially an \"accelerator,\" and its upper bound for generation quality is entirely determined by the chosen base model (BPT in this case). As shown in the appendix (Figure 10, Visualization of failure cases), XSpecMesh inherits all the generation deficiencies of the BPT model. This means the method can make a good model faster, but it cannot fix or improve the model's inherent flaws. This limitation should be more explicitly discussed in the main body of the paper."}, "questions": {"value": "1、According to Figure 5(a), the speedup plateaus and even slightly decreases when the number of decoding heads (D) exceeds 4. What do the authors believe is the main bottleneck causing this phenomenon? Is it that the acceptance rate of candidate tokens has reached its limit, or does the computational overhead from the additional decoding heads begin to outweigh the benefits of parallel prediction?\n\n2、The statement of a 1.7× speedup in this paper lacks sufficient rigor. It is recommended to include the average time required to generate 100 face to more accurately demonstrate the claimed acceleration.\n\n3、LoRA fine-tuning plays a crucial role in your method. Could the authors provide more analysis or intuition on how LoRA specifically helps the model predict multiple future tokens? For example, does LoRA primarily alter the information representation of the Transformer's final hidden state (hs) to better encapsulate sequence information for multiple future tokens?\n\n4、Table 1 indicates that the vocabulary size for mesh generation models is considerably smaller than for language models. How does this characteristic impact the design of speculative decoding? Does it make a \"multi-head\" approach more feasible than a \"draft model\" approach? Conversely, does this small vocabulary introduce other unique challenges for improving the token acceptance rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gl82PeWung", "forum": "7s68UdT65x", "replyto": "7s68UdT65x", "signatures": ["ICLR.cc/2026/Conference/Submission2295/Reviewer_rZkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2295/Reviewer_rZkX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939203916, "cdate": 1761939203916, "tmdate": 1762916180723, "mdate": 1762916180723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}