{"id": "1uXcaSmmUj", "number": 19161, "cdate": 1758294003358, "mdate": 1763743958659, "content": {"title": "Spiking Neuron as Discrete Gating for Long-Term Memory Tasks", "abstract": "Efficient long-term memory is important for improving the sample efficiency of Partially Observable Reinforcement Learning. In memory-based RL methods, the long-term memory capacity relies on the sequence models used in agent architecture. Two main approaches improve long-term dependency for sequence models, using linear recurrence and using information selection mechanisms such as gating. However, the sample efficiency of existing approaches remains low in long-term memory tasks. In this paper, we first present a saliency-based framework to illustrate why existing methods do not perform well on long-term memory tasks. Specifically, they cannot effectively filter out noisy information irrelevant to the memory task in the early stage of training. To this end, we design a novel linear recurrent module, in which the gating is controlled by spiking neurons. Spiking neurons output discrete values and can more effectively mask noise in the early stages of training, thus improving sample efficiency. The effectiveness of our proposed module is demonstrated on Passive Visual Match, a classic long-term memory task, and several different types of partially observable tasks. The code is attached in the supplementary material and will be made publicly available.", "tldr": "", "keywords": ["Spiking Neural Networks", "Sequence Models", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07b0ddd1dcaad9ecf5ec8247793839a005b67636.pdf", "supplementary_material": "/attachment/9be670246935a2e99aff8095d57f8ec480f06134.zip"}, "replies": [{"content": {"summary": {"value": "This study focuses on gated neural memory models, noting that the nature of the gate of such models strongly influences the effective time-duration over which a model can learn. Specifically, with sigmoidal gates, true \"gating\" is nearly impossible and requires very large gating-weights, which hinders learning. Instead, the authors propose to use spiking neuron models combined with surrogate gradient learning to learn discrete gating in memory neuron models. The effectiveness of this approach is demonstrated on the Passive Visual Match (PVM) task and the POPGym. The approach excels on the PVM task compared to other gated networks, especially for longer durations, and also on the RepeatPrevious memory task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The notion of learning discrete gating using surrogate gradients is novel and promising as far as I know. In the shown example, the presented approach excels exactly where the problem of noisy interference in a task is clear."}, "weaknesses": {"value": "The writing would benefit from a more concise writeup of the introduction and clarity of the aims, as well as a more elaborate writeup of the tasks and results. This would also allow some of the other results, like short-term memory, to be included in the main text.\n\nIn particular, while the approach does indeed work well for the specific scenario it was designed for, inputs followed by noisy delays followed by a relevant output phase, the performance on general memory tasks is not convincing compared to the noted SHM approach in Table 3, while the presented method seems to perform similar to GRU, LiT and FFM for harder versions of the tasks . It would be informative to determine what makes SHM better. \n\nGated-memory networks cover a large part of deep learning with many other tasks besides those shown. I would have like to see other memory tasks as well, for example  like the 1-2AX, saccade-anti-saccade tasks, the original \"add\" task."}, "questions": {"value": "Figure 1 seems to suggest that the approach is mostly focused on a very specific trial type: a signal, noise, and then a go signal/output phase. Does the approach generalize?\n\nIn the description of the spiking neuron, I don't see a reset. Is there a reset? If not, what is the effect?\n\nCan you provide more insight into when the gating helps and when not, compared to other methods? \n\nHow does the proposed method perform on other long-term memory tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8CpkjyF5Ye", "forum": "1uXcaSmmUj", "replyto": "1uXcaSmmUj", "signatures": ["ICLR.cc/2026/Conference/Submission19161/Reviewer_oAXs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19161/Reviewer_oAXs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645389142, "cdate": 1761645389142, "tmdate": 1762931171413, "mdate": 1762931171413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nThank you for your insightful comments and constructive suggestions. We have thoroughly revised our manuscript to address your feedback.\n\nBelow is a **summary** of the key changes made:\n\n---\n\n### 1. **Highlighted Contribution**\n1. Narrowed down the problem definition. We now clearly differentiate between **memory maintenance** and **memory update**, underscoring that our work specifically targets **long-term memory maintenance**. The practical importance of this challenge is further emphasized in the introduction and Section 3.\n2. Reorganized Sention 4 to emphasize the analysis of discrete gating for memory maintenance (Section 4.1) and its implementation (Section 4.2), while reducing the focus on the design of the recurrent cell.\n\n---\n\n### 2. **Improved Paper Structure and Readability**\n1. Moved background on POMDPs (previously in Section 2) and detailed cell implementations (previously in Section 4) to the **Appendix**, allowing greater focus on our core contributions.\n2. Simplified section titles and split multi-part figures (e.g., Figure 1) for better clarity.\n3. Reorganized all figures and tables in Section 5 to enhance visual flow and comprehension.\n\n---\n\n### 3. **Enhanced Analysis of Method Performance**\n1. Clearly outlined task types where our method excels (e.g., Passive Visual Match, Repeat Previous) versus those requiring stronger memory update capabilities (e.g., general and short-term memory task).\n2. Added **Table 1** to compare memory modeling complexity across methods, explaining that our focus on maintenance leads to simpler update mechanisms.\n3. Provided targeted result analysis in Section 5, linking performance differences to memory maintenance efficiency.\n4. Discuss our method's limitation of memory update (Appendix G).\n\n---\n\n### 4. **Expanded Experimental Validation**\n1. Added comparisons with **Mamba**, revealing its limitations in long-term memory settings.\n2. Conducted **ablation studies** to verify the roles of discrete gating, surrogate gradients, and stochastic thresholds.\n3. Included results on the **12-AX** and **T-Maze** tasks, demonstrating broader applicability.\n4. Provided **time complexity and memory consumption analysis** in the Appendix, confirming comparable efficiency to linear RNNs with a modest constant-factor overhead.\n\n---\n\n### 5. **Clarified Key Design Choices**\n1. Justified the **absence of a reset mechanism** to preserve historical context and support parallel scanning.\n2. Clarified that **discrete gating improves memory maintenance** by enabling strong noise filtering without gradient vanishing.\n\n---\n\nWe believe these revisions have substantially strengthened the paper. We are open to any further suggestions and appreciate your valuable time and guidance.\n\nSincerely,  \nThe Authors"}}, "id": "67M9sSXur2", "forum": "1uXcaSmmUj", "replyto": "1uXcaSmmUj", "signatures": ["ICLR.cc/2026/Conference/Submission19161/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19161/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19161/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744073803, "cdate": 1763744073803, "tmdate": 1763744073803, "mdate": 1763744073803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors suggest that noisy distractor observations greatly reduce the efficiency of memory models. To mitigate this issue, they propose using memory with discrete (spiking) neurons.\n\nFirst, the authors introduce a signal-to-noise analysis framework for memory. It uses a long-term task with known credit assignment, and is relatively straightforward. It takes the gradient with respect to the observations of the correct credit assignment divided by the total credit assignment.\n\nNext, they perform a theoretical analysis of SNR for linear and nonlinear RNNs, and then gated and non-gated RNNs. They focus on the effect of SNR and the gradient for both cases. Importantly, they prove that gating enables an SNR of 1, which is not possible with a non-gated recurrent update. \n\nThe authors go on to motivate a **discrete** and associative gating mechanism called the parallel spiking neuron. It uses a heaviside step gating mechanism surrogate gradient to enable backpropagation. They add stochasticity to mitigate an issue where a neuron can be stuck at 0, preventing gradient flow back through the RNN. They use this mechanism to construct a linear recurrent cell.\n\nThe authors calculate the SNR for their model with both soft and hard gating, as well as other prior work. Their method obtains greater SNR than other methods. Nonlinear RNNs demonstrate vanishing gradients, and gated linear RNNs report better SNR than nongated linear RNNs. Finally, discrete gating produces slightly better SNR than sigmoidal gating.\n\nThey report returns for other tasks, demonstrating their cell can learn more quickly than other models as the temporal dependence length increases. They perform further comparisons on POPGym tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well written, theoretically sound, and novel. In particular, I find viewing RNN performance through the lens of SNR interesting, and it serves as good motivation for the proposed RNN cell. Table 3 is also particularly refreshing, demonstrating that the proposed method is designed for high-noise scenarios and does not necessarily achieve SoTA performance on standard benchmarks. Finally, the authors provide code for reproducibility."}, "weaknesses": {"value": "More experiments are always beneficial. I think exploring other surrogate gradients would be interesting (why $\\arctan$ instead of $\\tanh$ or $\\mathrm{erf}$?) I think an ablation for introducing noise would also be useful. I believe Stable Hadamard Memory [1] already performs such an ablation, but it would be interesting to see how important it is for discrete gating to work.\n\n## References\n[1] Hung et al., Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning"}, "questions": {"value": "I think figure 1 has a caption that is far too long."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SAecElGE4q", "forum": "1uXcaSmmUj", "replyto": "1uXcaSmmUj", "signatures": ["ICLR.cc/2026/Conference/Submission19161/Reviewer_ELid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19161/Reviewer_ELid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979872211, "cdate": 1761979872211, "tmdate": 1762931171140, "mdate": 1762931171140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a linear recurrent module that uses Spiking Neurons as discrete, input-dependent gating mechanisms. On the Passive Visual Match task with varying memory lengths, showing superior performance as length increases. Ablation studies demonstrating that their discrete gate outperforms a continuous (sigmoid) version. Based on the existing limitations and issues discussed below, I recommend reject."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The work is the first to explore Spiking Neurons as a discrete gating mechanism within a linear RNN framework for memory-based reinforcement learning. This is a interesting architectural choice.\n* It achieves excellent performance on specific long-term memory tasks, such as RepeatPrevious task in POPGym."}, "weaknesses": {"value": "* It lacks comparison with existing state-of-the-art linear-time sequence models, such as Mamba2.\n* It is missing an quantitative analysis of computational efficiency.\n* It achieves SOTA performance only in a minority of environments; its performance in other environments is inferior to baselines, which limits the general applicability of the method."}, "questions": {"value": "* The motivation for using Spiking Neurons is unclear. Combining a simple RNN or SSM with a discrete output could also achieve the effect of discrete gating. Why use Spiking Neurons, which may introduce additional training instability?\n* Why does the method perform poorly in other tasks? Although the authors claim comparable performance to LRU on MuJoCo tasks, the implementation of LRU is much simpler and its performance is superior.\n* Mamba's selection mechanism also performs information filtering. Does the authors' method filter noise more effectively than Mamba?\n* The method performs poorly on tasks like 'Autoencode' and 'Battleship', underperforming SHM. Does this indicate that the method is only effective for tasks with explicitly defined noise phases?\n* The paper mentions in Appendix E that training instability and Q-value divergence occurred in some runs, leading to the discarding of those results. Selectively discarding \"invalid\" runs, rather than reporting results from all random seeds, may overestimate the average performance of the method and makes the comparison with baseline methods unfair."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "voiJ9czUWp", "forum": "1uXcaSmmUj", "replyto": "1uXcaSmmUj", "signatures": ["ICLR.cc/2026/Conference/Submission19161/Reviewer_gBTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19161/Reviewer_gBTK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981811183, "cdate": 1761981811183, "tmdate": 1762931170525, "mdate": 1762931170525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a new recurrent cell which uses discrete gating instead of continuous gating. The authors argued that this gating mechanism is inspired by spiking neurons and the reason why discrete gating is better than continuous gating function due to having large surrogate gradients. Continuous gating mechanisms like sigmoid function which can have small gradient values and can lead to vanishing gradients. The authors evaluated their mechanism using various memory and distraction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing of the paper is clear, and the figures are easy to follow. \n2. The mathematical equations in the main paper are also well presented and improved the readability. \n3. Results in Passive Visual Match is clear and understandable.\n4. Figure 3 and 4 also produced convincing results."}, "weaknesses": {"value": "1. Having Figure 3, Figure 4 and Table 1 together at the same spot makes it feel extremely crowded. Best to separate them the figures and tables. \n2. Why are results in the RepeatPrevious environment so much better than the baselines in the medium and hard category? \n3. The proposed method did not do well in the POPGym benchmark. Can you authors give some insights on why? Also, which methods are considered state of the art?"}, "questions": {"value": "1. What is the memory consumption with respect to memory size compared to other models? Is it more memory efficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "97mwplwhVH", "forum": "1uXcaSmmUj", "replyto": "1uXcaSmmUj", "signatures": ["ICLR.cc/2026/Conference/Submission19161/Reviewer_M6hE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19161/Reviewer_M6hE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999953771, "cdate": 1761999953771, "tmdate": 1762931169996, "mdate": 1762931169996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}