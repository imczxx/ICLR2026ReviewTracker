{"id": "qTqAL2t8Aa", "number": 15220, "cdate": 1758249076868, "mdate": 1759897320667, "content": {"title": "Robust Spiking Neural Networks Against Adversarial Attacks", "abstract": "Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. \nHowever, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs.\nWe find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.", "tldr": "", "keywords": ["Spiking Neural Networks", "Rubustness"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d4da000aade0cc6d2c002f2698e77e4bc4dc34d.pdf", "supplementary_material": "/attachment/bcf1ac7d3e489dce1e53d4cff22c2fc1cd7e9009.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a Threshold Guarding Optimization (TGO) approach against the adversarial robustness problem of directly-trained spiking neural network (SNNs) architectures. The method relies on defending neurons that have membrane potentials very close to the firing threshold, which SNN adversaries essentially exploit. The defense mechanism uses (1) layerwise loss regularizers that move neuron membrane potentials away from the firing thresholds, thus effectively creates sparsity in the surrogate gradients, and (2) noisy LIF neurons to reduce the likelihood of state-flipping under minimal adversarial noise disturbances. Experiments on CIFAR-10/100 with VGG-11 and WRN-16 architectures demonstrate that TGO is effective when combined with adversarial training based methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper analyzes a clear cause of adversarial robustness of SNNs, i.e., neurons having threshold-neighboring membrane potentials for test samples. This is also the case for ANN neuron activations, which naturally aligns well in this paper.\n\n- The narrative and descriptions of the TGO methodology is clear."}, "weaknesses": {"value": "- Experimental evaluations are rather ambigious to draw any conclusions (e.g., noisy inference processes, potentially missing random restarts and EoT, weak attack strengths). It also appears like with simple BPTT, TGO is not highly effective as a standalone defense. Furthermore, surrogate gradient ensemble evaluations are missing, which should have been the rigorous attack baseline for SNN adversarial robustness.\n\n- There are several missing details in terms of hyperparameters, evaluation settings and consistency of the results from the main text to the appendix (where there are really informative results existing)."}, "questions": {"value": "- One of the most critical components of the defense is the use of randomness and injecting noise during inference. This is well-known to significantly prohibit accurate adversarial robustness evaluations. However, authors state that they only employ EoT to investigate this in Appendix B, although EoT and reliably evaluated robust accuracies under random restarts should have been present in all evaluations of the main manuscript.\n\n- The paper talks about directly-trained SNNs and surrogate gradient choices, but never really states the exact surrogate gradient function used in training and evaluation of their models?\n\n- Following the above question, there is also already an established surrogate gradient ensemble based SNN attack, which the paper does not consider: https://openreview.net/pdf?id=I8FMYa2BdP . It essentially aims to evaluate directly trained SNNs more reliably, by allowing the white-box adversaries to try out different surrogate gradient functions for more stable attacks. This aligns with basic security principles, where white-box adversaries should have complete access and capabilities in evaluating models. I would expect the authors to evaluate their \"adversarially robust SNNs\" under such ensemble attacks, by reporting robust accuracies under surrogate gradient adaptive adversaries.\n\n- The notion of imposing gradient sparsity was already the main idea in the SR approach. How is the present paper different?\n\n- Why are the naive attacks in Table 1 for CIFAR100/WRN-16 with AT+TGO(Ours) more effective than the results in Table 5 when EoT is added? The main idea in EoT is to obtain more rigorous adversaries, without making the attack weaker?\n\n- In Table 2 APGD_CE row, increasing the steps from 80 to 100 makes the attacks slightly weaker, given the numbers in the table. This should not happen in any case. There is some ambiguity regarding the convergence of attacks. How is the attack success calculated with increasing number of iterations?\n\n- Are there any other datasets/architectures that this approach would scale and advance the SoTA? Can we use this method besides VGG and WRN type of spiking networks, or on images larger than 32x32 resolution?\n\n- In general, evaluations are also demonstrated at a stronger perturbation radius than in the training phase. Also, is there the usual \"random restarts\" idea implemented in these multi-step PGD attacks?\n\n- No hyperparameter details are present. What is the lambda hyperparameter value in the main results? It is not clearly described anywhere, and none of the results for lambdas match consistently between Table 1 and Table 7 either, there appears to be some rows shifted or something. Overall, it is not possible to extract accurate information from the current presentation of results either.\n\n- The results on DVS datasets are only present in Appendix A, very briefly without details. Can the authors elaborate further, how they actually implemented this? These attacks should be fundamentally different to implement, since the inputs are binary. How does the perturbation strength work here for instance? Also, there is a typo there: DVS-CRIAF10 -> DVS-CIFAR10.\n\n- Regarding Figure 4 right side loss landscapes - It is not described how two-dimensional loss landscape visualizations are generated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tpCwXPnJDO", "forum": "qTqAL2t8Aa", "replyto": "qTqAL2t8Aa", "signatures": ["ICLR.cc/2026/Conference/Submission15220/Reviewer_23Ay"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15220/Reviewer_23Ay"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760967980512, "cdate": 1760967980512, "tmdate": 1762925518714, "mdate": 1762925518714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training strategy, termed TGO, to enhance the robustness of spiking neural networks (SNNs) against adversarial attacks such as FGSM, PGD, and their variants. The main idea is to constrain the membrane potential to remain sufficiently distant from the firing threshold, thereby reducing sensitivity to perturbations. In addition, the strategy introduces neuron-level perturbations (NLIF) and regularizes the probability of spike flipping. Experiments on the CIFAR-100 dataset demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** The idea is straightforward and easy to understand.\n\n**S2.** The authors conduct extensive experiments, including Expectation over Transformation (EoT) and loss landscape analysis.\n\n**S3.** As shown in the tables, TGO achieves the best robustness performance compared with state-of-the-art (SOTA) training strategies."}, "weaknesses": {"value": "**W1.** The explanation of the idea is unnecessarily complicated. In particular, Theorem 2 seems redundant — it is difficult to follow due to the heavy notation, and after reading the proof in the appendix, it appears to be a straightforward extension of Theorem 1.\n\n**W2.** I believe the proof of Theorem 3 may be incorrect. According to Appendix E, the flipping probability from 1 to 0 should be expressed as the conditional probability and the same applies to the flipping probability from 0 to 1.\n$$ \nP_{1\\rightarrow 0} = P(\\eta[t] < V_{th} - V[t] | V[t] \\geq V_{th}).\n$$ \n\n**W3.** Theorem 1 is proved under an $\\ell_2$ constraint, whereas the experiments are conducted with $\\ell_\\infty$ perturbations. This inconsistency raises questions about the necessity and practical relevance of the theorem.\n\n**W4.** According to Eq. (7), the constraint loss needs to be computed in a layer-wise manner. Meanwhile, the NLIF module introduces perturbations at every layer. How efficient is TGO in terms of computation compared with other training strategies? Does the training time increase significantly as the network depth grows?\n\n**W5.** The paper does not specify the value of $\\delta$ in Eq. (7) or explain how it was chosen. Please clarify it.\n\n**W6.** The paper contains some typos, though they do not affect my overall rating. A few examples are listed below:\n\n1. Line 182: $|J_f(x)|_2^2$.\n2. In Theorem 1, $\\eta$ has a mean of 0, whereas in Lines 316–317 and Figure 2(d), another mean $\\mu$ appears.\n3. In Eq. (11), it seems that $P$ should be used instead of $\\Delta P$.\n4. The clean accuracy is not highlighted in Table 1.\n5. In Appendix C, the proof is written under the condition $||\\delta||_2 \\leq 1$ rather than $||\\delta||_p \\leq 1$."}, "questions": {"value": "Please address W2–W5 in the Weakness section. I will consider increasing my score if these concerns are fully addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nAteDReqoG", "forum": "qTqAL2t8Aa", "replyto": "qTqAL2t8Aa", "signatures": ["ICLR.cc/2026/Conference/Submission15220/Reviewer_FMzT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15220/Reviewer_FMzT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537134441, "cdate": 1761537134441, "tmdate": 1762925518410, "mdate": 1762925518410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a Threshold Guarding Optimization (TGO) method for enhancing the robustness of SNNs. TGO aims to reduce the number of threshold-neighboring spiking neurons, thereby decreasing the state-flipping probability. Noisy-LIF neurons are also adopted to eliminate the influence of adversarial perturbations. Experiments results show that TGO surpasses SOTA SNN-based adversarial defense methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The reasoning of the paper is clear and coherent. Reducing the number of threshold-neighboring spiking neurons provides a new insight in enhancing the robustness of SNNs.\n\n2.The theoretical analysis of the paper is reasonable.\n\n3.The improvement of TGO is significant, improving the robustness effectively. (Only if the experimental results are convincing, see weaknesses below)"}, "weaknesses": {"value": "1. In Line 021 in abstract and Line 061 in introduction, the authors mentioned their method can enhance ‘gradient sparsity’. Normally the sparsity corresponds to L0-norm [1]. However, in Theorem 1, the author aims to optimize L2-norm of the Jacobian matrix, which is inconsistent to optimizing the gradient sparsity. The term ‘sparsity’ seems inappropriate.\n\n2. What is Figure 2 used for? The main text does not mention or introduce Figure 2, leaving Figure 2 alone. What is $H[t]$ in Figure 2(a) and Figure 2(d)? Why does the state-flipping probability correspond to $H[t]$ (or $U[t]$) instead of $V[t]$? In Figure 2(c), it seems that your method only penalizes membrane potential under threshold, and membrane potential beyond threshold remains unchanged.\n\n3. The experimental results are unconvincing. \n- 3.1 The authors only conducted experiments in CIFAR100 (and a small experiment in DVS-CIFAR10). In Line 335, the authors mentioned CIFAR10, but I cannot see any experiment of CIFAR10 even in Appendix. \n- 3.2 The authors only adopted ANN-based attacks. As the paper focuses on SNNs, SNN-based attacks such as [2][3] must be included for comprehensive evaluation. \n- 3.3 In Figure 3, your method TGO+AT (about 59%) is lower than SR+AT (about 62%) in clean accuracy. However, in Table 1, TGO+AT (64.49%) is higher than SR+AT (60.37%) in clean accuracy. \n- 3.4 Moreover, the adversarial accuracy in APGD and MTPGD in Table 2 is significantly higher than PGD in Table 1. As APGD is much stronger than PGD, why does this situation occur? For instance, in Line 367, PGD10 obtained 22.75% accuracy, while in Line 399, APGD10 only obtained 29.19% accuracy.\n\nReferences:\n\n[1] Liu, Yujia, et al. \"Enhancing Adversarial Robustness in SNNs with Sparse Gradients.\" International Conference on Machine Learning. PMLR, 2024.\n\n[2] Lun, Li, et al. \"Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[3] Hao, Zecheng, et al. \"Threaten spiking neural networks through combining rate and temporal information.\" The Twelfth International Conference on Learning Representations. 2024."}, "questions": {"value": "1. In Line 079, ‘TGO combined with vanilla SNNs surpasses those adversarial training strategies for the first time’. Which experiment supports this contribution?\n\n2. Typos in formulas. Like Theorem 1, Line 272 Vth. Please check the whole manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "27T5jQyVE9", "forum": "qTqAL2t8Aa", "replyto": "qTqAL2t8Aa", "signatures": ["ICLR.cc/2026/Conference/Submission15220/Reviewer_TQh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15220/Reviewer_TQh4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550403826, "cdate": 1761550403826, "tmdate": 1762925517859, "mdate": 1762925517859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Threshold Guarding Optimization (TGO) method to enhance the adversarial robustness of SNNs. By regulating neuron membrane potentials and employing probabilistic firing via noisy neurons, TGO significantly reduces vulnerability to adversarial perturbations, outperforming existing methods in various adversarial scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and logically structured, making complex concepts accessible and easy to follow.\n- This paper provides a mathematical analysis linking “threshold-neighboring neurons” to adversarial vulnerability, which is a novel and interesting framework for SNN robustness research.\n- The authors demonstrate the effectiveness of the proposed TGO method across a wide range of adversarial attack scenarios. The experiments span multiple datasets, network architectures, and adversarial settings, showing strong and consistent results, including outperforming SOTA baselines."}, "weaknesses": {"value": "- The method introduces additional hyper-parameters such as coefficient parameter $\\lambda$ and noise level $\\sigma$. However, the effectiveness of the $\\lambda$ scheduling and sensitivity of the noise level $\\sigma$ is missing.\n- The paper does not report the additional training cost introduced by the TGO compared to baselines such as adversarial trainings (AT, RAT).\n\n**Limitation**\n\nAccording to the reported results, the proposed method appears to reduce clean accuracy, indicating a potential trade-off between robustness and standard performance."}, "questions": {"value": "Can the authors provide a more detailed derivation or intuition for Eq. (7)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "y6MeAjAvo7", "forum": "qTqAL2t8Aa", "replyto": "qTqAL2t8Aa", "signatures": ["ICLR.cc/2026/Conference/Submission15220/Reviewer_3pNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15220/Reviewer_3pNN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810155353, "cdate": 1761810155353, "tmdate": 1762925517187, "mdate": 1762925517187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}