{"id": "2rwqNNIKeI", "number": 20470, "cdate": 1758306522013, "mdate": 1763714113074, "content": {"title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering", "abstract": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks---HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG---demonstrates that our approach consistently outperforms strong baselines.", "tldr": "We propose an agentic retrieval framework for multi-hop QA that balances precision and recall via iterative LLM agents, yielding compact evidence, outperforming baselines, and boosting QA accuracy with less irrelevant context.", "keywords": ["Information Retrieval", "Question Answering", "Retrieval-Augmented Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbcee21eb63295c00dfe924c7c49fb1f5e6a5259.pdf", "supplementary_material": "/attachment/ddb382f4a4c22c6aeb60d52a93d3863d91e2f5ea.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PRISM which is an agentic retrieval system for multi-hop QA that explicitly separates precision and recall in each iteration step with Selector and Adder. The system consists of the Question Analyzer to decompose the complex query into sub-queries, the Selector to filter retrieved candidates to maximize precisions, and the Adder that revisits the discarded candidate to recover recall. The retrieved information after a few iterations is fed into the Answer Generator to give the final answer. The system is evaluated on 4 multi-hop datasets in comparison with OneR and IRCoT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The RAG system with modules are clearly introduced."}, "weaknesses": {"value": "- The system design is costly and there is no budget and latency analysis with baselines. Please provide #retrievals, #LLM calls, tokens, latency per query for all methods.\n- MultiHopRAG Table 1 lists PRISM as 24.74/40.64, while the surrounding text states 28.18/42.22\n- The novelty is limited, as the main modules in the system have been extensively explored in prior RAG work. For example, IRCoT already covers the 'decompose, retrieve and update' approach, works as Self-RAG, QD-RAG, RAPTOR, RankRAG also have retrieve, critique/select, and generate approach. \n- The Adder largely resembles a naive recall expansion and the paper should ablation Adder (not together with Selector), and the paper should also report results of precision and recall with varying k retrievals with a fixed re-ranker."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FiI6fDAxxG", "forum": "2rwqNNIKeI", "replyto": "2rwqNNIKeI", "signatures": ["ICLR.cc/2026/Conference/Submission20470/Reviewer_T7mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20470/Reviewer_T7mf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742005086, "cdate": 1761742005086, "tmdate": 1762933911618, "mdate": 1762933911618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets multi-hop question answering, and argues that existing retrieval methods struggle to balance precision (removing distractors) and recall (capturing all necessary evidence).\nThe paper proposes PRISM, an agentic retrieval framework that employs three LLM-based agents, Question Analyzer, Selector, and Adder, in an iterative loop to refine evidence through precision–recall balancing.\nAcross four QA benchmarks, PRISM achieves higher retrieval accuracy and downstream QA performance than strong baselines such as IRCoT and SetR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and easy to follow, with a good presentation of the motivation and framework.\n\n2. The core idea of separating precision- and recall-oriented retrieval through multiple agents is reasonable and intuitively makes sense for multi-hop QA."}, "weaknesses": {"value": "The technical contribution is limited. The proposed framework can largely be seen as a variant of existing agentic retrieval paradigms such as ReAct or IRCoT. In essence, its three-agent loop (Analyzer–Selector–Adder) fits naturally into the standard structure where an LLM iteratively decomposes complex queries, performs retrieval, summarizes or filters relevant evidence, and then issues follow-up sub-queries for missing information. As such, the framework does not introduce a new mechanism or learning objective, but rather re-organizes established steps in a slightly more structured form.\n\nThe comparison to prior work is too limited. For instance, the paper overlooks completeness-oriented retrieval frameworks such as ARM: An Alignment-Oriented LLM-Based Retrieval Method (Chen et al., 2025), which performs a “retrieve-all-at-once” alignment between questions and structured data representations. It also cites but does not thoroughly contrast with recent iterative agentic retrieval methods (e.g., ReAct-style or similar architectures), where the main differences are not rigorously analyzed. As a result, it remains unclear whether the proposed framework represents a substantive advance over these approaches or simply reorganizes familiar retrieval-reasoning patterns within a slightly different agentic structure."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MQ1OlOzUf6", "forum": "2rwqNNIKeI", "replyto": "2rwqNNIKeI", "signatures": ["ICLR.cc/2026/Conference/Submission20470/Reviewer_Agjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20470/Reviewer_Agjv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971212553, "cdate": 1761971212553, "tmdate": 1762933911013, "mdate": 1762933911013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a multi-agent RAG framework comprising three key components - Question Analyzer, Selector, and Adder. The method iteratively tries to balance precision (using selector) and recall (using adder). Evaluations are presented on HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG consistent improvements over zero-shot baselines like IRCoT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.  The explicit separation between precision (Selector) and recall (Adder) is intuitive and aligns with known trade-offs in retrieval-augmented reasoning.\n\n2. The paper is fairly well written and is easy to follow.\n\n3. Improvements in retrieval translate to gains across multiple wikipedia-style QA datasets.\n\n4. The ablations confirm that the Q Analyzer and Selector-Adder loop indeed contribute to improvements."}, "weaknesses": {"value": "1. Limited Novelty & Prior Art not fully acknowledged: The Selector and Adder roles are similar “critic” or “augmenter” agents explored in prior work. The paper could better situate itself relative to context compression and summarization methods like RECOMP (Zhang et al., 2024), which similarly aim to prune irrelevant evidence. Overall the technical novelty of the paper is limited, conceptually the contribution lies mainly in the modular packaging of ideas existing in prior literature. Accordingly, the contributions should be toned down.\n\n2. Missing comparisons: Key state-of-the-art agentic retrievers are missing from comparisons: CoRAG (Wang et al), R1-Searcher (Song et al), O2Searcher (Mei et al), which report stronger retrieval–generation integration with SLM finetuning. The baselines used (IRCoT, RankZephyr) are somewhat dated and primarily zero-shot. Without inclusion of finetuned systems, the claim is slightly overstated.\n\n3. Missing implementation details: The paper briefly notes using a dense retriever based on BGE-M3, but omits critical implementation specifics such as the indexing method, retrieval depth (k), and whether all baselines share the same retriever configuration and corpus index. Without these details, it is difficult to verify that comparisons are conducted under identical retrieval settings, which weakens the empirical rigor of the results.\n\n4. Scalability is not addressed: The method involves multiple LLM calls per query (Analyzer + multi-step Selector/Adder), which might scale poorly for large corpora or smaller models. There is no analysis of cost, latency, or performance when replacing LLMs with smaller models.\n\n5. The authors are encouraged to test their approach on more recent multi-hop QA datasets beyond wikipedia style corpora which are already seen by LLMs during training.\n\n6. PRISM reports P/R/F1 for its approach but does not report the same retrieval metrics for every baseline at the same retrieval depth (or at varying k). Reporting only aggregate QA EM conflates retrieval and generation effects.\n\n7. The paper might benefit from some qualitative examples. For instance, the authors could provide qualitative example cases where Selector removed correct but weak evidence.\n\n8. Adding sensitivity analyses for iterations N, prompt templates, and LLM backends would strengthen the paper."}, "questions": {"value": "Kindly see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LK0FCR1TL8", "forum": "2rwqNNIKeI", "replyto": "2rwqNNIKeI", "signatures": ["ICLR.cc/2026/Conference/Submission20470/Reviewer_peK8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20470/Reviewer_peK8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013540831, "cdate": 1762013540831, "tmdate": 1762933910403, "mdate": 1762933910403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}