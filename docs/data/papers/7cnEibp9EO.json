{"id": "7cnEibp9EO", "number": 23875, "cdate": 1758349655343, "mdate": 1759896793091, "content": {"title": "triCAM: A Real Monocular Multi-Modal Event-based Pedestrian Dataset", "abstract": "Event-based visions offer key advantages, such as low latency, high dynamic range, and microsecond temporal resolution. These strengths have motivated extensive research into their complementarity with other modalities, which led to the creation of several multi-modal event-based datasets. However, most of these datasets are designed for automotive or robotic domains, with limited attention to human-centered perception in everyday settings. In this paper, we introduce triCAM, a real-world monocular multi-modal event-based pedestrian dataset. triCAM integrates event streams, RGB images, depth images, IMU data, and pedestrian bounding box annotations. This dataset contains 20 sequences, each recorded in two different restaurants in both static and dynamic camera motions. By providing a rich dataset on pedestrian activities in socially interactive environments, triCAM contributes to the advancement of research in robust perception and human interaction understanding.", "tldr": "", "keywords": ["Multimodal learning", "Multisensor data", "Event camera", "RGBD camera", "Depth estimation", "IMU"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/420075fa7c6575300ed8433f8c814c33f3e39cc7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces triCAM, an event-based, real-world monocular multimodal pedestrian dataset. The dataset integrates event streams, RGB images, depth images, IMU data, and pedestrian bounding box annotations recorded in indoor and outdoor restaurant environments under both static and dynamic camera motions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This type of dataset seems uncommon in the event camera field."}, "weaknesses": {"value": "1. Does PEDESTRIAN only cover people in restaurants? According to the KITTI rankings, PEDESTRIAN should also cover people in open environments like roads.\n\n2. Is this dataset's task just pedestrian detection? I'd prefer to see results on depth estimation, event camera SLAM, or motion prediction. This results in the experimental section being too sparse, with tables and tables spanning less than a page. I'd prefer to see a more comprehensive, multi-task benchmark.\n\n3. The TRICAM dataset is too small. The 20 sequences total only 60 minutes of data, which is far from sufficient for a portable dataset that doesn't require any motion capture. Furthermore, it was captured in just two restaurants, so the environment and lighting diversity are limited compared to large-scale benchmarks.\n\n3. I have serious questions about the calibration results for the RGB and event cameras. In particular, I don't think the reconstructed checkerboard is good in Figure 3. I hope the authors can provide a visualization of the overlap between the event and RGB synchronized frames.\n\n4. I'm very surprised that this dataset collects such sensitive personal data without any privacy protection. This clearly violates ethical review rules."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AUigpvoIZf", "forum": "7cnEibp9EO", "replyto": "7cnEibp9EO", "signatures": ["ICLR.cc/2026/Conference/Submission23875/Reviewer_9EHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23875/Reviewer_9EHV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580055492, "cdate": 1761580055492, "tmdate": 1762942836445, "mdate": 1762942836445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces triCAM, a real-world monocular multi-modal dataset targeting pedestrian scenes in restaurant environments. triCAM contains synchronized streams from a Prophesee Gen3 event camera, an Intel RealSense D435i (RGB + depth + IMU), and an additional WitMotion IMU. The dataset comprises ~20 sequences recorded in two restaurants under both static and handheld (dynamic) camera motions, and includes calibration parameters and bounding-box annotations for both RGB and event-derived image representations. The authors describe the hardware/software setup, spatial/temporal synchronization pipeline (including E2VID reconstruction of events for calibration/annotation), dataset format (ROS bag + supplementary files), and present a baseline pedestrian detection evaluation using YOLOv8x on RGB-only, Event-only, and late-fusion Event+RGB setups. The manuscript positions triCAM as filling a gap in human-centric, socially interactive event datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**: Focus on indoor/outdoor restaurant pedestrian scenes (socially interactive, cluttered) is novel among event multi-modal datasets — most prior work targets automotive or robotics navigation. \n\n**Quality**: Use of a modern Gen3 event camera plus RealSense D435i and separate IMU provides complementary modalities; distribution in ROS bag format and inclusion of intrinsic/extrinsic calibration parameters increases usability.\n\n**Clarity**: Paper is structured clearly with useful tables/figures (sensor specs, sequence statistics, baseline results) that help readers understand dataset composition.\n\n**Significance**: Enables research directions (multi-modal pedestrian detection, event-guided pose/depth estimation, perception in socially interactive settings) that are underexplored in event vision."}, "weaknesses": {"value": "1. Figure/table layout and information density are insufficient. The paper’s sequence overviews and calibration/annotation examples (Figures 2, 3, 4) present a formal view but lack more details, so readers cannot reliably assess annotation or reconstruction quality.\n2. Dataset statistics and sample-level examples are incomplete. Although Table 3 lists per-sequence duration, number of people, and total events, the manuscript misses key sample-level statistics (e.g., per-frame or per-sequence bounding-box count distributions, frame counts per distance bin, quantitative occlusion tiers) and lacks representative “difficult sample” parallel visualizations that would let readers judge diversity and difficulty.\n3. Lacks clear problem definition and theoretical explanation. While the introduction and contribution sections state triCAM’s purpose and novelty, the manuscript does not formalize the tasks or evaluation objectives (for example, concrete definitions of the “multimodal fusion” problem being solved and the evaluation criteria).\n4. Experimental reporting is insufficient (narrow baselines, missing ablations and per-condition breakdowns). The evaluation uses YOLOv8x variants (Event-only, RGB-only) and a simple late-fusion, reporting only overall mAP50/Precision/Recall (Table 4). There are no per-condition results (static vs. dynamic, occlusion levels, distance bands), nor ablations on key hyperparameters (e.g., the 33.33 ms event aggregation window)."}, "questions": {"value": "1. Improve the layout of Figures 2–5 to make the information more readable (e.g., clearer captions)\n2. Consider presenting diversity metrics, for example: variation in lighting, motion, and occlusion; number of unique subjects; and environmental diversity (indoor/outdoor).\n3. Formally define the tasks that triCAM aims to support (e.g., “multimodal monocular detection,” “event–RGB fusion tracking,” etc.)\n4. Add ablation studies on key parameters such as the event aggregation window (33.33 ms).\n5. Provide per-condition analyses, such as static vs. dynamic scenes, different occlusion levels, lighting conditions, and distance ranges."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "fb0BgA6Aep", "forum": "7cnEibp9EO", "replyto": "7cnEibp9EO", "signatures": ["ICLR.cc/2026/Conference/Submission23875/Reviewer_79fb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23875/Reviewer_79fb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744629646, "cdate": 1761744629646, "tmdate": 1762942836239, "mdate": 1762942836239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents triCAM, a monocular multi-modal pedestrian dataset collected at two restaurant venues, combining event streams, RGB, depth, IMU, pedestrian boxes, and calibration. Sequences include static and dynamic camera motions. Events are binned at 33 ms to align with ~30 FPS depth frames, and the spatial/temporal synchronization pipeline is described in detail. The authors report YOLOv8x baselines for Event-only, RGB-only, and late fusion (NMS), with fusion outperforming single modalities in mAP metric. Despite the clear sync/calibration procedures and usable baselines, the paper has core issues: the motivation for the restaurant setting is weak, scale/diversity are limited, and there is no condition-wise analysis showing where events help (e.g., low light, fast motion, occlusion). The baselines and evaluation are insufficient (limited metrics/fusion strategies, no cross-scene generalization/transfer), and the overall presentation reads more like a careful engineering report than a dataset paper with compelling novelty. Therefore, I recommend rejection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive multimodal setup with clear sync/calibration.** The platform jointly captures events, RGB, depth, and IMU, enabling research on cross-modal fusion and alignment. The paper details event-to-frame temporal alignment and spatial calibration procedures that are reasonably reproducible, lowering the barrier to use.\n\n2. **Practical data organization.** The release includes pedestrian bounding boxes and starter training/evaluation scripts (e.g., YOLO), making it friendly for baseline reproduction and quick experimentation.\n\n3. **Empirical indication of modality complementarity.** Although the analysis is not yet thorough, initial results show Event + RGB fusion outperforming single modalities, suggesting genuine potential for multimodal gains."}, "weaknesses": {"value": "1. **Motivation for restaurant events is under-argued.** The paper cites event cameras’ advantages for high-speed and robust perception, but the collected restaurant scenes are not clearly high-speed. With only two venues, it is hard to claim representativeness for “socially interactive environments.” The paper does not concretely show which sub-conditions (e.g., strong backlight, low light, rapid motion, heavy occlusion, hand-held shake) in the dataset require events beyond what RGB can handle. \n\n2. **Scale and diversity are limited.** The dataset contains ~20 sequences across two restaurants, while per-sequence stats are given, there is no explicit partitioning by motion speed or illumination, and no targeted “event-advantage” splits (e.g., low light, motion blur, fast actions).  Table 4 reports improvements on the entire test set only. Condition-wise statistics (e.g., low light, fast motion, dynamic camera, high occlusion) are necessary to determine where events are most beneficial, rather than relying on a single overall number. \n\n3. **Label scope is narrow compared to stated goals.** Only bounding boxes for pedestrians are provided, tasks central to human-centered interaction (segmentation, pose/landmarks, ReID, trajectories/MOT) are absent, limiting the dataset’s relevance to the broader HRI/behavior understanding vision it outlines. \n\n4. **Baselines and evaluation are thin.** Metrics stop at mAP@50, COCO mAP@[.5:.95] is not reported. Fusion is late-fusion NMS only, no early/mid-level or joint training fusion. No cross-scene generalization, and no evidence that models trained on triCAM can generalize better to other datasets/scenes. \n\n5. **“Monocular” emphasis lacks compelling evidence.** The paper claims to be the first publicly available monocular multi-modal pedestrian dataset and contrasts with stereo-heavy rigs in related work, but it does not quantify the practical advantages of monocular over (i) using a single lens from a stereo rig or (ii) small-baseline stereo under the same setup (e.g., differences in calibration/sync complexity, cost, power, drift, failure cases)."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YKwUlEq0lN", "forum": "7cnEibp9EO", "replyto": "7cnEibp9EO", "signatures": ["ICLR.cc/2026/Conference/Submission23875/Reviewer_wXLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23875/Reviewer_wXLm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804265171, "cdate": 1761804265171, "tmdate": 1762942836047, "mdate": 1762942836047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a monocular multi-modal pedestrian dataset, triCAM. The dataset is captured with RGB, depth, event, and IMU sensors in two different restaurants. The images are further manually annotated by humans. In the experimental results, the mAP\nperformance using YOLOv8 is provided. The hardware, software setup, and calibration process are also introduced."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The collected triCAM provides a new multi-modality benchmark to the pedestrian detection community.\n- The collecting and post-processing pipeline is introduced clearly."}, "weaknesses": {"value": "- The major concern is about privacy protection. As a pedestrian dataset, it is not mentioned whether permission was obtained from the subjects, or whether the dataset will be made public, or under what license.\n- The dataset used manual annotations. But the annotation compensation was not mentioned, so I also added a corresponding flag for the ethics review.\n- This dataset is limited to two specific restaurant scenarios. Therefore, its general applicability is limited. The existing datasets mentioned in Table 1 cover more general situations than this dataset.\n- The paper only reported the performance on a specific object detector, which fails to reflect the challenge of this new dataset and the necessity of releasing such a dataset."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "As mentioned, this paper did not mention whether permission was obtained from the subjects, or whether the dataset will be made public, or under what license. Meanwhile, the dataset used manual annotations. But the annotation compensation was not mentioned."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1qmzmmV0LA", "forum": "7cnEibp9EO", "replyto": "7cnEibp9EO", "signatures": ["ICLR.cc/2026/Conference/Submission23875/Reviewer_ouX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23875/Reviewer_ouX8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889583577, "cdate": 1761889583577, "tmdate": 1762942835793, "mdate": 1762942835793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}