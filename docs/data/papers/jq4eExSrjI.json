{"id": "jq4eExSrjI", "number": 2195, "cdate": 1757020839565, "mdate": 1763699302843, "content": {"title": "Bayesian Data Reweighting Improves Retrieval in Knowledge-Based VQA", "abstract": "Knowledge-based Visual Question Answering (VQA) requires retrievers to incorporate external knowledge, e.g., documents, to answer questions. Existing retrievers are typically optimized with standard contrastive learning, which treats all non-positive pairs as equally informative, leading to false negative bias and difficulties in hard negative mining. To overcome these issues, we propose \\textbf{Bayesian Data Reweighting (BDR)}, a probabilistic framework that assigns learnable importance weights to query-document pairs and performs Bayesian inference over these weights. We derive closed-form posterior updates under conjugate priors and develop an efficient EM algorithm for weight estimation. This approach adaptively emphasizes informative pairs without explicit hard negative mining. Experiments on two representative multimodal retrievers demonstrate consistent improvements, with BDR achieving gains of up to $8.6$ points on individual datasets and an average recall of $68.6$ across all M2KR datasets, surpassing the previous state-of-the-art.", "tldr": "", "keywords": ["Multimodal Embedding Retrieval; Bayesian Data Reweighting; Retrieval-Augmented Generation;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8d18a54c7196365c79795c6c22a80678526dc0e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Bayesian Data Reweighting (BDR) — a probabilistic framework for improving multimodal retrievers in knowledge-based visual question answering (KB-VQA). Instead of treating all non-positive samples equally (as in standard contrastive learning), BDR introduces learnable importance weights for each query-document pair and performs Bayesian inference to adaptively emphasize informative negatives and down-weight false ones.\n\nThe authors derive closed-form posterior updates under conjugate priors using an auxiliary variable augmentation scheme, enabling efficient inference through a stochastic Expectation-Maximization (EM) algorithm. They prove theoretical guarantees for asymptotic consistency and finite-sample convergence, and empirically demonstrate that BDR improves retrieval performance across both CLIP-based retrievers (PreFLMR) and LLM-based retrievers (VLM2Vec).\n\nExperiments on multiple KB-VQA datasets (e.g., OKVQA, InfoSeek, EVQA) show consistent gains. Integrating BDR into retrieval-augmented generation also improves downstream VQA accuracy and BLEU scores, sometimes surpassing even large LLMs like GPT-4V in end-task performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. BDR introduces a Bayesian probabilistic treatment of sample weighting within contrastive learning, which is an elegant and theoretically grounded approach to mitigate false and hard negatives. The authors provide formal results (conditional conjugacy, consistency, and finite-sample bounds), giving BDR mathematical credibility beyond empirical heuristics.\n2. The stochastic EM algorithm allows practical application to large-scale datasets, overcoming scalability concerns typical of Bayesian inference.\n3. Evaluation spans multiple retrievers, architectures (CLIP, LLM-based), and datasets, showing consistent and substantial improvements. The study also assesses efficiency, retrieval quality, and VQA accuracy, providing a full picture of impact. BDR improves both retrieval and downstream VQA generation, outperforming prior state-of-the-art retrievers and even some much larger LLM systems.\n4. The presentation is good and clear."}, "weaknesses": {"value": "1. Although the stochastic EM is efficient, the paper does not fully quantify its additional training-time cost relative to vanilla contrastive learning or other reweighting baselines.\n2. Sections 3.2–4 are mathematically heavy; the presentation could be streamlined. Should be improved to avoid overwhelming readers unfamiliar with Bayesian inference.\n3. The study lacks direct comparison with other hard-negative mining or debiased contrastive learning methods under the same multimodal retrieval setting, which would help isolate BDR’s contribution."}, "questions": {"value": "1. How does the proposed approach's runtime compare with standard InfoNCE training (e.g., training time per epoch or GPU-hours)?\n2. How are the importance weights initialized during training? Do they converge to stable distributions, or require annealing / regularization to prevent collapse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fKaeSM9TUL", "forum": "jq4eExSrjI", "replyto": "jq4eExSrjI", "signatures": ["ICLR.cc/2026/Conference/Submission2195/Reviewer_SpeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2195/Reviewer_SpeN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484754269, "cdate": 1761484754269, "tmdate": 1762916133772, "mdate": 1762916133772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bayesian Data Reweighting (BDR), a novel framework designed to improve multimodal retrievers for knowledge-based visual question answering (VQA). BDR addresses the limitations of standard contrastive learning by assigning learnable, adaptive importance weights to positive and negative samples through a principled Bayesian inference procedure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method demonstrates performance gains across multiple model architectures (CLIP-based and LLM-based)\n- Theoretically, it provides proofs for inference via conjugate priors and establishes the statistical consistency of its objective."}, "weaknesses": {"value": "- The empirical improvements attributed to BDR appear marginal in several key scenarios. For instance, on the EVQA dataset in Table 1, the gains over the InfoNCE baseline are minimal. Furthermore, the average improvement reported in Table 3 is modest. These results raise questions about the practical significance and consistent advantage of BDR over strong baselines.\n- The experimental evaluation primarily uses the standard InfoNCE loss as the baseline. However, to properly situate BDR's contribution, it is crucial to compare against more advanced methods that also address false and hard negatives, such as the debiased contrastive loss and hardness-aware weighting schemes, which are discussed in the related work. Without such comparisons, the relative merit of the proposed Bayesian approach remains unclear.\n- The theoretical analysis relies on the assumption that negative samples are i.i.d. This is a strong and often unrealistic assumption in contrastive learning, where negatives are typically sampled from a shared batch, introducing structured correlations. \n- The paper positions BDR as a general framework for contrastive learning. However, its effectiveness is demonstrated solely within the domain of knowledge-based VQA. Evaluation on other canonical contrastive learning tasks may be helpful to substantiate the claim of generality."}, "questions": {"value": "The theoretical analysis proves global properties like consistency but does not formally link the Bayesian weighting mechanism to the core concepts of false and hard negatives. Could the authors provide further insight, either theoretically or empirically, into how the inferred weights correlate with the semantic \"hardness\" or \"falseness\" of negatives? A deeper analysis showing that the framework reliably suppresses false negatives (assigning near-zero weights) and up-weights informative hard negatives would significantly strengthen the causal claim behind its success."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4pM4Kx8xYt", "forum": "jq4eExSrjI", "replyto": "jq4eExSrjI", "signatures": ["ICLR.cc/2026/Conference/Submission2195/Reviewer_ygZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2195/Reviewer_ygZC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936569259, "cdate": 1761936569259, "tmdate": 1762916133235, "mdate": 1762916133235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the Knowledge-based Visual Question Answering (KB-VQA) task. The main motivation of this paper is that standard contrastive learning of KB-VQA ignores the potential hierarchical structure of negative pairs including true, false, and hard negatives. These negative pairs need to be handled carefully in the training phase of contrastive learning, therefore a Bayesian reweighting method is proposed by the authors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe topic of this paper looks reasonable.\n-\tThe paper is well-written.\n-\tThe experimental results are promising."}, "weaknesses": {"value": "-\tThe main motivation of this paper is that standard contrastive learning of KB-VQA ignores the potential hierarchical structure of negative pairs including true, false, and hard negatives. Although the authors cite related paper to justify this argument, it lacks obvious quantitative (e.g., the statistics of these hierarchical negative pairs in datasets) or visualized feature embeddings among different pairs to demonstrate the reasonability of this motivation in the introduction or experiment section. \n-\tFor the Efficient Inference with Stochastic Expectation Maximization, it is necessary to formulate the specific complexity using stochastic Expectation-Maximization\n(EM) algorithm.\n-\tFor the Augmented Likelihood and Conditional Conjugacy, it is confused that how the data- augmentation is introduced. To me, the random variable $\\mu$ is the alternative of explicit sample reweighting and how the random variable links the data-augmentation method? It is a parameter-augmentation method rather than the data-augmentation one? This section is quite mess and it is hard to follow easily."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cDIlo4s0xp", "forum": "jq4eExSrjI", "replyto": "jq4eExSrjI", "signatures": ["ICLR.cc/2026/Conference/Submission2195/Reviewer_s1bH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2195/Reviewer_s1bH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996753120, "cdate": 1761996753120, "tmdate": 1762916133028, "mdate": 1762916133028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify that treating all negative pairs as equally informative can lead to false negative bias, making hard negative mining particularly challenging. To address this issue, the paper introduces a novel Bayesian data reweighting approach that calibrates the contributions of positive and negative samples to improve knowledge retrieval for KB-VQA. An efficient EM algorithm is proposed to estimate the optimal weights for both positive and negative examples. The proposed method achieves superior performance compared to previous approaches on the M2KR benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a novel Bayesian data reweighting method for KB-VQA retrieval.\n\n- Demonstrates consistent improvements over baseline retrieval setups across various backbones and two architectures."}, "weaknesses": {"value": "- Please use the correct citation style in LaTeX for ICLR.\n- In some results (e.g., on OVEN), the baseline scores are higher and should be bolded in Table 1 instead of the proposed method.\n- Regarding the VQA performance on E-VQA, it is unusual that the VQA accuracy is much lower than EM, as they should theoretically be on par. Moreover, previous work typically reports the BEM score, which is the standard metric used in the original E-VQA paper. For fair comparison with prior work, the authors should report BEM instead of accuracy for E-VQA. Additionally, the oracle results reported in the original E-VQA paper are substantially higher under the BEM metric. It would be valuable if the authors could reproduce and report the corresponding BEM scores with Oracle for comparison.\n- The different configurations of the Gamma prior appear to yield exact identical performance. Could the authors elaborate on how these hyperparameters influence the final results and provide more details behind their selection?\n- Similarly, please provide more detailed insights on the three types of priors, beyond the brief intuition mentioned in the paragraph starting at Line 206."}, "questions": {"value": "- Line 424: What does BRCL stand for?\n\n- How does the training cost (computation/time) compare with the InfoNCE baseline?\n\n- Since the method assigns weights to all examples, could these weights be used to rank examples in the corpus for positive/negative selection or data pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ECzAiPD1mi", "forum": "jq4eExSrjI", "replyto": "jq4eExSrjI", "signatures": ["ICLR.cc/2026/Conference/Submission2195/Reviewer_8HZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2195/Reviewer_8HZW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011649366, "cdate": 1762011649366, "tmdate": 1762916132212, "mdate": 1762916132212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}