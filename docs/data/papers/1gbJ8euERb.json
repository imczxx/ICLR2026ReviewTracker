{"id": "1gbJ8euERb", "number": 11046, "cdate": 1758187965282, "mdate": 1759897612386, "content": {"title": "From Translation to Multilinguality: Revisit the Role of Parallel Data in Multilingual LLM Pretraining", "abstract": "Multilingual large language models (MLLMs) are commonly trained with parallel data (i.e., concatenated translation pairs) to introduce cross-lingual alignment signal and induce capabilities transfer for non-English languages. However, it remains unclear whether this de facto practice improves general multilingual ability beyond translation. We conduct a controlled, large-scale study comparing two ways of using parallel data in pretraining: (1) standard concatenated translation pairs as a single sample, and (2) treating each side as an independent sample. Across diverse experimental settings, we find consistent results: parallel concatenation yields substantial gains on translation metrics, but offers limited benefits for general monolingual abilities and cross-lingual abilities. This result suggests that while parallel-form alignment signals directly build translation ability, they do not readily transfer into broader multilingual competence through standard learning process. Motivated by this gap, we propose a pragmatic multi-step pipeline to leverage the translation ability induced by parallel data in a data-driven perspective, which consistently improves general monolingual and cross-lingual performance. Our findings clarify the role and limits of parallel data in MLLM pretraining and offer a practical recipe for building more comprehensively capable multilingual models.", "tldr": "", "keywords": ["multilingual large language models", "parallel data", "pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d0a5f089a519b7f2490ec759292c2c55ec7fa91.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The presented paper presents a thorough study on synthetic multilingual parallel data for LLM training. The authors find that parallel data is mostly beneficial for translation capabilities and does not generalize well to other multilingual benchmarks. Additionally, they propose a framework that boosts general monolingual and cross-lingual evaluation quality by mixing monolingual, code-switched, and parallel training data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Nicely presented results on multilingual synthetic parallel data\n- Thorough experiments on different settings to incorporate parallel data\n- Interesting finding that parallel data is mostly helpful for translation capabilities but not other multilingual benchmarks"}, "weaknesses": {"value": "### Weaknesses\n\n- To me it is unclear how \"Standard\" is concatenated. For an `en-de` parallel document with multiple paragraphs `<en_p1>, <en_p2>, <en_p3>` and `<de_p1>, <de_p2>, <de_p3>` is it `<en_p1>, <en_p2>, <en_p3>, <de_p1>, <de_p2>, <de_p3>` or `<en_p1>, <de_p1>, <en_p2>, <de_p2>, <en_p3>, <de_p3>`, i.e. document-level concatenation or interleaving? Both of these approaches might be worth investigating.\n- Flores is a sentence level translation testset, it would be beneficial to see how well the ablations/proposed approach works on document-level translation. My assumption is that the synthetic parallel data is mostly sentence-level which is more aligned with the Flores payloads but if the synthetic data would be document level we would see bigger benefits on broader multilingual benchmarks.\n- Section 4 Step 2 includes code-switched data where the English sentence is replaced with its translation with 50% probability. I have a few concerns about this approach specifically A) For some language pairs e.g. `en-ja` there might be severe re-structuring happening within the context of a paragraph i.e. the translation might condense multiple English sentences into a single Japanese sentence or change the order of the English sentences to produce coherent Japanese text. How are these edge cases handled? In theory, this isn't too different than the interleaving approach outlined above? B) If we add code-switched data it increases the models probability to produce code-mixed outputs (which is likely not desired?). Are there edge cases observed where the model outputs code-mixed responses or how can we be sure to avoid this behavior?\n- It is unclear where are the gains from Table 3 for General Monolingual Ability & General Cross-lingual Ability are coming from. Is it mainly the code-switched or the monolingual data that was generated? How is the monolingual data different than the split approach? Is the English side discarded?\n\n\n### Minor Comments\n\n- Figure 1 has cut-off % signs on the right side\n- Table 1 typo `FloresTranlate` -> `FloresTranslate`"}, "questions": {"value": "Part of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PopnRwL6t3", "forum": "1gbJ8euERb", "replyto": "1gbJ8euERb", "signatures": ["ICLR.cc/2026/Conference/Submission11046/Reviewer_XjaE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11046/Reviewer_XjaE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760490154546, "cdate": 1760490154546, "tmdate": 1762922227335, "mdate": 1762922227335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of parallel data in pre-training multilingual large language models (MLLMs). It systematically compares the standard practice of concatenating translation pairs with treating each side as an independent sample. The experiments reveal that while parallel data significantly improves translation performance, it does not directly enhance general monolingual competence or broader cross-lingual abilities. Building on this insight, the authors propose a practical pipeline that leverages translation capability from parallel data to synthesize targeted corpora, resulting in improved general multilingual performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- generally easy to follow.\n- Extensive experiments across 18 languages and multiple pre-training configurations provide robust and consistent evidence.\n- The ablation studies and controlled comparisons between concatenated vs. independent parallel data are thorough and informative."}, "weaknesses": {"value": "While the study is extensive, additional discussion on how the method scales with very large corpora or extremely low-resource languages could be helpful."}, "questions": {"value": "- I am curious to see whether this approach has been tested at the document level; most experiments seem sentence-level. Would document-level evaluation yield different insights?\n- Have you tried to increase larger proportion of parallel data > 20%?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U4tNCtZZkY", "forum": "1gbJ8euERb", "replyto": "1gbJ8euERb", "signatures": ["ICLR.cc/2026/Conference/Submission11046/Reviewer_QzCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11046/Reviewer_QzCn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975008437, "cdate": 1761975008437, "tmdate": 1762922226926, "mdate": 1762922226926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the use of parallel data in language modeling. They:\n- Ablate on using parallel data as is\n- Suggest that using synthetic monolingual data is more useful\n- They train models upto 8B, 300B tokens scale to study this."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Ablations are extensive and at a model scale that is more applicable to modern LMs.\n- The main utility of this paper to practitioners is scaling up pretraining with synthetic data. However, this study is somewhat limited. Deepening this study would greatly improve the draft."}, "weaknesses": {"value": "- The novelty of this paper is limited compared to the most Related Works that it cites, and is misses significant related work.\n- Kale et al 2021 already show that the parallel data matters less with scale.\n- In addition, the writing is unclear and feels rushed.\n\nMissed Related Work:\n- Work on Self-supervision in MT (https://arxiv.org/abs/2005.04816) and Translationese introducing biases in evaluation as background for using non-English synthetic data. An older survey with useful pointers: https://www.cfilt.iitb.ac.in/resources/surveys/2024/Survey%20Meet%20SyntheticData%202024.pdf\n- Using synthetic monolingual data for LM pretraining is a known technique, Eg. https://arxiv.org/abs/2403.13638 -  Even so, I feel this is the more interesting part of the paper and that this should have been explored further at a smaller scale.\n\nTypos And Presentation:\n- DATA PREPAREATION ---> DATA PREPARATION\n- Focus on monolingual ability is unclear\n- Figure on model proportions can be moved to appendix"}, "questions": {"value": "- What does equitable mean in this context (line 115)?\n- Do you have results split by language? Which languages improve the most? Does code-mixed data matter? or just using more synthetic data?\n- Why not use monolingual data as a baseline?  So far, the experiments mainly tell us that the amount of data in the non-English side matters, and you suggest using back/forward-translation, which would indeed likely increase the amount of data for low resource languages in the benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lj3EfLEaKo", "forum": "1gbJ8euERb", "replyto": "1gbJ8euERb", "signatures": ["ICLR.cc/2026/Conference/Submission11046/Reviewer_d1tf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11046/Reviewer_d1tf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154293830, "cdate": 1762154293830, "tmdate": 1762922226454, "mdate": 1762922226454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the gain of using parallel data for multilingual LLMs pretraining and finds that while parallel data gains on translation tasks they don't transfer into gains on other tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• A valuable topic to study how parallel data affect the performance of pretrained multilingual LLMs\n\t• Conducted experiments clearly suggest that (1) parallel data only help translation tasks; (2) standard concatenation of parallel data help translation performance significant more; (3) neither standard concetenation nor split protocol of parallel data can help with tasks other than translation."}, "weaknesses": {"value": "• The proposed remedy approach seem to improve multilingual LLMs performance marginally. More study is needed. \n\t• The study is limited on parallel data's contribution to training LLMs in two forms only: standard and split. Subtlties regarding finner grain of properties of the training data, such as overlapping of tokens and other statistics might be helpful to understand the  nature of parallel data towards training LLMs more."}, "questions": {"value": "1. Section 3.4, please clarify the evaluation protocol. Any finetuning after the pretraining? Or just using prompt template with the pretrained models for translation and other tasks? \n\t2. In all tables, please clarifiy the \\Delta column: (1) average of the difference on individual taks or target languages, or (2) the difference of average over all tasks or over all target languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FBlm56D8H2", "forum": "1gbJ8euERb", "replyto": "1gbJ8euERb", "signatures": ["ICLR.cc/2026/Conference/Submission11046/Reviewer_Yeax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11046/Reviewer_Yeax"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762765429444, "cdate": 1762765429444, "tmdate": 1762922225393, "mdate": 1762922225393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}