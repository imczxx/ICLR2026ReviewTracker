{"id": "uik3mfqaBE", "number": 21526, "cdate": 1758318519589, "mdate": 1759896917539, "content": {"title": "On alignment of unified multimodal large language models", "abstract": "Unified Multi-Modal Large Language Models (U-MLLMs) have demonstrated remarkable capabilities in text-to-image (T2I) generation, yet their safety alignment remains under-explored. As these models become increasingly powerful, the potential for generating toxic or harmful content grows correspondingly. Current T2I alignment methods primarily focus on enhancing image quality while neglecting safety considerations. Moreover, the reward signals employed in existing approaches are typically sparse, providing only a single score per image, which limits the granularity of feedback. This paper introduces a novel approach that integrates dense rewards into a Group Relative Policy Optimization (GRPO) framework for improving image quality, incorporating safety-specific reward signals to enhance safety alignment. Our method transforms dense reward into token-level weights that modulate the training process, enabling fine-grained optimization that suppresses problematic regions while focusing learning on well-aligned image regions. Experiments demonstrate strong performance: our method achieves competitive quality metrics (WISE: 0.50) while reducing unsafe content generation by 59.4\\% on the MMDT benchmark. This work advances both the quality and safety of U-MLLMs demonstrating a comprehensive approach for U-MLLM alignment.", "tldr": "", "keywords": ["unified multimodal large language models", "alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0fcad5b729c2567fc7d9148521f2e9d68d4239c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies post-training alignment of unified multimodal LLMs for text-to-image (T2I). It injects dense visual attributions (RAHF heatmaps, SHAP/LIME) into GRPO by converting attribution scores into token-level weights during policy optimization. For safety, it adds a composite negative reward combining Toxic-BERT and an NSFW detector. Across WISE, GenAI-Bench, MMDT, and T2I-Safety, the method maintains or improves image quality while reducing unsafe generations on MMDT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Clear, well-specified token-weighted GRPO with explicit equations and design choices.\n\n2.Safety reward is simple, transparent, and easy to implement.\n\n3.Substantial safety gains reported on MMDT.\n\n4.Broad benchmark coverage (WISE, GenAI-Bench, MMDT, T2I-Safety) with ablations."}, "weaknesses": {"value": "1.Limited novelty: the improvement over T2I-R1/GRPO appears small: (1) reuse of GRPO; (2)token weights derived from standard attributions (RAHF/SHAP/LIME); (3) a straightforward safety penalty using off-the-shelf classifiers.\n\n2.Missing experimental details: batch size, training steps, and prompt curation specifics (e.g., for T2I-CompBench).\n\n3.Underspecified safety hyperparameters: weights $w_{toxic}$ and $w_{nsfw}$ are not clear. \n\n4.Scope mismatch: the paper frames U-MLLM alignment broadly but does not evaluate I2T alignment."}, "questions": {"value": "1.Beyond reweighting, how does token-weighted GRPO change optimization dynamics relative to scalar-reward GRPO？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "q9wXqTEqwj", "forum": "uik3mfqaBE", "replyto": "uik3mfqaBE", "signatures": ["ICLR.cc/2026/Conference/Submission21526/Reviewer_71ZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21526/Reviewer_71ZQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563100898, "cdate": 1761563100898, "tmdate": 1762941819310, "mdate": 1762941819310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for addressing safety alignment in unified multimodal large language models (U-MLLMs), which are designed to process both text and image modalities within a single architecture. The authors propose a reinforcement learning approach named Dense-GRPO, an extension of Group Relative Policy Optimization (GRPO) that incorporates token-level dense reward weighting. This dense reward is derived from visual attribution methods—such as SHAP, LIME, and RAHF—and is intended to deliver fine-grained feedback for safety-aware training.\nComprehensive experiments conducted across multiple benchmarks, including MMDT, WISE, GenAI-Bench, and T2I-Safety, demonstrate a substantial improvement in safety metrics—with up to approximately 59% reduction in unsafe generations—while only minimally compromising image quality. The paper argues that fine-grained reward modeling enables the joint optimization of safety and visual quality in multimodal alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Focusing on the vital issue of safety alignment for multimodal large language models, this paper compellingly bridges a gap in a field that has largely centered on text-only models. The authors' approach to jointly optimizing safety and quality within a single RL framework is well-conceived. Their innovation of dense, token-level feedback effectively addresses the challenge of sparse rewards in multimodal contexts. Targeting a key obstacle to the real-world deployment of unified models, this research represents a valuable contribution to both theoretical and applied AI safety."}, "weaknesses": {"value": "While the paper is well-motivated, its technical formulation and experimental validation remain limited.\n\n First, the claimed dense reward is only used as token-level weighting rather than integrated into the advantage computation. This represents a conceptual misuse of “dense rewards” and does not address sparse-return issues in reinforcement learning.\n\nSecond, the “Dense-GRPO” method introduces only marginal modifications to GRPO, lacking substantial algorithmic innovation.\n\nThird, since the approach essentially reweights unsafe samples, a weighted Supervised Fine-Tuning (SFT) baseline should have been included to evaluate whether reinforcement learning is truly necessary.\n\nMoreover, the study does not examine how the weighting term influences policy stability, convergence behavior, or reward variance.\nOn the experimental side, the emphasis is placed heavily on safety metrics, while standard image-quality assessments such as Geneval are overlooked. Additionally, the individual contributions of different reward components (e.g., those based on SHAP, LIME, and RAHF) are not adequately disentangled.\n\nOverall, while the work offers promising empirical findings, it lacks the methodological depth and theoretical grounding needed to fully substantiate its claims."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GnLKn9GPtl", "forum": "uik3mfqaBE", "replyto": "uik3mfqaBE", "signatures": ["ICLR.cc/2026/Conference/Submission21526/Reviewer_Jysv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21526/Reviewer_Jysv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811932292, "cdate": 1761811932292, "tmdate": 1762941818989, "mdate": 1762941818989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a token-weighted GRPO framework for aligning Unified Multimodal Large Language Models (U-MLLMs) in text-to-image (T2I) generation. The method introduces dense, spatially localized rewards (from RAHF and SHAP/LIME attributions) and safety-specific penalties (from toxic-CoT and NSFW detectors). By assigning token-level weights during GRPO optimization, the model seeks to improve both image quality and safety alignment. Experiments on several benchmarks (WISE, GenAI-Bench, MMDT, and T2I-Safety) show improved safety and comparable visual quality relative to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses a meaningful and timely problem of balancing visual quality and safety alignment in multimodal LLMs through a clear and conceptually simple framework.\n2. The token-weighted GRPO design is straightforward and can be easily integrated into existing RLHF-style pipelines, offering a practical engineering solution.\n3. The paper reports results across multiple benchmarks, showing that the approach generalizes to both quality and safety objectives."}, "weaknesses": {"value": "1. The approach is more of an engineering extension of existing GRPO/DPO frameworks rather than a fundamentally new algorithm.\n2. Safety gains might stem from reusing the same toxic/NSFW evaluators in both training and testing, and key training details (e.g., λ, β schedules, G×K, random seeds) are missing.\n3. The framework's applicability to diffusion or flow-based models is claimed but not validated.\n4. The experiments are limited to Janus-Pro-7B.\n5. The model improves safety but may over-suppress valid or creative outputs; this balance is not analyzed."}, "questions": {"value": "1. Can authors please clarify the theoretical motivation for token-level weighting—e.g., how localized rewards stabilize optimization or mitigate sparse-signal variance?\n2. Is it possible to include independent cross-evaluator tests and provide training hyperparameters to ensure reproducibility?\n3. Can authors please provide a brief validation or discussion of how token weighting would transfer to diffusion backbones or other U-MLLM architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ywRbdnZSGU", "forum": "uik3mfqaBE", "replyto": "uik3mfqaBE", "signatures": ["ICLR.cc/2026/Conference/Submission21526/Reviewer_3D7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21526/Reviewer_3D7f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910532132, "cdate": 1761910532132, "tmdate": 1762941818629, "mdate": 1762941818629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses safety and quality alignment in Unified Multimodal Large Language Models (U-MLLMs)—models capable of both image-to-text (I2T) and text-to-image (T2I) generation. The authors argue that while recent U-MLLMs achieve strong generative performance, their safety alignment remains under-explored, and existing reinforcement learning (RL)-based alignment methods rely on sparse scalar rewards.To address these issues, the paper proposes a token-level dense reward framework integrated into Group Relative Policy Optimization (GRPO). Experiments on benchmarks such as WISE and MMDT show competitive image quality (WISE score: 0.50) and a 59.4% reduction in unsafe content compared to the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The implementation of usin fine-grained reward within GRPO is technically sound. \n2. The ensemble of reward models (Table 1)—spanning aesthetic, compositional, grounding, and safety signals—demonstrates significant engineering rigor. The dual-path evaluation (safe vs. unsafe prompts) is thoughtful.\n3. The paper is clearly written, with intuitive figures (e.g., Fig. 2–3) and a logical flow from problem formulation to method to evaluation. The distinction between quality-oriented and safety-oriented reward pathways is well articulated."}, "weaknesses": {"value": "1. The central claim that “safety alignment has been under-explored” appears overstated. While U-MLLMs may be a recent architecture, T2I safety alignment has been actively studied [1,2]. These and other works suggest that safety in T2I is not unexplored, even if not yet fully adapted to autoregressive U-MLLMs. Alternatively, in my view, the safety problem in U-MLLMs is not fundamentally different from that in standalone LLMs, T2I models, or I2T models. Therefore, existing safety alignment frameworks developed for LLMs, T2I, or I2T models are largely applicable to this setting.\n2. Loose Coupling Between Contributions: The paper presents two seemingly orthogonal contributions: (1) Introducing dense rewards for fine-grained quality optimization; (2) Adding safety-specific rewards to suppress harmful content. However, these are not meaningfully integrated. A more compelling story would be: “Existing RL alignment for U-MLLMs lacks fine-grained safety signals; we propose dense safety-aware rewards that jointly optimize quality and safety at the token level.” Instead, safety remains coarse-grained, undermining the paper’s emphasis on “dense” alignment.\n\n> [1] Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding proposes prompt-level safety intervention via embedding projection.\n> [2] AlignGuard: Scalable Safety Alignment for Text-to-Image Generation introduces scalable red-teaming and safety fine-tuning for diffusion models."}, "questions": {"value": "No question"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lcpcOHIdgS", "forum": "uik3mfqaBE", "replyto": "uik3mfqaBE", "signatures": ["ICLR.cc/2026/Conference/Submission21526/Reviewer_jAMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21526/Reviewer_jAMc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922748761, "cdate": 1761922748761, "tmdate": 1762941818396, "mdate": 1762941818396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}