{"id": "KzpAcxWjOC", "number": 8840, "cdate": 1758099677679, "mdate": 1759897760532, "content": {"title": "ESI-Bench: A Comprehensive Video Benchmark for Emotional and Social Intelligence", "abstract": "Recent work has increasingly focused on the evolution and intelligent modeling of interpersonal interaction and collaboration. Driven by advances in Multimodal Large Language Models (MLLMs), emotional intelligence (EI) and social intelligence (SI) have emerged as core competencies for AI systems: they enable agents to modulate behavior in complex contexts, infer others’ intentions, maintain social relationships, and ultimately support natural human-machine interaction and seamless collaboration. To systematically investigate AI capabilities and pathways for understanding EI and SI, the community has introduced benchmarks such as EQ-Bench, Social-IQ 2.0, and V-Social, advancing research on emotion understanding, social behavior modeling, and social common sense reasoning. However, existing approaches generated datasets exhibit limited semantic separability between options, low question answer relevance (QA relevance), low dataset complexity (with extremely high accuracy), low ground truth correctness,narrow modality coverage, and pronounced inherent biases. Meanwhile, these data construction pipeline suffer from high annotation costs and lengthy data-collection cycles. To address the shortcomings of the existing evaluation datasets, We introduce ESI-Bench, a benchmark comprising 1,105 videos and 5,490 meticulously generated QA pairs. It offers accurate cross-modal alignment, high semantic separability, strong QA relevance, reliable ground truths, and substantially reduced inherent bias, enabling clear performance stratification across state-of-the-art (SOTA) models.We also propose a semi-automated, high-efficiency data generation framework. Our framework integrates multiple models (open-source and closed-source) with complementary strengths and couples them with a lightweight manual verification loop,enabling low-cost, large-scale construction of high-quality emotional social intelligence datasets. This work provides a scalable paradigm for constructing rigorous emotional and social intelligence (ESI) evaluations and aims to advance research toward more capable human-AI interaction.", "tldr": "Addressing poor AI emotional and social intelligence benchmarks, proposes a high-efficiency,semi-automated framework to generate high-quality datasets, introducing ESI-Bench benchmarks  for improved evaluation and advancing human-AI interaction", "keywords": ["Emotional and Social Intelligence (ESI)", "AI Benchmark", "Semi-automated Data Generation Framework", "ESI-Bench"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb245a108793fc3ed941c40194ba8483051e0e1e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors identify key limitations in existing benchmarks—namely, narrow capability coverage and insufficient fine-grained analysis—and propose ESI-Bench, a comprehensive benchmark for evaluating emotional and social intelligence (EI/SI), which are essential for achieving human-level understanding. ESI-Bench offers high-quality, fine-grained, and broad-coverage evaluation across five dimensions: behavior and intention, interaction dynamics, emotional responses, emotion analysis, and action intent. It is constructed through a semi-automatic multimodal pipeline that integrates data acquisition, MLLM-assisted annotation, and multi-stage quality control, providing a robust benchmark for assessing emotional and social intelligence in vision-language models (VLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The tables are clear and easy to understand\n2. The details, such as the model and prompt used to build up the benchmark and evaluation, are complete."}, "weaknesses": {"value": "1. Why is it essential to analyze emotional intelligence (EI) and social intelligence (SI) holistically? Based on the current question design, some items seem to focus solely on EI or SI rather than their integration under ESI. It would be helpful for the authors to clarify what advantages are gained from joint evaluation compared to assessing EI and SI separately using dedicated datasets.\n\n2. Based on the provided metadata, it appears to include information such as tone, spoken words, speaker identity, and corresponding start–end frame indices. However, it is unclear how these data were collected. In particular, not all speech segments necessarily include a visible human face, raising questions about how speaker associations and tone annotations were reliably established in such cases.\n\n3. In Table 2, the lower performance results may indicate that the task is inherently more challenging or that the problem formulation is suboptimal. It would be helpful to clarify which of these factors contributes to the observed outcomes. Moreover, the ranking of models is inconsistent with their performance on Social-IQ2.0, suggesting possible differences in task design or evaluation metrics. How should model performance be interpreted in this context? Including human performance as a reference would provide valuable insight into what constitutes “good” performance on this benchmark.\n\n4. The dataset construction process relies heavily on LLMs, expecting them to assess question difficulty, verify ground-truth correctness, and distinguish between answer options. These steps are primarily carried out through prompt-based evaluation, similar to LLM-as-a-judge. However, the reliability of the generated outputs remains questionable, as LLM-based judgments may introduce inconsistencies or biases."}, "questions": {"value": "1. How can one determine whether the low performance of MLLMs arises from the limited number of frames (e.g., 16, potentially missing key frames), a lack of perceptual capability, or an insufficient understanding of emotional and social intelligence (ESI)?\n\n2. In the appendix and main paper, the data sources are derived from Social-IQ and Social-IQ 2.0, but I guess it should be Social-IQ2.0 and V-Social\n\n3. It would be better to avoid including too many related works in the introduction, as the long paragraph currently reads more like a dedicated Related Work section.\n\n4. It would be better to improve the figure alignment and layout to make the presentation clearer and easier to follow"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k12N8PWZpm", "forum": "KzpAcxWjOC", "replyto": "KzpAcxWjOC", "signatures": ["ICLR.cc/2026/Conference/Submission8840/Reviewer_Hs1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8840/Reviewer_Hs1u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521079116, "cdate": 1761521079116, "tmdate": 1762920608319, "mdate": 1762920608319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce ESI-Bench, a video QA dataset with 1,105 videos and 5,490 questions designed to evaluate the emotional and social intelligence (ESI) of Multimodal Large Language Models (MLLMs). The dataset was created via a semi-automated pipeline that the authors claim is low-cost and scalable. The authors suggest ESI-Bench offers higher quality, less bias, and greater difficulty than prior benchmarks, with state-of-the-art MLLMs achieve only 50-60% accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset is large-scale and features dense annotations, including metadata on speaker attributes and tone.\n- The automated QA generation pipeline is supplemented with manual quality reviews.\n- The low performance of SOTA models (50-60%) confirms that the benchmark is difficult and there is room for improvement\n- The authors included an ablation study on the annotation pipeline, which is welcomed"}, "weaknesses": {"value": "- My major concern is the paper's insufficient motivation for creating ESI-Bench. The authors critique existing social benchmarks for vague limitations like inherent bias, limited sensitivity for social signals, and low cross-modal alignment, but they provide little evidence for these claims. The paper offers no new insights into how these limitations arise, and how ESI-Bench effectively mitigates them.\n- For example, while the authors claim Social-IQ 2.0 has inherent biases (citing its high\"no context and question\" accuracy), they do not fully engage with prior work like DeSIQ that has already attempted to address this. ESI-Bench's relatively simple adversarial debiasing approach also provides no guarantee that ESI-Bench is immune to inherent biases, especially as MLLMs improve on their text reasoning capabilities. \n- The paper's definition of ESI is vague. While the authors cited the Bar-On model as motivation, the proposed ESI categories do not align with the Bar-On dimensions. Furthermore, the categories themselves do not appear to be mutually exclusive. For example, there is little clear distinction between “Emotion and Behavioral Response” and “Emotion and Behavior Analysis.”\n- The claims in Table 2 that ESI-Bench has higher quality and less bias are misleading. Since the dataset was constructed specifically to optimize for these metrics in Table 2, the comparison feels circular and is not an independent validation of its quality.\n- The paper lacks clarity in key areas. It is unclear which models were used to extract metadata (e.g., transcripts and expressions). The definitions for quality metrics, such as \"inherent bias\" and \"QA relevance,\" are also vague. Many of these crucial details are moved to the appendix and should be moved into the main paper for clarity."}, "questions": {"value": "- Since ESI-Bench is sourced from Social-IQ and Social-IQ 2.0, could the author compared ESI-Bench’s debiasing setup with the more complex perturbation methods used in DeSIQ? And how does ESI-Bench address limitations such as \"limited sensitivity to non-explicit social signals\" and \"cross-modal alignment accuracy remains inadequate\"?\n- How is ESI-Bench’s ESI definition materially different from that of Social-IQ?\n- Given that the quality metrics in Table 2 are automated and ESI-Bench was optimized for them, how would human annotators independently rate the quality (e.g., QA relevance, bias) of ESI-Bench compared to prior benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8jXUFjLgA", "forum": "KzpAcxWjOC", "replyto": "KzpAcxWjOC", "signatures": ["ICLR.cc/2026/Conference/Submission8840/Reviewer_oJyL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8840/Reviewer_oJyL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781722948, "cdate": 1761781722948, "tmdate": 1762920608032, "mdate": 1762920608032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ESI-Bench, a multimodal benchmark designed to evaluate Emotional and Social Intelligence (ESI) in Multimodal Large Language Models (MLLMs).\nIt includes 1,105 videos and 5,490 QA pairs, covering five ESI dimensions (Intention & Function of Action, Behavior-Reaction-Intention, Emotion & Behavioral Response, Emotion & Behavior Analysis, Interaction Dynamics & Context).\nThe authors claim contributions in:\n\nA high-quality, broad-coverage benchmark with low inherent bias and high semantic separability.\n\nA semi-automated data construction pipeline combining LLM/VLM generation, multi-stage validation, and limited human review.\n\nComprehensive evaluations on top-tier models (GPT-4o, Gemini, Doubao, Grok, Claude) showing that ESI-Bench is harder and more discriminative than Social-IQ 2.0 or V-Social."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\nThe paper argues that existing ESI-related benchmarks (Social-IQ, V-Social, SIV-Bench, EQ-Bench, etc.) fail to differentiate advanced models because they are either too easy, too biased, or lack multimodal grounding.\n\nIt correctly identifies an important research gap: evaluating fine-grained emotional and social reasoning in MLLMs, which is underexplored compared to general video reasoning (MVBench, MMBench-Video).\n\nThe authors situate ESI as a core cognitive ability for AGI—an angle aligned with ICLR’s interest in emergent intelligence and evaluation paradigms. This is indeed a timely topic."}, "weaknesses": {"value": "1. Weak theoretical foundation. The paper references psychological theories such as Bar-On (2006) and Fiske (1992), but these frameworks are not operationalized in the benchmark design. I cannot find the source of their claimed five “ESI dimensions” in the mentioned reference, which do not correspond to any validated theoretical or psychological constructs.\n\n2. Concern about the overclaimed coverage of the benchmark. According to the five dimensions, there is no evidence that the dataset actually measures emotional or social intelligence as defined in psychology (the commonly known social intelligence and emotional intelligence) — i.e., no mapping to empathy, emotion regulation, theory of mind, or social reasoning. As a result, the benchmark’s construct validity is highly questionable.\n\n3. Design remains surface-level. Following the above issue, the provided QA examples (Appendix A.5, e.g., “When does the man adjust the blindfold?”, “What is the woman’s emotion?”) test only basic perception — such as recognizing actions, emotions, or temporal sequences. There is no evidence of deeper reasoning about, e.g., dynamic emotional change over time, motivational understanding, goal inference, interpersonal causality (how one’s emotion influences another’s response), or social situational adaptation. Thus, the benchmark primarily reflects action recognition or static emotion labeling, not emotional or social intelligence. \n\n4. Derivative dataset and limited novelty. The benchmark reuses Social-IQ and Social-IQ 2.0 videos, with only new QA annotations. Therefore, it is essentially a re-annotation and filtering of existing data, not a fundamentally new dataset or paradigm. For ICLR, the level of innovation is incremental rather than groundbreaking.\n\n5. Lack of psychological or human baseline validation. There is no human performance baseline, inter-annotator agreement, or correlation with human emotional/social reasoning ability. Without such evidence, it is unclear whether the benchmark truly captures human-aligned ESI evaluation.\n\n6. Lack of technical contribution. The proposed benchmark evaluates various MLLMs; however, there is no novel technical solution to address emotional social intelligence.\n\n\nOthers:\nIt seems like the authors mixed up the terminology of social intelligence, emotional intelligence, and emotional social intelligence. Please try to differentiate them and give a clear definition. For instance, the origin of social intelligence, Edward Thorndike, is not referred to in this paper\nTypos can be observed in the paper, e.g., ln 29 and 171. In 387, the performance is not the best, but is marked in bold."}, "questions": {"value": "I appreciate the authors’ efforts to tackle this challenging and timely topic, which is indeed of great importance to the community. However, addressing emotional and social intelligence in AI is inherently a highly complex and demanding task, requiring not only careful data construction but also deep theoretical grounding, psychological validation, and multi-level reasoning design.\nWhile the paper takes an admirable step in this direction, the current version does not yet meet the high bar of conceptual and methodological rigor such a problem demands.\n\nBelow are the questions for the authors:\n\n1. How are the five “ESI dimensions” grounded in established psychology literature? Could you provide explicit references and mappings?\n\n2. How many QA pairs actually involve emotion reasoning or social inference, beyond simple perception?\n\n3. Did human raters verify that the questions require emotional or social reasoning rather than factual observation?\n\n4. Could the authors provide concrete examples where a model must infer motivation or relationship dynamics to answer correctly?\n\n5. How does ESI-Bench differ substantively from Social-IQ 2.0 besides re-generation of questions and bias filtering?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KKsd6p7YNt", "forum": "KzpAcxWjOC", "replyto": "KzpAcxWjOC", "signatures": ["ICLR.cc/2026/Conference/Submission8840/Reviewer_yrRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8840/Reviewer_yrRv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948318869, "cdate": 1761948318869, "tmdate": 1762920607703, "mdate": 1762920607703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive video benchmark designed to evaluate Emotional and Social Intelligence in MLLMs. The benchmark comprises 1,105 videos and 5,490 QA pairs, structured along five ESI dimensions. The authors also propose a semi-automated data construction pipeline that leverages multiple MLLMs for annotation, validation, and filtering, resulting in a dataset with high semantic separability, low inherent bias, and strong QA relevance. Experimental results show that ESI-Bench is more challenging than existing benchmarks, with SOTA models achieving low zero-shot accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "（1）\tThis benchmark demonstrably poses a significant challenge to existing SOTA models.\n（2）\tThe paper includes model performance dataset diagnostics evaluations, providing a holistic view of the benchmark's properties.\n（3）\tThe multi-stage generation and validation process, incorporating cross-model checks and human review, is a robust approach to ensuring data quality and reducing biases."}, "weaknesses": {"value": "（1）\tThe five-dimensional ESI framework is presented without deep grounding in psychological literature, it feel more like an engineering construct than a scientifically validated model.\n（2）\tThe evaluation lacks comparison with a wider range of recent multimodal video benchmarks, failing to fully contextualize its contribution and difficulty.\n（3）\tFigure 3 are rudimentary and lack quantitative depth, undermining the presentation of statistical claims and falling below the expected standard for visual clarity."}, "questions": {"value": "The comparison is currently limited to Social-IQ 2.0 and V-Social. To better situate ESI-Bench within the field, could the authors include performance comparisons on other recent and relevant video understanding benchmarks? This would help concretely demonstrate that the observed performance drop is due to the unique challenges of ESI rather than general video reasoning difficulties."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zVqN49loRI", "forum": "KzpAcxWjOC", "replyto": "KzpAcxWjOC", "signatures": ["ICLR.cc/2026/Conference/Submission8840/Reviewer_npj1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8840/Reviewer_npj1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332493579, "cdate": 1762332493579, "tmdate": 1762920607414, "mdate": 1762920607414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}