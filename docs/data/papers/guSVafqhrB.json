{"id": "guSVafqhrB", "number": 10940, "cdate": 1758185152017, "mdate": 1763517753058, "content": {"title": "Fine-Grained Activation Steering: Steering Less, Achieving More", "abstract": "Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)–level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.", "tldr": "Breaking LLM blocks to fine-grained atomic units for intervention: steering less achieves more", "keywords": ["Activation Steering", "Large Language Models", "Fine-Grained Intervention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9cada056ebe41df04f02a22bc25b2fe3d125857.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes AUSteer, a method for fine-grained activation steering in large language models. Instead of steering at the block level, AUSteer operates at the Atomic Unit (AU) level, corresponding to individual activation dimensions. The authors show that block-level activations mix helpful and harmful components, making coarse interventions inefficient. AUSteer identifies discriminative AUs through an activation momentum metric computed from contrastive pairs and applies adaptive per-AU scaling. Experiments on seven benchmarks and three model families demonstrate consistent improvements over block-level steering with far fewer activations, suggesting that steering less can achieve more."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is clearly defined and relevant. The idea of decomposing block activations into AUs is intuitive and well motivated. AUSteer is simple, interpretable, and does not require retraining. The experiments are broad and consistent across tasks and models, and the analysis convincingly shows heterogeneity within block activations."}, "weaknesses": {"value": "1) Efficiency claim lacks evidence:\n\nThe paper’s argument that a smaller steering footprint improves efficiency is not empirically verified. No inference-time or computational measurements are provided, and efficiency is used only in a representational sense.\n\n\n2) Lack of comparison with broader control variants. \n\nThe paper assumes that steering only a subset of AUs is inherently superior, but does not test a broader or fully generalized steering scheme where all AUs are jointly optimized or selectively weighted. Without such a comparison, it remains unclear whether partial AU control offers unique advantages beyond being a constrained version of more general steering"}, "questions": {"value": "1) Does efficiency refer to computational speed or representational precision?\n2) Have you measured inference cost, latency, or stability?\n3) Would steering all AUs with selective suppression perform similarly?\n4) How scalable is activation momentum computation for very large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b6xrjf7VXl", "forum": "guSVafqhrB", "replyto": "guSVafqhrB", "signatures": ["ICLR.cc/2026/Conference/Submission10940/Reviewer_xSGu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10940/Reviewer_xSGu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990058130, "cdate": 1761990058130, "tmdate": 1762922136401, "mdate": 1762922136401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AUSteer, a novel fine-grained activation steering method for LLMs that operates at the atomic unit level rather than the traditional block level (e.g., attention, FFN, or residual blocks). The authors identify a key limitation in existing steering methods: block-level activations are heterogeneous, mixing beneficial, irrelevant, and harmful components. As a result, conventional approaches (like CAA, SADI, or ITI) that steer all dimensions of a block simultaneously are coarse, inefficient, and potentially harming model performance.\n\nTo address this, AUSteer decomposes each block into fine-grained AU-level activations, where each AU corresponds to a single column of the weight matrix and each activation is a scalar. The method consists of two main components:\n\n-AU Localization via Activation Momentum: A metric that measures the discriminative power of each AU across positive and negative contrastive samples. It identifies which AUs consistently promote or suppress desirable activations.\n-Adaptive Steering : Instead of applying a fixed vector, AUSteer adjusts steering strength per input and per AU, scaling the intervention by the activation’s current value and discriminative score.\n\nExperiments are conducted on various LLMs (LLaMA2, Gemma2, Qwen3) and tasks, including commonsense reasoning, math problem-solving, and open-ended generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clearly identifies a fundamental issue: heterogeneity in block activations—and systematically decomposes it into atomic units.\n\n- Introduces the concept of activation momentum to measure discriminative importance without training.\n\n- Extensive experiments across three model families (LLaMA, Gemma, Qwen) and multiple tasks (reasoning, math, safety, alignment).\n\n- No retraining or fine-tuning required.\n\n- Ablation studies isolate the contribution of both components."}, "weaknesses": {"value": "- The formal derivation connecting activation momentum to discriminative causality is unclear.\n\n- AUSteer requires carefully curated positive–negative pairs, which may not be available or trivial to construct for all tasks.\n\n- While steering itself is efficient, computing activation momentum across many AUs and samples may still be computationally intensive for very large models.\n\n- Hyperparameter sensitivity is unclear and needs further demonstrations and explanations.\n\n- One wonders what is the runtime overhead for AU localization and steering per sample compared to block-level methods like SADI?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sz2zo3UMCj", "forum": "guSVafqhrB", "replyto": "guSVafqhrB", "signatures": ["ICLR.cc/2026/Conference/Submission10940/Reviewer_mmMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10940/Reviewer_mmMY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045897319, "cdate": 1762045897319, "tmdate": 1762922136039, "mdate": 1762922136039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, they present AUSteer, a more fine-grained activation steering technique, to control LLM's behavior during the inference time. First, they recognize the heterogeneity in block activation and explain this through comprehensive experiments. Inspired by these experiments, they developed a more fine-grained activation steering algorithm. In detail, first, use activation momentum to recognize the important atomic unit on the target tasks. Then, steer these atomic units' activation adaptively. They did comperihensive experiments to evaluate AUSteer. And the results are convincing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. They first use two sections to recognize and interpret the heterogeneity in block activation, which gives insight and inspiration for AUSteer.\n2. The method is natural and effective. \n3. The experiments are comprehensive, spanning three LLMs with different architectures and three different tasks."}, "weaknesses": {"value": "1. The biggest model used is 27B. Evaluating AUSteer on bigger models, e.g., 32B and 72B, and sparse models, e.g., MoE, even multi-modal models would be better.\n2. The optimal hyperparameters \\alpha and k are task-specific; how to set the hyperparameters for every tasks? And what is the hyperparameters used in Table. 1?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RzHdP0HC7j", "forum": "guSVafqhrB", "replyto": "guSVafqhrB", "signatures": ["ICLR.cc/2026/Conference/Submission10940/Reviewer_61go"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10940/Reviewer_61go"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10940/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762508573874, "cdate": 1762508573874, "tmdate": 1762922135557, "mdate": 1762922135557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary"}, "comment": {"value": "We thank all reviewers for their careful evaluation of our work. The paper is recognized for its **identification of activation heterogeneity and natural, effective methodology** (Reviewer 61go), its **novel decomposition into atomic units and extensive experiments** (Reviewer mmMY), and its **intuitive, well-motivated fine-grained design with simplicity and interpretability** (Reviewer xSGu). We appreciate the consistent recognition of both the method’s conceptual novelty and empirical strength.\n\nWe have carefully addressed all concerns raised by the reviewers and summarize the revisions below:\n\n1. **Scalability to larger and diverse LLMs:** We demonstrate AUSteer’s scalability on Qwen3-30B-A3B (sparse MoE) and Llama-3.3-70B-Instruct (4-bit quantized), confirming robust performance across architectures and model sizes.  \n2. **Theoretical and empirical justification of activation momentum:** We provide a clearer derivation and additional evidence showing how activation momentum contributes to discriminative causality and influences model outputs.  \n3. **Computation-overhead analysis:** We present detailed measurements showing that AUSteer achieves superior performance while requiring the least computational overhead among all baselines.  \n4. **Control-variant analysis:** We evaluate broader steering schemes (e.g., steering all AUs or large subsets) and show that effective steering must remain focused on task-relevant AUs, consistent with AUSteer’s design.  \n5. **Promotion vs. suppression:** Through theoretical reasoning and empirical results, we show that promoting beneficial AUs consistently outperforms suppressing unhelpful ones.  \n6. **Hyperparameter sensitivity and selection:** We provide expanded analyses and introduce a practical solution for hyperparameter selection in resource-constrained settings.  \n7. **Contrastive-sample construction:** Given that contrastive pairs are required by nearly all activation-steering methods, we offer a simpler, general, and empirically validated procedure for constructing them.\n\n**While the main motivations, insights, proposed method, core experiments, and contributions remain unchanged, the added discussions and clarifications strengthen the paper by explaining several aspects more clearly and deeply. All updates have been incorporated into the revised version and highlighted in blue. Thank the reviewers again for their valuable feedback.**"}}, "id": "VIng0UuEc3", "forum": "guSVafqhrB", "replyto": "guSVafqhrB", "signatures": ["ICLR.cc/2026/Conference/Submission10940/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10940/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission10940/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763520868717, "cdate": 1763520868717, "tmdate": 1763521003353, "mdate": 1763521003353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}