{"id": "5Tph6wFMOm", "number": 8925, "cdate": 1758102836390, "mdate": 1759897753469, "content": {"title": "SCoT: Teaching 3D-LLMs to Think Spatially with Million-scale CoT Annotations", "abstract": "Recent advances in 3D Large Language Models (3D-LLMs) show strong potential in understanding and interacting with 3D environments, yet their training data typically lack explicit reasoning processes, limiting complex spatial reasoning and task planning.\nTo address this, we annotate SCoT, a million-scale Chain-of-Thought dataset spanning three levels: a) Spatial Perception (what is there), recognizing object properties, relations, and scene attributes; b) Spatial Analysis (what does it mean), inferring rationality, functionalities, and physical implications; c) Spatial Planning (what should I do), integrating perception and reasoning for actionable strategies. Unlike prior datasets supervising only answers, SCoT annotates intermediate reasoning grounded in scene cues, specifically for analysis and planning tasks. Results show that CoT supervision greatly benefits complex analysis and planning but induces hallucinations and accuracy drops in simple perception. These findings highlight both the necessity and the nuanced challenges of scene-grounded reasoning for advancing 3D intelligence.", "tldr": "We present a million-scale 3D visual-language dataset with CoT annotations that unifies perception, analysis, and planning tasks to advance interpretable 3D intelligence.", "keywords": ["3D Large Language Model", "Chain-of-Thought", "Spatial Perception", "Spatial Analysis", "Spatial Planning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a88c6663689067a2bc5804929b91e2b910b8cd54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SCOT, a new million-scale dataset designed to teach 3D-LLMs to reason spatially. To address the limitations of existing datasets that lack explicit reasoning steps, SCOT provides structured Chain-of-Thought annotations across a three-level taxonomy of tasks: Spatial Perception, Spatial Analysis, and Spatial Planning. A key contribution is the use of scene-grounded annotations, which force the model to base its reasoning on verifiable 3D evidence, thus improving the transparency and accuracy of complex analysis and planning tasks. Experiments show that this method significantly enhances the model ability to perform complex spatial reasoning, while also demonstrating that overuse of CoT for simple perception can be detrimental."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces SCOT, a new million-scale dataset designed to teach 3D-LLMs to reason spatially. To address the limitations of existing datasets that lack explicit reasoning steps, SCOT provides structured Chain-of-Thought annotations across a three-level taxonomy of tasks: Spatial Perception, Spatial Analysis, and Spatial Planning. A key contribution is the use of scene-grounded annotations, which force the model to base its reasoning on verifiable 3D evidence, thus improving the transparency and accuracy of complex analysis and planning tasks. Experiments show that this method significantly enhances the model ability to perform complex spatial reasoning, while also demonstrating that overuse of CoT for simple perception can be detrimental."}, "weaknesses": {"value": "The scene-grounded reasoning lacks strong evidence, which relies on LLM judge evaluations rather than direct verification against ground-truth data, which may lead some problems on robustness and objectivity."}, "questions": {"value": "What is the impact of generating detailed CoT reasoning on inference efficiency, and how does this affect the suitability for real-time applications such as robotics?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YOCUW4G0Oa", "forum": "5Tph6wFMOm", "replyto": "5Tph6wFMOm", "signatures": ["ICLR.cc/2026/Conference/Submission8925/Reviewer_VyFW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8925/Reviewer_VyFW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793754045, "cdate": 1761793754045, "tmdate": 1762920675674, "mdate": 1762920675674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a million-scale chain-of-thought dataset aimed at improving the spatial reasoning capabilities of existing 3D LLMs, along with a baseline model, SCoT-Reasoner. Experiments show that models fine-tuned on the SCoT dataset exhibit performance improvements across various 3D visual question answering benchmarks. Additionally, these fine-tuned models exhibit reasoning processes that are transparent, faithful to the scene, and inherently more trustworthy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed SCoT dataset covers a diverse set of spatial tasks and is large in scale. The paper provides sufficient details to enable the research community to reproduce the data generation pipeline.\n2. The paper is well-presented and easy to follow."}, "weaknesses": {"value": "1. SCoT is based on only 800 scenes from ScanNet, which is a relatively small scale. While the number of CoT examples is large, I’m concerned whether such a limited range of scene samples can genuinely enhance models' spatial reasoning ability on unseen real-world 3D environments.\n2. In Section 3, the author describes the SCoT format as “Query–CoT–Answer.” However, the spatial perception data seems to include only QA pairs without explicit reasoning. Clarification is needed on how these are treated as CoT examples.\n3. Most experiments are conducted on the SCoT test set. It would strengthen the paper if the authors included additional evaluations on out-of-domain datasets such as MSR3D [1] and Hypo3D [2] to assess generalization.\n4. The authors are encouraged to provide more discussion, ideally supported by results, demonstrating that fine-tuning on SCoT offers greater benefits for spatial reasoning compared to fine-tuning on previous 3D SCoT datasets.\n\n[1] Linghu, Xiongkun, et al. \"Multi-modal situated reasoning in 3d scenes.\" NeurIPS 2024.\n\n[2] Mao, Ye, et al. \"Hypo3D: Exploring Hypothetical Reasoning in 3D.\" ICML 2025."}, "questions": {"value": "1. Throughout the paper, the three main tasks are introduced in the order of SCoT-Perception, SCoT-Analysis, and SCoT-Planning. Why is a different order used in the results section (Section 5.2)? Maintaining consistency would improve readability.\n\n2. It is unclear which model is evaluated in Table 5. The authors should clarify the model configuration or variant used for these results.\n\n3. SCoT-Reasoner appears to be a key model in the evaluation, yet most of its technical details are relegated to the appendix. I recommend moving more of these details into the main Method section to help readers better understand the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LRyPnAxZB5", "forum": "5Tph6wFMOm", "replyto": "5Tph6wFMOm", "signatures": ["ICLR.cc/2026/Conference/Submission8925/Reviewer_A8Pi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8925/Reviewer_A8Pi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837118160, "cdate": 1761837118160, "tmdate": 1762920675127, "mdate": 1762920675127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose SCoT, a large-scale CoT datasets for spatial reasoning. It spanning three levels including spatial perception, spatial analysis and spatial planning. SCoT annotates intermediate reasoning grounded in scene cues. It introduces<SI> token let CoT grounded in scenes context, and reduce hallucinations. Results shows model trained on their proposed datasets benefits complex analysis and planning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper constructed a valuable large-scale datasets which including diverse scenes with their classified three-tier taxonomy of 3D task.\n\n2. This paper propose a detailed data construction pipeline which provides empirical insights for spatial reasoning data construction."}, "weaknesses": {"value": "1. The novelty is not very clear in this paper, could the author emphasize the key differences between this work and previous spatial reasoning CoT studies, as listed in Table 1 such as 3D-R1 and SpaceR-151k, they are also including multi tasks and reasoning, so what's the biggest advantage of your dataset compared with them? Have you compared with these datasets in controlled settings?\n\n2. The base model used to train is too weak to verify the usefulness of the proposed method. I don't understand why using Vicuna-7B as the pretrained model even in 2025 today. \n\n3. I doubt the validity of the evaluation. If I understand correctly, part of the evaluation data are self-constructed by yourself? (In the table 2, and table 3.) The public evaluation data only appear in Table 7, but over half of results in Table 7 your proposed method can't beat other methods. I think it's extremely unfair to compare with other methods if using your self-built data in the main results, considering we don't know whether the performance gain comes from in distribution benefits. Are training and test scenes completely disjoint?\n\n4. Using same models (GPT-4.1, DeepSeek, Qwen) generating training data as evaluator, which will further introduce bias in evaluation."}, "questions": {"value": "Refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2oxAFSeqcW", "forum": "5Tph6wFMOm", "replyto": "5Tph6wFMOm", "signatures": ["ICLR.cc/2026/Conference/Submission8925/Reviewer_qGaY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8925/Reviewer_qGaY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884955354, "cdate": 1761884955354, "tmdate": 1762920674514, "mdate": 1762920674514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SCoT, a million-scale Chain-of-Thought (CoT) dataset designed to teach 3D Large Language Models (3D-LLMs) to reason spatially.\nSCoT organizes tasks into three tiers — Spatial Perception (“what is there”), Spatial Analysis (“what does it mean”), and Spatial Planning (“what should I do”) — and annotates CoT reasoning only where necessary.\nThe dataset introduces scene-grounded reasoning with explicit \\<SI> tags to ensure factual alignment with 3D context.\nExperiments on multiple baselines (Chat3D, ChatScene, Video3D-LLM) and the proposed SCoT-Reasoner show significant improvements in reasoning explainability, faithfulness, and planning accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "High-quality dataset: 1.1M diverse samples covering perception, reasoning, and planning with strong annotation rigor and cross-checking.\n\nInnovative design: The three-tier CoT taxonomy (perception–analysis–planning) effectively balances reasoning depth and hallucination control.\n\nGrounded CoT methodology: The \\<SI> tag mechanism enforces transparent, scene-based reasoning rather than textual hallucination.\n\nStrong empirical validation: Extensive quantitative and qualitative analyses demonstrate consistent gains in complex reasoning tasks.\n\nPractical impact: Provides a scalable framework for training reliable, interpretable 3D-LLMs relevant to embodied AI and robotics.\n\nSolid writing and clarity: The paper is well-organized, and figures (e.g., Fig. 1 & 3) effectively illustrate the framework and dataset pipeline."}, "weaknesses": {"value": "The use of LLM-based evaluators (ChatGPT, Qwen, DeepSeek) for “Explainability,” “Faithfulness,” and “Trustworthiness” is well-motivated but inherently subjective.\nIt would strengthen credibility if the authors validated inter-evaluator consistency (e.g., correlation scores between evaluators).\n\nThe paper does not evaluate how models trained on SCoT generalize to unseen 3D environments or other datasets (e.g., ARKitScenes or Omni3D).\n\nThe paper could provide more detailed ablations: Comparing models trained with different CoT lengths or varying levels of \\<SI> grounding.；Analyzing which task types (object vs. scene vs. planning) benefit most from CoT supervision.；Evaluating performance trade-offs when CoT is partially removed during inference.\n\nAlthough Table 5 highlights hallucination in perceptual CoT, the causes (e.g., linguistic priors vs. visual overfitting) are not deeply analyzed.\nThis discussion could offer more insight into why CoT sometimes harms visual fidelity."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7lgZa6TsMJ", "forum": "5Tph6wFMOm", "replyto": "5Tph6wFMOm", "signatures": ["ICLR.cc/2026/Conference/Submission8925/Reviewer_sqpn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8925/Reviewer_sqpn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014088446, "cdate": 1762014088446, "tmdate": 1762920673866, "mdate": 1762920673866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}