{"id": "fDuNdr3QZU", "number": 16320, "cdate": 1758263097229, "mdate": 1759897247733, "content": {"title": "ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior", "abstract": "Post-hoc interpretability methods typically attribute a model’s behavior to its components, data, or training trajectory in isolation. This leads to explanations that lack a unified view and may miss key interactions. While combining existing methods or applying them at different training stages offers broader insights, such approaches usually lack theoretical support. In this work, we present ExPLAIND, a unified framework that integrates all these perspectives. First, we generalize recent work on gradient path kernels, which reformulate models trained by gradient descent as a kernel machine, to realistic settings like AdamW. We empirically validate that a CNN and a Transformer are accurately replicated by this reformulation. Second, we derive novel parameter- and step-wise influence scores from the kernel feature maps. Their effectiveness for parameter pruning is comparable to existing methods, demonstrating their value for model component attribution. Finally, jointly interpreting model components and data over the training process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking. Our findings support previously proposed stages of Grokking, while refining the final phase as one of alignment of input embeddings and final layers around a representation pipeline learned after the memorization phase. Overall, ExPLAIND provides a theoretically grounded, unified framework to interpret model behavior and training dynamics.", "tldr": "We introduce ExPLAIND—an interpretability framework for jointly attributing model components, data, and training dynamics and apply it to investigate Grokking.", "keywords": ["Interpretability", "Neural Networks", "Grokking", "Learning Dynamics", "Data Attribution"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00ab2d55c17add3129e3b5258f6a8f2d1fe0a5b3.pdf", "supplementary_material": "/attachment/47e87814015e965974209410c84b13bc49dd8fcf.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ExPLAIND, a unified framework for interpreting deep learning models by integrating model, data, and training step attribution perspectives. Building upon the EPK formulation, the paper extends previous results to modern AdamW, accounting for settings like weight decay, momentum, and mini-batching. From this theoretical foundation, the paper derives influence scores from different perspectives that are additive and can be accumulated along different dimensions. These scores are empirically validated through parameter pruning experiments and a case study on Grokking. Overall, the work proposes a theoretical unification of interpretability aspects."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretically extending EPK to practical AdamW optimizers with weight decay, momentum, and batching is a nontrivial generalization.\n\n- The idea of unifying different attribution perspectives across model, data, and training step is commendable.\n\n- The analysis of grokking is an interesting training dynamics analysis through attribution scores."}, "weaknesses": {"value": "- Although the paper contains non-trivial theoretical derivations that seem to be sound, I am not fully convinced by its experimental evaluation. See questions below.\n\n- The title of the paper can be a bit misleading and overclaiming. There are so many different interpretability works from all these three perspectives. The proposed framework seems to be one solution, but I don't see it unifying previous works. For example, model parameter attribution can include circuit discovery for IOI, linear probing directions for truthfulness, and universal neuron identification. How does the proposed score apply to these cases?"}, "questions": {"value": "- Although the paper claims a multidimensional tensor influence score spanning parameter, data, and training steps, only parameter-level pruning is evaluated in Section 4.2. Are there equivalent evaluations at the data level or training-step levels\n\n- Also, the pruning experiment compares ExPLAIND only with one baseline? This single comparison does not provide strong evidence of superiority. It would be valuable to include other attribution-based baselines, similarly for other attribution levels.\n\n- While the Grokking analysis is intriguing, several conclusions, like the dominance of regularization over kernel influences, or the significance of the peak at step 1700 in Figure 3(a), are not entirely clear to me. The curves for kernel and regularization appear roughly similar throughout, and the relationship between the accuracy curve and influence dynamics could be articulated more clearly, as 1700 doesn't see significant in the accuracy curve."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FkUm1IJEjj", "forum": "fDuNdr3QZU", "replyto": "fDuNdr3QZU", "signatures": ["ICLR.cc/2026/Conference/Submission16320/Reviewer_Sg8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16320/Reviewer_Sg8X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770349189, "cdate": 1761770349189, "tmdate": 1762926458537, "mdate": 1762926458537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ExPLAIND, a unified interpretability framework that decomposes a trained model’s prediction into additive influences from training data, model components, and training steps. Building on and extending the Exact Path Kernel view, the authors derive an exact decomposition for modern training regimes (notably AdamW) and demonstrate that the kernelized reformulation reproduces CNN and Transformer predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Solid theoretical contribution: Clear extension of the Exact Path Kernel to AdamW with mini-batching, moments, and weight decay, stated as a formal theorem.\n- Empirical faithfulness check: The EPK reformulation matches original models’ decisions with 100 integration steps (accuracy 1.0; near-zero KL)\n- Unified, additive attributions: Influence tensors can be summed along axes to obtain parameter/data/step views that directly tie to predictions."}, "weaknesses": {"value": "-  Grokking analysis is insightful but mostly qualitative and on small models/tasks; generality to larger LLMs remains unproven.\n- Pruning is used to “validate” importance scores rather than to deliver new SOTA compression results; comparisons are limited."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cr3zc4fJpS", "forum": "fDuNdr3QZU", "replyto": "fDuNdr3QZU", "signatures": ["ICLR.cc/2026/Conference/Submission16320/Reviewer_ZFtG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16320/Reviewer_ZFtG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762326297480, "cdate": 1762326297480, "tmdate": 1762926458022, "mdate": 1762926458022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the Exact Path Kernel (EPK) to the case of optimization with weight decay. This stems from the historical observation that NN can be understood as kernel machines in a particular regime. This allows to define scalar quantifying the impact of a training exemplar on the prediction of a class, the *influence*.\n\nThese quantities are used to build a framework of interpretability. Then, the capabilities offered by this framework are benhcmarked on two toy tasks:\n- parameter pruning of a CNN on Cifar-2\n- the *grokking* of a small transformer on Mod 113"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theorem 3.1 is new, and brings the work of Bell et al closer to realistic experimental setups.\n\nThe sparsity gains in Sec 3.2 are interesting. \n\nI love to see the \"Emergence of cyclic patterns in the kernel\" (l413). I wonder if we expect to see similar phenomenon on other family of problems (even artificial ones) that exhibit grokking?\n\nOverall, the paper proposes an interesting line of ideas to understand training dynamics."}, "weaknesses": {"value": "### Scope\n\nCurrently, I have issues with the motivation of the paper. While the extension of Bell et al to AdamW is interesting, I am less sure to understand the usefulness of the tool in general.\n\nThe experimental section is devoted to two setups: CNN training on Cifar-2 (cats and dogs) for sparsity, and the mod 113 task used to exhibit grokking. These two tasks are rather artificial. Even grokking as a whole received recent criticism in its ability to accurately describe some phenomena (see Jeffares and Shaar). I put an excerpt of their position here:\n\n> This work argues that many prominent deep learning\nphenomena discussed in the research literature are\nnot representative of challenges encountered in real-\nworld applications of deep learning. Thus, not all\nefforts to understand these phenomena are equal in\nvalue – we should focus on using them to refine our\nbroad explanatory theories of important aspects of\ndeep learning rather than developing narrow ad hoc\nhypotheses to describe them in isolation. However,\nthis perspective is not consistently reflected in current\nresearch practices within the community.\n\nJeffares, A. and van der Schaar, M., Position: Not All Explanations for Deep Learning Phenomena Are Equally Valuable. In Forty-second International Conference on Machine Learning Position Paper Track. 2025.\n\nOverall, I struggle to grasp if the paper is selling a method ( in which case there is not enough evidence of usefulness), or simply analyzing grokking in Mod 113 with a specific toolbox (in which case it is overfitting a simple task with ad-hoc explanations). \n\n### Clarity\n\nI struggle to give a sense of quantities defined throughout the paper, like Tensor of Influences or Accumulated influence. The link with parameter pruning is not straightforward to me.  More toy examples could be useful, as sanity checks and for pedagogical purposes.  \n\n### Baselines\n\nBetter understanding of grokking in an artificial setup ring limited understanding of neural network training dynamic in broader settings. It could be interesting to apply these methods on bigger models trained on more realistic datasets.\n\nOther tools try to explain networks' behavior from data points. We can mention influence functions, and Sobol' indices for variance decomposition. Since no comparison is made with existing tools, it is hard to situate the paper. \n\nMlodozeniec, B.K., Eschenhagen, R., Bae, J., Immer, A., Krueger, D. and Turner, R.E., Influence Functions for Scalable Data Attribution in Diffusion Models. In The Thirteenth International Conference on Learning Representations.\n\nFel, T., Cadène, R., Chalvidal, M., Cord, M., Vigouroux, D. and Serre, T., 2021. Look at the variance! efficient black-box explanations with sobol-based sensitivity analysis. Advances in neural information processing systems, 34, pp.26005-26014.\n\nSame remark can be done for sparsity in Fig. 2. The baseline of Li et al (2017) is rather old."}, "questions": {"value": "### Q1\n\nIn corollary 3.2, can you clarify if the regularization term is separated from the momentum (*à la* AdamW) or part of the loss?\n\n### Q2\n\nFor mod 113, you rely on encoder/decoder architecture. For generations/question answering, decoders are typically sufficient. Why using an encoder/decoder pair here?\n\n### Q3\n\nIn Section 5.1, what is your basis to label a phase \"circuit formation\"? For me, it looks like a (still unproven) hypothesis.\n\n### Q4\n\nResults in Fig 3.b are not very surprising. If my understanding of lines 370 is correct, it means that the last linear layer and projection onto vocabulary tokens are not in the final phase. This very much look like people typically do for finetuning of off-the-shelf foundation models: frozen  pretrained weights + finetuning of the head.  Can you comment on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xMezKcOjCD", "forum": "fDuNdr3QZU", "replyto": "fDuNdr3QZU", "signatures": ["ICLR.cc/2026/Conference/Submission16320/Reviewer_F7oz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16320/Reviewer_F7oz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762390138557, "cdate": 1762390138557, "tmdate": 1762926457569, "mdate": 1762926457569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ExPLAIND introduces a unified framework for attributing neural network predictions to model components, training data, and training steps simultaneously, addressing the fragmentation of existing post-hoc interpretability methods that examine these factors in isolation.\n\n The framework extends the Exact Path Kernel formulation to realistic training scenarios by generalizing it to AdamW and momentum SGD optimizers with weight decay, learning rate schedules, and mini-batch updates through Theorem 3.1 and Corollary 3.2. The authors validate exactness empirically on a ResNet-9 trained on CIFAR-2 (binary classification, 10,000 samples) and a single-layer Transformer on modular addition (mod-113, 4,000 training samples), achieving perfect decision agreement and near-zero KL divergence with 100 integration steps. \n\nThe core contribution is a tensor of influences indexed by training steps, parameters, samples, and outputs, enabling multi-granularity attribution through flexible accumulation along different axes. Parameter-level scores are validated via competitive pruning experiments against magnitude-based baselines.\n\n A detailed grokking case study reveals decoder-driven memorization, middle-layer pipeline formation, and a late alignment phase where embeddings and decoder synchronize around learned representations, supported by layer-swapping ablations and cyclic geometry analyses in influence space. \n\nHowever, the work is limited to small models, requires storing full training trajectories, scales as O(NDMO) in memory, and provides no principled granularity-selection guidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Rigorous theoretical extension: Theorem 3.1 extends EPK from basic gradient descent to AdamW with realistic training dynamics (weight decay, first/second moment estimates, mini-batching, learning rate schedules). The mathematical derivation is sound with complete proofs in Appendix D.1.\n2. Exact model representation: Unlike approximate methods, ExPLAIND achieves perfect equivalence with the original model (100% accuracy match, zero KL divergence in Table 1) when using sufficient integration steps.\n3. Unified mathematical framework: Successfully integrates parameter-level, data-level, and step-level attribution into a single tensor of influences, providing a principled mathematical object for multi-perspective analysis.  \n\n4.Insightful Grokking analysis: The Grokking case study shows that ExPLAIND can yield interpretable, insights into model training dynamics."}, "weaknesses": {"value": "1. Gap between theoretical contribution and practical utility and scalability limitations:\n\n     The EPK extension (Theorem 3.1) is mathematically sound and interesting.\n     But the paper claims to provide a practical interpretability framework.\n     Demonstrated only on toy problems with manual analysis and no path to scale (ResNet9 on 2-class CIFAR subset, small Transformer     on algorithmic task).\n \n     O(NDMO) memory complexity: N steps × D parameters × M samples × O outputs.\n\n     Computational cost: ~240 H200 GPU-hours for toy experiments suggests prohibitive costs for realistic models.\n\n\n2. ExPLAIND requires training trajectory information (checkpoints $\\theta_s$, gradients ($\\nabla_{\\theta} f_{\\theta_s}(x_k)$)for all samples, optimizer states $m_s$ and $v_s$, batch membership indicators) at every training step. That questions its applicability to pretrained model where only final weights are available.\n\n\n3. No principled methodology for aggregation selection: Framework provides 5-dimensional influence tensor but missing guidance on which aggregations are meaningful for which questions. Also, why Grokking analysis uses layer-level aggregation?\n\n4. Insufficient validation and missing baselines: No comparison with other modern influence methods.\n\n5. Grokking insights purely qualitative: No statistical testing of identified phases, no quantitative metrics, no automated discovery validation.\n\n6. Parameter pruning only validates parameter scores, not the data attribution claims central to the paper.\n\n7. Integration steps hyperparameter: Table 1 shows 10 vs 100 integration steps for $\\phi^{test}$. They use 100, but no analysis of sensitivity or guidance on choosing this for new problems.\n8. Grokking generalization unclear: Does the alignment phase insight generalize beyond modular arithmetic? The phenomenon might be task-specific, limiting broader impact of the case study.\n9. Unclear justification for “influence” terminology:\nThe method defines influence scores purely via additive decomposition of the model output.\nHowever, additivity alone does not imply causal or functional influence. Without sensitivity or perturbation tests, calling these quantities “influence” may be misleading."}, "questions": {"value": "1. How can ExPLAIND be applied to pretrained models such as BERT, ViT, or deeper variants of ResNet when only the final model weights are available? Additionally, how computationally intensive would such an application be?\n\n2. Aggregation methodology: What is your principled method for choosing: \n     Which axes of the influence tensor to aggregate over?\n     What granularity (parameter-level, layer-level, etc.)?\n     For the Grokking analysis, why layer-level? Did you try other granularities and find similar patterns?\n3. Mini-batch identification: When 256 samples are in $Batch_i$  and you compute $\\nabla_{\\theta} L$ on the full batch, how do you identify individual sample contributions when decomposing via $1{x_k ∈ Batch_i}$? The gradient is computed once for the batch, not separately per sample.\n4. Comparison to Influence estimation methods: Standard influence estimation methods work on any pretrained model and provide causal approximations. In what scenarios would a practitioner choose ExPLAIND (requiring full training trajectory logging) over influence functions?\n5. Cyclic geometry emergence: Figure 5 and 10 show interesting cyclic patterns. Do these emerge in other modular arithmetic tasks (division, multiplication)? Are they specific to the mod-113 Transformer architecture?\n6. Have you tested whether removing parameters or samples with high ExPLAIND influence scores changes model predictions proportionally (validating the notion of “influence”)?\n\n7. Is there a way to subsample or compress the influence tensor without losing theoretical correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "13WRTlUxCl", "forum": "fDuNdr3QZU", "replyto": "fDuNdr3QZU", "signatures": ["ICLR.cc/2026/Conference/Submission16320/Reviewer_6LT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16320/Reviewer_6LT3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16320/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462669463, "cdate": 1762462669463, "tmdate": 1762926457069, "mdate": 1762926457069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}