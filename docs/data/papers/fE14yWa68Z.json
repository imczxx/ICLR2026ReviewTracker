{"id": "fE14yWa68Z", "number": 872, "cdate": 1756821624063, "mdate": 1763453240581, "content": {"title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning", "abstract": "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce Step-GRPO with Screenshot and GUI-Agent Feedback to improve the ability of LLMs to act as the agent-engine model. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude 3.5 Sonnet from 26.4\\% to 51.9\\% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9\\% to 45.4\\% and raises the appearance score from 3.4 to 3.7.", "tldr": "We propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase.", "keywords": ["code agent", "website generation", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/288ff7beae7429303595a9af7c19d0ba19b2281e.pdf", "supplementary_material": "/attachment/b388f977183d8ba3558143ad3ba5b6f4abd34e61.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces WebGen-Agent, a framework for website code generation that jointly leverages execution-based functional feedback and visual feedback to iteratively refine website code.\nIn addition, the authors propose Step-GRPO, which uses automated functional and visual feedback from WebGen-Agent as step-wise reward signals.\nExperimental results on WebGen-Bench validate that both WebGen-Agent and Step-GRPO meaningfully improve functional accuracy and appearance scores."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Extensive experimental evaluation.\n\n- Proposes both training-free (WebGen-Agent) and training-based (Step-GRPO) approaches."}, "weaknesses": {"value": "- Human annotation transparency (Appendix E, Tables 5 and 6): Details of human evaluation are missing, including the number of annotators, recruitment method, compensation, annotation interface, and inter-annotator agreement (e.g., Cohen's kappa).\n\n- Execution-only already beats Bolt.diy: The execution-only baseline in the table already surpasses Bolt.diy’s reported accuracy, yet the authors do not clarify why their setting performs better even before applying the proposed method. There might be potential mismatches in iteration counts or evaluation conditions (e.g., 20 iterations for Bolt.diy).\n\n- Relation to prior work (Bolt.diy): The difference is only briefly mentioned (\"similar to our work\" in line 452), but the precise factors behind the performance gap (e.g., reward type, iteration loop design, or evaluation feedback fidelity) are not explained.\n\n- Ambiguity in backbone settings: Some reports (e.g., Table 2, Figure 12) omit which LLM backbone was used, complicating interpretation."}, "questions": {"value": "Some of suggestions: \n- Please include the missing details of the human evaluation process.\n\n- Please clarify why the execution-only baseline already outperforms Bolt.diy.\n\n- Please provide more detailed distinctions between WebGen-Agent and related works such as Aider and Bolt.diy.\n\n- Please clarify which backbone is used in every Figure and Table. \n\n- Since Step-GRPO uses training-time feedback, the reward quality could vary depending on whether the test cases are gold or model-generated. If gold test cases exist in WebGen-Instruct, it would be insightful to compare benchmark performance when using gold vs. generated test cases as reward signals.\n\n- A recent work, namely TDDev [1], achieves 78.2 / 70.2 accuracy with GPT-4.1 / Claude-4 backbones on WebGen-Bench. It would be interesting to check whether WebGen-Agent scales with TDDev or to compare WebGen-Agent with TDDev under the same backbone (e.g., Qwen3-Coder-30B-A3B-Inst.).\n\n[1] Wan, Yuxuan, et al. \"Automatically Generating Web Applications from Requirements via Multi-Agent Test-Driven Development.\" arXiv preprint arXiv:2509.25297 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H9Cweii2ym", "forum": "fE14yWa68Z", "replyto": "fE14yWa68Z", "signatures": ["ICLR.cc/2026/Conference/Submission872/Reviewer_Cbyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission872/Reviewer_Cbyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635875702, "cdate": 1761635875702, "tmdate": 1762915633702, "mdate": 1762915633702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes WebGen-Agent, which augments a website coding agent with two automatic evaluators: a VLM that scores a landing‑page screenshot for visual quality, and a GUI agent that exercises the site and grades functionality. These signals are used in two modes: (i) inference‑time self‑revision for closed‑source LLMs (iteratively editing the codebase), and (ii) training for open models GRPO at every editing iteration of the codebase. The author compares WebGen-Agent with existing general-purpose coding agents on the WebGen-Bench dataset. The results suggest that the proposed method outperforms competing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is a novel and interesting idea that combines VLM‑based visual feedback with a GUI agent for functional feedback, moving beyond execution‑only signals.\n\n2. On WebGen‑Bench, the method significantly outperforms general‑purpose coding agents that lack VLM/GUI feedback, indicating the value of integrating visual and functional signals.\n\n3. The author conducts experiments with various open and proprietary LLMs, as well as thorough ablation studies."}, "weaknesses": {"value": "1. The VLM feedback is based on a single-page screenshot, which ignores the coherence and consistency across multiple pages.\n2. The reliability of the functionality reward is bottlenecked by the performance of the GUI agent. However, this remains a challenging and open problem, especially when the website is complex.\n3. Some experiment details are missing and confusing. For instance, the author doesn't mention how the human annotation is conducted. Also, OpenHands is an open platform; it's unclear how it is included as a baseline. \n4. While the proposed method is novel in the area of web coding agents, similar ideas have been well explored in a broader field: using external LLM-as-judge and/or agent-as-judge to provide reward or natural language feedback, fine-tuning the LLM with an existing RL algorithm."}, "questions": {"value": "Questions:\n\n* How did you implement the OpenHands baseline?\n* What is the exact definition of Accuracy in Table 3? Is it computed from Yes/Partial/No/Start Failed?\n* Why does the experiment only adopt one dataset? There are other existing website coding benchmark such as Web2Code and Design2Code.\n\nTypo: \n1. L235: Wrong inline citation format\n2. L134: is used to reflect**s**\n3. L1044: The definition of the scores are presented (subject-verb agreement)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8RzhGncSjW", "forum": "fE14yWa68Z", "replyto": "fE14yWa68Z", "signatures": ["ICLR.cc/2026/Conference/Submission872/Reviewer_jD87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission872/Reviewer_jD87"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724889043, "cdate": 1761724889043, "tmdate": 1762915633348, "mdate": 1762915633348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the creation of webpages with LLM. In particular, it uses a recent benchmark  (`WebGen-Bench`, from [Lu et al 2025](https://arxiv.org/abs/2505.03733)) to develop an agentic scaffold tailored to it, and benchmarks a variety of models with that scaffolding.\n\nOn top of that, the paper proposes to finetune smaller (7B) models with data distillation through SFT and a GRPO-like loss for RL showing significant gains"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The resulting benchmark numbers are impressive. The evaluation benchmark is quite novel, but the gains are still significant\n\n* The use of the reward signal in an RL setting is interesting by itself, and a good use-case of using VLM-guided models-as-a-judge evaluation as reward"}, "weaknesses": {"value": "* Most of the paper is dedicated to the agentic scaffolding, which I consider to be a minor contribution and that by itself alone is below the significance criteria for an ICLR paper\n\n* Only one benchmark is used, and therefore it is not clear if the scaffolding or the general proposal generalizes to other datasets\n\n* The final numbers of the 7B models are impressive, however most of the gains are obtained from the SFT step which consists in data-distilling Deepseek. The use of a large VLM model as judge also raises the question if the gains are not \"just\" distillation from a larger models, or if there is some learning that could generalize to larger models"}, "questions": {"value": "* From Table 4 it seems that the VLM used as judge does not necessarily have a major impact. Did you benchmark this behaviour when finetuning the smaller models with GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ZLJhOjPai", "forum": "fE14yWa68Z", "replyto": "fE14yWa68Z", "signatures": ["ICLR.cc/2026/Conference/Submission872/Reviewer_z3Vc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission872/Reviewer_z3Vc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859271438, "cdate": 1761859271438, "tmdate": 1762915633248, "mdate": 1762915633248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WebGen-Agent, a test-time inference framework that leverages screenshots and GUI-agent feedback signals to iteratively improve the appearance and performance of web generation from scratch. The authors demonstrate that using WebGen-Agent leads to substantial gains for both proprietary and open-source models compared to prior frameworks such as Bolt.diy. They also introduce Step-GRPO, which employs rewards derived from feedback based on screenshots and GUI-agent evaluations. The models Qwen2.5-Coder-7B-Instruct and Qwen3-8B are trained with SFT and Step-GRPO, achieving notable improvements over their base models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong ablation studies demonstrating the contribution of each component, including different rewards, backtracking, and the impact of SFT.\n\n- Significant improvements shown with both WebGen-Agent and step-GRPO.\n\n- Excellent reproducibility, with detailed appendices that include all prompts, sample outputs, and qualitative examples."}, "weaknesses": {"value": "- The source of instructions for the 700 trajectories used in SFT training (generated with DeepSeek-V3 using WebGen-Agent) is not specified. The authors should clarify whether the data originate from WebGen-Instruct or WebGen-Bench. This is a critical point not clarified in the paper to ensure there is no data leakage.\n\n- The paper does not provide a cost or time comparison between WebGen-Agent and Bolt.diy. For proprietary models, it would be helpful to include runtime or cost data, while for open-source models, a FLOPs or time comparison would clarify efficiency.\n\n- The results do not include performance for WebGen-LM-3B, 7B, and 32B with WebGen-Agent. Including these models would provide a more comprehensive comparison, especially since WebGenAgent-7B and WebGen-LM-7B share the same base model (Qwen2.5-Coder-Instruct-7B).\n\n- It would be beneficial to determine which model acts as the most reliable judge or produces the most aligned outputs. Benchmarks such as PairBench [1] or AgentRewardBench [2] could help evaluate this aspect.\n\n---\nMinor Comments\n\n- The formula fonts in Figure 2 should be larger for improved readability.\n\n[1] PairBench: Are Vision-Language Models Reliable at Comparing What They See?\n\n[2] AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories"}, "questions": {"value": "Please address the issues raised in weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z19d7LoL07", "forum": "fE14yWa68Z", "replyto": "fE14yWa68Z", "signatures": ["ICLR.cc/2026/Conference/Submission872/Reviewer_mw4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission872/Reviewer_mw4M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950491822, "cdate": 1761950491822, "tmdate": 1762915633136, "mdate": 1762915633136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}