{"id": "WyyawATBhs", "number": 11726, "cdate": 1758203355611, "mdate": 1763564145765, "content": {"title": "Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge", "abstract": "State-space models (SSMs) have emerged as efficient alternatives to Transformers for sequence modeling, offering superior scalability through recurrent structures. \nHowever, their training remains costly and the ecosystem around them is far less mature than that of Transformers. \nMoreover, the structural heterogeneity between SSMs and Transformers makes it challenging to efficiently distill knowledge from pretrained attention models. \nIn this work, we propose **C**ross-architecture distillation via **A**ttention **B**ridge (**CAB**), a novel data-efficient distillation framework that efficiently transfers attention knowledge from Transformer teachers to state-space student models. \nUnlike conventional knowledge distillation that transfers knowledge only at the output level, CAB enables token-level supervision via a lightweight bridge and flexible layer-wise alignment, improving both efficiency and transferability.\nWe further introduce flexible layer-wise alignment strategies to accommodate architectural discrepancies between teacher and student. \nExtensive experiments across vision and language domains demonstrate that our method consistently improves the performance of state-space models, even under limited training data, outperforming both standard and cross-architecture distillation methods.\nOur findings suggest that attention-based knowledge can be efficiently transferred to recurrent models, enabling rapid utilization of Transformer expertise for building a stronger SSM community.", "tldr": "", "keywords": ["Knowledge distillation", "SSM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1107d4419b0a58a0ec493949579e38815bca3071.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a distillation method that transfers knowledge from Transformer based architectures to Mamba based models. Utilizing state space duality that's presented in Mamba2, the method is extremely simple -- attach additional MLP layers to B and C to predict K and Q. The authors validate their method mainly using vision models, ViT and Vim, and also showing some results in NLP."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea is very straightforward, which will be easy for readers to understand and implement. The authors provide many experiments using both vision and language data."}, "weaknesses": {"value": "- Lack of novelty and contribution: Indeed, I am a fan of simple ideas. However, the results are quite suspicious due the following concerns;\n- Unfair experimental setup: the paper uses \\phi_B and \\phi_C that are 2 layer MLP with SiLU. This leads to more parameters as well as more FLOPs. This is a huge benefit for the authors' method.\n- Why vision? : ViT and Vim are fundamentally different that Vim rasterizes inputs, while there is no causal assumption for ViT. Also, it's hard to agree with the idea of matching both forward & backward B and C to K and V. It sounds weird as it will lead to forward and backward to be the same for B and C."}, "questions": {"value": "Line 49 : What is \"direct injecting Transformer attention into Mamba\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w77iD39UDB", "forum": "WyyawATBhs", "replyto": "WyyawATBhs", "signatures": ["ICLR.cc/2026/Conference/Submission11726/Reviewer_PJ6L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11726/Reviewer_PJ6L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536398453, "cdate": 1761536398453, "tmdate": 1762922765160, "mdate": 1762922765160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAB (Cross-architecture distillation via Attention Bridge), a framework for distilling knowledge from Transformer-based teacher models to state-space model (SSM) students, specifically Mamba. The core innovation is a lightweight, MLP-based \"Attention Bridge\" that aligns the implicit attention carriers in Mamba (the token-dependent projections $B$ and $C$) with the explicit key ($K$) and query ($Q$) representations from the Transformer.\n\nAfter the distillation process is complete, the MLP weights are discarded. The final product is a standalone Mamba model that has internalized the attention-based inductive biases from the Transformer teacher, with no inference-time overhead from the bridge components. The distillation process does not freeze the Mamba weights; instead, the gradients from the attention alignment loss and the standard output distillation loss are backpropagated through the MLP bridge and into all weights of the student model.\n\n\n\nThis method enables fine-grained, token-level supervision without the quadratic memory cost of aligning full attention matrices. It also employs a flexible, proportional layer-wise alignment strategy to handle architectural depth mismatches. The authors demonstrate CAB's effectiveness and data efficiency through extensive experiments on both vision (ImageNet) and language (OpenWebText, C4, WikiText) tasks, showing consistent improvements over standard and recent cross-architecture distillation baselines, particularly in low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The formulation of the \"Attention Bridge\" is a creative and novel solution to the cross-architecture distillation problem. The insight to treat Mamba's $B$ and $C$ projections as analogous to the Transformer's $K$ and $Q$ under a linear attention approximation is theoretically grounded and is a natural extension of State-Space Duality. \n\n* The experimental evaluation is thorough, spanning multiple domains, model scales, and data regimes. The consistent and often substantial improvements over strong baselines is very convincing. The ablation studies are well-designed and provide crucial insights into the contribution of each component.\n\n* The paper is well-written. The problem is motivated, the method is explained with sufficient detail, and the results are presented logically and convincingly. \n\n* This work has high practical significance. By enabling data-efficient knowledge transfer from Transformers to Mamba, it lowers the barrier to adopting high-performance SSMs in real-world scenarios where data is limited and computational resources are constrained."}, "weaknesses": {"value": "* The authors use a 2-layer MLP with SiLU activations for the bridge modules ($\\phi_B$) and ($\\phi_C$). While effective, the paper does not ablate this choice against a simpler linear projection. A brief justification or experimental result showing the superiority of a non-linear transformation over a linear one would strengthen this design decision, especially given the focus on efficiency.\n\n* The proportional layer mapping $g(l)$ is a simple and effective heuristic. However, the paper does not explore or ablate against other possible strategies (e.g., learned alignment, manual assignment based on layer depth). A brief discussion on the choice and potential alternatives would be useful.\n\n* A key, unexplored trade-off is whether distilling a Transformer's inductive biases into a Mamba model also transfers the Transformer's weakness in length generalization. State-space models like Mamba are prized for their ability to generalize to sequences longer than those seen in training, a known challenge for Transformers. The paper lacks any analysis or discussion of whether the distilled models retain their robustness to longer sequence lengths."}, "questions": {"value": "* Why was a 2-layer MLP chosen for the bridge instead of a simpler linear projection? Was this choice ablated?\n\n* Does the CAB method require the teacher and student to share the same tokenizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w4C3LWmic6", "forum": "WyyawATBhs", "replyto": "WyyawATBhs", "signatures": ["ICLR.cc/2026/Conference/Submission11726/Reviewer_bjGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11726/Reviewer_bjGZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664905278, "cdate": 1761664905278, "tmdate": 1762922764659, "mdate": 1762922764659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of cross-architecture knowledge distillation from pretrained Transformers to state-space models (SSMs), a critical need as SSMs offer superior runtime efficiency for sequence modeling but suffer from high training costs, immature ecosystems, and structural heterogeneity with Transformers. To solve this, the authors propose Cross-architecture distillation via Attention Bridge (CAB), a data-efficient framework that enables effective transfer of attention-based knowledge from Transformers to SSMs, even under limited training data. CAB overcomes these challenges through these key components: 1. Lightweight MLP attention bridge, Maps Transformer’s explicit attention carriers (\\(Q, K\\)) to Mamba’s implicit attention projections (\\(B, C\\)) via learnable MLPs (\\(\\phi_B, \\phi_C\\)). 2. Flexible layer-wise alignment. 3. Training framework: First aligns attention via the bridge (200M tokens), then performs soft KD (KL divergence) for output refinement (2B/4B tokens)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative cross-architecture distillation paradigm with structural alignment.\n\nThe paper demonstrates strong originality through creative solutions to long-standing cross-architecture knowledge transfer challenges. CAB introduces the Attention Bridge—a lightweight MLP-based module that maps Transformer’s explicit Q/K representations to Mamba’s implicit B/C projections—addressing the core issue of structural heterogeneity between attention-based models and SSMs. Unlike prior works, this design avoids quadratic overhead while enabling fine-grained token-level supervision.\n\n2. Comprehensive, and convincing experimental validation.\n\nThe experimental quality is exemplary, characterized by thoroughness, reproducibility, and statistically significant results. Across two distinct domains (vision with ImageNet-1k, language with OpenWebText/C4/WikiText), the method consistently outperforms baselines.\n\n3. Well-structured narrative with transparent method and experimental details.\n\nThe paper excels in clarity, with a logical flow that guides readers from problem to solution. The problem formulation is concise: it first highlights SSMs’ efficiency but high training costs, then critiques limitations of existing distillation (weak gradient signals, architectural mismatch), and motivates CAB’s design through visual evidence (Fig. 1 showing performance collapse from direct attention injection). \n\n4, Addressing critical gaps and advancing efficient sequence modeling.\n\nThe paper’s significance is far-reaching, both for research and practical applications. From a research perspective, it bridges the divide between Transformer’s rich pretrained knowledge and SSMs’ efficient inference, enabling SSMs to leverage the mature Transformer ecosystem—this opens new avenues for cross-architecture distillation beyond Transformer-to-Transformer or CNN-to-Transformer paradigms. Practically, the method’s data efficiency (effective with 1–20% training data) addresses real-world constraints in low-resource domains (healthcare, robotics, edge computing), where large datasets are scarce. Furthermore, it provides a generalizable framework for transferring attention-based inductive biases to recurrent models, inspiring future research on distillation between heterogeneous sequence modeling architectures."}, "weaknesses": {"value": "1. Empirical Considerations for Attention Bridge Design: Lack of Systematic Validation\n\nThe paper designs the attention bridge as a \"2-layer MLP + SiLU activation,\" but this design lacks theoretical basis and systematic comparison: (1) It does not test the impact of different network structures (1-layer MLP, 3-layer MLP, attention layer, etc.) on the alignment effect; (2) It does not analyze the trade-off between the bridge's parameter size and performance efficiency, if the bridge's parameters are reduced (e.g., 1-layer MLP), can the alignment effect still be maintained? If the parameters are increased (e.g., 3-layer MLP + attention mechanism), can performance be further improved without significantly increasing computational cost?\n\n2. Performance Degradation in Full Data Scenarios: Unresolved Core Contradictions\n\nAppendix A.1 of the paper shows that when training on full data for 300 epochs, the Top-1 accuracy of CAB (71.34%) is lower than that of soft distillation (74.56%). This is only alleviated by “early stopping alignment (switching to KL distillation after 35/50 epochs)”, but the reasons for the degradation and the fundamental solutions are not analyzed in depth: (1) It is not clear whether it is “over-constraining of the student model by attention supervision” or “the bridge structure introducing redundant information in full data”; (2) The early stopping strategy lacks an adaptive mechanism – how to determine the optimal early stopping epoch under different tasks and different teacher-student model combinations? Manually setting 35/50 epochs lacks universality.\n\n3. The Performance of Scaled to Large Models is unknown.\n\nAs shown in the experimental section of the paper, most of the models used in the experiments are limited to below M. Is CAB distillation efficient and performant enough, for example, using 1B, 7B as the teacher, or a larger model as the teacher or student? Is it possible that the mapping effect would be affected if a large model is used as the student?"}, "questions": {"value": "1. Choice of MLP Projection in Attention Bridge: Why use a 2-layer MLP with SiLU activation for φ_B and φ_C? Did you ablate other architectures (e.g., linear projections, 1-layer MLPs, different activations) and find the 2-layer MLP to be uniquely effective? \n\n2. Layer Mapping Function g(l): The paper uses proportional indexing for layer alignment (g(l) = ⌊l/L · T⌋). Did you test alternative alignment strategies (e.g., learned layer mapping, fixed one-to-one for matching depths) and find proportional indexing to be superior? \n\n3. Early-Stopped Alignment in Full-Data Regime: Appendix A.1 shows that CAB degrades performance if applied for 300 epochs in full-data settings, requiring early stopping. Why does excessive attention supervision harm performance? Is this related to overfitting to Transformer inductive biases that are suboptimal for Mamba in full-data scenarios?\n\n\n4. Data Scarcity Boundaries: The paper tests 1–20% of ImageNet and 200M–4B tokens for language. What is the minimum data threshold where CAB outperforms baselines? For example, does CAB still help with 0.5% of ImageNet, or does it fail due to insufficient supervision for the attention bridge? Or maybe would it affects the performance if test 40% or more part of data?\n\n\n5. Why Not Use Learned Activation Mappings for Linear Attention: Linear attention uses φ(Q) and φ(K) for efficiency. Did you consider aligning Mamba’s B/C to φ(K)/φ(Q) (instead of raw K/Q) to leverage linear attention’s structural similarity to Mamba?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Ly6hC4PWF", "forum": "WyyawATBhs", "replyto": "WyyawATBhs", "signatures": ["ICLR.cc/2026/Conference/Submission11726/Reviewer_BZVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11726/Reviewer_BZVQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954948071, "cdate": 1761954948071, "tmdate": 1762922764228, "mdate": 1762922764228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method to distill a pretrained transformer model into a student Mamba model. The core idea of the paper relies on the high-level shared functionality of the B and C projections of the SSM and the K and Q projections of the Transformer. The distillation method aligns these using a small learnable MLP projection module (the Attention Bridge). The alignment can also happen in situations where the number of layers in the student and teacher models is different by doing proportional indexing between layers in the learning process.\n\nOverall, I found that the paper has a combination of neat ideas, however is seriously lacking in experimental rigor as well as soundness (see weaknesses and questions). It also makes some claims that I believe are misleading. In the current state, I am leaning to reject the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a combination of intuitive ideas (e.g. the Attention Bridge, the layerwise proportional indexing for asymmetric teacher and students), and these are introduced and demonstrated well.\n\n- The chosen baselines make sense to compare against, and the proposed method seems generally superior to them.\n\n- The paper is generally well written and is easy to read and understand."}, "weaknesses": {"value": "- The paper restricts evaluation to situations in which the choice of teacher model is the same (or similar) size as the student model (e.g. Deit Tiny to Vim Tiny, Deit Small to Vim Small in most experiments, and only one experiment with the immediately larger model variation used as teacher). I think that distillation presents the most practical value when we can use a larger teacher to train a smaller student model (e.g. Deit base, large, or huge used as a teacher for a smaller Vim model). The paper contains a layer-wise alignment strategy in L216 which was used for the other comparisons, but not for the situation in which the teacher model is significantly larger (which is often going to be the case in a practical distillation scenario)\n\n- I find it problematic that we need to do 300 epochs of distillation on a randomly chosen subset of ImageNet, and the only reported metric is top-1 ImageNet accuracy. Moreover, we use a teacher model trained on ImageNet to train a student model that is also previously trained on ImageNet!\n\n- ImageNet-1K as a dataset in itself has several unique characteristics (e.g. artificially balanced classes, a particular kind of class distribution) etc that are usually not representative of real deployments. In my opinion, showing some results on additional tasks (e.g. ADE-20k segmentation like the ViM paper does) would have been helpful to strengthen the paper.\n\n- I find Figure 1 (a) slightly misleading for a number of reasons. For example, calling SSM-based training slow and attention based inference slow are both a massive stretch. The real inference performance of Transformers with FlashAttention etc can be faster than supposedly linear SSM implementations. At the same time, the training time of a Mamba-2 model can be lesser than an equivalent transformer due to faster convergence. Making such a strong claim in the most important figure of the paper could mislead the reader.\n\n- I think the LM side of the experiments could have used significantly more rigor. E.g. using additional base models at different parameter ranges, and evaluating on additional downstream tasks using a standard tool like LM Eval Harness. In general, the core result is just comparing DistillGPT to PhiMamba, which is very insufficient in my opinion.\n\n- The choice of LM models was conceivably due to a similar number of GFlops. However, this alone can be misleading given the different execution methods of the Transformer and Mamba models. I also find it slightly problematic that an 88M param model was used as a teacher for a 123M param model. At this model size, there is nothing concrete that can be ascertained about scaling trends, but at the very least, I believe that the teacher model here could have been a few more variations with higher parameter counts.\n\n- The student models in the paper are typically also pretrained models on a specific dataset. For a more generally applicable method, I think there should also have been results for when the student model is trained from scratch and does not contain the same data biases that the teacher model does."}, "questions": {"value": "- For the claim in the abstract, “Moreover, the structural heterogeneity between\nSSMs and Transformers makes it challenging to efficiently distill knowledge from\npretrained attention model” -> how do you reconcile this with a work like Mamba-2 showing the structured state space duality between Transformers and Mamba-2 like models?\n\n- Why is Section A.2 in the appendix and not the main paper? Results on a 1.5B model are a lot more relevant than the 123M parameter model in my opinion. Also, why not additional LM parameter variations?\n\n- “Standard ImageNet augmentations, including random cropping and horizontal flipping, are applied during training” -> this is often a critical detail in ImageNet runs. The chosen baseline method (DeiT) and the Vision Mamba model use slightly different ones, which may lead to inconsistent behavior. Could you please specify the exact augmentations and training parameters used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LLlIdY6Tv1", "forum": "WyyawATBhs", "replyto": "WyyawATBhs", "signatures": ["ICLR.cc/2026/Conference/Submission11726/Reviewer_r1ob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11726/Reviewer_r1ob"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11726/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979292033, "cdate": 1761979292033, "tmdate": 1762922763813, "mdate": 1762922763813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}