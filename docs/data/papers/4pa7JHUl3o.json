{"id": "4pa7JHUl3o", "number": 14294, "cdate": 1758232175641, "mdate": 1759897378323, "content": {"title": "Transcriptomics-Morphology Generation Via Treatment Conditioning With Rectified Flow", "abstract": "Predicting cellular responses to drug perturbations requires capturing complex dependencies between transcriptomic and morphological changes that single-modality approaches cannot adequately model. We introduce \\textbf{PertFlow}, a unified framework that jointly predicts gene expression profiles and generates cellular morphology images in response to drug treatments, conditioned on control cellular states. Our method integrates control transcriptomic and imaging data through multi-head cross-modal attention mechanisms, learning a shared latent representation that incorporates drug compound features, background cellular profiles, and treatment specifications. From this unified representation, PertFlow employs a regression head for RNA-seq prediction and rectified flow dynamics for stable morphological image generation, with cross-modal consistency losses ensuring coherent molecular and phenotypic predictions. PertFlow enables accurate predictions from either complete multi-modal inputs or single-modality data alone, demonstrating robust cross-modal learning. Our evaluation on paired RNA-seq and Cell Painting fluorescent imaging datasets demonstrates that PertFlow achieves stronger cross-modal consistency and accurate prediction of drug-induced changes compared to diffusion baselines.", "tldr": "Generating transcirptomics and morphology image with response to chemical perturbation.", "keywords": ["Multi-Modal", "Generative AI", "Transcriptomics", "Cellular Imaging"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dbaf9127c04c9e5e7b7fd04b402e6cd7c7946cd5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents PertFlow, a unified generative framework that jointly predicts transcriptomic changes (RNA-seq) and generates cellular morphology images in response to drug perturbations. Unlike existing single-modality or one-way cross-modal approaches, PertFlow integrates control RNA-seq, imaging data, and drug metadata into a shared latent space using cross-modal attention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The model combines a regression head for gene expression prediction and rectified flow dynamics for efficient image generation, enhanced by knowledge graph embeddings and cross-modal consistency losses. Evaluated on the GDPx3 dataset, PertFlow achieves strong transcriptomic prediction (Pearson r = 0.78), improved image generation quality (24.06 in FID), and superior cross-modal alignment compared to diffusion-based baselines."}, "weaknesses": {"value": "While this paper proposes a model structure for generating counterfactual (perturbed) cellular images, several important aspects remain unclear and limit the overall clarity and impact of the work:\n1. **Component Motivation and Ablation Studies**:\n    The motivation for including specific components, such as the drug encoder, RNA encoder, and PrimeKG, and design choices, ResNet vs MHA or Unet, for PertFlow, is not clearly explained. The advantages over prior works/methods are not justified either. There is no ablation study isolating the contribution of each module, making it difficult to assess their individual importance to the final performance.\n\n2. **Loss Design and Clarity**:\n   The choice of Pearson correlation as part of the loss function is mentioned, but not well justified either. Additionally, the roles and effects of several hyperparameters (e.g., $ \\alpha_{\\text{drug}} = 0.3 $ ) are not clearly discussed. \n\n3. **Experimental Setup and Evaluation Details**:\n The experimental setup is not clearly described in the main text. It remains unclear how evaluation metrics such as MSE, MAE, and RMSE are computed, and the composition of the test set is not specified. In addition, there are only comparisons with PertFlow variants, making it difficult to interpret the reported performance.\n\n4. **Baselines and Comparative Analysis**:\n Although the authors state that no baseline exists, simple baselines could still be constructed. For instance, training a flow-matching model conditioned on a single perturbation modality (e.g., RNA or SMILES) to generate images. Additionally, an evaluation based on perturbation classification accuracy from generated images would provide a more objective measure of biological fidelity.\n\n5. **Dataset Limitations**:\n The proposed method is evaluated on a single dataset, which limits the assessment of its generalizability.\n\n6. **Missing Details in Background and Related Work**:\n    Citations in the introduction are missing. Relevant background on flow-matching models and their prior applications is not sufficiently covered.\n\n7. **Notation and Representation Issues**:\n   The presentation of the method could be improved. For example, it is unclear which projected embeddings belong to the shared latent space and how they interact. Clearer notation would significantly improve readability and understanding.\n\n8. **Figure Readability**: \n   The legends and font sizes in several figures are too small and difficult to read. Improving the figure readability would greatly enhance the presentation quality."}, "questions": {"value": "1. How were the coefficients for different loss components and embeddings determined (e.g.,  $ \\alpha_{\\text{drug}} = 0.3 $)? Were these values obtained through hyperparameter tuning or heuristic selection?\n2. What's the training data size? What's the evaluation dataset?\n3. How do different types of RNA encoders or Drug encoders for training encoders affect the generative performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sRMs6Q8126", "forum": "4pa7JHUl3o", "replyto": "4pa7JHUl3o", "signatures": ["ICLR.cc/2026/Conference/Submission14294/Reviewer_Diqo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14294/Reviewer_Diqo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760832703980, "cdate": 1760832703980, "tmdate": 1762924739285, "mdate": 1762924739285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a novel multimodal generative model for treatment (or, perturbation) effect prediction in transcriptomics and cell painting morphological imaging data. The authors claim to be the first to pursue such an approach, and develop a novel architecture with various components verified to be required for its success, including: drug conditioning features, a GNN on a knowledge graph, cross-modal attention, and an interesting contrastive rectified flow triplet loss. The work indicates that combining the two modalities leads to complementary improvements on each, and that their specialized triplet loss with rectified flow dynamics offers significant improvements over pure diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is highly original in the sense that multimodal prediction of treatment effects is a burgeoning field, along with their approach in using rectified flow matching to broach the problem.\n- The ablation study of their method is a strength which demonstrates the contribution of their overall approach, and also presents some strong initial evidence that multimodal combination of RNA data with morphology can be complementary and mutually beneficial when representing both modalities.\n- While difficult for non-pathologists to judge, the various examples of images generated by their method and the empirical demonstration in Figure 5 indicates that the rectified flow approach is a fruitful method for generating realistic cell morphology images."}, "weaknesses": {"value": "- Only 1 dataset is in-scope for evaluation, limiting generalizability. Given that the model is multimodal, it seems to me that it should be capable of processing unimodal datasets, e.g., predicting transcriptomic profiles of phenotypic images from a new dataset, or generating phenotypic images given novel transcriptomic data as input (for example, by integrating over the missing modality). However, the authors do not evaluate their model's ability to generalize beyond the GDPx3 dataset. It might be argued that paired multimodal datasets are very rare in this field, which is undoubtedly true given the expensive nature of these assays and the biological expertise required even to generate data for just one modality, let alone both. That therefore necessitates, however, precisely a modelling regime that could learn from this small limited set of paired cross-modal data for the purpose of then building richer representations and/or generations off of other unpaired datasets.\n\n- It is unclear if the various results are on validation or on training set data; the evaluation is furthermore complicated by the lack of explanation of the dataset, and how many unique treatments are present in it. Indeed, the authors note that \"generalization to unseen cell lines or novel compounds is limited by the scarcity of paired multi-modal datasets with shared metadata\" (457-458), which suggests that the validation set is simply duplicates of the same compounds seen in training. If this is the case, then the value is extremely limited, as there would not be a useful application to virtual hit screening. At the same time, it is unclear if this is the case; it seems conceivable to me that this creative and novel method could actually be capable of generalizing to novel compounds (at least to some degree) given the inclusion of the knowledge graph and chemical features in their architecture, but there are no such experiments that characterize this potential limitation. \n\n- Figure 4 is far too unexplained to be at all convincing. Claiming two experts looked at pictures is not a particularly reproducible nor satisfying determination. A more comprehensive study would perhaps baseline against what the experts think of the real images, or if they can distinguish real from generated. It would also describe the qualifications of these experts.\n\n- The authors extensively rely on UMAPs in Figure 2 and Figure 7 (the latter constituting almost an entire page) to furnish their explanations and evidence of their model's ability to bridge the gap between modalities and \"generate biologically coherent treatment responses\" (423). Alas, **a UMAP is not a proof** -- in general, the inclusion of UMAPs in professional papers is often antithetical to developing a comprehensive understanding beyond the realm of surface-level impressions; they at best provide an intuition that some data clusters together and looks different from other parts of the data, and at worst serve no better purpose than that of a Rorschach test for how the data makes you feel. Furthermore, they are completely uninterpretable for colorblind readers. A rigorous analysis of the natural clustering (or lackthereof) within the data manifold should be quantitative and empirically measured at the very least by metrics such as silhouette scores (and only from there could a small UMAP potentially be appropriate to include for the purpose of giving visual intuition to an empirical measurement). \n\n- The training details in 275-284 are of some concern. One layer of self-attention with only 128-dimensional model dimension seems quite small for a transformer. It is also unclear how a multi-token representation to 16 tokens is obtained, is not each gene in the transcriptomics readout a token, or are they pooled in some manner to just 16? Only requiring 5 hours to train the model over 8 GPUs also seems extremely short for any kind of deeply connected model with transformer layers. Typical diffusion and flow matching models require substantially more training compute to become effective.\n\n- On the less fundamental aspects of presentation, there are various typos and poor stylization choices. For example, line 338 (\"duuring\"), the lack of complete sentences and descriptions on each figure caption, Figures are not PDFs with embedding text, consistent use of citations without parantheses - for example, 197-198 should be \"multi-layer self-attention (Vasawani et al., 2017)\" since the author's name is not being used, i.e. use \\citep for all such citations.\n\n- A large amount of related work is missing, in particular the limitations of perturbation effect prediction in transcriptomics are not discussed at all. It is absolutely critical to evaluate proper non-DL baselines in this context, given that many very trivial methods (e.g. simply taking a mean baseline) outperform most deep learning models in transcriptomics still [e.g., A, B, C, D]. Indeed, it is important to include non-DL baselines in this context; for example, if simply using the controls beats deep learning models in transcriptomics [B] then it is also worth evaluating the FID (and the remaining) image generation metrics against simply using the controls as a baseline to compare to the generated perturbation image. I do not find the argument on lines 289-290 \"we have no previous method to compare to as baseline\" to be at all convincing, since there are many simple non-ML baselines (mean, control, do-nothing, etc), or simple linear methods, that can could be compared to in this context.\n\nA - Ahlmann-Eltze, Constantin, Wolfgang Huber, and Simon Anders. \"Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines.\" Nature Methods (2025): 1-5.\n\nB - Wong, Daniel R., Abby S. Hill, and Rob Moccia. \"Simple controls exceed best deep learning algorithms and reveal foundation model effectiveness for predicting genetic perturbations.\" Bioinformatics (2025): btaf317.\n\nC - Ihab Bendidi, Shawn T Whitfield, Kian Kenyon-Dean, Hanene Ben Yedder, Yassir El Mesbahi, Emmanuel Noutahi, and Alisandra Kaye Denton. \"Benchmarking transcriptomics foundation models for perturbation analysis: one PCA still rules them all.\" NeurIPS 2024 Workshop on AI for New Drug Modalities (2024).\n\nD - Wenkel, Frederik, Wilson Tu, Cassandra Masschelein, Hamed Shirzad, Cian Eastwood, et al. \"TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction.\" arXiv preprint arXiv:2505.14919 (2025)."}, "questions": {"value": "- I am somewhat confused as to why the authors say that future work should explore including \"compound-target interaction graphs to enhance out-of-distribution performance\" (460) given that seems to be precisely what their knowledge graph encodes - do the ablations indicate that the current knowledge graph approach presented in this work is not helpful in this regard? It seems to improve results in Table 1 and Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T68fJax6eP", "forum": "4pa7JHUl3o", "replyto": "4pa7JHUl3o", "signatures": ["ICLR.cc/2026/Conference/Submission14294/Reviewer_YVRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14294/Reviewer_YVRz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567256614, "cdate": 1761567256614, "tmdate": 1762924738765, "mdate": 1762924738765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce PertFlow, a multimodal generative framework designed to jointly predict gene expression and morphological cellular responses to perturbations. PertFlow uses different encoders to process each modality, and a cross-modal attention mechanism is employed to align features across modalities. The RNA-seq output is obtained through direct prediction, while the treated images are generated using a flow model. An ablation study was performed to demonstrate the importance of each component of the framework. The authors also claim that the method is able to recover drug-induced phenotypes and morphology."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a highly relevant problem, namely the joint generation of gene expression profiles and cellular morphological responses induced by perturbations.\n\n2. Overall, the paper is clearly written and easy to follow.\n\n3. The ablation study effectively demonstrates the importance of each core component of the PertFlow framework."}, "weaknesses": {"value": "1. While the paper is easy to follow, I think the presentation can be improved. For instance, the architecture and dataset details could be moved to the appendix, and the space saved could be used to present additional results and discussion.\n\n2. The images are small and the text is tiny, which makes them difficult to read (Fig. 2, Fig. 6, Fig. 7).\n\n3. The related works section lacks several important methods that address the prediction of cellular responses to perturbations [1,2].\n\n3. The proposed baseline is somewhat weak, as the authors only performed an ablation study. While I understand that PertFlow is the first method to jointly predict RNA-seq and images, it should still be compared to state-of-the-art uni-modal approaches [1,2].\n\n4. The equations are not numbered, which makes the reading and referencing more difficult.\n\n5. Some figures lack clear explanations (Fig. 4, Fig. 7).\n\n6. The technical novelty of the method appears limited, as it can be seen mainly as a combination of encoders to obtain a shared representation used for conditioning the flow model.\n\nReferences: \n\n[1] PhenDiff: Revealing Subtle Phenotypes with Diffusion Models in Real Images, Bourou et al.\n\n[2] Revealing invisible cell phenotypes with conditional generative modeling, Lamiable et al."}, "questions": {"value": "1. Regarding the total loss, the weights are set to $\\omega_{rna}$ = $\\omega_{img}$= 0.5 Did you try other values? Intuitively, image generation is more challenging than RNA-seq prediction. Is it reasonable to assign them the same weight?\n\n2. If I understood correctly, the shared representation is used both to predict RNA-seq and to condition the flow model. How is this conditioning performed in practice?\n\n3. While the concept of joint generation is interesting, it is inherently more complex than uni-modal generation. Therefore, I am not fully convinced that joint models can outperform state-of-the-art single-modality generative approaches.\n\n4. Why not compare the image generation capabilities to other models, such as [1,2]?\n\n5. Regarding the metrics reported in Table 2, were they obtained per treatment, or were images evaluated unconditionally? How many images were used for the evaluation?\n\n6. Please increase the size of Table 1 and improve the readability of the text in the figures.\n\n7. In Table 2, I do not fully understand the choice of comparing PertFlow only against diffusion models (DDPM). It is known that Rectified Flow outperforms DDPM (as noted in Table 1 [3]), and your experiments seem to confirm this. What are the differences between. What are the differences between PertDiff_N, PertDiff_x0 and PertDiff_V?\n\n8. Your experiments show that using all components of PertFlow leads to better results. However, this is a bit confusing because the primary change appears to be the type of conditioning variable provided to the flow model. Could you elaborate on how this leads to significantly improved results?\n\n9. Figure 5(a) is not very informative, as it is already well-known that DDPMs are slow at inference. Did you try DDIM or other acceleration techniques?\n\n10. Figure 6 is small and the text is difficult to read. Moreover, it is hard to visually inspect generated images under different conditions. You could complement the visual results with additional empirical metrics, as done in [1].\n\nReferences: \n\n[1] PhenDiff: Revealing Subtle Phenotypes with Diffusion Models in Real Images, Bourou et al.\n\n[2] Revealing invisible cell phenotypes with conditional generative modeling, Lamiable et al.\n\n[3] Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow, Liu et al.\n\n[4] Denoising Diffusion Implicit Models, Song et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rUkPDfFPpi", "forum": "4pa7JHUl3o", "replyto": "4pa7JHUl3o", "signatures": ["ICLR.cc/2026/Conference/Submission14294/Reviewer_UAZS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14294/Reviewer_UAZS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768332621, "cdate": 1761768332621, "tmdate": 1762924738390, "mdate": 1762924738390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}