{"id": "8qk6eUnvbH", "number": 12932, "cdate": 1758211823133, "mdate": 1759897475756, "content": {"title": "Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning", "abstract": "Current visual reasoning methods mainly focus on exploring specific reasoning modes. Although improvements can be achieved in particular domains, they struggle to develop general reasoning capabilities. Inspired by this, we propose a novel adaptive reasoning paradigm, $\\underline{\\text{M}}$ixture-$\\underline{\\text{o}}$f-$\\underline{\\text{V}}$isual-$\\underline{\\text{T}}$houghts (**MoVT**), which unifies different reasoning modes within a single model and guides it to select the appropriate mode based on context. To achieve this, we introduce **AdaVaR**, a two-stage $\\underline{\\text{Ada}}$ptive $\\underline{\\text{V}}$isu$\\underline{\\text{a}}$l $\\underline{\\text{R}}$easoning learning framework: different modes are unified and learned during the supervised cold-start stage, and the mode selection capability is induced via an RL process with a carefully designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively guides the model to learn and differentiate multiple modes and perform context-adaptive mode selection, achieving consistent improvement across various scenarios, highlighting MoVT as an effective solution for building general visual reasoning models.", "tldr": "We introduce an mixture-of-visual-thoughts paradigm that unifies different visual reasoning modes within a model and guides it to adaptively select the appropriate mode based on context, achieving consistent gains across various scenarios.", "keywords": ["visual reasoning", "adaptive reasoning", "multimodal large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a45c2e994777f406b57179547c2f861a75e08d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Mixture‑of‑Visual‑Thoughts (MoVT), an adaptive reasoning paradigm that unifies text‑based and visually‑grounded chains of thought within a single LVLM and learns to select the appropriate mode per instance. The approach is instantiated via AdaVaR, a two‑stage pipeline: (1) SFT that teaches both modes using mode‑prefix tokens (e.g., <text>, <ground>), followed by (2) RL with AdaGRPO, an extension of GRPO introducing prefix‑guided mode exploration, a mode‑relative advantage signal for selection, and a curriculum from simpler, clearer tasks to more diverse ones. Experiments on eight benchmarks (covering math, spatial reasoning, hallucination, visual search, and general perception) show consistent gains over strong specialists and base models; the AdaVaR‑7B variant reportedly surpasses GPT‑4o on average, with ablations and an “oracle upper bound” analysis illustrating the complementarity of modes and the value of adaptive selection."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Clear, novel problem framing offering a compelling path toward generalist multimodal reasoning. \n\n- AdaGRPO is purpose‑built for mode selection, combining (i) prefix‑guided exploration to avoid mode collapse, (ii) a mode‑relative advantage estimator to directly train the gating decision, and (iii) a curriculum that stabilizes learning before exposing the agent to noisy, diverse tasks. \n\n- Strong empirical results and breadth. Evaluation on eight varied benchmarks demonstrates robust, cross‑domain improvements; ablations show each component’s necessity. \n\n- Well‑presented and easy to follow."}, "weaknesses": {"value": "- Scalability to k>2 reasoning modes (e.g. symbolic, tool‑augmented, variable CoT length) could be explored; naive exploration would scale rollout cost linearly with k and may become prohibitive without hierarchical or more efficient routing.  \n\n- The current setup enforces a single mode for the entire reasoning process, whereas many tasks might benefit from intra‑problem switching (e.g., ground first, then abstract reasoning).  \n\n- Prefix‑guided exploration requires n rollouts per mode (vs. 2n from a single policy in vanilla GRPO), increasing cost; quantitative training FLOPs, wall‑clock, and inference latency comparisons could be elaborated upon.  \n\n- The 1:1 SFT mix is reasonable but not properly analyzed; understanding sensitivity to data quality/quantity and skewed ratios (e.g., 3:1) would aid reproducibility and deployment.  \n\n- More systematic characterization of when mode selection fails (and why), interpretability of the gate’s decisions, and comparisons to simple ensembling would strengthen the paper."}, "questions": {"value": "- How would AdaGRPO’s mode‑relative advantage generalize beyond two modes—pairwise comparisons, a shared latent score, or another efficient scheme? Any preliminary results or complexity analysis?  \n\n- What fraction of errors are due to wrong mode choice versus within‑mode reasoning errors? Are there identifiable categories (e.g., deceptive diagrams) where the gate is brittle?  \n\n- Have you tried the framework with non‑Qwen LVLMs? Any architectural prerequisites for effective selection?  \n\n- Can you discuss multi-step or long-context reasoning—can the model switch modes dynamically inside a single reasoning chain? \n\n- There is limited interpretability of the mode selection. The paper does not sufficiently explain why the model chooses one mode over another beyond empirical observation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yCnR765PWY", "forum": "8qk6eUnvbH", "replyto": "8qk6eUnvbH", "signatures": ["ICLR.cc/2026/Conference/Submission12932/Reviewer_Yq9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12932/Reviewer_Yq9o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738169831, "cdate": 1761738169831, "tmdate": 1762923696550, "mdate": 1762923696550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel paradigm, Mixture-of-Visual-Thoughts (MoVT), and a corresponding two-stage training framework, AdaVaR, to build a general-purpose visual reasoning model. The core innovation lies in unifying different reasoning modes,specifically, text-based reasoning and visually-grounded reasoning within a single model and enabling it to adaptively select the appropriate mode based on the input context.Extensive experiments demonstrate that models trained with AdaVaR achieve consistent and superior performance across diverse benchmarks (mathematical, spatial, object-centric), outperforming specialized single-mode models and even surpassing GPT-4o on average accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well written and easy to follow\n2.The paper introduces the  \"Mixture-of-Visual-Thoughts\" paradigm, which systematically integrates different visual reasoning modes within a unified model and enables context-adaptive mode selection. \n3.The designed AdaVaR framework employs a logical two-stage training process. Through specific techniques such as mode prefixes, prefix-guided exploration, and mode-relative advantages, it successfully achieves multi-mode learning and adaptive selection. Experiments demonstrate consistent performance improvements."}, "weaknesses": {"value": "1.The Prefix-guided Mode Exploration mechanism in AdaGRPO enforces a rigid, equal exploration of the two predefined reasoning modes (text-based and visually-grounded) for each input sample. While this hard constraint effectively prevents mode collapse, it introduces a fundamental limitation: the exploration space is artificially confined to a discrete set of predetermined patterns. I concern this design may prematurely restrict the model's policy search to this predefined mode space, thereby limiting its capacity to discover potentially superior strategies that fall outside these rigid categories. For instance, the optimal strategy for a given problem might involve a hybrid or sequential application of both modes, or even an entirely novel reasoning path not encapsulated by either predefined mode.\n2.I think the classification of reasoning into strictly \"text-based\" and \"visually-grounded\" modes is conceptually limiting. we don't need to force the model to choose a single, rigid strategy at the start of its reasoning process, which is an unnatural constraint."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AtOBBlfWu8", "forum": "8qk6eUnvbH", "replyto": "8qk6eUnvbH", "signatures": ["ICLR.cc/2026/Conference/Submission12932/Reviewer_NdeY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12932/Reviewer_NdeY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898612277, "cdate": 1761898612277, "tmdate": 1762923696214, "mdate": 1762923696214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of existing visual reasoning models that focus on single reasoning modes and lack generalizability. It proposes a novel adaptive visual reasoning paradigm called Mixture-of-Visual-Thoughts (MoVT), which unifies two common reasoning modes (text-based and visually-grounded) into a single model and enables context-adaptive mode selection. To implement MoVT, the authors design the AdaVaR framework with a two-stage training process: a supervised cold-start stage (SFT) to unify and learn multiple reasoning modes, and a reinforcement learning (RL) stage using the proposed AdaGRPO algorithm to induce mode selection capability. Extensive experiments on 8 benchmarks (covering math-oriented and general scenarios) show that AdaVaR models (AdaVaR-3B and AdaVaR-7B) achieve consistent performance improvements, with AdaVaR-7B surpassing GPT-4o in average accuracy. The paper's main contributions include: proposing the MoVT paradigm for general visual reasoning, developing the two-stage AdaVaR framework, and validating the effectiveness of AdaGRPO and the overall method through experiments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The proposed MoVT paradigm addresses a critical gap in existing visual reasoning research by integrating complementary strengths of text-based and visually-grounded modes, breaking the domain limitation of single-mode models and providing a feasible solution for building general visual reasoning models.\n\n(2) The two-stage AdaVaR framework (SFT+RL) is practically designed: the SFT stage solves mode unification via a uniform sequence format with mode-specific prefixes, while the RL stage (with AdaGRPO) targets mode selection, ensuring the model both learns multiple modes and adapts to context.\n\n(3) The experimental design is relatively comprehensive: the paper evaluates on diverse benchmarks (math, visual search, hallucination control, etc.), compares with multiple single-mode baselines, and conducts ablation studies on key components (Ada-Adv, PG-Exp), providing sufficient evidence for the method's effectiveness."}, "weaknesses": {"value": "(1) In the SFT stage of AdaVaR, the paper sets the data mixing ratio of text-based and visually-grounded reasoning to 1:1 but provides no justification for this ratio. It neither explains why 1:1 is superior to other ratios (e.g., 2:1, 1:2) nor conducts ablation experiments to verify whether different ratios affect mode learning or subsequent RL-based mode selection. This unsubstantiated setting makes it unclear whether the ratio is optimal or merely an empirical choice, reducing the methodological rigor.\n\n(2) The paper mentions that the KL penalty term is omitted in the AdaGRPO formula (Section 3.3.1) but uses a KL penalty with a coefficient of 0.04 in training (Appendix A.3). However, it fails to explain the basis for choosing the 0.04 coefficient—for example, whether it was determined through experimental tuning (e.g., testing coefficients like 0.01, 0.02, 0.05) or based on prior work, and how different KL coefficients influence the model's mode selection bias and reasoning accuracy. The lack of this information impairs the reproducibility of the algorithm.\n\n(3) The unified reasoning format uses <text> and <ground> as mode prefixes (Section 3.1), but the paper does not explore the impact of prefix design on model performance. It neither tests whether longer prefixes (e.g., <text-based-reasoning>) or more distinguishable prefixes (e.g., <txt> vs. <grd>) affect the model's ability to distinguish modes nor verifies whether prefixes introduce unintended inductive biases (e.g., <ground> being more salient than <text>, leading the model to prefer it). This oversight leaves potential factors influencing mode selection unaddressed.\n\n(4) In the evaluation benchmarks (Section 4.1), the paper omits several widely used visual reasoning benchmarks, such as CLEVR (for compositional visual reasoning) and GQA (for structured visual question answering). Excluding these benchmarks makes it impossible to evaluate the model's performance on compositional or structured reasoning tasks, limiting the generalizability of its claim to \"general visual reasoning.\"\n\n(5) For some comparison models (e.g., Qwen2.5-VL-3B, LMM-R1), the paper marks results with \"*\" indicating reproduction but provides no details on the reproduction process. It does not specify whether the reproduction strictly followed the original paper's training settings (e.g., learning rate, batch size), whether there were discrepancies between reproduced results and original results, or how such discrepancies were resolved. This opacity raises concerns about the fairness of comparisons between AdaVaR and these baseline models.\n\n(6) The mode-switching mechanism (Section 4.1) is described as \"switching to another mode if the model gets stuck in repetitive logic,\" but the paper provides no quantitative definition of \"getting stuck in repetitive logic.\" It neither clarifies how many consecutive repeated tokens or logical loops constitute \"being stuck\" (e.g., 3 repetitions vs. 5 repetitions) nor verifies through ablation experiments whether this mechanism actually improves performance (e.g., comparing accuracy with and without mode switching). The lack of clear criteria makes the mechanism vague and difficult to reproduce.\n\n(7) In qualitative analysis cases (e.g., Figure 7: MathVista-General), the visually-grounded (GRD) mode incorrectly answers the donut question (\"No\") while the text-based (TXT) mode answers correctly, but the paper does not analyze the root cause of the GRD mode's error. It fails to clarify whether the error stems from incorrect bounding box localization, flaws in visual-information-based reasoning, or mode selection bias, weakening the depth of the qualitative analysis.\n\n(8) The Ethics Statement claims all data sources are open-source but provides no specific proof of authorization for using VoCoT (Li et al., 2025) data. It neither cites VoCoT's license type (e.g., MIT, CC BY-NC) nor confirms that data reuse complies with VoCoT's terms of use. This omission raises concerns about potential copyright issues with the data.\n\n(9) The Reproducibility Statement mentions that code, models, and data will be open-sourced after review, but the current version provides no preprint link, temporary repository, or sample data. This prevents reviewers from verifying key implementations (e.g., advantage calculation in AdaGRPO) or preliminary results, hindering the assessment of reproducibility.\n\n(10) The paper provides no theoretical analysis of AdaGRPO's convergence. It does not prove whether the algorithm can converge to a stable policy, how the three key components (e.g., prefix-guided exploration) affect convergence speed, or under what conditions the algorithm can avoid mode collapse (e.g., always selecting one mode). The lack of theoretical guarantees makes the algorithm's stability during long-term training questionable.\n\n(11) The paper lists extending MoVT to more reasoning modes (e.g., direct answering) as a future direction (Section B.4) but provides no preliminary experimental evidence. It neither tests whether AdaVaR can still learn mode selection after adding a <direct-answer> mode nor analyzes whether increasing the number of modes exacerbates exploration difficulty or reduces performance. This leaves MoVT's claimed scalability unsupported.\n\n(12) The paper does not discuss the model's training efficiency. For example, AdaVaR-7B is trained using 32 NVIDIA A100 GPUs for 29 hours (Table 4), but it does not compare this training cost (e.g., GPU hours, energy consumption) with other models (e.g., GPT-4o, Qwen2.5-VL-7B). This omission makes it impossible to evaluate whether AdaVaR's performance improvements are worth the increased computational cost, limiting its practical application value.\n\n(13) The paper does not mention the inference latency introduced by adaptive mode selection. It neither measures whether the mode selection process (generating prefixes, evaluating mode advantages) increases inference time compared to single-mode models nor discusses optimization strategies to reduce latency (e.g., early mode selection). This is a critical oversight for practical applications (e.g., edge devices with latency constraints).\n\n(14) In RL data filtering (Section 3.3.2), the paper removes \"easy questions that Qwen2.5-VL-7B can answer correctly in 8 random samples\" but provides no basis for choosing 8 as the threshold. It neither tests whether other thresholds (e.g., 5, 10) retain more useful data or reduce noise nor analyzes the distribution (e.g., task type, difficulty) of retained and removed data to confirm whether filtering improves data quality.\n\n(15) For SFT data, the paper uses GPT-4o to rewrite a subset of VoCoT data into multiple-choice format (Appendix A.2) but does not evaluate the quality of the rewritten data. It neither conducts manual verification to check whether GPT-4o introduces errors (e.g., incorrect options) or biases (e.g., overly simple options) nor compares model performance trained on original vs. rewritten data to verify the impact of rewriting.\n\n(16) The experimental results lack statistical significance analysis. When reporting accuracy values (e.g., Table 1), the paper provides no error bars, confidence intervals, or p-values. For example, it claims AdaVaR-7B's average accuracy surpasses GPT-4o (55.82 vs. 53.20) but does not confirm whether this difference is statistically significant (e.g., via t-test). This makes it impossible to determine whether performance improvements stem from the method itself or random fluctuations.\n\n(17) The paper reports GRD% (proportion of choosing the grounded mode) across datasets (Table 2) but does not analyze why the model strongly prefers the grounded mode in some datasets (e.g., POPE: 100% GRD%). It neither rules out overfitting to the POPE data distribution (e.g., all POPE tasks require object localization, leading the model to always select GRD) nor confirms whether this preference is true context adaptation rather than dataset-specific bias.\n\n(18) The paper notes that AdaVaR-7B (7B parameters) is more inclined to select the text-based mode for knowledge-related categories (e.g., geography) compared to AdaVaR-3B (3B parameters) (Section B.1) but does not analyze the root cause of this difference. It neither explores whether the 7B model's richer pre-trained knowledge reduces its reliance on visual grounding nor verifies whether the RL process for the 7B model is more likely to prioritize text-based reasoning, leaving the impact of model size on mode selection unaddressed.\n\n(19) The ablation experiment (Table 3) tests the impact of Ada-Adv, PG-Exp, diverse mixed data, and curriculum learning but omits ablation of the \"mode-specific prefix\" (a key component of mode unification in Section 3.1). It does not compare model performance with and without prefixes to confirm whether prefixes are essential for mode differentiation, weakening the completeness of ablation analysis.\n\n(20) The paper does not provide details on how multi-modal data (images+text questions) is processed in the SFT and RL stages. It neither clarifies whether images are preprocessed identically across modes (e.g., resolution, feature extraction) nor explains how the model fuses visual and textual information during mode-specific reasoning (e.g., whether visual features are weighted differently in GRD vs. TXT modes), leading to ambiguity in the multi-modal integration process.\n\n(21) The paper does not evaluate the model's robustness to image noise or occlusion. It neither tests whether AdaVaR's mode selection adapts appropriately when input images contain noise (e.g., Gaussian blur) or object occlusion (e.g., a partially covered donut) nor compares robustness with single-mode models. This is critical for assessing the model's practicality in real-world scenarios where images are often imperfect.\n\n(22) The paper does not conduct user studies to evaluate the readability or utility of generated reasoning processes. It neither assesses whether humans can easily understand the GRD mode's bounding box-integrated reasoning or the TXT mode's textual logic nor analyzes whether reasoning processes enhance trust in the model's answers (a key factor for real-world adoption), limiting the assessment of the method's practical value.\n\n(23) The reference list contains inconsistent formatting and incomplete entries. For example, OpenAI (2025) (Section 1.10) lacks authors and a full title (only \"Thinking with images\" and a URL), while some entries (e.g., DeepSeek-AI et al., 2025) have overly long author lists that could be abbreviated. This reduces the paper's professionalism and makes it difficult for readers to locate and verify cited works.\n\n(24) The optimization objective formula of AdaGRPO (Section 3.3.1, Equation in Section 3.3.1) contains a typo: the denominator repeatedly includes \\(\\pi_{\\theta_{old}}(o_{j}|i,q,o_{j,<t})\\), causing confusion about the correct formula. The paper does not correct this typo or provide a clear derivation, which may mislead readers and hinder implementation.\n\n(25) Algorithm 1 (Appendix A.3) for mode-relative advantage calculation uses \\(r[1..2n]\\) but does not specify the order of rewards (e.g., whether \\(r[1..n]\\) corresponds to text-based or visually-grounded rollouts). This ambiguity could lead to incorrect implementation of the advantage calculation, undermining reproducibility.\n\n(26) The paper initializes AdaVaR based on Qwen2.5-VL (Section 3.2) but does not specify the exact version (e.g., Qwen2.5-VL-Instruct vs. base Qwen2.5-VL) or confirm whether the pre-training data of Qwen2.5-VL is consistent across all compared models. If AdaVaR uses a more capable pre-trained base model (e.g., the Instruct version with better initial reasoning), performance gains could stem from the base model rather than the proposed framework, distorting the evaluation of the method's effectiveness.\n\n(27) The paper does not evaluate the model's performance under low-resource conditions. It neither tests how AdaVaR performs when training data is reduced (e.g., 50% of the original data) nor compares it with single-mode models to determine whether adaptive reasoning maintains its advantage in data-scarce scenarios. This limits the assessment of the method's data efficiency.\n\n(28) The paper calculates average accuracy across benchmarks (Table 1) but does not specify the weighting scheme. It neither states whether all benchmarks are weighted equally nor whether weighting is based on dataset size, nor analyzes how different weighting schemes affect average accuracy (e.g., if MathVista has more samples, size-based weighting could inflate the average). This ambiguity makes fair comparison of AdaVaR's overall performance with other models difficult.\n\n(29) The SFT stage uses 1 training epoch (Table 4), but the paper provides no justification for this choice. It neither tests whether more epochs (e.g., 2, 3) improve mode learning or cause overfitting nor analyzes the SFT stage's training loss curve to confirm that 1 epoch is sufficient for convergence.\n\n(30) The RL stage generates 8 rollouts per sample (4 text-based, 4 visually-grounded) (Appendix A.3), but the paper does not explain why 8 is chosen. It neither tests other rollout counts (e.g., 4, 16) to see if more rollouts improve mode comparison accuracy nor checks if fewer rollouts reduce computational cost without performance loss.\n\n(31) The paper provides training dynamics for math tasks (Figure 3) but only partial curves for other tasks (Figure 6). It does not present detailed training dynamics (e.g., accuracy, reward, GRD% over steps) for all benchmark categories (e.g., POPE, V*), making it difficult to fully understand how the model learns mode selection across different task types.\n\n(32) In qualitative analysis cases (e.g., Figure 7: MathVista-Geometry), the paper does not provide clear visual details of input images (e.g., the side lengths of the right triangle). Reviewers cannot verify whether the model's reasoning (e.g., using cosine or 30-60-90 triangle properties) is correct, weakening the credibility of qualitative results.\n\n(33) The paper uses \"reasoning mode\" and \"chain-of-thought (CoT)\" interchangeably (Section 1.10) but does not clearly define their relationship. It neither distinguishes whether a \"reasoning mode\" is a type of CoT nor clarifies if it is a broader concept, leading to confusion with existing CoT literature (e.g., whether MoVT's modes are compatible with other CoT variants).\n\n(34) The paper does not evaluate zero-shot generalization to unseen tasks. It neither tests AdaVaR on visual reasoning tasks not included in training (e.g., video-based reasoning, medical image reasoning) nor verifies whether it can adaptively select modes for new tasks, which is essential for validating the \"general visual reasoning\" claim.\n\n(35) The ablation experiment (Table 3) does not test the impact of data order in curriculum learning (e.g., training on diverse mixture first, then binary mixture). It does not confirm whether the \"easy-to-hard\" order is truly optimal or if other orders (e.g., hard-to-easy) yield better performance, limiting the analysis of curriculum learning's effectiveness.\n\n(36) The paper does not explain how the model makes mode selection decisions for individual samples. It neither provides a breakdown of the factors influencing mode selection (e.g., question complexity, image content richness) nor visualizes the decision process (e.g., attention weights on mode prefixes), making the mode selection mechanism opaque.\n\n(37) The paper does not discuss the model's adaptation to different image types (e.g., natural images, diagrams, documents). It neither tests whether AdaVaR adjusts mode selection appropriately for different image types (e.g., preferring TXT for diagrams, GRD for natural images) nor compares performance across image types, limiting the assessment of its generalizability across visual domains.\n\n(38) The paper does not control for the length of reasoning processes across modes. It neither ensures that reasoning processes in TXT and GRD modes have similar lengths (to avoid performance differences due to longer reasoning) nor analyzes how reasoning length affects accuracy, leading to potential confounding factors in performance comparisons.\n\n(39) The paper does not explore the trade-off between model size and performance. It neither tests whether smaller models (e.g., AdaVaR-1B) can still benefit from adaptive reasoning nor analyzes whether the performance gain of AdaVaR-7B over AdaVaR-3B is proportional to the increase in parameters, limiting the understanding of the method's scalability with model size.\n\n(40) The paper does not address the model's performance in cross-lingual visual reasoning. It only focuses on English scenarios (Ethics Statement) and does not test whether AdaVaR can adapt mode selection for non-English questions (e.g., Chinese, Spanish) nor compare cross-lingual performance with single-mode models, limiting its applicability in multilingual contexts."}, "questions": {"value": "*To facilitate discussions during the Rebuttal phase, authors are advised to respond point-by-point (indicating the question number).*\n\n(1) For the 1:1 data mixing ratio in the SFT stage, could you provide ablation experimental results comparing different ratios (e.g., 2:1, 1:2, 3:1) and explain why 1:1 is optimal for mode unification and subsequent RL training?\n\n(2) Regarding the KL penalty coefficient of 0.04 in the RL stage, could you detail the tuning process (e.g., tested coefficient values, performance changes with different coefficients) and explain how you determined that 0.04 balances mode selection bias and reasoning accuracy?\n\n(3) For the mode-switching mechanism, could you provide a quantitative definition of \"getting stuck in repetitive logic\" (e.g., number of consecutive repeated tokens, threshold for logical loops) and present ablation results comparing model accuracy with and without this mechanism?\n\n(4) For reproduced results of baseline models (marked with \"*\"), could you provide detailed training settings (e.g., learning rate, batch size, number of epochs) and compare them with the original papers? If there were discrepancies between reproduced and original results, how did you resolve them?\n\n(5) In the MathVista-General case (Figure 7), the GRD mode incorrectly answered the donut question. Could you analyze the root cause (e.g., bounding box localization error, flawed visual reasoning) and provide evidence (e.g., localization results, intermediate reasoning steps)?\n\n(6) In the Ethics Statement, you claim to use open-source data from VoCoT (Li et al., 2025). Could you provide specific proof of authorization (e.g., VoCoT's license text, confirmation of compliance with terms) to address potential copyright concerns?\n\n(7) In the Reproducibility Statement, you mention open-sourcing code, models, and data after review. Could you provide a clear timeline (e.g., date of release after acceptance) and a temporary link to sample data or a preprint repository to allow reviewers to verify key implementations?\n\n(8) Regarding AdaGRPO's convergence, could you supplement theoretical analysis (e.g., proof of convergence to a stable policy) or empirical evidence (e.g., convergence curves of policy loss over training steps) to confirm the algorithm's stability?\n\n(9) For extending MoVT to more reasoning modes (e.g., direct answering), could you provide preliminary experimental results (e.g., performance after adding a <direct-answer> mode) to support the claim of scalability?\n\n(10) Could you compare the training cost (e.g., GPU hours, energy consumption) of AdaVaR with other models (e.g., GPT-4o, Qwen2.5-VL-7B) and explain whether the performance improvement justifies the increased computational cost?\n\n(11) Could you measure the inference latency of AdaVaR (including mode selection time) and compare it with single-mode models? Additionally, do you have optimization strategies (e.g., early mode selection) to reduce latency for edge device applications?\n\n(12) For the RL data filtering threshold of 8 (Qwen2.5-VL-7B answering correctly in 8 random samples), could you conduct a sensitivity analysis (e.g., testing thresholds 5, 10) and analyze how different thresholds affect data distribution and model performance?\n\n(13) For SFT data rewritten by GPT-4o into multiple-choice format, could you provide results of manual quality assessment (e.g., error rate of options, bias in difficulty) and compare model performance trained on original vs. rewritten data?\n\n(14) Could you supplement statistical significance analysis for experimental results (e.g., p-values via t-test, confidence intervals) to confirm that AdaVaR's performance improvements over baselines are not due to random fluctuations?\n\n(15) For the 100% GRD% in POPE, could you analyze the dataset distribution (e.g., proportion of tasks requiring object localization) and conduct experiments on a modified POPE subset (with non-localization tasks) to verify whether the preference is context-adaptive or dataset-specific?\n\n(16) Regarding AdaVaR-7B's greater preference for text-based mode in knowledge-related categories, could you provide more data (e.g., GRD% across more knowledge categories) and analyze whether this is due to the 7B model's richer pre-trained knowledge or differences in the RL process?\n\n(17) Could you supplement an ablation experiment on \"mode-specific prefixes\" (comparing performance with and without prefixes) to confirm their necessity for mode differentiation?\n\n(18) Could you detail the processing of multi-modal data (images+text questions) in SFT and RL stages (e.g., image preprocessing steps, visual-text fusion methods) to clarify the multi-modal integration process?\n\n(19) Could you supplement experiments on model robustness (e.g., testing with noisy/occluded images) and compare AdaVaR's robustness with single-mode models to assess its practicality in real-world scenarios?\n\n(20) Do you plan to conduct user studies to evaluate the readability of reasoning processes and their impact on trust in model answers? If so, could you outline the study design (e.g., number of participants, evaluation metrics)?\n\n(21) Could you correct the formatting errors in the reference list (e.g., completing OpenAI (2025)'s title and authors, abbreviating long author lists) to improve professionalism and ease of verification?\n\n(22) Could you correct the typo in AdaGRPO's optimization objective formula and provide a clear derivation to avoid misleading readers during implementation?\n\n(23) In Algorithm 1, could you specify the order of rewards in \\(r[1..2n]\\) (e.g., \\(r[1..n]\\) for text-based rollouts) to ensure correct implementation of advantage calculation?\n\n(24) Could you confirm the exact version of the Qwen2.5-VL base model (e.g., Instruct vs. base) and verify that the pre-training data is consistent across all compared models to rule out base model advantages as a factor in performance gains?\n\n(25) Could you supplement experiments on low-resource conditions (e.g., training with 50% of original data) and compare AdaVaR's performance with single-mode models to demonstrate its data efficiency?\n\n(26) Could you clarify the weighting scheme for average accuracy across benchmarks (e.g., equal weighting, size-based weighting) and show how different schemes affect the average accuracy ranking of AdaVaR and baselines?\n\n(27) For the SFT stage's 1 training epoch, could you provide the training loss curve and ablation results with more epochs (e.g., 2, 3) to confirm that 1 epoch is sufficient for convergence and avoids overfitting?\n\n(28) For the RL stage's 8 rollouts per sample, could you conduct an ablation experiment (e.g., testing 4, 16 rollouts) and explain how the number of rollouts affects mode comparison accuracy and computational cost?\n\n(29) Could you provide complete training dynamics (e.g., accuracy, reward, GRD%) for all benchmark categories (e.g., POPE, V*) to allow a comprehensive understanding of mode selection learning across tasks?\n\n(30) For qualitative analysis cases (e.g., Figure 7: MathVista-Geometry), could you supplement clear visual details of input images (e.g., labeled side lengths of the triangle) to allow reviewers to verify the correctness of the model's reasoning?\n\n(31) Could you clarify the relationship between \"reasoning mode\" and \"CoT\" (e.g., whether modes are CoT variants) to avoid confusion with existing CoT literature?\n\n(32) Could you supplement zero-shot generalization experiments on unseen tasks (e.g., video-based reasoning) to validate AdaVaR's claim of \"general visual reasoning\"?\n\n(33) For curriculum learning, could you conduct an ablation experiment on data order (e.g., training on diverse mixture first) to confirm that the \"easy-to-hard\" order is optimal?\n\n(34) Could you visualize the mode selection decision process (e.g., attention weights on mode prefixes, feature importance of question/image) to make the mechanism more transparent?\n\n(35) Could you test AdaVaR's performance across different image types (e.g., natural images, diagrams, documents) and analyze whether mode selection adapts appropriately to each type?\n\n(36) Could you control the length of reasoning processes across modes (e.g., enforcing similar lengths) and re-evaluate performance to rule out reasoning length as a confounding factor?\n\n(37) Could you test smaller models (e.g., AdaVaR-1B) and analyze the relationship between model size and adaptive reasoning performance to clarify the method's scalability?\n\n(38) Do you have plans to extend AdaVaR to cross-lingual visual reasoning? If so, could you outline preliminary steps (e.g., training data preparation, mode selection adaptation for non-English questions)?\n\n(39) Could you supplement long-term training experiments (e.g., 2x the original training steps) to show whether AdaVaR avoids mode collapse and maintains stable performance?\n\n(40) Could you provide a systematic error analysis (e.g., breakdown of errors by mode, task type) to identify AdaVaR's current limitations and directions for improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r1Pd9RmCgF", "forum": "8qk6eUnvbH", "replyto": "8qk6eUnvbH", "signatures": ["ICLR.cc/2026/Conference/Submission12932/Reviewer_n9fj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12932/Reviewer_n9fj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919915308, "cdate": 1761919915308, "tmdate": 1762923695948, "mdate": 1762923695948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to augment reasoning models by allowing them to adaptively select  their reasoning patterns. In particular, it first prompts the models to choose between two reasoning modes, either focusing more on grounding objects or pure textual reasoning. A modified version of GRPO algorithm is then developed to train the models for mode selection, which splits the general rollouts and rewards into two types based on the reasoning modes. The method shows strengths in several types of reasoning scenarios, and experimental results validate its usefulness in selecting the right reasoning patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) It is an intuitive choice to adopt diverse reasoning modes for solving different problems, the proposed method demonstrates the advantages of explicit mode selection.\n\n(2) The proposed method demonstrates consistent improvement across multiple datasets.\n\n(3) The paper provides extensive analysis on the model behavior during training and contribution of individual components."}, "weaknesses": {"value": "(1) One concern I have is about the potentially limited scalability of the method. Specifically, solving real-life reasoning problems requires a collection of skills, while the method simplifies reasoning into textual and visual (grounding-based) reasoning. It is unclear how it can scale to larger domains in the current format, i.e., with the prompt listing all possible reasoning modes and explicit data/reward splitting in the training algorithm.\n\n(2) Related to the previous comment. How sensitive is the method to prompt design? Does prompting with more diverse reasoning modes work? What is the level of details needed to specify each reasoning mode? Does the prompt work for other models (in addition to Qwen2.5-VL)?\n\n(3) The paper evaluates mode selection based on two relatively extreme scenarios, i.e., math vs general, and largely ignores the variety of reasoning types in visual questions (e.g., probing the physical properties of multiple objects, comparing multiple objects, etc). It would be reasonable to further evaluate mode selection based on question types (e.g., on benchmarks with annotations like GQA)."}, "questions": {"value": "(1) Are there other important reasoning modes? How can the proposed method incorporate more diverse reasoning modes?\n\n(2) Please consider additional experiments on the effects of prompt design (related to questions above). It would also be useful to apply the same prompts on existing state-of-the-art.\n\n(3) Is the proportion of grounding mode sufficient to evaluate mode selection? Please justify.\n\n(4) How would the model choose between the two modes when answering visual questions with diverse focuses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JdXKTfUoDH", "forum": "8qk6eUnvbH", "replyto": "8qk6eUnvbH", "signatures": ["ICLR.cc/2026/Conference/Submission12932/Reviewer_kCu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12932/Reviewer_kCu9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12932/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965606914, "cdate": 1761965606914, "tmdate": 1762923695555, "mdate": 1762923695555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}