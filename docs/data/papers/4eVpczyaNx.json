{"id": "4eVpczyaNx", "number": 4379, "cdate": 1757669786521, "mdate": 1759898035752, "content": {"title": "Distilling Safe LLM Systems via Soft Prompts", "abstract": "Large Language Models (LLMs) have enabled machine learning to be integrated in complex tasks across various domains. This is a cause for concern since LLMs may respond to carefully crafted prompts with unsafe content, thus necessitating concrete safety mechanisms. Current solutions involve dual-model systems combining LLMs with guard models. However, the substantial memory and computational demands of guard models pose significant challenges for deployment. This paper proposes an efficient method for approximating the behavior of dual-model systems using learned embeddings, also known as soft prompts.We introduce a novel distillation framework which optimizes the total variation distance between the outputs of an LLM paired with a guard and the same LLM equipped with our soft prompts. At test time, the learned soft prompts are prepended to user prompts, providing safety at a fraction of the memory and compute costs incurred by a guard model. Our evaluations on various benchmarks demonstrate improved safety of the LLM, offering an efficient alternative to guard models for memory- and computation-constrained settings such as hardware applications.", "tldr": "We distill safe LLM systems into learnt embeddings for better memory and computational efficiency.", "keywords": ["LLM Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f5dbeef5ce604453bf58dd6483649f2d17536644.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TV-DiSP, a framework for distilling dual-model safety system, composed of a base LLM and a guard model, into a single model augmented with soft prompts. Instead of running a full guard model at inference time, the authors train soft prompt embeddings to minimize the total variation distance between the response distribution of the two-model system and that of the distilled single model. At test time, the learned soft prompts are prepended to user prompts, enabling the LLM to emulate the “safe” behavior of the guarded system with dramatically lower memory and compute overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Timely motivation: tackles a real deployment bottleneck—guard models are expensive for on-device inference.\n* Simplicity: soft-prompt distillation is lightweight, easy to reproduce, and compatible with quantized models.\n* Comprehensive experiments: includes multiple datasets, model sizes, and PEFT baselines.\n* Balanced evaluation: assesses both safety (SGS) and usefulness (MMLU).\n* Generalization results: shows modest gains even on out-of-distribution datasets like Detect-Jailbreak."}, "weaknesses": {"value": "* Limited novelty. TV-based loss and prompt distillation are modest extensions over prior PEFT safety methods (GuardFormer, SafeLoRA, Safety-Prefix, etc.).\n\n* Guard dependency. The method is entirely dependent on the guard model’s decisions, so any bias or blind spot in the guard is replicated in the distilled system.\n\n* No ablations on number or location of soft prompts. Appendix B.2 provides limited analysis (10–200 prompts) but no clear convergence curves.\n\n* Questionable evaluation metric. The “Safety Guard Score (SGS)” is derived from a different guard model (LlamaGuard 8B) and may not reflect true human-judged safety i.e llm as a safety judge might be incorrect/biased etc.\n\n* No latency measurements (token/sec) or deployment case studies despite mention of edge-device motivation."}, "questions": {"value": "1. How does TV-DiSP compare against supervised fine-tuning on guard-labeled data, without distillation?\n2. Did you evaluate safety degradation over time (catastrophic forgetting) when mixing safety and general downstream data?\n3. How sensitive is the method to the choice of guard model (e.g., LlamaGuard, Nvidia-Nemo Guard vs Granite Guardian)?\n4. Can this approach compound safety errors if the guard model mislabels benign responses?\n5. Would adding adversarial training or data augmentation improve robustness beyond imitation of the guard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eFVg4Z4ITh", "forum": "4eVpczyaNx", "replyto": "4eVpczyaNx", "signatures": ["ICLR.cc/2026/Conference/Submission4379/Reviewer_F8Yb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4379/Reviewer_F8Yb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761332334121, "cdate": 1761332334121, "tmdate": 1762917327898, "mdate": 1762917327898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TV-DiSP, a method that distills a dual-model safe LLM system (base model + guard) into a single model using soft prompts. The approach trains soft prompts by minimizing the total variation distance between the outputs of the safe system and the base model with soft prompts. This aims to retain safety while avoiding the compute and memory cost of a separate guard model. Experiments on several small instruction-tuned models show improved safety scores with limited overhead. The paper compares against alternative parameter-efficient tuning methods and loss objectives, and includes ablations on soft prompt size and a brief discussion of adversarial robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strength 1**: The paper proposes soft prompts to approximate the behavior of a dual-model safe LLM pipeline, improving safety while requiring only minimal additional compute and memory, making it suitable for edge deployment.\n\n**Strength 2**: The use of total variation as the distillation objective provides a clear performance deviation bound, contributing a sound theoretical foundation.\n\n**Strength 3**: Experiments cover multiple base models, different training distributions, in- and out-of-distribution safety benchmarks, and comparisons with several PEFT and loss-based baselines."}, "weaknesses": {"value": "**Weakness 1**: Both training supervision and evaluation rely on LlamaGuard models. The method may overfit guard model preferences instead of reducing real harmful behavior.\n\n**Weakness 2**: The only measure of usefulness reported is 5-shot MMLU accuracy, which is a weak proxy for real-world utility in safety-critical contexts. Safety mechanisms may increase refusal rates or interfere with instruction following, but the authors do not report metrics that quantify harmful refusal or degraded responsiveness. As a result, the safety–utility trade-off remains unclear.\n\n**Weakness 3**: Safety alignment requires high stability and low risk. Training only for a single epoch on a relatively small dataset does not provide evidence that the learned safety behavior is converged or robust. The reported improvements may not reflect a stable solution but rather transient behavior due to insufficient training."}, "questions": {"value": "Q1: Could the authors provide justification for limiting training to a single epoch? Since safety alignment requires reliable and stable behavior, additional evidence such as performance curves across training steps would help verify that the reported safety gains reflect a converged solution rather than transient behavior.\n\nQ2: Given the stated motivation of supporting efficient deployment, could the authors clarify why no Qwen3-series models were included? \n\nQ3: Could the authors provide explicit numerical results for compute and memory efficiency in tabular format? The current figures give helpful intuition, but concrete numbers would allow clearer comparisons across models and methods, especially for deployment-oriented readers.\n\nQ4: The discussion in Appendix Section C acknowledges the vulnerability of TV-DiSP to adversarial/jailbreak attacks but remains largely qualitative. Could the authors provide empirical results against recent prompt-based or embedding-level attack methods specifically targeting soft prompts? Even a small-scale evaluation would help quantify how much robustness is preserved compared to the dual-model system."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The method aims to replace a dual-model safety pipeline with a lightweight soft-prompt mechanism. While this improves efficiency, it also shifts the safety boundary into a small learned embedding space. If an attacker manages to bypass or suppress the influence of these soft prompts, the system may revert to unsafe behavior without the protection of a guard model. The paper currently provides limited empirical evidence on adversarial robustness, so the real-world deployment risk is not fully characterized. Additional validation would be needed before deployment in safety-sensitive scenarios."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PGsCf7s7Cs", "forum": "4eVpczyaNx", "replyto": "4eVpczyaNx", "signatures": ["ICLR.cc/2026/Conference/Submission4379/Reviewer_2MZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4379/Reviewer_2MZs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513305064, "cdate": 1761513305064, "tmdate": 1762917327605, "mdate": 1762917327605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TV-DiSP, a safety distillation framework that replaces a dual-model safe LLM system (base model + guard model) with a single LLM augmented using learned soft prompts. The method aims to approximate the behavior of the safe system by minimizing a total variation distance (TVD) objective between the two output distributions. The distilled model produces refusal responses for unsafe prompts without requiring the guard model at inference, reducing both memory and compute costs. Experiments demonstrate improved Safety Guard Score (SGS) with significantly reduced inference overhead."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed framework is conceptually simple and offers improvements in computational efficiency compared to dual-model systems while maintaining competitive safety performance with regards to Safety Guard Score (SGS) only.\n- Although TVD has been previously used in previous distillation works (e.g., [1]), this work presents empirical evidence that optimizing TV distance yields a better safety-utilization balance than KL divergence and REINFORCE on the benchmarks.\n\n[1] Wen et al., f-Divergence Minimization for Sequence-Level Knowledge Distillation, ACL 2023"}, "weaknesses": {"value": "- The rationale for TVD is explained only briefly (lines 153-154) and remains general. Theorem 3.1 concerns generic distributional closeness guarantees, but the paper does not sufficiently demonstrate why TVD is inherently more appropriate for safety alignment than other divergences. If there is no clear reason, it seems just applying different distance metric in distillation (also covered by previous work [1]) instead of introducing the new method for safety tuning.\n- Insufficient evaluation of model usefulness. Usefulness degradation is a critical part of safety alignment, yet the main safety-cost plots (Fig 2, 3, 4, 6) omit any utility measure. Only Fig 5 includes MMLU results, and it clearly reveals substantial performance drops compared to both the Base LLM and the Safe LLM system. There is also no deeper discussion of over-refusal and precision/recall or F1 over safety measures.\n- Distillation versus supervised training is not justified. If safety labels are available (either human or synthetic), a supervised fine-tuning baseline would be natural comparison. It should be explained why distillation of the guard model’s imperfect predictions is preferable and what types of errors the proposed approach is expected to inherit.\n- Comparison to alternative cost-reduction strategies is missing. If resource usage is the primary motivation, another reasonable baseline would be to distill large guard into a small guard and maintain a two-stage system [2]. This work does not discuss why shifting safety into soft prompts is preferable beyond efficiency, nor whether dual-model safety is fundamentally more robust.\n- Notation clarity. Section 3 uses a single symbol $p$ for both LLM generation and guard scoring, without clarifying model parameters, which complicates understanding.\n\n[1] Wen et al., f-Divergence Minimization for Sequence-Level Knowledge Distillation, ACL 2023\n\n[2] Lee et al., HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models, ICLR 2025"}, "questions": {"value": "- In Sec. 3, do the base LLM and guard model share any parameters? If not, please distinguish them with separate parameter sets (e.g., $\\theta$ for the LLM and $\\phi$ for the guard).\n- How is the distribution $p(r|x)$ represented in practice?\n- Have you evaluated the model under metrics that separately measure precision vs recall of safe refusals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0J7yGONyFp", "forum": "4eVpczyaNx", "replyto": "4eVpczyaNx", "signatures": ["ICLR.cc/2026/Conference/Submission4379/Reviewer_txFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4379/Reviewer_txFF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634253715, "cdate": 1761634253715, "tmdate": 1762917327261, "mdate": 1762917327261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to distil an LLM-guard as additional prompts in the prompt embedding space to boost the LLM safety. Their approach works under the paradigm of a dual-model system, where after every generation, an LLM guard assesses the model's output to be harmful given the prompt and overrides the response with a fixed refusal response if it is harmful. Such systems can often be non-scalable due to prohibitive inference cost. Thus, they propose training additional input parameters in the prompt input embedding space to directly train the model to generate the refusal response based on the probability of the harm from the guard model. Experimental results show a tradeoff between improved safety of the generated responses and reduced compute, as assessed by the LLM-guard scores of the generations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is mostly well-written with minimal typos and is easy to follow. \n- It is well-motivated to address the inference cost of such classifier-based guardrails.\n- Experimental results show that the inference computation remains similar to the original model while enhancing its safety.\n- Both in and out-of distribution datasets are compared for comprehensiveness.\n- Use of the guard model enables on-policy training and is shown by the use of PPO but is not fully highlighted."}, "weaknesses": {"value": "- Performance gains are very sensitive to the guard model used, as Table 2 shows minimal gains as compared to Figure 1. Other metrics should be included that leverage the ground-truth labels of these. \n- Performance should be split between harm-inducing and safe prompts separately to see if it is not overrefusing. \n- Figure 5 shows little usefulness of the tuned model, which means the training is overrefusing and not able to clearly separate safe and harmful prompts.\n- Since a simple safety finetuning can also have the same gains in reducing the inference cost and increasing the safety, the motivation behind the architectural design is not clear. Low performance of LoRA is not clear and should be further investigated in Figure 4. \n- It is not clear why figures 4 and 5 show a trend with respect to the learning rate since only the best performance should be compared with the two trade-off metrics instead. \n- Experimented models are limited in size (upto 3B models, quantized to 4 bit), leaving open questions about generalization to larger or unquantized models. \n- Figure 6 should be extended to Toxigen and other LLMs. \n- More discussion is needed regarding the use of many samples but only a single epoch for training. \n- KL is shown to outperform the TV-DiSP method but it is not clear why the authors have still decided to go with the TV-DiSP loss. \n- While it is highlighted that number of prompts can be tuned to hit a balance between the inference compute and safety, it should be clearly presented in the main figure by showing TV-DiSP with different number of prompts. \n- In addition to the Llamaguard > 0.5 score, the exact average llamaguard scores and LLM judge based rejection evaluations of the generations should also be included, following existing works in the literature. \n- Examples of generated responses are not included. \n- It is not clear why existing benchmarks (JailbreakBench, StrongReject) are not used (with their jailbreaks). Furthermore, the details of the self-curated benchmark, DetectJailbreak, are not provided. \n  - Chao, Patrick, et al. \"Jailbreakbench: An open robustness benchmark for jailbreaking large language models.\" Advances in Neural Information Processing Systems 37 (2024): 55005-55029.\n  - Souly, Alexandra, et al. \"A strongreject for empty jailbreaks.\" Advances in Neural Information Processing Systems 37 (2024): 125416-125440.\n- Missing related works: The architecture design is similar to Zheng et al., 2024 but is omitted from the discussion. Circuit breakers can also be seen as a fast fine-tuning strategy. \n  - Zheng, Chujie, et al. \"Prompt-driven llm safeguarding via directed representation optimization.\" ICML 2024. \n  - Zou, Andy, et al. \"Improving alignment and robustness with circuit breakers.\" Advances in Neural Information Processing Systems 37 (2024): 83345-83373.\n- Minor:\n  - Update the colors in Figure 1 right for better understanding. \n  - Line 362: across all"}, "questions": {"value": "- What is the effect of changing the loss function to have binary labels of safe and unsafe as opposed to the probability? Safe can be detected as the guard model's probability or ground-truth in the dataset. \n- see above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6AQsCq5gCB", "forum": "4eVpczyaNx", "replyto": "4eVpczyaNx", "signatures": ["ICLR.cc/2026/Conference/Submission4379/Reviewer_dR65"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4379/Reviewer_dR65"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4379/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849132055, "cdate": 1761849132055, "tmdate": 1762917326615, "mdate": 1762917326615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}