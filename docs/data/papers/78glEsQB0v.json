{"id": "78glEsQB0v", "number": 4691, "cdate": 1757747115197, "mdate": 1759898019796, "content": {"title": "SMixer: Rethinking Efficient-Training and Event-Driven SNNs", "abstract": "Spiking Neural Networks (SNNs) offer a promising, energy-efficient paradigm for computation, but their practical application is hindered by challenges in architecture design and training costs. For example, Spiking ResNet exhibits relatively low performance, whereas high-performance Spiking Transformers are not truly event driven and cannot be implemented on asynchronous chips. Moreover, the intrinsic time steps and neuron state dynamics result in a substantial computational overhead for training SNNs on GPUs. In response to these problems, we discuss rational architectural design for SNNs and argue that such designs should exhibit three key characteristics: operations fully supported by asynchronous scenarios, low training overhead and competitive performance. In light of this, we adopt the event-driven friendly Spiking Mixer (SMixer) as the foundational architecture and develop a spike feature Spatial-Temporal Pruning (STP) framework with a high pruning ratio and no trainable parameters to reduce the training overhead. Based on a statistical analysis of sparse spike features, STP eliminates redundant spike features across both spatial and temporal dimensions, thereby reducing the input features and computational load during training. It adaptively selects the most salient spike tokens spatially and dynamically constrains neuron firing rates temporally. By leveraging STP and architectural adaptation, SMixer accelerates training while ensuring a fully event-driven characteristics and maintaining competitive performance, offering valuable insights for the design of efficient, event-driven SNNs.", "tldr": "", "keywords": ["Spiking Neural Network", "Efficient Computation", "Prune Method"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2700b2e87465b4b033c3b6419ec25eef2f5c744.pdf", "supplementary_material": "/attachment/22725cacdb46d0dde0cf384303670e8a8b50599f.zip"}, "replies": [{"content": {"summary": {"value": "This study proposes a Spiking-token Mixer, which completely eliminates self-attention computation in SNN-based Transformers while maintaining comparable accuracy. Additionally, a DSTSP module is introduced to effectively improve training efficiency. Overall, the proposed approach demonstrates good generalizability and shows potential for deployment on edge neuromorphic hardware."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method fully avoids the computational cost of self-attention while achieving accuracy comparable to state-of-the-art models.\n\n2. The plug-and-play DSTSP module can be seamlessly integrated into various architectures, demonstrating strong applicability."}, "weaknesses": {"value": "1. The analysis in Figure 4 lacks detail. In particular, rough explanations of RIE and MS-SSIM should be provided. Additionally, the claim that SIV in SNNs performs better than in ANNs may warrant theoretical justification, which should not be difficult to provide.\n\n2. The order of introducing DSSP and DSTP could be revised. The paper first mentions DSTP followed by DSSP, whereas the detailed descriptions appear in the opposite order. It is recommended to maintain consistency between textual structure and figure presentation.\n\n3. Table 5 appears to contain incorrect annotations. Does “E-SpikeFormer” refer to Spike-driven Transformer V3 (TPAMI)? It seems the table might have been copied, and the label “(This Work)” was not removed. In this case, once I-LIF neurons remove the temporal dimension during training, could the observed performance degradation be directly related to the ineffective functioning of DSTP? Further analysis is suggested.\n\n4. In this work, there is a trade-off between performance and pruning-based efficiency gains. However, the degradation in performance appears relatively large, while the improvement in efficiency is limited.\n\n5. Although the paper emphasizes deployment potential on brain-inspired edge hardware and points out the difficulty of QK matrix multiplication on such devices, no edge-hardware experiments are conducted.\n\n6. The DSTSP design is relatively simple—essentially the most basic pruning strategy in SNNs. If the pruning selection scheme were made more sophisticated, performance degradation could likely be reduced while providing greater acceleration."}, "questions": {"value": "1. The statement around line 166—“a model that uses only high-SIV tokens demonstrates considerably better accuracy than one relying on low-SIV tokens (97.9% vs 79.9%)”—needs clarification: which two models are being compared? To my knowledge, no mainstream SNN model should perform as poorly as 79.9% on this dataset.\n\n2. Around line 183, it is stated that SIV requires only a simple summation. However, ignoring the channel dimension, the number of required summations should be H × W × T. If this understanding is incorrect, clarification is needed. Additionally, because zero entries cannot be omitted, the claim that this operation is highly compatible with SNN computation seems unjustified—particularly since the paper does not discuss hardware implementation. This raises concerns regarding the true value of the Spiking-token Mixer.\n\n3. In Table 3, the reported accuracies of 98.12% and 98.43% on DVS128 are impossible, as the dataset contains only 288 test samples. Typically, such anomalies might arise if test-time ``droplast'' were enabled, but this is not the case here. Therefore, the authors must clearly explain how these erroneous accuracy values were obtained.\n\n4. Figure 6 indicates that a substantial amount of background noise persists even after applying DSTSP. Does this imply that the pruning strategy still has significant headroom for improvement? Theoretically, increasing pruning rates should not affect target features. How do the authors interpret this apparent inconsistency?\n\n5. Does the proposed DSTSP perform any operation during inference? If not, how is the change in activation size handled? If yes, wouldn’t such operations break the fundamental asynchronous timestep-by-timestep inference characteristic of SNNs? This requires clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e7LiV01b0v", "forum": "78glEsQB0v", "replyto": "78glEsQB0v", "signatures": ["ICLR.cc/2026/Conference/Submission4691/Reviewer_tRYC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4691/Reviewer_tRYC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308579765, "cdate": 1761308579765, "tmdate": 1762917513746, "mdate": 1762917513746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The whole work focus on designing a completely lightweight spiking neural network, and calling on a paradigm for developing SNN model. Overall, the spiking-token token-mixer(SMixer), and a pruning method (DSTSP) significantly decrease the parameters that model needs and computational cost while mataining competitive performance comparing with main stream SNN models. Moreover, the pruning method can be effectively applied to several models to decrease cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In the article, the author concerned about some issues in SNN nowadays. Indeed, in the pursuit of more competitive performance in SNN, some architecture in ANN are widely used. Nevertheless, the author tried to stop such methods, and further propose a new paradigm to constrain construction for SNN. This is pioneering and full of originality. Moreover, the new structure and pruning method significantly reduce number of parameters and computational cost, which SNN aims to achieve all along, not to mention the two maintain similar performance with main stream models. As for writing skills, authors basically clarify their thought process to develop the new structure of SNN and pruning method with little confusion."}, "weaknesses": {"value": "Firstly, in the preceding part of the article, the author mentioned Asynchronous Chip for several times. But no experiment about deployment on them was conducted.\nSecondly, as you present in Figure 6, I think it lacks persuasiveness just at first glance. Some objects' even can be harder to distinguish from background.\nLastly, in Table 2 (performance on ImageNet), if you add a item like \" Smixer+DSTSP\", it will be more persuasive."}, "questions": {"value": "I have some doubts about vertical axis meaning in Figure 4. I'd like to know more details about the intuition of Figure 4. \n\nFor other questions, please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OPjTHTW7gt", "forum": "78glEsQB0v", "replyto": "78glEsQB0v", "signatures": ["ICLR.cc/2026/Conference/Submission4691/Reviewer_2HdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4691/Reviewer_2HdN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758302098, "cdate": 1761758302098, "tmdate": 1762917513344, "mdate": 1762917513344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SMixer, a Spiking Neural Network architecture based on the Spiking-token Mixer paradigm, along with a Dynamic Spatial-Temporal Spiking Pruning (DSTSP) framework to reduce training overhead. The authors argue that rational SNN architectures should exhibit three key characteristics: fully event-driven operations, low training overhead, and competitive performance. They demonstrate that SMixer can replace Spiking Self-Attention mechanisms in various architectures while maintaining comparable performance, and that DSTSP can significantly reduce computational costs during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-motivated problem: The paper clearly identifies three important challenges in SNN design: the incompatibility of Spiking Transformers with asynchronous hardware, high training costs, and the need for competitive performance.\nComprehensive evaluation: The experiments span multiple domains (image classification, time series, object detection) and datasets (ImageNet, CIFAR, DVS-Gesture, COCO), demonstrating broad applicability.\nPractical approach: The DSTSP framework introduces no trainable parameters and uses simple addition-based operations, making it lightweight and aligned with SNN characteristics.\nSolid empirical results: The method achieves substantial reductions in GPU memory (to 76.44%) and energy consumption (to 53.03%) on ImageNet while maintaining or even slightly improving accuracy.\nGood ablation studies: The paper includes thorough ablations on pruning strategies, spatial-temporal ratios, and module placement."}, "weaknesses": {"value": "Limited novelty of SMixer architecture: The Spiking-token Mixer concept appears to be directly adapted from STMixer (Deng et al. 2024). The main architectural contribution seems incremental—demonstrating that this existing architecture can replace SSA in other frameworks. The novelty primarily lies in the pruning framework rather than the architecture itself.\n\nTraining vs. inference confusion: The DSTSP framework is designed to reduce training overhead on GPUs, but the motivation emphasizes event-driven deployment on neuromorphic chips. There's a disconnect:\nSNNs are typically trained on GPUs anyway (as authors acknowledge)\nThe pruning ratios used during training may not reflect deployment scenarios\nHow does training with high pruning ratios affect actual neuromorphic deployment efficiency?\n\nPerformance degradation concerns:\nOn some tasks, there are non-trivial accuracy drops (e.g., 1.3% on SpikformerV2-8-384 T=4, 2% on QKFormer variants)\nOn COCO object detection, the 1.5% mAP drop is dismissed as \"acceptable,\" but this is subjective\nThe paper doesn't discuss scenarios where DSTSP might fail or provide guidelines for when not to use it\n\nWriting and presentation:\n\nThe term \"Spiking-token Mixer\" vs \"STMixer\" vs \"SMixer\" is used inconsistently\nFigure 5 could be clearer—the temporal pruning illustration is hard to parse\nSome claims lack support: \"The intrinsically low firing rates of SNNs result in significant network redundancy\" (line 83)—citation needed"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pXBpLc3djk", "forum": "78glEsQB0v", "replyto": "78glEsQB0v", "signatures": ["ICLR.cc/2026/Conference/Submission4691/Reviewer_B5FE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4691/Reviewer_B5FE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883054990, "cdate": 1761883054990, "tmdate": 1762917513092, "mdate": 1762917513092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SMixer, an event-driven SNN architecture that replaces spiking self-attention with a Spiking-token Mixer to improve hardware feasibility while retaining competitive accuracy. Building on SMixer, the authors introduce Dynamic Spatial-Temporal Spiking Pruning (DSTSP)—a parameter-free framework that exploits sparsity in spike features by (i) pruning low-information tokens in space based on a Spike Intensity Value (SIV) metric and (ii) reducing uninformative time steps and spike counts in time, thereby cutting training overhead without materially degrading performance. Across ImageNet-1K, CIFAR-10/100, DVS datasets, time-series forecasting, and COCO detection, the approach demonstrates substantial reductions in memory/energy and higher throughput, with accuracy maintained close to baselines; in some ImageNet settings it even slightly improves top-1. The work argues for SNN designs that are simultaneously fully event-driven, low-overhead to train, and competitive in performance, positioning SMixer + DSTSP as a practical blueprint for neuromorphic deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem framing and design criteria. The paper precisely motivates why spiking self-attention is ill-suited to asynchronous chips and formalizes three desiderata—fully event-driven operation, low training overhead, and competitive performance—guiding the architectural choices. \n\n2. Hardware-friendly mixer in place of SSA. Replacing spike-matrix multiplications with a learnable mixer weight (WM) preserves the benefits of token mixing while avoiding timing-sensitive QK interactions, improving deployability on neuromorphic hardware. \n\n3. Parameter-free pruning mechanism. DSTSP is simple (addition/sorting), incurs negligible extra compute, and integrates into training without auxiliary modules or epochs; pruned features also skip back-prop, directly lowering cost. \n\n4. Principled redundancy metric (SIV) with empirical support. The SIV-based selection aligns with semantic saliency: high-SIV regions correspond to foregrounds and yield higher accuracy than random or high-SIV pruning baselines. \n\n5. Comprehensive experiments and tangible efficiency gains. On ImageNet, SMixer and SMixer + DSTSP achieve comparable accuracy to Spiking Transformers while reducing memory and energy and boosting throughput; similar trends hold on CIFAR/DVS, time-series, and COCO detection. \n\n6. Analytical backing. The paper provides operation and energy models showing why spatial-temporal pruning reduces cost relative to the original mixer; it also contrasts DSTSP with weight-pruning methods on cost/accuracy/sparsity."}, "weaknesses": {"value": "1.The robustness of the SIV (Social Influence Value) model under adversarial inputs or input perturbations has not been thoroughly investigated. This limitation raises concerns regarding the reliability of SIV in real-world applications, where data quality and integrity cannot always be guaranteed.\n2.While pruning effectively reduces model complexity and inference costs, it inherently leads to a reduction in model capacity. The extent to which this capacity loss can be mitigated by simply increasing the number of training epochs remains underexplored. Without a clear understanding of this trade-off, it is uncertain whether prolonged training truly restores the representational power of the pruned model or merely overfits to the remaining parameters.\n3.The paper does not clarify why DSTSP is intrinsically incompatible with the SSA architecture.\n4.DSTSP’s efficacy is only validated on small neuromorphic datasets; its scalability to large-scale, real-world benchmarks like HardVS remains untested."}, "questions": {"value": "1.Give more ablation on robustness of SIV.\n2.Does increasing the number of training epochs adequately compensate for the capacity loss induced by pruning?\n3.Please further explain why DSTSP is uniquely suited for the SMixer architecture rather than the SSA architecture.\n4.Provide experimental results on HarDVS dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DAKulYBNXp", "forum": "78glEsQB0v", "replyto": "78glEsQB0v", "signatures": ["ICLR.cc/2026/Conference/Submission4691/Reviewer_sSmP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4691/Reviewer_sSmP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4691/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883073132, "cdate": 1761883073132, "tmdate": 1762917512684, "mdate": 1762917512684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}