{"id": "nCJF1CDapQ", "number": 22926, "cdate": 1758337166555, "mdate": 1759896839822, "content": {"title": "TwinShield: Secure Foundation Model Execution by Unifying TEEs and Crypto-protected Accelerators", "abstract": "Recent advances in Transformer-based foundation models (FMs) have driven significant developments across diverse AI tasks, facilitating their deployment in security-sensitive domains. Despite their capabilities, FMs impose substantial inference costs, driving reliance on third-party cloud infrastructure equipped with high-performance computation resources. However, these cloud platforms cannot be fully trusted and remain vulnerable to data breaches, introducing dual confidentiality challenges: protecting user data from exposure and safeguarding models against unauthorized access. Mainstream protection mechanisms leverage trusted execution environments (TEEs), where confidentiality and integrity are enforced through hardware-based isolation, encryption, and integrity verification. But executing inference entirely within TEEs incurs a significant overhead, which is further exacerbated in large-scale FMs. Recent studies have proposed schemes that combine TEEs with untrusted accelerators (e.g., GPUs) to offload partial inference operations. However, prior offloading schemes cannot solve dual confidentiality challenges in FM inference, since operations such as ***Attention*** depend on dynamic operands that prevent secure precomputation and must remain within TEEs. Moreover, the communication overhead between TEEs and accelerators grows dramatically with model scale, constituting a new system design challenge for FMs.\nTo address these challenges, we propose ***Twinshield***, a framework that enables secure inference of Transformer-based FMs in heterogeneous TEE–accelerator systems with dual protection for both model and data. ***Twinshield*** improves efficiency through ***protocol-level*** outsourcing, which securely offloads the majority of operations to accelerators, and enhances throughput via a ***system-level*** design that overlaps TEE preparation, communication, and accelerator execution. Our evaluation on representative LLMs and VLMs shows that ***Twinshield*** offloads about 87% of computations to accelerators and achieves $3.3\\times$–$5.1\\times$ speedups over baselines. The code is publicly available at https://anonymous.4open.science/r/Twinshield/README.md.", "tldr": "", "keywords": ["trusted execution environments", "cloud computing", "model privacy", "data privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f397b4560107012ec1d39f29b3bb0adc97a111c6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "TwinShield is a secure foundation model inference framework that combines Trusted Execution Environments (TEEs) with untrusted accelerators to protect both user data and model parameters during cloud-based execution. It introduces two cryptographic outsourcing protocols, OutAttnMult for attention layers and OutLinearMult for linear layers, that securely offload heavy computations, and a pipelined scheme (OutPipe) that overlaps TEE processing and GPU computation to reduce communication overhead. Experiments on large language and vision-language models show 3.3×–5.1× speedups over TEE-only baselines while maintaining model accuracy and verifiable integrity. TwinShield demonstrates that secure and efficient foundation model inference is achievable without sacrificing confidentiality or performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an underexplored problem by simultaneously protecting sensitive user data and proprietary model parameters during cloud-based foundation model inference.\n\n- The authors evaluate TwinShield on a diverse set of large-scale models, including both language and vision-language models, and provide comprehensive analyses of computational cost and theoretical security."}, "weaknesses": {"value": "- The paper does not sufficiently explain why GPU-based TEE solutions (e.g., NVIDIA H100 Confidential Mode) fail to provide the same confidentiality and integrity guarantees as CPU-based TEEs, making the solution less attractive.\n- The related work discussion omits lightweight cryptographic or obfuscation-based alternatives, such as Secure Transformer Inference Protocol [1] and TransLinkGuard [2]. Although [1] focuses on multi-party computation without TEEs and [2] primarily protects model weights, both employ permutation-based obfuscation strategies that could achieve comparable confidentiality goals with less complexity. Including a direct comparison with these techniques would clarify the relative trade-offs between cryptographic cost, trust assumptions, and performance efficiency.\n\n[1] Yuan, Mu et al. “Secure Transformer Inference Protocol.” (2023).\n\n[2] Li, Qinfeng et al. “TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment.” (2024)."}, "questions": {"value": "- The performance evaluation in Figure 7 focuses on prefilling latency. How does TwinShield's performance, particularly the OutPipe pipelining, hold up during the autoregressive decoding phase?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBs2GZBiFr", "forum": "nCJF1CDapQ", "replyto": "nCJF1CDapQ", "signatures": ["ICLR.cc/2026/Conference/Submission22926/Reviewer_Ry8i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22926/Reviewer_Ry8i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977939773, "cdate": 1761977939773, "tmdate": 1762942441919, "mdate": 1762942441919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for secure inference of transformer-based foundation models by combining trusted CPU processors with untrusted accelerators like graphics processing units. The approach addresses two key challenges: enabling dual protection of both user data and model parameters during inference, and mitigating communication overhead between processors and accelerators at scale. The authors design three protocols: OutAttnMult for securely outsourcing attention computations, OutLinearMult for linear layer operations, and OutPipe for pipelining that overlaps processing preparation, communication, and acceleration to reduce idle time. The core cryptographic technique uses scale-then-permute masking with diagonal scaling matrices and permutations to obfuscate operands. The framework employs fixed-point quantization and uses Freivalds algorithm for integrity verification. Evaluation on multiple large language models and vision-language models demonstrates that the system offloads approximately 87 percent of computations to accelerators, achieving 3.3 to 5.1 fold speedups compared to processor-only baselines, with quantization causing only marginal perplexity increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses practical dual protection need: Unlike prior systems protecting only data or only models, this paper targets simultaneous confidentiality for both during cloud inference, a real deployment requirement for foundation model services\n- System-level optimization through pipelining: The proposed pipelined design overlaps processor preparation, data transfer, and acceleration computation, achieving measured 52.4 percent improvement by exploiting attention head independence. This practical contribution delivers measurable throughput gains.\n- Comprehensive evaluation on large models: Testing spans models up to 14 billion parameters including both language and vision-language architectures, with detailed ablation studies demonstrating individual component contributions\n- Implementation accessibility: The authors provide code built on established frameworks (ggml, llama.cpp), supporting reproducibility"}, "weaknesses": {"value": "- Recycled cryptographic techniques: The scale-then-permute strategy and diagonal matrix masking derive directly from prior systems designed for convolutional neural networks. Adaptation to attention represents engineering contribution rather than novel cryptographic innovation. The distinction between this paper and prior work is primarily architectural rather than methodological.\n- Missing critical security evaluation: The paper omits comparison with \"No Privacy Left Outside\" (S&P 2024), which demonstrates that processor-shielded partitioning approaches are vulnerable to model extraction attacks. This represents a fundamental threat to the security model. Related recent work such as HyperTheft shows ciphertext side channels can extract model weights from processor-shielded systems, directly contradicting claimed security guarantees.\n- Incomplete related work coverage: Missing substantial recent secure transformer inference work including cryptographic multiparty computation approaches (Iron, BOLT, Nimbus, STIP), homomorphic encryption methods (THE-X, CipherFormer), and hybrid processor-accelerator designs (THEF, SecureInfer). This gap undermines novelty claims."}, "questions": {"value": "- Security against recent attacks: How does this paper defend against model extraction demonstrated in \"No Privacy Left Outside\" (S&P 2024) and HyperTheft? These papers show processor-shielded partitioning is fundamentally vulnerable. Please provide: empirical evaluation against these attack methods, information leakage analysis from permutation patterns, and security guarantee comparison with prior systems subsequently broken.\n- Comprehensive baseline comparisons: Comparisons are limited to processor-only execution, Slalom, and ShadowNet. Please compare with recent transformer-specific secure inference systems: Nimbus, BOLT, STIP, and THEF. Provide performance breakdown showing whether advantages derive from cryptographic protocols, system optimization, or quantization. Compare security-efficiency tradeoffs against cryptographic multiparty computation and homomorphic encryption approaches.\n- Practical evaluation of: Detailed communication cost analysis (bytes transferred, protocol rounds, comparison against related work reporting these metrics), Freivalds verification overhead (actual repetition count in practice and resulting integrity guarantee)\n- What are specific technical advantages over THEF, PrivMLLM, and SecureInfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4aJ3l6GCOn", "forum": "nCJF1CDapQ", "replyto": "nCJF1CDapQ", "signatures": ["ICLR.cc/2026/Conference/Submission22926/Reviewer_9Fty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22926/Reviewer_9Fty"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051253340, "cdate": 1762051253340, "tmdate": 1762942441516, "mdate": 1762942441516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to use existing trusted execution environments for securely evaluating transformer models. The main novelty is to outsource some part of the computation to an untrusted GPU."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Some improvements for outsourcing some computation for TEEs. \n\nExperimental evaluation on small transformer models."}, "weaknesses": {"value": "I am not fully convinced that using the B factor is secure if it remains fixed throughout the outsourcing.\n\nThe novelty of the proposed approach also appears somewhat limited compared to existing work.\n\nFurthermore, I disagree with the paper’s argument that using GPU based TEEs—such as the NVIDIA H100 for secure GPU outsourcing—is problematic. In fact, this approach appears to be aligned with current industry trends and represents a practical direction for secure computation."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9YDXYsYAbe", "forum": "nCJF1CDapQ", "replyto": "nCJF1CDapQ", "signatures": ["ICLR.cc/2026/Conference/Submission22926/Reviewer_Vks3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22926/Reviewer_Vks3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128688776, "cdate": 1762128688776, "tmdate": 1762942441276, "mdate": 1762942441276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a scheme, named TwinShield, to securely offload attention computation in LLMs and VLMs from a trusted CPU TEE to an untrusted accelerator. Similar approaches have been proposed and studied before for MLPs/CNNs when only inputs (but not model weights) need to be protected. TwinShield uses the masked product approach from the previous work, but protects additional product computation needed for attention calculation through secret scaling and a secret column-wise permutation (OutAttnMult, OutLinearMult). For performance, the paper also proposes to pipeline the computation and communication between the CPU TEE and the accelerator (outPipe). The experimental results on an Intel SGX CPU TEE with an NVIDIA A40 GPU show 3-5x speedups compared to the SGX TEE-only case for LLMs (Llama3, Qwen3, Phi4) and VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper points out the limitations in previous secure computation outsourcing schemes for ML inference in the context of Transformers, and proposes a potential solution that can enable protecting both inputs and model weights. If the scheme is truly secure and efficient, it advances the state-of-the-art in secure inference outsourcing.\n\nThe proposed scheme is prototyped on a commercial TEE system, and the performance was evaluated for multiple LLMs and VLMs. The performance results look promising compared to the CPU TEE-only solution based on Intel SGX."}, "weaknesses": {"value": "The following summarizes the main questions that the paper needs to address to more fully justify the security and the effectiveness of the proposed scheme. \n\n1. Security\nWhile the proposed scheme may prevent the substitution attack discussed in the paper, the current security analysis is insufficient to truly prove the security. The paper needs a formal security proof.\n\nThe security analysis (in Section 4.1) simply states that Q+R_Q forms a one-time pad and is indistinguishable from A*R_Q. However, it is not clear if a simple scalar matrix (A) can truly hide R_Q and completely remove the correlation between Q+R_Q and A*R_Q. As an example, if an input (X) has mostly zero elements, Q+R_Q will be close to R_Q, and it appears that guessing A will be easier with Q+R_Q. The security guarantee should hold for all possible inputs and weight values and even when an adversary may know some of their values. \n\n2. Evaluation\nThe paper only shows the performance improvement over the baseline using Intel SGX, which had limited secure memory capacity. However, practical deployments of CPU TEEs are mostly based on VM-based TEEs such as Intel TDX and AMD SEV, which support much larger protected memory capacity. The performance overhead of running large ML models should be much lower for the modern CPU TEE designs. To better justify the effectiveness of the proposed scheme, the paper needs to show its benefit for VM-based TEEs. \n\nAlso, the reported latency of tens of seconds still seem quite slow. To more fully evaluate the overhead and practicality, the paper should provide performance comparisons with the GPU TEE and the baseline w/o TEE. It is also unclear how the overhead of offline computations is considered in the evaluation. Finally, it will be helpful to study both prefill and decode latencies (the current evaluation appears to only study prefill latency).\n\n3. Motivation\nThe main technical arguments that the paper makes to motivate outsourcing partial inference operations to untrusted accelerators are not well supported.  \n\nThe paper refers to a previous study on the Intel SGX, which had a limited secure (enclave) memory capacity, to point out running the ML model entirely in a CPU TEE is too expensive. However, more recent VM-based CPU TEEs such as Intel TDX and AMD SEV support much larger protected memory capacity and should have much lower overhead. In that sense, to be more compelling, the paper needs to show the performance benefits compared to these VM-based CPU TEEs.\n\nThe paper states that recent accelerator TEEs (such as NVIDIA GPU TEE) provides weaker guarantees than CPU-\nbased TEEs to motivate the use of untrusted accelerators. While the problem of how/if we can securely offload attention computation to untrusted accelerators is an interesting question by itself, the claim that accelerator TEEs are less secure than a CPU TEE is not justified. In fact, CPUs often rely on more complex optimizations such as speculative execution and are often shared with more untrusted software. Recent attacks on CPU TEEs such as TEE.fail show that CPU TEEs can be more easily attacked through side-channels or DRAM interposers - attacks that  do not apply to accelerators with HBMs. If one cannot trust the security of an accelerator TEE, it is not clear why it is reasonable to assume that a CPU TEE (which is more complex and exposed to more types of software and physical attacks) can be trusted."}, "questions": {"value": "1) Can you provide a formal proof of security? It is not clear how a simple scalar matrix (A) can completely remove the correlation between Q+R_Q and A*R_Q.\n\n2) How will the performance benefit of TwinShield change for VM-based CPU TEEs (Intel TDX, AMD SEV)? How does the TwinShield performance compare to the baseline w/ no TEE and the GPU TEE?\n\n3) How does the paper's performance evaluation include/consider the offline computation? Recent studies (such as https://arxiv.org/pdf/2207.07177) suggest that offline overhead is important to consider for throughput."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ORR2srHGyh", "forum": "nCJF1CDapQ", "replyto": "nCJF1CDapQ", "signatures": ["ICLR.cc/2026/Conference/Submission22926/Reviewer_ckdE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22926/Reviewer_ckdE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153082652, "cdate": 1762153082652, "tmdate": 1762942441034, "mdate": 1762942441034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}