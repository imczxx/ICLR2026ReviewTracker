{"id": "3knS4J9isg", "number": 8939, "cdate": 1758103212170, "mdate": 1759897752734, "content": {"title": "PLOT: Pseudo-Labeling via Object Tracking for Monocular 3D Object Detection", "abstract": "Monocular 3D object detection (M3OD) is crucial for scalable perception across fields like autonomous driving, robotics, and surveillance. However, progress is hindered by limited 3D annotations and the inherent ambiguity of single-image geometry. Current methods often rely on strong geometric assumptions or carefully curated datasets, which limit their applicability to real-world scenarios. In this paper, we present $\\textbf{PLOT}$ ($\\textbf{P}$seudo-$\\textbf{L}$abeling via $\\textbf{O}$bject $\\textbf{T}$racking), a training-free framework that generates 3D annotations from monocular videos without auxiliary sensors or model retraining. PLOT tracks object and background trajectories to estimate camera motion and perform object association in pose-unknown settings. These trajectories are integrated through the shape fusion of frame-wise pseudo-LiDARs, yielding reliable annotations under occlusion and viewpoint shifts. Recognizing temporal coherence as a fundamental requirement for reliable shape fusion and video perception, we design a global object memory that preserves consistent object identities across frames. PLOT achieves robust annotation quality and strong generalization on both M3OD video benchmarks and in-the-wild videos, proving its effectiveness across diverse and unconstrained domains. The code and weights will be publicly released upon acceptance.", "tldr": "PLOT produces accurate 3D labels directly from monocular videos without auxiliary sensors or retraining.", "keywords": ["Monocular 3D Detection; Open-vocabulary 3D Labeling; Pseudo-labeling;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c5a11ed88abc64bec0b648e687e1f4bb40679ca.pdf", "supplementary_material": "/attachment/4e2149b5991027fa47578992c5261ad1ab19a5a1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel annotation-driven strategy to enhance 3D object detection, introducing a semi-automatic labeling technique that aims to reduce manual annotation costs while maintaining detection accuracy. The method combines geometric priors with weakly supervised signals to generate pseudo labels, which are then applied to 3D detection tasks on the KITTI dataset. Experimental results show measurable improvements compared to several baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed labeling strategy provides a creative solution to the high-cost bottleneck of manual 3D annotations, which is an important and underexplored aspect of 3D detection research.\n2. The paper is well-motivated and clearly connects the challenges in 3D annotation with detection performance, demonstrating technical feasibility through the KITTI experiments."}, "weaknesses": {"value": "1. The method is only validated on KITTI, a relatively small and well-studied dataset.To establish broader applicability, evaluation on additional datasets such as nuScenes, Waymo.\n2. It is not explicitly clarified whether the same 3D detection or configuration is used across the proposed and compared methods. Since the labeling technique is applied to detection task, any difference in detection could confound the comparison results. A clear statement on this consistency is necessary to validate fairness."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kpkSVMF8at", "forum": "3knS4J9isg", "replyto": "3knS4J9isg", "signatures": ["ICLR.cc/2026/Conference/Submission8939/Reviewer_pYBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8939/Reviewer_pYBp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790179475, "cdate": 1761790179475, "tmdate": 1762920682457, "mdate": 1762920682457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PLOT (Pseudo-Labeling via Object Tracking), a novel framework for generating 3D object detection annotations from monocular videos. The core contribution is a training-free pipeline that does not require auxiliary sensors (like LiDAR) or model retraining. This addresses the critical challenges of 3D annotation scarcity and the poor generalization of existing models to \"in-the-wild\" scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength is the training-free nature of the pseudo-labeling framework. This frees the method from reliance on specific datasets or sensors (e.g., LiDAR or known camera poses), demonstrating strong generalization to \"in-the-wild\" videos, which is a major limitation of current M3OD methods.\n- The method's \"trajectory-guided shape fusion\" aggregates information from multiple frames to build a \"completed pseudo-LiDAR\". This effectively addresses occlusion, truncation, and sparse point cloud issues that plague single-frame methods, leading to more accurate attribute estimation. The ablation in Table 4 confirms that using more frames (e.g., 20) for fusion significantly improves AP.\n- The pseudo-labels generated by PLOT lead to excellent performance when used for training."}, "weaknesses": {"value": "- The method estimates orientation for dynamic objects based on their motion. However, for static objects, it falls back to using PCA on the fused pseudo-LiDAR. While the authors claim the denser fused cloud makes PCA more robust, this remains a strong heuristic. This approach may fail for objects with near-symmetrical shapes (like vans) or where the principal axis of variance does not align with the object's heading.\n- While the framework is presented as \"training-free,\" its performance is fundamentally dependent on several complex, pre-trained foundation models, including a 2D detector (GSAM), a dense point tracker (AllTracker), and a monocular depth estimator (UniDepth/MoGe2). A critical issue is the lack of discussion on the temporal consistency of these depth estimates from a monocular estimator. How does PLOT mitigate potential temporal flickering or scaling inconsistencies in the depth predictions from frame to frame?"}, "questions": {"value": "- Table 4 analyzes the computation time for shape fusion. However, to assess practical utility, what is the end-to-end latency of the entire PLOT pipeline (from video input to pseudo-label output)? This should include the full overhead of running GSAM, AllTracker, and UniDepth.\n- The paper uses an advanced dense point tracker (AllTracker), yet also introduces GOM to resolve tracking failures like ID switches. To what extent is GOM correcting for AllTracker's own tracking errors versus primarily handling detection dropouts from the 2D detector (GSAM)? The line between these two error sources seems blurry.\n- Camera ego-motion is estimated via Procrustes alignment (Eq. 4) on background points lifted to 3D by the depth estimator. How robust is this alignment to noise from the monocular depth predictions, especially non-uniform noise? How does depth error propagate to the camera motion estimate and, subsequently, to the quality of the final shape fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2eVgdVDipl", "forum": "3knS4J9isg", "replyto": "3knS4J9isg", "signatures": ["ICLR.cc/2026/Conference/Submission8939/Reviewer_9QpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8939/Reviewer_9QpT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882262811, "cdate": 1761882262811, "tmdate": 1762920682066, "mdate": 1762920682066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for pseudo-labeling 3D bounding boxes around objects of interest from in-the-wild videos. They propose a pipeline where they track points on objects and on the background to derive both camera motion and object motion. Then, they use an off-the-shelf metric depth estimator to lift these points into 3D, and aggregate points of objects of interest to derive a 3D bounding box around them, with the heading angle derived from the object motion in world frame. Using their pseudo-labels, they demonstrate strong performance on KITTI and Waymo datasets, and also present an interesting demo of pseudo-labels on in-the-wild video."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed pipeline is intuitive and reasonable, drawing from recent improvements in 2D tracking and effectively using them to derive 3D motion and labels.\n- While a straightforward use of PA, the work derives reasonable estimates for camera and object motion.\n- The improvement from training on the pseudo-labels is good to see in Figure 13.\n- The proposed method demonstrates strong results compared to prior work in KITTI and Waymo.\n- The ablation study in Tables 4 and 5 are reasonable, showing improvement with more frames."}, "weaknesses": {"value": "- While the pipeline is reasonable, this reviewer wonders if it's necessary to derive camera motion in such a manner. For instance, if we put the entire sequence into COLMAP, or perhaps some SLAM method, could it recover relative camera poses similarly reasonably? Such poses can directly be lifted to metric scale using the same metric depth estimator. Given advancements in the field, MonSt3R-based SLAM and VGGT must also be considered as possible system-level competitors as well, as they directly output 3D points, which one can derive 3D boxes from.\n- This paper seems to derive tracklets by propagating tracked masks from one frame to the next using an off-the-shelf point tracker. New masks are matched via hungarian matching and the authors also present GOM, which appears to be a tracklet rebirth mechanism. This is a reasonable, performant pipeline, but it's difficult to say it is this paper's contribution as I believe these are common practices in object tracking frameworks.\n- Building on the previous point, such pipelines often prove to be brittle to unexpected motion and perhaps have difficulty with long-term consistency. I would like to ask if an off-the-shelf model like SAM2, which directly outputs mask tracklets could be a one-touch alternative that performs well. To the best of my understanding, the input & output is the same: input RGB video, output tracked masks.\n- While it is good to see visualizations of boxes on in-the-wild videos, it is difficult to truly assess the quality of 3D from the 2D projection (since the 3D was derived from that 2D frame, it's easier for it to look aligned). Some visualizations of point clouds or boxes in another view could prove important: for instance, visualizing the crossroad image at the top of Figure 17 from a road-level view to assess if all objects are on a consistent plane.\n- Is the performance improvement in Table 4 with more frames purely due to a depth of a single frame being unable to give the full _extent_ of an object? If so, would reasonable priors on the size of a car in general serve to improve performance of a single-frame baseline?"}, "questions": {"value": "I would appreciate it if the authors address my concerns regarding simpler alternatives, as outlined in the first few points in the Weaknesses section. The authors provide a reasonable and performant pipeline, but it's not yet clear to this reviewer that all such components are necessary, especially in light of advancements in video models. As such, at this stage, I recommend a 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lSYxQmnrbt", "forum": "3knS4J9isg", "replyto": "3knS4J9isg", "signatures": ["ICLR.cc/2026/Conference/Submission8939/Reviewer_7swF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8939/Reviewer_7swF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962711076, "cdate": 1761962711076, "tmdate": 1762920681717, "mdate": 1762920681717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SafeScale, a novel and scalable framework for generating synthetic driving data to improve the safety and robustness of autonomous driving planners. The core problem addressed is the difficulty of collecting real-world data for rare and dangerous \"corner cases\" that often cause modern data-driven planners to fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n1.\tThe paper tackles a critical and high-impact problem in autonomous driving: improving safety by addressing corner cases. The primary contribution—a clear, empirical demonstration that scaling targeted synthetic data can directly and significantly improve a planner's performance in the real world.\n2.\tThe SafeScale framework is well-designed and technically sound. The idea of decomposing real scenes into modular asset libraries (backgrounds, appearances, behaviors) is a powerful concept that enables both scalability and fine-grained control.\n3.\tThe experimental evaluation is exemplary. The main result, presented in Figure 4, provides convincing evidence of a \"synthetic data scaling law,\" which is the paper's central claim. The ablation studies are insightful and strongly support"}, "weaknesses": {"value": "Weaknesses:\n\n1.\tThe contribution can be characterized as a sophisticated and highly successful data engineering framework. Its primary novelty lies in the clever integration of existing components and the powerful empirical demonstration of the scaling law, rather than in a fundamental algorithmic advance. The method for generating corner cases is 'reactive'—it relies on first analyzing the specific failure modes of a baseline planner (DiffusionDrive) on a specific benchmark (NAVSIM). A more scientifically profound direction, which the paper does not explore, is how to proactively and universally model and generate corner cases. For instance, could one develop a general model that learns the underlying distribution of safe driving data and then synthesizes challenging out-of-distribution scenarios in a principled way, without being tied to the failures of one specific planner? This would elevate the contribution from a highly effective, bespoke data augmentation solution to a more fundamental model of driving risk.\n2.\tThe traffic participant behavior library is built from trajectories observed in the NAVSIM dataset. While the framework can create novel scenarios by placing these behaviors in new contexts, it is fundamentally limited by the vocabulary of behaviors present in the source data. The paper would be strengthened by a discussion of this limitation. Can this method generate truly novel, out-of-distribution behaviors, or is it primarily a powerful recombination engine?\n3.\tThe paper states that the asset extraction pipeline is highly scalable as it relies on sensor data and 3D annotations. However, the acquisition of clean, large-scale 3D annotations is a known bottleneck and a significant cost factor for the entire industry. A brief discussion on the sensitivity of the SafeScale pipeline to the quality and scale of these initial annotations would be welcome. For instance, how do sparse or noisy annotations affect the quality of the generated assets and the final planner performance?\n4.\tThe use of a generative model for view synthesis is a key strength, but these models are not perfect. They can introduce subtle artifacts, temporal inconsistencies, or a lack of physical realism (e.g., incorrect shadows, reflections). The paper does not discuss the potential sim-to-real gap of this rendering stage. While the end-to-end results prove the data is highly effective, a qualitative analysis or discussion of the generative renderer's failure modes would add nuance and provide a more complete picture"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y681cKOHMZ", "forum": "3knS4J9isg", "replyto": "3knS4J9isg", "signatures": ["ICLR.cc/2026/Conference/Submission8939/Reviewer_y8oj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8939/Reviewer_y8oj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8939/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003851074, "cdate": 1762003851074, "tmdate": 1762920681303, "mdate": 1762920681303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}