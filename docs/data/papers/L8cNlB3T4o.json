{"id": "L8cNlB3T4o", "number": 11857, "cdate": 1758204281714, "mdate": 1763575973381, "content": {"title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models", "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset, for instance, may not reflect weak reasoning at all, but instead a formatting slip, a calculation error, or dataset noise. Without disentangling such causes, benchmarks give an incomplete picture and cannot reliably guide model improvement. We introduce ErrorMAp, the first method to systematically chart the sources of LLM failure. ErrorMAp provides tools to extract a model's unique ``failure signature'', uncover what benchmarks actually measure in practice, and broaden the scope of identified model errors to reduce blind spots. This enables developers to debug models more effectively and helps benchmark creators align dataset goals with actual outcomes. Additionally, it supports benchmark consumers in identifying which models best suit their specific needs. ErrorMAp is designed to work flexibly with any model and dataset, making it adaptable to evolving architectures and emerging data sources without requiring changes to its logic. We apply our method across 21 datasets and 73 models to automatically generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns in current language models. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMAp and ErrorAtlas lay the foundation for next-generation evaluation --- one that exposes hidden weaknesses and directs meaningful progress. Unlike success, which is typically measured using task- or dataset-level metrics, our approach introduces a deeper layer of evaluation that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and method code publicly available, with plans to update ErrorAtlas as new benchmarks emerge.", "tldr": "", "keywords": ["LLMs", "Error Analysis", "Taxonomy"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88ccb75f9324a2cbe49ba48cb34777547085c5f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The article has adjusted the default Latex template, significantly reducing the margins. This allows for more content to be written within the required number of pages. I believe this is unfair to other paper authors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "N/A"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jH9TWCijKw", "forum": "L8cNlB3T4o", "replyto": "L8cNlB3T4o", "signatures": ["ICLR.cc/2026/Conference/Submission11857/Reviewer_7RK4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11857/Reviewer_7RK4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383642884, "cdate": 1761383642884, "tmdate": 1762922872143, "mdate": 1762922872143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces error map, which is a method to systematically analyze why LLMs fail rather than just where they fail. And for that, they use LLM-based analysis in three stages to create error taxonomies. \n\nThe pipeline is three-stage and it turns model mistakes on benchmark into layered error taxonomy. And the error atlas is a static taxonomy distilled by applying error map across many models and datasets.\n\nThis atlas is built by sampling failures from 21 datasets in many models to produce high-level categories like factual error, misinterpretation, incomplete content, etc."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a clear general pipeline for systematic error analysis.\n\n2. The error atlas spans about 21 datasets and many models which enables cross-model and cross-benchmark comparisons.\n\n3. There is a concrete taxonomy with readable category names and definitions which is useful."}, "weaknesses": {"value": "The usage of LLMs to judge LLMs failure modes is fundamentally problematic. And while the authors acknowledge this, it does introduce systematic blind spots with the judge model shares failure modes with evaluated models.\n\nThe validation is not extremely strong. The 92% taxonomy accuracy comes from the same LLM judge and only 53% similarity occurs across prompt variations which suggest some instability. If the authors had done some Newman evaluation and had published agreement scores between the two evaluations, that would have increased the trust.\n\nOnly a 10% sampling rate seems somewhat arbitrary without power analysis and the binomial test could be made more robust as it is currently quite simplistic.\n\nThe single first-order attribution oversimplifies the cascading failures and the small error threshold is quite data-set-agnostic and unjustified. Also, the use of a single-judge model, gpt-oss, introduces specific biases.\n\nForcing errors into single categories also loses nuance. Multi-level classification would probably be more appropriate.\n\nThere is a minor inconsistency in 73 models being said in the abstract, whereas later text saying 75 models. \n\nThe authors pruned categories that appear in fewer than 20% of the datasets which could draw up informative but niche failure modes."}, "questions": {"value": "Can you please clarify whether the total number of models is 73 or 75, And ensure consistency across sections and figures.\n\nYou sample about 10% of failures per model dataset pair. How stable are the resulting categories and proportions if the sampling rate is varied or if you stratify by error type rarity or task family?\n\nWhen pruning categories that appear in less than 20% of the datasets, you may delete real safety critical or domain-specific failure modes. Can you provide a solution to that?\n\nThe robustness is tested via prompt variations and shows a decent amount of moderate similarity. Can you please do cross-judge robustness, for instance, comparing different LLMs in stage 1 and stage 3?\n\nWhat happens at category boundaries? Several categories may overlap in practice. For instance, incomplete content vs formatting error or reasoning error vs incorrect method. Can you quantify these intercategory confusions? What are the priority rules when multiple categories fit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N61QIoaWLp", "forum": "L8cNlB3T4o", "replyto": "L8cNlB3T4o", "signatures": ["ICLR.cc/2026/Conference/Submission11857/Reviewer_h4hL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11857/Reviewer_h4hL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935456755, "cdate": 1761935456755, "tmdate": 1762922871733, "mdate": 1762922871733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional benchmark evaluations can indicate when a model fails but offer little insight into why it fails.To address this limitation, the authors propose a new diagnostic framework called ErrorMap, and, based on it, construct a static taxonomy of model errors named ErrorAtlas.They further analyze the error distributions across different models and demonstrate how this approach can be applied in multiple scenarios, including model diagnosis, benchmark design, and domain-specific evaluation. Additionally, the paper presents experimental validation of ErrorMap’s reliability, coverage, accuracy, and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work approaches LLM evaluation from a new perspective—explaining why models fail rather than merely identifying when they fail. By constructing a systematic error analysis framework, the authors contribute a method that is both theoretically meaningful and practically valuable. It helps model developers identify weaknesses, reveal capability gaps, and provides a scientific foundation for model improvement and evaluation.\n- The core framework, ErrorMap, is conceptually clear and logically structured. Its derivative system, ErrorAtlas, provides a concrete and reusable static taxonomy for analyzing model failures. The paper also includes numerous case studies and quantitative results, enabling readers to clearly understand the method’s effectiveness in practice."}, "weaknesses": {"value": "- The paper’s main contribution is a tool-based framework for analyzing LLM errors, whose greatest value lies in broad adoption. However, the authors have not yet released the code , making it impossible to evaluate the system’s real-world performance, stability, and efficiency.In my opinion,this limitation significantly weakens the practical impact and overall value of the work.\n- The per-instance diagnostic stage of ErrorMap relies heavily on an LLM-as-judge mechanism. Experimental results show that label consistency across prompt variations is only 53% (measured by Sentence-BERT similarity), indicating that the process is quite sensitive to prompt design.For a tool intended for practical use, stability is crucial.If simply modifying a prompt or substituting a different judge model could alter the core conclusions of the ErrorMap framework, this would undoubtedly weaken its reliability and reference value.And although the paper briefly discusses this issue, it lacks a systematic analysis of its potential impact."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z594UjfyAw", "forum": "L8cNlB3T4o", "replyto": "L8cNlB3T4o", "signatures": ["ICLR.cc/2026/Conference/Submission11857/Reviewer_ECKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11857/Reviewer_ECKn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072444634, "cdate": 1762072444634, "tmdate": 1762922870186, "mdate": 1762922870186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}