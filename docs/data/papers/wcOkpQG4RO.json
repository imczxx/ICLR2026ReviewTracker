{"id": "wcOkpQG4RO", "number": 14568, "cdate": 1758239153384, "mdate": 1759897361851, "content": {"title": "On the Effect of Sampling Diversity in Scaling LLM Inference", "abstract": "Large language model (LLM) scaling inference is key to unlocking greater performance, and leveraging diversity has proven an effective way to enhance it. Motivated by the observed relationship between solution accuracy and meaningful response diversity, we systematically study the effect of prompt diversity in scaling inference. We theoretically explain why diversified sampling improves Best-of-$N$ scaling, showing that responses generated from meaningful diverse prompts after Best-of-$N$ selection exhibit significantly lower error rates than those produced from stationary prompts. To promote solution diversity, we analyze perturbation fidelity and show that moderately relevant perturbations improve performance, providing guidance for effective perturbation design. Further, we present a set of effective perturbations, including task-level and query-level ones, and analyze the conditions under which they succeed. We systematically evaluate diversified sampling across tasks, finding relative gains of 10.8\\% in EM@100 for reasoning, 9.6\\% for mathematics, and 9.5\\% in Pass@100 for code generation.", "tldr": "", "keywords": ["test-time computing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dc9579b05109c6af1d98dcd12cc18d7770938d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work studies the effect of prompt diversity in scaling inference. The authors also explain how diversified sampling helps lower Best-of-N error rates. They conduct a systematic study of different prompt perturbation approaches and notice that the proportion of k correct solutions increases when the produced candidates are more dissimilar. The paper  (1) theoretically demonstrates that prompt perturbations improve best-of-N performance by analyzing “blind spots” and “rates of convergence”, (2) analyzes the range of perturbation that yields positive gains, (3) analyzes conditions under which the gains of diversified sampling are not outdone by other operations and (4) empirical evaluations on reasoning, math and code generation to see which task-level and query-level perturbations have more consistent gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper conducts a timely analysis and contains useful empirical work.\n\n- The taxnonomy of perturbations is novel. The formalisms are very clear \n\n- Theorem 3.5 is elegant in deadling with two main desiderata of inference-time scaling\n\n- A good set of similarity metrics is used to properly quantify diversity or lack thereof"}, "weaknesses": {"value": "### Major\n1. The paper reads like a collection of observations without a clear conclusion or even actionable items or tools for the community to benefit from. I am open to being concinved otherwise, but I cannot recommend acceptance given the way the paper reads now.\n\n2. My main concern is novelty, as the conclusions seem self-evident. Findings like a non-linear 'sweet spot' for perturbation are trivial, while noting that diversity-oriented methods fail under majority-voting reward settings is tautologous.           \n\n### Minor \n3. The meaning of \"policy solver mode diversity\" is unclear. Please write it more clearly. \n\n4. I only understood what Jabberwocky was after reading the Appendix. Please make it clearer in the main body. See Q1.\n\n5. While the paper is generally on the easier side of parsing, there are very dense paragraphs that use specific **ad-hoc**(paper-specific) concepts that are not common. For example, line **246 to 254** is difficult to parse; I suggest using colors or stylized text. Figure 3 and 4 are also very unclear. The colors are difficult to separate and the line styles hinder the delivery of the empirical insights.\n\n6. Relevant citations regarding empirical analysis of temperature scheduling effects or ways of increasing diversity are missing [1] [2]\n\n[1] Zhu, Alan, et al. \"BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation.\" arXiv preprint arXiv:2502.01697 (2025).\n\n[2] Ahmed, Eltayeb, et al. \"Intent Factored Generation: Unleashing the Diversity in Your Language Model.\" arXiv preprint arXiv:2506.09659 (2025)."}, "questions": {"value": "### Major \nQ1. Why did you choose the Jabberwocky perturbation? What is special about it? If nothing then what makes it so representative of that family of perturbation.\n\nQ2. The empirical evaluation is not above 1.2 temp? Why this arbitrary limitation? \n\nQ3. What is the takeaway from Table 2? \n\nQ4. Have you considered using Semantic Entropy as an alternative metric for diversity? [1]\n\n### Minor\nQ4. Is the illustration asset used in the textboxes copyright-free? Please clarify the licenses if you have not created the image yourself. This does not bear any weight on the score. I just want to make sure proper attribution is always given.  \n\n[1] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" arXiv preprint arXiv:2302.09664 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KbKfabcNxn", "forum": "wcOkpQG4RO", "replyto": "wcOkpQG4RO", "signatures": ["ICLR.cc/2026/Conference/Submission14568/Reviewer_XCHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14568/Reviewer_XCHZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775249438, "cdate": 1761775249438, "tmdate": 1762924957872, "mdate": 1762924957872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate the impact of prompt diversity on scaling inference in LLMs. They provide a theoretical explanation for why diversified sampling improves Best-of-N metric performance. The paper also analyzes the perturbation fidelity and demonstrates that moderately relevant perturbations yield the best results, offering practical guidance for designing effective perturbations. Building on this analysis, the authors propose a set of validated perturbation strategies at both the task level and query level, and discuss the conditions under which these approaches succeed. The conclusion is validated through experiments conducted across diverse tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Theoretical explanation.** The paper presents a formal theoretical result showing that diversified sampling reduces the failure probability in Best-of-N inference. This offers a conceptual foundation rather than only relying on empirical results.\n\n**Experimental results.** The authors conduct extensive experiments across diverse tasks and settings, providing strong empirical evidence to support their ideas.\n\n**Practical implications.** The work offers practical insights into designing effective perturbations that enhance LLM inference performance, providing insights and directions for future work and practical applications."}, "weaknesses": {"value": "**1. Missing cost consideration in experiments.**\nWhile the experiments demonstrate that diversified sampling improves LLM performance, the paper does not discuss the associated computational or time costs. An analysis of the trade-off between performance gains and inference cost would strengthen the empirical evaluation and clarify the practical value of the approach.\n\n**2. Connections and readability.**\nThe paper lacks clear connections between sections—specifically between notation, theoretical analysis, and experimental validation. Providing smoother transitions and contextual links would greatly improve readability and help readers understand how each component correspons to other parts.\n\n**3.Strong assumptions.**\nHypothesis 3.1 (Variation under auxiliary diversity) assumes that, for any fixed input $\\bf{r}$, the variation induced by auxiliary diversity is uniformly bounded below by a constant $\\widehat{\\mu}_1$. This uniform bound may be overly strong and requires further justification in realistic settings. How about the inputs that are already easy or perturbations have little effect?\n\nSee also in questions below."}, "questions": {"value": "1. Regarding the hybrid diversity term $\\xi \\sim \\Pi$, what exactly are $\\xi$ and $\\Pi$ in the proposed strategies and in the experimental setups? \n   \n2. The experimental results mainly focus on performance across different tasks. How does the proposed diversified sampling perform across different LLMs, for instance, models of varying sizes?\n\n3. Could the authors provide a more detailed explanation of Hypothesis 3.1 and its practical meaning? (See also Weakness 3)\n\n4. Some minor questions: \n\n     (1) What does ``RNG'' refer to in Remark 3.2 and Hypothesis 3.3?\n\n     (2) How are $N^{K}$ and $N^{\\text{inf}}$ defined in Theorem B.5 in Appendix B.4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ezKe40HgXB", "forum": "wcOkpQG4RO", "replyto": "wcOkpQG4RO", "signatures": ["ICLR.cc/2026/Conference/Submission14568/Reviewer_BKDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14568/Reviewer_BKDX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851875146, "cdate": 1761851875146, "tmdate": 1762924957523, "mdate": 1762924957523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that sampling diversity, varying prompts instead of reusing the same one, improves Best-of-N performance in LLM inference. The authors theoretically prove that diverse sampling reduces errors and speeds convergence, and empirically find that moderately relevant perturbations (neither too similar nor too random) yield the best results. Through task-level and query-level perturbations, they report consistent gains across reasoning, math, and coding tasks, up to 10.8% in EM@100, demonstrating exploration diversity as an effective, test-time strategy for enhancing LLM performance without model changes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a novel and systematic study of sampling diversity in LLM inference, combining theoretical proof, perturbation design, and empirical validation.\n\n2. Demonstrates that exploration diversity can significantly enhance LLM performance without retraining, offering a practical, general-purpose strategy for improving test-time scaling across domains."}, "weaknesses": {"value": "1. The writing and presentation are weak and difficult to follow. For example, in Section 3 (line 144), the term RNG appears without any prior introduction or explanation. Additionally, the two hypotheses are presented without sufficient depth or illustrative support; although Remark 3.2 attempts clarification, it remains unclear. Including concrete examples or clearer illustrations would greatly improve readability and understanding.\n\n2. The central insight of this paper is that incorporating prompt and sampling diversity at inference time can significantly enhance the reasoning performance of large language models. Nonetheless, this direction is not entirely new: prior studies, such as DIPPER [1], have also highlighted the importance of fidelity and diversity, while [2] demonstrated that modifying prompts to induce greater diversity can further improve ensemble performance.\n\n[1] Lau, Gregory Kang Ruey, et al. \"Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks.\" arXiv preprint arXiv:2412.15238 (2024).\n\n[2] Naik, Ranjita, et al. \"Diversity of thought improves reasoning abilities of large language models.\" (2023)."}, "questions": {"value": "It appears that the task-level perturbations employ a thinker model to generate multiple solutions through sophisticated prompts. I wonder whether incorporating these generated solutions into the raw CoT prompt would result in longer or more elaborate reasoning chains compared to the original CoT reasoning. Could the authors provide corresponding results or statistical analyses on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tFDNuZh5M4", "forum": "wcOkpQG4RO", "replyto": "wcOkpQG4RO", "signatures": ["ICLR.cc/2026/Conference/Submission14568/Reviewer_kUBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14568/Reviewer_kUBb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969886233, "cdate": 1761969886233, "tmdate": 1762924956980, "mdate": 1762924956980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a sampling strategy that injects an auxiliary diversity source and aims to improve performance, arguing that diversified prompt perturbations enhance Best-of-N inference scaling across reasoning, math, and code tasks. It begins by theoretically justifying its claim, and then empirically demonstrates that the diversified perturbations are effective in different tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- It gives a theoretically sound justification of why exploration diversity improves best-of-N.\n- The evaluation is comprehensive, which covers the task-level perturbations and query-level perturbations."}, "weaknesses": {"value": "- The presentation could be improved if the notation or symbols could be used more concisely or minimally. And “RNG” is never defined.\n- The line 144 is misleading. Unless the success probability of LLM solving one particular problem is defined precisely to be conditioned on the diversity source, it (i.e., the success probability) should be static. I can understand the author’s intention. Maybe the author can define it as a sampling-based success probability where different sampling diversities would have different expectations. \n- The hypothesis 3.3 could be unreasonably strong, as it assumes that any auxiliary diversity resources would not make the final success probability sufficiently worse. This is generally not true in practice. A more reasonable assumption could be placed within a finite set of diversity choices, which bounds the worst case.\n- Intuitively, the first absolute central moment defined in eq. 2 should be larger (or the constant \\hat{\\mu}_1 should be larger) if the auxiliary diversity is larger. Could the author show or explain this is always (approximately) true?\n- I am aware that there are some relevant works on diversified sampling that are not discussed. Specifically, [1] aims to inject auxiliary diversity into the inference stage to boost performance, where the diversity of the selected instructions is maximized using logdet. [2] balances diversity and risk in LLM sampling based on a prefix tree. These works should at least be discussed or even compared.\n- Only cosine similarity metric is adopted as a measure of diversity. I suggest the author could compare with the approach from [1] which use a greedy approach to maximize the diversity.\n\n[1] Hu, W., Lau, G.K., Liu, D., Chen, J., Ng, S., & Low, K.H. (2024). Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning tasks. ArXiv, abs/2412.15238.\n\n[2] Zhou, Y., Keuper, M., & Fritz, M. (2025). Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation. In Proceedings of ACL 2025."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jfMrx6vroQ", "forum": "wcOkpQG4RO", "replyto": "wcOkpQG4RO", "signatures": ["ICLR.cc/2026/Conference/Submission14568/Reviewer_1sDt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14568/Reviewer_1sDt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982825563, "cdate": 1761982825563, "tmdate": 1762924956597, "mdate": 1762924956597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}