{"id": "gxqlFvPgpr", "number": 24199, "cdate": 1758353986798, "mdate": 1762944767267, "content": {"title": "PRISM-EE: A Peer-Federated Framework for Cost-Aware Large Language Model Evaluation", "abstract": "Large Language Model evaluation faces three critical problems: static benchmarks suffer from data contamination, human-judged systems have systematic biases, and most importantly, both ignore cost, The key factor determining real-world deployment decisions. We introduce PRISM-EE (Peer-Reviewed Intelligence Scoring Methodology with Economic Evaluation), a peer-federated framework where AI models evaluate each other through specialized roles: competitors solve problems, content creators design challenges, and judges evaluate solutions. This approach generates fresh content dynamically while reducing human bias. PRISM-EE evaluates models on dual tracks: raw performance and cost efficiency. Using Swiss-style pairing, we achieve stable ratings in 25-30 matches with ±18 Elo precision, compared to 100+ matches required by existing systems. We tested 48 models across clinical reasoning, mathematics, and programming domains. Results reveal dramatic cost variations invisible to traditional benchmarks: substantial efficiency gaps between models with similar capabilities, with some models delivering 97% of top performance at just 0.16% of the cost. PRISM-EE achieves 89% judge agreement compared to 72% for human evaluators, with gaming resistance through cross-provider validation and transparent logging. The framework includes a comprehensive governance system ensuring fair evaluation opportunity for all models regardless of provider size. Our open-source framework makes economic efficiency a primary evaluation criterion, enabling better deployment decisions where both performance and cost matter.", "tldr": "PRISM-EE enables AI models to evaluate each other with cost awareness, revealing that some models achieve 97% of top performance at 641× lower cost than traditional benchmarks suggest.", "keywords": ["Large Language Model Evaluation", "Cost-Aware Evaluation", "Dual-Track Rating", "Swiss-Style Pairing"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0217d33d4aa6106ede5b6ca0b4529b04c2d71fc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper suggests a cost aware evaluation framework for LLMs that compares to Elo rankings but adds the ability to evaluate models based on their economic value."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Evaluating cost of LLMs is an important potentially overlooked measure."}, "weaknesses": {"value": "- The exposition of the paper is poor. The main text is very difficult to follow with very little being completely explained and terms left undefined.\n- The method of evaluation, data on which evaluation is carried out, and means of defining cost are unclear. \n- Almost none of the variables throughout section 2.3 are defined. \n- Discussion of related work to contextualize this method is also limited."}, "questions": {"value": "I suggest a more careful overview of the methods in the main text with careful definition of the data for evaluation and role of each model including how cost values are obtained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RyTKBVTcAl", "forum": "gxqlFvPgpr", "replyto": "gxqlFvPgpr", "signatures": ["ICLR.cc/2026/Conference/Submission24199/Reviewer_p4NV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24199/Reviewer_p4NV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516726972, "cdate": 1761516726972, "tmdate": 1762942991387, "mdate": 1762942991387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "IZqRoKCMc1", "forum": "gxqlFvPgpr", "replyto": "gxqlFvPgpr", "signatures": ["ICLR.cc/2026/Conference/Submission24199/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24199/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762944766496, "cdate": 1762944766496, "tmdate": 1762944766496, "mdate": 1762944766496, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a novel evaluation framework which aims broadly to address cost-aware large language model evaluations. They use an elo-based ranking system to quantify model performance and capabilities, and use models as both the question generators and the judges in an LLM-arena inspired set-up. Models are additionally used to monitor other model outputs to ensure that model question generators and judges are performing at adequate quality levels. They find that this set-up converges to stable ratings between models in just 25-30 matches with high precision as compared to traditional frameworks, while also allowing for cost-aware weighting that enables practitioners to make informed decisions whether the increased gain from a more expensive model is proportional to the cost increase. They contribute the following: a cost aware evaluation framework, peer-federated methodology, swiss-style pairing for evaluation, explainability of consistent model generation, governance to combat model gaming, and open source framework."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces an original approach at both model evaluation in a human label-free setting as well as diving into the impact of cost as an axis of evaluation for large language models. While traditional elo-scoring approaches for LLM evaluations are not novel in themselves, this spin provides practitioners with scalable evaluation approaches that provide more clear determinations of the impact of cost on performance. They provide a holistic evaluation over 48 models and three separate benchmarks (each from different domains)to rigorously evaluate their methodology. The paper structure is generally clear, with clearly outlined key contributions of a cost aware evaluation framework, peer-federated methodology, swiss-style pairing for evaluation, explainability of consistent model generation, governance to combat model gaming, and open source framework. The results show significant improvement of score convergence as opposed to current traditional methods, requiring only 25 matches as compared to 100 for current practices. Further, their cost-aware alternative approach of evaluation highlights the capacity of cheaper models and quantifies the gap considering cost which is a significant concern to true model deployment."}, "weaknesses": {"value": "Generally, my main concerns with this paper are the calculation/interpretation of the proposed gains by the method. I feel that some claims are not properly supported by either data or appropriate explanations of significance. At a high level, many of the quantitative results are displayed as percentage performance or gap in elo scores between two models. It is not clear how to interpret a difference between two models of varying magnitudes. Is a difference of 5 significant? Why or why not? For example, in the 641x efficiency variation between similar performing models to Qwen 3.2 235B, it is unfair to cherry pick the most expensive model Gemini 2.5 Pro to compare against, especially because raw elo scores are different. What makes a significantly different Elo score if you argue that these are the same/similar (1555.4 vs 1603.9)?\n\nAnother weakness comes within the synthetic data regime for evaluation-- how do we test what may be beyond model capabilities? How do we trust generator model to actually generate accurate pairs besides using group consensus? In a landscape where many models may be trained on a similar data corpus, associated error modes and biases may be propagated in this setting. It would be helpful if this was discussed as either a limitation or if an extension regarding integration of human labels could be proposed. \n\nThe narrative could be improved to further motivate the significance of the results and spend less focus on governance/elo-derivations. It would be helpful to fully explain results from ablations given optimal parameters in Section 2.4. Some parameters are optimally chosen by the opinion of the author (i.e. judge weighting/cost sensitivity depends on user objective so is an opinion-based hyperparameter), while Elo and p_judge are more objective. Provide more details regarding this difference, potentially even including visualizations to illustrate the robustness of the approach to other hyperparameters.\n\nIn Line 214, \"perfect fairness\" is a dangerous claim, especially when you have to include the caveat that one of the models didn't fall within this perfect fairness regime.  You should not claim this without more detailed fairness evaluation (Was there opportunity for bias in the question generation? The ordering of the models? etc)."}, "questions": {"value": "The four layered governance framework seems out of place in the narrative. Is this just to add names to the design decisions? Please integrate this in with the associated design decisions that were made based on this framework.\n\nIs arena disproportional in its matching because their algorithm is focusing on models at the top of the leaderboard? Likewise, for inverse match-count weighting, do we need all matches to receive priority equally or are there some cases where we care less about certain models because they are confidently at the bottom?\n\nIn Table 2,  most efficient cost is not the most informative result since that is API information by numbers of tokens generated. You should include top raw elo score and top cost adjusted elo score here.\n\nSection 3.9 has a duplicated paragraph.\n\nHow does the impact of more models from a given provider in Strata 4 impact the stability of Strata 4? What are the full breakdown of which model goes where? Do you see that if there are more OpenAI models (for example) that you are testing in general, then these may have an unfair advantage as they could upweight models within their own family since they are more likely to prefer their own styles of results?\n\n“a naive all-premium strategy using GPT-4.1 for all tasks costs 226,500, while our tiered approach achieves 98.2% performance at just 12,923—a 94.3% cost reduction.” It seems that you are dividing the elo scores for \"performance capturing\", but this is a misleading quantification/result as we still need to quantify what is a statistically significant elo gap. For example, does a gap of 50 imply that the model A beats model B 5% of the time? Further, Table 7 mixed strategy also has fault in how you derive because what is 98.2 vs 97% performance captured? If you are just optimizing, you could reduce cost down to $266 if you are trying to reduce cost by only using Qwen. But you won’t do this because those 3% are important. So what makes those 3% important not to miss vs those 1.8%? It would be helpful to discuss this and provide formalization of how results should be interpreted.\n\nTables/figures don’t always have captions/titles, such as the figure on page 8. It is not labelled and there are errors in that figure's top right sub-figure (Qwen in legend) as well as bottom right subfigure (unclear what legend represents).\n\nDefine Swiss pairing at the beginning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wh2qdCakNa", "forum": "gxqlFvPgpr", "replyto": "gxqlFvPgpr", "signatures": ["ICLR.cc/2026/Conference/Submission24199/Reviewer_frWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24199/Reviewer_frWg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766966625, "cdate": 1761766966625, "tmdate": 1762942990888, "mdate": 1762942990888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces a new framework for language model evaluation. This new framework uses a peer federated framework consisting of competitors, content creators, and judges. The framework has a dual track where one of the tracks evaluates models on cost-relevant performance. This framework reveals substantial efficiency gaps between models with similar performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Making economic efficiency a priority is an important improvement over the existing benchmarks and leaderboards. The project involves significant technical work, and to my knowledge, is an advance in automating benchmarking."}, "weaknesses": {"value": "What is meant by “97% of top performance” (is it 97% of the top benchmark elo? Is it 97% of top benchmark performance). Wouldn’t the better comparison be the win rate if you are using an Elo benchmark score? This needs to be clarified. \n\nThere are many long sentences that make the document somewhat hard to read. For instance, this sentence needs to be reworded: “The framework must address the critical trust problem of ”who watches the watchers” when the evaluators themselves are AI models, manage economic stakes worth millions in deployment decisions where 641× efficiency gaps directly impact business outcomes, and scale to 100+ models while preventing sophisticated collusion attempts.”\n\nI think the largest issue is the lack of clear discussion and comparison to prior work. Is this the first evaluation framework to introduce all these contributions, ie, peer-federated methodology, fair opportunity guarantees? If it is the first, then it should be mentioned; if it is not the first, then there should be a more comprehensive discussion of prior systems. A few papers are cited, but it's not clear what was done in the previous work and how this improves on these attempts, for instance, “Content Creators: “Models selected from Strata 4 (1520+ Elo) that generate case scenarios and questions for evaluation (Wang et al., 2022)”. I’d like to see more comprehensive evaluations of how this improves over other systems (There is one comparison in the first paragraph). For instance, a clear table showing previous metrics. It would also be great to have a side-by-side comparison of previous evaluations leaderboards ie, Chatbot Arena and PRISM-EE ranking. \n\nThe paper seems to have many technical contributions, but needs a significantly improved and simplified presentation. Overall, it is hard for me to judge the true technical content and novelty."}, "questions": {"value": "How does the elo raw elo, cost elo, compare to chatbot arena elos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J1nwIdM7WI", "forum": "gxqlFvPgpr", "replyto": "gxqlFvPgpr", "signatures": ["ICLR.cc/2026/Conference/Submission24199/Reviewer_nYiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24199/Reviewer_nYiC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983036932, "cdate": 1761983036932, "tmdate": 1762942990644, "mdate": 1762942990644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}