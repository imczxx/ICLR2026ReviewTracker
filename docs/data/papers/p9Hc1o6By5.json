{"id": "p9Hc1o6By5", "number": 19542, "cdate": 1758297097425, "mdate": 1759897033749, "content": {"title": "Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models", "abstract": "With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model’s ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.", "tldr": "A general cross-modal fusion architecture of MLLM with shortcut connnections for multi-level multi-grained feature integration.", "keywords": ["Multimodal Large Language Models；Cross-modal Alignment；Multi-granularity Fusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10150f3326504b87ec31ab5b2cc26e37f644f798.pdf", "supplementary_material": "/attachment/efffac5c3913013c5a3c47ef52ddcb88a0807f4a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SparseCut, a cross-modal fusion architecture for multimodal large language models (MLLMs). The key idea is to introduce sparse shortcut connections between layers of the vision encoder (ViT) and the language model (LLM). SparseCut proposes an efficient multi-grained fusion module, which merges low- and high-resolution image features via cross-attention within each shortcut before they are fed into the LLM. This allows multi-resolution integration without increasing the token sequence length, thereby avoiding quadratic attention cost. Experiments on several benchmarks (VQAv2, GQA, VizWiz, SciQA-IMG, MMBench, SEEDBench, etc.) show consistent improvements over LLaVA-1.5 and DeepStack, both on Vicuna-7B and Vicuna-13B backbones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper’s motivation and contributions are explicitly stated and consistently reflected in the methodology and experiments.\n- The multi-grained fusion approach seems to address the inefficiency of concatenating high-resolution tokens, offering a simple architectural solution."}, "weaknesses": {"value": "- The baseline (DeepStack [1]) shown in the tables of the manuscript appears to be relatively outdated, dating back to June 2024. Could the authors provide comparisons with more recent models or benchmarks?\n- Some baselines (e.g., Qwen2.5-VL, VITRON, LLaVA-NeXT) are missing, making the claimed SOTA improvement less convincing.\n- It is unclear whether this mechanism can be well scaled to video or higher resolution inputs, which are more important for MLLM processing long sequences.\n\n[1] Meng, Lingchen, et al. \"Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms.\" Advances in Neural Information Processing Systems 37 (2024): 23464-23487."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QxcEqItolQ", "forum": "p9Hc1o6By5", "replyto": "p9Hc1o6By5", "signatures": ["ICLR.cc/2026/Conference/Submission19542/Reviewer_UcZ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19542/Reviewer_UcZ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846142872, "cdate": 1761846142872, "tmdate": 1762931429218, "mdate": 1762931429218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SparseCut, a general multimodal fusion framework for MLLMs, which enhances cross-modal understanding by establishing sparse shortcut connections between multiple layers of the vision encoder and the language model. These shortcuts allow hierarchical and multi-grained visual information to be integrated efficiently without extending the LLM’s input length. By fusing high- and low-resolution visual features through cross-attention, SparseCut preserves rich semantics while maintaining computational efficiency. Experiments on various benchmarks show consistent improvements over LLaVA and DeepStack, achieving better performance with minimal additional cost and strong scalability across different base LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a method that can efficiently integrates multi-level and multi-resolution visual features without increasing computational cost.\n2. Through shortcut connections, SparseCut effectively incorporates multi-granularity visual features into the LLM while preserving its original context length and computational efficiency.\n3. The experimental results demonstrate strong generalization and scalability across different base LLMs."}, "weaknesses": {"value": "1. The choice of shortcut pattern (density, distribution) may require manual tuning.\n2. The method relies on a frozen vision encoder, potentially limiting deeper cross-modal alignment."}, "questions": {"value": "1. The choice of shortcut pattern (density, distribution) may require manual tuning.\n2. The method relies on a frozen vision encoder, potentially limiting deeper cross-modal alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "073PQnsfzn", "forum": "p9Hc1o6By5", "replyto": "p9Hc1o6By5", "signatures": ["ICLR.cc/2026/Conference/Submission19542/Reviewer_opN4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19542/Reviewer_opN4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894698985, "cdate": 1761894698985, "tmdate": 1762931428629, "mdate": 1762931428629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents SparseCut, a cross-modal fusion approach that introduces sparse shortcut pathways to efficiently inject multi-level visual information into LLMs. The proposed design aims to enhance semantic fusion while maintaining low computational overhead for the LLM. Experimental results are reported to show notable performance gains across several benchmarks, suggesting promising generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall idea is conceptually clear and well motivated.\n2. The evaluation covers a reasonably broad set of benchmarks, demonstrating the generalization capabilities of the approach.\n3. The manuscript is clearly written and easy to read."}, "weaknesses": {"value": "1. The experiments are confined to the Vicuna-based LLaVA framework. To support the claim of wide applicability, additional validation on more diverse and up-to-date LLM backbones (e.g., Qwen2.5 series) and multimodal architectures (e.g., Qwen2.5-VL, InternVL-2.5) would be essential.\n2. The paper emphasizes efficiency on the language side but neglects the additional cost incurred by processing higher-resolution images through the vision encoder. Reporting overall metrics such as end-to-end FLOPs or inference FPS would provide a more accurate assessment of the actual computational burden.\n3. The baseline (LLaVA-1.5) is evaluated at a lower image resolution, whereas the proposed method adopts a much higher resolution. Since increased resolution itself can yield substantial performance improvement, this discrepancy makes it difficult to isolate the contribution of the SparseCut design. A fair comparison under identical resolution settings is needed."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ucCrYlsE2s", "forum": "p9Hc1o6By5", "replyto": "p9Hc1o6By5", "signatures": ["ICLR.cc/2026/Conference/Submission19542/Reviewer_PBEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19542/Reviewer_PBEk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899128947, "cdate": 1761899128947, "tmdate": 1762931428285, "mdate": 1762931428285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SparseCut, a general cross-modal fusion architecture for Multimodal Large Language Models (MLLMs) to address limitations in existing MLLMs—specifically the neglect of mid/low-level visual semantics and high computational costs from multi-grained feature integration.\nExperiments validate SparseCut’s effectiveness across multiple benchmarks. Ablation studies confirm that sparse/uniform shortcuts and multi-grained fusion contribute to performance gains"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tAddresses two critical pain points of existing MLLMs—loss of mid/low-level visual semantics (by leveraging multi-level vision encoder layers) and high computation from multi-resolution features (by fusing features before shortcut injection)—filling gaps in current cross-modal fusion designs.\n2.\tSparseCut is compatible with diverse base LLMs (Vicuna, Phi-3) and scales across model sizes (3.5B–13B). The shortcut pattern (order/distribution/density) is configurable, making it a flexible framework rather than a task-specific solution.\n3.\tBy avoiding input context length expansion (a common issue with multi-grained fusion), SparseCut maintains low computational complexity while enhancing performance—critical for practical deployment of large MLLMs."}, "weaknesses": {"value": "1.\tWhile the paper tests sparse/uniform, dense/bottom patterns, it lacks a systematic exploration of why the U-shaped order is optimal (e.g., no comparison to linear/ random connection orders) or how to dynamically adjust shortcut density/distribution for different tasks (e.g., fine-grained recognition vs. coarse visual reasoning).\n\n2.\tThe paper mentions freezing the vision encoder during training but provides no analysis of training stability (e.g., whether sparse shortcuts mitigate overfitting) or convergence speed compared to baselines. It also does not explore the impact of pretraining/fine-tuning data size on SparseCut’s performance.\n\t\t\n3.\tQuantitative benchmarks dominate the evaluation, but there is no qualitative analysis (e.g., case studies of VizWiz unanswerable questions or MMBench reasoning) to illustrate how multi-level/multi-grained fusion specifically improves cross-modal understanding (e.g., reducing hallucinations)."}, "questions": {"value": "Besides the weakness, I have extra questions:\n\nCompared to UNet format connections, what’s the performance of the method that fuses shallow features of ViT with deeper features of LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c4NqHemgIG", "forum": "p9Hc1o6By5", "replyto": "p9Hc1o6By5", "signatures": ["ICLR.cc/2026/Conference/Submission19542/Reviewer_YYJc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19542/Reviewer_YYJc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19542/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900033124, "cdate": 1761900033124, "tmdate": 1762931427900, "mdate": 1762931427900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}