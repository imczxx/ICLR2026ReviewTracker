{"id": "0xE0kNdGIz", "number": 19240, "cdate": 1758294737975, "mdate": 1763436530040, "content": {"title": "Vulcan: Crafting Compact Class-Specific Vision Transformers For Edge Intelligence", "abstract": "Large Vision Transformers (ViTs) must often be compressed before they can be deployed on resource-constrained edge devices. \nHowever, many edge devices require only part of the *all-classes* knowledge of a pre-trained ViT in their corresponding application scenarios. This is overlooked by existing compression methods. Lightweight models produced by these methods retain a substantial amount of class-irrelevant knowledge and suffer suboptimal performance on target classes. To address this, we analyze the knowledge distribution of ViT and reveal a knowledge disentanglement within it: neurons in the feed-forward network (FFN) modules encode class-specific knowledge, while the multi-head attention (MHA) modules capture class-agnostic patterns. Building on this insight, we introduce Vulcan, a pruning-oriented post-training method for deriving compact class-specific models from a pre-trained ViT under given resource budgets. Vulcan follows a novel *train-then-prune* paradigm, which introduces redundancy into ViTs deliberately by collapsing FFN neurons onto those with the highest class-specific activations and by enforcing low-rankness in MHA weights. This design mitigates the irreversible knowledge loss of direct pruning, so that the post-trained model can be compressed into a compact one with negligible performance loss. Notably, the derived edge ViTs not only achieve significant reductions in size and computation but also even surpass the original ViTs in performance on specific classes. Comprehensive experiments with five base ViTs covering three representative visual tasks on four datasets demonstrate that Vulcan-derived ViTs outperform the base ViTs on class-specific tasks by up to 15.12\\% in accuracy, with only 20\\%–40\\% of their sizes. Compared with state-of-the-art structured pruning methods, Vulcan improves class-specific accuracy by up to 13.92\\%. Code is available at [Vulcan](https://anonymous.4open.science/r/Vulcan-4CEB).", "tldr": "We introduce Vulcan, a pruning-driven post-training method that follows a novel train-then-prune paradigm to derive compact class-specific Vision Transformers (ViTs) from pre-trained models.", "keywords": ["Class-specific model derivation", "Vision Transformer", "structured pruning", "edge intelligence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16810310850aaff7422421a02ac42f7ab7566beb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Vulcan, a pruning-oriented post-training framework for deriving class-specific compact Vision Transformers for edge deployment. The key insight comes from that FFN neurons encode class-specific knowledge, while MHA layers capture class-agnostic patterns. Based on this, Vulcan proposes two complementary techniques: \n(1) CCNC clusters FFN neurons by activation and collapses them toward anchor neurons, and \n(2) TNNR, which induces low-rank structure in MHA weights to enable near-lossless SVD pruning. \nA unified train-then-prune optimization via augmented Lagrangian enforces these redundancies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Edge deployment with individualization is an interesting and important problem in edge cloud cooperation, and the authors provide a practical and effective pipeline solution.\n2. The insight on knowledge disentanglement between FFN and MHA modules is convincing and well-supported by visualization and interpretability experiments.\n3. Experimental evaluation is comprehensive, covering multiple architectures (DeiT, Swin), tasks (classification, detection, segmentation), and hardware setups (Jetson, RTX 4090).\n4. The ablation studies and visual analyses are comprehensive."}, "weaknesses": {"value": "1. The writeup is somewhat complicated. Although the steps and concepts are strictly formalized, there is little intuition or explanation, making the formulations and notations hard to follow. For example, in the term $a^{(l,i)} = \\sum_i \\sigma(X[i] \\cdot n^{(l,i)}_1)$ , it seems  $i$ is summed out on the right-hand side, but the left-hand side still includes $i$ , which is confusing.\n2. While conceptually clear, the method section is dense and overly mathematical, which makes it difficult to grasp the main ideas of the augmented Lagrangian optimization.\n3. The methodology feels more like a collection of engineering techniques combined into one pipeline rather than a focused research exploration, with too many tricks applied at once."}, "questions": {"value": "1. Could CCNC overfit to small subtasks if activations are dominated by a few classes?\n2. How sensitive is the neuron clustering step to dataset imbalance or class overlap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3XtkFLbKL", "forum": "0xE0kNdGIz", "replyto": "0xE0kNdGIz", "signatures": ["ICLR.cc/2026/Conference/Submission19240/Reviewer_Z9iw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19240/Reviewer_Z9iw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877438251, "cdate": 1761877438251, "tmdate": 1762931218147, "mdate": 1762931218147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the class-specific requirements of deploying ViT on edge devices. Many edge scenarios require only a small number of target classes to be recognized, but the large amount of irrelevant class knowledge carried by pre-trained ViT consumes resources and can interfere with target class performance. The authors claim that by analyzing the knowledge distribution of ViT modules, they find that FFNs favor class-specific knowledge, while MHAs favor class-independent patterns. Based on this, they propose a train-then-prune post-training framework Vulcan. Vulcan introduces redundancy into ViTs deliberately by collapsing FFN neurons onto those with the highest class-specific activations and by enforcing low-rankness in MHA weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe problem studied in this paper is meaningful, namely, achieving lightweight model deployment at the edge. The authors also have a good insight that not all types of information are useful.\n2.\tCCNC clusters FFN neurons by \"activating the highest anchor point\", and TNNR adaptively distributes pruning rates by effective rank. Both are consistent with the observation of \"FFN class specific/MHA class independent\"; they are unified in the augmented Lagrangian objective, which facilitates direct implementation of structured pruning.\n3.\tThe authors conducted sufficient experiments to demonstrate the effectiveness of the proposed framework."}, "weaknesses": {"value": "1.\tThe paper uses visualization and the phenomenon that \"data-free SVD is better than score-based pruning\" to support the claim that \"MHA is more class-agnostic \", but lacks more rigorous verification. Can some other models or datasets be used to further support this observation?\n2.\tTNNR introduces additional computations of singular values/effective ranks, which may incur significant overhead; the paper does not provide data on wall clock duration and scalability for large models/big data scenarios.\n3.\tThe author can provide a rough sketch of the theoretical analysis in the main paper to improve the completeness of the entire work."}, "questions": {"value": "1.\tThe paper uses visualization and the phenomenon that \"data-free SVD is better than score-based pruning\" to support the claim that \"MHA is more class-agnostic \", but lacks more rigorous verification. Can some other models or datasets be used to further support this observation?\n2.\tCan the authors report the relative training time/GPU memory overhead of TNNR (relative to fine-tuning only the task loss) on different platforms/datasets?\n3.\tThe random subtask is only divided into 3 categories. Can you provide more mean ± variance or confidence intervals of the random divisions, especially under high shear rates of 0.8/0.9, to prove stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FCIi34GkzL", "forum": "0xE0kNdGIz", "replyto": "0xE0kNdGIz", "signatures": ["ICLR.cc/2026/Conference/Submission19240/Reviewer_J15G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19240/Reviewer_J15G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897054706, "cdate": 1761897054706, "tmdate": 1762931217816, "mdate": 1762931217816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Vulcan, a framework for class-specific pruning of pre-trained Vision Transformers (ViTs). In ViTs, the FFN layers primarily capture class-related features, while the MHA layers tend to preserve class-agnostic information. Based on this property, Vulcan applies different compression strategies to the two components: redundant neurons in FFN layers are merged through neuron clustering and collapse, and attention matrices in MHA layers are regularized with truncated nuclear norm (TNNR) to induce low-rank structures. After a post-training stage, the framework directly derives a compact ViT specialized for a subset of target classes. Experiments on multiple datasets demonstrate that Vulcan improves classification accuracy under high pruning ratios and achieves notable speedups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental evaluation is extensive, covering multiple datasets (ImageNet, CIFAR-10/100, COCO) and tasks (classification and detection). Results are reported under various pruning ratios with comprehensive metrics including accuracy, FLOPs, parameter count, and hardware latency.\n2. Implementation details are clearly presented, making the method reproducible. The post-training behavior, hyperparameter sensitivity, and neuron visualization provide additional insights into the framework."}, "weaknesses": {"value": "1. The overall idea builds on the known observation that FFN layers in ViTs capture class-related information, and both the neuron clustering and low-rank decomposition techniques are well-established; thus, the level of novelty is limited.\n2. The application scenario involves only a small number of target classes, which could often be addressed by lightweight models trained on smaller datasets, leaving the necessity of this approach unclear.\n3. The method section contains an excessive amount of symbols and equations, making the presentation somewhat dense and difficult to follow."}, "questions": {"value": "1. It is not specified whether the baseline models were also retrained on the same class subsets—could this affect the fairness of the comparisons?\n2. The paper does not compare Vulcan with lightweight models designed for limited-class scenarios; would Vulcan still provide a clear advantage under such settings?\n3. The FFN collapse and MHA low-rank compression losses are directly added together—could there be potential interactions between the two, and is such joint optimization theoretically justified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "06QSOoQbW7", "forum": "0xE0kNdGIz", "replyto": "0xE0kNdGIz", "signatures": ["ICLR.cc/2026/Conference/Submission19240/Reviewer_QM9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19240/Reviewer_QM9G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961261966, "cdate": 1761961261966, "tmdate": 1762931217515, "mdate": 1762931217515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets class-specific edge deployment by compressing general ViTs into compact, per-class models. It hypothesizes FFN layers are more class-specific while MHA is class-agnostic, and proposes a train-then-prune pipeline: CCNC for FFN and TNNR+SVD for MHA under FLOPs/parameter budgets. Across five ViTs, three tasks, and four datasets, the approach keeps only 20–40% of parameters while matching or improving base accuracy, outperforming structured pruning baselines by up to 13.92%, with hardware-friendly dimensional alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically formulates class-specific ViT compression for edge, supports the FFN/MHA functional split with analyses, and pairs CCNC (FFN) with TNNR+SVD (MHA) under explicit budget constraints.\n2. Introducing redundancy and then pruning by effective rank/activation clustering mitigates irreversible knowledge loss typical of prune-then-train; results show consistent gains at 0.6/0.8 pruning ratios over strong structured-pruning baselines.\n3. Budgets are enforced in GFLOPs/params, post-pruning dimensions are aligned to hardware-friendly multiples (e.g., ×8), and SVD on attention heads is shown competitive with saliency-based pruning."}, "weaknesses": {"value": "1. It could be very interesting if this framework could be extended as a streaming service platform which can publish tuned weights to edge models in real time.\n2. Although some hardware cannot support numerical types, the combination of the proposed method and quantization could be important."}, "questions": {"value": "I see you have covered many experimental results in the appendix. I don't have more questions. Good luck."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rQDDlW5yKR", "forum": "0xE0kNdGIz", "replyto": "0xE0kNdGIz", "signatures": ["ICLR.cc/2026/Conference/Submission19240/Reviewer_pANN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19240/Reviewer_pANN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990193626, "cdate": 1761990193626, "tmdate": 1762931217082, "mdate": 1762931217082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}