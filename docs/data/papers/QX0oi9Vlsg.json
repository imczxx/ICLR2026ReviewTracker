{"id": "QX0oi9Vlsg", "number": 2659, "cdate": 1757182765254, "mdate": 1759898134554, "content": {"title": "Adversarial Bottleneck Method for Vision-Language Large Model Explainability", "abstract": "Nowadays CLIP is a leading vision-language model, showing strong functionality, especially in tasks like search engine matching. However, its high performance is often accompanied by the complexity of the decision-making process, making the interpretability of the model a major challenge. Existing XAI methods mainly focus on unimodal settings, with state-of-the-art methods often being attribution algorithms based on adversarial attacks. These methods perform well in unimodal tasks such as image classification. However, expanding these methods to handle cross-modal tasks (such as image-text alignment and cross-modal retrieval) presents several obstacles. For multimodal tasks, the most effective XAI methods currently rely on the bottleneck principle, which limits information flow to analyze model decisions. In this paper, we propose a new approach that integrates adversarial attribution methods with the bottleneck principle. This approach not only interprets multimodal models such as CLIP but also preserves the advantage of unimodal attribution algorithms in precisely identifying key features that influence model decisions within a specific modality. By introducing our model, we can obtain a more robust and broadly applicable representation for vision-language models, further enhancing their transparency and trustworthiness in complex tasks. Comprehensive experiments demonstrate that, compared to state-of-the-art XAI methods, our approach improves the interpretability of text and images by 69.12\\% and 19.36\\%, respectively. Our code is available at https://anonymous.4open.science/r/ABM-5C28/", "tldr": "", "keywords": ["Bottleneck Principle", "Explainability", "Cross-Modal Explainability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ba6fbd610416f07b2024a3968d25afd24b99edf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ABM, a VLM interpretation method that works by fusing adversarial attribution and information bottleneck. ABM improves over M2IB by performing adversarial updates on intermediate-layer bottleneck variables with a sign-based step scaled by the original magnitude and clipped by a constraint, which overcomes the limitation of M2IB including sensitivity to hyper-parameters and difficulty in gradient-based optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea to integrate adversarial attribution and information bottleneck for VLM explanation is interesting.\n2. The authors provide theoretical insights into the proposed method.\n3. The evaluation is comprehensive."}, "weaknesses": {"value": "1. The impact of this paper is small to me. It improves over M2IB, a very specific VLM interpretation technique. And the improvement mainly focuses on the optimization of M2IB.  M2IB is not the SOTA method for CLIP interpretation, with later work such as Grad-ECLIP [1] showing much better performance. \n2. The readability of the math part is poor.  Many symbols are reused to represent different meanings (e.g., $T$ represents both text and the total steps, $g$ appears in both eq.3 and eq.16 with different meanings). And many clarifications are posited far away from the equation (e.g., the explanation in line 315 should be put earlier). This makes the paper very difficult to read. \n3. ABM is built on M2IB, but the paper does not introduce the specific algorithm or derivation of M2IB, only a high-level introduction of M2IB is given on Sec.3.2.1. This makes the paper less self-contained. For example, when introducing eq.11, the KL term seems very strange as it has not been mentioned before. Also, I would recommend the authors to include an algorithm box to better clarify ABM.\n4. I do not totally understand the proof of theorem3.1. There are some typos. For example, in eq.16, it states, $\\tilde{z}^{t+1}=g(z^t)$, then in 17, why not $I(\\tilde{z}^{t+1}, e_{m’})= I(g(z^t), e_{m’})$? Moreover, how can you entirely ignore the higher-order terms when proving the monotonicity?\n5. In line 372, the definition of confidence drop is confusing. Shouldn’t a larger drop confidence drop when removing the important features denote better interpretability? The correct definition should be the drop in performance if only the high-attribution parts are kept.\n\n[1] Zhao, Chenyang, et al. \"Gradient-based visual explanation for transformer-based clip.\" ICML 2024."}, "questions": {"value": "1. Could you compare your method with Grad-ECLIP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FCvAMAyNuc", "forum": "QX0oi9Vlsg", "replyto": "QX0oi9Vlsg", "signatures": ["ICLR.cc/2026/Conference/Submission2659/Reviewer_P2P4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2659/Reviewer_P2P4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318924417, "cdate": 1761318924417, "tmdate": 1762916323919, "mdate": 1762916323919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study the interpretability of vision language models (VLMs) and attempt to improve the existing M2IB method (reviewed in equation 2) by the framework called \"Adversarial Attribution Theory\". In Sec. 3.2.1. the authors argue that the M2IB method may give untrustworthy results because of the fixed choice of hyperparameter $\\beta$ and the hyperparameter choice of the gradient descent method (Lines 216-220). Then, it seems the authors review some calculus in section 3 to propose a new optimization approach, by introducing the extra $C_z$ clipping operator. They present numerical results in section 4."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "-The authors' motivation to improve the interpretability of CLIP is sensible."}, "weaknesses": {"value": "The paper has several flaws in its theoretical claims, technical soundness, and presentation.\n\n-**Incorrect theoretical result in Theorem 3.1**: Theorem 3.1, the paper's only theorem, tries to prove Equations 7 and 8. However, the proof of Equation 7 is incorrect and suffers from a basic mistake. In the Appendix sec A, the authors mention the following equation in their proof (Lines 663–665):\n\n> $$I(\\tilde{z}^0, x_m) = H(x_m) − H(z | x_m) = H(x_m)$$\n\nThe above equation is wrong and the correct equation is $I(Z;X)= H(X) - H(X|Z)$, where the conditional entropy is mistakenly stated to be $H(Z|X)$ by the authors. This error invalidates the authors’ claim, because they use the wrong equation to claim $H(z | x_m) = 0$. This is while, in the correct form, $H(x_m|z) \\neq 0 $ unless $x_m$ is a function of $z$, being exactly the opposite of the authors' statement in front of Equation 13 and disproving their claim in Equation 7.\n\n-**Trivial theoretical claim in Equation 8 of Theorem 3.1**: In addition to the wrong statement in Equation 7, the authors state a trivial result in Equation 8 (second part of Theorem 3.1). If I understood the claim correctly (the presentation is unclear), Equation 8 says that, assuming a zero error for the first-order Taylor series expansion (as supposed in Line 671 and not mentioned in Theorem 3.1), applying the gradient ascent with the clipped gradient in Equation (9) will only increase the function value.\n\nHowever, I believe this is a trivial statement under the assumption of zero error for the first-order Taylor series expansion. Given the first-order Taylor expansion, the function value will trivially increase locally along any direction with a positive inner product with the gradient direction, and the clipping operation is the projection of the gradient vector on the $\\ell_\\infty$-norm ball, which will obviously have a positive inner product with the gradient direction. I am wondering in what sense Equation (8) in Theorem 3.1 proves something non-trivial, as this part of the theorem is the only remaining theoretical claim in the draft, excluding the wrong Equation (7).\n\n-**Clarity of the presentation**: I cannot understand the purpose of Equations 3 and 4. Equations 3 and 4 seem to state the fundamental theorem of calculus, that if one integrates the gradient of a function along a path from $z^0$ to $z^T$, the output is the function difference evaluated at $z^0$ and $z^T$. I cannot appreciate why the authors spend more than half a page on this basic fact. In what sense does this discussion support the known optimization iteration (projected gradient ascent using $\\ell_\\infty$-norm ball) in Equation 5?\n\n-**Presentation issues**: There are several writing errors in the text. Some obvious cases are the misspecified references “(?)” in Lines 182–184 and the wrong in-line position of the $\\log$ input in Equation 11.\n\nAlso, I could not see how the authors obtained Equation (10) from what they discussed before. $z^{(t)}$ is supposed to follow the update Equation (6) with the mutual information between the CLIP input and output variables. How does this equation lead to Equation (10)? What is the role of the mutual information in getting the update rule in (10) from Equation (6)? As said, it is difficult to follow the discussion after this equation due to the incoherence in writing and notations."}, "questions": {"value": "1. Can the authors clarify the correctness of Equation (7) and the non-triviality of Equation (8) in Theorem 3.1, which appears to be the main technical claim of this work?\n\n2. How do the authors derive the update rule (10) from Equations (6) and (9)?\n\n3. The authors criticize the M2IB method of Wang et al. (2023) in Equations (1) and (2) for relying on the $\\beta$ hyperparameter in Equation (2) and for using gradient ascent to solve (2) (as stated in Lines 210–213). Can the authors clearly explain how their algorithm differs from that of Wang et al. (2023)? Specifically, is the only modification the introduction of the clipping operator in Equations (6) and (9) in place of the vanilla gradient ascent update?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i2GAIdH4qs", "forum": "QX0oi9Vlsg", "replyto": "QX0oi9Vlsg", "signatures": ["ICLR.cc/2026/Conference/Submission2659/Reviewer_WHDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2659/Reviewer_WHDb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723243111, "cdate": 1761723243111, "tmdate": 1762916323638, "mdate": 1762916323638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper are proposed to enhance explanations for large-scale Vision-Language Models (VLMs) like CLIP. It identifies critical flaws in two major categories of existing Explainable AI (XAI) methods when applied to this cross-modal setting. To overcome these limitations, the paper proposes a framework called the Adversarial Bottleneck Method (ABM). ABM synthesizes the strengths of both adversarial attribution and the information bottleneck principle."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of ABM using an adversarial update process to achieve the goals of the information bottleneck is considerably novel. The method reframes the bottleneck optimization problem, replacing a difficult-to-tune hyperparameter $\\beta$ with an iterative optimization process governed by a more intuitive parameter. The theoretical reasoning provided in Theorem 3.1 establishes a theoretical intuition for why this adversarial method works. The quantitative results presented in this work also show sufficient improvements compared to other baselines."}, "weaknesses": {"value": "1. The paper positions itself as eliminating heuristic hyperparameters, primarily targeting M2IB's parameter $\\beta$. However, it introduces its own hyperparameter $T$. The ablation study in Figure 2 shows that the model's performance on images is sensitive to the choice of $T$, also requiring the heuristic and empirical choice of hyperparameter tuning. While $T$ may be more intuitive than $\\beta$, it is still a hyperparameter that requires tuning for optimal performance. The claim of eliminating parameters is somehow an overstatement.\n2. One question is about the evaluation scheme for test modality. The evaluation for text interpretability in Appendix C is defined as a binary indicator. While the authors follow the evaluation in M2IB, this metric could easily mask the true performance differences between methods or accurately reflect the quality of the explanation. Therefore, the stability of text results across different T values in Table 5 compared to the image modality looks like a result of this binary metric rather than true performance. Could the author provide some discussions about this point?\n3. Minor typos: Page 4, Section 3.2.1 \"Integrated Gradients (?) or Grad-CAM (?)\""}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YmSMnHgQBN", "forum": "QX0oi9Vlsg", "replyto": "QX0oi9Vlsg", "signatures": ["ICLR.cc/2026/Conference/Submission2659/Reviewer_fCUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2659/Reviewer_fCUp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980139866, "cdate": 1761980139866, "tmdate": 1762916323196, "mdate": 1762916323196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Adversarial Bottleneck Method (ABM), a framework for improving explainability in vision-language large models (VLLMs). It provides a bound relating adversarial risk to mutual information between latent codes and input features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ the method is theoretical solid. It integrates adversarial robustness and information bottleneck theory.\n+ The experiments are comprehensive. The evaluation covers diverse VLLM tasks (captioning, VQA, entailment)."}, "weaknesses": {"value": "- Baseline comparison.\nComparisons focus mainly on M2IB, VIB, and GradIB. It would be informative to include more recent multimodal explainability baselines like BLIP-Explain or ALIGN-Attribution."}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2KV24rTjtz", "forum": "QX0oi9Vlsg", "replyto": "QX0oi9Vlsg", "signatures": ["ICLR.cc/2026/Conference/Submission2659/Reviewer_PZGW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2659/Reviewer_PZGW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116853404, "cdate": 1762116853404, "tmdate": 1762916322760, "mdate": 1762916322760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}