{"id": "iBLHGdBImw", "number": 24787, "cdate": 1758360357167, "mdate": 1759896748526, "content": {"title": "From Numerical Solvers to Graph Surrogates: Physics-Informed Losses for Data-Efficient CFD Modeling", "abstract": "Graph neural networks (GNN) represent a promising method for creating robust and physically interpretable surrogate models for fluid dynamics. These surrogates offer a significant advantage over traditional computational fluid dynamics (CFD) solvers based on numerical methods because they require much less computational cost. In a GNN designed as a surrogate model for spatio-temporal partial differential equations, message passing can be interpreted as the propagation of physical quantities such as velocity, pressure, and temperature. The complexity of the Navier-Stokes equations, however, can limit the generalizability of existing models and lead to long training times.\nWe show that including a physics-informed loss function based on the numerical methods used to generate the training data, specifically the finite volume method, can reduce the amount of data needed to train an accurate physics-informed surrogate compared with a purely data-driven baseline. By reducing the dataset size by 20\\% and applying this approach, we achieved a 33\\% reduction in convergence time. For larger datasets, model accuracy improved by up to 7.4\\% within the same timeframe. Our method also avoids interpolation between cell centers and vertices, which can introduce errors from numerical discretization. Applying this soft constraint during training can support the development of future CFD surrogate GNN models that perform well even with smaller datasets.", "tldr": "", "keywords": ["Physics Informed Neural Networks; Graph Neural Networks; Fluid Dynamics Surrogate"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d9030d18b4c981fb65fb7231aa580cea92efbdd6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors enhance MeshGraphNets by physics-informed loss function based on finite-volume method. The approach significantly reduces convergence time by up to 33% and improves predictive accuracy by up to 7.4% on dataset generated by authors using OpenFOAM.  It also allows for training with smaller datasets. The authors test these benefits across various data scales and demonstrate that the method is able to create more efficient and physically consistent GNN-based surrogate models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Authors integrated FVM residuals into the GNN loss function and achieved improved predictive accuracy and more physically consistent simulations.\n2. Convergence time was reduced by up to 33%, it is beneficial in data-limited scenarios.\n3. Less training data is needed.\n4. Avoids numerical errors from data interpolation by using cell-centered discretization."}, "weaknesses": {"value": "Mostly, the paper feels largely incomplete. \n\n1. Firstly, the title of paper in pdf is different from openreview: \"Effects of soft physics constraints on graph neural network-based fluid mechanics modeling\" vs \"From Numerical Solvers to Graph Surrogates: Physics-Informed Losses for Data-Efficient CFD Modeling\".  Although the meaning is probably the same, it is a bit confusing.\n2. Generalization ability is limited. The entire study focuses on incompressible flow over a cylinder with fixed Reynolds number. The paper acknowledges that \"The largest remaining errors occur in the turbulent wake regions\", which are inherently difficult to model. This raises questions about how well the method would generalize to more complex scenarios. \n3. \"No physics\" baseline definition. While \"no physics\" is clear in principle, the authors need to explain whether the baseline models were optimized with the same SOAP optimizer and learning rate scheduler. If it is not true, some of the performance gains attributed to physics could be partially influenced by optimization techniques.\n4. The paper states that the loss function is \"simply a linear weighting\" and defines the coefficients a, b, c (equation 5). However, there's no discussion on how these weights were determined (and their values are not mentioned), if they were tuned, or their sensitivity to different flow conditions / dataset sizes. The choice of these weights can significantly impact the balance between data fidelity and physical consistency.\n5. We don't know exactly how dataset pruning was done. Do authors drop samples randomly? Or do they drop some simulations based on some criteria? This is not stated on paper.\n6. Strong justification of choosing SOAP optimizer is needed.\n7. Provide explanation about why there are not direct comparisons with some previous studies mentioned in \"Related work\" section."}, "questions": {"value": "1. Please fix the title in pdf or in openreview.\n2. Provide more scenarios where you train and evaluate your approach. It is needed to better show generalization ability.\n3. See W3. Explain what does \"no physics\" mean. Is it optimized with the same SOAP optimizer or it is vanilla MeshGraphNets.\n4. Provide exact values of coefficients a,b,c in equation 5 with the procedure how they were determined. Ideally, do ablation study with different values of these coefficients.\n5. Provide more explanation for this statement: \"Despite these advances, the trade-offs between data-only training and physics-informed training remain poorly characterized\". Now it is a little bit vague.\n6. Authors say \"In conjunction with standard practice, the equations are nondimensionalized to discard the effects of physical units e.g. density and viscosity\". Probably, \"discard\" is wrong word.  In nondimensionalization, the effects of density and viscosity are not discarded. Instead, they are encapsulated within dimensionless numbers like the Reynolds number.\n7. See W5. Please explain more about dataset pruning procedure.\n8. Figure 3 shows comparison at a \"representative timestep of the simulation\". But the title of the picture above sounds like \"Comparison for u at timestep 0 | MSE: 0.000000...\". Authors should provide explanation and fix this issue. \n9. Justify choosing SOAP optimizer (why not to choose any other optimizer?).\n10. Probably a typo on page 8: \"Figure 3 presents both the lowest data loss achieved and the average loss across epochs. \" But there are not such values represented on Figure 3.\n11. See W7."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dPqkEqaSEB", "forum": "iBLHGdBImw", "replyto": "iBLHGdBImw", "signatures": ["ICLR.cc/2026/Conference/Submission24787/Reviewer_4MZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24787/Reviewer_4MZT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558016601, "cdate": 1761558016601, "tmdate": 1762943197802, "mdate": 1762943197802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper completes MeshGraphNets with soft physics constraints to improve surrogate modeling of Navier-Stokes equation. This improvement might be helpful in data-limited settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*\tSolid discussion on nonconvex optimization. The designed optimizer converges fairly fast.\n*\tThe topic of reducing the size of datasets is in fact important.\n*\tAccuracy near 1% achieved for pressure"}, "weaknesses": {"value": "*\tThe use of physics-informed loss functions in combinations with MeshGraphNets appeared earlier in several works, [1,2], what dims originality of the presented paper.\n*\tJust one modelling example was presented. \n*\tVelocity streamlines not illustrated and velocity error non analysed.\n*\tThe title in the PDF (Effects of soft physics constraints…) is different from that in the OpenReview system (From Numerical Solvers to Graph Surrogates…). \n*\tThe acronym “iff.” may not be clear for some readers. \n*\t“Figure 3 presents both the lowest data loss achieved and the average loss across epochs”. There are no loss functions in Fig. 3.  \n\n\n[1] Würth, T., Freymuth, N., Zimmerling, C., Neumann, G., & Kärger, L. (2024). Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes. Computer Methods in Applied Mechanics and Engineering, 429, 117102. https://doi.org/10.1016/j.cma.2024.117102\n\n[2] Zhang, H., Jiang, L., Chu, X., Wen, Y., Li, L., Liu, J., Xiao, Y., & Wang, L. (2025). Combining physics-informed graph neural network and finite difference for solving forward and inverse spatiotemporal PDEs. Computer Physics Communications, 308, 109462. https://doi.org/10.1016/j.cpc.2024.109462"}, "questions": {"value": "See weaknesses. Other questions:\n* Why pressure not involved in the data loss term (L_data)? \n* How exactly L_momentum and L_mass were defined?\n* How long the training took in terms of hours? Try to compare versus data modelling with a numerical method.\n*\tIt would be interesting to see evolution of the term||u_actual – u_predicted|| versus time. Same for pressure.\n*\tCan the trained model be reused to solve the equation on another grid or at other Reynolds? Same question for other boundary conditions and domain/obstacle shapes. \n*\tHow well the conservation law, div V = 0, holds on the learned data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jCjges1dkB", "forum": "iBLHGdBImw", "replyto": "iBLHGdBImw", "signatures": ["ICLR.cc/2026/Conference/Submission24787/Reviewer_vYFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24787/Reviewer_vYFJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913837480, "cdate": 1761913837480, "tmdate": 1762943197568, "mdate": 1762943197568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to augment MGNs with a physics-informed loss derived from the finite volume method (FVM) residuals of the Navier–Stokes equation + continuity equation, enforcing soft conservation of mass and momentum. The resulting total loss combines data loss with physics residuals, weighted by tunable coefficients. To improve training efficiency under data scarcity, they pair this approach with the SOAP optimizer (Shampoo + Adam preconditioning), along with a theoretical analysis showing that appropriate learning-rate scaling maintains convergence guarantees under static data reduction.\nUsing OpenFOAM-generated datasets for incompressible 2D cylinder flow (Re = 100), the authors show that:\n* adding the soft physics loss reduces number of epochs to converge by up to 33% even when the dataset is reduced by 20%\n* model accuracy improves by up to 7.4%"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* results are provided as practical outcomes\n* theoretical evaluation of optimizer with static reduction and schedule scaling\n* avoidance of interpolation errors through cell-centered discretization"}, "weaknesses": {"value": "* the approach lacks novelty (even practical)\n* evaluation is restricted to laminar low-Reynolds 2D cylinder flow which is not the most complicate setting for Navier-Stokes eq.\n* the obtained improval is small and made be in the range of the errors of its assessment\n* the errors of obtained metrics were not estimated\n* no ablations and hyperparameter search\n* no computational expenses provided\n* no reproducible code"}, "questions": {"value": "* How your approach differs from other physics-informed GNNs with FV loss?\n* How the time per epoch changed compared to baseline MGN?\n* How your approach will deal with turbulent flows? What about space varying viscosity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQNKmxNBm6", "forum": "iBLHGdBImw", "replyto": "iBLHGdBImw", "signatures": ["ICLR.cc/2026/Conference/Submission24787/Reviewer_QAWR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24787/Reviewer_QAWR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995362281, "cdate": 1761995362281, "tmdate": 1762943197320, "mdate": 1762943197320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a way to integrate physics loss, derived from Finite Volume Method (FVM) residuals, into a MeshGraphNet (MGN) framework to improve data efficiency and convergence. The motivation appears to originate from the attempt to strike a balance between data-driven vs. physics-driven (PINN-style), which, if achieved, could be a significant step towards advancing the CFD-AI domain."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I appreciate the authors systematically demonstrating the improvements in convergence speed and accuracy under multiple dataset reduction levels. \n\n2. I also appreciate the theoretical proof that under standard smoothness and bounded-variance assumptions, the SOAP optimizer with the scaled learning-rate schedule achieves the same asymptotic convergence rate as full-data training, under certain conditions."}, "weaknesses": {"value": "1. **Generalizability concerns**: The authors only demonstrate performance across one dataset, raising concerns about whether the performance gain consistently prevails across other datasets and varying geometries. I encourage the authors to consider benchmarks such as airfoils (AirfRANS) and those considered in the MeshGraphnet paper. \n\n2. **Lack of sufficient discussion on the dataset**: The generation of the validation dataset is not clearly discussed. The data generation subsection should discuss various choices, such as why the authors considered unit inlet velocity and Re = 100. In addition, it should also discuss the discretization and important statistics such as the number of nodes, edges, etc., to provide an idea about the scale of the dataset. \n\n3. **Lack of clearly defined problem statement**: Is the author's objective to predict the next-time step or predict a roll-out, as done in MeshGraphnet? If it is to predict only the next time step, that has limited practicality. \n\n4. **Lacking baselines**: Showing improvement over one method (meshgraphnet) is unprecedented in an ML paper. There are many GNN-based methods out there that could benefit from the authors' physics-informed loss and learning rate scheduling, such as FVGCN [Li et al. 2023 in the paper], CFDGCN[2], FV-informed GCN [3], and BSMS-GNN[4]. The authors should consider these baselines to demonstrate that their approach could enhance the existing GNN-based CFD methods in terms of data need and convergence. \n\n5. **Confusing statements**: \" For the 80% dataset size, the physics-informed model reached convergence in 33% fewer epochs compared to the baseline, representing the largest observed gain in training efficiency.\" - What was your stopping criterion that determined this number (33%)? \n\n6. Other minor issues: Figure 2 does not have a professional look. Titles in the subfigures have missing closing brackets. \n\n[1] AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions, NeurIPS'22. \n\n[2] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction, ICML 2020.\n\n[3] Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning‑based CFD Simulation, ICML 2024.\n\n[4] Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network, ICML 2023."}, "questions": {"value": "1. Why is Fig. 3 showing a comparison of only one variable (velocity u-component) at timestep 0? What about other variables, e.g., v and pressure? Could you also add the baseline Meshgraphnet's prediction as well to provide a holistic comparison of their predictive quality?\n\n2. Seek weaknesses 3 and 5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8blJ7SIxC", "forum": "iBLHGdBImw", "replyto": "iBLHGdBImw", "signatures": ["ICLR.cc/2026/Conference/Submission24787/Reviewer_DU8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24787/Reviewer_DU8H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207368347, "cdate": 1762207368347, "tmdate": 1762943197080, "mdate": 1762943197080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}