{"id": "n12TKdYwHQ", "number": 7373, "cdate": 1758018262789, "mdate": 1762919822476, "content": {"title": "RobustIT: Adapter-Centric and Attack-Agnostic Anti-Backdoor Instruction Tuning", "abstract": "Large visual language models (LVLMs) have demonstrated excellent instruction-following capabilities, yet remain vulnerable to stealthy backdoor attacks when fine‑tuned using contaminated data. Existing defenses typically assume full access to model parameters or rely on known trigger patterns and clean validation sets, which are assumptions that fail in real‑world, efficient tuning applications where visual encoders and core LLM weights are frozen and attack priors are unavailable. Motivated by the empirical insight that LVLM adapters quickly overfit fixed triggers, we propose  \\textbf{R}obust \\textbf{I}nstruction \\textbf{T}uning~(\\textbf{RobustIT}), a lightweight, \\emph{attack‑agnostic} framework that tunes only adapter modules and text‑embedding layers. RobustIT combines two complementary regularizations: (1) \\emph{\\textbf\n  {Input Diversity Regularization}}, which applies randomized spatial, color, and textual perturbations to disrupt fixed trigger–response mappings or consistent spurious cues; and (2) \\emph{\\textbf{Anomalous Activation Regularization}}, which dynamically sparsifies adapter channels exhibiting abnormally sharp activations associated with backdoor patterns. This dual strategy steers the model toward semantically grounded representations, without touching frozen cores or requiring any trigger supervision. Extensive evaluations on seven backdoor attacks across Flickr30k and MSCOCO show that RobustIT drives attack success rates to near zero with under 15\\% extra training cost, while preserving or improving standard task performance of tuned models, and also highlight the critical role of efficient fine-tuning safeguards in securing real-world deployments of LVLMs.", "tldr": "", "keywords": ["robustness", "security", "large visual language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e4923ff7607033f36ad0c98392c1d5f055d854ee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a method to defend against data poisoning attacks. In this case the model is trained or fine-tuned using poisoned data, opening backdoor to attack the model during deployment. The proposed method consists of two regularizations, namely Input Diversity Regularization and Anomalous Activation Regularization. The method focuses on testing the defence in cases when fine-tuning of the model is performed via adapters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe method is effective and it drives the attack success rates to essentially 0. At the same time it also boosts performance, presumably because parts of it could be seen as data augmentation and useful regularization to boost the learning ability.\n\n•\tThe method is lightweight as the training times do not become significantly longer.\n\n•\tThere are relatively extensive additional analyses.\n\n•\tThe method does not need access to the attack details to work."}, "weaknesses": {"value": "•\tWhile the paper focuses on adapter-centric setups where adapters are inserted into the model and fine-tuned, it seems to me the method itself is more general and does not depend on this specific setup (e.g. AAR likely can apply to standard weights too). As a result, the focus on adapters feels less well-justified. Would it be possible to evaluate the method also in cases where fine-tuning without adapters is performed? Also only using IDR on its own gives almost all of the benefits (similarly also AAR, but slightly worse).\n\n•\tThere are various existing defences (mentioned in the related work section), which presumably work quite well but are not compared against, because they do not focus on cases where only adapters are fine-tuned. Perhaps extending them to the adapter-centric case is easy and so they should be compared against? Using adapters quite often means we basically assume full access to the model, so adapter-centricity may not be a very crucial aspect.\n\n•\tThe main part of the paper only evaluated one model, while it seems the experiments are not computationally expensive, only taking e.g. around half an hour or less if I understand correctly. It would be good to report the key results on multiple models. There are a few results in the appendix though.\n\n•\tThe paper would benefit from proof-reading as the number of typos and other issues is relatively higher. E.g. in the abstract “fail in real-world, efficient tuning applications”, and in other parts “Falmingo”, “Updation”.\n\n•\tIn general it feels the referenced work is less up to date, e.g. Flamingo is now one of the older MLLMs, while the paper states it is modern."}, "questions": {"value": "•\tCan the method be easily applied also for the case when full model is being fine-tuned or trained?\n\n•\tIs it easy to modify the existing defences for data poisoning into the setup where adapters are inserted?\n\n•\tWhat kind of adapters are considered? E.g. LoRAs are very common but it seems a different kind of adapter was considered. It could be useful to experiment with different types of adapters and show the method works there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GRtAdlqTlO", "forum": "n12TKdYwHQ", "replyto": "n12TKdYwHQ", "signatures": ["ICLR.cc/2026/Conference/Submission7373/Reviewer_ckUZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7373/Reviewer_ckUZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760731352723, "cdate": 1760731352723, "tmdate": 1762919502814, "mdate": 1762919502814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YUpCzBfLbI", "forum": "n12TKdYwHQ", "replyto": "n12TKdYwHQ", "signatures": ["ICLR.cc/2026/Conference/Submission7373/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7373/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762919820908, "cdate": 1762919820908, "tmdate": 1762919820908, "mdate": 1762919820908, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RobustIT, a defense framework designed to mitigate backdoor attacks during instruction tuning of vision–language models (VLMs). The method introduces two core components: Input Diversity Regularization (IDR), which applies data augmentation to increase input diversity and weaken trigger dependencies, and Anomalous Activation Regularization (AAR), which penalizes neurons that respond abnormally to backdoored inputs. The authors claim that RobustIT is attack-agnostic and adapter-centric, and evaluate it across seven backdoor attacks and multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is conceptually simple and easy to understand\n\n- The topic of backdoor defense in vision–language models is highly relevant and important, given the growing deployment of multimodal systems."}, "weaknesses": {"value": "- **Overclaim between the claimed scope and the actual experiments:**\nThere is a clear mismatch between the claimed scope and the actual experiments. The paper repeatedly positions its contribution as targeting Large Vision–Language Models (LVLMs) such as Flamingo, Otter, BLIP-2, and MiniGPT-4, but the experiments use outdated adapter-based models (e.g., frozen CLIP ViT-L/14 visual encoder). Most current LVLMs do not rely on Clip adapters. This inconsistency raises concerns about overclaiming in the **title, abstract, and introduction**. The authors should clarify the intended model scope and justify the use of these models.\n\n- **Unclear justification for Input Diversity Regularization (IDR):**\nThe rationale behind using data augmentation to suppress triggers is not convincingly explained. The paper lacks both theoretical grounding and empirical verification of how specific augmentations mitigate backdoor activation. Furthermore, the robustness of the data augmentation based defense against stronger or adaptive triggers remains untested.\n\n- **Limited validity of Anomalous Activation Regularization (AAR):**\n   - The proposed assumption—that abnormal activations reliably capture trigger behavior—appears underexplored.\n\n  - The setup (attack type, poisoning rate, affected layers) is insufficiently detailed, leaving ambiguity about the analysis conditions.\n\n  - No cross-attack or cross-model comparisons are shown to verify generality.\n\n  - If the attacker constructs semantically aligned backdoors (where poisoned and clean samples share similar semantics), the activation differences may disappear, and AAR could fail completely.\n\n  - The authors should verify whether the observed activation patterns remain consistent across datasets and model architectures.\n\n- **Incomplete baselines and comparisons:**\n\n   - Table 1 lacks results without trigger inputs, which are essential to establish a fair ASR baseline, since large models occasionally exhibit unsafe generations even without triggers.\n\n   - Figure 3 should use ASR (Attack Success Rate) directly instead of “100 – ASR,” which is non-standard and confusing.\n\n   - In Table 4, the observation that β = 0 achieves the best defense and that disabling dynamic sparsification yields the lowest ASR seems counterintuitive and warrants explanation.\n\n- **Insufficient coverage of poisoning rates and trigger scales:**\nThe evaluation is conducted only under a 1% poisoning ratio, which limits the reliability of the conclusions. Results under different poisoning rates (e.g., 0.5%, 2%, 5%) and different trigger sizes or intensities should be included to assess scalability and robustness.\n\n- **Reproducibility concerns:**\nThe authors rely on several prior attack implementations but do not provide any released or referenced code for these baselines. Given the empirical nature of this work, reproducibility is crucial for validating both attack and defense effectiveness. Without access to baseline configurations, the experimental credibility remains uncertain."}, "questions": {"value": "Overall, I find this work to have multiple unresolved issues, including the soundness of its **defense mechanism, the reasonableness of its assumptions, and the reproducibility of its experimental results**. I strongly recommend that the authors address these issues thoroughly. I will reconsider my rating after reading the authors’ response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TN8JAju3qc", "forum": "n12TKdYwHQ", "replyto": "n12TKdYwHQ", "signatures": ["ICLR.cc/2026/Conference/Submission7373/Reviewer_pTJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7373/Reviewer_pTJS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543244376, "cdate": 1761543244376, "tmdate": 1762919502439, "mdate": 1762919502439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RobustIT, a novel approach for anti-backdoor fine-tuning. RobustIT employs two regularization techniques, Input Diversity Regularization (IDR) and Anomalous Activation Regularization (AAR), to avoid the strong correlation between the backdoor trigger and the target response. Experiments demonstrate that both IDR and AAR are effective against multiple multi-modal backdoor attacks, with IDR + AAR performing the best."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is easy to follow.\n2. The methodology is simple yet effective.\n3. Ablation on key components (IDR and AAR) and hyperparameters (adapter sparsity $\\gamma$ and momentum $\\beta$) is well presented."}, "weaknesses": {"value": "1. The Equations are a little messy. Some symbols are not well-defined, e.g., $H$ in Eq. (3) and $f$ in Eq. (7). These symbols should be clearly defined when they first appear.\n2. There is no experiment on different backbone models, weakening the discussion on the generalization of RobustIT. Currently, all experiments adopt the same backbone Otter-MPT1B-RPJama-Init, which may introduce bias when assessing RobustIT. \n3. There is no clear definition of zero-shot and one-shot settings in experiments.\n4. There is a lack of two critical ablation studies on the sample size and the poisoning rate (based on **the attacker's capability**). A key feature of RobustIT is attack-agnostic. Thus, both the sample size and the poisoning rate are uncontrollable for the defender. RobustIT adopts a 1% default poisoning rate and does not mention the sample size in the main paper. Although RobustIT performs well when the attacker poisons 1% of the total samples, it is not straightforward to infer that RobustIT will still perform well when the attacker poisons a significantly larger number of samples or when the attacker utilizes more samples to do fine-tuning. We need to carefully examine the boundary of RobustIT's effectiveness. Moreover, a poisoning rate of 0% serves as a good baseline for RobustIT to assess how IDR and AAR impact fine-tuning when there is no attacker present at all."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h05hnux30V", "forum": "n12TKdYwHQ", "replyto": "n12TKdYwHQ", "signatures": ["ICLR.cc/2026/Conference/Submission7373/Reviewer_R9DC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7373/Reviewer_R9DC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977157207, "cdate": 1761977157207, "tmdate": 1762919502085, "mdate": 1762919502085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}