{"id": "uA8BcIuZc4", "number": 13272, "cdate": 1758215880688, "mdate": 1759897450002, "content": {"title": "scREBOUND: An Efficient Design of single-cell Foundation Model with Batch Representation", "abstract": "Recent advances in single-cell foundation models (scFMs) have demonstrated the promise of large-scale pretraining on single-cell RNA sequencing (scRNA-seq) data for a wide range of downstream biological tasks. However, existing models such as scGPT, UCE, scFoundation, and scMulan demand substantial computational resources for both training and inference, limiting their accessibility and practical deployment in academic settings. Furthermore, the systematic noise within different experimental batches of scRNA-seq datasets, also termed as batch effect, cannot be well removed with the masked token prediction tasks that are commonly used by these models. This significantly jeopardizes the zero-shot performance of these models on new data experiments. In this work, we present a novel and efficient design for single-cell foundation models that significantly reduces computational costs while improving the robustness of cell representation learning. Our architecture introduces a biologically-informed compression strategy to reduce input token numbers of each cell without sacrificing key transcriptomic signals. We also proposed a novel biologically-informed batch encoding strategy and introduced a multi-granular supervised contrastive loss to account for the batch effect during the model pre-training phase. We validate our design through extensive experiments across diverse datasets, demonstrating competitive performance in key zero-shot tasks including cell type annotation, batch effect removal, cross-species knowledge transfer, and missing value imputation, while achieving up to 17x reduction in inference time and 30x reduction of memory usage compared to the SOTA model scGPT. Our design makes foundational single-cell modeling more accessible and robust.", "tldr": "", "keywords": ["single-cell genomics", "foundation model", "representation learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9210eb8e6c25b718c84e13ad8b4cb061d4acf235.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an encoder only scRNA-seq foundation model, by compressing genes into clusters or meta-genes using ESM2-guided clustering .The training process follows a multi-granular contrastive loss with the addition of a min-cut regularizer and a batch encoder. \nOverall, the model shows promising performance, particularly being more efficient than other baselines (e.g. scFoundation, scGPT) on batch integration, annotation, cross-species transferand imputation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It's noteworthy that the model is quite efficient: lower inference time and memory cost vs SCGPT\n\n- The approach on the ESM2-guided clustering seems natural. It gives a biology-aware compression notion, that is quite crucial fighting for efficiency in scFMs. I find it principled and easy to reason about it.\n\n- I like how the paper handles mismatched label depths across batches through the bin-code contrastive setup.\n\n- overall, the results seem solid across integration, annotation, cross-species trasfner, and imputation\n- I find the methodolgy clearly written and easy to follow up."}, "weaknesses": {"value": "- I observe that there is a dependence on labels/ontologies. What happens with noisy or missing ontology mappings?\n- I find the sensitivity of the methodology being limited explored. Particularly, the authors propose a fixed 256 meta-gene parameter. Why 256? It'd be very useful to see the performance behavior with respect to varying sizes of clusters. How robust is the performance vs tissue-specfiic settings?"}, "questions": {"value": "- Check the Weaknesses section.\n- Moreover, the paper focuses on zero-shot settings. How would scREBOUND + finetuning compare to scGPT or scFoundation when they're finetuned as well?\n- Regarding systemic mislabels, does the neutral pool mask over them? or does the performance degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RzgJaudNm3", "forum": "uA8BcIuZc4", "replyto": "uA8BcIuZc4", "signatures": ["ICLR.cc/2026/Conference/Submission13272/Reviewer_GCU6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13272/Reviewer_GCU6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940581688, "cdate": 1761940581688, "tmdate": 1762923948034, "mdate": 1762923948034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents scREBOUND, a computationally efficient single-cell foundation model designed to address batch effects across experiments through a batch embedding network and multi-granular contrastive loss regularization. The authors introduce a protein-language-model-informed gene compression strategy that reduces gene tokens per cell while preserving biological information. scREBOUND was evaluated on four downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper demonstrates substantial improvements in computational performance, achieving up to 17x reduction in inference time and 30x reduction in memory usage compared to state-of-the-art models like scGPT.\n- The evaluation is, for the most part, well done and comprehensive. It includes most of the state-of-the-art foundation models and evaluates them on four zero-shot tasks across eight different datasets. Furthermore, the paper includes several ablations to disentangle the performance gains attributed to different complements of the model. \n- Using summary statistics of batches to embed them is an interesting idea."}, "weaknesses": {"value": "- Compressing genes to meta genes using a pLM was already introduced in [1], which isn't cited in the paper. This severely limited the paper's novelty. Furthermore, the authors should compare their approach to SATURN, especially for the cross-species transfer task. As the pLM informed meta gene module allows for the mapping of (protein-coding) genes across species to meta-genes directly.\n- The batch correction evaluation is only done on ARI, NMI, and ASW. It only measures how well the biological signal is retained in the latent representation. However, [2] introduces two axes for evaluating batch correction, batch integration, and biological conservation. In the paper, the batch integration metrics are missing. Figures 6 and 7 clearly demonstrate that batch effects persist in scREBOUND's latent representation.\n- [3] shows that reducing the size of scFMs can help improve performance on several downstream tasks.\n- The results tables present point estimates without standard errors or confidence intervals, making it difficult to assess the statistical significance of performance differences between scREBOUND and baseline methods. For example, computing ARI and NMI using Leiden clustering can yield very different values depending on the random seed.\n- All evaluations in the paper are done in a zero-shot fashion. Several tasks benefit significantly from fine-tuning. Therefore, it would be beneficial to include fine-tuning of the models in the evaluation.\n- It is unclear how the features for the batch encoder were chosen. An ablation on them would be very interesting. Furthermore, the embedding of the batch encoder could be explored more in the paper to understand what kind of representations the model learn for different batches.\n- Figures 4 and 5 suggest that most of the performance gains come from the addition of the contrastive loss rather than the batch encoder. The contrastive loss requires cell type labels during pretraining, whereas the other models did not use them. This could lead to issues for underrepresented cell types in the training data.\n- Theorem 1 feels disconnected from the rest of the paper and the empirical results.\n\n[1] Rosen, Yanay, et al. \"Toward universal cell embeddings: integrating single-cell RNA-seq datasets across species with SATURN.\" Nature Methods 21.8 (2024): 1492-1500.\n[2] Luecken, Malte D., et al. \"Benchmarking atlas-level data integration in single-cell genomics.\" Nature methods 19.1 (2022): 41-50.\n[3] Theus, Alexander, et al. \"CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer.\" bioRxiv (2024): 2024-11."}, "questions": {"value": "- How does scREBOUND compare to SATURN. How different is the meta-gene module proposed in SATURN from scREBOUND's?\n- How does scREBOUND perform on batch integration metrics (kBET, AWS (batch-label), ...) compared to other methods?\n- How does Theorem 1 relate to the empirical findings of the paper and compressing genes using a pML?\n- How were the features for the batch encoder selected? How does the performance change when we change the set of selected features? (What kind of representation of batches did the batch encoder learn?)\n- How was the cell type overlap between the 1 million randomly selected cells and the cells in the test set for zero-shot cell type annotation? Shouldn't several cell types not be represented in the randomly selected training set?\n- scGPTs' performance seems to improve with a higher percentage of masked genes for imputation (Table 6). Is there a potential reason for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NMB7to9SUH", "forum": "uA8BcIuZc4", "replyto": "uA8BcIuZc4", "signatures": ["ICLR.cc/2026/Conference/Submission13272/Reviewer_UZvz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13272/Reviewer_UZvz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956907461, "cdate": 1761956907461, "tmdate": 1762923947642, "mdate": 1762923947642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents scREBOUND, a computationally efficient foundation model for single-cell RNA-seq data that directly tackles two key challenges in current single-cell foundation models (scFMs): (1) high computational cost, and (2) poor handling of batch effects. The authors propose (a) a biologically informed gene compression module (protein language model–guided mincut-based grouping of genes into 256 meta-genes), (b) a batch encoder capturing batch-specific features, and (c) a multi-granular contrastive loss leveraging hierarchical cell ontologies to align cell types across batches of varying label granularity. scREBOUND is pretrained with masked token prediction and evaluated on multiple downstream zero-shot tasks: batch effect removal, cell type annotation, cross-species knowledge transfer, and missing value imputation. Results show that scREBOUND achieves competitive or superior accuracy compared to state-of-the-art single-cell foundation models (scGPT, UCE, scMulan, scFoundation), while reducing inference runtime and memory usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper successfully tackles the computational bottleneck of scFMs. scREBOUND achieves significant advantages in runtime and GPU memory consumption.\n\n-  The use of a protein-language-model-informed gene compression strategy is novel. By reducing the gene tokens to 256 meta-genes, the model reduces the number of tokens processed by the transformer while retaining key biological features. Furthermore, the paper provides a theoretical analysis showing that the mincut regularized compression strategy upper-bounds the mutual information loss after feature compression.\n\n-  The combined approach of the batch embedding network and the multi-granular contrastive loss effectively addresses the systematic noise inherent in scRNA-seq data.\n\n- scREBOUND consistently performs well across a diverse suite of zero-shot tasks. It demonstrated top performance in batch effect removal across all eight test datasets."}, "weaknesses": {"value": "-All the main results are framed in a purely zero-shot setting. In practice, users often do a light finetuning or task-adaptive pretraining step, and several influential models in this space (such as Geneformer) are typically reported with some form of adaptation. Because the paper does not compare against Geneformer or against scGPT/scFoundation in a finetuned regime, it’s unclear whether scREBOUND would still outperform strong baselines once they are allowed to adapt to the target dataset.\n\n- A second, more structural point: the gene–meta-gene compression is built from protein-language-model embeddings of genes. That’s elegant, but protein-embedding similarity does not necessarily reflect transcriptional or regulatory co-function — e.g. co-expression, pathway co-membership, TF–target relationships, ligand–receptor pairs, or chromatin-linked interactions may never be close in protein-embedding space. So the current graph might be biased toward sequence/structure similarity rather than cellular co-usage. This clustering stage could optionally ingest an external gene–gene interaction network (STRING, BioGRID, Reactome functional edges, or even dataset-specific co-expression graphs)."}, "questions": {"value": "- The authors adopt Fourier encoding to transform continuous expression values into embeddings, citing poor MLP performance on long-tailed gene expression distributions. Could the authors briefly elaborate on why Fourier encoding was ultimately preferred, given that scREBOUND’s design emphasizes computational efficiency?\n\n-  The compression module reduces all input genes to 256 meta-genes. Was this specific number chosen through empirical tuning (e.g., grid search on performance vs. efficiency), or is there a theoretical motivation related to the information content or redundancy in scRNA-seq data?\n\n-  The meta-gene construction relies on a gene graph induced from protein LM embeddings. But protein similarity does not necessarily capture transcriptional co-regulation, TF–target pairs, or pathway-level co-usage. Could your method accept an external gene–gene interaction network (e.g. STRING/BioGRID/co-expression from the same atlas) as an additional adjacency or regularizer in the mincut step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G6wR3zupvq", "forum": "uA8BcIuZc4", "replyto": "uA8BcIuZc4", "signatures": ["ICLR.cc/2026/Conference/Submission13272/Reviewer_oADU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13272/Reviewer_oADU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995145639, "cdate": 1761995145639, "tmdate": 1762923947265, "mdate": 1762923947265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}