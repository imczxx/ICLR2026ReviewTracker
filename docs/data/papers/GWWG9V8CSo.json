{"id": "GWWG9V8CSo", "number": 7433, "cdate": 1758021578508, "mdate": 1763298487225, "content": {"title": "Breaking Rank Bottlenecks in Knowledge Graph Embeddings", "abstract": "Many knowledge graph embedding (KGE) models for link prediction use powerful encoders. However, they often rely on a simple hidden vector-matrix multiplication to score subject-relation queries against candidate object entities. When the number of entities is larger than the model's embedding dimension, which is often the case in practice by several orders of magnitude, we have a linear output layer with a rank bottleneck. Such bottlenecked layers limit model expressivity.\nWe investigate both theoretically and empirically how rank bottlenecks affect KGEs. We find that, by limiting the set of feasible predictions, rank bottlenecks hurt the ranking accuracy and distribution fidelity of scores. Inspired by the language modelling literature, we propose KGE-MoS, a mixture-based output layer to break rank bottlenecks in many KGEs. Our experiments show that KGE-MoS improves ranking performance of KGE models on large-scale datasets at a low parameter cost.", "tldr": "Knowledge Graph Embeddings models for link prediction often suffer from “rank bottlenecks” in their output layers resulting in limited prediction accuracy and miscalibrated probabilities. We address this by proposing a mixture-based output layer.", "keywords": ["Knowledge Graph Embeddings", "Rank Bottlenecks", "Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b57199ba53115ba0673cbc4e773f622846c437cd.pdf", "supplementary_material": "/attachment/ed94395ef22269cf0a51fd157f9aa5adbb6730a5.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates rank bottlenecks in knowledge graph embedding (KGE) models caused by the common low‑rank linear output layer. The authors theoretically show that such bottlenecks limit expressivity for ranking, sign, and distributional reconstruction tasks. They introduce KGE‑MoS, a mixture‑of‑softmaxes output layer that nonlinearly combines multiple softmax components to break low‑rank constraints at low parameter cost. Experiments on multiple large‑scale datasets demonstrate that KGE‑MoS consistently improves ranking accuracy and probabilistic fit compared with standard KGEs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors analyze the rank bottleneck problem in knowledge graph embedding (KGE) models and provide comprehensive theoretical constraints and inexpressibility proofs under three common tasks (DR, SR, and RR).\n- A lightweight and general method, KGE‑MOS, is proposed by replacing the standard linear output layer with a mixture‑of‑softmaxes.\n- The proposed method is evaluated on four datasets (FB15k‑237, Hetionet, ogbl‑biokg, and openbiolink) and achieves performance improvements on large knowledge graphs."}, "weaknesses": {"value": "- The proposed method may not scale effectively to extremely large graphs (e.g., tens of millions of entities), where increasing embedding dimensions remains a common approach to enhance expressivity.\n- The evaluation currently involves a limited set of relation‑rich datasets; it would be valuable to include additional experiments on large open KGs such as Wikidata.\n- The performance degradation on smaller datasets indicates potential instability, which requires a clearer explanation.\n- The training time increases due to multiple softmax computations."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u5FiSPq8y6", "forum": "GWWG9V8CSo", "replyto": "GWWG9V8CSo", "signatures": ["ICLR.cc/2026/Conference/Submission7433/Reviewer_rUSu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7433/Reviewer_rUSu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620424953, "cdate": 1761620424953, "tmdate": 1762919554237, "mdate": 1762919554237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Argues that factorization-based KGE models produce a low-rank scoring matrix so that its representational power is limited by a rank bottleneck. Then transforms entity-relation embeddings uses multiple different non-linear transformations and subsequently ensembles; this increases the rank of the scoring matrix at low additional cost. Reports on a small experimental study using multiple KGE models.\n\nI recommend to reject this paper because it is oblivious to key related work (W1), it partially does not study the actually relevant problems (W2), and it's experimental study is not convincing (W3)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Hard to say, as key related work is not represented."}, "weaknesses": {"value": "W1. Not novel and key related work missing. The rank bottleneck has been studied in more detail, using tighter bounds, and applied to more models in [A]. It also proposes an ensemble approach. This paper is neither cited nor discussed.\n\nW2. Does not use the right problems. The paper asks whether a KGE model can express every ranking. That's not relevant, however, if a KGE model can express every ranking, but only whether it can rank positives higher than negatives: the relative ranking of, say, two positives does not matter.\n\nW3. Experimental results not convincing. Improvements of FB15k-237 are tiny and results (in terms of MRR) fall behind what has been reported for the baseline models in other papers. This reduces trust in the other datasets, which haven't been used that much in related work as far as I know. Datasets such as Wikidata5M, which have been used, are missing.\n\nWang et al., On Multi-Relational Link Prediction with Bilinear Models, AAAI, 2018"}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OdCgU7D0nz", "forum": "GWWG9V8CSo", "replyto": "GWWG9V8CSo", "signatures": ["ICLR.cc/2026/Conference/Submission7433/Reviewer_g8md"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7433/Reviewer_g8md"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834867242, "cdate": 1761834867242, "tmdate": 1762919553880, "mdate": 1762919553880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A drop-in output layer for select Knowledge Graph Embedding models to increase model expressivity and thus predictive power by addressing \"rank bottlenecks\". The output layer consists in a mixture of softmaxes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Problem novelty: rank bottleneck problem not studied yet in KGE literature, to the best of my knowledge.\n- Paper is well written, and it includes comprehensive material.\n- Contribution is an original adoption of methods from language modelling literature.\n- Evaluation: good mixture of benchmark datasets."}, "weaknesses": {"value": "- The rank bottleneck problem could use a more in-depth introduction, to broaden up the audience.\n- Contribution limited to adopting a MoS layer to existing KGE architectures.\n- KGE-MOS does not support translation-based KGE methods (e.g. RotatE).\n- Evaluation: limited impact of \\*-MoS on predictive power. results at par with baselines.\n- Experimental results presented in the paper does not justify the adoption of KGE-MOS in practice due to computational overhead (e.g. 2.75 slower to train)"}, "questions": {"value": "- Figure 1: Why adding a relation type as target object? Could you please elaborate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MyP1MNNV8s", "forum": "GWWG9V8CSo", "replyto": "GWWG9V8CSo", "signatures": ["ICLR.cc/2026/Conference/Submission7433/Reviewer_EZ1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7433/Reviewer_EZ1a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924855348, "cdate": 1761924855348, "tmdate": 1762919553146, "mdate": 1762919553146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks into a limitation in tensor-multiplicative scoring functions used in KGE models, which the authors describe as a kind of \"rank bottleneck\" that prevents models from properly capturing complex relationships, especially in large graphs with high connectivity. To address this, they introduce KGE-MOS, which modifies the output layer by combining multiple softmax outputs. The idea is that this mixture can produce a more expressive scoring function and help models rank entities more accurately, particularly on large-scale knowledge graphs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper’s main strength is its theoretical analysis of the bottleneck problem. The authors do a solid job of identifying and characterizing this limitation and even relate it to graph connectivity, which is interesting. The experiments are also convincing overall that KGE-MOS seems to improve results in the right settings (i.e., large graphs) without blowing up the parameter count. It’s a clear and well-motivated piece of work."}, "weaknesses": {"value": "I do have a few concerns about the experimental part.\n\nW1. The baselines are reasonable (DISTMULT, ConvE, etc.), but they’re a bit outdated. There are more recent KGE architectures — some Transformer- or GNN-based — that also use similar scoring layers. It would strengthen the argument a lot if the authors could show that their method helps even those stronger baselines, not just the classic ones.\n\nW2. The ablation on the number of mixtures, $K$, is only done on DISTMULT and on a single dataset (ogbl-biokg). That’s informative, but it’s hard to know how general the trend is. Running at least one more model or dataset would make the case much stronger."}, "questions": {"value": "Q1. The approach reminds me a bit of ensemble-based KGE methods (e.g., [1]). It might be worth clarifying how KGE-MOS is different from those, since both seem to combine multiple outputs to improve expressivity.\n\nQ2. While the paper has a nice theoretical discussion of the rank bottleneck itself, the expressivity of the proposed fix is mostly justified through intuition and experiments. Have the authors considered analyzing the expressivity of KGE-MOS more formally? It would help complete the theoretical story."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N4wmJd5T1p", "forum": "GWWG9V8CSo", "replyto": "GWWG9V8CSo", "signatures": ["ICLR.cc/2026/Conference/Submission7433/Reviewer_Wgvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7433/Reviewer_Wgvc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030873072, "cdate": 1762030873072, "tmdate": 1762919552163, "mdate": 1762919552163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}