{"id": "HAiP0SjlAF", "number": 5419, "cdate": 1757908120690, "mdate": 1759897976519, "content": {"title": "Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties", "abstract": "GUI agents are designed to autonomously execute diverse device-control tasks by interpreting and interacting with device screens.\nDespite notable advancements, their resilience in real-world scenarios—where screen content may be\npartially manipulated by untrustworthy third parties—remains largely unexplored.\nIn this work, we present the first systematic investigation into the vulnerabilities of mobile GUI agents.\nWe introduce a scalable attack simulation framework named AgentHazard,\nwhich enables flexible and targeted modifications of screen content within existing applications.\nLeveraging this framework, we develop a comprehensive benchmark suite comprising both a dynamic task execution environment\nand a static dataset of state-rule pairs.\nThe dynamic environment encompasses 122 reproducible tasks in an emulator with various types of hazardous UI content,\nwhile the static dataset consists of over 3,000 attack scenarios constructed from screenshots collected from a wide range of commercial apps.\nImportantly, our content modifications are designed to be feasible for unprivileged third parties.\nWe perform experiments on 6 widely-used mobile GUI agents and 5 common backbone models using our benchmark.\nOur findings reveal that all examined agents are significantly influenced by misleading third-party contents\n(with an average misleading rate of 42.1\\% and 40.7\\% in dynamic and static environments, respectively).\nWe also find that the vulnerabilities are closely linked to the perception modalities and backbone LLMs.", "tldr": "This study presents the first systematic investigation of mobile GUI agents' vulnerabilities to on-screen content manipulated by untrustworthy third parties.", "keywords": ["Mobile GUI Agents", "UI Security", "Adversarial Attacks", "AgentHazard", "Empirical Evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/190144783e62319abd423372e23e913f6aee27c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies robustness of mobile GUI agents against misleading content originating from unprivileged third parties and introduces AgentHazard, a configurable attack-simulation framework that patches adversarial content into Android apps at run time. Built on this, the authors assemble a benchmark with a dynamic interactive environment and a static dataset containing benign and adversarial variants. Two metrics are used: Success Rate Drop (∆SR) and Misleading Rate (MR), where MR counts whether an agent executes a predefined misled action tied to injected content. Experiments span six representative mobile agents (e.g., M3A, T3A, AutoDroid, AriaUI, UI-TARS-1.5) and five LLM backbones across text/vision/multimodal settings. Results show substantial vulnerability (average MR ≈ 42.1% dynamic; 40.7% static), with multi-modal inputs generally more susceptible, UI-TARS-1.5 relatively robust, and Claude-4-sonnet outperforming GPT-4o variants on the static dataset. A small adversarial training study with Qwen2.5-VL suggests modest robustness gains but no fundamental fix."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem framing around unprivileged third-party content and the resulting need to test agents outside “clean” environments; Figure 1 offers a compelling case of destructive behavior induced by short misleading strings. \n\nAgentHazard is described as a practical, scalable simulator that modifies both screenshots and the UI element tree in real time via an Android Accessibility-based tool and a cooperating Python attack module; the configuration language (Appendix D, Listing 1) is concrete and appears easy to author.\n\nUseful findings for the community: multi-modal inputs increase susceptibility on average; robustness depends strongly on both model family and GUI-specific training; simple popup-style attacks are much easier to detect than natively rendered modifications (Table 5)."}, "weaknesses": {"value": "Although the paper claims feasibility for unprivileged third parties, the simulator “patches adversarial content both on the screen and the structured UI element tree in real time.” Real third parties can plausibly control in-app content regions (e.g., posts, product titles), but they cannot modify another app’s UI tree; the paper should more clearly separate (i) realistic adversary control over pixels/strings inside third-party-controlled regions from (ii) the tool’s additional modifications to the UI tree used to keep text-based perception consistent. As written, the phrase “as native GUI elements” risks overstating real-world feasibility.\n\nStep 4 (Attack Rule Generation) relies on LLMs to produce adversarial strings given prompts P, but the exact model, decoding parameters, and filtering are not provided in the main text; inter-annotator criteria for “feasibility” (Step 2) and quality control for rules Rsuccess/Rattack are also not quantified (e.g., agreement rates).\n\nStealthiness study (Table 5) depends on a single LLM-based detector and reports a relatively high 10.3% “No Attack” detection rate without confidence intervals or sample counts; the detector may not be a reliable surrogate.\n\nIn addition, I have some concerns on the novelty of this paper, I acknowledge it may be interested by someone working on the GUI agents (the practical contribution is more than the technical contribution)."}, "questions": {"value": "You state that “content modifications are designed to be feasible for unprivileged third parties” while AgentHazard also modifies the UI element tree. In real apps, third parties cannot alter another app’s UI tree. Please clarify precisely which parts of your injected content are intended to model realistic third-party control and which are simulator conveniences to keep text-only perception in sync. Can you report results where only on-screen pixels are modified but the UI tree is left untouched?\n\nThe paper positions itself as a first systematic investigation for mobile agents. Could you delineate, in the main text, how your dynamic and static components differ from prior mobile-agent robustness evaluations you cite (e.g., MobileSafetyBench) to avoid ambiguity in the novelty claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QdxLrLhPnj", "forum": "HAiP0SjlAF", "replyto": "HAiP0SjlAF", "signatures": ["ICLR.cc/2026/Conference/Submission5419/Reviewer_UHX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5419/Reviewer_UHX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745011693, "cdate": 1761745011693, "tmdate": 1762918051560, "mdate": 1762918051560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerability of mobile GUI agents to misleading content from untrusted third-party sources. The authors develop AgentHazard, a framework for injecting adversarial content into Android applications without requiring elevated privileges, and construct a benchmark comprising 122 dynamic tasks and over 3,000 static scenarios. Through evaluation of six mobile GUI agents and five backbone LLMs, they demonstrate that current agents exhibit significant vulnerability, with average misleading rates exceeding 40% in both dynamic and static settings. While the paper addresses a practically important problem and provides a useful evaluation framework, it suffers from limited scope in attack modeling, insufficient depth in analyzing root causes, and superficial treatment of defense mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The threat model is realistic and well-motivated, focusing on unprivileged attackers controlling legitimate third-party content rather than privileged access.\n2. The dual evaluation, dynamic interaction and static state-rule analysis, offers complementary insights. Broad testing across six agents and five LLMs, including multimodal and text-only models, enhances generalizability.\n3. Several empirical findings are valuable: visual modalities show higher vulnerability than text, different LLMs exhibit varying sensitivity to attack types."}, "weaknesses": {"value": "1. The study only tests two attack types, wrong clicks and early exits, which miss many real threats like phishing, permission abuse, or data leaks. Saying it’s “hard to measure” complex attacks isn’t a good excuse. The focus on single misleading items makes the setup too clean and unrealistic.\n2. The paper shows what fails but not why. It never explores why visual models are weaker, maybe attention or data bias? Without tools like attention maps or gradient analysis, the study stays at a surface level.\n3. The defense experiment is poorly designed and reuses data from failed samples. It also ignores better-known defenses like careful prompting, confidence checks, or human review.\n4. Using GPT-4o to both make and judge attacks causes bias. It may just favor its own style, not real-world tricks."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "An identically titled paper has already been published in the EdgeFM '25 Proceedings. Here is the official publication record: https://doi.org/10.1145/3737902.3768354"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q0SYCbGSFW", "forum": "HAiP0SjlAF", "replyto": "HAiP0SjlAF", "signatures": ["ICLR.cc/2026/Conference/Submission5419/Reviewer_F9Tf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5419/Reviewer_F9Tf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962374107, "cdate": 1761962374107, "tmdate": 1762918051188, "mdate": 1762918051188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides an investigation into the vulnerability of mobile GUI agents. The main contribution is a benchmark suite including both dynamic emulators and static screenshots. The benchmark evaluates how much GUI agents are affected by content deliberately crafted to deceive them by untrustworthy third-party sources. Test results reveal that all widely used mobile GUI agents are vulnerable to misleading third-party content, indicating that their robustness needs improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper highlights the importance of the ability of defense against third-party adversarial attacks for GUI agents. And it presents a fine-grained benchmark suite to evaluate this.\n- This paper tests several current GUI agents on the benchmark. The significant performance drop highlights the safety problem of current agents.\n- This paper provides several findings by studying the impact of multimodal, LLM-backbone, and GUI-specific training on the results."}, "weaknesses": {"value": "- The Evaluation is limited. The paper reveals that the GUI-specific training has an impact on the success rate and the misleading rate. However, only GUI-TARS is tested, which affects the credibility of the results. End-to-end trained models like Mobile-Agent-v3[1], UI-Genie[2] may have different results on the benchmark.\n\n-  The authors argue that, unlike existing attacks on web-based agents, this work focuses on mobile GUI agents, which have higher security requirements and stricter control over user privacy, making existing web-based attack strategies difficult to execute. However, apart from introducing an additional Android application, the proposed method still relies on modifying the screen and XML, and its specific differences from existing attacks are not clear. \n\nNote: I will raise my score if my concerns are addressed.\n\n[1] Ye, Jiabo, et al. \"Mobile-agent-v3: Fundamental agents for GUI automation.\" \n\n[2] Xiao, Han, et al. \"UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents.\""}, "questions": {"value": "From Table 1, we can observe that the SR of UI-TARS only drops a little with misleading information. However, the Qwen-2.5-VL-7B-Instruct trained with benight data demonstrates a significant performance drop in Table 3. What causes this misalignment? What is the effect of the adversarial training on models that are specially fine-tuned on GUI Agent tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RNjfnHX67I", "forum": "HAiP0SjlAF", "replyto": "HAiP0SjlAF", "signatures": ["ICLR.cc/2026/Conference/Submission5419/Reviewer_Nvyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5419/Reviewer_Nvyy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968360665, "cdate": 1761968360665, "tmdate": 1762918050822, "mdate": 1762918050822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Hijacking JARVIS, a systematic benchmark for evaluating the robustness of mobile GUI agents against misleading content from unprivileged third parties. The paper proposes AgentHazard, a scalable Android attack simulation framework capable of injecting adversarial text and UI elements without system privileges, and constructs both a dynamic environment and a static dataset. Experiments on several representative agents and five backbone LLMs show that some models are vulnerable. However, the UI-TARS-1.5 agent, which is specifically fine-tuned for downstream tasks involving GUI-related operations, exhibits more robust behavior when confronted with misleading information"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and clearly written, making it easy to follow.\n2. The combination of a dynamic interactive environment and a large-scale static dataset provides both high ecological validity and reproducibility. This will benefit future research on security and robustness in GUI agents.\n3. The paper reveals key trends—particularly that multimodal perception increases susceptibility to misleading content, while domain-specific GUI training enhances robustness—providing actionable insights for future model design."}, "weaknesses": {"value": "1. The most critical question for me is: what is the fundamental difference between third-party attacks and traditional pop-up–based attacks? From the standpoint of agent security and defense, third-party attacks can essentially be viewed as a special case of pop-up attacks. Since prior work has already demonstrated that agents are vulnerable to pop-up–based adversarial content, the novelty of verifying vulnerability to third-party attacks appears relatively limited. In practice, when designing agents, we must consider and defend against high-privilege pop-up attacks, which are generally considered a more powerful and comprehensive threat model. In this case,  is it still necessary to investigate defenses against third-party attacks?\n2. Only one GUI-specialized model (UI-TARS-1.5) is tested. Including more domain-finetuned agents would strengthen the conclusions and provide richer insights into effective robustness strategies.\n3. AgentHazard currently modifies only textual and UI-tree elements; it does not support manipulation of visual assets (e.g., images, icons) or multi-step dynamic attacks, leaving important real-world attack vectors unexplored."}, "questions": {"value": "Disregarding the issue of third-party permissions, what is the fundamental distinction between third-party attacks and pop-up attacks, and which distinct vulnerabilities of the model does this difference validate?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XxtEFfSGBw", "forum": "HAiP0SjlAF", "replyto": "HAiP0SjlAF", "signatures": ["ICLR.cc/2026/Conference/Submission5419/Reviewer_4CEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5419/Reviewer_4CEc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067879027, "cdate": 1762067879027, "tmdate": 1762918050403, "mdate": 1762918050403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}