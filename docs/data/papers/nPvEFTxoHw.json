{"id": "nPvEFTxoHw", "number": 23509, "cdate": 1758344739305, "mdate": 1759896811149, "content": {"title": "VizAgentBench: Benchmarking Multimodal Agent Reasoning on Coordinated Multi-View Visual Analytics Tasks", "abstract": "Multimodal Large Language Models (MLLMs) can now act as full-fledged desktop agents, yet their visual reasoning skills remain largely evaluated on single, static charts. Real decision making, however, happens in dashboards that combine multiple coordinated views (MCVs) and rely on rich interactions such as brushing, filtering, and drilling down. We introduce VizAgentBench, the first benchmark that challenges agents to perceive screenshots of a live MCV dashboard, issue declarative interaction commands, and answer analytical questions whose solutions may be hidden behind dynamic tooltips or axis changes. VizAgentBench is constructed by (1) surveying 14 visualization research papers to derive a design space of chart-and-interaction templates; (2) mining 10 public Kaggle datasets across finance, healthcare, sports, and socio-economics; and (3) generating 192 dashboards paired with same number of question–answer tasks using a large language model plus manual validation by a group of graduate students in data science. On our benchmark, state-of-the-art LLM agents achieve only 40% accuracy, revealing substantial headroom. We release the dashboards, data, and an open-source API that separates perception from action, lowering the barrier to agent research on interactive visualization.", "tldr": "VizAgentBench is a new visual question answering benchmark for LLM agents that evaluates their ability to perceive interactive visualizations, perform visualization interactions, and answer complex analytical questions.", "keywords": ["vision question answering", "multimodality"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc29bffb58d90484ba22643fc6a23fef73c43d37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VizAgentBench, the first benchmark designed to evaluate multimodal large language model (MLLM) agents on interactive, multi-view data visualization tasks. It spans 10 Kaggle datasets across diverse domains, containing 192 datasets. Evaluation across 4 agent frameworks and 7 MLLMs shows that current systems achieve only about 40% accuracy, indicating significant room for improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Problem Setting. Moves beyond static chart understanding to dynamic, interactive reasoning, aligning better with real-world analytics workflows.\n2. Clear Separation of Perception and Action. The API cleanly decouples screenshot understanding from action execution, promoting modular evaluation of perception vs. planning.\n3. Reproducibility and Openness. Full release of data and code lowers the entry barrier for future agent research."}, "weaknesses": {"value": "1. Evaluation Ambiguity. It’s not entirely clear how success metrics is calculated. Also, it would be helpful to show the horizon of the task, and human performance to clarify the gap.\n2. Lack of in-depth analysis. There is not a clear analysis of different error modes.\n3. Extensibility. It is unclear whether the framework could support tasks beyond QA with more intensive interaction like data update or chart creation."}, "questions": {"value": "1. Can you provide a detailed analysis of error modes and horizons of tasks?\n2. Was a human baseline tested to contextualize the ~40% model accuracy?\n3. Could the framework be extended to sequential reasoning or time-evolving dashboards (e.g., live updates, streaming data)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rsg9MyVfMZ", "forum": "nPvEFTxoHw", "replyto": "nPvEFTxoHw", "signatures": ["ICLR.cc/2026/Conference/Submission23509/Reviewer_aEAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23509/Reviewer_aEAe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559345643, "cdate": 1761559345643, "tmdate": 1762942691224, "mdate": 1762942691224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VizAgentBench, a benchmark for interactive dashboard understanding where agents must act on coordinated multi‑view visualizations to answer questions. It comprises 25 templates over five chart types, yielding 192 tasks from 10 datasets, with a dual mouse/command interface that separates perception from action. Across 4 frameworks and 7 MLLMs, the best Pass@1 is ~44%, revealing substantial difficulty and recurrent errors in perception, memory, and planning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper evaluates various agent frameworks and analyzes the impact of the framework design on performance.\n2. Multiple chart types and MCV metrics are incorporated to ensure a more comprehensive and realistic evaluation."}, "weaknesses": {"value": "1. The contributions of the paper seem limited - the paper does not propose clear new insights and just applies a variety of MLLMs on the MCV tasks. The benchmark doesn't seem to offer any insights for improvement (better agent framework or model improvement).\n\n2. There is a lack of human baselines. It would be helpful to see how well humans perform on these problems for comparison.\n\n3. There may be bias in question generation. Since the questions are produced by GPT-4.1, they are likely to be more aligned with the knowledge and preferences of OpenAI models.\n\n4. The benchmark has not been released, and the paper provides no example questions or prompts for reference. I am also concerned about the quality of the tasks, as the example in Figure 3 is poorly presented - part of the legend overlaps with the pie chart, and the text contains incorrect capitalization."}, "questions": {"value": "1. Can you provide a more in-depth analysis or case study on the impact of different agent frameworks? For example, why does OpenManus outperform ReAct and AutoGen?\n\n2. Are 10 interaction turns sufficient to solve the problems? What happens if more turns are allowed?\n\n3. In the final results, are both “Mouse Interaction” and “Command” actions available to agents? If so, could you show the agents’ action preferences? Would the results improve if the agent were restricted to using only one type of action?\n\n4. It's helpful to see how reasoning/thinking improves the agent performance. Can you do experiments/analysis around it?\n\n5. Can you provide the breakdown of results for 2-view, 3-view, and 4-view MCVs? Do agents struggle more as the number of views increases?\n\n6. What is the token consumption for running the evaluation?\n\n7. Figure 3 appears quite unprofessional, as the text in the screenshot is cut off at the edge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rqQxTsrIGp", "forum": "nPvEFTxoHw", "replyto": "nPvEFTxoHw", "signatures": ["ICLR.cc/2026/Conference/Submission23509/Reviewer_wTgB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23509/Reviewer_wTgB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591250542, "cdate": 1761591250542, "tmdate": 1762942690906, "mdate": 1762942690906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes by introducing VizAgentBench, a benchmark for evaluating multimodal agents on interactive, multi-view visual analytics (MCVs), which addresses the limitation of existing static, single-view benchmarks, and by constructing 192 dashboards (with QA tasks) and evaluating 7 SOTA MLLMs across 4 frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: VizAgentBench targets the intersection of multimodal agents and interactive MCVs, an area ignored by prior GUI benchmarks and chart QA datasets. The coordination logic ensures the benchmark reflects real-world analytical workflows.\n2. Quality: The benchmark is diverse (10 datasets across finance/healthcare/sports, 25 templates, 192 tasks) and rigorous (manual QA validation, controlled experiments across models/frameworks). \n3. Clarity: The paper effectively communicates complex concepts (e.g., MCV coordination, agent-environment interaction) through simple examples and visual aids. The experimental results are transparent, with Table 6 providing granular data on model-framework pairs to enable comparison.\n4. Significance: The low accuracy of top models (~44% Pass@1) demonstrates a critical gap in current agents, while the case study identifies specific failures (visual recognition, context memory) that prioritize future research. This work will likely become a standard benchmark for evaluating agent performance on visual analytics."}, "weaknesses": {"value": "1. After submission, the paper does not provide any code or data, whether as supplementary materials or via an anonymous link. This impedes the early validation of its key components, a critical aspect of the essential requirements for a benchmark-focused paper.\n2. While the 25 templates are \"manually authored\", they lack validation against real-world dashboards. For example, the paper fails to confirm whether the distribution of templates (such as 4-view MCVs, as shown in Table 4) aligns with the frequency of view counts in industry tools like Tableau or Power BI. This casts doubt on whether the agent performance measured by VizAgentBench can be generalized to real-world dashboards.\n3. The case study, e.g. in Section 5.3, outlines specific failures but does not analyze the distribution of these failure patterns, avoiding deeper exploration and research.\n4. The paper does not report human accuracy on VizAgentBench tasks. Without this baseline, it is impossible to judge whether the ~44% accuracy of top models stems from a fundamental limitation of current AI or an excessively complex benchmark, one that even humans would struggle with."}, "questions": {"value": "Address the issues in the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rxfnzaRwH4", "forum": "nPvEFTxoHw", "replyto": "nPvEFTxoHw", "signatures": ["ICLR.cc/2026/Conference/Submission23509/Reviewer_LUjd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23509/Reviewer_LUjd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979190426, "cdate": 1761979190426, "tmdate": 1762942690635, "mdate": 1762942690635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VizAgentBench, a benchmark for evaluating multimodal language model (MLLM) agents on interactive, multi-view visualization reasoning. Unlike previous static chart QA datasets, VizAgentBench requires agents to manipulate coordinated dashboards through actions like filtering and brushing to answer analytical questions. Built from 10 real-world datasets and 25 visualization templates, it includes 192 dashboard–question pairs. Experiments on seven MLLMs and four agent frameworks reveal that even leading systems (e.g., GPT-5) achieve only around 44% on Pass@1, highlighting major challenges in perception, memory, and reasoning for visual analytics agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novelty: Introduces the first benchmark explicitly targeting interactive, multi-view visual reasoning by agents.\n\n- Technical soundness: Provides a clear generation pipeline, coordination logic, and reproducible environment.\n\n- Comprehensive evaluation: Compares multiple agent frameworks and leading MLLMs.\n\n- Insightful findings: Identifies key failure modes such as lack of visual memory and unstable multi-turn reasoning.\n\n- Open-source impact: The proposed API design (separating perception from action) can accelerate follow-up research."}, "weaknesses": {"value": "- Scale and coverage: Only 192 dashboards and questions may not ensure sufficient diversity or statistical robustness.\n\n- LLM generation bias: Since GPT-4.1 generated all questions, potential bias favoring OpenAI models should be addressed through cross-model verification or ablation.\n\n- Limited quantitative validation: Manual inspection is described but lacks measurable inter-rater reliability or coverage statistics.\n\n- Evaluation metrics: Accuracy and Pass@k alone may not fully capture interaction efficiency or reasoning depth. Fine-grained metrics such as per-category pass rates, reasoning chain length, or time cost would improve analysis.\n\n- Interaction settings: The “10-turn limit” seems arbitrary without evidence that it fits typical reasoning complexity.\n\n- Incomplete error analysis: The qualitative case study could be expanded with statistical analysis of all failed cases."}, "questions": {"value": "- How do the authors ensure that GPT-4.1’s question generation does not systematically advantage OpenAI models during evaluation?\n\n- Can agents combine the two interaction modalities (mouse and command) within a single task?\n\n- How was the 10-turn interaction limit chosen, and how sensitive are results to this parameter?\n\n- Could additional efficiency metrics (e.g., average turns to success) be reported?\n\n- Would a “visual memory” module improve performance in multi-hop reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dNGmKgNEun", "forum": "nPvEFTxoHw", "replyto": "nPvEFTxoHw", "signatures": ["ICLR.cc/2026/Conference/Submission23509/Reviewer_WDAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23509/Reviewer_WDAr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23509/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983153461, "cdate": 1761983153461, "tmdate": 1762942690362, "mdate": 1762942690362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}