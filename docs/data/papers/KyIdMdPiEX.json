{"id": "KyIdMdPiEX", "number": 15668, "cdate": 1758253703180, "mdate": 1759897290116, "content": {"title": "Stance Elicitation as a Black-Box Framework for Auditing LLM Alignment", "abstract": "Measuring bias and alignment in large language models (LLMs) has typically\nrelied on external datasets, subjective coding, or costly benchmarks. We\nintroduce a simple, model-intrinsic framework based on stance elicitation\nand morphisms into semantic spaces, yielding a black-box representation\nof an LLM’s semantic space. By conditioning policy statements on entity\ndescriptors, we generate stance vectors that contrast generic labels\nwith explicit rules and programmatic specifications, thereby exposing\npolitical biases, systematic gaps between stated and revealed alignment,\nand demographic stereotyping in state-of-the-art LLMs. The framework\nis lightweight, scalable, and model-agnostic, offering a reproducible\nfoundation for alignment auditing that is free of subjective criteria\nand broadly applicable to political and social analysis.", "tldr": "Stance elicitation lets us construct semantic spaces for LLMs, making biases, alignment inconsistencies, and stereotyping explicit.", "keywords": ["large language models", "alignment", "bias", "stance elicitation", "semantic spaces", "black-box analysis", "interpretability", "evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a22e684bdc4ff013e849dfdfad6ce5cc4fac3677.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a black-box framework called stance elicitation to audit how large language models (LLMs) are politically and morally aligned. Instead of looking inside the model, the method asks structured questions like “Would entity X agree with policy Y?” to map each model’s semantic space into measurable stances. The authors test several major LLMs across political and moral statements. The study also identifies systematic biases and gaps between stated and revealed alignment, plus demographic stereotyping."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The framework is simple, intuitive and works as a black-box audit.\n\n2. It’s transparent and well-validated across multiple models.\n\n3. The work bridges AI auditing and social science in a fresh, useful way."}, "weaknesses": {"value": "1. The method mostly observes bias but doesn’t really explain or fix it.\n\n2. There’s still a chance that the prompts themselves introduce bias — the paper doesn’t fully control for that.\n\n3. The evaluation focuses on politics in Western contexts, which might limit generalization.\n\n4. Some visual and statistical analyses (e.g., PCA plots) feel more descriptive than rigorous."}, "questions": {"value": "Please see the weaknesses section — the paper’s presentation is difficult to follow and could benefit from further refinement, especially in improving clarity and figure readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jg2UOjiOQD", "forum": "KyIdMdPiEX", "replyto": "KyIdMdPiEX", "signatures": ["ICLR.cc/2026/Conference/Submission15668/Reviewer_StgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15668/Reviewer_StgR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696491959, "cdate": 1761696491959, "tmdate": 1762925923749, "mdate": 1762925923749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method based on stance elicitation for characterizing the political biases present in Large Language Models (LLMs). Specifically, the paper develops a series of prompts fundamentally consisting of an entity descriptor, a policy statement, and a yes/no question about the latter two components. The paper then uses the yes/no outputs from LLMs in order to characterize their political biases. It further compares the output biases of Pew Research Center’s political typology and RILE scores as a form of docking for validity. The paper also makes some observations about how some entities, like generic person descriptors, do not lead to reliable political biases in the outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a significant problem and has some potentially significant results. First, characterizing the political bias present in LLMs, especially in a black box manner, is a societally important problem, as LLMs are increasingly being used to analyze political content and even simulate political opinions. Furthermore, results like the instability of alignment of personalities with political beliefs are an interesting result from a bias analysis standpoint and may explain some of the results seen in other LLM political bias papers (as pointed out in the paper)."}, "weaknesses": {"value": "There are some weaknesses in the grounding, novelty, and clarity of the paper. For the grounding of the paper, I think there are weaknesses both methodologically and empirically. For methodology its not clear how much the methodology of prompting the LLM to get numerical responses would lead to different results if perturbations were applied. For example its seem likely that perturbing the verbiage of elements like the entity and policies, even if the verbiage is roughly semantically equivalent but maybe with a different sentiment polarity, in the prompts could alter the responses. In Ng et al. “Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification,” the authors found that differences in the datasets lead to downstream task performance differences. Additionally, how would results change if LLM were given a more nuanced way to evaluate positions of entities on policies, like a Likert scale versus a binary yes or no. Finally, the use of a tensor implies that each of the axes are all of the same thing along those axes. It's not clear from a qualitative perspective that policy issues are equivalent to something like moral principles; I would agree that they are related, but not that they are equivalent. Thus, it's not clear that a tensor is the right representation of the outputs of the models, given the entity and policy tests. From the empirical perspective, I think there are some issues with the completeness of the policy probes and presentation of some of the political viewpoints/parties. For the latter, in the U.S., the libertarian party is not a major party and does not represent a third pole of U.S. politics. In fact, most libertarians caucus and vote along Republican lines in major U.S. elections. For the former, while it's probably not possible to fully enumerate all policy positions to really find the contours of a party, there does not seem to be any grounding that the policy positions used in the prompting represent a reasonable basis to separate political parties. \n\nFor the novelty, it's not clear that the proposed method significantly improves upon the political surveying process done by Rozado in his works. The proposed method is, at its core, the same procedure Rozado uses (i.e., prompting LLMs on political questions and topics to evaluate their bias) but with a different structure around the prompts and a quantitative analysis of the outputs. \n\nFor clarity, I think the paper would really benefit from something like a flowchart to understand the full process of prompt generation $\\rightarrow$ tensor $\\rightarrow$ analysis of the tensor. I personally had a hard time tracing where the conclusions were coming from."}, "questions": {"value": "1. What is the reason the PCA is used for nearly all projections except one (policies uses t-SNE) despite mentioning having used PCA, t-SNE, and UMAP?\n\n2. How would results change if LLM were given a more nuanced way to evaluate positions of entities on policies? For example, what about using a Likert scale with degrees of agreement or disagreement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fvyEQCGoR5", "forum": "KyIdMdPiEX", "replyto": "KyIdMdPiEX", "signatures": ["ICLR.cc/2026/Conference/Submission15668/Reviewer_Kcmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15668/Reviewer_Kcmj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757048885, "cdate": 1761757048885, "tmdate": 1762925923341, "mdate": 1762925923341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “stance elicitation” method to audit LLM alignment. It builds a stance tensor from model responses to policy statements and uses PCA/t-SNE to project ideological positions. The authors claim it reveals consistent political and philosophical structures and highlights gaps between stated and revealed alignments. However, the setup and evidence are informal and mostly qualitative."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The stance tensor idea is a clear, general way to visualize model positions.\n* The generic vs. rule-based contrast is interesting for alignment diagnostics.\n* Some visual patterns (party clusters, philosophy alignment) seem intuitive and interpretable."}, "weaknesses": {"value": "* No robustness checks or variance across seeds/prompts; results may not be stable.\n* Heavy reliance on visualizations without statistical testing or quantitative evaluation.\n* Lacks comparison to recent political bias auditing baselines.\n* Code and details are missing; the paper also doesn’t follow ICLR formatting."}, "questions": {"value": "1. The paper claims to reveal political and philosophical structures “more faithfully than prior auditing approaches” (Sec. 1, Conclusion). Which prior methods are these, and why are there **no quantitative comparisons** to established frameworks such as Bang et al. (2024), Argyle et al. (2023), or Motoki et al. (2025)?\n2. None of the figures include **error bars, variance, or statistical tests**. How stable are the findings across random seeds, temperatures, or prompt paraphrases?\n3. Several evaluation items were reportedly **generated or validated by the same model families** later audited. How do the authors rule out circularity or data leakage?\n4. The RILE correlation (r = 0.99) seems implausibly high without sample size or CIs. How many parties were included, and is this robust to translation and summarization?\n5. Why does the submission **not use the official ICLR template**? Was this intentional?\n6. Will a compliant version and code with fixed API details be released for reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDuEh9HcMQ", "forum": "KyIdMdPiEX", "replyto": "KyIdMdPiEX", "signatures": ["ICLR.cc/2026/Conference/Submission15668/Reviewer_y9FM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15668/Reviewer_y9FM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064963842, "cdate": 1762064963842, "tmdate": 1762925922873, "mdate": 1762925922873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}