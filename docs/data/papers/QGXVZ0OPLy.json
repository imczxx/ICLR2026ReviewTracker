{"id": "QGXVZ0OPLy", "number": 23646, "cdate": 1758346732986, "mdate": 1763710212983, "content": {"title": "Unlocking the Power of Co-Occurrence in CLIP: A DualPrompt-Driven Method for Training-Free Zero-Shot Multi-Label Classification", "abstract": "Contrastive Language-Image Pretraining (CLIP) has exhibited powerful zero-shot capacity in various single-label image classification tasks. However, when applying to the multi-label scenarios, CLIP suffers from significant performance declines due to the lack of explicit exploitation of co-occurrence information. In pretraining, due to the contrastive property of its used objective, the model focuses on the prominent object in an image, while overlooking other objects and their co-occurrence relationships; in inference, it uses a discriminative prompt containing only a target label name to make predictions, which does not introduce any co-occurrence information. Then, an important question is as follows: \\textit{Do we need label co-occurrence in CLIP for achieving effective zero-shot multi-label learning?} In this paper, we propose to rewrite the original prompt into a correlative form consisting of both the target label and its co-occurring labels. An interesting finding is that such a simple modification can effectively introduce co-occurrence information into CLIP and it exhibits both good and bad effects. On the one hand, it can enhance the recognition capacity of CLIP by exploiting the correlative pattern activated by the correlative prompt; on the other hand, it leads to object hallucination in CLIP, where the model predicts objects that do not actually exist in the image, due to overfitting to co-occurrence. To address this problem, we proposed to calibrate CLIP predictions by keeping the positive effect while removing the negative effect caused by suspicious co-occurrence. This can be achieved by using dual prompts consisting of the discriminative and correlative prompts, which introduce label co-occurrence while emphasizing the discriminative pattern of the target object. Experimental results verify that our method can achieve performance than the state-of-the-art methods.", "tldr": "", "keywords": ["Zero-shot multi-label classification", "co-occurrence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16e16cc4c9dcf65e74ffe8dbd55fdad6b064c73e.pdf", "supplementary_material": "/attachment/29ae782ed0bf39190ab911511cdfce99e366ff75.zip"}, "replies": [{"content": {"summary": {"value": "The author tries to achieve zeroshot multi-label classification based on CLIP, and explore this task with labeling the object co-occurrence in CLIP. The paper improve the prompt to achieve the goal, and also calibrate CLIP predictions to achieve SOTA results. The experimental results are verified on mutiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes a better prompt (discriminative prompt + correlative prompt) on the CLIP model (or similar models) to achieve better performance.\n\n- The method only needs a few training data to derive the co-occurring labels."}, "weaknesses": {"value": "- The statement in the abstract is a bit vague. it would be better to add a sentence to summarize what method you used to 'keeping the positive effect while removing the negative effect caused by suspicious co-occurrence'.\n\n- The results seems promising on two datasets (MSCOCO and VG-256), but it would be great if the NUS-WIDE results can also be provided since it widely used with much larger dataset / number of labels and provide more convincing results."}, "questions": {"value": "- In the introduction, the author mentioned that \"There are two major challenges in achieving effective zeroshot multi-label classification\", one is \"CLIP tends to capture the global features of the dominated objects and neglecting the local features on otheres\", another is \"CLIP does not explicitly leverage label correlations\". So I want to post two questions on these statements. 1). the first statement seems not like a challenge in achieving effective zeroshot multi-label classification for CLIP, since the author mentioned ViT can solve this issue while CLIP already have the ViT version, this paper's best model is also based on ViT. Why using ViT seems like a trouble in this paragraph, can you explain a bit more? 2). In the second challenge, the author shows an example of the co-occurrence probability, and demonstrate that CLIP cannot precisely model co-occurrence relationships, but it suddently comes to the conclusion that \" resulting in unfavorable recognition performance\". So why it results in unfavorable recognition performance? I think more explanation should be added here.\n\n- In the experiment section, why the training data usage is 1% in MSCOCO and 2% for VG-256? How will this number influence the effectiveness of your method?\n\n- Besides, I notice that this paper is also submitted in ICML 2025 and NeurIPS 2025 but without replying to any reviewer comments [[link](https://openreview.net/forum?id=2WLEEo4tqT)] [[link](https://openreview.net/forum?id=oCDPvJ2uyc)], could you simply summarize these questions and provide the answer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PwcX765wH9", "forum": "QGXVZ0OPLy", "replyto": "QGXVZ0OPLy", "signatures": ["ICLR.cc/2026/Conference/Submission23646/Reviewer_3XJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23646/Reviewer_3XJU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760921055301, "cdate": 1760921055301, "tmdate": 1762942746463, "mdate": 1762942746463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DualPrompt, a simple yet effective method for improving zero-shot multi-label classification with CLIP. By incorporating co-occurrence information into the prompt design via both Discriminative Prompt and Correlative Prompt, the authors aim to leverage label dependencies during inference without training. The paper provides theoretical justification through a causal inference framework and achieves competitive performance across standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The method is simple and elegant, requiring no fine-tuning and remaining compatible with standard CLIP backbones.\n\n* The use of ChatGPT for co-occurrence estimation is a clever and pragmatic idea, enabling training-free augmentation of prompts.\n\n* The paper is generally well-written, and the causal graph perspective helps to clarify the strengths and pitfalls of the method."}, "weaknesses": {"value": "* Remaining Co-occurrence Issues Not Fully Analyzed:\nAlthough the proposed method aims to mitigate overfitting caused by co-occurrence, it is likely that some object hallucination or over-reliance on correlations remains, especially when the co-occurring object is much more prominent than the target object. However, there is no in-depth analysis or diagnostic results to explore these residual errors in detail.\n\n* Narrow Dataset Evaluation:\nThe experiments are conducted only on MS-COCO and VG-256. While VG-256 covers a wide label space, this is still limited compared to related works. Additional benchmarks such as VOC2007 or NUS-WIDE, which are widely used in multi-label classification, would strengthen the empirical evaluation and demonstrate the robustness and generalizability of the approach.\n\n* Minor Issues:\nThere are a few editorial errors, e.g., the phrase “Eq. equation” appears, which should be corrected. Such issues, while minor, slightly detract from the overall polish of the paper."}, "questions": {"value": "* Could you provide more detailed error analysis or qualitative cases showing the failure modes of DualPrompt, especially in cases where co-occurrence may still lead to false positives?\n\n* Have you tried evaluating the method on other commonly used multi-label datasets (e.g., VOC2007, NUS-WIDE)? If not, is there any reason for excluding them?\n\n* How sensitive is the model to the choice of co-occurring labels generated by ChatGPT? Are there failure cases when ChatGPT introduces incorrect or spurious co-occurrence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JLgzFNPt2B", "forum": "QGXVZ0OPLy", "replyto": "QGXVZ0OPLy", "signatures": ["ICLR.cc/2026/Conference/Submission23646/Reviewer_PYxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23646/Reviewer_PYxD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537243055, "cdate": 1761537243055, "tmdate": 1762942746105, "mdate": 1762942746105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DualPrompt, a simple yet effective method for improving zero-shot multi-label classification with CLIP. By incorporating co-occurrence information into the prompt design via both Discriminative Prompt and Correlative Prompt, the authors aim to leverage label dependencies during inference without training. The paper provides theoretical justification through a causal inference framework and achieves competitive performance across standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The method is simple and elegant, requiring no fine-tuning and remaining compatible with standard CLIP backbones.\n\n* The use of ChatGPT for co-occurrence estimation is a clever and pragmatic idea, enabling training-free augmentation of prompts.\n\n* The paper is generally well-written, and the causal graph perspective helps to clarify the strengths and pitfalls of the method."}, "weaknesses": {"value": "* Remaining Co-occurrence Issues Not Fully Analyzed:\nAlthough the proposed method aims to mitigate overfitting caused by co-occurrence, it is likely that some object hallucination or over-reliance on correlations remains, especially when the co-occurring object is much more prominent than the target object. However, there is no in-depth analysis or diagnostic results to explore these residual errors in detail.\n\n* Narrow Dataset Evaluation:\nThe experiments are conducted only on MS-COCO and VG-256. While VG-256 covers a wide label space, this is still limited compared to related works. Additional benchmarks such as VOC2007 or NUS-WIDE, which are widely used in multi-label classification, would strengthen the empirical evaluation and demonstrate the robustness and generalizability of the approach.\n\n* Minor Issues:\nThere are a few editorial errors, e.g., the phrase “Eq. equation” appears, which should be corrected. Such issues, while minor, slightly detract from the overall polish of the paper."}, "questions": {"value": "* Could you provide more detailed error analysis or qualitative cases showing the failure modes of DualPrompt, especially in cases where co-occurrence may still lead to false positives?\n\n* Have you tried evaluating the method on other commonly used multi-label datasets (e.g., VOC2007, NUS-WIDE)? If not, is there any reason for excluding them?\n\n* How sensitive is the model to the choice of co-occurring labels generated by ChatGPT? Are there failure cases when ChatGPT introduces incorrect or spurious co-occurrence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JLgzFNPt2B", "forum": "QGXVZ0OPLy", "replyto": "QGXVZ0OPLy", "signatures": ["ICLR.cc/2026/Conference/Submission23646/Reviewer_PYxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23646/Reviewer_PYxD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537243055, "cdate": 1761537243055, "tmdate": 1763732046903, "mdate": 1763732046903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets zero-shot multi-label image classification with CLIP, noting that vanilla CLIP underperforms because it (i) focuses on the most salient object and misses others, and (ii) does not explicitly leverage label co-occurrence during pretraining or inference. The authors introduce correlative prompts (CoP) that append a target label with its frequent co-occurring labels inside the prompt, explicitly injecting co-occurrence information at inference time. They show CoP can boost recognition of less-salient objects but also induces object hallucination when correlated context appears without the target."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method provides a simple, training-free mechanism to inject label co-occurrence at inference, which meaningfully improves zero-shot multi-label performance.\n2. The approach is backbone-agnostic and complements patch-centric methods (e.g., TagCLIP), showing additive gains when combined.\n3.The study includes informative diagnostics (class-wise AP changes, true/false positives, retrieval visualizations) that clarify how prompts alter model behavior."}, "weaknesses": {"value": "1.The theoretical derivation from a TDE subtraction to a simple additive fusion lacks rigorous justification; the introduced λ parameter is set to 1 without sensitivity analysis.\n2.The use of softmax across labels for multi-label probabilities and the summation of two scores raises calibration concerns and probabilistic interpretation remains unclear.\n3.The evaluation scope is limited to two datasets and broader generalization and cross-dataset portability of co-occurrence priors are not assessed.\n4.The method’s inference-time overhead should be reported (text encoding caching, similarity computation cost) for scalability considerations.\n5.The comparisons should control for image resolution, prompt ensembling, and preprocessing; decision thresholds should be explicitly defined and consistently applied."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xsjcmhJmD", "forum": "QGXVZ0OPLy", "replyto": "QGXVZ0OPLy", "signatures": ["ICLR.cc/2026/Conference/Submission23646/Reviewer_C23F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23646/Reviewer_C23F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622146838, "cdate": 1761622146838, "tmdate": 1762942745770, "mdate": 1762942745770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of CLIP in zero-shot multi-label classification, where the model fails to explicitly leverage label co-occurrence relationships, leading to suboptimal performance. This paper proposes a novel method called DualPrompt, which introduces co-occurrence information by using two types of prompts: a discriminative prompt containing only the target label and a correlative prompt that includes both the target and its co-occurring labels. While the correlative prompt enhances recognition by capturing label dependencies, it also causes object hallucination due to overfitting to co-occurrence patterns. To mitigate this, this paper develops a causal inference-based calibration mechanism that retains the benefits of co-occurrence while suppressing its adverse effects. Extensive experiments on MS-COCO and VG-256 datasets demonstrate that DualPrompt outperforms both training-based and training-free state-of-the-art methods, achieving superior performance even without any training data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tProposes a novel, training-free method (DualPrompt) that explicitly injects label-co-occurrence information into CLIP for zero-shot multi-label classification.\n2.\tIntroduces a lightweight dual-prompt strategy—discriminative vs. correlative—that can be applied to any CLIP backbone without training.\n3.\tProvides a principled causal-inference formulation that disentangles the helpful mediated effect of co-occurrence from the harmful “object-hallucination” effect, yielding calibrated predictions.\n4.\tDemonstrates consistent gains over both training-free (CLIP, TagCLIP) and training-based (DualCoOp, TaI) competitors on MS-COCO and VG-256.\n5.\tShows that only ~1 % of training data is sufficient to estimate reliable co-occurrence statistics, keeping data requirements minimal.\n6.\tValidates interpretability via visualization: retrieved images align better with true co-occurrence patterns while suppressing false positives, confirming that the method preserves semantic plausibility."}, "weaknesses": {"value": "1.\tThe paper only conducts experiments on MS-COCO and VG-256 datasets, which is not sufficient to demonstrate the generalizability of the proposed method. It is recommended to include experiments on more commonly used datasets in multi-label learning to further validate the effectiveness and robustness of the approach.\n2.\tThe paper lacks discussion of recent works that also exploit label co-occurrence, e.g., SST [1] and SCPNet [2]. Although these papers address different settings, a comparative summary of how their co-occurrence modeling differs from DualPrompt is necessary to clarify novelty and avoid overlap.\n3.\tThe paper places RELATED WORKS after the experiments and just before CONCLUSION, which breaks the usual flow and forces readers to assess the method’s novelty without early context. \n4.\tThe derivation of Eq. (3) contains errors. Please carefully re-check every step.\n5.\tTable 1 reports TaI-CLIP with ResNet-101 trained on COCO Captions, but the corresponding ViT-B/16 entry under the same training condition is missing.\n6.\tThe manuscript contains numerous typographical and consistency errors that must be thoroughly corrected: in Figure 2 “false negatives” is mislabeled, “prompr” should be “prompt”, “Eq. equation” is redundant, the tables mix “x” and “×”, the caption of Figure 5 is wrongly copied from Figure 6, Section 5.5 lists “microwave, oven, toaster, sink, refrigerator” while Figure 7 shows “bottle, cup, fork, knife, spoon, bowl”, the reference “Counterfactual reasoning for multi-label image classification via patching-based training” is duplicated, and “cane” should be “can”.\n7.\tOn line 201 the symbol F^t appears in the causal path L^t → F^t → Y without any prior definition; please clarify what this feature vector represents and how it differs from F^d and F^c to avoid confusing the reader.\n\n[1] Chen, Tianshui, et al. “Structured semantic transfer for multi-label recognition with partial labels.” Proceedings of the AAAI conference on artificial intelligence. Vol. 36. No. 1. 2022.\n\n[2] Ding, Zixuan, et al. “Exploring structured semantic prior for multi label recognition with incomplete labels.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}, "questions": {"value": "In Figure 6, why does DualCoOp++ (which presumably augments DualCoOp with additional parameters) yield lower mAP/F1 than DualCoOp?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8AVb6omKHd", "forum": "QGXVZ0OPLy", "replyto": "QGXVZ0OPLy", "signatures": ["ICLR.cc/2026/Conference/Submission23646/Reviewer_QsUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23646/Reviewer_QsUa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23646/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987889779, "cdate": 1761987889779, "tmdate": 1762942744888, "mdate": 1762942744888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}