{"id": "ZnwBhBZhFb", "number": 5131, "cdate": 1757852437876, "mdate": 1759897992769, "content": {"title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Generative Video Super-Resolution", "abstract": "Cascaded pipelines, which use a base text-to-video (T2V) model for low-resolution content and a video super-resolution (VSR) model for high-resolution details, are a prevailing strategy for efficient video synthesis. However, current works suffer from two key limitations: an inefficient pixel-space interface that introduces non-trivial computational overhead, and mismatched degradation strategies that compromise the visual quality of AIGC content. To address these issues, we introduce SimpleGVR, a lightweight VSR model designed to operate entirely within the latent space. Key to SimpleGVR are a latent upsampler for effective, detail-preserving conditioning of the high-resolution synthesis, and two degradation strategies (flow-based and model-guided) to ensure better alignment with the upstream T2V model. To further enhance the performance and practical applicability of SimpleGVR, we introduce a set of crucial training optimizations: a detail-aware timestep sampler, a suitable noise augmentation range, and an efficient interleaving temporal unit mechanism for long-video handling. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded systems. Video visual comparisons are available \\href{https://simplegvr.github.io/}{here}.", "tldr": "A Simple Baseline for Latent-Cascaded Generative Video Super-Resolution", "keywords": ["High-resolution text-to-video generation; Generative Video Super-Resolution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97a3051ce47ecd8f2507fdd602e4e47d6c6575b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an algorithm, SimpleGVR, for Video Super-Resolution (VSR) tailored for AIGC videos. SimpleGVR enhances the resolution of a generated video by operating directly in the latent space. The main contributions of the paper includes: (a) A simple, lightweight yet effective model for VSR for videos generated by latent diffusion models, (b) New degradation strategies to simulate low-resolution synthetic videos, and (c) new sampling scheduler and noise augmentation for improving the low-res latent codes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Motivation\n- Studying on efficient high-resolution video generation is critical for efficient video generation. \n\n### Method\n- The paper shows a simple yet efficient pipeline for AIGC video enhancement.\n- The paper shares interesting insights into degradation for AIGC videos and a new framework for upsampling latent codes.\n\n### Experimental results \n- The authors show extensive experimental evaluations, especially their model designs. \n- SimpleGVR outperforms many baselines over common benchmarks. \n- SimpleGVR enables long-clip training/inference (77 frames) under limited GPUs. \n\n### Writing/Presentation\nThe paper is well-written and easy-to-follow."}, "weaknesses": {"value": "### Motivation\n- Although reasonable and intuitive, the core idea (cascaded pipeline) is not new and exists in many video generation papers, such as Imagen-video from Google. \n\n### Experimental results \n- Temporal consistency is not well evaluated. Vbench metrics shown in the paper cannot capture temporal inconsistency.\n- As many no-reference visual assessment metrics are not convincing, it is highly recommended a user study is conducted for more direct comparison."}, "questions": {"value": "- Although only trained for AIGC videos, will SimpleGVR work for generic low-res videos by encoding real low-res videos into the latent space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aujFywj1ti", "forum": "ZnwBhBZhFb", "replyto": "ZnwBhBZhFb", "signatures": ["ICLR.cc/2026/Conference/Submission5131/Reviewer_w3HT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5131/Reviewer_w3HT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622190727, "cdate": 1761622190727, "tmdate": 1762917901664, "mdate": 1762917901664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SimpleGVR, a lightweight video super-resolution method designed for cascaded text-to-video generation systems. The approach performs super-resolution directly in the latent space, avoiding redundant encoding and decoding operations in pixel space. A latent upsampler is used to integrate low-resolution latent features, and a multi-step diffusion denoising process generates high-resolution latent representations. The authors further propose two degradation modeling strategies tailored to the characteristics of generative video content, along with several training techniques to enhance detail quality and temporal consistency. Experiments show that SimpleGVR achieves higher efficiency and comparable visual quality to existing multi-stage cascaded methods on their constructed datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The overall structure and motivation of the paper are reasonable. Performing video super-resolution in the latent space effectively avoids redundant VAE encoding and decoding, thereby reducing computational overhead. This idea is well justified within the cascaded text-to-video generation framework.\n\n2.The experimental design includes ablation studies and hyperparameter analysis, which systematically demonstrate the effects of different components such as degradation modeling, latent injection strategies, and timestep sampling methods."}, "weaknesses": {"value": "1.The work exhibits limited originality. The core idea of latent-space video super-resolution closely resembles prior studies such as FlashVideo, LaVie, and SeedVR. The proposed latent upsampler, channel concatenation, noise range setting, and timestep sampling strategy are largely incremental refinements or empirical combinations of existing approaches, lacking substantial algorithmic or theoretical innovation.\n\n2.The experiments are conducted on the authors’ self-constructed AIGC100 and VBench110 datasets, which are not publicly available, making it difficult to assess fairness and reproducibility. The comparison settings are not fully aligned, as details about the base T2V model, training data, and inference steps are not clearly specified.\n\n3.The paper does not compare with stronger diffusion-based VSR methods such as DiffVSR or VideoGigaGAN, nor does it evaluate whether the proposed model generalizes to different upstream T2V generators such as CogVideoX."}, "questions": {"value": "1.The paper states that injecting noise into the low-resolution branch helps the model correct structural errors and recover fine details, but the underlying mechanism is not well explained. It is recommended that the authors clarify whether the noise distribution is consistent between training and inference, and whether any mismatch could affect model stability. The paper also claims that the model can “correct structural errors,” but the limits of this ability are unclear. Current experiments only show mild distortions; more extreme cases would better demonstrate robustness.\n\n2.In the degradation modeling section, the flow-based degradation uses optical flow and blur to mimic artifacts observed in generated videos, yet it remains unclear whether these synthetic artifacts truly resemble those of real T2V outputs. The authors are encouraged to provide quantitative comparisons, such as spectral statistics or color-blending analysis. In the model-guided degradation strategy, where the upstream generator is involved in synthesizing training data, there is a risk that SimpleGVR might merely learn the biases of the teacher model rather than general restoration capability. A cross-model experiment, such as training on data from model A and testing on model B, would help evaluate generalization.\n\n3.The AIGC100 test set is manually curated by the authors, but the selection criteria and data distribution are not disclosed. It is recommended to report the category composition, motion intensity distribution, or include evaluations on public benchmarks to enhance credibility.\n\n4.The evaluation currently relies solely on no-reference metrics such as MUSIQ, DOVER, and VBench, without subjective assessment or explicit temporal consistency measures. Incorporating human perceptual scores (e.g., MOS) or motion-consistency metrics would provide a more comprehensive evaluation of video quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WR2ZKIhmKn", "forum": "ZnwBhBZhFb", "replyto": "ZnwBhBZhFb", "signatures": ["ICLR.cc/2026/Conference/Submission5131/Reviewer_ExS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5131/Reviewer_ExS4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804080992, "cdate": 1761804080992, "tmdate": 1762917901309, "mdate": 1762917901309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address the heavy computational cost and suboptimal visual quality of AIGC videos. It proposes a method named SimpleGVR, and designs explicit degradation pipelines to match the distribution of AIGC videos. Compared to existing VSR methods, it is interesting to consider the VSR problem within the paradigm of video generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1)\tThe observations of “video generation + vsr” pipeline are interesting. There are two key observations, architectural redundancy and degradation misalignment. Both of them are key to high-quality video generation.\n\n(2)\tThe synthesis pipeline of LR videos is quite reasonable, since it considers the distribution of T2V model."}, "weaknesses": {"value": "(1)\tThe motivation for the two synthetic degradations is unclear. Figure 4, used to justify common degradations in AIGC videos, is unconvincing. For example, the color blending example, which is very similar to natural lighting or shadow variation and seems to have little impact on video quality. Providing clearer, more representative cases where these degradations noticeably degrade perceptual quality would strengthen the rationale for the subsequent degradation design.\n\n(2)\tThe evaluation is limited in both dataset and metrics. The paper adopts AIGC100 as the testing dataset, which aligns with the goal of enhancing AIGC videos but lacks diversity to verify the method’s generalization. Since AIGC100 contains AI-generated videos whose degradations differ from those in most compared models’ training data, it is unclear whether the improvement comes from the model itself or from the matched degradation distribution. Additional results on general datasets (e.g., VideoLQ) are suggested. As for evaluation metrics, besides MUSIQ and DOVER, it would be more convincing to include CLIPIQA, MANIQA, and Warping Error for a more comprehensive quality assessment.\n\n(3)\tThe comparison methods are also limited. It would strengthen the experimental evaluation to include more recent or representative approaches, such as MGLD, DLoRAL, Dove, and SeedVR2, to provide a more comprehensive and convincing performance comparison."}, "questions": {"value": "(1)\tA large portion of the paper, including Figure 2, is devoted to describing the combination of LR and GT features. However, the conclusion drawn from this design seems rather intuitive, since the latent upsampler introduces additional 3D ResBlocks compared to latent interpolation. This part feels redundant and could be streamlined for clarity.\n\n(2)\tFigure 8 is hard to understand. The meaning of symbol “l” is not explained and annotations of “2k/2k+1/2k+2” are ambiguous. And also, what does “shift window” mean exactly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dxWZDCKpIt", "forum": "ZnwBhBZhFb", "replyto": "ZnwBhBZhFb", "signatures": ["ICLR.cc/2026/Conference/Submission5131/Reviewer_D3ZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5131/Reviewer_D3ZN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935729840, "cdate": 1761935729840, "tmdate": 1762917900280, "mdate": 1762917900280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a diffusion-based latent video super resolution model for T2V model outputs. It first inject the low-res latent into VSR model by a proposed latent upsampler and channel concatenation operations. To mitigate the gap between synthetic low-res latent and real T2V model output latents, the author proposed two degradation schemes, i.e., flow-based and model-guided degradation. Finally it also introduce multiple training receipts for improved performance, controllable generative details and long video super resolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- VSR directly performs on latent space, instead of introducing decoding and re-encoding from lossy and compute heavy VAE.\n- There are comprehensive low-res latent injection methods and discussion, which gives a better understanding of the best low-res to high-res latent mapping.\n- It proposed flow-based and model-guided degradation schemes to simulate the degradation characteristics of the T2V base model outputs.\n- The detail-aware timestep sampler, optimal noise augmentation range, and interleaving temporal unit mechanism enhance the model generalization ability and robustness."}, "weaknesses": {"value": "- The overall model design is specific to the base T2V model used. Specifically, the design of flow-based degradation is used to mimic the artifacts from the specific base model. It would be helpful to better understand the generalization ability of the proposed model on other baseline T2V models.\n- The flow-based degradation though novel, may introduce multiple stages like optical flow estimation, color blending, distance-based weighting, etc. This can bring complexity of the pipeline and is likely prone to error."}, "questions": {"value": "- For the proposed low-res latent injection method, it first increase the channel and spatial/time resolution and reduce them in the end. What is the rationale behind it? And is there any ablation about any simpler designs like directly increasing the channel and spatial/time resolution to match the high-res latents?\n- How is the proposed detail-aware sampler implemented in details? Is it the shifted/warped timestep toward high noise level sampler?\n- How to resolve the memory consumption when dealing with mulitple video chunk for each transformer layers? Are all intermediate results cached while computing so that they can be shifted high window?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RQ1ufACugz", "forum": "ZnwBhBZhFb", "replyto": "ZnwBhBZhFb", "signatures": ["ICLR.cc/2026/Conference/Submission5131/Reviewer_tr3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5131/Reviewer_tr3b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5131/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983700035, "cdate": 1761983700035, "tmdate": 1762917899835, "mdate": 1762917899835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}