{"id": "BzqkbJTsbq", "number": 8957, "cdate": 1758104244154, "mdate": 1759897751814, "content": {"title": "DPG: Exploiting Data and Process Knowledge for Diffusion Guidance", "abstract": "Diffusion models have excelled in imperfect-label guidance tasks, including weak-label guidance (e.g., style transfer) and degraded-label guidance (e.g., image super-resolution and deblurring), where supervision is incomplete or compromised. Current methods are either tailored to specific tasks, limiting generalization and transferability, or rely solely on loss-guided methods for imperfect-label tasks, imposing insufficient constraints and overlooking valuable domain priors, leading to suboptimal results. We argue that these limitations stem from the failure to uncover the universally applicable latent knowledge in these tasks and methods. To address this, we propose DPG, a universal framework that exploits data knowledge inherent to the tasks and process knowledge derived from reverse diffusion in imperfect-label guidance tasks. Explicitly, we diffuse imperfect-label data or its variants and then incorporate them into the initial stages of reverse diffusion, elevating the precision and efficiency of outputs in aligning with labels. As well, we capitalize on the temporal progression of the diffusion model’s denoising path, ensuring that each step surpasses its predecessor in satisfying label constraints to refine optimization choice, thus improving guidance fidelity and accelerating convergence. By integrating this knowledge, DPG can achieve generalization and optimal performance in imperfect-label tasks, paving the way for future innovations in unified frameworks. Extensive experiments demonstrate DPG's effectiveness, consistently generating high-quality outputs. The code will be public.", "tldr": "We propose a unified framework, termed DPG, for addressing imperfect-label guidance tasks by incorporating both data and process knowledge.", "keywords": ["Deep learning", "image generation", "style transfer", "image restoration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc61507f17dd530d90b486c2922e5b01e2fac398.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper redefines style transfer as a weak-label guidance problem and image super-resolution / deblurring as degraded-label guidance. It argues that these existing methods are typically tailored to specific tasks, lack generalization and transferability, and often rely solely on loss-based optimization while neglecting underlying domain priors. To address this limitation, the authors propose a unified framework that leverages data knowledge inherent to each task and process knowledge extracted from the reverse diffusion process to solve image restoration and style transfer tasks jointly. They claim that unifying these two tasks is inherently challenging because style transfer depends on abstract style information, whereas restoration tasks use strong, explicit conditioning. Furthermore, they critique loss-gradient-based guidance methods, emphasizing that loss values may not accurately reflect image fidelity and are vulnerable to error propagation. Their method interpolates between the noised label prediction and the generated image from the reverse diffusion process at each step, while simultaneously enforcing label information through a loss computed between the predicted $x_0$ and the target $y$"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses an important and timely problem: unifying weak-label (style transfer) and degraded-label (image restoration) tasks under a single diffusion-guided framework.\n- The proposed approach, integrating data knowledge and process knowledge, is conceptually innovative and could provide new insights for handling tasks with imperfect or indirect supervision."}, "weaknesses": {"value": "- The motivation for unifying two inherently distinct tasks, style transfer and image restoration, is not sufficiently substantiated. The authors (L99-L111) suggest potential knowledge transfer between the two, but no empirical evidence or references are provided. In fact, these tasks possess fundamentally different supervision properties: style transfer relies on weak, abstract cues, whereas restoration requires high-fidelity alignment with the reference. This discrepancy raises concerns about negative transfer when attempting to integrate both under a single framework. Including empirical evidence or prior works demonstrating positive transfer between these tasks would strengthen the argument.\n- The inclusion of task-specific components, such as the label-processing function $M$ and hyperparameters like $\\eta_1$, appears to contradict the paper’s claim of a unified framework. If these elements must be individually tuned per task, the true level of unification remains ambiguous.\n- In L263, the authors briefly compare their approach with SDEdit, but the described mechanism, mixing the noised conditioning image during the diffusion process, closely resembles the method proposed in ILVR [a]. A proper citation and discussion of ILVR should therefore be included for completeness.\n\nMinor Weaknesses\n\n- The deblurring setup assumes a Gaussian blur kernel, which may be an overly restrictive assumption and limit general applicability.\n- The paper seems to inconsistently use \\citet and \\citep throughout; all citations should be revised for consistency according to the chosen bibliography style.\n\n[a] Choi, Jooyoung, et al. ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models., CVPR 2021"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GAa1rtTJzh", "forum": "BzqkbJTsbq", "replyto": "BzqkbJTsbq", "signatures": ["ICLR.cc/2026/Conference/Submission8957/Reviewer_B5jh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8957/Reviewer_B5jh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928814537, "cdate": 1761928814537, "tmdate": 1762920694314, "mdate": 1762920694314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DPG, which is a unified guideline framework for tasks with incomplete labels, covering both weak label settings (e.g., style transfer) and degraded label settings (e.g., super-resolution, denoising). The core idea is (1) to incorporate “data knowledge” by diffusing the (possibly processed) label itself and incorporating it into the early steps of the reverse diffusion process, and (2) to enforce “process knowledge” through a stepwise constraint, whereby each step of denoising improves the label alignment compared to the previous step. Experiments on style transfer and inverse problems show comparable qualitative and quantitative results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed DFG can be used for stylization, SR, and de-noising, especially without changing the architecture. In addition, DFG can be easily applied to the existing reverse diffusion process.\n- The paper distinguishes between “what to inject\" (signal derived from the label), “when to inject” (early steps), and “how to keep progress\" (stepwise constraint). This modular presentation makes the pipeline easy to understand."}, "weaknesses": {"value": "- The authors redefine stylization as “weak label guidance” and classic inverse problems (e.g., super-resolution, denoising) as “degraded label guidance,” which differs from established usage in the computer vision literature. Using the available measurement/observation $y$ as a guide during the reverse diffusion process is already common practice in existing algorithms for both stylization and inverse problems. Therefore, this work cannot be considered the first to \"unify\" these approaches; such a formulation risks confusing the readership rather than clarifying the contribution.\n- To capture the measurement/observation inforamtion, $z_{0|t}$ is refined  using eq. (9). This objective function $\\ell_1 (z_{0|t},  y)$ is already frequently used to improve the performance of stylization and inverse problems. This cannot be considered a novelty of the proposed DPG.\n- For process knowledge integration, $\\alpha_\\text{margin}$ would strongly influence the performance of tasks. Depending on other hyperparameters such as $T_1$, $\\eta$, and $\\gamma$, the quality of results may change. However, the authors do not provide any ablation studies. They just offer simple guidelines in the supplementary material. Based on the currently available information, the algorithm cannot be reproduced. \n- The qualitative comparisons presented in the paper cannot confirm that the proposed DPG clearly outperforms existing algorithms. Furthermore, the image quality is not sufficient to compare the algorithms."}, "questions": {"value": "- For the data knowledge integration, why are two linear interpolations fundamentally required in eq. (7) - (i) mixing the latent ($z_t$ and $\\hat{c}_t$) and (ii) mixing the predicted noise (${\\epsilon}(z_t, \\cdot)$ and $\\epsilon(c_t, \\cdot)$? In other words, what stability principle makes both convex combinations necessary together, rather than one of them (or another mechanism) being sufficient?\n- How high are the runtime and memory requirements of DFG compared to the standard reverse diffusion processes and existing algorithms?\n- The ablation studies in relation to the hyperparameters"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wNv5NRFBAB", "forum": "BzqkbJTsbq", "replyto": "BzqkbJTsbq", "signatures": ["ICLR.cc/2026/Conference/Submission8957/Reviewer_Ybdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8957/Reviewer_Ybdq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930031573, "cdate": 1761930031573, "tmdate": 1762920693848, "mdate": 1762920693848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DPG is a training-free guidance method for “imperfect-label” problems: both weak labels (like a style image) and degraded labels (like low-res or blurred inputs). It first applies a simple task-specific pre-process to the label (e.g., upsampling or Wiener deblur). In the early denoising steps only, DPG injects “data knowledge” by blending that noised label with the current latent and by mixing its predicted score with the model’s normal score, giving the sampler scale-matched structural hints without hard constraints. At each step, a task loss nudges the current clean prediction toward the label (e.g., VGG/CLIP losses for style; re-degradation consistency for SR/deblur). Across the whole trajectory, DPG adds “process knowledge” via a margin term that enforces step-to-step improvement in the task loss, which suppresses zig-zagging and error accumulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novelty on label integration: forward-diffusing the label and blending it for additional integration of the label seems interesting. While I am not fully convinced why this should work well, it seems novel. \n- Performance: compared to baselines, DPG seems to be superior on various tasks."}, "weaknesses": {"value": "- Unfortunately, the paper format doesn't follow ICLR guidelines yet. The guidelines say as follows: ```Citations within the text should be based on the \\texttt{natbib} package\nand include the authors' last names and year (with the ``et~al.'' construct\nfor more than two authors). When the authors or the publication are\nincluded in the sentence, the citation should not be in parenthesis using \\verb|\\citet{}| (as\nin ``See \\citet{Hinton06} for more information.''). Otherwise, the citation\nshould be in parenthesis using \\verb|\\citep{}| (as in ``Deep learning shows promise to make progress\ntowards AI~\\citep{Bengio+chapter2007}.'').``` To follow this guideline, the first sentence of the introduction section should start with \"In recent times, diffusion models (DMs) (Ho et......)\". Unfollowing this ICLR format guideline makes it hard to read the paper. \n- DPG introduces many hyperparameters, because it should determine 1) mixing weights, 2) early-step window, 3) margin, and 4) inner step sizes. If this parameter should be changed according to tasks, the author's main claim that DPG is a generalizable framework for weak-label and degraded-label tasks should not hold, because this means not a generalizable framework but an adjustable framework according to tasks.\n- Justification depth: While intuitions are clear, the paper lacks a deeper analysis of line 250: \"By adding noise and applying guidance, we let the model select the most relevant information for the task.\" My question is whether just mixing the noisy label is enough to change the model's behavior to select the most relevant information. In the rebuttal and discussion phase, I would like to discuss this with the author and revise the paper to correctly understand the reader. In current form, I am more towards that just mixing is not enough to enforce the model to choose relevant information only."}, "questions": {"value": "- In Appendix, I found that the hyperparameters should be changed according to tasks. Is DPG sensitive to hyperparameter changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wYOLKYncl7", "forum": "BzqkbJTsbq", "replyto": "BzqkbJTsbq", "signatures": ["ICLR.cc/2026/Conference/Submission8957/Reviewer_D3Cj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8957/Reviewer_D3Cj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938238114, "cdate": 1761938238114, "tmdate": 1762920693516, "mdate": 1762920693516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DPG (Diffusion Process Guidance), a training-free framework for improving diffusion model guidance in imperfect-label tasks such as style transfer, super-resolution, and deblurring.\nDPG combines data knowledge, by injecting diffused representations of imperfect labels into early denoising steps, and process knowledge, by enforcing progressive consistency across timesteps.\nThis unified approach refines diffusion trajectories without task-specific tuning, achieving better perceptual quality and fidelity across diverse restoration and translation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a unified and training-free framework that creatively combines data- and process-level knowledge for diffusion guidance.\nThe idea of injecting diffused label information and enforcing progressive alignment offers a novel and generalizable formulation across multiple imperfect-label tasks.\nExperiments are comprehensive and consistent, showing clear improvements in perceptual quality.\nThe paper is well-written and easy to follow, with solid motivation and clear methodological structure."}, "weaknesses": {"value": "1. **Clarity and originality of the “unified framework” claim.**\n   While the paper frames DPG as a unified, training-free guidance framework, several prior works have already explored similar directions. For example, *Manifold Preserving Guided Diffusion* (ICLR 2024) formulated a single training-free framework that covers super-resolution, deblurring, face-ID, CLIP, and style guidance—all without an explicit forward operator. Thus, the novelty of unification alone may be limited. The introduction dedicates considerable space to emphasizing potential advantages of such unification, but the paper would benefit from demonstrating concrete benefits (e.g., improved generalization across tasks or reduced hyperparameter tuning) rather than discussing them conceptually.\n\n2. **Task-specific dependency of the mapping function ( M ).**\n   Although the framework is described as general, it still requires defining a mapping function ( M ) to transform task-specific labels ( y ) into an image-like form. This step is not always straightforward and may vary considerably across tasks. In some inverse problems (e.g., phase retrieval or Fourier-domain tasks in *Diffusion Posterior Sampling*), such a mapping is not directly feasible, which somewhat limits the claimed generality of the approach.\n\n3. **Missing discussion and comparison with closely related work.**\n   The *Manifold Preserving Guided Diffusion* paper—arguably the most methodologically similar prior work—is not cited or discussed. That work also introduces an ( x_0 )-alignment loss with target information, conceptually close to Equation (9) here. Including a citation, a brief discussion, and possibly a quantitative comparison (e.g., in Fig. 4 or Table 1) would clarify the distinction between DPG and prior frameworks and make the contribution positioning clearer.\n\n4. **Need for deeper analysis of the core components.**\n   The main novelty appears to lie in the *Data Knowledge Integration (DKI)* and *Process Knowledge Integration (PKI)* modules, yet their effects are not analyzed in depth.\n   – For **DKI**, injecting label information during early denoising is an intuitive and appealing idea, particularly since early predictions are often blurry and unstable. However, the ablation table (Table 2) seems inconsistent—entries (b) and (c) report significantly lower PSNR than the “w/o D” or “w/o P” variants, which might be a reporting error.\n   – To help readers better appreciate DKI’s role, it would be valuable to visualize several denoising trajectories, comparing predicted ( x_0 ) with and without DKI. Such qualitative evidence would make the mechanism’s effect clearer and strengthen the empirical analysis."}, "questions": {"value": "- Basically, all points mentioned in the Weaknesses section.\n- The role of ( $\\alpha_{\\text{data}}$ ) is intuitively clear—it controls how much information is drawn from the label—but the meaning of ( $\\gamma_{\\text{data}}$ ) is less so. It seems analogous to a CFG-style guidance term interpolating between predictions from label-mixed and original latents. Why is this additional guidance needed if prediction from the label-mixed latent alone is already available? What new information is obtained from extrapolating between the two predictions? Given that this doubles the computational cost, please justify the necessity of this step and, if possible, provide qualitative or quantitative results across several key tasks for different ( $\\gamma_{\\text{data}}$ ) values.\n\nIf I have misunderstood this mechanism, I would welcome clarification. My rating could increase accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2LrB5RdnR2", "forum": "BzqkbJTsbq", "replyto": "BzqkbJTsbq", "signatures": ["ICLR.cc/2026/Conference/Submission8957/Reviewer_dEcG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8957/Reviewer_dEcG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979360022, "cdate": 1761979360022, "tmdate": 1762920693026, "mdate": 1762920693026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}