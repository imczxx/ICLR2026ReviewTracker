{"id": "4nMPx7BHIg", "number": 8711, "cdate": 1758095653067, "mdate": 1759897768341, "content": {"title": "Learning Based on Neurovectors for Tabular Data: A New Neural Network Approach", "abstract": "In this paper, we present a novel learning approach based on Neurovectors, an innovative paradigm that structures information through interconnected nodes and vector relationships for tabular data processing. Unlike traditional artificial neural networks that rely on weight adjustment through backpropagation, Neurovectors encode information by structuring data in vector spaces where energy propagation, rather than traditional weight updates, drives the learning process, enabling a more adaptable and explainable learning process. Our method generates dynamic representations of knowledge through neurovectors, thereby improving both the interpretability and efficiency of the predictive model. Experimental results using datasets from well-established repositories such as the UCI machine learning repository and Kaggle are reported both for classification and regression. To evaluate its performance, we compare our approach with standard machine learning and deep learning models, showing that Neurovectors achieve competitive accuracy while significantly reducing computational costs.", "tldr": "A learning architecture based on a new concept called neurovector to solve classification and regression problems for tabular data", "keywords": ["classification and regression", "learning architecture", "energy-based learning", "tabular data"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f3272f66539f35b0af39b26008d350b66c2feb3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper present a predictive model based on Neurovectors, which are created to model similarity between data points. Unlike typical backpropagation used in neural network, the proposed model is trained usng energy-driven process. The model is evaluated on three tabular datasets and compared with basic baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-The authors tackle an important problem of tabular data classification, where typical neural networks are comparable to shallow models.\n-The idea is interesting and the model is inspired by LLMs"}, "weaknesses": {"value": "I could miss some important details but I think that the model is not correct. Looking at the formula of f(\\tau) on page 4, there might not exist any neurovectors from the train set which have the same value at any feature. Take, for instance, a training set composed of two 2D points (1,1) and (2,2). If we want to make prediction for point (3,3), then what is returned by f(\\tau)? \n\nEven if the above could be corrected the method looks very similar to k-NN approach. Therefore I do not see much novelty in this method.\n\nFinally, the evaluation is below the standards of ICLR: 3 simple datasets and only shallow (and one MLP) baselines is not sufficient. Even with strong novelty, the method has to be evaluated on more examples."}, "questions": {"value": "The authors could explain points (1) from the weakness section. Moreover, they also should elaborate on the connections with k-NN."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hlk78oHEiS", "forum": "4nMPx7BHIg", "replyto": "4nMPx7BHIg", "signatures": ["ICLR.cc/2026/Conference/Submission8711/Reviewer_13uC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8711/Reviewer_13uC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761020484533, "cdate": 1761020484533, "tmdate": 1762920515436, "mdate": 1762920515436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel supervised learning method and reports its accuracy on three small datasets. Unfortunately, the paper fails to point out that the new method is a variation of k-nearest neighbor with k=1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The central idea is interesting and the experimental results are believable."}, "weaknesses": {"value": "The major weakness is that the method proposed in this paper is not novel; it is a variation of k-nearest neighbor. Specifically, Equations 6 and 7 say that the predicted label of a test example is the label of the training example with maximum count(NV) score. The score of a training example is the number of its feature values that equal the value of the same feature in the test example.\n\nThe predicted label is the label of the single nearest (most similar) neighbor of the training example, where similarity is measured as the number of identical feature values.\n\nSection 3.3 provides a method for editing the training set by upweighting examples that provide correct predictions. A conceptually similar idea is proposed by Wilson, D.L. (1972) Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and Cybernetics, 2(3), 408-421.\n\nOther weaknesses:\n\nEquation 1 says that features are real-valued but then Section 3.2.1 requires exact matches, which is not sensible for real numbers.\n\nEquations 8 and 9 are purely heuristic, so it is not justified to call the method energy-based.\n\nThe experiments are insufficient: on only three small datasets. The results in Table 2 are not impressive: the new method does not yield systematically better accuracy.\n\nThe paper claims that Python dictionary lookups have time complexity. This is true only in the average case, and the worst case is O(n).\n\nThe FLOPS discussion on page 8 is pointless because the datasets are all small. 10^10 FLOPS is less than a second on a GPU nowadays."}, "questions": {"value": "No specific questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bY4zIlXEal", "forum": "4nMPx7BHIg", "replyto": "4nMPx7BHIg", "signatures": ["ICLR.cc/2026/Conference/Submission8711/Reviewer_UCPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8711/Reviewer_UCPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535223942, "cdate": 1761535223942, "tmdate": 1762920514095, "mdate": 1762920514095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a **“neurovector”** paradigm for tabular learning that avoids backpropagation and trainable weights. Each training row is stored as a neurovector made of tokens from feature–value pairs; at inference, a test instance is tokenized the same way, candidate neurovectors are retrieved by token overlap, and the prediction is taken from the **most-overlapping** candidate. Formally, for test instance (x_j) with tokens (\\tau_{j,l}), let (C_j) be the set of training neurovectors that share at least one token with (x_j). Let (M(NV,j)) be the number of shared tokens between a candidate (NV \\in C_j) and (x_j). The method predicts\n$$\nm=\\arg\\max_{NV\\in C_j} M(NV,j), \\qquad \\hat{y}_j = y_m .\n$$\n\nTies are broken by an **energy** score. For classification:\n$$\nE(NV)=\\frac{(s(NV))^2}{u(NV)} ,\n$$\nwhere (s(NV)) is the number of past correct uses of (NV) and (u(NV)) is the total uses. For regression:\n$$\nE_{\\mathrm{reg}}(NV)=\\frac{(s(NV))^2}{u(NV)} ,\\exp!\\left(-\\alpha,\\mathrm{MAE}(NV)\\right).\n$$\n\nThe approach aims to be **interpretable** (explicit token overlaps) and **efficient** (no gradient steps; create-on-error storage). On several UCI/Kaggle datasets, the authors report competitive accuracy compared to standard ML/DL baselines with reduced computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Simplicity & interpretability:** Prediction follows transparent token overlaps; energies provide per-instance diagnostics.\n* **Gradient-free training:** Create-on-error storage avoids backpropagation/hyperparameter sweeps, attractive for low-resource settings.\n* **Clear, reproducible core:** Retrieval and tie-breaking rules are explicit; basic complexity can be reasoned about via hash lookups and candidate ordering.\n* **Potential efficiency:** If storage/candidates remain small, inference could be fast and memory-light in practice."}, "weaknesses": {"value": "* **Limited evaluation:** Only a few datasets; no multi-seed cross-validation; several strong tabular baselines are missing (CatBoost/LightGBM/XGBoost, TabPFN, FT-Transformer); statistical tests and average-rank analyses are absent.\n* **Tokenization brittleness:** Using **exact numeric values** as tokens risks near-zero overlap; discretization/quantization schemes (or similarity metrics) are not explored.\n* **Compute claims unclear:** FLOP comparisons are indirect; no **wall-clock**, **RAM footprint**, or scaling curves vs. dataset size/feature cardinality; unclear training vs. inference accounting.\n* **Protocol clarity:** Split definitions and tuning budgets per model are not consistently documented; potential for selection bias.\n* **Theory gap:** No bounds on error, storage growth, or retrieval accuracy; the energy function lacks principled justification."}, "questions": {"value": "1. **Numerical features:** How do you handle continuous values? Please report results with **binning/quantization** (e.g., equal-width, quantile, learned discretizers) and analyze sensitivity.\n2. **Evaluation protocol:** Which split strategy is final (ratios, seeds)? Please provide **mean±std over 10–30 random splits** per dataset and **significance tests**.\n3. **Compute & memory:** Report **wall-clock**, **RAM** (dictionary size vs. (|\\text{train}|)), and scaling of candidate set size (m) with (|\\text{train}|) and feature cardinality. Add **Pareto plots** (accuracy vs. compute/memory).\n4. **Baselines:** Include **CatBoost/LightGBM/XGBoost** and recent deep tabular baselines (**FT-Transformer, TabPFN**) on a **larger benchmark suite** (10–20 datasets) with **average ranks**.\n5. **Ablations:** (a) exponent in (\\text{success}^2/\\text{use}); (b) remove energy or replace with a **learned** tie-breaker; (c) **store-all** vs. **store-on-error**; (d) robustness to **label noise** and missing values.\n6. **Collision control:** How are rare categories/high-cardinality handled? Any hashing scheme or token pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w3udQaW2mq", "forum": "4nMPx7BHIg", "replyto": "4nMPx7BHIg", "signatures": ["ICLR.cc/2026/Conference/Submission8711/Reviewer_HGG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8711/Reviewer_HGG6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932703373, "cdate": 1761932703373, "tmdate": 1762920513673, "mdate": 1762920513673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel learning paradigm for tabular data that replaces backpropagation with energy propagation in vector spaces. Instead of weight updates, the model encodes information through interconnected nodes and vector relationships, aiming for higher interpretability and computational efficiency.  Experimental results demonstrate the effectiveness of this method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a well-motivated idea. Transforming tabular data into vectorized or text-like representations to make them compatible with large language models (LLMs). This direction is timely and meaningful, as it moves beyond conventional tree-based models toward architectures that can leverage foundation models.\n2. The paper is clearly written and well-structured, making the proposed approach easy to follow and conceptually accessible."}, "weaknesses": {"value": "1. The experimental evaluation is rather limited in scope. The paper includes only a few datasets, and the results in Table 2 are not convincing. For instance, on Breast Cancer, the proposed method performs comparably to the baseline; on Absenteeism at Work, results are reported as N/A; and Red Wine Quality is a small, non-representative dataset. To substantiate the claimed advantages, additional experiments on more diverse and large-scale tabular datasets are necessary. Moreover, the paper omits comparisons with strong deep learning baselines specifically designed for tabular data, such as FT-Transformer, TabNet, or NODE. Including these methods would provide a more meaningful and fair evaluation of the proposed approach, particularly in assessing its scalability and competitiveness against modern deep architectures.\n2. In the experiments, does “Gradient Boosting” refer to XGBoost, Gradient Boosting Decision Trees (GBDT), Gradient Boosting Regression Trees (GBRT), or another variant of the Gradient Boosting family? Please clarify which specific implementation or library was used, as different versions can differ substantially in optimization strategy, regularization, and performance."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Dvm6qdycM", "forum": "4nMPx7BHIg", "replyto": "4nMPx7BHIg", "signatures": ["ICLR.cc/2026/Conference/Submission8711/Reviewer_Mhyi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8711/Reviewer_Mhyi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967388131, "cdate": 1761967388131, "tmdate": 1762920513305, "mdate": 1762920513305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}