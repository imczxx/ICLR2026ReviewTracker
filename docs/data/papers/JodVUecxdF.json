{"id": "JodVUecxdF", "number": 22622, "cdate": 1758333656924, "mdate": 1759896856176, "content": {"title": "MAFE: Enabling Equitable Algorithm Design in Multi-Agent Multi-Stage Decision-Making Systems", "abstract": "Algorithmic fairness is often studied in static or single-agent settings, yet many real-world decision-making systems involve multiple interacting entities whose multi-stage actions jointly influence long-term outcomes. Existing fairness methods applied at isolated decision points frequently fail to mitigate disparities that accumulate over time. Although recent work has modeled fairness as a sequential decision-making problem, it typically assumes centralized agents or simplified dynamics, limiting its applicability to complex social systems. We introduce **MAFE**, a suite of Multi-Agent Fair Environments designed to simulate realistic, modular, and dynamic systems in which fairness emerges from the interplay of multiple agents. We demonstrate MAFEs in three domains---loan processing, healthcare, and higher education---supporting heterogeneous agents, configurable interventions, and fairness metrics. The environments are open-source and compatible with standard multi-agent reinforcement learning (MARL) libraries, enabling reproducible evaluation of fairness-aware policies. Through extensive experiments on cooperative use cases, we demonstrate how MAFE facilitates the design of equitable multi-agent algorithms and reveals critical trade-offs between fairness, performance, and coordination. MAFE provides a foundation for systematic progress in dynamic, multi-agent fairness research.", "tldr": "This paper presents the Multi-Agent Fair Environment (MAFE) framework, a flexible tool for modeling multi-agent systems with fairness considerations, demonstrated through case studies in healthcare, education, and finance.", "keywords": ["Dynamic Fairness", "Fairness", "Multi-agent Environments"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78f9c9b04bb9f102a2d1993bfa5977c583eca305.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the Multi-Agent Fair Environments (MAFE) that provides a fairness benchmark for multi-agent sequential decision-making systems. They mention that previous work mostly focus on static or single-agent settings, but most real-world scenarios are multi-agent. They offer 3 domains: loan processing, healthcare, and higher education, and offer heterogeneous agents. They propose the Fair Multi-Agent Cross Entropy Method (F-MACEM) method to jointly optimize over agents for reward and fairness components. They use real-world data to compute priors over the domains in consideration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The study of long-term fairness in multi-agent sequential decision making is an important topic.\n* The proposed domains use real-world data as part of the simulation.\n* The empirical results look good using their proposed method, and the benchmark methodology seems to make sense."}, "weaknesses": {"value": "* The paper advertises a benchmark that promotes reproducibility and compatibility with standard MARL libraries. However, I do not see any experiments showcasing this. There are F-MAPPO and F-MADDPG, but the authors do not specify whether these come from off-the-shelf libraries. I think the paper would benefit from including comparisons of more baseline off-the-shelf MARL algorithms.\n* While the use of real-world datasets to instantiate the MAFEs is nice, there is limited discussion on how realistic the environment dynamics portray real-world observations. I do not recall seeing this listed as a limitation anywhere.\n* I find it strange that the F-MACEM method is proposed in this paper but its analysis appears limited and all discussion of it is moved to Appendix. What is its novelty, how does it compare with more existing algorithms, including those more similar to it that e.g. uses evolutionary methods?\n* The related works, especially Sec. 2.2, can benefit from more analysis/citations and what sets your work apart. For example, [1, 2].\n* The linear combination of reward and fairness terms as an augmented reward yields a potentially nontrivial multi-objective optimization problem which may be hard to select appropriate weights and directly solve.\n\n\nMinor: Equation 1 of Section 5, I think $\\theta_n, \\alpha, \\beta$ not defined.\n\n[1] Aloor, J. J., Nayak, S. N., Dolan, S., & Balakrishnan, H. (2024). Cooperation and Fairness in Multi-Agent Reinforcement Learning. ACM Journal on Autonomous Transportation Systems, 2(2), 1–25. doi:10.1145/3702012\n[2] Ju, P., Ghosh, A., & Shroff, N. B. (2023). Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/2306.00324"}, "questions": {"value": "Please see weaknesses. Some additional questions:\n* In Figure 3, what baseline is used? How are interventions done?\n* Is there a way to specify soft/hard constraints in this framework? Suppose some fairness objectives are extremely important, so the agent must satisfy them. Simply setting a very high weight on it may yield an overly conservative policy. Let me know if I missed this discussion anywhere in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VUB5WZ1LTR", "forum": "JodVUecxdF", "replyto": "JodVUecxdF", "signatures": ["ICLR.cc/2026/Conference/Submission22622/Reviewer_YFah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22622/Reviewer_YFah"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767598901, "cdate": 1761767598901, "tmdate": 1762942307121, "mdate": 1762942307121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAFE (Multi-Agent Fair Environments), a benchmarking suite designed to evaluate fairness-aware policies in multi-agent reinforcement learning settings. It provides three domain-specific environments: MAFE-Loan (financial lending), MAFE-Health (healthcare), and MAFE-Edu (education), and demonstrates their utility through experiments using an evolutionary strategy adapted for fairness-aware optimization."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. I acknowledge that there is currently a gap in the community for more sophisticated benchmarks for measuring fairness in dynamic environments, and this paper addresses that need.\n\n2. The modeling details, particularly those based on real data, represent substantial extensions over existing benchmarks such as D’Amour et al."}, "weaknesses": {"value": "1. The proposed algorithm is a straightforward adaptation of the cross-entropy method, and the environments are not benchmarked against existing fairness-aware RL algorithms cited by the authors. While algorithmic novelty is not essential for a benchmark paper, I believe the environments should still be evaluated using state-of-the-art fairness-aware RL algorithms to better demonstrate their utility. However, such comparison is completely missing from the paper and the environments are benchmarks on two standard MARL algorithms.\n\n2. From my understanding, all environments assume cooperation among agents. This is a major limitation, as such full cooperation is rarely the case in reality. Since the paper’s goal is to provide realistic benchmark environments, I see this as a significant issue. In addition, it would be valuable for these environments to include a single-agent variant, where other agents are controlled by the environment through logic or heuristics. This would increase the impact of the work by allowing the benchmark to be used for both single-agent and multi-agent research.\n\n3. The reported runs using an MLP, 100 episodes with 400 steps each (a total of 40k steps), take 2–3 days to complete. This seems surprisingly long, given that the simulations are based on relatively simple mathematical equations (with no rendering or complex physics, unlike many existing benchmarks). This could significantly limit the benchmark’s practicality and adoption. Most RL algorithms typically require 500k–1M steps for training, which would take more than a month at this rate!! The authors should optimize the runtime or provide simplified versions that can be completed in less than a day using an MLP model, if their goal is to have their benchmark be used by the community.\n\n4. The benchmark imposes certain unnecessary constraints on the algorithm and policy. For example, it requires the policy to be permutation equivariant. Such requirements add unnecessary complexity and reduce the benchmark’s practical usability. A good benchmark should be model agnostic and should not impose specific requirements like this on the algorithm.\n\n**Minor:** Many of the citations show the authors twice, like Atwood et al. Atwood et al. (2019).\n\n\n**Overall Assessment:** Aside from the weaknesses mentioned above, my main concern is whether this work is suitable for the main conference track, given its limited algorithmic contribution. I believe it would be a much better fit for a dataset/benchmark track, although such a track does not currently exist at this conference."}, "questions": {"value": "1. Why are the policies required to be permutation equivariant? How is this requirement imposed in the benchmark? If an algorithm that is not permutation equivariant outperforms one that is, what would that imply for the benchmark?\n\n2. Why do the runs take 2–3 days for only 40k environment steps? Could you provide profiling of how much time is spent on environment simulation versus the algorithm itself?\n\n3. What happens if agents have conflicting objectives and cannot cooperate? Can the benchmark handle such non-cooperative scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X9elnHddzi", "forum": "JodVUecxdF", "replyto": "JodVUecxdF", "signatures": ["ICLR.cc/2026/Conference/Submission22622/Reviewer_qhsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22622/Reviewer_qhsr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933561799, "cdate": 1761933561799, "tmdate": 1762942306887, "mdate": 1762942306887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAFE (Multi-Agent Fair Environments) a benchmark framework for evaluation fairness-aware algorithms in heterogenous multi-agent, multi-stage decision-making systems. Unlike existing fairness benchmarks that consider static or single-agent settings, MAFE models interacting agents whose joint decisions affect long-term population-level equity. The framework extends the Dec-POMDP formalism to include reward and fairness component functions, enabling defining more complex reward functions and group fairness disparities in outcomes and rates. The authors provide three realistic domains; healthcare, loan allocation, and education each inspired from real-world datasets supporting heterogenous agents and fairness measures. They also introduce a variant of the Cross-Entropy Method, and emprically show how different reward and fairness compositions yield different fairness-utility tradeoffs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a novel environment for studying fairness in heterogenous multi-agent sequential systems. Their component functions for observables provides a flexible design for reward and fairness computations.\n- The three provided environments are reflecting realistic scenarios and structures.\n- Experiments show that even siimple interventions on policies improve fairness performance over time."}, "weaknesses": {"value": "- The setting is limited to cooperative settings, where all the agents need to consider the same objective function. The current setting omits competitive or semi-cooperative dynamics that occur in real systems.\n- Only basic interventions are provided as baselines. While the environments show fairness-utility trade-offs through interventions, these are relatively simple and deterministic policies. The framework would be more interesting if it supported adaptive or policy-driven interventions that evolve with time."}, "questions": {"value": "- How would the framework handle scenarios involving multiple agents of the same type, such as several hospitals, schools, or insurance providers operating simultaneously? \n- The authors state that MAFE environments are built upon real-world datasets, but this dependency is not clearly described in the main text. Could authors clarify this?\n\nOther comments\n- There is a newer journal version of [1], where the authors extend their framework to a multi-agent setting and provide additional analysis and results [2] which could be more interesting for the scope of this paper.\nTypos\n- In related works, after line 88, all the references repeated. \"Yin et al. Yin et al. (2024) frame..\".\n\n[1] Bhagyashree Puranik, Upamanyu Madhow, and Ramtin Pedarsani. A dynamic decision-making framework promoting long-term fairness. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, pp. 547–556, 2022.\n[2] B. Puranik, O. Guldogan, U. Madhow and R. Pedarsani, \"Long-Term Fairness in Sequential Multi-Agent Selection With Positive Reinforcement,\" in IEEE Journal on Selected Areas in Information Theory, vol. 5, pp. 424-441, 2024, doi: 10.1109/JSAIT.2024.3416078"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FwSw8o5OaH", "forum": "JodVUecxdF", "replyto": "JodVUecxdF", "signatures": ["ICLR.cc/2026/Conference/Submission22622/Reviewer_Ebmy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22622/Reviewer_Ebmy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957744350, "cdate": 1761957744350, "tmdate": 1762942306418, "mdate": 1762942306418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}