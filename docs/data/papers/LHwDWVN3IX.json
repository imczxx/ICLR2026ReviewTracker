{"id": "LHwDWVN3IX", "number": 5378, "cdate": 1757905889501, "mdate": 1759897978849, "content": {"title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models", "abstract": "Text-to-image generation models (e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called EDITOR for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation. Code: https://anonymous.4open.science/r/EDITOR.", "tldr": "", "keywords": ["prompt inversion", "text-to-image diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49275b31801ffc9222a9755c01478377398cc677.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes EDITOER for inverting prompts in text-to-image diffusion models. Specifically, EDITOR first initializes a prompt using image captioning model, then optimize the text embedding of the initialized prompt based on the image reconstruction, and finally applies a pretrained embedding-to-text model to retrieve the prompt. Experiments show that the proposed method improves image similarity, textual alignment, and prompt interpretability, and can be effectively applied to various applications."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper clearly points out the challenge of current prompt inversion and proposes an effective solution.\n2.\tExtensive experiments validate the advancement of the proposed method. The application study is interesting and further shows the potential of the method."}, "weaknesses": {"value": "1.\tReverse-engineering for latent text embedding has been explored in previous study [1], but is not discussed in this work. Also, it would also be interesting to see the effect of initialization with the DDIM null text inversion mentioned in [1], which is widely used in the text-to-image model for maintaining the image content.\n2.\tThe key technical innovation locates in the embedding-to-text model, but it is based on a preliminary work, not newly proposed here. So, the technical innovation of this paper is marginal. \n3.\tThe description of the embedding inversion is unclear. (1) It needs to train two E2T models, $M_{corr}$ and $M_{zero}$?  (2) The training details of embedding-to-text model are missing, e.g., the number of training text-representation pairs. (3) The details of beam search.\n4.\tThe paper lacks an ablation study comparing the use of $M_{zero}$ only with using $M_{zero}$ and $M_{corr}$.\n5.\tLine 200 mentions “attacker”, but this term is neither explained nor referenced elsewhere in the paper.\n6.\tThere are errors/typos in the paper. (1) Line 284 $T(y)$ (2) Line 836, wrong description for Figure 9."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mwDJGoyQVh", "forum": "LHwDWVN3IX", "replyto": "LHwDWVN3IX", "signatures": ["ICLR.cc/2026/Conference/Submission5378/Reviewer_zXzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5378/Reviewer_zXzv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662936933, "cdate": 1761662936933, "tmdate": 1762918031252, "mdate": 1762918031252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDITOR, a prompt‑inversion pipeline for text‑to‑image diffusion models that aims to recover readable prompts that reliably reproduce a target image. EDITOR initializes from an image‑captioning prompt, optimizes the text encoder’s contextual embedding in continuous space to match the target image under a fixed seed, and converts the optimized embedding back to text with an embedding‑to‑text (E2T) model plus a small correction module."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Optimizing contextual embeddings after the transformer and deferring discrete text decoding to an E2T model is novel\n\n2. EDITOR improves image similarity and prompt interpretability/text alignment."}, "weaknesses": {"value": "1. EDITOR depends on a trained E2T module;  this adds implementation and computation costs.\n\n2. Mapping embedding to text to embedding may not be strict,  paraphrases can drift semantics. The extent to which this harms re-generation fidelity and editability is under-measured. Authors are suggested to give more details.\n\n3. Experiments focus on COCO/LAION/Flickr subsets.The scale of the dataset is relatively limited."}, "questions": {"value": "1. The pipeline introduces a trained embedding-to-text (E2T) model and a correction module, increasing complexity. The paper gives limited profiling of training time, memory, and sample efficiency.\n\n2. What about the performance on more datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uC3XiuW5N2", "forum": "LHwDWVN3IX", "replyto": "LHwDWVN3IX", "signatures": ["ICLR.cc/2026/Conference/Submission5378/Reviewer_SgN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5378/Reviewer_SgN7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847085871, "cdate": 1761847085871, "tmdate": 1762918030851, "mdate": 1762918030851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EDITOR, a prompt-inversion pipeline for text-to-image diffusion models that aims to produce prompts that are interpretable and effective at re-generating the target image. It (i) initializes from a caption, (ii) optimizes the contextual text embedding directly in the encoder’s continuous space to reconstruct the given image, and (iii) maps that optimized embedding back to fluent text via an embedding-to-text (E2T) model with a small beam-search correction. Experiments on COCO/LAION/Flickr and transfer tests (e.g., SDXL-Turbo, SD3.5-Medium) report consistent gains vs. PEZ/PH2P and caption-only baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clean, modular pipeline that others can readily reuse.\n- Better similarity and more fluent prompts than PEZ/PH2P and captioners across multiple datasets and model variants.\n- Produces prompts that are human-interpretable, aiding provenance/attribution and even downstream editing."}, "weaknesses": {"value": "- The method composes established components; the main idea (optimize contextual embeddings, then decode to text) is a practical tweak rather than a new paradigm.\n- Only 100 images per dataset is used for evaluation; it's unclear how stable gains are across broader distributions.\n- Protocol choices (initialization, token/step budgets) could affect PEZ/PH2P competitiveness; a standardized compute budget table would be good to have."}, "questions": {"value": "1. Add confidence intervals/paired tests and per-image scatter plots for key tables to show variance/outliers.\n2. Provide per-stage cost (init, inversion by iteration, etc.), plus scaling with prompt length and denoising steps.\n3. Provide sensitivity analysis to noise seeds and to the choice/mixture of caption initializers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FGMoypMcWf", "forum": "LHwDWVN3IX", "replyto": "LHwDWVN3IX", "signatures": ["ICLR.cc/2026/Conference/Submission5378/Reviewer_H81s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5378/Reviewer_H81s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938868134, "cdate": 1761938868134, "tmdate": 1762918030208, "mdate": 1762918030208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for generating fluent, human-readable prompts. The approach begins by initializing with prompts produced by an image captioning model, which serve as a starting point for optimizing embeddings within the continuous latent space of diffusion models. These optimized embeddings are then transformed back into natural language using an embedding-to-text model. The effectiveness of the proposed technique is demonstrated through experimental comparisons with PEZ and PH2P across the MS COCO, LAION, and Flickr datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses an interesting problem of reverse engineering diffusion models. \n\n2.Comprehensive evaluations and ablations show the effectiveness of the approach in comparison to prior work."}, "weaknesses": {"value": "1.The work has limited novelty in the sense that it combines the gradient based optimization of prior work with the latent space of an existing model. \n\n2.The notations and equations are incorrect. The cross-entropy loss and the MLE loss are not correctly defined in equation 4 and 6. \n\n3. The approach does not consider recent multimodal architectures such as SD3.\n\n4. Comparison to recent prompt inversion/search techniques such as [1].\n[1] STEPS: Sequential Probability Tensor Estimation for Text-to-Image Hard Prompt  Search. CVPR 2025."}, "questions": {"value": "1. Can the authors revisit and clarify the notations and equations. What is the difference between text decoder D and M.textdecoder in the algorithm. Also image is denoted by \\mathbf{x} or $x$ at places. \n\n2. How does the approach extend to embedding spaces of the more recent architectures like SD3? Can this be mapped to the embedding of the captioning model? \n\n3. How does it compare to the recent work on prompt search which also generates human readable prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WpukdRahVd", "forum": "LHwDWVN3IX", "replyto": "LHwDWVN3IX", "signatures": ["ICLR.cc/2026/Conference/Submission5378/Reviewer_Kp1H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5378/Reviewer_Kp1H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053130018, "cdate": 1762053130018, "tmdate": 1762918029727, "mdate": 1762918029727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}