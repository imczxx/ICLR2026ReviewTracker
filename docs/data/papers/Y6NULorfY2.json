{"id": "Y6NULorfY2", "number": 10271, "cdate": 1758165635548, "mdate": 1763475903600, "content": {"title": "Online Budget-Aware Guidance for Blockwise Discrete Text Diffusion", "abstract": "Diffusion language models (DLMs) are emerging as strong alternatives to autoregressive decoders, combining iterative refinement, global context access, and natural controllability. Yet controllable text diffusion remains underexplored: unlike vision, where plug-and-play guidance is central, existing methods for text rely on fixed or decaying step-size schedules that are brittle across prompts, constraints, and remasking strategies. The problem is amplified in modern blockwise DLMs, where gradient volatility and coverage fluctuate across spans, making schedule-based heuristics unstable. \nWe present BALE(Budget-Aware Logit Editing), a plug-and-play controller that replaces heuristic schedules with an online, budget-aware rule. At each step, BALE fuses multi-constraint gradients and selects guidance strength by balancing a residual-targeted movement rule, stability caps based on an EMA Lipschitz proxy and a KL surrogate. Beyond per-step control, BALE allocates a fixed sequence-level budget across blocks and reallocates dynamically at boundaries. A discrete-time KL bound and a continuous-time SDE view explain why BALE stabilizes blockwise decoding. \nOn LLaDa-8B, BALE improves lexical (must-have, forbidden), semantic (sentiment, formality), and combined controls over prompt tuning and schedule-based baselines, yielding higher satisfaction rates at matched or lower perplexity and stronger human/LLM preference. BALE is lightweight, scheduler-free, and advances controllable text diffusion toward practical deployment.", "tldr": "We present BALE, a plug-and-play controller for diffusion LMs that adaptively allocates guidance, improving controllability without hurting fluency.", "keywords": ["Diffusion Language Models", "Constrained Generation", "Online Control", "Scheduler-free", "BALE"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ced901a001dee10f9aafa0f5fef5547101509952.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In blockwise discrete text diffusion, guidance scales $\\lambda$ are usually set by fixed/decaying schedules. These are brittle as curvature spikes at block starts, changing coverage within blocks, and heterogeneous constraints cause overshoot or under-editing. This paper treats guidance as an online, budget-aware control problem. Replace the hand schedules with a plug-and-play controller that: enforces a global KL edit budget and partitions it blockwise with dynamic reallocation at block boundaries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Interesting module. BALE turns \"pick a $\\lambda$-schedule into an online constrained control problem with an explicit KL budget and dual update.  The controller reasons at the same granularity DLMs operate (blocks with inner steps), with dynamic reallocation at boundaries, tacking curvature spikes and coverage shifts that fixed schedules can't. On semantic control (WritingPrompts) and lexical control (CommonGen-style), BALE+PT improves success/accuracy while keeping R_PPL competitive, i.e., better controllability and fluency trade-offs."}, "weaknesses": {"value": "See below questions. Too few analysis on hyperparameter analysis for an experimental paper."}, "questions": {"value": "1. How many trials are repeated for all experiments? What is the standard deviation/error across trials? At least reporting more than 3 seeds. If missing standard deviation, table 1 reports WritingPrompts semantic control is not convincing as the numbers are very close. If missing standard deviation/error, then there is no statistical significance in this prompt experiment as most prompt tuning experiments have numbers very close. Any harder tasks on prompt tuning?\n2. What is the wall-clock time and memory overhead reported for this module and all baselines?\n3. What is the hyper parameter analysis on the total guidance budget $B$? As the title shows, online budget aware, the authors should plot success vs R_PPL and cumulative KL spent to show under different budget, how all metrics play together for both BALE and other baselines.\n4. For Equation (15), how $\\rho$ is determined? Any hyperparameter analysis? $\\rho \\in \\{ 0.6, 0.8, 1.0, ... \\}$ while holding $B$ fixed? Just picking 0.5 and 1.5 seems sloppy.\n5. For Equation (7) skipping or not skipping, can you quantify the tradeoff: stability (variance across seeds, again repeated experiments at least 3 times), success, and R_PPL. This isolates the value of dynamic sharing vs. frozen allocation. Can you put a variance table showing mean $\\pm$ std across 3-5 trials for the best setting vs frozen allocation for stability?\n6. For Equation (8), a simple weighted sum or an orthogonalized variant, can the authors when to do sum when to orthogonalized variant? \n7. Tasks are too simple for the prompt tuning experiments. Given nowadays other papers like GEPA or TextGrad already choose very hard tasks. Can you choose tasks on lower than 90% accuracy for all the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dRZoW1geOI", "forum": "Y6NULorfY2", "replyto": "Y6NULorfY2", "signatures": ["ICLR.cc/2026/Conference/Submission10271/Reviewer_HGar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10271/Reviewer_HGar"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938451232, "cdate": 1761938451232, "tmdate": 1762921624991, "mdate": 1762921624991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BALE, an online, plug-and-play controller for blockwise discrete text diffusion models (DLMs). The core problem it addresses is that existing guidance methods, which rely on fixed or heuristic schedules for the guidance strength ($\\lambda_t$), are \"brittle\" and fail to adapt to the changing dynamics of different prompts, constraints, and blocks.\n\nBALE's solution is to replace the fixed schedule with an online rule that picks $\\lambda_t$ at every step. This rule is a 3-way-min that balances: (i) a target step size to reduce the current constraint violation (the \"residual\"), (ii) a stability cap based on an EMA of the gradient (a Lipschitz proxy) to prevent exploding steps, and (iii) a budget cap based on a remaining KL-divergence allowance for the current block. The method also proposes a way to distribute a total sequence-level budget across the different blocks.\n\nExperiments on LLaDa-8B show that BALE achieves higher constraint satisfaction (for lexical, semantic, and combined control) than standard schedule-baselines (constant, linear, cosine) while maintaining competitive fluency, a finding also supported by human evaluation.\n\nI am willing to increase my score if the authors can satisfactorily address the key concerns outlined below."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1.  Important Problem: The paper is right on the money: the brittleness of heuristic schedules is a real and annoying problem for controllable diffusion, and the blockwise setup makes it even worse. Tackling this is a valuable contribution.\nS2.  Principled Design: The core idea of replacing a fixed schedule with an online, 3-part (target, stability, budget) controller is smart. It's a much more principled way to think about guidance than just \"let's decay $\\lambda_t$ with a cosine.\"\nS3.  Block-Aware: I appreciate that the method doesn't just apply a generic controller. It specifically accounts for the blockwise architecture by proposing a block-level budget allocation (Eq. 6) and reallocation (Eq. 7).\nS4.  Solid Empirical Results: The experiments are thorough (auto + human eval) and clearly demonstrate that BALE consistently outperforms the standard baselines across a good variety of tasks."}, "weaknesses": {"value": "W1.  Swapping One Magic Number for Another: The paper heavily criticizes \"pre-tuned constants\" in other methods, but then introduces its own new magic number: the global, sequence-level budget $B$ (which is set to `3` in Appendix C). The paper provides no justification for this value, no sensitivity analysis, and no discussion of how one would set $B$ for a new task or model. Why is introducing and tuning $B$ fundamentally better than just tuning the global scale of a standard cosine schedule? It feels like we've just swapped one problem for another.\nW2.  Sketchy KL Surrogate Derivation: The theoretical justification for the budget feels shaky. The paper starts with a KL upper bound in Eq. 4 ($D_{KL} \\le \\frac{\\lambda^2}{8}||g||_2^2$) but the actual cost function used in Eq. 11 is $b_t^{KL} = \\frac{1}{4}\\frac{\\lambda_{t}^{2}g_{2,t}}{r_{t}}$. This jump seems to have several holes:\n     $g_{2,t}$ is the average curvature ($\\frac{1}{V}\\sum g_v^2$), so $||g||_2^2 = V \\cdot g_{2,t}$. Where did the vocab size $V$ go?\n     Why did the coefficient change from 1/8 to 1/4?\n     The division by $r_t$ (coverage) is just hand-waved in Sec 2.3 (\"scales as...\") rather than being derived from Eq. 4. This makes Eq. 11 feel less like a \"KL surrogate\" and more like a new, complex heuristic inspired by KL.\nW3.  What's the Point of Dynamic Reallocation? The paper introduces a mechanism for dynamic budget reallocation between blocks (Eq. 7) but then calls it \"Optional\" in Algorithm 2. The analysis (like Fig 3, right) only shows within-block spending and never clarifies if this reallocation was even on for the main experiments. There's no ablation for it. Is this feature actually doing any work, or is it just noise?\nW4.  Unfair Apples-to-Oranges Comparison: The comparison to AR models in Sec 5.4 / Table 6 is misleading. It compares BALE+Diffusion (LLaDa) against DeAL+AR (Mistral). This compares the backbone and the controller at the same time, telling us nothing about the controller's merits. The paper claims in Sec 3.7 that BALE can apply to AR decoding, so a fair comparison would have been to run BALE on the same Mistral-7B backbone."}, "questions": {"value": "Q1.  KL Surrogate: Can you please walk me through the exact derivation from the KL bound in Eq. 4 to the final cost function in Eq. 11? Specifically, please justify: (a) Why the vocab size $V$ (implicit in $g_{2,t}$) disappears, (b) why the constant 1/8 becomes 1/4, and (c) how division by $r_t$ is formally derived, rather than just intuitively motivated.\nQ2.  Global Budget $B$: How did you choose the global budget $B=3$? How sensitive is the model's performance to this single hyperparameter? Can you please make a concrete argument for why tuning $B$ is fundamentally superior to tuning the global scale of a simpler cosine schedule?\nQ3.  Dynamic Reallocation: Was the \"Optional\" dynamic budget reallocation (Eq. 7) enabled for the main results in Tables 1-3? If yes, can you please provide an ablation study showing its benefit over the static-only allocation (Eq. 6)? If no, why was it included as a component of the method?\nQ4.  AR Comparison: Since you state BALE is applicable to AR models (Sec 3.7), why did you not provide a fair, apples-to-apples comparison by running BALE on the Mistral-7B backbone against DExperts/DeAL? The current comparison in Table 6 seems inconclusive.\n\n### Additional Comments:\n\nC1.  In Sec 3.4, the stability cap $\\lambda_{cap}$ depends on an EMA of the pre-normalized gradient $g_t$ . But in Sec 3.3, the fused gradient $g_t$ is immediately RMS-normalized. Does this mean you have to keep a separate, un-normalized copy of the gradient just for the EMA tracker? Please clarify the flow.\nC2.  Eq. 14 for $\\lambda_{target}$ looks suspicious. You divide the residual $\\Delta_t^*$ by $g_{2,t}$ (the average squared gradient). Shouldn't you divide by a norm of the gradient (like $||g_t||_1$ or $||g_t||_2$) to get a step size? This looks like a unit mismatch. Please double-check this formula."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qdOX5onXRT", "forum": "Y6NULorfY2", "replyto": "Y6NULorfY2", "signatures": ["ICLR.cc/2026/Conference/Submission10271/Reviewer_NboQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10271/Reviewer_NboQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993024995, "cdate": 1761993024995, "tmdate": 1762921624478, "mdate": 1762921624478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes an algorithm for controlled generation of diffusion LLMs that leverages: \n- Adaptive gradient scaling for stability \n- Novel budgeting algorithm for allocating KL divergence from base distribution across blocks\n\nDemonstrates performance relative to pure prompt tuning + simple constant, linear, and cosine guidance schedules on lexical constraints, semantic constraints, and both lexical + semantic constraints. Provides ablation demonstrating the necessity of individual components. ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Plug and Play Guidance for Large Scale Diffusion Models**: \nThis paper applies plug and play guidance for discrete diffusion at the 8 billion parameter scale, which (to the best of my knowledge) has not been done before. \n\n**Ablations**: The ablations are quite extensive, which provides insight into what components of the algorithm are relevant. They also ablate over different backbones, showing that BALE generalizes across different diffusion models. I very much appreciated the dedicated section to analyzing the performance of the algorithm in terms of the individual components.  \n\n**Controlled generation for block diffusion**: While guided generation has been explored in the context of diffusion language models, I am unaware of works that directly address the difficulties associated with block diffusion and external constraints."}, "weaknesses": {"value": "**Missing Literature**: This paper does not mention or cite [1], which has discrete diffusion guidance as a primary contribution. While [1] does not examine performance on LLada or larger diffusion models, it is still important to clarify how the proposed method BALE compares to this work as they are directly related. \n\n**Incompatible Math Framing**: This submission makes connections to continuous diffusion theory in section 2.2, but these connections are quite tenuous. I do not see how making this connection provides any intuition, since they are different mathematical processes entirely: the prior in mask diffusion is a dirac over the mask token with no mass anywhere else, and the prior in Gaussian diffusion is a Gaussian with support everywhere. Brownian motion is infinitesimally “spiky” (there are always small, tiny movements”, but the CTMC that governs masked diffusion has exactly one jump — from the original token to the mask token — per position. \n\n**Empirical Performance**: The method seems to provide marginal gains over simple prompt tuning (2%) on the lexical constraint, and I feel that this gives important insight into what constraints this method is best suited for. For constraints like keyword coverage, perhaps simple prompt tuning is sufficient. Also, cosine decay is missing from table 2, despite being characterized as the strongest baseline. There doesn’t seem to be any justification as to why cosine decay is excluded for lexical control. \n\n**Contribution**: The gradient re-scaling seems to be a good implementation of common knowledge (gradient updates that are too big lead to instability). The novel contribution seems to be the adaptive budgeting of KL divergence both within and across blocks.  The scheduling algorithm seems to rely on heuristics and well-executed engineering modifications as opposed to insights that advance our fundamental understanding of discrete diffusion guidance. \n\n[1] Simple Guidance Mechanisms for Discrete Diffusion Models. Schiff et al. ICLR 2025."}, "questions": {"value": "How much of the gains against the gradient-based baselines in Tables 1, 2, and 3 come from the schedule v.s come from the gradient smoothing / rescaling? In Table 4, it seems that removing the gradient cap is far more impactful than removing the budget cap (1.5% decrease v.s 3% decrease, almost double). If the constant, linear and cosine schedules were used in conjunction with gradient scaling, how would BALE compare? \n\nWhat does “RMS curvature” mean? Is it actually the curvature of the gradient trajectory, or is just the gradient volatility? From what I understand, computing curvature would mean a second order gradient computation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JLRPhgxMR1", "forum": "Y6NULorfY2", "replyto": "Y6NULorfY2", "signatures": ["ICLR.cc/2026/Conference/Submission10271/Reviewer_18cY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10271/Reviewer_18cY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112530472, "cdate": 1762112530472, "tmdate": 1762921623826, "mdate": 1762921623826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a scheduler for the guidance strength (gradient drift) in controllable text generation with block diffusion. Specifically, the authors propose BALE, a budget-aware logit editing guidance controller that dynamically allocates guidance strengths across the blocks and steps. Human evaluation and automatic metrics show that BALE achieves good controllability in lexical and semantic constraints and also combined tasks, compared to static scheduling of guidance or AR baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper tackles an important problem in the plug-and-play-style control in controllable text generation: how to decide the optimal strength of classifier guidance. The experiments show advantages of the proposed BALE method compared to several popular ways of conducting control (e.g., constant or fixed-schedule lambda, or with AR models). The analysis shows the method is indeed dynamically allocating strengths of guidance."}, "weaknesses": {"value": "(1) The motivation and practicality of using a global control \"budget\". What is the motivation behind having a global budget B in the first place? How was an optimal value of global budget B decided in realistic/more general tasks and use cases?\n\n(2) The main experiments were done on sentiment, style, and lexical control tasks. These are good illustrative examples but could be too simple especially given the progress in LLMs generally. Could the method be used on more advanced controls, e.g., maximizing rewards from reward models or human preference?\n\n(3) Baseline comparisons. The main experiments compared with vanilla approaches like a constant lambda. How many constant lambda's were tried and used in these experiments? Is there any exploration to make sure the selected values are optimal for each task?"}, "questions": {"value": "Please see the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "haCecoIL16", "forum": "Y6NULorfY2", "replyto": "Y6NULorfY2", "signatures": ["ICLR.cc/2026/Conference/Submission10271/Reviewer_E7Z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10271/Reviewer_E7Z3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162109063, "cdate": 1762162109063, "tmdate": 1762921623427, "mdate": 1762921623427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}