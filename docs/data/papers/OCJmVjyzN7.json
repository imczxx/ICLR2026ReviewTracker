{"id": "OCJmVjyzN7", "number": 2007, "cdate": 1756976026509, "mdate": 1759898174519, "content": {"title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-body Loco-manipulation Control", "abstract": "Humanoid robots require precise locomotion and dexterous manipulation to per-form challenging locomanipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the scarcity of data capturing unified loco-manipulation behaviors, and (2) the limited precision and stability of existing reinforcement learning (RL) controllers, which arises from redundant velocity-tracking objectives and unstructured upper-body perturbations. To address these limitations, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. For data scarcity, we employ unified latent learning to extract manipulation-aware locomotion knowledge from low-cost video data. Moreover, an efficient data collection pipeline is devised to augment the dataset and scale the benefits. Furthermore, WholeBodyVLA is equipped with a loco–manipulation–oriented (LMO) RL policy, which replaces the continuous command interface with a discrete one and integrates structured manipulation-induced perturbations. Such a design enables the humanoid to advance, turn, and squat with high precision while maintaining stability under diverse manipulation-induced disturbances. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco–manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. Our method demonstrates robust multi-tasking capabilities and strong generalization. Code and checkpoints would be made public.", "tldr": "We present WholeBodyVLA, a unified VLA framework enabling large-space humanoid loco-manipulation via unified latent learning and loco–manipulation–oriented RL.", "keywords": ["Humanoid Robots", "Vision-Language-Action Model", "Locomotion and Manipulation", "Whole-Body Control", "Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/579ba365d7c59e61b47f4b8efa00c06c9b2e22ea.pdf", "supplementary_material": "/attachment/1a39b36cc340f499c513cbd270e625c74b297f42.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents WholeBodyVLA, a unified framework for humanoid loco-manipulation. The framework consists of two main components: Unified Latent Learning and the Loco-Manipulation-Oriented (LMO) RL Policy. In Unified Latent Learning, two separate VQ-VAE–based latent models are trained on manipulation and locomotion videos. The LMO policy is a reinforcement learning controller that executes discrete locomotion commands and adopts a two-stage curriculum to enhance training stability and precision. During inference, the VLA model decodes latent actions into upper-body joint angles and locomotion commands, which the LMO policy translates into torques for the robot’s lower body. Experiments conducted on the AgiBot X2 humanoid across three real-world loco-manipulation tasks demonstrate that WholeBodyVLA outperforms both modular and end-to-end baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an interesting and timely topic in humanoid robot control, focusing on whole-body loco-manipulation, which is of high relevance to the embodied AI and robotics communities.\n\n- The proposed method is conceptually straightforward and well structured, making it relatively easy to reproduce and extend in future research.\n\n- The experimental results demonstrate that the proposed method outperforms several strong baselines, indicating the effectiveness and practicality of the approach.\n\n- The paper includes comprehensive ablation experiments that provide necessary insights into the contribution of each component."}, "weaknesses": {"value": "The paper could benefit from clearer elaboration and justification of certain key arguments:\n- The authors identify data scarcity and instability in RL controllers as the two main challenges in humanoid loco-manipulation. However, the connection between data scarcity and degraded loco-manipulation performance is not sufficiently explained or empirically verified. It would be helpful to clarify how the lack of data leads to poor generalization and to provide stronger evidence that the proposed use of video-based learning effectively mitigates this limitation.\n- Some sections of the method description could be written more clearly, particularly regarding model training details and implementation specifics (see “Questions” section for further comments).\n\nThe locomotion component is relatively simple, relying on a small set of discrete motion primitives. This limits the expressiveness and adaptability of the policy for more complex movements.\n\nThe experimental evaluation could be improved by including longer-horizon tasks that require extended locomotion or sequential coordination, which would better demonstrate the robustness and scalability of the proposed framework."}, "questions": {"value": "- Is my understanding correct that the LAMs are trained using human egocentric videos, while the VLA (or VLM) is trained using teleoperation data? If so, could the authors clarify why and how human and robot data can share the same latent action space?\n\n- Would it be possible to pretrain the LAM using the teleoperation dataset’s camera videos instead of (or in addition to) human egocentric data? This might provide a more consistent distribution between perception and control domains.\n\n- Regarding the data scarcity issue, is the problem primarily addressed by pretraining the LAM on large-scale human video data to provide latent supervision for robot policy learning? Please confirm if this interpretation is correct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OBqCPLp6uC", "forum": "OCJmVjyzN7", "replyto": "OCJmVjyzN7", "signatures": ["ICLR.cc/2026/Conference/Submission2007/Reviewer_1asV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2007/Reviewer_1asV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462092153, "cdate": 1761462092153, "tmdate": 1762915988956, "mdate": 1762915988956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WholeBodyVLA, a unified Vision-Language-Action (VLA) framework that enables bipedal humanoid robots to perform large-space loco–manipulation tasks (e.g., squatting to grasp, turning to place, pushing 50 kg carts).\nThe method integrates two key innovations: 1. Unified Latent Learning (ULL) — jointly learns locomotion and manipulation latent actions from action-free egocentric videos. 2. Loco–Manipulation–Oriented (LMO) RL Policy — replaces conventional continuous velocity tracking with a discrete command interface and introduces structured upper-body perturbations from manipulation data to improve balance and stability. Experiments on the Agibot X2 humanoid show that WholeBodyVLA outperforms modular and end-to-end baselines (e.g., GR00T N1.5, OpenVLA-OFT) by 21.3–24 % in success rate, while maintaining robust multi-tasking and visual generalization under unseen conditions (e.g., different box contents or weights)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper clearly identifies the missing link between manipulation-aware locomotion and VLA-based manipulation, proposing the first end-to-end unified framework that integrates both in real-world humanoid control.\n2.\tThe discrete command interface + structured perturbation RL is elegantly engineered. The two-stage curriculum and well-defined reward shaping (directional accuracy, stand-still penalty) demonstrate deep insight into locomotion stability and precision.\n3.\tEvaluation on multiple real-world tasks (bag packing, box loading, cart pushing) and ablations (without RL, with velocity-based RL, with shared LAM, etc.) show both component-wise contributions and robustness. The use of both real-robot and MuJoCo simulations strengthens credibility."}, "weaknesses": {"value": "1.\tThe upper-body movements demonstrated in the three tasks appear quite limited — the shoulder seems to move primarily along the pitch axis. However, since the tabletop objects can be positioned in more diverse locations, introducing richer interaction motions could better showcase the capabilities of the VLA.\n2.\tThe current tabletop task is rather simple and singular. Did the authors also collect additional related data that could be used to supplement or expand the current manipulation tasks?"}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oIzGqjokHu", "forum": "OCJmVjyzN7", "replyto": "OCJmVjyzN7", "signatures": ["ICLR.cc/2026/Conference/Submission2007/Reviewer_76f3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2007/Reviewer_76f3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897490627, "cdate": 1761897490627, "tmdate": 1762915988837, "mdate": 1762915988837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WholeBodyVLA, a unified VLA framework for end-to-end humanoid loco-manipulation on Agibot X2. To address data scarcity for manipulation-aware locomotion, the authors introduce unified latent learning with two VQ-VAE latent action models (separately for manipulation and locomotion). For precise and stable execution, a Loco–Manipulation–Oriented (LMO) RL controller with a discrete command interface is trained as the lower-body controller. Real-robot experiments on three suites (bag packing, box loading, cart pushing) are reported, where WholeBodyVLA achieves higher success rates than modular and open-source end-to-end baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear paper writing and problem formulation. The paper identifies the gap between modular pipelines and true end-to-end manipulation-aware locomotion, motivating why coupling is necessary for stability and task success.2. Unified latent learning with modality-separated LAMs. The paper identified the sub-optimality of using mixed-data to train a single LAM and propose to train separate LAMs for manip and locomotion\n2. Separation of latent action spaces. Training distinct latent action models for manipulation/locomotion is a sensible design choice that addresses mixed-motion dataset and improves learning for the high-level policy.\n3. Lower-body controller tailored for loco-manipulation. The LMO RL controller with a discrete command interface is well aligned with whole-body objectives (heading, position, stance), and the design is empirically validated.\n4. Real-robot evaluation. Experiments on physical hardware across multiple task families strengthen external validity and demonstrate the practicality of the method."}, "weaknesses": {"value": "1. Ambiguity in loco-manipulation demands of the tasks. In all three demos, target objects appear within immediate arm’s reach at the start, so grasping could be completed without meaningful locomotion. Even though the side stepping motion is stable, it is still difficult to determine whether improvements arise from integrated loco-manipulation versus decoupled manipulation with stepping. \n2. Baseline fairness. Some baselines were not evidently fine-tuned on the same Agibot/LMO data or adapted to the same low-level interface. Since GR00T was originally trained with the data collected by NVIDIA's own low-level controller, directly plugging into LMO can be suboptimal or OOD. A stronger comparison would fine-tune GR00T/OpenVLA on the same teleop/robot dataset (of AgiBot) and interface, with matched training distribution.\n3. Limited analysis of latent-learning/data scaling. The paper motivates latent action learning as a solution for data scarcity, but there is no study of dataset size (with/without action labels) versus performance, nor a comparison of scaling first-person video versus teleop data. This limits conclusions about where latent learning delivers the biggest gains."}, "questions": {"value": "1. Could you include real-world trials with ≥1 m initial separation (for example, robot stand 1m away from the table/cart) for all demos? Plots of success vs. start distance would help.\n2. For GR00T/OpenVLA baselines, can you fine-tune on the identical Agibot/LMO data? If already done, please add these details.\n3. Can you provide scaling curves for (a) total hours of action-free video for latent learning and (b) hours of robot/teleop data, and ablate the contribution of each?\n4. The current low-level LMO policy appears tuned for flat-ground walking and squatting motions. How does the approach generalize to more complex whole-body coordination tasks such as stair climbing, uneven terrain traversal, or heavy object lifting (which is common for loco-manipulation) that require significant upper–lower body coupling? If such generalization is not yet supported, could the authors discuss what architectural or training changes would be necessary to extend the controller to these regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BFYFJ3IC5E", "forum": "OCJmVjyzN7", "replyto": "OCJmVjyzN7", "signatures": ["ICLR.cc/2026/Conference/Submission2007/Reviewer_N1iB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2007/Reviewer_N1iB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958774191, "cdate": 1761958774191, "tmdate": 1762915988624, "mdate": 1762915988624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WholeBodyVLA addresses the challenge of enabling humanoid robots to perform large-space whole-body loco–manipulation, which requires tight integration of precise locomotion and dexterous manipulation. The authors identify two key limitations in prior work: (1) the scarcity of data that jointly captures manipulation-aware locomotion, and (2) the instability and imprecision of existing RL controllers due to continuous velocity-tracking objectives and unstructured upper-body perturbations. To overcome these, they propose a unified latent learning framework that trains separate Latent Action Models (LAMs) for manipulation and locomotion on low-cost, first-person videos, thereby enabling a vision-language-action (VLA) policy to jointly predict upper- and lower-body commands. For stable execution, they introduce a Loco–Manipulation–Oriented (LMO) RL policy featuring a discrete command interface (e.g., forward/turn/squat) and structured perturbations derived from real manipulation trajectories. Evaluated on the AgiBot X2 humanoid, WholeBodyVLA autonomously completes multi-step loco–manipulation tasks, including squatting to grasp a box, placing it onto a cart, and pushing a load exceeding 50 kg. The method outperforms strong end-to-end and modular baselines by 21.3% on average and demonstrates robust generalization under unseen visual conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\n\nWholeBodyVLA introduces a unified vision-language-action framework for humanoid loco-manipulation, extending latent action learning from tabletop manipulation to full-body coordination. The work’s key novelty lies in the dual-LAM design, which explicitly separates locomotion and manipulation latent spaces to mitigate conflicts between camera ego-motion and hand–object motion—an insightful adaptation that broadens latent learning’s applicability. Additionally, the proposed Loco-Manipulation-Oriented (LMO) reinforcement learning scheme combines a discrete command interface and structured upper-body perturbations to achieve precision, start–stop control, and directional stability, marking a tailored optimization for humanoid whole-body control. While the system’s components (VQ-VAE, DINOv2, RL control) are not inherently new, their orchestration for large-space humanoid manipulation represents a meaningful, though evolutionary, advance with the first systematic real-world validation.\n\n**Quality**\n\nExperiments cover three multi-step real-world tasks with ablations on control precision, stability, and generalization under visual perturbations. While comprehensive, the analysis lacks deeper investigation into sample efficiency, LAM hyperparameter sensitivity, or scalability beyond ~50 teleoperated demos per task. The results validate feasibility but not robustness or data efficiency, limiting the strength of empirical claims. \n\n**Clarity**\n\nThe paper is well-structured and includes helpful runtime details (e.g., 10 Hz VLA, 50 Hz RL). However, critical parameters (codebook size, temporal stride k, reward weights) are buried in appendices, hindering reproducibility. Moreover, the interaction between locomotion and manipulation latents—central to the “unified” claim—remains underspecified, reducing interpretability.\n\n**Significance**\n\nThe work addresses a key challenge in humanoid autonomy: manipulation-aware locomotion in a unified perception-to-control pipeline. Its real-world demonstration of multi-step, heavy-load tasks offers tangible engineering significance. While not a conceptual leap, it represents an important integration step toward practical deployment."}, "weaknesses": {"value": "**Insufficient Evidence for “Unified” Whole-Body Control**\n\nDespite the “unified” claim, the system decouples upper-body manipulation and lower-body locomotion: the VLA predicts separate manipulation and locomotion latents, a lightweight decoder outputs arm joint targets and a discrete locomotion command, and the LMO policy executes lower-body torques. This constitutes co-scheduled rather than end-to-end joint whole-body control, relying on downstream modules to handle cross-coupling.\n\n**Limited Generalization and Robustness Evaluation**\n\nVisual generalization is evaluated only under narrow appearance or load variations, such as unseen bags or cart weights. However, critical distribution shifts, including changes in scene layout, terrain friction, lighting, occlusion, or moving obstacles, are not tested, which limits the assessment of real-world robustness.\n\n**Non-standardized Tasks & Metrics Hinder Comparability**\n\nAll three tasks (bag packing, box loading, cart pushing) are custom-designed with ad-hoc subgoal scoring, lacking standardized benchmarks or community-agreed metrics. This hinders reproducibility and cross-method comparison.\n\n**No Failure Mode Analysis**\n\nTable 2 shows numerous subgoal-level failures across phases, yet the paper provides no breakdown of failure modes such as approach, grasp, turn, squat, or push to identify dominant bottlenecks."}, "questions": {"value": "**Questions**\n\n**Q1: Failure Mode Distribution** – In the reported 25×6 subgoal evaluation, what proportion of failures arises from each stage (approach, grasp, turn, squat, push)? Can the authors provide a quantitative breakdown and correlate it with LMO directional and stance-height errors to reveal whether locomotion deviations systematically cause manipulation failures?\n\n**Q2: Visual Generalization Boundary** – Beyond the tested appearance and load perturbations, has the system been evaluated under changes in ground friction, slope, lighting, occlusion, or dynamic obstacles? What are the success rates and degradation patterns under such environmental variations?\n\n**Suggestions**\n\n**S1: Detailed Failure Taxonomy** – Include a frequency breakdown of failures by task stage and relate them to quantitative LMO metrics (heading deviation, height tracking). This would help identify dominant failure patterns and clarify coupling between locomotion and manipulation.\n\n**S2: Broader Robustness Evaluation** – Extend experiments to cover terrain/friction variation, slopes, lighting/occlusion, and moving obstacles, providing explicit success curves or degradation trends to substantiate robustness beyond appearance shifts.\n\n**S3: Include standardized benchmarks and metrics** – Complement author-defined tasks with community-recognized scenarios and quantitative metrics (e.g., path/pose accuracy, time-to-goal, safety violations), enabling reproducible cross-paper comparison beyond the custom subgoal scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RsAlrYC2j2", "forum": "OCJmVjyzN7", "replyto": "OCJmVjyzN7", "signatures": ["ICLR.cc/2026/Conference/Submission2007/Reviewer_C1fR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2007/Reviewer_C1fR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994674046, "cdate": 1761994674046, "tmdate": 1762915988415, "mdate": 1762915988415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}