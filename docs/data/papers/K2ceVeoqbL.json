{"id": "K2ceVeoqbL", "number": 24715, "cdate": 1758359607100, "mdate": 1759896753024, "content": {"title": "DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation", "abstract": "We present DexCanvas, a large-scale hybrid real-synthetic human manipulation dataset containing 7,000 hours of dexterous hand-object interactions seeded from 70 hours of real human demonstrations, organized across 21 fundamental manipulation types based on the Cutkosky taxonomy. Each entry combines synchronized multi-view RGB-D, high-precision mocap with MANO hand parameters, and per-frame contact points with physically consistent force profiles. Our real-to-sim pipeline uses reinforcement learning to train policies that control an actuated MANO hand in physics simulation, reproducing human demonstrations while discovering the underlying contact forces that generate the observed object motion. DexCanvas is the first manipulation dataset to combine large-scale real demonstrations, systematic skill coverage based on established taxonomies, and physics-validated contact annotations. The dataset can facilitate research in robotic manipulation learning, contact-rich control, and skill transfer across different hand morphologies.", "tldr": "A large-scale dataset of human manipulation demonstrations with physically consistent contact force annotations for training dexterous robotic hands.", "keywords": ["robot learning", "dexterous manipulation", "reinforcement learning", "physics simulation", "human demonstrations", "datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3068a47646e498d9a47fb4ae80ce35eb7deed34b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "DexCanvas presents a 7,000-hour large-scale hybrid real-synthetic dataset of human dexterous manipulation, seeded from 70 hours of motion-captured demonstrations. The dataset is structured across 21 manipulation types (per the Cutkosky taxonomy) and augments each motion with physics-validated contact forces via a “real-to-sim RL pipeline” that trains actuated MANO hand policies to reproduce demonstrations in simulation, thereby recovering per-frame contact forces. This bridges gaps between pure mocap and synthetic data, providing the first large dataset combining multi-view RGB-D, precise kinematics, and physically consistent forces."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provided detailed instructions on data processing and synthesis. This can help future studies to reproduce and expand the dataset.\n- The authors have explored training an RL policy that controls the MANO hand using the dataset."}, "weaknesses": {"value": "1.  The “contact force extraction” pipeline relies entirely on simulated replay rather than physical measurement. Specifically, the authors train an RL policy to match the object trajectory observed in mocap, and then read the simulator’s internal contact forces as ground truth. This procedure amounts to a sim-to-real matching step, not genuine force recovery, since the resulting forces are determined by the simulator’s contact parameters rather than the real dynamics. The mapping from motion to force is underdetermined, and the paper does not provide validation showing that these forces correspond to reality (e.g., via tactile or instrumented measurements). Therefore, the claim of “physically validated force profiles” is overstated.\n\n2. The trajectory-tracking policy employed in this work follows a standard imitation or RL-with-demonstrations setup. Similar approaches have been extensively explored, for example, in Rajeswaran et al. (2018, “Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations”). It would strengthen the paper if the authors could demonstrate how established algorithms such as DAPG or related imitation-based RL methods could directly leverage the proposed dataset, rather than re-implementing a similar framework."}, "questions": {"value": "Please see the weaknesses section. Thanks!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sVBaKV0VsT", "forum": "K2ceVeoqbL", "replyto": "K2ceVeoqbL", "signatures": ["ICLR.cc/2026/Conference/Submission24715/Reviewer_Mv9s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24715/Reviewer_Mv9s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959195485, "cdate": 1761959195485, "tmdate": 1762943171988, "mdate": 1762943171988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DexCanvas, a large-scale hybrid real-synthetic dataset for dexterous manipulation, constructed from 70 hours of real human demonstrations and expanded to 7,000 hours via physics simulation. The key innovation lies in a real-to-sim pipeline that uses reinforcement learning to reproduce human motions in simulation, thereby extracting physically consistent contact forces and object dynamics that are missing in motion capture data. The dataset is systematically organized using a manipulation taxonomy and provides rich multimodal annotations, including MANO parameters, RGB-D, and force profiles, enabling research in contact-rich robotic control and cross-morphology skill transfer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel real-to-sim pipeline using reinforcement learning to generate scalable, high-quality contact force annotations from human demonstrations, offering a cost-effective alternative to instrumented hardware. The dataset is rich in multimodal sensory signals, systematically organized by manipulation taxonomy, and provides rare physical annotations that are valuable for dexterous manipulation research."}, "weaknesses": {"value": "The DexCanvas dataset makes a valuable contribution by bridging human demonstrations with physics-based robotic learning, and the proposed RL-based real-to-sim pipeline for extracting contact forces is both innovative and promising for scalable data generation. However, I have some major concerns.\n\nFirst, the work is very similar to ManipTrans[1], which retargets human trajectories and objects be manipulated scanned for both single and two hands to physical plausible demonstration data by RL and also generates large scale manipulation trajectories.\n\nSecond, although the authors conduct statistical analysis about the proposed dataset, its value for the target tasks are not demonstrated. Baseline and benchmark with the dataset compared with prior works without the dataset or using other datasets on several downstream tasks are required to consolidate the work.\n\n[1] CVPR2025：MANIPTRANS: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning\n]\n\nIn addition, I have a few moderate concerns. \n\nFirst, the objects used are 3D-printed with embedded markers, which may differ from real-world objects in terms of material properties, surface textures, mass distribution, and shape variation—potentially affecting the realism and generalization of policies trained on this data. \n\nSecond, while the dataset is expanded to 7,000 hours via simulation, the real demonstrations are collected on only 30 objects, and although perturbations are applied, the diversity of object geometries and manipulation contexts could be limited. \n\nThird, overclaim of the contribution of the dataset. In Table 1, the manipscope of the work is general. However, in fact, the current dataset focuses exclusively on single-handed manipulation, leaving bimanual coordination unexplored. \n\nOverall, the dataset is well-constructed and offers potential for advancing dexterous manipulation research. However, given the limited contribution compared with prior work and the missing of the target tasks evaluation, I suggest borderline reject and the authors to further refine the work."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XESMq92pw9", "forum": "K2ceVeoqbL", "replyto": "K2ceVeoqbL", "signatures": ["ICLR.cc/2026/Conference/Submission24715/Reviewer_5HJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24715/Reviewer_5HJA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963459733, "cdate": 1761963459733, "tmdate": 1762943171763, "mdate": 1762943171763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a large-scale dataset of 7,000 hours simulated  human dexterous manipulation derived from 70 hours of mocap-based demonstrations with 21 manipulation types. The real-to-sim pipeline trains actuated MANO-hand controllers in simulation to reproduce human object trajectories, offering synchronized RGB-D, MANO parameters, and contact annotations. This dataset might inspire research in contact-rich robot learning, control, and cross-morphology skill transfer"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Integrates real human demonstrations with physics-based rollouts -- using physics tracking policy to clean data is an emerging field.\n\nUses a 22-camera mocap system synchronized with RGB-D sensors and precisely aligned 3D-printed objects for accurate geometry and motion data"}, "weaknesses": {"value": "Although the paper acknowledges several limitations, (1) despite its larger scale, it employs only a limited set of primitive objects; (2) its policy is based on MANO rather than a robot hand, requiring additional retargeting for robot learning that inevitably degrades data quality; and (3) it adopts a per-object policy that lacks scalability and generalization to unseen clean data. These issues have already been addressed by many RL-based studies, including but not limited to DexTrack [1], ManipTrans [2], and Dexplore [3]. Given these existing works, the paper fails to demonstrate advantages over such limitations, which constitutes a critical weakness.\n\n[1] Liu, Xueyi, et al. \"Dextrack: Towards generalizable neural tracking control for dexterous manipulation from human references.\" arXiv preprint arXiv:2502.09614 (2025).\n\n[2] Li, Kailin, et al. \"Maniptrans: Efficient dexterous bimanual manipulation transfer via residual learning.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[3] Xu, Sirui, et al. \"Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration.\" Conference on Robot Learning. PMLR, 2025.\n\nSuch limitations are also partially due to no downstream learning evaluation (e.g., trainingBC/diffusion/VLA models on DexCanvas vs. prior datasets) nor real‑robot transfer to establish practical value at deployment time; **As explicitly claimed in the supplementary, the authors defer these important experiments to rebuttal**, I am not sure if it is an encouraged behavior for ICLR\n\n\nSuccess and force read‑outs can be sensitive to different physical parameters. The paper lacks ablation or sensitivity studies to demonstrate robustness of recovered forces across reasonable physics parameter ranges.\n\nAlthough the dataset is large, much of its scale stems from repeatedly rolling out the same simulation policy with objects placed in slightly varied initial positions. As a result, the data diversity primarily reflects pose perturbations rather than behavioral or strategy diversity. It remains unclear how the augmented dataset captures genuinely distinct manipulation behaviors to benefit the robot learning side"}, "questions": {"value": "For the in‑hand category or tasks requiring precise manipulation, what would be the orientation tracking, contact persistence, or slip statistics in addition to positional success with fixed threshold"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATx6aWgtq5", "forum": "K2ceVeoqbL", "replyto": "K2ceVeoqbL", "signatures": ["ICLR.cc/2026/Conference/Submission24715/Reviewer_D3V2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24715/Reviewer_D3V2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979277742, "cdate": 1761979277742, "tmdate": 1762943171524, "mdate": 1762943171524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces DexCanvas, a large dataset for manipulation with dexterous hands, by bridging mo-cap human demonstrations with learning. It is prepared by using 70 hours of real human manipulation data using multi-modal capture (RGB-D and Vicon-style motion capture) spanning 21 types of manipulation. The mo-cap data has some systematic errors when replayed open-loop in sim. Hence, the work uses RL (PPO) to make these simulated manipulation demos physically consistent; the RL agent adds a residual action to the mocap data to track the original demos trajectories of hand and objects. Once they have such data, they record attributes like contact points and forces from the sim to produce a richer demonstration data than the original mo-cap data from the real world. This creates the original 70 hours of demo data with physics-aware contact information in sim. Then, they add perturbations to the environment to create a larger set of rollouts, which gives you 100x the training size -- going to 7000 hours of demo data. The authors demonstrate this pipeline that effectively reconstructs natural human motion, creates physics-aware demo data, and provides fine-grained force profiles for various grasping strategies. This is great work that is well motivated. But there are no tests to validate the hypothesis of the dataset.\n\nAn alternative story for the paper could be the _pipeline_ that allows for such physics-aware data collection. You start with some mocap data, and explain the pipeline (& RL) to move it to sim and record physically consistent motion, contact, and pose data -- which gives you a much richer dataset, for a much richer manipulator. The output of this paper could be that pipeline, and a subsequent paper could be the actual dataset, after it is appropriately validated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Well motivated. Good comparison with current dexterous hand data that's out there. We need more robotics data, especially for hands that are not parallel jaw grippers. Crucially, we need contact-informed data, not just kinematics data.\n\n* Effort has gone into capturing different types of manipulations, getting MANO parameters like the shape params (beta), that account for hand pose beyond just joint angles in the fingers."}, "weaknesses": {"value": "1. The whole paper is a dataset -- but there is no downstream use of the dataset to validate its effectiveness. The authors state that _\"Additional experiments demonstrating cross-dataset applicability and downstream task evaluation will be provided during the rebuttal phase.\"_ They are already baking in work for the rebuttal before the reviewers ask for any clarifications or expts. I am not sure if that's what the rebuttal phase is for. Reviewers won't get enough time to understand the new material you already plan to add for the rebuttal (architectures, training pipelines, everything else that is done after the dataset is created).\n1. The effectiveness of a trained policy is tested - but that is the policy trained to generate the dataset. There is no test to demonstrate the effectiveness of the dataset.\n1. Single RL policy per object-manipulation pair: A single policy could benefit from cross-task transfer. This is already mentioned as a limitation in the paper.\n1. Contact-rich forces are being recorded in sim (IsaacGym). But simulation is not the best for accurate contact-rich data. And aspects like object deformation is not captured. Papers like [1] show how we need a good contact model for things like USB-insertion, and truly show the limits of contact-rich simulation. Hence, this simulated data will like be a supplementary add-on; the main demos would be real human demos with true force information to serve as the ground truth.\n\n[1] \"Efficient Online Learning of Contact Force Models for Connector Insertion\" https://arxiv.org/pdf/2312.09190"}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4IrkDFhEQO", "forum": "K2ceVeoqbL", "replyto": "K2ceVeoqbL", "signatures": ["ICLR.cc/2026/Conference/Submission24715/Reviewer_osbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24715/Reviewer_osbF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128014423, "cdate": 1762128014423, "tmdate": 1762943171259, "mdate": 1762943171259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}