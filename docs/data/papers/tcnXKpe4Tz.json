{"id": "tcnXKpe4Tz", "number": 15398, "cdate": 1758250943861, "mdate": 1759897309616, "content": {"title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "abstract": "Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present SAFER (Sparse Autoencoder For Enhanced Reward model), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-aligned preference datasets and quantify the salience of individual features by activation differences between preferred and dispreferred responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general performance. Our approach establishes a new methodology for auditing and refining reward models in high-stakes LLM alignment tasks.", "tldr": "", "keywords": ["Safety", "Reward Model", "Sparse Autoencoder"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b915868a03325c1f93f8838d1100274f7454db4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SAFER, a framework that uses sparse autoencoders to find human-interpretable, safety-related features in reward models. The framework first ranks features via activation contrasts between chosen and rejected responses, and then select for safety related ones via GPT-4o annotation, narrowing down to a set of relevant safety-related features. Using these as a guidance, the authors can score the relative contribution of a triplet to safety alignment behavior and selectively manipulate the labels of top and bottom rated pairs to poison or denoise the preference data. The y show that the method selectively improve safety alignment with little impact on general chat quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The paper introduces a novel method that uses SAE located safety features to guide preference data poisoning or denoising, demonstrating targeted effects to RM safety category score rather than non-safety category like Chat.\n\nQuality: The paper provides clear methodology description, experimental results and ablation studies. I like the Appendix results where the authors ablate different parts of the SAE design choices to justify the current choice. \n\nClarity: Text flow is logical and figures are clearly described. Background related to the methods are sufficiently introduced. \n\nSignificance: The methods SAFER enables mechanistic insights into what safety features affect RM scoring and provides a way to do targeted data manipulation on preference datasets."}, "weaknesses": {"value": "1. Overall weakness: I like the idea of using SAE to gain more mechanistic insight into data manipulation, but I do question whether coarse level of data manipulation like flipping labels need the granularity of SAE. For example, one alternative I can imagine is that the authors can rate the trio with a set of fine-grained rules which could contain safety-related rules or language-related rules etc. If they score based on these different axes, it's likely that they can also identify the data entries where safety related difference contribute mostly to the chosen rejected ratings. Computationally, it could be cheaper since it doesn't require training a SAE. On the other hand, SAE does provide more fine-grained information which the authors don't explore at data manipulation. E.g. in the rating examples provided, the authors can obtain activations of particular tokens within a response. If more targeted data poisoning or improvement on token/token group level can lead to good effects too then that would be a more convincing use of SAE? \n2. 3.3.1 experimental results: It is nice to show that the manipulation doesn't affect Chat subcategory. However, in the rewardbench there are also other 2 categories Chat Hard and Math which are also non safety related. It would be helpful to show the manipulation consequence on these 2 subsets too.\n3. Appendix A.3: the authors mention a line of previous work using different methods to decide on preference data for poisoning or denoising. Does random or reward-based baseline that the authors compare to reflect one of these competitive baselines? If so, I would like to see explicit connection like \"reward-based baseline is a implementation of which paper's method and we show that ours is better\" if not I think it would be helpful to compare at least one of these competitive baselines?"}, "questions": {"value": "1. Regarding weakness 1, could authors provide better justification of using SAE as opposed to other potentially simpler methods identifying safety alignment contributing pairs? \n2. Regarding weakness 2, could authors provide scores for other 2 non-safety subsets?\n3. See Weakness 3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WmKh9XqH9j", "forum": "tcnXKpe4Tz", "replyto": "tcnXKpe4Tz", "signatures": ["ICLR.cc/2026/Conference/Submission15398/Reviewer_Wp5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15398/Reviewer_Wp5f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517305635, "cdate": 1761517305635, "tmdate": 1762925676809, "mdate": 1762925676809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAFER (Sparse Autoencoder For Enhanced Reward model), a framework that uses Sparse Autoencoders (SAEs) to interpret and manipulate reward models (RMs) used in RLHF. The approach trains SAEs on reward model activations to uncover monosemantic, human-interpretable features associated with safety-related behavior. SAFER quantifies feature salience via activation differences between chosen and rejected responses and applies this for two tasks of data poisoning and data denoising."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tMechanistic insight into reward models. Applies SAEs to reveal safety-relevant latent features, addressing a major interpretability gap in RLHF.\n2.\tTargeted control of safety alignment. Demonstrates that feature-guided poisoning can selectively degrade safety without harming general chat performance.\n3.\tSolid empirical setup. Uses both LLaMA-3.2-1B and 3B reward models, with careful ablations on layer choice, dictionary size, and sparsity.\n4.\tReadable and reproducible. The methodology, hyperparameters, and datasets are transparent, and the results are easy to follow.\n5.\tClear empirical validation. Shows consistent improvements from denoising and interpretable UMAP clustering of safety-related features."}, "weaknesses": {"value": "1.\tLimited novelty. The paper mainly applies existing SAE methods to reward models; the core algorithmic contribution is modest.\n2.\tReliance on synthetic safety evaluation. “Safety” features and dataset manipulations are defined through model-generated or GPT-4o-labeled judgments rather than verified human annotation.\n3.\tNo causal interpretability. SAFER identifies correlational feature activations but does not test causal steering (e.g., changing activations to modify reward outcomes).\n4.\tNarrow scope. The experiments are restricted to the safety domain; the approach’s generality to other alignment aspects (e.g., helpfulness or fairness) remains unproven.\n5.\tLimited real-world significance. The improvements (both in interpretability and safety metric control) are informative but relatively small, leaving unclear how this scales to larger, production-grade RMs.\n6.\tDual-use risk not deeply addressed. While noted in the ethics section, the data-poisoning mechanism could be exploited, and mitigation strategies are not explored experimentally."}, "questions": {"value": "1.\tCan the authors validate the identified \"safety features\" through direct activation steering (e.g., modifying or ablating features during inference)?\n2.\tHow robust are the safety-related clusters across different model scales or architectures?\n3.\tCould the same feature-based analysis be applied to \"helpfulness\" or \"honesty\" RMs without retraining the SAE?\n4.\tWhat safeguards would prevent SAFER-like methods from being misused for systematic alignment degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pulnmrXl3n", "forum": "tcnXKpe4Tz", "replyto": "tcnXKpe4Tz", "signatures": ["ICLR.cc/2026/Conference/Submission15398/Reviewer_88cy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15398/Reviewer_88cy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779827126, "cdate": 1761779827126, "tmdate": 1762925676486, "mdate": 1762925676486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAFER, a framework that leverages Sparse Autoencoders (SAEs) to decompose the high-dimensional and entangled neural activations within a reward model (RM) into a set of sparse and interpretable features. This process reveals and enables manipulation of the RM's internal decision-making mechanisms. The core innovation is a contrastive feature extraction method: by systematically analyzing the difference in activation strength of safety-related features between preferred (safe) responses and rejected (unsafe) responses, SAFER calculates a contrastive safety score for each feature to quantify its association with safety-related decisions. Experiments demonstrate that flipping the labels of high-scoring data points (data poisoning) can precisely compromise the model's safety, whereas removing low-scoring data points (data denoising) effectively enhances its safety alignment, all with minimal impact on the model's general conversational abilities. This work provides a powerful underlying tool for understanding, auditing, and improving reward models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovatively using the classical mechanism interpretability tool SAE to study the reward model provides a completely new, more fundamental perspective for understanding the black box of RLHF, which is highly enlightening.\n\n2. This paper not only stops at explanation but also conducts intervention experiments. In these experiments, data poisoning can significantly reduce security performance while almost unaffected general conversational ability. This demonstrates that SAFER indeed captures specific neural circuits highly relevant to safety, rather than broad semantic features.\n\n3. The experimental design and validation are rigorous and standardized. It not only double-verifies the effectiveness of the method in the two directions of *poisoning* and *denoising*, but also conducts exhaustive ablation studies on the key hyperparameters of SAE (such as the layer positions, number of features, sparsity, etc.)."}, "weaknesses": {"value": "1. Only two smaller reward models (1B and 3B Llama-3.2-RM) were used, making it difficult to determine whether the proposed method can generalize well to other model architectures and model sizes.\n\n2. SAFER demonstrates the strong correlation between certain features and safety decisions. However, this still remains at the observational level. The paper did not conduct causal intervention experiments , such as directly modifying the activation values of specific safety features through feature steering to observe the corresponding changes in the reward model outputs. This would be a crucial step to prove that these features are the fundamental cause of safety decisions rather than merely correlated signals. The authors mentioned this in future work, which also highlights a limitation of the current study.\n\n3. I am concerned that the framework calculates static feature safety scores ($s_i$) through global aggregation of feature activation. This approach may flatten or average the dynamic semantics of features across different contexts, thereby failing to capture the more complex safety logic within the reward model. For example, suppose an SAE feature related to \"weapons\" is activated in the context of describing historical or fictional safety scenarios, versus being activated in an unsafe context providing dangerous instructions, their safety implications are entirely different. Under the SAFER computational framework, activations from safe contexts would increase its accumulated *pro-safe* value ($h_i^+$), while activations from unsafe contexts would increase its accumulated *anti-safe* value ($h_i^-$). This could lead to two potential adverse consequences: First, if these two contexts appear with similar frequencies in the dataset, their signals would cancel each other out, causing the final safety score $s_i$ to approach zero, leading to the incorrect neglect of this safety-critical feature. Second, if one context dominates the dataset, the feature would be assigned a static *safe* or *unsafe* label, which could mislead subsequent intervention measures (such as data denoising) to incorrectly penalize the model's ability to handle sensitive topics in harmless contexts."}, "questions": {"value": "Same as Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWwWo5ZHZJ", "forum": "tcnXKpe4Tz", "replyto": "tcnXKpe4Tz", "signatures": ["ICLR.cc/2026/Conference/Submission15398/Reviewer_fdCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15398/Reviewer_fdCC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981980822, "cdate": 1761981980822, "tmdate": 1762925676064, "mdate": 1762925676064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAFER (Sparse Autoencoder For Enhanced Reward model) , a framework designed to address the \"black box\" nature of reward models used in RLHF. The authors propose that by applying sparse autoencoders to the internal activations of a safety-trained RM, they can extract sparse, human-interpretable, monosemantic features that correspond directly to safety-related concepts.\n\nThe method involves training an SAE on RM activations and then using a contrastive method to identify features that have the largest activation difference between \"chosen\" (safe) and \"rejected\" (unsafe) responses. These features are filtered using GPT-4o to isolate a core set of safety-relevant features. This approach is validated through two primary experiments: (1) flipping the labels of the top-scoring safety-aligned data points can degrade the RM's safety performance, (2) conversely, removing the lowest-scoring (noisiest) data points can improve the RM's safety performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method (SAFER) is a precise and targeted intervention. Figure 1 and Table 2 show that SAFER's poisoning method causes a sharp decline in safety scores while leaving chat performance almost untouched.\n2. SAFER can also work as a filtering method. In the denoising experiments, it shows an improvement in RM safety from 94.86 to 96.46 on the 3B model by removing just 4% of data.\n3. Ablation experiments are sufficient; having a separate discussion on token-level and sequence-level is a plus."}, "weaknesses": {"value": "1. A significant methodological weakness is the reliance on a proprietary black-box GPT-4o to filter the features. The framework first uses a contrastive score to find candidate features, but then \"use GPT-4o to interpret and assign safety relevance ratings\" and only retains features with a perfect score of 5. Though the authors validate Human-GPT-4o alignment (Figure 5), but this is on their specific task, and it doesn't address the dependency.\n2. The effect of removing denoising data does not monotonically increase as more \"noisy\" data is removed. For both the 1B and 3B models, the peak safety performance occurs at a 4% removal rate, after which performance slightly degrades or fluctuates. This suggests that the $score_{safe}$ metric is not a perfect proxy for \"data quality\" and that simply removing all low-scoring samples may not be the optimal strategy. The paper does not fully explore or explain this non-monotonic behavior.\n3. Lack of scale. Features that are clearly separable in small models may become more polysemantic or distributed via superposition in larger models. Will this method of extracting \"monosemantic\" safety features work on larger models? (e.g. Qwen-3-8B, Llama-3.1-70B)"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "US6E7ZLass", "forum": "tcnXKpe4Tz", "replyto": "tcnXKpe4Tz", "signatures": ["ICLR.cc/2026/Conference/Submission15398/Reviewer_u1t1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15398/Reviewer_u1t1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15398/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136329004, "cdate": 1762136329004, "tmdate": 1762925675729, "mdate": 1762925675729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}