{"id": "YrlMkdmB5n", "number": 16490, "cdate": 1758265108605, "mdate": 1759897237616, "content": {"title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation", "abstract": "Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexplored in prior works on unified VLMs. To address this gap, this paper systematically investigates the generalization across understanding and generation tasks in unified VLMs. Specifically, we design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. We evaluate multiple unified VLM architectures to validate our findings. Our key findings are as follows. First, unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data. Second, better alignment between multimodal input and output spaces will lead to better generalization. Third, the knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters. Our findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs. The code and datasets will be released upon publication of the paper.", "tldr": "This paper systematically examines whether unified vision-language models enable mutual benefits between generation and understanding tasks, and identifies key factors that influence this synergy.", "keywords": ["Unified Vision-Language Model", "Generative Model", "Generalization", "Multimodal Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00fffedf0e9caeafd4ce1dbc746beb0933684348.pdf", "supplementary_material": "/attachment/6675da17560104d599e1b2acc3fda488212b3bf7.zip"}, "replies": [{"content": {"summary": {"value": "This paper conducts a systematic study on whether unified vision-language models that integrate both understanding and generation are truly necessary. Using controlled synthetic datasets (SmartWatch and modified CelebA) and several model variants combining SigLIP and VQ-VAE encoders/decoders, the authors show that unified training yields mutual benefits between understanding and generation tasks. They further demonstrate that alignment between visual input and output spaces is crucial—when disrupted, cross-task generalization collapses. Moreover, knowledge learned from generation tasks can transfer to understanding, primarily through the shared base language model rather than modality adapters. Experiments on LLaVA-1.5-7B confirm these findings on real-world benchmarks. Overall, the paper provides compelling empirical evidence and analysis supporting the necessity of unified VLMs and offers design insights for future multimodal systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear and well-structured experimental setup that allows precise evaluation of cross-task generalization between understanding and generation.\n2. The authors provide strong empirical evidence showing that unified training consistently yields mutual benefits across diverse architectures.\n3. The analysis of input–output alignment is insightful and convincingly demonstrates how spatial alignment between vision token spaces governs cross-task transfer.\n4. The paper offers actionable guidance for future unified VLM design by examining the effects of data scaling and adapter sharing."}, "weaknesses": {"value": "1. The study omits diffusion-based unified architectures, limiting the generality of its conclusions.\n2. The evaluation focuses on basic vision-language tasks and does not explore challenging reasoning or compositional generalization cases.\n3. The paper does not test SigLIP-based generation in pixel space, as no decoder or visual reconstruction is included, which limits the completeness of the evaluation. An approach is to train a SigLIP-decoder following Emu2 and validate its generation ability.\n4. The paper relies on outdated backbones such as Vicuna-7B-v1.5 and LLaVA-1.5-7B, so its conclusions may not generalize to newer multimodal models. Evaluating the proposed findings on recent architectures would better demonstrate their robustness."}, "questions": {"value": "My major concer is in weakness 3 and 4. I'd be happy to increase the score if they are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L0hfhmylep", "forum": "YrlMkdmB5n", "replyto": "YrlMkdmB5n", "signatures": ["ICLR.cc/2026/Conference/Submission16490/Reviewer_VtLR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16490/Reviewer_VtLR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810191656, "cdate": 1761810191656, "tmdate": 1762926589164, "mdate": 1762926589164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors challenge the common assumption that a unified architecture naturally leads to mutual enhancement between understanding and generation, providing the first systematic, architecture-agnostic study of this phenomenon. Their findings can be summarized as follows. Unified training on both understanding and generation tasks yields mutual benefits, outperforming task-specific models. The alignment of visual input and output spaces is crucial for effective cross-task generalization. Knowledge transfer occurs from generation to understanding tasks, and this transfer is mainly realized within the base LLM, not the modality adapters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good motivation with principled analysis: the paper provides controlled, reproducible experiments to validate the features about unified VLMs.\n\n2. Good experimental design: two datasets with full controllability and detailed ablations (alignment, scaling, bias).\n\n3. Clear practical relevance: In addition to empirical experiments, the paper also provides actionable guidelines (maintaining aligned latent spaces, balancing task ratios)."}, "weaknesses": {"value": "1. Limited scope of architectures: The paper excludes diffusion-based or hybrid models (e.g., Transfusion, Emu3). Especially, the generation performance of SigLIP-SigLIP and VQ-SigLIP are missing. In addition, Harmon [a] adopts MAR encoder (different from SigLIP or VQ), which is not discussed and included in experiments. This somewhat narrows the generality of the conclusions.\n\n2. Synthetic bias: Although the synthetic datasets provide control, they are relatively simple compared to real-world multimodal distributions. The gap between these and natural images could limit ecological validity.\n\n3. Lack of deeper theoretical grounding: The study is empirical; a more formal analysis (e.g., information-theoretic or representational similarity argument) could enhance interpretability.\n\n4. Lack of experiments with real-case unified VLMs: The paper presents experiments based on LLaVA-V1.5-7B, but LLaVA-V1.5-7B is not a unified VLM. Why not directly conduct experiments with existing unified VLMs, e.g. Janus?\n\n[a] Wu S, Zhang W, Xu L, et al. Harmonizing visual representations for unified multimodal understanding and generation[J]. arXiv preprint arXiv:2503.21979, 2025."}, "questions": {"value": "1. Can the conclusions of this paper explain the results of previous papers (stated in Line131-142)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hKnhegiOl8", "forum": "YrlMkdmB5n", "replyto": "YrlMkdmB5n", "signatures": ["ICLR.cc/2026/Conference/Submission16490/Reviewer_yW45"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16490/Reviewer_yW45"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833416678, "cdate": 1761833416678, "tmdate": 1762926588713, "mdate": 1762926588713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether jointly training vision-language models (VLMs) on image understanding and generation tasks leads to mutual benefits. Through controlled experiments with several unified architectures, the authors find that mixed training improves both modalities, especially when visual input and output spaces are well aligned. They also show that knowledge from generation tasks transfers to understanding tasks, supporting the 'necessity' of unified VLMs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates in depth an important and timely topic in multimodal model design. It remains unclear whether specialized or unified VLMs are preferable, and understanding the trade-offs between these approaches is valuable.\n\n- The experimental setup is systematic and controlled, using synthetic datasets that help isolate the factors influencing cross-task generalization."}, "weaknesses": {"value": "- The paper is unclear about how the proposed unified VLMs actually work. Figure 1 doesn’t really help in understanding the architecture, and several components are insufficiently explained. In particular, it’s not clear what the 'generation vision adapter' does: if the LLM outputs image tokens directly, why do these need to be adapted before being fed back into the model? The generation process for the SigLIP-SigLIP and LLaVA settings is also confusing. For models with a VQ decoder, image generation is straightforward, but for those without it’s never explained how the model turns its outputs into actual images (whether through a diffusion module or something else). The lack of generation results for the SigLIP-SigLIP adds to the confusion.\n\n- Although the authors claim to test on 'real-world scenarios', most of the analysis is done on extremely simple datasets of faces and synthetic clocks. Of course, this can be useful to have a controlled setup, but it really limits the practical impact of the findings. It is unclear whether the models trained on these two tasks are of any practical usefulness, or if they just work on these very specific datasets. The choice of models is also extremely limited, as the authors only use Vicuna-7B as a backbone, and even the experiments on LLaVA are restricted to a version finetuned starting from the same base model. It would be important to test whether the evaluated models can handle more realistic multimodal tasks and to verify that the reported findings extend to different LLM backbones.\n\n- The significance of some findings is not entirely clear. For example, the result that most of the cross-task generalization happens within the LLM rather than in the vision adapters seems somewhat expected. Since the LLM has by far the largest number of parameters and is the component where image and text tokens interact through attention, it is natural that most of the transfer would occur there. In Section 4.2, the conclusion that SigLIP encodings already provide semantic visual tokens also feels rather intuitive, so this experiment adds limited new insight. On the contrary, I am unsure whether the same conclusion holds for models using a VQ encoder, as the two panels on the right of Fig. 7 seem to suggest the opposite, and previous studies such as [a] have demonstrated that image tokens in unified models with VQ encoding carry extremely limited semantic content.\n\n**Minor and typos:**\n- Line 24: 'this' -> 'these'.\n- Line 35: 'has' -> 'have'.\n- The orientation of the arrows in Fig. 1 can be confusing.\n- Line 50: is 'conversely' the right word? To me, it looks like there is no opposition between the concepts expressed by the two sentences.\n\n**Reference:**\n\n[a] Serra et al., The Narrow Gate: Localized Image-Text Communication in Native Multimodal Models, NeurIPS 2025"}, "questions": {"value": "- What is the exact role of the 'generation vision adapter' and why is it required if the LLM already produces image tokens?\n- How does image generation work for the SigLIP-SigLIP and LLaVA models?\n- Why are the models trained on image captioning data but never tested on such task?\n- In the experiment of Fig. 3, are unified and single-task models trained on the same amount of data and for the same number of steps?\n- Overall, the paper seems to suggest that unified training is beneficial for understanding and generation performance. However, as the authors report in lines 62-65 real-world unified VLMs do not show a significant edge over single-task models. Do the authors have an explanation for this discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6LwVC6uMe8", "forum": "YrlMkdmB5n", "replyto": "YrlMkdmB5n", "signatures": ["ICLR.cc/2026/Conference/Submission16490/Reviewer_2bLg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16490/Reviewer_2bLg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929656171, "cdate": 1761929656171, "tmdate": 1762926588400, "mdate": 1762926588400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tests whether putting vision-language understanding (like VQA/captioning) and generation (text-to-image) into a single unified model actually helps, by training on controlled datasets that contain both kinds of supervision, and it finds that unified models do better than task-specific ones on both tasks when they share and align the visual spaces used to read and to generate images; when that alignment is broken, the gains largely disappear, showing that the benefit comes from sharing a common visual representation. Even more interesting, knowledge can flow from the generation side to the understanding side, if the understanding data is missing a concept but the generation data has it, the unified model still learns it, and the authors argue this transfer mainly happens inside the LLM component, which therefore should be the fusion hub in future Janus/Chameleon-like unified VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Comparison in Fair setting: authors build controlled datasets that contain both understanding and generation signals and then train unified vs task-specific models on the same budget\n- Identifying visual-space alignment as the key driver, they break the alignment and show the gains drop.\n- The constructed case where generation has a concept and understanding doesn’t, and the unified model still learns it, is a compelling demonstration of cross-task transfer"}, "weaknesses": {"value": "- Prior works such as MetaMorph[1] have provided some evidence of mutual benefit transfer between understanding and generation tasks, limiting the novelty of the findings here.\n- This work primarily uses smaller datasets built with rule-based text and attribute-style supervision (SmartWatch, templated CelebA) where concepts are annotated, which is not a realistic setting for VLMs. \n- The training tasks are closely related on both understanding and generation side: The understanding side is basically attribute VQA/captioning, and the generation side is text-to-image for those same attributes.\n- The work draws strong conclusion from narrow evidence\n\n\n[1] Metamorph: Multimodal understanding and generation via instruction tuning. Tong, Shengbang, David Fan, Jiachen Li, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. ICCV 2025."}, "questions": {"value": "Experiment with negative-transfer tasks, when one task is noisy or much larger, does it ever hurt the other?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H1qwO91dyO", "forum": "YrlMkdmB5n", "replyto": "YrlMkdmB5n", "signatures": ["ICLR.cc/2026/Conference/Submission16490/Reviewer_UE2t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16490/Reviewer_UE2t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969580881, "cdate": 1761969580881, "tmdate": 1762926587856, "mdate": 1762926587856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}