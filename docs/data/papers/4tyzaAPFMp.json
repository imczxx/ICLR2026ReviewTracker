{"id": "4tyzaAPFMp", "number": 6920, "cdate": 1758001948219, "mdate": 1759897883864, "content": {"title": "Compositional Discrete Diffusion for Imbalanced 3D Scene Synthesis and Dataset Generation", "abstract": "3D semantic scene synthesis using discrete diffusion models faces severe challenges due to extreme class imbalance, where background voxels vastly outnumber foreground objects. This imbalance becomes particularly problematic in discrete diffusion for two reasons: (1) the denoising process operates in probability space rather than feature space, making minority classes vulnerable to majority absorption, and (2) learned transition probabilities exhibit systematic bias toward backgrounds, which compounds across diffusion steps, causing irreversible loss of foreground information. We identify this phenomenon as \\textit{probabilistic flow collapse}---a fundamental limitation of existing methods. To address this, we propose the Compositional Discrete Denoising Diffusion Probabilistic Model (Comp-D3PM), which synthesizes 3D scenes by compositionally denoising foreground and background voxels through separate transition dynamics. Our contributions are threefold: (1) we formally characterize probabilistic flow collapse and introduce a two-stream architecture that prevents minority-class absorption through compositional modeling; (2) based on this architecture,we enable arange of applications, including the generation of image–semantic scene datasets; and (3) we demonstrate on CarlaSC and SemanticKITTI that Comp-D3PM produces significantly more realistic and diverse scenes while preserving semantic integrity.", "tldr": "", "keywords": ["Diffusion Models for Vision", "Semantic Scene Generation", "Dataset Generation", "Monocular SSC"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1a2bf0801bf6268d4c50855aa2771e2ce468dbb2.pdf", "supplementary_material": "/attachment/5bcab76afca033558c2c6facdedfad1d752dc7b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a key problem of class imbalance in 3D semantic scene generation tasks and formalizes this issue with the novel concept of \"probabilistic flow collapse.\" To address this issue, a compositional diffusion model (Comp-D3PM) is proposed, which decouples foreground and background generation, showing a significant improvement compared to baselines. The authors conduct various experiments to illuminate the effectiveness of Comp-D3PM. Ablation studies substantiate the efficacy of the core innovation. The paper is well-motivated, the arguments are sound, and the experimental results are impressive, clearly demonstrating enhanced semantic integrity. Furthermore, the work shows potential for downstream applications such as synthetic dataset generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In response to the identified problem, the Comp-D3PM framework offers an intuitive and effective solution. By decomposing the scene into foreground and background components and learning independent transition dynamics for each, the method directly addresses the root cause of \"probabilistic flow collapse.\" This \"divide-and-conquer\" strategy is exceptionally clear.\n\n2. The proposed method achieves performance far superior to the existing state-of-the-art (e.g., Semcity) across all core metrics (F3D, K3D, FID, KID) on both real-world (SemanticKITTI) and synthetic (CarlaSC) datasets.\n\n3. The paper does not stop at the generation task itself but also showcases Comp-D3PM's applicability to scene completion, editing, refinement, and, most importantly, \"dataset generation.\""}, "weaknesses": {"value": "1. While the paper's ablation studies validate the effectiveness of \"compositional modeling\",  they do not provide a dedicated experiment to demonstrate the necessity of the third-stage \"BEDiT_C\". It is currently unclear how a two-stage model (generating and directly merging the foreground and background) would perform compared to the full three-stage pipeline.\n\n2. The method cleverly addresses the imbalance between foreground and background. However, a severe class imbalance within the foreground often exists (e.g., 'cars' are far more numerous than 'pedestrians'). The paper should discuss whether the \"probabilistic flow collapse\" phenomenon might also occur within the foreground generation.\n\n3. The paper qualitatively claims that the overall complexity of the three-stage model is \"comparable\" to baseline methods. Authors should compare the parameter count and inference time with baselines. \n\n4. Although the downsampling strategy effectively resolves the \"hole\" issue in the background, it is inherently a low-pass filtering operation. This could lead to the loss of high-frequency details, such as sharp object edges and fine geometric structures, resulting in over-smoothed outputs. The paper does not address this potential negative side effect, nor does it analyze whether the employed metrics (like F3D/K3D) are insensitive to such high-frequency detail loss.\n\n5. The generated dataset lacks a quality assessment. It is recommended to calculate the FID and KID metrics against the KITTI dataset.\n\n6. To assess the value of the synthetic data, one could perform hybrid training with real data to demonstrate the resulting performance improvement."}, "questions": {"value": "1. Could you provide an ablation study that compares the full three-stage model with a two-stage version that only generates the foreground and background and then directly merges them? This would clarify the specific performance gain brought by the BEDiT_C fusion module.\n2. When calculating metrics, was the Ground Truth downsampled, or was the original data used?\n\n3. Could you please elaborate on the partitioning of the data processing and training stages? Specifically, are the foreground, background, and their fusion trained in a stepwise manner?\n\n4. What is the generalizability of your method, and can it be effectively transferred to related approaches? A discussion on this aspect would further underscore the value and significance of the proposed method.\n\n5. If possible, could you provide further theoretical derivations regarding how class imbalance accumulates across the diffusion steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TCZ39tMAKj", "forum": "4tyzaAPFMp", "replyto": "4tyzaAPFMp", "signatures": ["ICLR.cc/2026/Conference/Submission6920/Reviewer_XMZv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6920/Reviewer_XMZv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448271456, "cdate": 1761448271456, "tmdate": 1762919157100, "mdate": 1762919157100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of class imbalance in 3D semantic scene synthesis, where background voxels dominate and foreground details are lost during diffusion. The authors identify this problem as probabilistic flow collapse and propose Comp-D3PM, a compositional discrete diffusion framework that models foreground and background separately before fusing them into coherent scenes. The method incorporates downsampling and BEV-aware architectures to improve training stability and efficiency. Experiments on SemanticKITTI and CarlaSC show performance gains over prior work, and the approach also enables the automatic generation of synthetic 3D datasets for semantic scene completion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies and formally defines the phenomenon of probabilistic flow collapse in discrete diffusion models, providing a clear theoretical explanation for how class imbalance leads to foreground information loss.\n\n2. The proposed Comp-D3PM introduces a compositional framework that disentangles foreground and background diffusion processes, offering a conceptually simple yet effective way to address class imbalance in 3D scene generation.\n\n3. Beyond scene synthesis, the framework is extended to practical applications such as semantic scene completion, inpainting, and automatic dataset generation, showing good generality and real-world potential."}, "weaknesses": {"value": "1. According to the foreground–background division shown in Figures 1 and 3, vehicles should belong to the foreground. However, in Figure 4, the background generation branch also produces vehicles, which appears inconsistent with the stated compositional setup.\n\n2. In Section 4.1, the paper claims that downsampling resolves background holes, but $BEDiT^C$ later uses the downsampled and fused low-resolution scene as a condition to generate the final high-resolution output. Since the training scenes already contain holes at high resolution, it is unclear how the final results avoid reintroducing them.\n\n3. The PDD framework also performs downsampling and pyramid-based generation. Although its low-resolution representations are hole-free, the final full-resolution outputs still contain holes. It is unclear whether Comp-D3PM effectively overcomes this issue.\n\n4. The proposed score-based downsampling strategy is not experimentally compared to PDD’s approach. The claimed advantages remain theoretical without quantitative evidence.\n\n5. The paper does not report detailed training or inference costs. A direct comparison with SemCity and PDD in terms of GPU memory usage, training time, and inference speed would clarify the computational trade-offs.\n\n6. Since a major goal is reducing background invasion into objects, relying solely on human evaluation (BIS) is insufficient (for evaluating the Background Invasion). \n\n7. While the separation of background and foreground may help scene generation, all examples are unconditional and randomly generated. Without explicit control, the model might overfit, producing scenes overly similar to the training data. Do you have any analysis of this?\n\n8. Typographical and formatting issues:\n   - Line 27: missing space in “(b)Our”\n   - Figure 1(a): “pedestrian” label partially covered\n   - Figure 2: oversized and low resolution\n   - Figure 3: unclear axis units and inappropriate placement in Related Work\n   - Line 152: missing space between “labels” and citation\n   - Line 161: missing spaces after “objects” and “clouds”\n   - Line 162: missing space after “representations”\n   - Line 286: missing space after “DiT”\n   - Lines 332–333: incorrect quotation marks\n   - Line 375: missing space after “ISC”\n   - Line 471: missing space after “SemanticKITTI”"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9CODPKKzFi", "forum": "4tyzaAPFMp", "replyto": "4tyzaAPFMp", "signatures": ["ICLR.cc/2026/Conference/Submission6920/Reviewer_yPUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6920/Reviewer_yPUW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716094465, "cdate": 1761716094465, "tmdate": 1762919156513, "mdate": 1762919156513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the probabilistic flow collapse problem in discrete diffusion models for 3D semantic scene synthesis, where foreground information is absorbed by dominant background classes. The authors propose Comp-D3PM, a compositional two-stream diffusion framework that disentangles foreground and background dynamics to mitigate this issue. The method achieves significant improvements over prior works on SemanticKITTI and CarlaSC, and further supports applications such as SSC refinement and automatic dataset generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies and formalizes the phenomenon of probabilistic flow collapse in discrete diffusion, providing a clear explanation for failure cases observed in prior 3D scene generation models.\n- It proposes a targeted two-stream architecture that disentangles foreground and background diffusion and integrates them through a BEV-aware Transformer (BEDiT), resulting in a well-motivated and coherent design.\n- The method achieves competitive results on SemanticKITTI and CarlaSC, showing clear improvements in structural fidelity and semantic consistency over previous approaches."}, "weaknesses": {"value": "- The method is only evaluated on street-level, vehicle-mounted datasets (SemanticKITTI and CarlaSC), leaving its generality to other types of scenes unclear and possibly tied to this specific inductive bias.\n- The generated scenes remain relatively simple and lack the structural and semantic complexity observed in real-world 3D environments.\n- The paper lacks a comparison or discussion of alternative imbalance-handling strategies, like loss re-weighting"}, "questions": {"value": "Have the authors considered applying the proposed compositional discrete diffusion framework to indoor scene generation tasks? It would be interesting to see whether the same idea generalizes beyond driving scenes and whether probabilistic flow collapse also appears in more structured indoor environments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B9FaMNQwzu", "forum": "4tyzaAPFMp", "replyto": "4tyzaAPFMp", "signatures": ["ICLR.cc/2026/Conference/Submission6920/Reviewer_fY8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6920/Reviewer_fY8p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808039386, "cdate": 1761808039386, "tmdate": 1762919156133, "mdate": 1762919156133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets a failure mode in discrete diffusion for 3D semantic scenes where extreme class imbalance causes background to overwhelm foreground (“probabilistic flow collapse”). It proposes a compositional pipeline: (1) split foreground/background into two diffusion streams with distinct transition dynamics, (2) compress features via a BEV-aware DiT block, (3) apply a TF-IDF-like downsampling to reduce background dominance, and (4) fuse the streams during sampling. The method is evaluated on SemanticKITTI and CarlaSC for unconditional scene generation/completion against to 2 baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ Addresses a real, common issue (severe class imbalance) with a simple, implementable recipe.\n+ Authors should clear qualitative improvements at object boundaries\n+ The idea is clear and easy to follow."}, "weaknesses": {"value": "+ However, the novelty feels incremental: The core idea of splitting a scene into foreground (FG) and background (BG) components and using a compositional approach (generate BG, then generate FG conditioned on BG) is a well-established strategy in generative modeling. This FG/BG separation is a foundational concept, especially in 3D scene understanding where static backgrounds and dynamic foreground objects are often modeled independently. In fact, much of the field is already moving in this direction, with separate research tracks for dynamic 4D object generation and static scene reconstruction. While the authors effectively apply this idea to D3PMs for semantic scenes, the high-level concept itself is not new.\n\n+ Moreover, the architectural choices, such as the BEV-aware DiT (BEDIT), are strong but are primarily effective adaptations of established components (DiT, BEV representations) rather than novel contributions themselves.\n\n+ Related to the first point, the \"probabilistic flow collapse\" phenomenon is an excellent description of the problem, but it is not \"resolved\". The paper does not fix this collapse within the diffusion model itself. Instead, it sidesteps the problem with a compositional architecture. This is an effective engineering solution, but it does not represent the fundamental advance as Sec. 3.2 might imply. Actually, long-tail class imbalance is a long standing research area in machine learning, and there are many existing techniques (e.g., re-weighting) that could be adapted to diffusion models to fundementally address this issue without resorting to a compositional pipeline, which add a significant amount of computational overhead."}, "questions": {"value": "I would appreciate it if the authors could clarify the following points:\n\n+ The evaluation is limited to two autonomous driving datasets, SemanticKITTI and CarlaSC. How do the authors expect the Comp-D3PM framework to perform on other types of 3D semantic data, such as indoor scenes (e.g., ScanNet), which have significantly different class distributions, object densities, and structural priors?\n\n+ The proposed method seems to rely on a manually defined, clear-cut separation of classes into \"foreground\" and \"background\" (as implied by Fig. 2). How does the model handle ambiguous classes (e.g., \"terrain\")? Does this hardcoded categorical split limit the model's flexibility, and how would it be adapted to a new dataset with different categories?\n\n+ Could the authors provide a more direct comparison of the computational overhead (e.g., total inference time/steps, FLOPs) of the proposed three-stage process (BG generation, FG generation, fusion) compared to the single-stage baselines?\n\n+ The proposed method only compared with limited baselines with few empirical results, which is a bit lack persuasiveness to me. Could the authors compare their method with more recent diffusion-based 3D scene generation methods to better demonstrate the effectiveness of their method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9xdglv3e4w", "forum": "4tyzaAPFMp", "replyto": "4tyzaAPFMp", "signatures": ["ICLR.cc/2026/Conference/Submission6920/Reviewer_ARTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6920/Reviewer_ARTA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954268011, "cdate": 1761954268011, "tmdate": 1762919155799, "mdate": 1762919155799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}