{"id": "KldJntSnTZ", "number": 3014, "cdate": 1757316705983, "mdate": 1763095568658, "content": {"title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation", "abstract": "We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a single decoder-only transformer architecture. OneCAT uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution image inputs and outputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) design trained with a unified autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) with proposed scale-aware adapter (SAA) that drastically reduces decoding latency compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT outperforms existing unified models across benchmarks for multimodal understanding, generation, and editing.", "tldr": "", "keywords": ["unified multimodal model", "decoder-only architecture", "mixture-of-expert", "autoregressive"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0efca60830dd888b965d4260cd0dd88f4942db86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents OneCAT, a decoder-only unified multimodal model that removes external vision encoders at inference, routes text and vision tokens to modality-specific FFN experts, and performs multi-scale autoregressive image generation via a Scale-Aware Adapter. Training follows a three-stage recipe that includes teacher-based hidden-state distillation, large-scale mid-training with native and dynamic resolutions, and supervised fine-tuning. The authors report strong understanding scores on common VLM benchmarks, competitive image generation and editing metrics, and latency improvements versus encoder-based understanding and diffusion-based generation systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The decoder-only formulation is clean and practical for deployment, the modality-specific expert design is straightforward to implement, and the multi-scale adapter provides a concrete mechanism to support autoregressive generation at different scales. The three-stage training pipeline is well organized and the benchmark coverage is broad, with latency tables that highlight potential efficiency gains at inference."}, "weaknesses": {"value": "1. **Overstated novelty and missing isolating ablations:** The core architecture largely reuses a base LLM with duplicated FFNs as modality experts and adds a scale-aware adapter, which reads as an engineering consolidation rather than a new principle. The paper does not isolate the contribution of Modality-MoE versus simpler heads or gating, nor does it compare the adapter to alternative scale-conditioning methods. Adding ablations that replace Modality-MoE with a shared FFN plus a lightweight router and that replace the adapter with per-scale prompts or per-scale layer norms would clarify where the gains originate.\n\n2. **Evaluation fairness and claim calibration:** GenEval results appear to mix raw prompts and LLM-rewritten prompts across systems, and some reported numbers on editing and compositionality are not state-of-the-art when compared side by side. The paper should report like-for-like GenEval in both raw and rewritten settings for OneCAT, mark baselines consistently, include seeds and confidence intervals, and calibrate claims to competitive performance rather than broad SOTA.\n\n3. **Attribution gap due to training-recipe confounds:** The three-stage pipeline introduces strong confounders, yet the paper does not disentangle how much of the improvement comes from the decoder-only architecture (Modality-MoE + SAA) versus the teacher, data scale, or schedule. There is no “no-distillation” control, no reduced-data or reduced-compute runs, no compute-matched comparison to an encoder+adapter baseline, and no evidence that SAA remains necessary without the teacher. As a result, the headline gains cannot be causally attributed to the proposed architecture, which materially weakens the contribution."}, "questions": {"value": "1. **Overstated novelty and missing isolating ablations:** Provide a concise delineation of the technical delta over a shared-FFN baseline and include one small controlled swap where Modality-MoE is replaced by a shared FFN with a lightweight router and SAA is replaced by per-scale prompts or per-scale layer norms under matched settings; if extra runs are infeasible, include an analytic rationale and a few representative failure cases that demonstrate why the proposed choices are necessary.\n\n2. **Evaluation fairness and claim calibration:** State explicitly whether the reported GenEval score uses the LLM-rewriter and whether baselines are aligned; add a short two-row sensitivity table for OneCAT (raw vs. rewritten) with decoding settings, resolution, and seeds, and calibrate any “SOTA” language where competitors lead.\n\n3. **Attribution gap due to training-recipe confounds:** Include a minimal control to isolate architectural gains from recipe effects (e.g., a no-distillation run on a small subset, a compute-matched encoder-plus-adapter baseline, or a teacher-off ablation with SAA), and provide a brief component-wise attribution indicating the approximate fraction of improvement contributed by each element."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O5yjSkNwqr", "forum": "KldJntSnTZ", "replyto": "KldJntSnTZ", "signatures": ["ICLR.cc/2026/Conference/Submission3014/Reviewer_Ymvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3014/Reviewer_Ymvq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566881849, "cdate": 1761566881849, "tmdate": 1762916503589, "mdate": 1762916503589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "0W5WjhdxuL", "forum": "KldJntSnTZ", "replyto": "KldJntSnTZ", "signatures": ["ICLR.cc/2026/Conference/Submission3014/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3014/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763095567984, "cdate": 1763095567984, "tmdate": 1763095567984, "mdate": 1763095567984, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OneCAT, a unified multimodal model that integrates understanding, generation, and editing within a single decoder-only Transformer. It introduces a modality-specific Mixture-of-Experts for efficient multimodal processing and a Scale-Aware Adapter (SAA) to handle hierarchical visual tokens from a multi-scale VAE. The SAA enables dynamic multi-resolution autoregressive generation with reduced latency compared to diffusion-based models. Experiments show that OneCAT achieves competitive or superior results across multimodal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper presents a clean and unified architecture that performs multimodal understanding, generation, and editing within a single decoder-only Transformer, removing external visual encoders or tokenizers and simplifying the overall pipeline.\n\n2.The proposed Scale-Aware Adapter (SAA) effectively handles hierarchical visual tokens, enabling dynamic multi-resolution generation and significantly reducing decoding latency compared to diffusion-based methods.\n\n3.The use of a modality-specific Mixture-of-Experts (MoE) improves computational efficiency by selectively activating experts for different modalities, contributing to strong empirical performance across multiple benchmarks."}, "weaknesses": {"value": "1.The novelty of the work is limited — both the Mixture-of-Experts (MoE) mechanism and the exploration of unified multimodal modeling within a purely autoregressive framework have been widely studied. The contribution here appears more as a system-level integration rather than a conceptual breakthrough.\n\n2. The proposed Scale-Aware Adapter (SAA) seems primarily beneficial for image generation and has little connection to multimodal understanding. Its structure is quite similar to LoRA, and the performance improvement shown in Table 10 appears modest compared to the added computational and architectural complexity. In the unified multimodal setting, the motivation for introducing a module solely aimed at improving generation quality feels somewhat limited."}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NB0j9MAGpw", "forum": "KldJntSnTZ", "replyto": "KldJntSnTZ", "signatures": ["ICLR.cc/2026/Conference/Submission3014/Reviewer_et3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3014/Reviewer_et3S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632381381, "cdate": 1761632381381, "tmdate": 1762916503355, "mdate": 1762916503355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **OneCAT**, a unified multimodal model based on a single decoder-only transformer architecture. The model is designed to perform multimodal understanding, generation, and editing tasks seamlessly. The paper demonstrates that a pure autoregressive, decoder-only model can be a sufficient and powerful foundation for general-purpose multimodal intelligence, offering up to 10x faster generation inference than diffusion-based unified models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The core contribution is a single decoder-only, autoregressive model that is encoder-free and VAE-tokenizer-free during inference. This elegant design leads to significant, well-documented inference speedups (up to 61% faster prefill, 10x faster generation) by eliminating architectural bottlenecks.\n2. OneCAT successfully integrates three distinct modalities of operation (understanding, generation, editing) within a single set of weights, using clever components like the Modality-MoE for task-specific routing.\n3. The paper provides robust ablation studies that convincingly justify the design choices, particularly the novel understanding-distillation strategy and the necessity of the custom MLLM teacher."}, "weaknesses": {"value": "1.  The paper candidly notes a performance gap compared to top-tier *encoder-based* understanding-only models (e.g., Qwen2.5-VL-3B). While the authors reasonably attribute this to a significant (8x) difference in training data, it remains a limitation of the current model. It would be valuable to see scaling-law experiments projecting performance with comparable data.\n2.  The three-stage training pipeline is highly complex. It involves: 1) training a custom MLLM teacher, 2) a specialized \"Expert Pretraining\" stage with distillation, 3) unified mid-training, and 4) unified SFT. This complexity, along with the large and diverse data requirements, could be a significant barrier to reproduction and adoption.\n3. The ablation study (Table 8) shows that model performance is highly dependent on the *custom* MLLM teacher; using a standard Qwen2.5-VL teacher leads to training instability and worse results. This tight coupling is a potential weakness, as the overall system's success relies heavily on this carefully constructed (and non-trivial) teacher."}, "questions": {"value": "1.  You attribute the understanding performance gap to data scale (0.5T vs 4T tokens). Have you run any scaling experiments (e.g., on smaller models or data subsets) to analyze this trend and project at what data scale OneCAT might close the gap with specialized encoder-based models?\n2.  The custom teacher model is critical. The ablation shows it's superior to a standard Qwen2.5-VL teacher. Could you elaborate on *why* this is the case? Is it purely the parameter alignment (as suggested in Sec 4.3.1), or does the teacher's training (MLP-only) result in a different internal representation that is more \"distillable\" for the decoder-only student?\n3.  Figure 10 provides a helpful visualization for the SAA. Beyond this, could you provide any quantitative analysis of specialization? For example, do the different scale-specific adapters show different activation norms or gradient flows when processing tokens from their designated scales vs. other scales?\n4.  The CFG scales used seem very different: $\\lambda_t=20$ for text-to-image and $\\lambda_t=3$ for editing. A scale of 20 is quite high. How sensitive is the model to this hyperparameter? Is there a reason for this large discrepancy between tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CLiL2M9Kzm", "forum": "KldJntSnTZ", "replyto": "KldJntSnTZ", "signatures": ["ICLR.cc/2026/Conference/Submission3014/Reviewer_5Rua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3014/Reviewer_5Rua"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659497815, "cdate": 1761659497815, "tmdate": 1762916503124, "mdate": 1762916503124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OneCAT, a single decoder-only multimodal model for image understanding, text-to-image generation, and image editing. It combines a Modality-MoE (text, understanding, generation experts) with multi-scale autoregressive decoding via a Scale-Aware Adapter, and is trained using an encoder teacher with all-layer hidden-state distillation. Experiments show competitive quality across tasks with notably lower inference latency compared to encoder+diffusion pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors design a teacher tailored to the student’s architecture and distill from all intermediate layers, not just final-layer logits. In an encoder-free unified setup, this is a relatively novel choice and demonstrably improves stability and end-task performance for both understanding and generation.\n\n2. The paper clearly details the training and evaluation pipeline, making the overall methodology easier to follow and assess.\n\n3. The resulting unified model is more efficient than prior baselines, delivering better inference efficiency while maintaining comparable quality."}, "weaknesses": {"value": "1. Limited architectural novelty: The novelty of the model architecture is limited; it only combines previous encoder-free understanding models, such as EVE and VAR. The MoE architecture is also proposed in BAGEL.\n2. Unclear objective switch: No clear motivation for replacing the diffusion-like objective used in BAGEL with VAR.\n3. Uneven efficiency comparison. Reported speed/latency gains are measured against a larger BAGEL model; results would be more convincing with size-matched baselines or compute-normalized comparisons.\n4. “Encoder-free” but still reliant on vision components: Although inference is encoder-free, training depends on (i) a ViT-based teacher for understanding distillation and (ii) a multi-scale VAE tokenizer for generation.\n5. Formatting issues. The submission does not follow the official ICLR template, which affects readability."}, "questions": {"value": "1. The custom-trained teacher works better for distillation. Should we interpret this as the capability gap between teacher and student being smaller (and thus easier to match)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hbpStXAmGc", "forum": "KldJntSnTZ", "replyto": "KldJntSnTZ", "signatures": ["ICLR.cc/2026/Conference/Submission3014/Reviewer_GN52"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3014/Reviewer_GN52"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3014/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888926490, "cdate": 1761888926490, "tmdate": 1762916502839, "mdate": 1762916502839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}