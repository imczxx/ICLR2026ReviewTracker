{"id": "VIMW3eys6x", "number": 11980, "cdate": 1758204987537, "mdate": 1759897541438, "content": {"title": "Dual-Space Smoothness for Robust and Balanced LLM Unlearning", "abstract": "As large language models evolve, Machine Unlearning has emerged to address growing concerns around user privacy, copyright infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning methods often suffer from catastrophic forgetting and metric imbalance, for example by over-optimizing one objective (e.g., unlearning effectiveness, utility preservation, or privacy protection) at the expense of others. In addition, small perturbations in the representation or parameter space can be exploited by relearn and jailbreak attacks. To address these challenges, we propose PRISM, a unified framework that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a representation space stage that employs a robustly trained probe to defend against jailbreak attacks, and (ii) a parameter-space stage that decouples retain–forget gradient conflicts, reduces imbalance, and smooths the parameter space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE, across conversational-dialogue and continuous-text settings, show that PRISM outperforms SOTA baselines under multiple attacks while achieving a better balance among key metrics.", "tldr": "", "keywords": ["AI safety", "Large Language Model Unlearning", "Robustness", "Jailbreak Attacks", "LLM Safety", "Relearning Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba1762a175e337ca654e12c083c8ab9f4a80b94d.pdf", "supplementary_material": "/attachment/0e92382f6ba5722c395437113f8519a868afc68b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes PRISM, a unified \"dual-space smoothness\" unlearning framework that couples representation-space robustness (via a trained probe) with parameter-space smoothing (SAM-style), explicitly targeting both jailbreak (Prefill, AutoDAN, Multi-turn) and relearning attacks (retrain unlearned models on forget-samples)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Method engineering: Integrates three components: (1) robust probe guidance, (2) weight-space smoothness, and (3) gradient orthogonalization, into a min-max training loop. The components are modular and reusable in other unlearning pipelines.\n+ Evaluates across two distinct scenarios, including continuous text and conversational dialog, and two model families. Covers multiple attack types (prefill, AutoDAN, multi-turn) and relearning scenarios. Includes an ablation study on representation smoothness, parameter smoothness, and gradient decoupling, along with sensitivity analyses on hyperparameters and probe layer selection."}, "weaknesses": {"value": "## Major Issues\n**Motivation, notation, and threat model:**\n1. *Superficial motivation*: The paper shows GA and SAM+NPO failing with utility collapse, but does not investigate why these methods fail or whether simpler fixes (e.g., with retain loss or regularization) could address the issues. The collapse may represent *poor hyperparameter choices rather than fundamental method limitations*.\n2. *Severe mathematical notation and formulation issues*: \n+ Undefined notation: For example, lines 135–141, the paper states that Let $z(x):= g(h(x))$ denote the PCA-reduced representation,” and  \"Define the acceptance center $c_{a} := \\frac{1}{|\\mathcal{D}_{a}|} \\sum_{x \\in \\mathcal{D}_a} z(x)$...\".  \n\nHowever, the functions $g$, $h$, and dataset $\\mathcal{D}_a$ are not formally defined.  The dimensionality of $g$ and $h$ is also missing.\n\n+ Inconsistent notation: For example, lines 211-212 (Section 4.1): \"$z(x):= h_{\\theta_{0},L}(x) \\in \\mathbb{R}^{d} ....$\" This contradicts the earlier. In Eqn. 8, $\\mathcal{L}_{\\text{gen}}$ is undefined. What is this loss component? I spotted several similar issues across the paper; I will not correct everything. This is a major problem for a paper at this venue, as it makes the paper hard to understand and less clear. \n3. *One more critical point is that the unlearning problem formulation and threat model are missing*: The paper discusses jailbreak and relearning attacks, but lacks a formal threat model: What does the adversary know about the victim's architecture, data, or training procedure? Can the adversary query the model during attacks? What are the budget and computational resources constraints of the adversary? Defining a clear threat model and problem formulation provides a clearer picture of when the method should or should not be expected to hold. \n\nTo my taste, *the motivation is weak, and the presentation is poor*. \n\n**Methodology**:\n\n1. The idea of the paper is motivated by SAM, and SAM is well established for machine unlearning [1]. Perturbing latent space to defend has been explored for a long time [2]. The idea of gradient projection/orthogonalization so that the forget gradient is orthogonal to the retain gradients is well-established in machine unlearning, such as [3],[4],[5],[6]. PRISM appears to recombine these ideas into one. Method originality is weak, in my opinion.\n2. PRISM is a min–max optimization based unlearning framework aimed to improve robustness, balance forgetting, utility, and stability. However, the paper did not discuss the **theoretical analysis of convergence properties or optimality guarantees for the min-max formulation**. \n3. All the analysis in the paper uses the first-order Taylor approximation. First, this assumption used for approximations should be stated more clearly. Importantly, I am not convinced that this approximation is good when the *added perturbation is relatively large*. Deep networks are highly nonlinear, so I wonder if there is a way to justify this assumption better. An empirical analysis of the approximation error would make the approximation more convincing.\n4. The paper claims to enforce \"dual-space smoothness,\" but the mathematical connection between representation space and parameter space smoothness is not rigorously established. Smoothness in each space has been shown to improve model robustness. Why smoothness in both spaces should improve robustness?\n5. Probe-guided forgetting seems like we know exactly attacker is, then use the attacker's knowledge to develop the defender; this scenario seems impractical.\n6. There is a well-known phenomenon in adversarial machine learning that, when defending against a specific kind of attack, might cause an increase in vulnerability to other attacks [7].  Not investigating whether PRISM introduces new vulnerabilities is a critical omission.\n\n**Experiment**\n\n1. PRISM uses two-stage training (probe + unlearning), other baselines use single-stage training. I wonder whether this comparison is fair. In a continuous unlearning setting, where unlearning requests arrive over time, how would PRISM be applied? Would it require retaining the probe?\n2. Some important baselines are missing, such as Representation Misdirection for Unlearning [10].\n3. The two-stage training (probe + unlearning) significantly increases computational overhead, for example, in Table 1, PRISM: 11.223 seconds/step vs. NPO: 6.475 seconds/step (~73% overhead).\n4. Comparisons to other defense methods are missing, making it difficult to assess PRISM's effectiveness.\n5. Missing recent works on robust unlearning, such as [8][9].\n6. Relearning attacks use \"randomly sampled small subsets\" (Appendix line 1539), yet the exact subset sizes are not specified, and no analysis is provided on the variability across different random samples.\n7. Layer hyperparameter sensitivity: PRISM depends on selecting a specific intermediate layer to extract the representation. The probe's effectiveness is highly sensitive to layer choice, as different layers encode different types of information. Is there a principal way to find the optimal layer?\n8. Experiments primarily on 7B-8B models; scalability to larger models unclear.  It would be nice if the paper discusses PRISM's performance on larger models and other architectures, such as Mixture-of-Experts. \n\n## Minor issues\n+ Line 159, line 162:  Figure 3b/a not Figure 2b/a\n+ Line 222-223: notation error.\n\nI suggest the authors should carefully proofread their paper before updating the script.\n\n## References\n[1] Fan, Chongyu, et al. \"Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond.\" Forty-second International Conference on Machine Learning.\n\n[2] He, Zhezhi, Adnan Siraj Rakin, and Deliang Fan. \"Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[3] Shamsian, Aviv, et al. \"Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization.\" arXiv preprint arXiv:2503.02312 (2025).\n\n[4] Pan, Zibin, et al. \"Federated unlearning with gradient descent and conflict mitigation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 19. 2025.\n\n[5] Hoang, Tuan, et al. \"Learn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024.\n\n[6] Wang, Yue, et al. \"GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs.\" arXiv preprint arXiv:2503.09117 (2025).\n\n[7] Tramer, Florian, and Dan Boneh. \"Adversarial training and robustness for multiple perturbations.\" Advances in neural information processing systems 32 (2019).\n\n[8] Łucki, Jakub, et al. \"An Adversarial Perspective on Machine Unlearning for AI Safety.\" Transactions on Machine Learning Research.\n\n[9] Thaker, Pratiksha, et al. \"Position: Llm unlearning benchmarks are weak measures of progress.\" 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2025.\n\n[10] Li, Nathaniel, et al. \"The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning.\" International Conference on Machine Learning. PMLR, 2024."}, "questions": {"value": "+ How does the quality of the probe affect unlearning performance and robustness?\n+ The WMDP benchmark consists of 2 forget sets (WMDP biology and WMDP cyber) alongside 2 retain sets corresponding (can use Wikitext instead, as in [10]). Regarding Section 5.1, line 314-315, the paper states that \"the forget set is composed of questions randomly sample from the paraphrased WMDP-bio\". What is the \"paraphrased\" WMDP-bio? Why do we need the retain data generated by the unlearned models with WMDP queries?\n+ Please see Major Issues"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBEs1uKc4h", "forum": "VIMW3eys6x", "replyto": "VIMW3eys6x", "signatures": ["ICLR.cc/2026/Conference/Submission11980/Reviewer_KUGw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11980/Reviewer_KUGw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848708864, "cdate": 1761848708864, "tmdate": 1762922976362, "mdate": 1762922976362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PRISM, a machine unlearning framework that enforces smoothness in the representation space and parameter space. In the representation space, they adversarially train a robust classifier that separates the representation of the forget set and the retain set. Then, they consider the minimax of a perturbed loss that combines the NPO loss and a guidance term from the trained robust classifier. When finetuning on this loss, they project the gradient to the orthogonal space of the gradient of the retain set, to protect utility. They also apply Sharpness Aware minimization here to reach smooth regions. On two models and three datasets, they show their framework achieves better Unlearn Score (in terms of verbatim memorization), while preserving utility and improving robustness to relearning (for up to 100 steps) and multiple jailbreak attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Using adversarial robustness for unlearning in both representation space (for defensing jailbreak attacks) and the parameter space (for relearning attacks) is an interesting idea. \n- Their empirical evaluations corroborate the claimed performance (better unlearning utility tradeoff, and robustness against these attacks) of the proposed method."}, "weaknesses": {"value": "- Several claims are not directly validated.  For example, is it possible to quantify margins for small datasets to directly show that PRISM widens the margin in the representation space? There is also limited discussion on the probe capacity, pooling, and sensitivity to the adversarial radius. This makes the robustness claim a bit unclear. \n\n- Relearning robustness is varied by steps only. The evaluation does not sweep the size of the available forget subset. \n\nMinor clarity issue: $\\mathcal{L}_{\\text{gen}} $ is not explicitly defined in the main text"}, "questions": {"value": "PRISM’s representation-space component hinges on a robust probe separating forget vs retain features at some layer. How does the method behave when these distributions are not separable at any layer (e.g., PII removal where forget and retain are near-identical except for personal tokens)? In this setting, can the probe meaningfully guide updates, or does it worsens the unlearning score? Does the orthogonal projection simply stall progress when gradients are highly colinear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uhiDcrMaYt", "forum": "VIMW3eys6x", "replyto": "VIMW3eys6x", "signatures": ["ICLR.cc/2026/Conference/Submission11980/Reviewer_6fND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11980/Reviewer_6fND"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927973631, "cdate": 1761927973631, "tmdate": 1762922973830, "mdate": 1762922973830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM (Probe-guided Iterative Smoothness Minimization) — a novel dual-space unlearning framework for large language models (LLMs). The key idea is to enforce smoothness in both representation and parameter spaces to achieve robustness against jailbreak and relearning attacks while balancing unlearning effectiveness, utility preservation, and privacy protection.\nExperiments on MUSE-Books, MUSE-News, and WMDP-bio show PRISM consistently outperforms baselines (NPO, SAM+NPO, DOOR, Task Vector) in both unlearning performance and robustness under adversarial attacks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Unified framework: Bridges representation-space and parameter-space robustness under a single smoothness-driven formulation.\n\nBalanced performance: Demonstrates superior trade-offs among unlearning effectiveness (UE), post-unlearning performance (PP), and privacy leakage.\n\nStrong empirical validation: Outperforms all baselines across MUSE and WMDP tasks, with consistent robustness to both jailbreak and relearning attacks.\n\nComprehensive evaluation: Includes ablations (Table 4), parameter studies, and diverse attack types.\n\nReproducibility: Code and dataset details are clearly provided."}, "weaknesses": {"value": "The formal connection between probe-based representation smoothness and overall model generalization could be elaborated more rigorously.\n\nNo ablation on the computational overhead of dual-space optimization."}, "questions": {"value": "How sensitive is PRISM to the probe layer selection? Could higher transformer layers generalize better for representation-space smoothness?\n\nCan PRISM be extended to behavioral unlearning (e.g., refusal tuning or bias mitigation)?\n\nWhat is the empirical trade-off between unlearning speed and robustness when scaling to 70B+ models?\n\nDoes the probe retain knowledge of harmful content after training, potentially introducing privacy concerns?\n\nCould PRISM integrate into a continual unlearning pipeline (e.g., for incremental data deletions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VzSWC9yth4", "forum": "VIMW3eys6x", "replyto": "VIMW3eys6x", "signatures": ["ICLR.cc/2026/Conference/Submission11980/Reviewer_hH6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11980/Reviewer_hH6e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942928911, "cdate": 1761942928911, "tmdate": 1762922973444, "mdate": 1762922973444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PRISM, a unified framework to enhance the robustness and balance of large language model (LLM) unlearning. Existing methods often over-optimize one objective—like forgetting effectiveness or utility preservation—while remaining vulnerable to relearning and jailbreak attacks. PRISM tackles these issues by enforcing dual-space smoothness across both the representation and parameter spaces. In the representation space, a robustly trained probe defends against jailbreak manipulations by promoting feature-level smoothness. In the parameter space, Sharpness-Aware Minimization (SAM) and gradient decoupling between retain and forget objectives ensure stability and prevent the model from relearning forgotten knowledge. Extensive experiments on WMDP and MUSE benchmarks show that PRISM outperforms prior unlearning baselines (e.g., NPO, GA, DOOR, SAM+NPO) with improved robustness, better forgetting–utility balance, and higher resistance to adversarial attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a novel dual-space smoothness framework (PRISM) that jointly regularizes representation and parameter spaces, offering a fresh and well-motivated perspective on robust LLM unlearning.\n2.\tThe min–max formulation and incorporation of Sharpness-Aware Minimization (SAM) are conceptually sound and effectively connected to the goal of preventing both jailbreak and relearning attacks.\n3.\tThe paper conducts comprehensive experiments on multiple benchmarks (WMDP, MUSE) and clearly demonstrates improved robustness and better trade-offs between unlearning, utility, and stability compared with prior methods."}, "weaknesses": {"value": "1.\tThe method introduces considerable architectural and computational complexity (e.g., robust probe, dual-space optimization, gradient orthogonalization) without providing a clear runtime or memory analysis to justify the added cost.\n2.\tThe comparative baselines are somewhat limited and do not include more recent or parameter-efficient unlearning approaches, making it difficult to fully assess PRISM’s relative advantage.\n3.\tSome theoretical explanations lack formal grounding, particularly the connection between representation-space smoothness and jailbreak robustness, which is presented intuitively but without rigorous analysis or ablation evidence."}, "questions": {"value": "1.\tHow sensitive is PRISM to the choice of layer used for the representation-space probe? Would using deeper or earlier layers change the trade-off between forgetting and utility?\n2.\tCould the authors clarify the mathematical connection between representation-space smoothness and jailbreak robustness? Is there any empirical ablation isolating the probe’s contribution?\n3.\tHow does PRISM perform under multiple sequential unlearning requests or when the forget set is large? Does smoothness enforcement accumulate interference?\n4.\tCould the authors provide ablation results under the orthogonal gradient setting when each smoothness component (PS or RS) is removed individually? It would be helpful to see how the model behaves when parameter-space smoothness or representation-space smoothness is independently disabled, to better understand their respective contributions beyond the combined RS + GCD ablation shown in Table 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ufU3XcFUyw", "forum": "VIMW3eys6x", "replyto": "VIMW3eys6x", "signatures": ["ICLR.cc/2026/Conference/Submission11980/Reviewer_3Qb5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11980/Reviewer_3Qb5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958522352, "cdate": 1761958522352, "tmdate": 1762922972946, "mdate": 1762922972946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}