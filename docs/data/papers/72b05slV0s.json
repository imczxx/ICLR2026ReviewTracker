{"id": "72b05slV0s", "number": 14473, "cdate": 1758236730589, "mdate": 1759897368086, "content": {"title": "Efficient Test-time Scaling via Iterative Deepening", "abstract": "Recent reasoning models, such as OpenAI’s O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we focus on whether LLMs can benefit from matching the pattern of correct responses. Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500, AIME, and GPQA-diamond benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.", "tldr": "In this paper, we propose ID-sampling, a novel test-time sampling algorithm that can more efficiently sample correct responses on reasoning tasks.", "keywords": ["Large language models", "reasoning", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/59eacff48d880ab69fbabe3068cb18085b45030b.pdf", "supplementary_material": "/attachment/0a17d42afbc770f8c1255d0e61e578d2966a5269.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ID-Sampling, a test-time scaling method that inserts lightweight trigger sentences (e.g., *“Wait, let me reconsider.”*) at appropriate positions to induce self-correction during reasoning. The method is conceptually simple and model-agnostic. It introduces a trigger position predictor and performs multiple inference branches, selecting answers based on vote consensus. Experiments on AIME’24/25, GPQA, Math-500, and other reasoning benchmarks demonstrate that ID-Sampling improves pass@k accuracy under equivalent total sample budgets. The authors provide extensive analysis on linguistic markers, trigger placement, and interactions with model scale."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Originality**: While test-time scaling has been explored recently, the idea of using lightweight trigger insertions to induce self-correction is novel and orthogonal to simple sampling or self-consistency strategies. The paper highlights an interesting emergent behavior of modern LLMs.\n* **Quality**: Empirical results are strong and consistent across diverse reasoning tasks. The *Equivalent N* setting is carefully designed to ensure fair comparison among sampling-based baselines.\n* **Clarity**: The paper is clearly written, well-structured, and provides intuitive explanations of why the approach works. Figures illustrating trigger positions and linguistic markers further help understanding."}, "weaknesses": {"value": "1. **Lack of variance and multi-run reporting.**\n   Because reasoning evaluation with sampling is high-variance (especially on AIME-like tasks with only 30 questions), results should be averaged over multiple random seeds or include standard deviations / confidence intervals. The current reporting appears based on single runs, which weakens statistical reliability.\n\n2. **Natural trigger generation is acknowledged but not analyzed.**\n   The paper explicitly notes that the model may naturally produce trigger-like sentences *before* manual insertion. However, it does not quantify:\n\n   * the frequency of such cases,\n   * whether redundant triggering harms or saturates reasoning,\n   * or how forced triggers interact with already-correct trajectories.\n     This represents a missing failure-mode analysis.\n\n3. **Limited exploration of the γ hyperparameter.**\n   Although γ tunes allowed insertion positions, its scaling behavior is only partially analyzed. It is unclear whether larger γ values eventually hurt performance."}, "questions": {"value": "1. **On experimental stability:**\n\n   * Are results averaged over multiple inference runs / seeds?\n   * If so, please report mean ± std or confidence intervals.\n   * If not, can the authors comment on observed variance across seeds?\n\n2. **On natural triggers:**\n   Section 4 acknowledges that models sometimes spontaneously generate trigger markers (e.g., *“wait”*). Could the authors quantify:\n\n   * how often this happens,\n   * how it affects ID-Sampling performance,\n   * and whether forced insertion becomes redundant or harmful?\n\n3. **γ scaling behavior:**\n   Have the authors explored performance when γ continues increasing beyond the reported values? Is there a tipping point where too many potential trigger positions reduce accuracy by distracting the model?\n\n\n### **Additional Suggestions**\n\n* The related work section is missing discussion on two highly relevant works on efficient test-time scaling:\n\n  1. Inference scaling laws: An empirical analysis of compute-optimal inference for LLM problem-solving. ICLR 2025\n  2. Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling. NeurIPS 2025\n\n  Both works analyze compute allocation under sampling, which overlaps with the core motivation of this paper. Please discuss both works on Related work section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "66Q4jfe37O", "forum": "72b05slV0s", "replyto": "72b05slV0s", "signatures": ["ICLR.cc/2026/Conference/Submission14473/Reviewer_1ofN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14473/Reviewer_1ofN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812665629, "cdate": 1761812665629, "tmdate": 1762924873572, "mdate": 1762924873572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a sampling technique that can further scale test-time computation for reasoning models and deliver better performance on reasoning-intensive benchmarks. They showed the theoretical result + empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* empirical validation + ablation study\n* short theory result"}, "weaknesses": {"value": "* presentation is a bit confusing, didn't show experiments until the end\n* Captioning of FIgure 4/5/6 very difficult to parse\n* improvement over baseline is quite small at k=16, which seems to suggest the improvement ultimately diminishes as k increases"}, "questions": {"value": "Do authors believe benefit of ID would scale as people choose larger k?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lrYw8nx8md", "forum": "72b05slV0s", "replyto": "72b05slV0s", "signatures": ["ICLR.cc/2026/Conference/Submission14473/Reviewer_haoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14473/Reviewer_haoU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016012352, "cdate": 1762016012352, "tmdate": 1762924873089, "mdate": 1762924873089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies LLM behaviors with \"longer reasoning\" by inserting phrases like \"wait\" during reasoning to trigger rethinking behavior. Using three methods (BoN, Pass@N, Majority Voting) across three datasets MATH, AIME, GPQA the paper proves to be more effective than S1.\n\nThe method is effective on both reasoning/non-reasoning models tested on a diverse family of models Phi, Llama, R1-Distill, and Qwen3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The proposed method is supported by enough experiments that I am persuaded that this method of inserting \"wait\" will work in the way paper describes it."}, "weaknesses": {"value": "(1) In terms of novelty I am not sure how helpful this is to the field. The fact that we can boost performance by simply inserting a few words have already been shown by S1. Though the paper differs to S1 in that it comes up with their own schedule based on their observations how llm reasoning emprically behaves, I dont think its novel enough.\n\n(2) Some tables are misleading. For instance Table 1.b at N=32 both Vanilla and Id-Sampling scores 20.69 for cons@N, however the paper highlights Id-sampling only. \n\n(3) We can easily see most of the performance gains are relatively small. Especially for Table2(AIME2025) the performance gap is mostly <6 points. Considering AIME only has 30 questions Im not sure if this is a statistically confident performance gain.\n\n(4) While the paper has a short mention on the justification of their selection of the trigger sentence, it sounds extremely empirical. I wouldn't be surprised if their is a random word that just works better than 'wait'."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mahj3cr8ta", "forum": "72b05slV0s", "replyto": "72b05slV0s", "signatures": ["ICLR.cc/2026/Conference/Submission14473/Reviewer_zWVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14473/Reviewer_zWVa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059767560, "cdate": 1762059767560, "tmdate": 1762924872662, "mdate": 1762924872662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}