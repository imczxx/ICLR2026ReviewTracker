{"id": "vhfgC7QG4r", "number": 7356, "cdate": 1758017542665, "mdate": 1759897857620, "content": {"title": "Causal Regression: Learning Causal Mechanisms for Robust and Interpretable Prediction", "abstract": "The performance of standard regression models, which primarily learn statistical associations, is vulnerable to label noise. \nThis paper proposes Causal Regression, a paradigm that shifts the focus toward learning invariant causal mechanisms. \nWe introduce CausalEngine, a neural architecture that operationalizes this framework. It first performs abduction to infer a distribution over latent cause, and subsequently applies a causal mechanism to make a prediction. \nThe mathematical properties of the Cauchy distribution facilitate an analytical inference process. This design sidesteps the need for sampling-based approximations, thereby eliminating the high-variance gradients and computational overhead they introduce, leading to stable and efficient end-to-end training.\nThis design also provides a structured form of interpretability by decomposing predictive uncertainty into two distinct sources: epistemic uncertainty, arising from incomplete knowledge of an individual, and aleatoric uncertainty, stemming from inherent environmental randomness.\nOur experiments demonstrate CausalEngine's significant robustness against label noise. Especially in high-noise regimes where strong baselines falter, our approach exhibits a significantly smaller drop in performance. This work suggests that shifting the modeling focus from statistical associations to causal structures is a promising direction for building AI systems that are more reliable and interpretable.", "tldr": "", "keywords": ["Robustness; regression; Causal Mechanisms"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9be4bb117149df29abbde1cdfdeee3be1a24b2df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CausalEngine, a new framework aiming to incorporate causality into regression tasks by first inferring a distribution over latent factors $U$ from observations $X$, and then predicting $Y$ via a linear mechanism $Y=f(U,\\epsilon)$ where $\\epsilon$ is an irreducible error term. The authors validate their approach on both synthetic and real datasets, highlighting its robustness to label noise, including experiments in which 40% of  $Y$ values are corrupted. They further explore different corruption schemes, showing that the method maintains performance across multiple types of label perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a relevant and well-known limitation of standard machine learning models: the lack of causal reasoning, which is particularly important in out-of-distribution or noisy settings.\n\n- The authors explore extracting latent causal variables from observed data, an approach that has proven successful in the literature (see, e.g., references in W3)."}, "weaknesses": {"value": "Major: \n\n- **W1**: I have found the presentation hard to follow, which makes it difficult to assess the soundness of the approach:\n   - It is unclear to me where the causal component lies within the proposed architecture. As far as I understand, $Z$, from which the distribution of $U$ is constructed, is an encoding of $X$, and therefore aggregates information from $X$ that may include both causal and spurious factors. Are the components of $U$ intended to represent the true causal factors of $X$? If so, how can the authors ensure that these components correspond to the actual causal variables generating $X$, rather than to features merely correlated with it? \n\n    - It is unclear to me how the inferred latent $U$, which seems not explicitly disentangled into interpretable factors, can support interventional reasoning, a core aspect of causal inference.\n\n   -  It is also unclear to me how CausalEngine can be considered “interpretable-by-design,” as claimed by the authors (e.g., l. 462), since $U$ itself does not appear to be interpretable.\n\n- **W2**: The manuscript largely omits discussion of related literature in causal representation learning, see e.g., [1-3], which seems closely related to the proposed ideas. This omission makes it difficult to assess the novelty of the approach.\n\n- **W3**: All the paper’s results lack an estimate of the experimental uncertainty, i.e., an estimate of results’ variability subject to the random initialization with different random seeds. This is critical to ensure significance of the results and all associated claims.\n\n- **W4**: Table 2 is somewhat confusing: it appears that the Exogenous model is more robust than the proposed standard model. Did I understand this correctly? Moreover, the different baselines are not clearly defined prior to the table.\n\n- **W5**: I could not find a thorough discussion of the limitations in the main text.\n\nMinor:\n- Related works are relegated to the appendix.\n- Key elements, such as SCM definitions, are only in the appendix and should be recalled in the main text.\n- Figure 2 is small and in low resolution, which makes it difficult to read.\n\nReferences:\n\n[1] Locatello et al., 'Challenging common assumptions in the unsupervised learning of disentangled representations'. ICLR 2019.\n\n[2] Ahuja, et al., 'Interventional causal representation learning'. ICML 2023.\n\n[3] Mitrovic, et al., 'Representation learning via invariant causal mechanisms'. arXiv, 2020."}, "questions": {"value": "- **Q1** (Related to W1) - Could the authors provide a theoretically grounded analysis of why the components of $U$ can be expected to correspond to the true causal factors generating $X$?\n\n- **Q2** (Related to W2) - Could the authors provide more discussion and experiments on interventions, illustrating whether the inferred $U$ can be used for interventional reasoning?\n\n- **Q3** (Related to W3) – Could the authors position their work within the causal representation learning literature?\n\n- **Q4** (Related to W4) - Could the authors repeat their experiments using different random seeds and report the individual results and standard deviations?\n    \n- **Q5** - I am somewhat surprised by the strong performance reported when 40% of the labels are randomly corrupted. Could the authors provide further discussion or analysis on this behavior? Could it be that the model is modeling label noise observed during training through the exogenous noise term?\n\n\nWhile I remain open to a constructive discussion, I believe the paper requires substantial improvement. At this stage, the gap between the current submission and a version that would meet the bar for acceptance still feels wide."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jYfOIeSQRY", "forum": "vhfgC7QG4r", "replyto": "vhfgC7QG4r", "signatures": ["ICLR.cc/2026/Conference/Submission7356/Reviewer_EzQ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7356/Reviewer_EzQ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750011808, "cdate": 1761750011808, "tmdate": 1762919492844, "mdate": 1762919492844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a regression framework for learning predictive models that aim to capture invariant causal mechanisms rather than statistical associations. The proposed architecture, \"CausalEngine\", decomposes prediction in four stages: Perception, Abduction, Action, and Decision, in an attempt to conceptually mirror the causal reasoning pipeline from Pearl's framework.\n\nThe proposed pipeline works as follows: Given input features $X$, a perception module extracts a representation $Z$, and then an abduction module infers a latent cause $U$. The latent variable $U$ is modeled as a Cauchy distribution whose location and scale parameters are predicted from $Z$. A linear causal mechanism then maps $U$ (after injecting Cauchy noise) to a decision score $S$, from which the final prediction $Y$ is obtained. The combination of the Cauchy distribution and linear mechanism (from $U$ to $Y$) ensures analytic tractability, allowing uncertainty to be propagated in closed form without sampling. The model is trained by minimizing the negative log-likelihood of the true label under the predicted Cauchy distribution.\n\nThe experiments evaluate robustness under different kinds of label noise (shuffle, outlier, asymmetric, and systematic). Across these conditions, CausalEngine reports smaller degradation in performance compared to other regressors."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "[Originality] Proposes a regression model that performs non-linear extraction of a latent representation followed by a closed-form Cauchy-based linear prediction layer.\n\n[Quality] The analytical tractability of the Cauchy-linear formulation is well justified and demonstrated.\n\n[Quality] The codebase is clearly written and well commented. While the files are quite large and the structure is not really modular, it's overall well-suited for an open-source community.\n\n[Quality] The model shows robustness improvements for the chosen setting through comprehensive experiments, though whether this setting captures the intended causal challenges is discussed below."}, "weaknesses": {"value": "(I write each weakness in a bulletpoint and then add details below)\n\n- Misuse of causal terminology.\n\nThe model does not perform Pearlian *abduction-action-prediction*: there are no interventions or counterfactual predictions. The \"abduction\" step is inference of a latent variable from data, not abduction in the causal sense.\n   \nIn Pearl's framework [1]:\n\n1. Abduction = infer the posterior over exogenous (noise) variables $U$ given observed evidence $E$\n2. Action (Intervention) = modify the model (e.g. $do(X = x)$)\n3. Prediction = compute ($P(Y | do(X = x), U)$), i.e., counterfactual reasoning.\n\nIn contrast, in this paper:\n\n1. Perception: learn a representation ($Z = f(X)$) (this is ok)\n2. Abduction: infer a latent embedding ($ U \\sim p(U|Z)$)\n3. Action: compute a linear transformation ($S = WU + b$) (with noise)\n4. Decision/Prediction: map $S$ to $Y$ through a simple transformation.\n\nThe \"abduction\" in this paper is latent inference, not inference over exogenous noise.\nThe \"action\" is a deterministic linear mapping, not an intervention.\nThe Decision/Prediction is output generation and not counterfactual prediction.\n\nImportanly: the $U$ in the paper **is not** the same as Pearl's exogenous variables.\nExogenous variables are conceptually non-causal. They don't cause each other, they just determine the randomness of the world. In this paper, the $U$ is the latent cause of $Y$, not a background noise variable. In Pearl's framework, you can't infer exogenous variables from observations (because they are independent sources of noise), while here the authors train a NN to map $X\\rightarrow{}U$.\n\nWhile it's ok to be metaphorical and use causal inference ideas for inspiration, if there is no formal connection then the paper needs to be reframed.\n\nOther examples of inaccurate/imprecise usage of terminology (not a complete list):\n (line 054-055): the usage of abduction differs from its standard meaning in the causal inference literature,\n (line 152-153): and $u$ and $e$ are undefined.\n    \n[1] Pearl J., \"Causality: Models, Reasoning, and Inference\", Cambridge University Press, 2009\n\n\n- Unsupported interpretability claims.\n\nThe architecture's intermediate variables $Z$, $U$, and $S$ are unlabeled latent representations, not semantically grounded causes. The paper equates structural modularity with interpretability, but provides no experiments, visualizations, or analyses showing that these representations correspond to meaningful factors. Hence, the model is not inherently interpretable - it's only structured.\n\nExample lines where this is mentioned:\n (line 75): \"interpretability-by-design\",\n (line 115): \"transparent interpretability by design, moving beyond the limitations of post-hoc explanations\" - Interpretability would still require post-hoc inspection to understand what $Z$, $U$, and $S$ represent,\n and (line 124).\n\n\n- Arbitrary theoretical justification.\n   \nThe link between \"linear causality\" and reward modeling in large language models is conceptually unsound. The cited paper [2] is not evidence that real causal mechanisms are linear. Likewise, describing the Cauchy distribution as \"the language of causality\" is rhetorical, not theoretically grounded.\n\n[2] Ouyang et al., \"Training language models to follow instructions with human feedback\",  NeurIPS 2022\n\n\n\n- Restrictive modeling assumptions.\n\nThe method relies on two strong assumptions:\n1. Linearity of the causal mechanism.\n2. Cauchy-distributed latent and noise terms.\n\nThese assumptions severely limit the model's expressivity and applicability beyond simple or low-dimensional settings. The paper downplays this by referring to them as \"seemingly restrictive\" when they are in fact fundamental constraints. In the introduction, the assumptions are presented as \"philosophical insights\", but they are subjective design choices. While these assumptions do enable analytically tractable inference (a strength), the authors should acknowledge them as clear limitations rather than framing them as inherently correct or universally desirable.\n\n\n- Mismatch between motivation and evaluation.\n\nThe paper's framing (lines 142-149) focuses on brittleness under distribution shifts, confounding, and spurious correlations. However, all experiments are about label noise in otherwise i.i.d. settings. The experiments test robustness to annotation errors, not causal generalization or invariance across environments. \n\n\n- No mechanism to distinguish causation from correlation.\n\nThe method is trained purely on observational data, so it remains subject to the same correlation-causation ambiguities as standard regression. Without interventions, invariance constraints, or multiple environments/tasks, the model cannot distinguish causal from spurious relationships [1]. These issues only disappear in the special case where the true causal graph and functional form match *exactly* the model's assumed structure (which is extremely unlikely in practice).\n\n[1] Pearl J., \"Causality: Models, Reasoning, and Inference\", Cambridge University Press, 2009"}, "questions": {"value": "- Correct the use of causal terminology or reframe your paper as taking inspiration from Pearl's framework.\n\n\n- I'm not convinced that the causal framing adds value to this paper. The paper might be clearer if presented as a robust regression method. The model addresses an associational (level 1) prediction problem and does not involve interventions or counterfactual reasoning. While the terminology is \"causal-inspired\", the method itself does not perform causal inference. For instance, the derivation of the closed-form expression for $Y$ is probability algebra rather than causal calculus (e.g., backdoor adjustments or counterfactual reasoning).\n\n\n- Clarify the true problem setting. If the experiments demonstrate robustness to corrupted labels (and not to distribution shifts, for example), then the rest of the paper should be written from this perspective. If not, you need additional experiments (e.g. feature-shifting experiments).\n\n\n- Avoid philosophical overreaching. Replace slogans like \"language of causality\" or \"interpretability-by-design\" with precise technical descriptions. Avoid implying that the work establishes general causal robustness. Present your assumptions as pragmatic simplifications for analytic tractability and discuss their implications for generalization and where they might fail (e.g., nonlinear SCMs).\n\n- Either remove interpretability claims or perform some qualitative analysis (e.g., visualizing $U$ representations and relating them to known latent factors).\n\n- In the proposed future direction, you mention exploring \"non-linear decision functions\". Wouldn't this make $Y$ no longer Cauchy-distributed and thereby break the analytic tractability that the current framework relies on?\n\n- Consider choosing a more descriptive name for the method. \"Causal Regression\" or \"Causal Engine\" sound very general, while the proposed method is better described by its linear mechanism and Cauchy-distributed uncertainty, e.g., \"CauchyLinearRobustRegressor\". Analogously, in logistic regression, the term \"logistic\" is part of the method name to convey what the method does and its underlying assumptions.\n\n\n\nMinor:\n- Types of noise (shuffle, asymmetric, outlier, systematic) are defined only in the appendix, though they are central to the paper.\n- Reconsider how to introduce the Cauchy distribution in abstract. It's disconnected from the previous sentence.\n- Show both MdAE and mean-based metrics.\n- Figure 2 is too small to read. If it can't fit bigger in the main text, consider adding a LaTeX table in the main text and adding the figure in the abstract.\n- Double dash: -— in line 100\n- Double dash: —- twice in line 054-055\n- Reword \"we introduce two key assumptions\", assumptions are made or adopted, rather than introduced.\n- This question: \"What distributional assumptions and functional forms are appropriate for causality?\". \"for causality\" is too general, in practice, you're choosing assumptions that are appropriate for your particular setting, not for causality in general.\n- From the paper:\n\"These results support our core hypothesis: by modeling the underlying causal graph, the Causal Engine is more effective at disregarding spurious correlations introduced by label noise than traditional models that rely solely on statistical associations.\". In general, the method does not model the underlying causal graph. Also, the experiments don't demonstrate robustness to spurious correlations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fh6mED6tkV", "forum": "vhfgC7QG4r", "replyto": "vhfgC7QG4r", "signatures": ["ICLR.cc/2026/Conference/Submission7356/Reviewer_Pd3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7356/Reviewer_Pd3p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858151762, "cdate": 1761858151762, "tmdate": 1762919492528, "mdate": 1762919492528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a robust prediction framework called CausalEngine (CE), inspired by counterfactual inference (abduction, action and prediction). Given an input $X$, the model first extracts a representation $Z$ and infers a latent variable $U$ by modeling the posterior $p(U \\mid Z)$ (similar to $P(U \\mid X)$). The final prediction is then generated by applying a simple mechanism that maps $U$ (augmented with exogenous noise $\\epsilon$) to the output $Y$. Both the latent cause $U$ and the noise term $\\epsilon$ are modeled using Cauchy distributions. Experiments on both synthetic datasets and public regression benchmarks demonstrate that CausalEngine achieves improved robustness under various types of label noise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1**. The paper includes many strong baselines, and the proposed CausalEngine (CE) consistently demonstrates robustness under various types of label noise."}, "weaknesses": {"value": "**W1**. **Lack of theoretical justification**.\n\nThe paper lacks a theoretical analysis to support why CE should yield robust predictions. The current modeling and theoretical discussions are not sufficient to demonstrate that CE provides robustness guarantees. I strongly suggest formalizing the prediction problem and clearly stating under which assumptions CE is expected to produce robust results.\n\nFor example, consider the following SCMs:\n\n$$X \\leftarrow f_X(U_X), \\quad Y \\leftarrow f_Y(X, U_Y, \\epsilon)$$\n\nwhere $U_X$ is independent of $U_Y$. Then learning $P(U \\mid X)$ in CE does not help robustness: $P(U \\mid X)$contains no causal information about $Y$ beyond $X$ itself, and perturbing $\\epsilon$ will still affect the prediction.\n\nIn contrast, if the SCM is\n\n $$X \\leftarrow f_X(U_X, U_Y), \\quad Y \\leftarrow f_Y(U_Y, \\epsilon)$$\n\nthen learning $p(U \\mid X)$ may help, because $U_Y$ influences both $X$ and $Y$; inferring $U_Y$ could somewhat “recover” the direct cause of $Y$.\n\nSCMs here are only illustrative; other formalizations are possible.\nGrounding the method in clear assumptions would help readers understand when CE should be expected to improve robustness.\n\nAdditionally, since $U$ is unobserved, recovering the exact latent variable is generally impossible. **The paper should analyze whether CE can recover U up to an equivalence class**, i.e., whether different but equivalent representations still lead to robust predictions under the linear mechanism or Cauchy prior. Without such an analysis, the method may not generalize beyond the tested scenarios.\n\n**W2**. **Limited experimental scope.**\n\nThe experiments involve only datasets with relatively low $X$ dimensionality (fewer than 100 features). It is unclear whether CE scales to high-dimensional inputs. Moreover, the paper assumes that the final mapping from $U$ to $Y$ is linear (line 93–98), but the public and synthetic datasets do not validate whether this assumption holds.\n\nThe introduction claims inspiration from linear reward heads used in LLM alignment:\n\n*“Inspired by the success of reward modeling in large language models… where a simple linear head over a powerful representation can capture complexity…”*\n\nGiven this motivation, it would be compelling to test CE in a similar setting, e.g., using pretrained LLM embeddings as features for prediction on real-world tasks."}, "questions": {"value": "**Q1**. Although the paper is motivated by causal concepts, the procedure seems to reduce to (1) learning a stochastic representation and (2) predicting from that representation. This appears similar to existing methods such as Variational Autoencoders (VAEs), which learn a latent stochastic representation $Z$ from $X$, and then perform downstream prediction tasks using $Z$. What is the key conceptual difference between CE and VAE-style latent representation learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2J8vfaRLFs", "forum": "vhfgC7QG4r", "replyto": "vhfgC7QG4r", "signatures": ["ICLR.cc/2026/Conference/Submission7356/Reviewer_3K37"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7356/Reviewer_3K37"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762311168082, "cdate": 1762311168082, "tmdate": 1762919492157, "mdate": 1762919492157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Causal Regression, a regression task that frames the target outcome as a function of latent variables that must be inferred from the given data. The solution presented is called CausalEngine, which uses a counterfactual reasoning process of perception, abduction, action, and decision to achieve this result. Notably the abduction step fits a posterior Cauchy distribution over the latent variables given the data, and the action step models the causal mechanism that maps the latent variables to the target. An extensive set of experiments is provided to test the robustness of the method in noisy settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The reframing of general regression tasks in causal terms is an interesting novel concept.\n\n2. The assumptions of the Cauchy noise and the linear mechanism are clearly stated, justified, and emphasized as key components of the model.\n\n3. The experiments are very comprehensive and demonstrate clear empirical advantages over baselines."}, "weaknesses": {"value": "4. The causality aspect of the model seems undefined. While the action step is learning a “mechanism” for predicting the target, it is unclear how this is different from simply learning some kind of correlation between the latent variables and the outcome. There is no intervention being performed, it is unclear how confounding bias affects the system, and it is unclear how learning the latent variables mitigates this bias.\n\n5. The claims that the abduction step learns latent variables and that the action step learns a mechanism is not supported by any identification results. It is unclear whether any of the learned results has any guarantees to be causally meaningful.\n\nOverall, while I believe that the proposed method has interesting results, I do not quite understand what makes the model causal. If this is cleared up, I will raise my score."}, "questions": {"value": "6. Does the distinction between epistemic and aleatoric uncertainty in Eq. 3 and Eq. 6 affect performance? Could the $\\varepsilon$ term in Eq. 6 be omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "55wK2datYd", "forum": "vhfgC7QG4r", "replyto": "vhfgC7QG4r", "signatures": ["ICLR.cc/2026/Conference/Submission7356/Reviewer_3666"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7356/Reviewer_3666"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762340830383, "cdate": 1762340830383, "tmdate": 1762919491810, "mdate": 1762919491810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}