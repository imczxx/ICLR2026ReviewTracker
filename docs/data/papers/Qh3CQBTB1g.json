{"id": "Qh3CQBTB1g", "number": 16160, "cdate": 1758260814190, "mdate": 1759897257527, "content": {"title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation", "abstract": "With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models that underpin them. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.", "tldr": "", "keywords": ["long video gengration model;  video gengration benchmarks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6de15cf438f00db188e2b4f8b05bffa2edf7c1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NarrLV, a benchmark aimed at evaluating the narrative expressiveness of long video generation models. Drawing from film narratology, it defines a new unit called the Temporal Narrative Atom (TNA) to quantify narrative richness. The benchmark consists of:\n\n* An LLM-generated prompt suite with variable TNA counts to control narrative complexity.\n\n* An MLLM-based question–answering metric that evaluates three aspects: narrative element fidelity, narrative unit coverage, and narrative unit coherence.\n\nExperiments benchmark several recent video generation models (e.g., Wan, HunyuanVideo, FreeLong, FIFO-Diffusion) and report that NarrLV correlates well with human judgments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Multi-dimensional metric: The three-level evaluation (fidelity, coverage, coherence) provides a more nuanced understanding of narrative quality.\n\n* Quantitative validation: The authors include human–metric alignment tests showing reasonable consistency, supporting reliability.\n\n* Potential for extensibility: The benchmark can, in principle, scale with longer prompts or future models."}, "weaknesses": {"value": "* Synthetic prompts: All prompts are LLM-generated, which limits real-world representativeness and linguistic diversity. Real datasets such as VidProM (Wang & Yang, NeurIPS 2024) already offer large-scale, human-written long video prompts but are neither cited nor used.\n\n* Missing qualitative examples: The paper lacks visual examples of generated videos as supplementary material, reducing interpretability and reader engagement.\n\n* Limited practical insight: The analysis mainly confirms intuitive findings (e.g., models struggle with multi-event coherence) without providing new conceptual insights or design implications for improving generation models."}, "questions": {"value": "* Since all prompts are LLM-generated, how do the authors ensure they reflect real-world user prompts? Why not use or compare with real datasets like VidProM?\n\n* The appendix lacks model details. Can the authors provide inference settings (duration, FPS, resolution) and confirm all models were tested under equal conditions?\n\n* How stable are the MLLM-based metrics across different question templates or MLLMs? Any variance or seed analysis to support reliability claims?\n\n* Can the authors show prompt–video examples or visual comparisons to verify that the metrics align with perceptual quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XkdgVkDrsd", "forum": "Qh3CQBTB1g", "replyto": "Qh3CQBTB1g", "signatures": ["ICLR.cc/2026/Conference/Submission16160/Reviewer_5Gfb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16160/Reviewer_5Gfb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714486584, "cdate": 1760714486584, "tmdate": 1762926325839, "mdate": 1762926325839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a novel benchmark, NarrLV, to comprehensively evaluate the narrative expression capabilities of long video generation models. The evaluation is centered on elements inspired by film narrative theory, and an evaluation metric based on an MLLM-based QA framework. Experimental results identify the strengths and limitations of existing video generation models, where the insights can fuel future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Inspired by film narrative theory, the novel benchmark, NArrLV, defines the smallest narrative unit as Temporal Narrative Atom (TNA), which is a quantitative measure of narrative richness in generated video. It further identifies three key dimensions, i.e., scene attribute, target attribute, and target action, that influence the TNA. NarrLV contains a prompt suite, which can flexibly generate prompts with a desired TNA number.\n- For evaluation, this work follows a progressive narrative expression paradigm that focuses on narrative element fidelity, narrative unit coverage, and narrative unit coherence, and proposes an MLLM-based QA framework and the ground truth TNAs.\n- Evaluation makes several observations of the existing video generation models. The quantitative findings allow researchers to make informed decisions in model improvement in future work."}, "weaknesses": {"value": "- W1: I acknowledge that the narrative prompt suite is highly valuable. In lines 190-192, it describes the factors that influence the number of TNA is similar to TC-Bench. Can the author articulate the differentiating factor between TC-Bench and NarrLV? Specifically, I would like to understand what the novelty of NarrLV is that enables it to extend to prompts with a higher number of TNA counts.\n- W2: I acknowledge that the proposed benchmark suite and evaluation focus on the narrative expressiveness of the video generation models. However, it would be more comprehensive to understand other related metrics such as hallucinations, content coherence, and image quality."}, "questions": {"value": "- See Weakness 1\n- In line 242, why does the paper evaluate the models under 20x6x3 = 360 prompts? It is unclear to me what the 360 prompts actually cover. \n- Please elaborate on the alignment between the human preference annotations and the MLLM-based classification metrics. Specifically, since human annotation is based on subjective pairwise preference and the MLLM determines objective fulfillment of conditions (fidelity, coverage, coherence), how can we confidently derive that the metric is aligned with human annotation?\n- Please provide more information about the annotator. Specifically, their background, workload, and duration spent on each annotation task. Did the annotator get paid for the annotation task?\n\nSuggestions:\n- For Figure 2, it would be more intuitive to replace (b) Evaluated Models with a video generation step. I.e., given the (generated) prompt. A video generation model will generate a corresponding video, which will be used in the next stage (evaluation). The current listed models are specific to this work, and I believe the figure can be more general."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hpQcy9RSdk", "forum": "Qh3CQBTB1g", "replyto": "Qh3CQBTB1g", "signatures": ["ICLR.cc/2026/Conference/Submission16160/Reviewer_Ev65"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16160/Reviewer_Ev65"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760799658581, "cdate": 1760799658581, "tmdate": 1762926325269, "mdate": 1762926325269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the lack of adequate evaluation methods in the field of long video generation, specifically for assessing a model's ability to generate narrative content. The authors propose a new benchmark named NarrLV, the core idea of which is to draw from film theory to define the Temporal Narrative Atom to quantify the richness of narrative content. Authers further introduce an automated prompt generation pipeline and a question-answering evaluation system. The proposed metric encompasses three evaluative dimensions: narrative element fidelity, narrative unit coverage, and narrative unit coherence. The authors claim through experiments that this metric demonstrates high consistency with human judgment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of evaluating the narrative capabilities of long videos is an important and unresolved challenge in the current field.\n- The authors' attempt to construct a systematic, automated evaluation framework is a direction worth exploring.\n- The experimental results show a strong correlation with human preferences, which increases the benchmark's credibility as a proxy for human evaluation."}, "weaknesses": {"value": "**1. Significant Omission of Long Generation Method**\n\nFirst, I question the rationale for evaluating base models like WAN. on a \"long video\" benchmark. These models are fundamentally designed to generate short video clips. Evaluating them on tasks far outside their intended design (i.e., long video, which I would argue implies a duration of several minutes) does not seem to yield meaningful insights and may be an unfair comparison.\n\nMy primary criticism is the benchmark's almost exclusive focus on training-free methods. This has led to a very large oversight: the paper almost completely ignores the entire category of multi-shot long video generation. This area represents the true state-of-the-art in generating coherent, long-form narratives and sequences.\n\nTo be a comprehensive and valuable benchmark for the community, it is essential to include, evaluate, or at the very least, thoroughly discuss these more advanced methods. I strongly recommend the authors incorporate the following highly relevant works, which are central to the long video problem:\n\n- Captain Cinema: Towards Short Movie Generation\n- Skyreels-v2: Infinite-length film generative model\n- Moviedreamer: Hierarchical generation for coherent long visual sequence\n- MovieAgent: Automated Movie Generation via Multi-Agent CoT Planning\n- VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention\n\nEven for methods that are not open-source, a benchmark paper has a responsibility to cite and situate them within the landscape. Without addressing these hierarchical, agent-based, and multi-shot approaches, the paper's current contribution as a long video benchmark feels incomplete and overlooks the most relevant research.\n\n**2. Concerns on TNA**\n\nTNA simplifies \"narrative\" into a series of discrete physical state changes (attributes, actions). This ignores higher-level narrative elements like causal logic, character motivation, emotional development, or cinematic language. Therefore, NarrLV evaluates something more akin to \"sequential instruction following\" rather than true \"storytelling.\"\n\nThe upper limit for TNA in the experiments is only 6. While this reveals the bottlenecks of current models, it is far from the ultimate goal of long video. The benchmark's effectiveness in handling more complex, longer sequences (e.g., more than 10 TNAs) has not been validated."}, "questions": {"value": "Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iyesOYGr3i", "forum": "Qh3CQBTB1g", "replyto": "Qh3CQBTB1g", "signatures": ["ICLR.cc/2026/Conference/Submission16160/Reviewer_ixiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16160/Reviewer_ixiX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761114454448, "cdate": 1761114454448, "tmdate": 1762926324875, "mdate": 1762926324875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper indicates that current video generation models have advanced in producing long-duration clips, while their narrative capabilities have not been systematically evaluated by recent benchmarks. This paper proposes NarrLV, the first benchmark for long-form video narrative, which features dedicated metrics including element fidelity, unit coverage, and unit coherence. NarrLV is also presented as an end-to-end framework capable of automatically generating prompts and conducting evaluations. Comprehensive comparisons across 11 SOTA models through NarrLV reveal insights into the relation between prompt semantics and narrative units."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper serves a complementary role to VBench. As mentioned by the authors, the VBench series focuses on quality metrics such as controllability, commonsense, and physical plausibility, but notably does not cover narrative metrics.\n- The definition of TNA is novel and the proposed metrics are both reasonable and appropriate. Inspired by film narrative theory, the authors define TNA as the basic elements of video content. Building upon TNA, the three metrics quantitatively measure diversity, fidelity, and coherence. This contrasts with previous solutions, which involved simply throwing frames into VLMs and directly asking them about these abstract concepts. Therefore, NarrLV provides a much more convincing evaluation framework."}, "weaknesses": {"value": "- This paper does not evaluate more advanced, closed-source models, such as Sora2 and Kling. However, given the significant cost and access constraints associated with these models, this omission is understandable and acceptable for an academic research paper\n- While Section 3.2 describes the prompt generation pipeline in detail, it lacks a sufficient analysis of the resulting prompts. For example, it is recommended that the authors conduct a statistical analysis of the theme categories within the generated prompts. This would help researchers interpret the evaluation results more clearly."}, "questions": {"value": "Following up on the previously mentioned weakness, could the authors provide an illustration of the thematic distribution of the testing prompts? A visualization, such as pie chart, would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7H9l2nlmmY", "forum": "Qh3CQBTB1g", "replyto": "Qh3CQBTB1g", "signatures": ["ICLR.cc/2026/Conference/Submission16160/Reviewer_gGXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16160/Reviewer_gGXr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983896453, "cdate": 1761983896453, "tmdate": 1762926324433, "mdate": 1762926324433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}