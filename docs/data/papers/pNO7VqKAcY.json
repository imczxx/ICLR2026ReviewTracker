{"id": "pNO7VqKAcY", "number": 20586, "cdate": 1758307954404, "mdate": 1759896969399, "content": {"title": "Scaling Bayesian Experimental Design to High-Dimensions with Information-Guided Diffusion", "abstract": "We present DiffBED, a Bayesian experimental design (BED) approach that scales to problems with high-dimensional design spaces. Our key insight is that current BED approaches typically cannot be scaled to real high--dimensional design problems because of the need to specify a likelihood model that remains accurate throughout the design space. We show that without this, their design optimisation procedures exploit deficiencies in the likelihood and produce implausible designs. We overcome this issue by introducing a generative prior over feasible designs using a diffusion model. By guiding this diffusion model using principled information-theoretic experimental design objectives, we are then able to generate highly informative yet realistic designs at an unprecedented scale: while previous applications of BED have been restricted to design spaces with a handful of dimensions, we show that DiffBED can successful scale to designing high-resolution images.", "tldr": "", "keywords": ["Bayesian Experimental Design", "Active Data Acquisition", "Diffusion Models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60bfaaab0cc74a9576e92f6f05340bbc353e51a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DiffBED, a Bayesian experimental design method for high-dimensional design spaces that uses a pretrained diffusion model as a prior over feasible designs. The approach guides the reverse-diffusion process with gradients of expected information gain (EIG), producing proposals that stay on the data manifold (realistic) while remaining highly informative for learning the latent target. The authors argue that naïvely maximizing EIG in high dimensions can “reward-hack” likelihood misspecification; DiffBED mitigates this by sampling from an EIG-tilted prior rather than optimizing EIG directly. On image-level tasks—including 512×512 shoe designs—DiffBED outperforms standard BED and several strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly shows how naive EIG in high-dimensional settings can suffer from likelihood misspecification, which makes staying on the data manifold meaningful.\n\n2. Exponential tilting of a diffusion prior is a clean way to sample realistic, informative designs—no extra training required.\n\n3. This method uses a pretrained diffusion model, so there’s no retraining at every step which is time consuming for sequential BED.\n\n4. This method works on large images (512×512) and outperforms standard BED.\n\n5. This method improves the similarty to $\\theta_{true}$ , which indicates true target recovery."}, "weaknesses": {"value": "1. The paper doesn’t compare against a simple constrained EIG optimizer that stays on the data manifold (e.g., trust-region steps or directly maximizing EIG with a prior penalty). Without that apples-to-apples baseline, it’s hard to tell whether the gains come mainly from enforcing feasibility or from sampling the EIG-tilted prior itself.\n\n2. Exponential tilting can collapse to a single mode on multimodal landscapes, and picking a good $\\alpha$ is non-trivial because it trades diversity against feasibility.\n\n3. The method leans on a strong pretrained diffusion prior. If an appropriate prior doesn’t exist or is domain-mismatched, it may be inapplicable, and tilting can’t add mass where the prior has zero mass.\n\n4. Results focus on discrete outcomes; if y is continuous, EIG estimation and gradients are harder."}, "questions": {"value": "1. Did you try a direct EIG optimizer that stays on the data manifold (e.g., trust-region steps or EIG + prior penalty)? What’s your intuition for how it would perform vs. DiffBED?\n\n2. How sensitive are performance and diversity to $\\alpha$?\n\n3. On multi-peak landscapes for $p_{ref}(\\theta)$, do you see mode collapse when $\\alpha$ is small?\n\n4. What happens when the prior is biased or domain-mismatched—does it still produce meaningful designs?\n\n5. For continuous y, how would you estimate EIG, and which gradient estimator would you use (just a brief idea is fine)?\n\n*** Duplicate word *** \n\nline 222, by solving time-reversal of Equation (9)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G2rhCEdS6C", "forum": "pNO7VqKAcY", "replyto": "pNO7VqKAcY", "signatures": ["ICLR.cc/2026/Conference/Submission20586/Reviewer_nkQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20586/Reviewer_nkQr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948045481, "cdate": 1761948045481, "tmdate": 1762933993394, "mdate": 1762933993394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of scaling Bayesian Experimental Design (BED) to high-dimensional design spaces by first identifying a critical issue: directly optimizing Expected Information Gain (EIG) in high dimensions exploits model misspecification, producing unrealistic designs (e.g., noise images) where the likelihood is overconfident. The authors propose DiffBED, which constrains design optimization using a diffusion model as a reference prior over feasible designs, sampling from a distribution proportional to $p_{ref}(\\epsilon) \\cdot \\exp(EIG(\\epsilon)/\\alpha). Designs are generated via information-guided diffusion, combining the diffusion model's score function with EIG gradients in the reverse SDE. The authors conducted experiments on image-based tasks, and DiffBED generates realistic and informative designs in high-dimension space, while standard gradient-based BED fails by producing noise despite high model EIG."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem studied in this paper is well-motivated, and the authors clearly justified that EIG optimization inherently seeks regions where the likelihood is overconfident, identifying model misspecification as the fundamental barrier to scaling BED.\n\n2. The reward hacking analogy well motivates the approach, and by incorporating pre-trained diffusion models as reference priors, the proposed method prevents reward hacking while maintaining high informativeness.\n\n3. Experimental results demonstrate that DiffBED is able to scale on high-dimensional settings."}, "weaknesses": {"value": "1. The proposed method is highly dependent on having a high-quality, pre-trained diffusion model for the design space. This does not fully solve the high-dimensional problem as shift it from needing a perfect likelihood to needing a perfect generative prior. It would be great if the authors could elaborate more on this part.\n\n2. The proposed method is computationally expensive as it requires running a full, guided reverse-diffusion process for each experimental iteration.\n\n3. I think the authors should position the method as a hybrid, not a purely Bayesian one. It combines principled Bayesian inference (for $\\theta$) with a pragmatic, regularized optimization (for $\\xi$). The design objective $p^{*}(\\xi)$ introduces $p^{ref}(\\xi)$ as an external regularizer not derived from the original generative model."}, "questions": {"value": "1. Please see the comments in Weakness part.\n\n2. Could the authors provide more details on the BED baseline used in the experiments? I would suspect that the main computational bottleneck is the EIG gradients calculation, so how much does the proposed method cost more than the BED baseline?\n\n3. It seems that the Rank baseline is highly competitive in the conducted experiments, can the authors discuss in more detail?\n\n4. The proposed method depends on high-quality pre-trained diffusion models and encoders, which are feasible for vision but limited in other domains. Could the authors discuss the feasibility of applying DiffBED beyond vision (e.g., molecules, audio, text, tabular data), what happens when good diffusion models might not exist, and discuss when DiffBED is preferable over simpler pool-based approaches like Rank?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "idwUNo2Xst", "forum": "pNO7VqKAcY", "replyto": "pNO7VqKAcY", "signatures": ["ICLR.cc/2026/Conference/Submission20586/Reviewer_MFdK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20586/Reviewer_MFdK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967233384, "cdate": 1761967233384, "tmdate": 1762933992915, "mdate": 1762933992915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach to apply diffusion models to propose informative and plausible designs for Bayesian adaptive experimental design in high-dimensional design spaces. The paper shows that, in such settings, likelihood model misspecification can severely misguide BED algorithms towards designs which have an apparent high expected information gain (EIG), though due to errors in the likelihood model, leading traditional methods to produce no longer meaningful designs in high dimensions. To mitigate that, the proposed method employs pre-trained diffusion models whose samples are guided by an EIG-based gradient estimator towards designs which are informative for the given task without the need to retrain the diffusion model. Experiments are presented on high-dimensional experimental design problems involving images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is mostly well written and follows a clear structure which is relatively easy to follow.\n* The proposed method seems relatively simple to implement within modern generative modelling frameworks.\n* Some of the principles and insights in this paper may be applicable beyond its setting, such as the presentation on the issues with misspecification in BED, how a prior over feasible designs can help mitigate these issues, and how to apply pre-trained diffusion models to design tasks without a need for retraining."}, "weaknesses": {"value": "* In Sec. 3, it is not clear how the decomposition of the EIG from Eq. 5 to 6 was derived. Eq. 5 was derived using the alternative formulation of the TEIG in terms of $\\theta$, instead of $y$, which was the one presented in Eq. 4, making things confusing. Eq. 6 seems to have been derived by adding and subtracting TEIG from the model's EIG, but why the step in Eq. 5 was necessary is unclear.\n* It is not explained how Eq. 11 was derived. I believe that might require some background in diffusion models, though such a derivation should at least be found somewhere in the appendix, given that the EIG guidance term is a crucial contribution from this paper.\n* Experiments are presented on image design tasks, which escape traditional problems in experimental design, making it difficult to assess the impact of the paper outside this context. Despite the complexity of optimisation and sampling in high-dimensional design spaces, the probabilistic models in these experiments seem reasonably simpler than traditional models found in science and engineering applications where BED typically finds its applications. Hence, I'm unsure of this paper's impact.\n* The presented performance plots show that, besides the gradient-descent BED baseline, the proposed DiffBED's performance is mostly very close to the performance of simpler baselines. Yet, I reckon that std. deviations, which I interpret as the shaded areas around the curves, are quite small as well. Therefore, the practical significance of this paper's contribution remains unclear.\n* I'm also unsure about the claim in Sec. 5 that \"the first to identify that model misalignment is not constant across design space and show the potential for reward hacking\". Prior work in BED has discussed issues related to model misspecification and their effect on design optimisation, such as Foster et al. (2025), though to a different extent. In addition, the issue of model mismatch as a function of the designs is explicitly modelled in, e.g., Bayesian calibration frameworks by the model discrepancy/error term (Kennedy & O'Hagan, 2001), which have been recently applied to BED (Oliveira et al., 2024; Sürer et al., 2024). Additional references are listed below.\n\nMinor:\n* A few typos are present throughout the text, which require careful revision.\n* Eq. 10 is missing a time differential $\\mathrm{d}t$ next to the term in brackets.\n* I believe it should be $\\xi_t$, instead of $x_t$, in the inline equation on line 244.\n\nReferences:\n* Kennedy, M. C., & O’Hagan, A. (2001). Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3), 425–464.\n* Oliveira, R., Sejdinovic, D., Howard, D., & Bonilla, E. V. (2024). Bayesian Adaptive Calibration and Optimal Design. 38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n* Sürer, Ö., Plumlee, M., & Wild, S. M. (2024). Sequential Bayesian experimental design for calibration of expensive simulation models. Technometrics, 66(2), 157-171."}, "questions": {"value": "Please, see weakness points above. In addition, I have the following more specific questions.\n\n* Have other high-dimensional experiment settings been considered, beyond images? As far as I understand, diffusion models could also be applied in lower-dimensional settings and compared against other BED baselines in more traditional problems. Some synthetic problems can have their dimensionality adjusted in ablation problems to show how the performance of standard methods potentially degrade as the dimensionality increases, while hopefully DiffBED maintains reasonable performance levels.\n\n* Did each experimental trial have a different target image in Sec. 6.1? Or was the same image used across each experiment, with the different random seed only affecting optimisation behaviour?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4DoImuPbKp", "forum": "pNO7VqKAcY", "replyto": "pNO7VqKAcY", "signatures": ["ICLR.cc/2026/Conference/Submission20586/Reviewer_tuUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20586/Reviewer_tuUX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047481419, "cdate": 1762047481419, "tmdate": 1762933992427, "mdate": 1762933992427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}