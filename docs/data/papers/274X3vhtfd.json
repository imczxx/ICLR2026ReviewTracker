{"id": "274X3vhtfd", "number": 15746, "cdate": 1758254805875, "mdate": 1759897284807, "content": {"title": "Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks", "abstract": "Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.", "tldr": "Instead of relying solely on Laplacian embeddings for feature augmentation, using spectral embeddings from diverse graph matrices can offer improvements in GNN performance.", "keywords": ["Laplacians", "Graphs", "Neural Network", "Embeddings"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77d25e6b1c1ef6f000a7ede3defb105dd6762d75.pdf", "supplementary_material": "/attachment/48b76d1c7edcc693e6f619adfb70d864034b9e94.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Interpolated Laplacian Embeddings (ILE), a two-parameter family of spectral operators M(t,s)=tD−sA, which generalizes the standard Laplacian and adjacency spectra. The authors argue that different t,s combinations capture community versus core–periphery structures and can thus serve as adaptive spectral features for node classification. Experiments on synthetic and small real graphs show that ILE can outperform conventional Laplacian embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a framework which is lightweight, broadly applicable to existing GNNs, and empirically demonstrates some improvement."}, "weaknesses": {"value": "- Regarding the motivation of this paper, while the paper highlights the different structural preferences of Laplacian vs. adjacency spectra, it lacks structure–label linkage evidence, such as datasets exhibit community-like vs. core–periphery-like label structures?\n- Multiple typos such as wILE, otherwILE, arILE (unclear meaning), richess → richness, and computational of eigenvectors → computation of eigenvectors.\n- Experiments:\n    - Need strong degree/centrality baseline**s** to ensure gains are not from trivial structure cues; better to include some recent spectral augmentation baselines.\n    - No statistical significance tests.\n- Some recent spectral augmentation for graph data works [1–6] are not discussed or compared, which are highly-related to this work.\n\n**Reference**:\n\n[1] Revisiting Graph Contrastive Learning from the Perspective of Graph Spectrum. NeurIPS. 2022.\n\n[2] Spectral Feature Augmentation for Graph Contrastive Learning and Beyond. AAAI. 2023.\n\n[3] Spectral Augmentations for Graph Contrastive Learning. AISTATS. 2023.\n\n[4] Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classifications. AAAI. 2025.\n\n[5] Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning. 2024.\n\n[6] Spectrum Guided Topology Augmentation for Graph Contrastive Learning. 2022."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "euURXNKAio", "forum": "274X3vhtfd", "replyto": "274X3vhtfd", "signatures": ["ICLR.cc/2026/Conference/Submission15746/Reviewer_A9i6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15746/Reviewer_A9i6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760594789287, "cdate": 1760594789287, "tmdate": 1762925984636, "mdate": 1762925984636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a graph positional embedding/encoding (PE) that can be used as additional features to be used by a GNN model. The encoding is a spectral encoding from a matrix interpolating between the Laplacian and the adjacency matrix, where the interpolation coefficients can be chosen by the used depending on the data at hand. The validity of the new positional encoding is showcased on some synthetic data as well as on some established graph datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The idea behind the paper is simple yet elegant and intuitive. The paper does a good job at describing the motivation behind the method and and its intuition.\n- The paper is quite well-written."}, "weaknesses": {"value": "The experimental validation, which is crucial for validating the effectiveness of a method such as the one presented here, is extremely weak. \n- The paper fails to compare to any baseline method. \n    - The simplest baseline would be to concatenate the Laplacian PE with the one obtained from the adjacency matrix, as this would give both community information and Core-Periphery information, while being simpler and requiring no hyperparameters. \n    - There is a plethora of PE methods that are not included here. Examples are: ElasticPE, RWSE, SignNet, HKdiagSE. How does the proposed method compare to these methods?\n    - Hows does the proposed method compare to learned embeddings? For example, [1] learns the importance of core-periphery information and selects accordingly which eigenvectors to consider.\n- The paper only uses few datasets. In particular, in the main paper, only results on synthetic data are reported. The results on real world data should be included in the main text, and possibly extended to more datasets.\n- In practice, one would want to select the coefficient values for $s, t$ based on some validation split, as described in section 4.5. In this scenario (not reported in the experiments), it is unclear whether the results would be significantly better than the standard Laplacian encoding. \n\nMoreover, the overall contribution is too small for ICLR. The idea is very simple, so it should be complemented by a solid theoretical analysis or by an extensive experimental validation.\n\n---\n\n[1] Ito et al. Learning Laplacian Positional Encodings for Heterophilous Graphs. AISTATS 2025."}, "questions": {"value": "- Why are the experiments limited to node-level tasks? The LPE has been used also for graph-level tasks. Do you think you method could be useful also in that scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V6GM5sO1kp", "forum": "274X3vhtfd", "replyto": "274X3vhtfd", "signatures": ["ICLR.cc/2026/Conference/Submission15746/Reviewer_NDbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15746/Reviewer_NDbB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601377314, "cdate": 1761601377314, "tmdate": 1762925984020, "mdate": 1762925984020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Node features are critical but can be limited in graph neural networks (GNNs). To remedy this issue, this paper proposes to derive spectral Laplacian eigenvectors of a family of graph matrices as augmented node features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation of the proposed method is clearly explained and valid.\n\n2.\tThis paper generalizes the idea of utilizing spectral embeddings in node feature augmentation. Moreover, theoretical analyses and explanations are provided.\n\n3.\tVarious types of experimental settings are adopted for better validation."}, "weaknesses": {"value": "1.\tUsing spectral embedding as node features has been researched early, both in GNNs and Graph Transformers, such as Specformer[1]. Please compare with those methods for a more convincing and comprehensive understanding.\n\n[1] Specformer: Spectral Graph Neural Networks Meet Transformers, ICLR 2023\n\n2.\tThis paper provides a clear theoretical understanding of the proposed method. However, theoretical power or performance is not discussed. Lemma 3.1 is straightforward.\n\n3.\tCalculating eigenvectors involves high computation overhead, especially in real-world networks. Please provide an analysis or empirical results on how the proposed method handles the efficiency issue.\n\n4.\tThere are some explicit typos in the paper, such as “otherwise” in line 113."}, "questions": {"value": "Please refer to my weakness comments, especially W2 and W3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B2NbEobPbZ", "forum": "274X3vhtfd", "replyto": "274X3vhtfd", "signatures": ["ICLR.cc/2026/Conference/Submission15746/Reviewer_uFh8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15746/Reviewer_uFh8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796774980, "cdate": 1761796774980, "tmdate": 1762925982622, "mdate": 1762925982622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of applying Graph Neural Networks (GNNs) to graphs with limited or absent node features. The authors propose a feature augmentation technique called Interpolated Laplacian Embeddings (ILEs). These embeddings are derived as the eigenvectors of a two-parameter family of graph matrices, $M(t,s) \\equiv tD - sA$, which generalizes the standard graph Laplacian ($t=1, s=1$). The authors provide a theoretical interpretation of this matrix family using quadratic forms, arguing that the parameters $(t, s)$ allow the resulting embeddings to interpolate between capturing community structure and core-periphery structure. Empirical results on synthetic and real-world node classification tasks suggest that, for certain choices of $(t, s)$, ILEs can outperform standard Laplacian embeddings as augmented features."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper tackles a practical and significant problem. The reliance of GNNs on high-quality node features is a well-known limitation.\n\nS2. The theoretical analysis in Section 3.2 provides an intuitive interpretation of the $M(t,s)$ family. By analyzing the quadratic form $x^{\\top}M(t,s)x$, the authors clearly explain how the parameters $t$ and $s$ balance the Laplacian quadratic form (favoring community structure) and the adjacency quadratic form (favoring core-periphery structure). \n\nS3. The experimental evaluation tests the proposed method on synthetic Stochastic Block Models (SBMs), real-world datasets without any node features, and real-world datasets with node features corrupted by noise."}, "weaknesses": {"value": "W1. The central idea of using a generalized graph matrix is not new. The proposed $M(t,s) = tD - sA$ family is a simplified version of the \"universal adjacency matrices\" family and is closely related to other existing families, such as the \"deformed Laplacian\", which the paper itself discusses. The proof in Lemma 3.1, showing that ILEs subsume the deformed Laplacian (up to an identity shift), is straightforward. \n\nW2. A significant weakness is the absence of a method to select the $s$ and $t$ parameters. The experiments demonstrate that some $(s, t)$ pair from a manually selected grid can outperform the Laplacian baseline. This finding is expected; given a two-parameter family, it is highly probable that some combination will outperform a fixed baseline ($s=1, t=1$) on a given dataset. The paper's contribution would be much stronger if it proposed a data-driven method to select or learn $s$ and $t$, or at least provided a deeper analysis linking specific graph properties to optimal $(s, t)$ values. The suggestions in Section 4.5 (e.g., \"perform cross-validation\") are generic and computationally expensive, as this would require repeated eigendecompositions.\n\nW3. The experiments are missing key baselines. The primary comparison is against the standard Laplacian and the adjacency matrix. Given the discussion of other matrices like the other Laplacian and the deformed Laplacian, it is unclear why these were not included as direct baselines. More importantly, the paper positions itself within the spectral augmentation literature but fails to compare ILEs against any other spectral augmentation or positional encoding methods.\n\nW4. The paper's organization and writing is not good. This extensive background, which includes theorems and definitions from other papers, could be streamlined to focus more on the paper's specific contributions."}, "questions": {"value": "Q1. The caption for Figure 1 mentions \"Laplacian embedding ($k=1$)\" and \"Adjacency embedding ($k=1$).\" What does $k$ represent? \n\nQ2. The paper suggests cross-validation for $s$ and $t$, which seems computationally prohibitive, requiring a new eigendecomposition for each $(s, t)$ pair. Have the authors studied the sensitivity of performance to $s$ and $t$? \n\n\nQ3. The quadratic form interpretation in Section 3.2 is key to the paper's argument. How much of this analysis is new versus a direct application of existing analysis from work on related matrices, like the deformed Laplacian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YU91kIr3uS", "forum": "274X3vhtfd", "replyto": "274X3vhtfd", "signatures": ["ICLR.cc/2026/Conference/Submission15746/Reviewer_VLFB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15746/Reviewer_VLFB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927967409, "cdate": 1761927967409, "tmdate": 1762925982241, "mdate": 1762925982241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose using eigenvectors of a family of interpolating matrices — which include the Adjacency and Laplacian matrices as particular cases — for positional encoding in GNNs. The interpolation depends on two parameters, and the authors report results for these different augmentations on node classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well written. Although there are several works on spectral positional encodings for GNNs, the specific experiments on how these embeddings behave when using interpolating matrices appear to be new."}, "weaknesses": {"value": "I believe the paper falls short. The idea is interesting, but the fact that eigenvectors from different matrices have different effects on downstream tasks is already known (as the authors themselves acknowledge), and the experiments presented here merely confirm this. There is no real new insight into when one should prefer one matrix over another — or, equivalently, one set of (t,s) parameters over another. In this sense, I think that the contribution is incremental."}, "questions": {"value": "Although the matrix M(t,s) in Equation (2) depends on two parameters, the eigenvectors depend (up to a constant) only on the ratio of these parameters. Therefore, it seems that the whole positional encoding scheme effectively depends on a single parameter. Please verify if this is indeed the case.\n\nFinally, I find the citation to Spielman for the \"Courant–Fischer theorem\" on page 5 inappropriate. Citing a 2019 paper for a result established over a century ago seems unusual."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VqSgqQbKZf", "forum": "274X3vhtfd", "replyto": "274X3vhtfd", "signatures": ["ICLR.cc/2026/Conference/Submission15746/Reviewer_tWch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15746/Reviewer_tWch"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944059644, "cdate": 1761944059644, "tmdate": 1762925981833, "mdate": 1762925981833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}