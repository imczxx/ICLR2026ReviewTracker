{"id": "PSixwg3wpL", "number": 23075, "cdate": 1758339137859, "mdate": 1759896833418, "content": {"title": "RUSID: Robust Uncertainty-aware Single Image Deraining beyond Certainty", "abstract": "Rainy weather induces rain streaks, blurs details, and reduces contrast, impairing image quality, making single image deraining a classic research topic. However, existing learning-based image restoration methods fail to account for uncertainties in both data and model dimensions, thus being unable to produce satisfactory results. To address this challenge, we introduce a novel framework called the Uncertainty-aware Visual-priors Prompt-interaction Network (UVPNet). UVPNet comprises three key modules: the Distribution-aware Visual Priors Learning (DVPL) module, which aims at data-wise aleatoric uncertainties, the Certainty-Uncertainty Prompt Fusion (CUPF) module, which tackles model-wise epistemic uncertainties, and the Channel Spatial Uncertainty Weighting Block (CSUWB). UVPNet leverages uncertainty modeling through visual semantic and depth priors and distributionally representative prompts by integrating data-wise and model-wise uncertainty learning. To the best of our knowledge, our UVPNet first utilizes uncertainty modeling with visual priors for single image deraining. Extensive experiment results demonstrate that our UVPNet outperforms state-of-the-art methods on both public synthetic datasets and real-world images while maintaining low complexity.", "tldr": "Robust single image rain removal by utilizing uncertainty-modeling visual priors and prompt learning.", "keywords": ["Image deraining", "Visual priors", "Uncertainty-modeling", "Prompt learning", "SAM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b5e650616cca7948eb936de6cce278c9222318b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UVPNet, a single image deraining framework that addresses both aleatoric and epistemic uncertainties by integrating visual-prior-based probabilistic modeling and prompt interaction. The proposed framework consists of three key components: a Distribution-aware Visual Priors Learning (DVPL) module for data-wise uncertainty, a Certainty-Uncertainty Prompt Fusion (CUPF) module for epistemic uncertainty, and a Channel Spatial Uncertainty Weighting Block (CSUWB) for uncertainty-enhanced feature refinement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is the first to integrate uncertainty modeling with visual priors in single image deraining, offering a new perspective for handling complex degradations.\n2. The DVPL effectively captures aleatoric uncertainty using semantic and depth priors, while the CUPF fuses certainty and uncertainty prompts to enhance epistemic robustness, making the framework interpretable and principled.\n3. The method achieves state-of-the-art performance across multiple synthetic and real-world benchmarks."}, "weaknesses": {"value": "1. The paper lacks a deeper theoretical justification or quantitative insight into how uncertainty modeling directly contributes to the observed performance gains.\n2. The paper suffers from an incomplete structure due to the absence of a dedicated and systematically organized “Related Work” section. Relevant prior studies are briefly mentioned in the introduction and method sections, but there is no comprehensive review of existing approaches in uncertainty modeling, deraining networks, or visual priors.\n3. Although the model claims low complexity, the paper does not report training time or memory consumption, making the efficiency claims difficult to validate.\n4. Comparisons do not include recent foundation or diffusion-based restoration methods, which may undermine the strength of the claimed superiority.\n5. The paper provides insufficient visual comparisons, with a lack of qualitative examples against multiple state-of-the-art baselines. This omission prevents readers from visually assessing the improvements in detail preservation, artifact suppression, and perceptual quality."}, "questions": {"value": "See the above parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fVcd6xHeKV", "forum": "PSixwg3wpL", "replyto": "PSixwg3wpL", "signatures": ["ICLR.cc/2026/Conference/Submission23075/Reviewer_RjkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23075/Reviewer_RjkL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441181080, "cdate": 1761441181080, "tmdate": 1762942501694, "mdate": 1762942501694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An uncertainty-aware work on image deraining with visual priors"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work explicitly models both aleatoric (data) and epistemic (model) uncertainty for single image deraining.\n\n2. The authors provide extensive experiments that are convincing and thorough.\n\n3. The ablation studies in Table 4 are detailed and effectively prove the contribution of each proposed component.\n\n4. The paper is generally well-structured, with clear explanations of the problem, methodology, and results."}, "weaknesses": {"value": "1. The paper does not specify how the certainty and uncertainty prompts are initialized. Are they randomly initialized, or are they derived from the input or features?\n\n2. In Eq. (8), the output  is defined using PGM and PIM, but these modules are only cited and not described in the main text, leaving their specific function a bit vague for a reader not intimately familiar with PromptIR.\n\n3. While the method performs well on the tested real-world dataset, a broader discussion of its generalization ability to more diverse and challenging real-world conditions (e.g., heavy rain, night rain, combined rain and fog) would strengthen the work. Why did the authors not to choose real-world benchmarks to conduct experiments?\n\n4. This approach seems to put the cart before the horse, since the purpose of image deraining is precisely to improve the performance of high-level vision algorithms. Yet now we are using high-level visual information to guide the deraining process. Is this setup truly reasonable?\n\n5. When utilizing two \"anything\" models, does the author's proposed method still maintain an efficiency advantage compared to current existing methods?\n\n6. The authors need to add an overall pipeline diagram to illustrate the information flow of the entire method. \n\n7. In fact, I feel this work is semi-finished and requires substantial improvements in motivation, methodology, experiments, and discussion."}, "questions": {"value": "1. The authors are strongly advised to provide a clearer explanation of the motivation and methodology, as the current version appears quite confusing.\n\n2. Why not conduct comparisons on real-world datasets, given the abundance of existing real rainy image datasets available?\n\n3. Why is there inconsistency in the style of the figures?\n\n4. I did not find the supplementary materials. In my opinion, the current work does not sufficiently demonstrate the significance to the research community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VxfFnhXiZH", "forum": "PSixwg3wpL", "replyto": "PSixwg3wpL", "signatures": ["ICLR.cc/2026/Conference/Submission23075/Reviewer_hK5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23075/Reviewer_hK5F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540883827, "cdate": 1761540883827, "tmdate": 1762942501537, "mdate": 1762942501537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an uncertainty-based approach for robust single image deraining. The method incorporates uncertainty modeling into the Segment Anything Model (SAM) and depth priors. These priors, re-parameterized as Gaussian distributions, are injected into the deraining model as visual prompts. Furthermore, the approach introduces channel-wise and spatial-wise uncertainty into the deraining process, enabling variable deraining results from a fixed rainy input. The proposed approach has outperformed current SOTA methods on Rain13K and real-world rainy images."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces uncertainty into both the visual prompts and deep features\n- The quantitive results demonstrate its superiority over the listed methods\n- The proposed method presents better real-world deraining results compared to the listed methods."}, "weaknesses": {"value": "- The paper omits a systematic literature review, particularly regarding uncertainty-based methods in image deraining and restoration. This makes it difficult to situate the proposed contributions within the existing research literature.\n- While introducing uncertainty into visual prompts and deep features is a core contribution, the paper lacks critical supporting evidence. For instance, visualizations of the uncertainty maps are absent, leaving the reader to speculate on their form and function.\n-  Quantitative comparison against other uncertainty-based methods is absent.\n- Qualitative comparison in Figure 5 is not enough, with comparisons on only two synthetic and one real-world images. \n- The analysis of the learned prompts is lacking. Incorporating visualization techniques (e.g., t-SNE, as used in PromptIR) would greatly help in interpreting what the uncertain prompts has learned.\n- A significant discrepancy exists in the quantitative results. For example, on the Rain100L dataset, the paper reports 32.73 for CPRAformer, whereas the official paper reports 35.98. The authors must clarify their evaluation protocol in detail (e.g., Y channel vs. RGB).\n- Organizational and Notational Inconsistencies (some of them are listed below):\n    *   The training loss function is not presented in the paper.\n    *   There are inconsistent notations, such as the use of $\\odot$ in Eq. (6) versus $*$ in Eq. (13) for what appears to be the same operation.\n    *   Figure 2 incorrectly uses \"GAP\" where it should be \"GMP.\"\n    *   Table 5 is missing the 0S and wS configurations discussed in Section 3.3.\n    *   The venue \"Nips'23\" in Table 5 should be formatted correctly as \"NeurIPS '23.\""}, "questions": {"value": "- Given the proposed uncertainty modeling, what is the method's capability for training on datasets with various degradations (e.g., combining rain streaks, raindrops, and haze)? Does the uncertainty framework help the model handle these different degradations?\n- Which component—the certain or the uncertain prompts—is primarily responsible for learning degradation-aware information?\n- To further demonstrate practical utility, could the model be trained and evaluated on real-world datasets like the SPADataset? Providing more qualitative results on challenging real-world scenarios would strongly validate the method's robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9jA7eUgBNJ", "forum": "PSixwg3wpL", "replyto": "PSixwg3wpL", "signatures": ["ICLR.cc/2026/Conference/Submission23075/Reviewer_ZMHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23075/Reviewer_ZMHb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895396367, "cdate": 1761895396367, "tmdate": 1762942501150, "mdate": 1762942501150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RUSID (UVPNet), a single-image deraining network that aims to handle both data and model uncertainties using visual priors and prompt fusion. It includes three main components: DVPL for data uncertainty, CUPF for model uncertainty, and CSUWB for feature reweighting. Experiments on synthetic and real rainy datasets show small improvements over recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper is clearly organized and supported with detailed architecture figures and ablation studies.\n\n(2) Combining uncertainty modeling and prompt-based priors is an interesting direction that could inspire further work.\n\n(3) The experimental coverage across different datasets and metrics is relatively complete."}, "weaknesses": {"value": "(1) The method piles together many fashionable ideas like uncertainty, prompts, and visual priors, but the technical substance is shallow. Each proposed block is only a small variant of existing attention or distribution modules with new names.\n\n(2) The “uncertainty modeling” is not really probabilistic. Using a Softplus-activated variance map does not provide genuine uncertainty estimation or calibration.\n\n(3) There is no clear evidence that modeling uncertainty actually helps deraining. The paper lacks analysis, visualization, or any metric showing that uncertainty estimates correlate with better restoration.\n\n(4) The reliance on SAM and Depth Anything priors raises questions about practicality. These huge models make the pipeline heavy, and the claimed efficiency is not credible."}, "questions": {"value": "(1) What concrete benefit does the uncertainty modeling provide? Can you show calibration curves or any statistical validation?\n\n(2) How is CUPF fundamentally different from the prompt fusion in PromptIR? The difference seems marginal.\n\n(3) The ablation shows very little performance change when SAM and Depth Anything priors are removed. Does that not undermine the main motivation?\n\n(4) How can this approach be deployed in real applications given its dependency on large pretrained priors and high memory cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gpe1uk5vZL", "forum": "PSixwg3wpL", "replyto": "PSixwg3wpL", "signatures": ["ICLR.cc/2026/Conference/Submission23075/Reviewer_V41S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23075/Reviewer_V41S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918598986, "cdate": 1761918598986, "tmdate": 1762942500962, "mdate": 1762942500962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}