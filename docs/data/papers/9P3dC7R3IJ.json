{"id": "9P3dC7R3IJ", "number": 13984, "cdate": 1758226500086, "mdate": 1759897398375, "content": {"title": "FedSFT: Resource-Constrained Federated Black-Box Adaptation of Large Language Models", "abstract": "Federated fine-tuning enables privacy-preserving adaptation of large language models (LLMs) by allowing decentralized training without sharing raw data. However, its real-world deployment is often hindered by restricted access to model parameters and substantial computation, communication, and memory overhead. To address these challenges, we propose $\\textbf{Fed}$erated $\\textbf{S}$urrogate $\\textbf{F}$ine-$\\textbf{T}$uning (FedSFT), a novel framework for federated black-box fine-tuning of LLMs that requires access only to the token probabilities of output sequences and significantly reduces resource demands on clients. In each communication round of FedSFT, clients fine-tune a small model that serves as a surrogate for the large model hosted on the server. The server then leverages the logit offsets between the tuned and untuned small models to adjust the output of the untuned large model and distills the knowledge to update the small model for the next training round. Experimental results show that FedSFT significantly reduces client-side computation, communication, and memory overhead while maintaining competitive performance compared to direct federated fine-tuning of large models. FedSFT offers a promising solution for efficient and privacy-preserving black-box fine-tuning of large models on resource-constrained clients, broadening the accessibility and applicability of state-of-the-art LLMs.", "tldr": "", "keywords": ["Large language model", "federated learning", "fine-tuning", "resource efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ff3d4338a03b937937c91f09b46511cc39ec097.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents FedSFT, a federated surrogate fine-tuning framework designed to adapt black-box large language models in settings where clients have strict resource constraints and cannot access model parameters directly. Rather than fine-tuning the large model locally, each client fine-tunes a small surrogate model using LoRA, uploads only the low-rank deltas, and the server then aggregates these updates and constructs a composite model combining outputs of the untuned large model, untuned small model, and tuned small model. This composite model is used for server-side knowledge distillation, guiding future surrogate model updates. Experiments across GPT-2, OPT, and LLaMA architectures on sentiment control and instruction-following demonstrate that FedSFT maintains performance close to direct federated fine-tuning of large models while significantly reducing communication, computation, and memory overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Full-parameter federated fine-tuning of LLMs is infeasible for clients with only 4–8 GB VRAM, whereas FedSFT requires deploying only a lightweight surrogate model.\n- FedSFT supports black-box settings where the full model weights are inaccessible, enabling fine-tuning of proprietary LLMs.\n- The paper demonstrates strong empirical results across diverse experiments involving multiple model architectures and tasks."}, "weaknesses": {"value": "- The server-side knowledge distillation relies on a public dataset that may come from a different domain, which could expose information about the task distribution or degrade performance in highly sensitive or domain-specific applications.\n- The evaluation of non-IID data heterogeneity remains limited; although a Dirichlet split is included, the experiments do not fully capture more realistic cross-domain shifts or highly personalized client distributions.\n- The method still requires access to the large model during inference, preventing clients from achieving standalone capability and reducing practicality in offline or low-connectivity environments.\n- While the system is framed as privacy-preserving, there is no analysis of whether surrogate LoRA updates, logit offsets, or distillation signals could leak sensitive client information, nor are techniques such as secure aggregation or differential privacy considered to mitigate these risks."}, "questions": {"value": "- The experiments are limited to only 10 clients, which may not adequately demonstrate scalability in realistic federated deployments. Testing with 100+ heterogeneous clients would better reflect practical system behavior and strengthen claims about efficiency and performance under scale.\n- While the paper reports meaningful reductions in system costs (memory, communication, computation), there is no direct analysis of the accuracy–efficiency trade-off, making it difficult to fully assess how cost savings impact model capability.\n- In Figure 5, as α increases, FedIT’s performance tends to decline more linearly than FedSFT; the paper should clarify why a larger α weakens baseline performance and provide theoretical or empirical justification for this behavior.\n- In Figure 6, FedSFT generally outperforms alternatives as small-model size grows, but at 350M parameters FedIT appears stronger, which deserves explanation. Clarifying the underlying cause—such as insufficient surrogate capacity or instability in knowledge distillation when models are too small—would improve interpretability of the scaling results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RNH3OSmuB3", "forum": "9P3dC7R3IJ", "replyto": "9P3dC7R3IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13984/Reviewer_RoZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13984/Reviewer_RoZa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442944923, "cdate": 1761442944923, "tmdate": 1762924483079, "mdate": 1762924483079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FedSFT is a black-box federated fine-tuning method where each client tunes a small LoRA-augmented surrogate; the server builds a composite teacher by adding the surrogate’s logit offsets to the frozen large model and distills this back into the surrogate for the next round, requiring only output token probabilities. Across LLMs and tasks, it matches direct federated fine-tuning while sharply reducing client costs (e.g., OPT per-round communication drops from 12.5 MB to 3 MB), making FL feasible on resource-constrained devices."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Much lower client costs (comm/computation/memory): e.g., per-round OPT communication drops from 12.5 MB → 3 MB, with overall reductions that make FL feasible on bandwidth- or memory-limited devices.\n2. Extensive experiments have shown the effectiveness of the framework."}, "weaknesses": {"value": "1. Sharing the logits of large models is a strong assumption. Also in the paper, it is mentioned that there will be a knowledge distillation on the server side. This is not realistic as well, this is like giving the model for free to the client. \n2. Where does the performance gain come from. Is it because the composite/ensemble of several models? or the proposed training pipeline?"}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kLmpV1PJlP", "forum": "9P3dC7R3IJ", "replyto": "9P3dC7R3IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13984/Reviewer_GGd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13984/Reviewer_GGd7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862996768, "cdate": 1761862996768, "tmdate": 1762924482555, "mdate": 1762924482555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FedSFT, a framework for federated adaptation of large language models in cases where model parameters cannot be accessed. Each client trains a small local model using LoRA, and the server aggregates these updates to build a composite model that combines the black-box LLM with the aggregated surrogates through knowledge distillation. The aim is to make collaborative model tuning possible under limited resources and privacy constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•  The paper tackles a timely and relevant problem: adapting large models in federated settings when direct access to parameters is restricted.\n•  The overall design—client-side LoRA fine-tuning combined with server-side distillation—is simple and well motivated, and achieves strong efficiency gains while maintaining good accuracy.\n•  The motivation and formulation are clearly articulated, and the framework could inspire future work on privacy-aware model adaptation.\n•  Experimental results show substantial reductions in resource usage (4–9×) without large performance drops, highlighting potential practical value."}, "weaknesses": {"value": "•  The “black-box” assumption conflicts with the claim that the model can provide full output logits, which real API-based systems (e.g., GPT-4, Claude) do not expose.\n•  The experimental comparisons are not capacity-matched: FedSFT fine-tunes a 1.3B surrogate model, while baselines fine-tune 13B models. This mismatch weakens the claim of “comparable performance with higher efficiency.”\n•  Important baselines are missing—such as small-model FedLoRA without distillation or a centralized distillation variant—making it unclear which part of the framework drives the improvements.\n•  Overall, the work reads as an engineering composition of existing ideas (federated learning, LoRA, and distillation) rather than a significant methodological advance."}, "questions": {"value": "•  Black-box assumption realism: Section 3.1 assumes that the black-box LLM can return full logits. In practical API settings this is unrealistic—would the method still function with only sampled text outputs?\n•  Baseline fairness and model capacity: All reported baselines use a 13B model, whereas FedSFT uses a 1.3B surrogate. Are there results using the same model size for a fairer comparison?\n•  Ablation and component contribution: The current ablations vary α and model scale but do not isolate core components. Since LoRA and aggregation mainly support the distillation stage, can the authors clarify how much each part contributes to overall performance?\n•  Centralized vs federated benefits: Given that the experiments are simulated on a single machine, what is the actual gain from federated aggregation compared with a centralized distillation setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9uGCow79fg", "forum": "9P3dC7R3IJ", "replyto": "9P3dC7R3IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13984/Reviewer_oRdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13984/Reviewer_oRdb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136017851, "cdate": 1762136017851, "tmdate": 1762924482056, "mdate": 1762924482056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}