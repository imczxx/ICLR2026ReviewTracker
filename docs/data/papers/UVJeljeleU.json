{"id": "UVJeljeleU", "number": 14981, "cdate": 1758246449331, "mdate": 1759897337770, "content": {"title": "CircuitTuning: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates", "abstract": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called CircuitTuning which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.", "tldr": "CircuitTuning selectively updates sparse task-relevant circuits in LLMs, boosting math reasoning accuracy by up to 11.4% with minimal changes and little impact on other abilities.", "keywords": ["Mechanistic Interpretability", "Math Reasoning", "Fine-tuning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45c0648845c6b5bbbd4e2cae9d983753c3c4ebfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a circuit-based parameter-efficient finetuning on reasoning tasks involving chain-of-thought style reasoning traces.\nSpecifically, the method consists of three steps:\n1. Identifying a \"pivot token\" in the reasoning trace, which is a token at a position with a large impact on whether the reasoning trace will lead to the correct or wrong answer.\n2. Identifying a circuit that is responsible for producing this pivot token, and by extension, for the model producing the correct or wrong answer.\n3. Selectively finetuning only this circuit on the task of interest.\nThe paper shows that this method yields improvements generally comparable to or better than another parameter-efficient finetuning method, namely LoRA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The first step of the proposed method, i.e., identifying a pivot token in chain-of-thought reasoning traces appears could be useful for interpretability research in general, since one limitation of all currently available, practical circuit analysis methods is that they require a minimal pair of inputs that differ in only a single token."}, "weaknesses": {"value": "In my view, the contribution of this paper is not substantive enough.\n\nFrom a novelty perspective, only the first step of the proposed method, i.e., the identification of \"pivot tokens\", is novel. Circuit identification (Step 2) is performed using an existing method, and selective finetuning (Step 3) has been proposed in prior work (Wang et al., ICLR 2025: HeadMap: Locating and Enhancing Knowledge Circuits in LLMs; https://openreview.net/forum?id=jUsrbOuQ5e).\n\nNow, methodological novelty is not a necessary criterion, as there are many other kinds of contributions a paper can make. However, I'm struggling to identify what this other kind of contribution could be in the case of this paper. It's possible to see the paper as contributing an empirical comparison of the proposed parameter-efficient finetuning (PEFT) method to other PEFT methods, but this contribution is very limited for the following reasons:\n- Evaluation is limited to only a subset of one benchmark, GSM-Symbolic. (Some more datasets appear in the paper, but these are used as control tasks to verify the absence of side effects, not for evaluating the efficacy of the proposed method)\n- The comparison to existing PEFT methods is not systematically controlled: The number of finetuning instances is different and it is unclear if the comparisons are fair in terms of overall computational cost, since LoRA doesn't require circuit identification (which can be costly) but the proposed method does.\n- Claimed parameter efficiencies (\"modifying as little as 1.59% of model components\") are potentially misleading, because \"components\" refers to both attention heads and MLP neurons. Since there are many more MLP neurons than heads, even finetuning all attention heads but no MLP neurons would likely yield a very low ratio of modified \"model components\" under this metric."}, "questions": {"value": "Minor comment:\n\nline 115: \"We propose a novel technique, called CircuitTuning, to improve the mathematical reasoning capabilities of an LM, without affecting other abilities.\"\nThis is an imprecise statement since the experiments clearly show an impact on other abilities (Table 2). The above phrasing would be more appropriate if there were no statistically significant changes observed among all the tested abilities/benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J9uy6bcL6S", "forum": "UVJeljeleU", "replyto": "UVJeljeleU", "signatures": ["ICLR.cc/2026/Conference/Submission14981/Reviewer_iuiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14981/Reviewer_iuiG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621766700, "cdate": 1761621766700, "tmdate": 1762925312864, "mdate": 1762925312864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CircuitTuning, a novel, mechanistically-inspired method for improving the mathematical reasoning abilities of LLMs. The method operates in three stages: 1) It first generates both correct and incorrect reasoning traces for a given problem to identify the \"pivotal token\" where the model's reasoning diverges towards an error. 2) It then uses a masking technique (Desiderata-based Component Masking, DCM) to localize the specific attention heads and MLP neurons that are most responsible for generating the correct next token. 3) Finally, it applies targeted gradient updates exclusively to this sparse sub-network. The authors evaluate CircuitTuning on the GSM-Symbolic benchmark across four models from the Gemma and OLMo families. They show that their method can improve math reasoning accuracy by up to +12.1% while modifying a very small fraction of model parameters (as low as 0.17%), and importantly, without significantly degrading performance on general benchmarks like MMLU, TriviaQA, and TruthfulQA."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  The three-stage approach of localizing the error token, identifying the responsible components, and performing targeted updates is a novel way to bridge mechanistic interpretability and model fine-tuning. The \"Branching Method\" for finding the pivotal token is a particularly strong and well-motivated part of this methodology.\n\n2.  The method's main strength is its ability to achieve significant performance improvements while modifying a tiny fraction of the model's parameters (e.g., 0.17% for Gemma-9B). The results in Table 2 convincingly show that this surgical approach avoids the catastrophic forgetting that can plague broader fine-tuning methods, preserving performance on general benchmarks.\n\n3.   The paper is written with exceptional clarity. The method, experimental setup, and results are described in sufficient detail to facilitate understanding."}, "weaknesses": {"value": "1.  The LoRA baseline is trained on a different (and often larger) dataset than CircuitTuning. The performance difference could be attributed to the curated, high-signal \"Error-Localization\" dataset rather than the targeted update mechanism itself. To isolate the benefit of the proposed update strategy, LoRA should be trained on the exact same dataset.\n\n2.  The method's performance relative to LoRA is not consistent across models. LoRA significantly outperforms CircuitTuning on the Gemma-2B model and also wins on the OLMo-13B model. The paper lacks a discussion or analysis of why this might be the case. \n\n3.  The method's reliance on generating paired correct/incorrect reasoning traces may be difficult to scale to more complex, open-ended domains like code generation or scientific reasoning, where a single, easily verifiable \"correct trace\" may not exist. \n\n4.  In several cases (e.g., Gemma-2B Branching, OLMo-7B Branching), the \"w/o mask\" ablation performs better than the \"w/ mask\" version. This is counter-intuitive to the central hypothesis that updating only a sparse, localized circuit is optimal."}, "questions": {"value": "1.  Could you please clarify the rationale for training the LoRA baseline on the larger GSym-Train set instead of the smaller, curated Error-Localization dataset used for CircuitTuning? To make a more compelling case for your method's targeted update mechanism, would it be possible to run an experiment where LoRA is trained on the exact same data used by CircuitTuning?\n\n2.  What is your hypothesis for the inconsistent results when comparing CircuitTuning to LoRA? Specifically, why do you think LoRA achieves a much larger accuracy gain on Gemma-2B (+16.8%) and a better gain on OLMo-13B (+5.5%) compared to your method? Does the effectiveness of CircuitTuning depend on model scale or architecture?\n\n3.  The \"w/o mask\" ablation, which performs token localization but allows gradient updates to all parameters, sometimes outperforms the \"w/ mask\" version. How do you interpret this result? Does it challenge the core assumption that only a very sparse set of components should be updated?\n\n4.  How do you envision adapting CircuitTuning to tasks where reasoning errors are more semantic or distributed across a sequence, rather than hinging on a single pivotal token (e.g., improving factual consistency in a summary or stylistic tone in a story)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3EuT8k6UPN", "forum": "UVJeljeleU", "replyto": "UVJeljeleU", "signatures": ["ICLR.cc/2026/Conference/Submission14981/Reviewer_U3ZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14981/Reviewer_U3ZQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976859518, "cdate": 1761976859518, "tmdate": 1762925312056, "mdate": 1762925312056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a select-the-finetune method, that targets subnetworks important to reasoning during fine-tuning. The authors evaluated the method on several common benchmarks, and showed that the method outperforms LoRA under certain settings.\n\nAfter reading the paper, I believe that although the proposed method is well grounded on previous related works, the overall framework is premature and computationally expensive, and the advantages of the method are not well presented in the paper. In addition, the evaluation setup is not well-designed, and the performance of the method is not consistent. Therefore I think major revision is needed for the current manuscript, and I recommend rejecting the paper."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The research problem in the paper is well-motivated, focusing on the circuit phenomenon in reasoning models.\n2. The proposed method is clearly explained, combining existing diagnostic frameworks and reasoning-focused training."}, "weaknesses": {"value": "Methodological:\n\n1. The scalability of the proposed method is under question: procedures such as token localization could incur significantly larger cost as the model is scaled-up, since more challenging reasoning tokens/traces will need to be detected and selected.\n2. The advantages of the proposed method is not clearly stated in the paper. Is it memory efficiency? Or better interpretability of the reasoning paradigm? Without stating the advantages, the method will likely be less recognized by practitioners or researchers.\n3. The method seems too tailored in the sense that different models/settings may need significantly different training setups, which is not generalizable. \n\nExperimental:\n\n1. The performance improvement is not consistently better than other baselines, and the chosen baseline (LoRA) are not representative of the state-of-the-art. The authors should at least compare with more representative baselines, such as full fine-tuning or advanced LoRA methods [1]\n2. The fairness of baseline comparison is under question: The authors did not explicitly compare the computational overhead of different methods. Since sub-procedures like token localization can incur substantial computational cost, the authors should explicitly mention it in the paper for clarification. \n3. No supplementary materials or code is provided. This makes the reproducibility of the method under question.\n\n[1] DoRA: Weight-Decomposed Low-Rank Adaptation, https://arxiv.org/abs/2402.09353"}, "questions": {"value": "1. Could the authors provide a more detailed explanation of the targeted parameter update procedure? It seems that the mask is fixed during training, which is counterintuitive, since we would think that the circuits should be dynamically changing.\n2. There are previous work discussing subnetworks in reasoning model training, and sparse training on principal weights crucial to reasoning [1, 2]. It would be great if the authors could add discussions on these works in the paper.\n3. The detailed computational overhead is not outlined in the paper. Could the author provide some detail in the cost of each stage of the proposed method, and compare with the baseline? This will ensure fairness of comparison.\n\n[1] Reinforcement Learning Finetunes Small Subnetworks in Large Language Models, https://arxiv.org/abs/2505.11711\n\n[2] LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning, https://arxiv.org/abs/2506.00772"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UBgSIDgTn8", "forum": "UVJeljeleU", "replyto": "UVJeljeleU", "signatures": ["ICLR.cc/2026/Conference/Submission14981/Reviewer_Bsk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14981/Reviewer_Bsk9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14981/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983614230, "cdate": 1761983614230, "tmdate": 1762925311550, "mdate": 1762925311550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}