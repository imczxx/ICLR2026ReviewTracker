{"id": "jwr5lqwJKt", "number": 13805, "cdate": 1758222885842, "mdate": 1759897411456, "content": {"title": "SAYNEXT: A Benchmark and Cognitively Inspired Framework for Next-Utterance Prediction with Multimodal LLMs", "abstract": "We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker’s next utterance. Instead, humans can readily anticipate forthcoming utterances based on multi-modal cues—such as gestures, gaze, and emotional tone—from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues  spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitive-inspired design to emulate the predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results verify the feasibility of next-utterance prediction with LLMs from multimodal cues, and emphasize the indispensable role of non-verbal cues as the foundation of natural human interaction. We believe this exploration not only opens a new direction toward more human-like, context-sensitive AI interaction but also offers a pathway to uncovering cognitive concepts from dialogue data for human-centered AI.", "tldr": "We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue", "keywords": ["Multimodal Large Language Model", "Benchmark", "Human-Computer Interaction", "Cognitively Inspired Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b36934b7e911dfc8a1d1c7a62f81168ba6783298.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SAYNEXT, a new benchmark and cognitively inspired multimodal framework for next-utterance prediction, aiming to enable LLMs to anticipate human dialogue responses by leveraging both verbal and non-verbal cues such as gestures, gaze, and emotion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes an original task of next utterance prediction grounded in cognitive science, linking dialogue modeling with human predictive processing.\n\n2. Builds a large scale multimodal dataset SayNext PC that includes synchronized video, audio, and text data from real interactions, filling a major gap in multimodal dialogue research.\n\n3. Designs the SayNext Chat dual route framework with learnable cognitive priming tokens, achieving consistent improvements across lexical, semantic, and emotional metrics."}, "weaknesses": {"value": "1. Although the paper introduces a new task of next utterance prediction, it does not convincingly demonstrate its usefulness or downstream impact on other dialogue or reasoning tasks, with only brief conceptual discussion in the introduction.\n\n2. The evaluation on the proposed dataset lacks depth and insight, offering mainly quantitative comparisons without detailed analysis of model behavior or failure cases.\n\n3. The cognitive analogy combining dual route processing and priming is interesting but insufficiently explored, lacking stronger ablation or interpretability evidence to support the cognitive claims."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hZk2cJeGSw", "forum": "jwr5lqwJKt", "replyto": "jwr5lqwJKt", "signatures": ["ICLR.cc/2026/Conference/Submission13805/Reviewer_AYCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13805/Reviewer_AYCo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793658058, "cdate": 1760793658058, "tmdate": 1762924336187, "mdate": 1762924336187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task, namely human next-utterance prediction. The authors argue that predicting human utterances reflects LLMs' ability to better understand human emotions and other needs, which is beneficial for building LLMs applied in scenarios involving intensive human interaction, such as embodied intelligence. Therefore, this paper proposes a benchmark, SayNextBench, to evaluate LLMs' ability to understand information and predict human next utterances in multimodal scenarios. The benchmark includes SayNext-PC2K and SayNext-PC19K.\n\nTo evaluate the models, this paper proposes four complementary evaluation metrics: Subject-Dependent Evaluation, Subject-Independent Evaluation, Cross-Scenario Evaluation, and Scalability Evaluation.\n\nAdditionally, this paper proposes a dual-route prediction MLLM, SayNext-Chat, and experiments show that its ability to predict human next utterances is significantly stronger than other zero-shot models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-presented, rigorously organized, and well-written.\n\nThis paper attempts to propose a novel task, construct a relatively comprehensive benchmark, and train a model, making the work quite thorough."}, "weaknesses": {"value": "I will place the weaknesses and my questions together in this section.\n\nI may have some misunderstandings about the motivation of this paper. I do not fully understand the difference between the next-utterance prediction task and dialogue tasks, as well as the necessity of proposing this as a separate task. If an LLM can effectively model the contextual logic in a dialogue, then predicting a human's next utterance that aligns with the context should not be a problem. What is the difference between multimodal next-utterance prediction and multimodal dialogue? Does it lie in responding to human A's dialogue or predicting human A's next utterance? With all due respect, there is no difference between the two—predicting human A's next utterance is equivalent to responding to human B's dialogue. In early dialogue tasks, most datasets were constructed by collecting dialogue histories between two human annotators.\n\nPerhaps the task proposed in this paper would be more appropriately termed fine-grained human intent classification or prediction, and the authors need to emphasize the importance of fine granularity and its relationship with previous fine-grained intent recognition work.\n\nAdditionally, there is a one-to-many issue here, meaning the provided reference responses are not unique. In this paper, the evaluation mainly involves calculating token and semantic relevance between predicted responses and reference responses, without assessing the consistency between predicted responses and the context or other multimodal contexts. Human evaluation comparing the quality of responses across models can alleviate my concerns to some extent.\n\nIn Section 2.3, the paper mentions four evaluation perspectives, which also serve as guiding principles for constructing the benchmark. Is there a correspondence with the subsequent experimental analysis? It would be better if the subsequent experimental analysis and the models' performance in these four capabilities could be aligned."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KQaHqXN8Bv", "forum": "jwr5lqwJKt", "replyto": "jwr5lqwJKt", "signatures": ["ICLR.cc/2026/Conference/Submission13805/Reviewer_7tXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13805/Reviewer_7tXM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832435577, "cdate": 1761832435577, "tmdate": 1762924335727, "mdate": 1762924335727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark for predicting the next utterance in a dialogue in the context of a sports interview, where there is access to the previous dialogue turn (the interviewer's question) and video from the only the interviewee (the sports star) whilst this question is being asked.  \n\nThe model is used to generate natural language responses, which are compared to the ground truth response using a variety of metrics.\n\nThe paper also proposes, and experiments with, a novel cognitively-inspired \"priming\" technique, in which a code book of \"priming factors\" are created.  At inference time, numerical values for these factors can be predicted, where are used in turn to better inform the response generation task.  \n\nThe dataset, SayNext-Bench is used to train a model, which is compared to zero-shot generation using multi-modal LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I found this to be an enjoyable and interesting paper.  It is not straightforward to collect this kind of multimodal dialogue dataset, and it is clear that a great deal of work has gone in to it.  Although very constrained in terms of its domain, I'm sure it will be a useful resource, particularly for researchers wishing to study emotional dialogue – on account of the domain, the corpus is unusually rich in emotional dialogue.\n\nAlthough I didn't find all of the metrics investigated very convincing, and the setup does have some limitations (see below) the emotion metrics will be useful, and the corpus representations a great multimodal emotion prediction + generation task.\n\nThe work was clearly motivated and had good use of examples and diagrams."}, "weaknesses": {"value": "I wasn't entirely clear what the primary purpose of the paper was, making evaluation somewhat difficult.  As a benchmark, it is good, for the reasons mentioned above.  However, one limitation is the narrowness of the domain, being constrained not just to sports interviews, but in fact to tennis interviews.  Another limitation is that only the single interviewer turn is included, meaning that it is impossible to generate accurate responses that rely on past dialogue context, or in fact, to events that had happened in the match immediately preceding that would be common ground to both speakers (this is clear from the ground truth examples, which often refer to this information).\n\nThe authors take care to use metrics that circumvent these limitations, but it does mean that metrics such as BLEU and ROUGE – and to some extend, the BERT metrics – are effectively meaningless, since no generation can achieve anything close to the ground truth on these scores.  (The authors do acknowledge these issues).  \n\nI suspect that it also means that the model fine-tuned on the benchmark's training set have an unfair advantage over the zero-shot LLMs, which are not as far as I can tell, provided with any tennis-related context.  This makes the comparisons less useful.  I'm not sure why you didn't consider fine-tuning other multimodal models, particularly so that you could have more convincingly demonstrated the value of the priming approach (which was of the most interesting parts of the paper).\n\nAnother limitation is the lack of video for the interview, meaning that the benchmark will not generalise to many more natural human-human dialogue settings.\n\nI suggest that at its core, this is not really a dialogue prediction task, but rather an video emotion-prediction task, with emotion predictions made (rather cleverly) through the medium of a text-based response.  This is valuable in itself, but probably should be more clearly acknowledged.  \n\nIf on the contrary, it really is a text prediction task you are aiming at, there should have been more comparison to text-only models in the main paper (there seemed to be just a limited number of these comparisons in the appendix), much more context added, and metrics should have included figures such as perplexity, which would have avoided the major problem of responses all having low lexical overlap.  You should also have considered citing the wealth of related turn prediction task that already exist in speech and text based dialogue."}, "questions": {"value": "I struggled to understand the way in which IEMOCAP is adapted to SayNext-Bench (§4.2.4) – I really could not understand how this was done, and also why the figures for LO in the bottom row of Table 2 are so dramatically better than all the others."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BAoWDGlPor", "forum": "jwr5lqwJKt", "replyto": "jwr5lqwJKt", "signatures": ["ICLR.cc/2026/Conference/Submission13805/Reviewer_WYoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13805/Reviewer_WYoH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189517022, "cdate": 1762189517022, "tmdate": 1762924335291, "mdate": 1762924335291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study addresses a key limitation of large language models (LLMs): their struggle to accurately predict humans’ next conversational utterances, unlike humans who use multimodal cues (gestures, gaze, tone). The team developed SayNextBench (a benchmark) and SayNext-PC (a dataset of real-world dialogues like post-match interviews), plus a cognitively inspired dual-route model (SayNext-Chat). By integrating verbal and non-verbal signals and using “priming factors” to capture intent, the model outperforms state-of-the-art (SOTA) MLLMs in lexical overlap, semantic similarity, and emotion consistency, paving the way for more human-like AI dialogue."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The SayNext-PC dataset scales from 2K to 19K samples, covering diverse cultures and scenarios, while its four evaluation protocols (plus user studies) add rigor."}, "weaknesses": {"value": "Overall, this type of data construction lacks novelty for me, and there are too many such works in the literature. Additionally, the method used is too simplistic and lacks any theoretical support.\n\nSpecifically, first, all models show low lexical overlap (max ~5%), highlighting an inherent challenge: even optimized models struggle to replicate human phrasing, revealing limitations in capturing personalized language styles. Second, the core dataset focuses on sports post-match interviews, a relatively narrow scenario. While cross-scenario validation was done, generalization to casual chats or professional settings (e.g., workplace communication) remains unproven.\n\nFurthermore, the model relies on GPT-4.1 to extract priming factors and assign vectors, reducing openness and reproducibility—ordinary researchers may face barriers to low-cost reuse. Additionally, the study only focuses on single-turn dialogue prediction. Real conversations are multi-turn, so the model lacks long-term tracking of context coherence or speaker habits, limiting practical use.\n\nFinally, the 20 fixed priming factors were chosen empirically; no testing was done on dynamic adjustment for different scenarios (e.g., casual vs. formal dialogue may need different cognitive-emotional dimensions). Training also requires A100 GPUs, creating high hardware barriers that hinder widespread adoption."}, "questions": {"value": "1. Since low lexical overlap is a common issue, would adding multi-turn dialogue data or training the model to learn individual speakers’ language habits help it better mimic human phrasing?\n\n2. The 20 priming factors were set based on experience, could this number be dynamically adjusted for different scenarios? For example, casual chats and formal interviews may require different cognitive-emotional dimensions, so would flexible factor counts improve performance?\n\n3. Among non-verbal cues (gestures, facial expressions, tone), which contributes most to next-utterance prediction? If only core cues are retained, can we reduce computational costs without sacrificing performance?\n\n4. Beyond post-match interviews, would the model’s performance drop significantly in more casual (e.g., friend chats) or professional (e.g., client communication) scenarios? What strategies could enhance cross-scenario robustness?\n\n5. Currently, GPT-4.1 is used to generate priming factors, would switching to open-source models (e.g., Llama series) drastically reduce effectiveness? Are there low-cost alternatives to make this framework accessible to more researchers?\n\n6. While emotion consistency is strong, real dialogues often include complex emotions like humor or sarcasm. Can the model accurately capture and predict such nuanced next utterances?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iCvygYeILX", "forum": "jwr5lqwJKt", "replyto": "jwr5lqwJKt", "signatures": ["ICLR.cc/2026/Conference/Submission13805/Reviewer_gkQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13805/Reviewer_gkQJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13805/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762945540145, "cdate": 1762945540145, "tmdate": 1762945540145, "mdate": 1762945540145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}