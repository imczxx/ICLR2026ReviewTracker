{"id": "7p2YQtAIfv", "number": 2111, "cdate": 1756991020588, "mdate": 1758854574994, "content": {"title": "BACKDOOR COLLAPSE: ELIMINATING UNKNOWN THREATS VIA KNOWN BACKDOOR AGGREGATION IN LANGUAGE MODELS", "abstract": "Recent advancements in Large Language Model (LLM) safety have revealed that backdoor attacks pose significant threats. Adversaries implant stealthy behaviors that remain dormant under normal inputs but are abruptly triggered by specific patterns. Such vulnerabilities are exacerbated by the widespread practice of downloading pre-trained checkpoints from public repositories and the increasing reliance on large-scale, imperfectly curated datasets. Although existing defense mechanisms demonstrate promising results in specific scenarios, they often rely on impractical assumptions about backdoor triggers or target behaviors, such as known trigger length, fixed poison ratio, or white-box access to attacker objectives. In this paper, we propose Locphylax, a defense framework that requires no prior knowledge of trigger settings, which is based on the key observation that when deliberately injecting known backdoors into an already-compromised model, both existing unknown and newly injected backdoors aggregate in the representation space. Locphylax leverages this phenomenon through a two-stage process: first aggregating backdoor representations by injecting known triggers, then performing recovery fine-tuning to restore benign outputs. Extensive experiments across multiple LLM architectures demonstrate that: (I) Locphylax reduces the average Attack Success Rate to 4.41% across multiple safety benchmarks, outperforming existing baselines by 28.1%~69.3%. (II) Clean accuracy and downstream utility are preserved within 0.5% of the original model, ensuring negligible impact on legitimate tasks. (III) The defense generalizes across supervised fine-tuning, RLHF, and direct model-editing backdoors, confirming its robustness in practical deployment scenarios.  Our code is available at ~\\url{https://anonymous.4open.science/r/ICLR2026-Locphylax}.", "tldr": "", "keywords": ["backdoor"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/72c6830974a414d79367199502d6a7bb4ff6dcb2.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}