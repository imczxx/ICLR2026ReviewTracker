{"id": "mJiPqOzc3O", "number": 14708, "cdate": 1758242193902, "mdate": 1763685224436, "content": {"title": "Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge", "abstract": "Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators.\nWe promise to release models and data upon acceptance.", "tldr": "We propose to incorporate fundamental physics knowledge into learning neural operators to enhance its data efficiency, long-term consistency, and OOD generalization.", "keywords": ["Neural Operator", "PDE", "Fundamental Physics Knowledge"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11a84aef845b47a1af558b96d01e32a4d362b3af.pdf", "supplementary_material": "/attachment/2ffe07d3872b2df894d6fea8d324ebc8b54975c7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a data-efficient learning framework for PDE dynamics forecasting by jointly learning from both the original PDEs and their simplified basic forms. Extensive experiments on a wide range of 1D/2D/3D PDE problems demonstrates the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is well-motivated. The authors provide a critical observation by evaluating existing SciML foundation models. They find a strong correlation between a model's performance on the original PDE and its performance on the fundamental components of that PDE (e.g., pure diffusion for a reaction-diffusion system). However, the absolute error on these basic terms remains high, indicating that even powerful models lack a robust understanding of the foundational physics, which motivates the need for explicit training on these concepts.\n- Methodological Innovation:​​ The paper proposes a simple yet effective multiphysics training framework. It first derive a \"basic form\" from the original PDE by retaining terms governing essential dynamics and removing terms that cause computational stiffness or high cost. The model is trained on a composite dataset from simulations of both the original PDE and the basic form."}, "weaknesses": {"value": "- Heuristic Nature of Decomposition:​​ The process for selecting terms for the \"basic form,\" while physically intuitive, remains heuristic. A more formalized principle or an ablation study discussing the impact of alternative decompositions more prominently would strengthen the methodology. \n- Inadequate Mechanistic Explanation for the Efficacy of Basic Form Data​: A significant weakness of the paper lies in its insufficient exploration of the underlying mechanisms by which the \"basic form\" data aids the learning of the original PDE. The attribution of performance gains solely to the incorporation of \"fundamental physics knowledge\" is a high-level concept that lacks granularity. A more rigorous analysis is required to dissect how the basic form data contributes. A possible explanation is that data from the basic form may provide more diverse initial conditions. Can the data from the basic form be replaced with an equivalent amount of original PDE data? Although this would incur greater simulation costs, it would help clarify the specific ways in which data from the basic form aids the model in learning the original PDE."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Rcm76I4eQ9", "forum": "mJiPqOzc3O", "replyto": "mJiPqOzc3O", "signatures": ["ICLR.cc/2026/Conference/Submission14708/Reviewer_dWHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14708/Reviewer_dWHm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571490102, "cdate": 1761571490102, "tmdate": 1762925071902, "mdate": 1762925071902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multiphysics training scheme that jointly learns from full PDE simulations and their decomposed “basic forms” the authors argue that this injects “fundamental physics knowledge” into neural operators (NOs) and improves data efficiency and OOD generalization.  The key contribution lies in identifying and leveraging \"fundamental physics knowledge\" through decomposed basic PDE forms.  This has not  been explored extensively in the neural operator literature.  \nThe authors target two central SciML issues, 1. data hunger and 2. poor OOD transfer, for operator learning across 1D/2D/3D PDEs (Diffusion-Reaction, Navier–Stokes, Kuramoto–Sivashinsky, plus ScalarFlow). \nFormulations of PDEs and “basic forms” are standard and correctly specified\nThe paper is generally well written with helpful overview figures (Fig. 3 pipeline; Fig. 4 gallery of PDEs/basic forms) and plots tying simulation cost to nRMSE. Implementation, data splits, and training schedules are placed in appendices. \nMinor typos remain but do not impede readability.\nCentral claims are supported with experiments and results that are a bit light on content\nThe validation of physics (central theme) is light given no explicit checks on mass/energy conservations.  Another issue is the heuristic treatment of the fundamental physics term."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors target two central SciML issues, 1. data hunger and 2. poor OOD transfer, for operator learning across 1D/2D/3D PDEs (Diffusion-Reaction, Navier–Stokes, Kuramoto–Sivashinsky, plus ScalarFlow). \nThe key contribution lies in identifying and leveraging \"fundamental physics knowledge\" through decomposed basic PDE forms.  This has not  been explored extensively in the neural operator literature.  \nProposed benefits such as:\nData efficiency, Long horizon stability , OOD generalization , are all desirable."}, "weaknesses": {"value": "The term \"fundamental physics knowledge\" is somewhat vague and could be better defined\nSection 3.1 could be more systematic in explaining the decomposition principles\nSome notation inconsistencies (e.g., switching between v and u for solutions\n\nMissing error bars in main results (added later in appendix)\nthere is limited statistical analysis\nThe ScalarFlow experiment (Section 4.5) is somewhat disconnected and brief\nClaims about \"fundamental physics knowledge\" being key are not fully validated (could be just multi-task regularization)\nThere is a lack of theoretical insight, no formal explanation of why the approach works beyond intuition is provided.\nThe decomposition rules in Section 3.1.1 seem ad-hoc without principled justification\n\nEvaluation is very basic, results only compare against vanilla baseline and spatiotemporal downsampling\nThere is no comparison/discussion with other data-efficient methods or recent foundation models\nReal-world evaluation is very limited (only ScalarFlow)\nInconsistent terminology: \"Fundamental physics knowledge\" vs \"basic forms\" used interchangeably\nMissing details: How are the mixture ratios exactly determined? Training time comparisons?\nLimited discussion: When would this approach fail? What about PDEs that don't decompose nicely?\nPresentation issues: Some figures (especially in appendix) are too small to read clearly\nScalability concerns: All experiments on relatively small-scale problems\n\nGrammer-Typos\nline 483: and outha ha h-of-distribution generalization."}, "questions": {"value": "See weakness section , addressing those would be good\nrecommend to:\nBetter justify the decomposition principles\nProvide theoretical analysis or at least intuition for why the approach works\nCompare with more baselines\nDiscuss limitations and failure cases more thoroughly"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethical issues"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g7p98K39ls", "forum": "mJiPqOzc3O", "replyto": "mJiPqOzc3O", "signatures": ["ICLR.cc/2026/Conference/Submission14708/Reviewer_kRiv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14708/Reviewer_kRiv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788075973, "cdate": 1761788075973, "tmdate": 1762925071492, "mdate": 1762925071492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses basic forms of standard PDEs. It finds out that if put half of the data generation efforts into generating basic form of PDE, the performance and generalization will generally improves. The paper test on diffusion-reaction, 2d NS, 3d NS, and KS equation. The improvement on 2D NS is the most significant."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an important but often overlooked probably on efficient data generation for neural operators.\n- When comparing against the baseline, the paper controls the overall run time budget.\n- The experiment show consistent improvement."}, "weaknesses": {"value": "- It is not always obviously which is the best basic form to each target equation. I don't think there is a canonical choice, and the performance depends on the choise of augmented basic PDE. For example on 2D NS the treatment is more significant, but not as much on KS. It would be better to add some ablations to justify the choice.\n- Overall I like this paper but I don't like the storytelling. The main message should be \"it is helpful to generate additional data in simpler form\". It is a bit speculative to claim about \"fundamental physics knowledge\". In my opinion it can be awkward to say diffusion is the \"fundamental physics\" to diffusion reaction equation, or convection is \"fundamental physics\" to Navier Stokes equation.\n- Line 108, the authors use Spearman correlation which is defined for ordinal (rank) correlation. Instead it would be better to use Pearson correlation as the rank does not matter here."}, "questions": {"value": "- In the experiment, we need to be a bit careful how we measure the simulation cost (Table 1). In practice, the runtime of numerical solvers depends on the choices of gridsize, timestep, and numerical tolerance etc. Here how the simulation cost is measured? \n- If we instead add low fidelity data (with lower gridsize) with the same equation, would the performance improve?\n- How about adding smaller RE on NS, it would requires low simulation cost too?\nIn general it would be helpful to add a bit more discussion on the simulation cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YQDNMqJia5", "forum": "mJiPqOzc3O", "replyto": "mJiPqOzc3O", "signatures": ["ICLR.cc/2026/Conference/Submission14708/Reviewer_FhDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14708/Reviewer_FhDJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858697342, "cdate": 1761858697342, "tmdate": 1762925070870, "mdate": 1762925070870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}