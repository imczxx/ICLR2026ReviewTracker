{"id": "l1cLdEjESj", "number": 17409, "cdate": 1758275685198, "mdate": 1759897177020, "content": {"title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction–Reasoning Synergy", "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision–Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and  3D Visual Grounding tasks,  demonstrating the superior multi-task capabilities.", "tldr": "", "keywords": ["video-based 3D MLLM", "geometric priors", "Cross-Task Adapter", "Metric Depth calibration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/13d25f8fc7880021b30f51b183bdf51fd5603bb6.pdf", "supplementary_material": "/attachment/95a113cbb29e77a85af0620f40acfe08fb9bdd79.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Vid-LLM, a video-based Multimodal Large Language Model for 3D scene understanding that works directly from video, removing the need for external 3D data. The method leverages geometric priors and incorporates them efficiently using a Cross-Task Adapter (CTA) to align 3D geometry with vision-language features. A Metric Depth Model ensures accurate real-scale geometry, and a two-stage distillation strategy improves training. Experiments show Vid-LLM excels at 3D Question Answering, Dense Captioning, and Visual Grounding, outperforming existing methods in multi-task 3D reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of 3D geometric information into 3D LLMs is well-motivated in this paper.\n\n2. The paper is clearly written and easy to follow.\n\n3. The experiments effectively demonstrate the improvements brought by the proposed modules.\n\n4. It is encouraging to see that the proposed method achieves competitive performance on both pose estimation and depth estimation tasks."}, "weaknesses": {"value": "1. Integrating 3D foundation models into 3D MLLMs is not a novel concept. Recent works such as VG-LLM [1] and 3DRS [2], which also utilize VGGT to enhance the 3D-awareness of 3D MLLMs, are not cited or compared in this paper. Including a discussion or direct comparison with these approaches would strengthen the related work section and better contextualize the contributions of this work.\n\n2. In Tables 7 and 8, it is unclear how the proposed model's performance compares to that of the teacher model, VGGT. Providing this comparison would improve the clarity and completeness of the evaluation.\n\nReferences:\n\n[1] Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors. NeurIPS 2025.\n\n[2] MLLMs Need 3D-Aware Representation Supervision for Scene Understanding. NeurIPS 2025."}, "questions": {"value": "1. Please incorporate comparison with recent works.\n\n2. Please add performance comparison with VGGT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p7iAJM29om", "forum": "l1cLdEjESj", "replyto": "l1cLdEjESj", "signatures": ["ICLR.cc/2026/Conference/Submission17409/Reviewer_1jxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17409/Reviewer_1jxT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889973308, "cdate": 1761889973308, "tmdate": 1762927307840, "mdate": 1762927307840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes Vid-LLM, a video-based 3D multimodal large language model that performs both 3D reconstruction and vision-language reasoning directly from monocular video inputs. The main contributions of this submission are\n\n1. Cross-Task Adapter (CTA) that aligns geometric priors with vision-language features, enabling joint geometry–semantic interaction.\n2. Metric Depth Model (MD) that restores real-scale 3D geometry with bin-based depth prediction and adaptive scale alignment.\n3. Two-Stage Training Strategy with dual-teacher distillation and joint optimization to improve convergence and stability.\n\nThe authors conduct extensive evaluations on 3D-QA, dense captioning, and grounding benchmarks showing consistent performance gains over 3D- and video-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and structured. Overall it easy to follow the technical narrative and experiments.\n- The related work section provides a comprehensive overview of 3D multimodal large language models and video-based reasoning approaches.\n- The overall design, including the Cross-Task Adapter (CTA), Metric Depth (MD) module, and the two-stage training strategy, is well motivated and technically sound. The integration of geometric and semantic cues is conceptually coherent and justified.\n- The ablation studies are thorough, demonstrating the necessity of each proposed component to the final performance.\n- The experimental results show consistent and significant improvements across multiple 3D vision-language reasoning benchmarks."}, "weaknesses": {"value": "- Missing details and clarifications:\n    - In Figure 2, the predicted 3D reconstruction results are used for generating 3D position embeddings. It would be helpful to clarify whether gradients are propagated through this path when jointly training the reconstruction and reasoning modules.\n    - In Section 3.4, Equation (6), the notation $\\text{Norm}(\\cdot)$ appears ambiguous. Please clarify whether it denotes feature normalization (e.g., L2-normalization) or the 2-norm operation itself. An explicit definition would avoid confusion.\n    - In Section 3.4, Equation (6), what is the dimensionality of $T_{\\text{tea}}^{\\text{lang}}$? Is the feature-level loss$L_{\\text{feat}}^{\\text{lang}}$ averaged over N samples similar to $L_{\\text{feat}}^{\\text{geo}}$? Clarifying this would help in understanding the implementation consistency.\n- Simplification of the bridge token mechanism:\n    \n    The current Cross-Task Adapter introduces bridge tokens for cross-modal interaction. Would a simpler alternative—such as applying self-attention directly on the concatenation of $ T_{\\text{geom}} $ and $ T_{\\text{lang}} $—achieve similar performance with lower complexity? Some empirical or ablation analysis could strengthen the design justification.\n    \n- Qualitative analysis:\n    \n    More qualitative results and failure case discussions would be valuable to understand when the model fails. The authors are suggested to include them in the camera-ready version."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4AKSugWFr6", "forum": "l1cLdEjESj", "replyto": "l1cLdEjESj", "signatures": ["ICLR.cc/2026/Conference/Submission17409/Reviewer_Zqjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17409/Reviewer_Zqjo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950109624, "cdate": 1761950109624, "tmdate": 1762927307347, "mdate": 1762927307347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Vid-LLM, a compact multimodal model that unifies 3D reconstruction and vision-language reasoning within a single framework. I really like this idea -- combining reconstruction with VL reasoning not only bridges geometry and semantics but also achieves faster inference through a more efficient architecture. The work is conceptually elegant and technically well-motivated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The model design is well thought out, especially the Cross-Task Adapter (CTA) that enables effective geometry–semantics interaction.\n\n- The overall training pipeline, including two-stage distillation and joint optimization, is clear and coherent.\n\n- The experimental setup is solid - reconstruction and 3D-VL baselines are comprehensive, and the ablation studies are extensive enough to verify the core hypothesis and support the narrative.\n\n- Results demonstrate consistent gains in both reasoning and reconstruction efficiency, validating the proposed synergy between the two tasks."}, "weaknesses": {"value": "The main baseline (VGGT + LLaVA-3D) appears to involve two independently fine-tuned components that are only concatenated during inference, rather than being jointly optimized end-to-end. This makes the comparison to the proposed method somewhat unfair, as Vid-LLM benefits from full end-to-end training while the baselines do not. It would strengthen the paper to clarify this distinction and, if possible, include a jointly optimized baseline to better isolate the benefit of the Cross-Task Adapter."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5j306LhoSB", "forum": "l1cLdEjESj", "replyto": "l1cLdEjESj", "signatures": ["ICLR.cc/2026/Conference/Submission17409/Reviewer_D2AN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17409/Reviewer_D2AN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056158993, "cdate": 1762056158993, "tmdate": 1762927307026, "mdate": 1762927307026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}