{"id": "lxqTU8Ofb2", "number": 7031, "cdate": 1758005592693, "mdate": 1763630339989, "content": {"title": "Bias-variance Tradeoff in Tensor Estimation", "abstract": "We study denoising of a third-order tensor when the ground-truth tensor is **not** necessarily Tucker low-rank. Specifically, we observe\n$$\nY=X^\\\\ast+Z\\in \\\\mathbb{R}^{p_{1} \\\\times p_{2} \\\\times p_{3}},\n$$\nwhere $X^\\\\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\\\\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$,\n$$\n\\\\| \\\\widetilde{X} - X^\\ast \\\\|^2_{\\\\mathrm{F}} = O \\\\Big( \\\\kappa^2 \\\\Big\\\\{ r_{1}r_{2}r_{3} + \\\\sum_{k=1}^{3} p_{k} r_{k} \\\\Big\\\\} \\\\; + \\\\; \\\\xi_{(r_{1},r_{2},r_{3})}^2 \\\\Big) \\\\quad \\\\text{  with high probability.}\n$$\nHere, the bias term $\\xi_{(r_1,r_2,r_3)}$ corresponds to  the best achievable approximation error of $X^\\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$;    $\\kappa^2$   quantifies the noise level; and the variance term  $\\kappa^2 \\\\{r_{1}r_{2}r_{3}+\\sum_{k=1}^{3} p_{k} r_{k}\\\\}$ scales with the effective number of free parameters in  the estimator $\\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias-variance tradeoff: as we increase  the ranks of estimator $\\widetilde{X}$, the bias   $\\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a  convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.", "tldr": "Rank-adaptive HOSVD gives explicit bias–variance bounds for tensor denoising without exact low-rankness; includes a unified matrix SVD results.", "keywords": ["Bias–variance tradeoff", "tensor denoising", "Tucker decomposition", "HOSVD", "rank-adaptive estimation", "truncated SVD", "3D MRI experiments."], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d371540a937462f9dfb9ca97e2dd06adaf12dc8e.pdf", "supplementary_material": "/attachment/c401240850f31fa58210aaf43698a8b93ac51b83.pdf"}, "replies": [{"content": {"summary": {"value": "This paper is focused on denoising a third-order tensor signal corrupted by additive noise, by means of a low-multilinear-rank approximation.\n\nIts main message is that, by employing a simple spectral algorithm that is essentially a one-step HOOI initialized by truncated HOSVD (THOSVD), the resulting approximation error (with respect to the sought signal) can be decomposed into the sum of a bias term which decays in (each component of) the multilinear rank of the model, plus a variance term which grows in (the components of) the rank."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) This bias-variance analysis is certainly novel and of interest to the community.\n\n2) The raised connections with existing results, in particular those of Zhang and Xia (2018), are also of interest."}, "weaknesses": {"value": "The paper has several presentation issues, thus needing substantial polishing. My main concerns in this regard are:\n\n1) The designed algorithm per se is not novel, simply being a variant of a one-step HOOI initialized by truncated HOSVD (THOSVD).\n\n2) The paper fails to cite and acknowledge important results and previous work on this topic, notably the fact that the THOSVD and similar algorithms such as the sequentially truncated HOSVD are quasi-optimal, as shown in the relatively celebrated paper:\n[Vannieuwenhoven, N., Vandebril, R., & Meerbergen, K. (2012). A new truncation strategy for the higher-order singular value decomposition. SIAM Journal on Scientific Computing, 34(2), A1027-A1052.]\nIn particular, some of the proof techniques used in this paper, such as the telescoping sums introduced to control the approximation error by multilinear projection (see Appendix A), seem to be inspired by the classical proof of this quasi-optimality result, as given in the above paper. These connections should be pointed out. \n\n3) The numerical experiments are in my view ill-designed, and the corresponding section is not sufficiently clear:\n- In Section 4.1, it is quite difficult to see differences among the given images. The paper should include zoom boxes or arrows for highlighting specific details which support the conclusions, or even change the parameters in order to illustrate its findings in a more clear way. Also, why does the discussion here stops short of considering high values or rank $r$ for seeing its effect on the variance? \n- While the paper focuses on the Tucker model, the synthetic tensors described in 4.2 follow instead an orthogonal canonical polyadic decomposition (due to the diagonal core). Why that choice? Moreover, this model is exactly low-rank (as seen in Table 1, which by the way is redundant since this information is already shown in Table 2), contrarily to the whole point of the Introduction of handling the case of approximately low-rank signals.\n- The following conclusion given in Section 4.2 is quite vague, and makes us think that the goal here is to evaluate the given algorithm instead of illustrating the bias-variance tradeoff: \"We observe that the error consistently decreases as the SNR parameter $\\lambda$ increases. Overall, one-step HOSVD is robust across the tested sizes and ranks, yielding accurate estimates on the synthetic tensors.\" Yet, if that is the case, then a comparison with other similar algorithms (at least THOSVD as a baseline) is required. But in my view, given that the algorithm is not novel, the paper should rather focus on showing how the utility of the derived result on the bias-variance tradeoff, for instance by plotting separate curves for the bias and the variance terms as a function of the ranks and showing that they follow the predicted trends (bias decay and variance growth) with the correct scaling.\n- By the same token, if an exactly low-rank model is to be used, then experiments such as those reported on Table 2 should not stop at a model rank $r$ below the true rank $s$: this would allow seeing the effect of a rank overestimation on the variance.\n\n4) The paper states that the results given for the matrix are a \"byproduct\" of the tensor analysis, but this claim seems misleading to me. The statement of Theorem 2 is not a particular case of Theorem 1; these results are in fact of a quite different nature: Theorem 1 has a probabilistic flavor and requires a spectral gap condition in order to hold, while Theorem 2 is completely deterministic and valid for any signal. Even the proof arguments are quite dissimilar.\n\n5) Last but not least, a careful revision is in order. In particular:\n- Several results referred to as Theorems (for instance, in lines 280 and 281) are actually other types of results (Lemma, Corollary).\n- It seems that the constant in (6) should be denoted by C, according to the discussion in lines 242-243."}, "questions": {"value": "1) Would it be possible have a deterministic result similar to that of Theorem 2 for tensors, with a bound depending on the model degrees of freedom and, say, some norm (spectral?) of the noise tensor?\n\n2) The paper argues that its results extend in a way those of Zhang & Xia, and in particular match those previous results in the case of an exactly low-rank model. Have the authors also checked that their bounds match the known results for rank-one tensor PCA? Namely, for $p_1=p_2=p_3=p$, an appropriately normalized noise (say, $\\kappa = 1/\\sqrt{p}$) and $r_1=r_2=r_3=1$, the condition on the signal magnitude for having an $O(1)$ error should match the known conjectured computational threshold $p^{1/4}$. It seems to me that this is the case, which would be interesting to point out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FKXdJShRTX", "forum": "lxqTU8Ofb2", "replyto": "lxqTU8Ofb2", "signatures": ["ICLR.cc/2026/Conference/Submission7031/Reviewer_28be"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7031/Reviewer_28be"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641506228, "cdate": 1761641506228, "tmdate": 1762919233074, "mdate": 1762919233074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the problem of noisy tensor estimation when the the ground-truth tensor is not necessarily Tucker low-rank. By utilizing some  classical linear algebra results such as Mirsky’s and Ky Fan’s theorems, the authors derive a clean rank-adaptive bias–variance tradeoff. A series of simulations is carried out to confirm the existence of tradeoffs across different regimes of spectral decay and noise."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A new tensor estimation setting has been not considered in the literature. \n2. A rigorous theoretical analysis is carried out to show clean the rank-adaptive bias–variance tradeoff in such a tensor estimation problem. \n3. As a byproduct, a convenient bias-variance tradeoff in matrix estimation has been obtained."}, "weaknesses": {"value": "1. The minx lower bounds for both tensor and matrix cases are not provided. \n2. The main theoretical derivations seem to be direct extensions of the work Zhang & Xia (2018).\n3. How to tune the target Tucker rank for the proposed Algorithm 1 has not been provided."}, "questions": {"value": "1. Why the authors consider the simple one-step HOSVD algorithm rather than the popular HOOI algorithm?\n2. Can the proposed proof strategy be readily extended to accommodate general $d$th-order tensors, and if so, what specific steps or modifications would be required to achieve this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tsq91aLFue", "forum": "lxqTU8Ofb2", "replyto": "lxqTU8Ofb2", "signatures": ["ICLR.cc/2026/Conference/Submission7031/Reviewer_eK8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7031/Reviewer_eK8H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020092228, "cdate": 1762020092228, "tmdate": 1762919232481, "mdate": 1762919232481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proves a new bias-variance tradeoff for Tucker decompositions of general third-order tensors.  No assumption is made on the input tensor being close to low-rank, which makes the results highly applicable to real data.  The authors provide demonstrations with real datasets to reinforce this.  Overall, this is a very nice submission."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- No assumption is made on the input tensor being exactly low-rank or close to low-rank.  This makes the result applicable to any input tensor, and thus very useful.\n\n- As the authors explain, the proven bound is optimal up to constants.\n\n- The authors show that the HOSVD algorithm achieves the bound.\n\n- There are nice real data illustrations.\n\n- The paper is very clearly written."}, "weaknesses": {"value": "- It would have been nice if the authors extended their analysis to tensors beyond third order tensors.\n\n- A few useful references are missing."}, "questions": {"value": "(1) Line 35: missing both older and more recent papers on latent variable learning.  Obviously the authors should cite \n\tAnimashree Anandkumar, Rong Ge, Daniel J Hsu, Sham M Kakade, Matus Telgarsky, et al. \"Tensor decompositions for learning latent variable models.\" J. Mach. Learn. Res., 15(1):2773–2832, 2014.  \nhere instead of the community detection part.\nI also suggest the authors cite \n\tYifan Zhang, Joe Kileel, \"Moment estimation for nonparametric mixture models through implicit tensor decomposition\", SIAM Math. Data Sci. 5.4 (2023), 1130-1159.\nas well as \n\tSamantha Sherman, Tamara G. Kolda. \"Estimating higher-order moments using symmetric tensor decomposition.\" SIAM Matrix Anal. Appl. 41.3 (2020), 1369-1387.\n\n\n\n(2) Lines 43-50: regarding the low-rank Tucker tensor decomposition, the authors should cite \n\tRuhui Jin, Joe Kileel, Tamara G. Kolda, Rachel Ward. \"Scalable symmetric Tucker tensor decomposition.\" SIAM Matrix Anal. Appl. 45.4 (2024), 1746-1781\n\n\n\n(3) Can the authors add some remarks on how the analysis should go for higher than third order tensors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jnr4lHSePc", "forum": "lxqTU8Ofb2", "replyto": "lxqTU8Ofb2", "signatures": ["ICLR.cc/2026/Conference/Submission7031/Reviewer_GWqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7031/Reviewer_GWqa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043860150, "cdate": 1762043860150, "tmdate": 1762919232014, "mdate": 1762919232014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of denoising a third-order tensor $Y = X^* + Z$, where $Y$ is the observed noisy tensor, $ X^* $ is the ground-truth signal, and $ Z $ is a noise tensor. Existing work mainly assumes that $ X^* $ is of low Tucker-rank as compared to the dimension. In contrast, this paper does not impose this assumption, and considers a simple variant of the HOSVD algorithm. They show that the recovery error of this algorithm can be upper bounded by the sum of two terms: a bias term that depends on the best achievable estimation error of $X^*$ over the space of tensors of low Tucker-rank $(r_1, r_2, r_3)$, a variance term that characterizes the estimation error if the true signal itself is indeed of Tucker-rank $(r_1, r_2, r_3)$. \n\nAs a byproduct, the paper provides a similar (and even simpler) non-asymptotic bias-variance decomposition for the standard truncated SVD estimator for matrices."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper's main result (Theorem 1, the bias-variance tradeoff error bound) is novel and interesting, as it extends the exact low-rank setting, which is common in existing literature, to the approximate low-rank setting. This is a natural generalization and one would expect both the variance and bias terms in the error bound to be optimal up to a constant. Further, this error bound holds uniformly for all low Tucker-rank tensors, making it adaptive to all target Tucker ranks. The paper is also clearly written."}, "weaknesses": {"value": "(i) Given the upper bound on the variance term in existing literature, it seems to me that the main contributions of this paper is a bit incremental, especially that the proof techniques are also similar.\n(ii) Theorem 1 has an assumption on the singular value gap of the true signal $X^*$. Is this generally satisfied in practice? It would be better to give some examples for which this assumption holds."}, "questions": {"value": "Is it possible to compute the bias term $\\xi (r_1, r_2, r_3)$ for some examples? Again, this would be helpful in understanding how the approximate low-rank case compares to the exact low-rank case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQGrARndBm", "forum": "lxqTU8Ofb2", "replyto": "lxqTU8Ofb2", "signatures": ["ICLR.cc/2026/Conference/Submission7031/Reviewer_6hRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7031/Reviewer_6hRm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056259984, "cdate": 1762056259984, "tmdate": 1762919231682, "mdate": 1762919231682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "We sincerely thank all four reviewers for their time and thoughtful feedback. We also appreciate their recognition of the strength and generality of our theoretical results, as well as their comments on the accessibility and applicability of our analysis.\n\nWe would like to point out that our manuscript provides a framework to handle ground-truth tensors of arbitrary ranks, and our analysis achieves a clean, rank-adaptive bias–variance tradeoff. To our knowledge, these results have not been well-explored in the existing literature. \nIn addition, we have made the following main revisions to the manuscript:\n- Following Reviewer eK8H's suggestion, we added matching lower bound results as Theorem 2 and Theorem 7 with their proofs in Section B of the appendix for both the tensor and matrix settings, which establish that our results are minimax optimal.\n- We added the additional references suggested by the Reviewers GWqa and 28be.\n- We have revised the tensor simulation setting to employ a full-rank model and have replaced the diagonal core tensor with a full core tensor, as suggested by Reviewer 28be.\n- Following the reviewers’ feedback, we have refined our real-data experiments on MRI volumes. In the revised manuscript, we now analyze five volumes and place particular emphasis on intermediate and higher ranks under large noise levels (large $\\lambda$). In addition, we provide a quantitative summary of these results in Table 1.\n- To address Reviewer 6hRm's concern, we validated the singular-gap assumption on our real-data examples; this is now documented in Section A of the appendix. This provides concrete evidence that singular-gap assumption is not merely an idealized technical assumption of the analysis, but is indeed observed in practice.\n- For the synthetic tensor setup, we selected a representative example to empirically quantify the individual contributions of bias and variance. The new Figure 3 in Section A of the appendix illustrates this behavior.\n- We corrected several typographical and presentation errors pointed out by the reviewers.\n\nAll changes in the revised manuscript are highlighted in red."}}, "id": "GrvnIoWnQz", "forum": "lxqTU8Ofb2", "replyto": "lxqTU8Ofb2", "signatures": ["ICLR.cc/2026/Conference/Submission7031/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7031/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission7031/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763630243862, "cdate": 1763630243862, "tmdate": 1763630243862, "mdate": 1763630243862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}