{"id": "ZNDLv4qwqA", "number": 12991, "cdate": 1758212516471, "mdate": 1759897472173, "content": {"title": "CodeRule-RL: Standard-Guided RL with Per-Rule Reward Scheduling for Code LLMs", "abstract": "Large language models for code often pass unit tests yet remain brittle in practice. They may overfit to a test suite, rely on undefined semantics, or fail under small perturbations. We use the coding standard as training guidance and keep unit tests outside the training loop. Each rule provides a machine-checkable outcome that we convert into per-rule reward components with a simple frequency-aware schedule. The only optimization target is higher pass@1 (single attempt functional success).\nWe present CodeRule-RL, a reinforcement learning approach that optimizes pass@1 as the sole objective and uses coding standard feedback only as auxiliary guidance. Rule outcomes are converted into per-rule reward components, and a simple frequency aware curriculum prioritizes rules that are violated most often and reduces their weight as compliance improves. The model, optimizer, data, and prompts remain fixed. Training adjusts only reward weights. Unit tests may appear in prompts to express specifications, but they are not executed during training.\nOn the public CodeContests+ C subset, CodeRule-RL attains higher pass@1 while reducing training wall clock time by more than one order of magnitude compared with RL that executes tests during training. Across 1.5B–7B backbones, it consistently improves functional success, delivering a relative pass@1 gain of 87\\%.", "tldr": "We optimize pass@1 as the sole objective by turning coding-standard diagnostics into per-rule rewards (no tests, no preference pairs), using a frequency aware schedule that yields strong pass@1 gains on a frozen CodeContests+ subset.", "keywords": ["Code LLM", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6b1704e70994371a14a7f5eaaa47ee99662ec76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a reinforcement learning framework that enhances code generation by utilising coding standards as structured, machine-checkable guidance, rather than relying on costly unit-test execution during training. The method converts each coding-rule violation into a separate, bounded reward component and introduces a frequency-aware curriculum that dynamically prioritizes frequently violated rules, gradually reducing their influence as compliance improves. Using GRPO for policy optimization, CodeRule-RL focuses solely on maximizing pass@1 at evaluation time. Experiences show improvement in pass@1 with faster training compared to text-executing RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Fine-grained reward design: Converts heterogeneous rule violations into per-rule reward components, allowing more interpretable and stable credit assignment than scalar execution rewards.\n- Frequency-aware scheduling: A Curriculum mechanism that adaptively emphasises frequently violated rules, reducing interference and improving sample efficiency.\n- Efficiency gains: Removes test execution from the RL loop, achieving over 10× faster training while maintaining or improving performance."}, "weaknesses": {"value": "- Lack of related works and baselines: There are few related works reported in the paper on rule-guided code generation. The following papers seem to relate:\n    - Dolcetti, Greta, et al. \"Helping LLMs improve code generation using feedback from testing and static analysis.\" arXiv preprint arXiv:2412.14841 (2024).\n    - Agrawal, Lakshya A., et al. \"Monitor-guided decoding of code lms with static analysis of repository context.\" Advances in Neural Information Processing Systems 36 (2023): 32270-32298.\n    - Yao, Feng, et al. \"Training Language Models to Generate Quality Code with Program Analysis Feedback.\" arXiv preprint arXiv:2505.22704 (2025).\n- Limited scope of evaluation: Experiments focus mainly on C language and MISRA C:2012; unclear generality to other languages or coding standards.\n- Functional correctness dependence: While tests are excluded from training, pass@1 remains the only optimization target—may overlook broader program semantics or long-horizon correctness. How do other metrics change as the training goes on?\n- No ablation on rule-set granularity: The paper doesn’t quantify how different rule subsets (Mandatory vs. Advisory) or rule complexity affect learning dynamics.\n- Lack of benchmarks: The paper only evaluated on CodeContests+. Evaluation on other benchmarks is missing.\n- Writing format: Some equation numbers are missing."}, "questions": {"value": "- How is your framework compared with other RL-based frameworks with rule-guided reward?\n- How does your framework generalize to other languages or coding standards?\n- How do other coding metrics (eg. number of passing tests, static-rule-violation count, code robustness, recall@k) change as the training goes on?\n- How does your framework perform on different rule subsets?\n- How does your framework perform on different benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdXHhw1SsB", "forum": "ZNDLv4qwqA", "replyto": "ZNDLv4qwqA", "signatures": ["ICLR.cc/2026/Conference/Submission12991/Reviewer_DEr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12991/Reviewer_DEr9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868570913, "cdate": 1761868570913, "tmdate": 1762923741173, "mdate": 1762923741173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CodeRule-RL, a reinforcement-learning scheme for code LLMs that never executes unit tests during training. Instead, it treats a coding standard (MISRA C:2012) as a source of per-rule, machine-checkable signals and builds a frequency-aware curriculum that adjusts only the weights of rules over time while keeping data, prompts, and optimizer fixed. Experiments on a C subset of CodeContests+ show sizeable pass@1 gains across Qwen2.5-Coder and DeepSeek-Coder backbones and substantial efficiency gains versus an execution‑based RL baseline. The paper also reports a VFR via Infer and finds little degradation on HumanEval/MBPP."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Compelling efficiency: Training is roughly 13× faster and 9× less latency-heavy than execution-based RL. This could make RL for code far more practical in large-scale systems.\n2. Simple&clean design: The per-rule reward shaping and the Top-K EMA schedule are straightforward yet effective. The idea that the curriculum exists within the reward function is clean and easy to replicate.\n3. Performance gains: consistent pass@1 improvements across model families/sizes"}, "weaknesses": {"value": "1. Objective framing is overstated: Saying the model “optimizes pass@1 as the sole objective” is inaccurate and potentially misleading. The method optimizes a proxy reward based on static rule outcomes; pass@1 is simply how performance is measured. The authors should clarify this distinction and, ideally, provide a correlation analysis to demonstrate alignment between the two.\n2. Single-language scope: The entire study is in C / MISRA C:2012. The claim that the approach is “standard-agnostic” would be much stronger with even one more domain. For example, Python with PEP 8 or JavaScript with ESLint.\n3. Limited hyperparameter analysis: Many knobs, like clip bounds, EMA rates, gating thresholds, could affect outcomes, but only warm-up length is explored. More systematic sensitivity tests would increase confidence.\n4. Possible reward-hacking behavior: Since rewards are static-rule-based, models might learn superficial tricks (like adding redundant casts) to satisfy the rules without improving semantics. The paper briefly mentions this risk but doesn’t examine it empirically."}, "questions": {"value": "1. Can you clarify the Table 1 vs. Table 2 discrepancy for Qwen-3B base results? Are these different prompt settings or evaluation slices?\n2. Have you computed a correlation between rule reward and pass@1 improvement across training steps?\n3. Do you have any cross-language or cross-standard experiments (e.g., PEP 8, ESLint)? Even small-scale results would strengthen the “standard-agnostic” claim.\n4. How sensitive are results to reward-clipping bounds and schedule hyperparameters ($\\tau, W, \\lambda, \\epsilon_p$)?\n5. When VFR decreases (e.g., for smaller models), which rule categories worsen?\n6. Could you include a fixed-weights per-rule shaping ablation to separate the effects of per-rule decomposition from scheduling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8H5Tdfh06T", "forum": "ZNDLv4qwqA", "replyto": "ZNDLv4qwqA", "signatures": ["ICLR.cc/2026/Conference/Submission12991/Reviewer_PsCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12991/Reviewer_PsCi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978452112, "cdate": 1761978452112, "tmdate": 1762923740839, "mdate": 1762923740839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CodeRule-RL, a reinforcement learning framework designed to improve the functional correctness (pass@1) of code-generating large language models. The core idea is to use feedback from a static code analyzer, specifically violations of a coding standard like MISRA C:2012, as the reward signal during RL training. This approach deliberately avoids executing unit tests in the training loop, leading to gains in training efficiency. The authors demonstrate that across various model sizes (1.3B to 7B), their method consistently improves pass@1 on the CodeContests+ benchmark while reducing training time compared to RL methods that rely on unit test execution."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "*  The method is evaluated across two different model families (Qwen and DeepSeek) and multiple model sizes, showing consistent improvements in `pass@1` (Table 1). This demonstrates the general applicability of the approach beyond a single model architecture. The training appears stable.\n*  Figure 2 provides a helpful overview of the system architecture."}, "weaknesses": {"value": "*   **Misleading Phrasing of the Optimization Objective:** The paper repeatedly states that it \"optimizes `pass@1` as the sole objective\" (e.g., Abstract, Lines 9-10). This is misleading. The actual reward signal being maximized during RL training is a function of coding standard violations, not the `pass@1` metric. The underlying hypothesis is that maximizing this proxy reward will *indirectly* lead to better `pass@1`, which is the *evaluation metric*. The current phrasing conflates the training objective with the evaluation goal and should be clarified for accuracy. \n*   **Insufficient Ablation to Justify the Reward Signal's Superiority:** The central claim is that coding standards provide effective *guidance* for functional correctness. However, the experiments in Table 2 do not fully support this. The comparison is between CodeRule-RL and CURE (a unit test-based RL method). While this shows CodeRule-RL is much more *efficient*, it does not prove that the rule-based signal is a better or even comparable *guide* for `pass@1`. The comparison conflates the reward source (rules vs. tests) with different RL implementations. A more convincing experiment would be to add a baseline within the authors' own framework: **`CodeRule-RL + unit tests`**, where the GRPO algorithm is used but the reward is derived from both executing unit tests and the rules. If the current `CodeRule-RL` (using only rule feedback) outperforms this new baseline, it would strongly support the claim that rule-based feedback is a superior training signal. Without this, one might conclude that the method is simply a faster, but potentially less effective, proxy for the true signal of functional correctness.\n*   **Limited Discussion on Novelty** The idea of using automated feedback from tools like compilers or linters as a reward signal for RL has been explored in prior work. For instance, Dou et al. (2024, \"StepCoder\") explicitly use RL with feedback from compiler errors as part of their reward function. While the authors' approach of using *only* per-rule static analysis feedback, the novelty seems to be very incremental."}, "questions": {"value": "The reported training efficiency of 1.6 hours is a major claimed advantage. However, the hyperparameters section reveals this is for only 80 optimization steps, which is an unusually short duration for a GRPO-based fine-tuning experiment. This raises questions about the scale of the experiment and the robustness of the findings.  Furthermore, could you provide more details on the size of the training dataset (e.g., number of unique prompts) to help contextualize whether the observed gains are the result of a comprehensive training process or short-term tuning on a limited set of problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yW4wnLLYEv", "forum": "ZNDLv4qwqA", "replyto": "ZNDLv4qwqA", "signatures": ["ICLR.cc/2026/Conference/Submission12991/Reviewer_x6gb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12991/Reviewer_x6gb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994119658, "cdate": 1761994119658, "tmdate": 1762923740365, "mdate": 1762923740365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CodeRule-RL proposes standard guided reinforcement learning for code models where pass@1 is the only optimization target and MISRA C:2012 rule outcomes provide auxiliary, per rule reward signals. The paper defines a spec to reward mapping that converts static analyzer findings into bounded per rule penalties, aggregates them with a clipped reward, and schedules rule weights by a simple frequency driven curriculum that focuses on the most violated rules and reduces their weight as violation rates drop. Unit tests may appear as text in prompts but are not executed during training. Experiments on a frozen CodeContests+ C subset and several backbones show higher pass@1 and much lower reward latency and wall clock time than RL that executes tests during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is explicit and testable. The paper defines the per rule signals and the squashing map, the exponential penalty, and the clipped aggregate reward with clear bounds and rationale, which makes reuse straightforward. The curriculum definition uses an EMA of rule frequencies, a Top K active frontier, warmup and cool down, and a weight mask, again with equations and typical values, which supports replication. The training objective and optimizer are standard and fully specified. The evaluation is careful about decontamination, seed reporting, decoding settings, and toolchains. Gains in pass@1 are consistent across two model families and multiple sizes, with a marked reduction in training wall clock time and reward latency relative to execution based RL. The qualitative example illustrates that the policy learns rule aligned edits while preserving task logic, matching the aggregate improvements."}, "weaknesses": {"value": "Scope is limited to single translation unit C and MISRA C:2012 with one static analyzer. The paper claims a standard agnostic design yet does not test a second analyzer or a different rule family. The curriculum has several hyperparameters $ (\\lambda, \\tau, W, T_{\\mathrm{warm}}, T_{\\mathrm{cool}}, K(0)) $. Defaults are given, but sensitivity analysis is limited, so brittleness under other data or analyzers is unclear. Gains in $ \\mathrm{pass@1} $ without running tests may depend on correlation between static compliance and runtime success, and the paper notes association rather than causation. The VFR study with Infer is useful and suggests that smaller models may trade security for functionality after training; checking false positives and false negatives across analyzers would help. The appendix reports general coding benchmarks with little or no drop, yet results are brief and deserve clearer placement in the main text. Priority masking and gating are described, yet the ablation contrasts the curriculum against all rules without isolating the effect of each gate or the chosen thresholds."}, "questions": {"value": "Can you isolate the effect of priority masking and within sample precedence by disabling each gate in turn or varying the gate threshold, and add sensitivity curves for $ \\lambda $, $ \\tau $, $ W $, $ T_{warm} $, $ T_{cool} $, and the Top $ K $ schedule to show how $ pass@1 $ changes\n\nCan you quantify the link between static compliance and runtime success by executing held-out tests after training under the same prompt setting, reporting the correlation between violation rate and $ pass@1 $ across bins, and giving examples where compliance rises while $ pass@1 $ drops\n\nCan you add statistical support for Table 1 and the CURE comparison by reporting confidence intervals or a randomization test, listing per-task variance, and extend compiler checks with extra flags and sanitizers beyond GCC 13 and Clang 17 to show stability across toolchains"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GClUqQ1S0B", "forum": "ZNDLv4qwqA", "replyto": "ZNDLv4qwqA", "signatures": ["ICLR.cc/2026/Conference/Submission12991/Reviewer_FFS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12991/Reviewer_FFS7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997849635, "cdate": 1761997849635, "tmdate": 1762923739798, "mdate": 1762923739798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}