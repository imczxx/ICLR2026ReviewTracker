{"id": "URpbmVEsqB", "number": 1439, "cdate": 1756882463644, "mdate": 1759898208769, "content": {"title": "LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding", "abstract": "Despite achieving significant progress in 2D image understanding, large multimodal models (LMMs) struggle in the physical world due to the lack of spatial representation. Typically, existing 3D LMMs mainly embed 3D positions as fixed spatial prompts within visual features to represent the scene. However, these methods are limited to understanding the static background and fail to capture temporally varying dynamic objects. In this paper, we propose LLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual representation in 4D scene understanding. The spatiotemporal prompt is generated by encoding 3D position and 1D time into a dynamic-aware 4D coordinate embedding. Moreover, we demonstrate that spatial and temporal components disentangled from visual features are more effective in distinguishing the background from objects. This motivates embedding the 4D spatiotemporal prompt into these features to enhance the dynamic scene representation. By aligning visual spatiotemporal embeddings with language embeddings, LMMs gain the ability to understand both spatial and temporal characteristics of static background and dynamic objects in the physical world. Additionally, we construct a 4D vision-language dataset with spatiotemporal coordinate annotations for instruction fine-tuning LMMs. Extensive experiments have been conducted to demonstrate the superiority of our method on various tasks of 4D scene understanding. Our code will be open-sourced on paper acceptance.", "tldr": "We propose a general vision-language large multimodal model for 4D scene understanding.", "keywords": ["4D scene understanding", "large multimodal model", "spatiotemporal prompt", "multimodal learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68b37f49089168ed9799163a1386f5eba5ed6231.pdf", "supplementary_material": "/attachment/45ad4cffc5c5822b0d93c1014eb4d81d17ea79fa.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors introduce LLaVA-4D, an advanced vision-language large model designed specifically for comprehensive 4D scene understanding. By leveraging the robust LLaVA multimodal framework, the authors’ primary innovation involves the incorporation of a novel spatiotemporal prompt. This innovation blends static 3D spatial perception with dynamic temporal dimension information, enhancing the model's capability to grasp scene dynamics. Specifically, LLaVA-4D encodes both 3D spatial coordinates and time into cohesive 4D coordinate embeddings, subsequently embedding these into visual features that have been temporally and spatially decoupled. This integration aligns these refined visual features with corresponding language embeddings, significantly augmenting the model's comprehension of dynamic scenes. Furthermore, to facilitate training and evaluation, the authors developed Chat4D, a comprehensive 4D vision-language dataset featuring detailed spatiotemporal coordinate annotations. Experimental findings underscore LLaVA-4D's superiority over existing 3D language-multimodal models (LMMs) in tackling 3D tasks and its clear leadership in 4D tasks, marking a significant advancement in 4D scene understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe LLaVA-4D model is a groundbreaking innovation, endowing large language models with 4D understanding, transcending previous 3D-focused limitations.\n2.\tThe authors have a clear reason for decoupling spatiotemporal visual features, and ablation experiments also prove that this decoupling module can make the representation of spatiotemporal features stronger.\n3.\tThe Chat4D dataset, developed by the authors, addresses a critical void in 4D scene understanding for multimodal large language models. This contribution promises to enhance future research endeavors positively."}, "weaknesses": {"value": "1.\tThe paper lacks clarity on LLaVA-4D's capabilities, failing to specify its maximum video frame rate and duration. Additionally, it omits critical details about the 4D data in the Chat4D dataset, such as the average video length. \n2.\tWhen comparing with state-of-the-art models, it seems that other models haven’t been trained or fine-tuned on Chat4D. (As shown in Table 1, they don’t even have the ability to output information related to the temporal dimension.) This makes it hard to directly show that LLaVA-4D is better than other models at understanding the temporal dimension.\n3.\tWhen testing the model’s ability to understand the temporal dimension, the study only uses TAcc as the evaluation metric. We need more evaluation methods—like action interval prediction and the corresponding measurement metrics, for example."}, "questions": {"value": "1.\tThe 4D benchmark in Chat4D is mainly based on 4D dynamic scene reconstruction datasets. The authors used GPT-4V and text-only GPT to generate a lot of global 4D descriptions. During this process, how did the authors assess and fix the hallucination problem of large models?\n2.\tWill the code and dataset be made open-source later?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DQOdiKMpPG", "forum": "URpbmVEsqB", "replyto": "URpbmVEsqB", "signatures": ["ICLR.cc/2026/Conference/Submission1439/Reviewer_NHKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1439/Reviewer_NHKE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923193852, "cdate": 1761923193852, "tmdate": 1762915770074, "mdate": 1762915770074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel vision-language multimodal model that supports dynamic scenes. By embedding 4D spatiotemporal cues, LMMs can simultaneously understand the spatial features of static backgrounds and the temporal features of dynamic objects. A 4D spatiotemporal cue fused with optical flow is designed, and a 4D vision-language dataset containing 879.1K samples is constructed. Based on this dataset, a three-stage training pipeline is designed to enhance the model's understanding capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset makes a significant contribution, and the method can significantly improve the ability to understand 4D scenes.\n\n- The writting is clear and motivation is easy to understand.\n\n- The three-stage training avoids the convergence difficulties caused by directly training 4D features, enabling the model to smoothly transition from basic 2D/3D capabilities to 4D capabilities."}, "weaknesses": {"value": "- Data quality: 4D data relies on GPT-4V to extract spatiotemporal information and GPT to generate instructions, which may contain annotation errors (such as coordinate deviation and timestamp misalignment), affecting the model fine-tuning effect. However, the paper does not evaluate the impact of annotation errors on performance.\n\n- Temporal coding: it relies solely on optical flow to estimate motion information. However, optical flow is prone to inaccurate estimation in fast-moving or occluded scenes, which may lead to deviations in the temporal characteristics of dynamic objects and affect the accuracy of the model's trajectory prediction for high-speed moving objects.\n\n- Lack of experiments: the contribution in TAcc of optical flow and the kind of temporal coding like frame rate-based vs. motion speed-based are unclear."}, "questions": {"value": "- What is the inital of w_p and beta? \n\n- The paper uses CLIP-ViT-L-336px as the visual encoder, but why not use a more suitable encoder for video (such as the Video Swin Transformer) was not chosen?\n\n- What are the advantages of LMMs frameworks over pure vision models in 4D tasks, and how do they compare to 4D Gaussians and VG4D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "suqeKFTFvO", "forum": "URpbmVEsqB", "replyto": "URpbmVEsqB", "signatures": ["ICLR.cc/2026/Conference/Submission1439/Reviewer_n3mW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1439/Reviewer_n3mW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988421797, "cdate": 1761988421797, "tmdate": 1762915769918, "mdate": 1762915769918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLaVA-4D, a framework that enhances MLLM for 4D spatiotemporal scene understanding with a new 4D feature embedding solution. The contribution is a spatiotemporal prompt that encodes 3D position and 1D time into a 4D coordinate embedding to better distinguish static backgrounds from dynamic objects. The model also disentangles visual features from multi-view videos into separate spatial and temporal components, which are then fused with the 4D prompts. Experiments are conducted on several 3D benchmarks and a new Chat4D benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a new 4D spatial-temporal understanding framework for MLLMs. Some new designs for visual encoding and positional embedding are proposed. A new data recipe and a new benchmark for 4D understanding are constructed. \n\n- The new designs looks reasonable overall. Ablation studies show the effectiveness compared to baseline solutions."}, "weaknesses": {"value": "- My main concern is the performance of the proposed method. A quite complex solution is proposed in the paper (with a new visual encoding solution and some new designs for embedding), but the results are not that impressive. Many recent methods, like Spatial MLLM [r1], 3UR-LLM [r2], and Coarse Correspondences [r3], that can achieve better performance on ScanQA are not compared or discussed. \n\n[r1] Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence\n\n[r2] 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understanding, TMM\n\n[r3] COARSE CORRESPONDENCES Boost Spatial-Temporal Reasoning in Multimodal Language Model, CVPR 2025\n\n- How about the performance of LLaVA-4D on some new and more challenging benchmarks like VSI-Bench, which is specifically designed for 3D MLLM?\n\n- For baselines of the new Chat4D benchmark, I would recommend adding results of SoTA proprietary models like GPT5, Gemini-2.5-pro, and open-source generalist MLLM like Qwen3-VL for better references. It would also provide more comparsions and discussions of the new benchmarks compared recent 3D/4D MLLM benchmarks. It would also be helpful to highlight the value and unqiue properties of the new benchmarks."}, "questions": {"value": "Please refer to my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3MSgsTf9a3", "forum": "URpbmVEsqB", "replyto": "URpbmVEsqB", "signatures": ["ICLR.cc/2026/Conference/Submission1439/Reviewer_gcu5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1439/Reviewer_gcu5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994656370, "cdate": 1761994656370, "tmdate": 1762915769720, "mdate": 1762915769720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}