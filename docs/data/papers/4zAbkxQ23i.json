{"id": "4zAbkxQ23i", "number": 13518, "cdate": 1758218847046, "mdate": 1763660206698, "content": {"title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "abstract": "As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application\nin multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 unique Wikipedia documents curated and ranked by human annotators. Through the extensive evaluation on seven multimodal retrievers and fifteen VLMs, RAVENEA reveals some undiscovered findings: (i) In general, cultural grounding annotations can enhance multimodal retrieval and corresponding downstream tasks. (ii) Lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% on cVQA and 6.2% on cIC). (iii) Performance varies widely across countries, with culture-aware retrieval augmented VLMs showing more stable results on Korean and Chinese contexts than in the other countries. These findings highlight the critical limitations of current multimodal retrievers and VLMs, and underscore the need to enhance RAG visual culture understanding. Our RAVENEA can serve as a foundational tool for advancing the study of RAG visual culture understanding of multimodal AI.", "tldr": "We present RAVENEA, a large-scale benchmark with 10K+ human-ranked Wikipedia docs for culture-aware vision-language tasks. We find retrieval boosts lightweight VLMs by 3.2% (cVQA) and 6.2% (cIC), showing the power of cultural augmentation.", "keywords": ["Visual culture understanding", "Cultural benchmark", "Multimodal retrieval-augmented generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c27b4978b73ec71f6156a155a29ee9a5e4724c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RAVENEA, a manually curated benchmark for visual culture understanding in multimodal systems. The benchmark focuses on two key tasks: culture-focused Visual Question Answering (cVQA) and culture-informed Image Captioning (cIC). It contains 1,868 multimodal instances and 18,680 human-ranked Wikipedia passages (10 per instance).\nEach instance is drawn from existing datasets (CVQA and CCUB) and is augmented with culturally relevant Wikipedia passages. Annotators re-rank the top-10 BM25-retrieved documents based on GPT-4o captioning and can also add any missing articles they consider relevant.\nThe authors evaluate seven multimodal retrievers and examine their impact when integrated into 14 widely used Vision-Language Models (VLMs). They also introduce Culture-Aware Contrastive Learning (CaCL), a fine-tuning approach applied to CLIP and SigLIP, to produce culturally sensitive retrievers (CaCLIP and CaSigLIP2), which consistently outperform their frozen counterparts.\nEmpirical results show that retrieval-augmented generation (RAG) provides the most substantial gains for lightweight VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Comprehensive exploratory analysis:\nThe paper conducts an analysis of RAVENEA across tasks, model scales, and cultural dimensions.\n\n2) Human annotation:\nThe dataset construction involves human ranking of retrieved documents, supplemented by quality control and reporting of strong inter-annotator agreement scores.\n\n3) Novel training approach (CaCL):\nThe proposed Culture-Aware Contrastive Learning objective produces retrievers that generalize well across architectures. The released CaCLIP model can become a valuable resource for future multimodal RAG research.\n\n4) Responsible dataset curation:\nThe authors demonstrate attention to ethical concerns by implementing automated face detection and blurring of identifiable individuals in all images."}, "weaknesses": {"value": "1) Dependence on GPT-4o and BM25 in data construction:\nSince image captions are generated by GPT-4o and used as text queries for BM25 retrieval, the initial dataset quality depends heavily on the performance and biases of these models. This reliance may explain why larger VLMs show limited improvements with RAG, as the retrieved contexts may be redundant relative to their internal knowledge.\nFuture iterations could improve recall and diversity by incorporating more diverse sources of models for generation and retrieval, including multimodal retrievers rather than relying solely on text-based BM25 or including visually similar images and multilingual sources.\nRAVENEA’s document set is capped at the top-10 BM25 hits (plus any extras an annotator happens to recall). If a genuinely relevant article never appears in that shortlist, it cannot be selected, so the benchmark measures only how well models re-rank a limited candidate pool rather than how well they retrieve unseen but correct evidence. I would like to see more quantitative evidence on how often this recall limitation occurs to ensure it does not significantly impact the dataset’s completeness or the conclusions drawn from it."}, "questions": {"value": "1) Retrieval quality control:\nWas any evaluation conducted to compare the quality of BM25-retrieved documents against those retrieved by modern text-only or multimodal retrievers? It would be helpful to know whether better initial retrieval could enhance the benchmark’s representativeness.\n\n2) Annotation statistics:\nCould the authors provide summary statistics on the yes/no response distributions for the three cultural relevance annotation questions (country association, topic alignment, visual depiction)? These would clarify dataset balance and inter-question correlations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pFj7daqKrK", "forum": "4zAbkxQ23i", "replyto": "4zAbkxQ23i", "signatures": ["ICLR.cc/2026/Conference/Submission13518/Reviewer_F5ZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13518/Reviewer_F5ZE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709180418, "cdate": 1761709180418, "tmdate": 1762924126669, "mdate": 1762924126669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAVENEA, a new benchmark dataset and evaluation framework designed to assess systematic generalization, reasoning, and visual understanding in AI models. The benchmark is inspired by the RAVEN or RPM (Raven’s Progressive Matrices)-style tasks but extends them by introducing multi-dimensional relational reasoning challenges that involve complex scene configurations, compositional attributes, and logical rule dependencies.\n\nThe authors propose:\n\n- A novel dataset generation pipeline that controls for compositional structure and visual diversity, ensuring that models cannot overfit to superficial cues.\n- Several generalization splits (e.g., attribute, relation, and logic-based splits) to test extrapolative reasoning.\n- A comprehensive evaluation of existing architectures (CNNs, ViTs, relational networks, multimodal transformers, and neuro-symbolic models) on this benchmark, revealing clear performance gaps in out-of-distribution (OOD) reasoning.\n- Discussion and visualization of error patterns, showing that models often rely on shortcut heuristics rather than true reasoning.\n\nThe benchmark is positioned as a rigorous and diagnostic tool for studying systematic generalization and compositional reasoning in visual intelligence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Problem Definition and Task Design\n\nThe paper clearly defines the problem and presents tasks that align well with practical applications. It focuses on the scenario of “visual content + external cultural knowledge”, using two downstream tasks—cVQA (multiple-choice visual question answering) and cIC (image captioning)—to evaluate the full pipeline of retrieving → consuming knowledge → answering/generating.\n\n- Data Scale and Annotation Quality Control\n\nThe dataset covers 8 countries and 11 categories, totaling 1,868 instances.\nEach image is paired with 10 Wikipedia documents, which are manually re-ranked, resulting in 18,680 image–text pairs.\nThe annotation process includes three dimensions of cultural relevance (national association, thematic alignment, and visual presence in the image), inter-annotator agreement checks, and spot audits.\n\n- Methodological Contribution: Culturally Aware Contrastive Learning (CAC)\n\nWithin the CLIP/SigLIP framework, the authors introduce a combination of binary classification loss, ranking-margin loss, and text-side diversity regularization, specifically designed to enhance cultural relevance ranking (CaCLIP / CaSigLIP)."}, "weaknesses": {"value": "- Limited Domain Coverage and Source Bias\n\nAs acknowledged by the authors in the “Limitations” section, the dataset covers only 8 countries and 11 categories, and relies primarily on English Wikipedia articles. This setup is prone to cultural and regional exposure bias, which may affect the generalizability of the claimed cross-cultural conclusions.\n\nFuture work should incorporate GLAM (Galleries, Libraries, Archives, Museums) institutional collections and non-English knowledge sources to achieve a more balanced and representative dataset — a direction the authors themselves briefly mention in the ethics and outlook discussion.\n\n- Small and Inconsistent Human Evaluation for cIC\n\nThe human evaluation for the cIC (cultural image captioning) task is based on only 10 images, 4 annotators, and 420 sentences in total, with a moderate inter-rater agreement of κ ≈ 0.595.\n\nGiven the evaluation spans 14 models and multiple retrieval variants, the sample size is too limited to support robust conclusions.\nThe authors are encouraged to expand the human evaluation sample and include statistical significance testing to strengthen reliability.\n\n- Conservative RAG Configuration Potentially Underestimates Strong Models\n\nDuring evaluation, the retrieval-augmented generation (RAG) setup uses only the Top-1 document truncated to the first 256 tokens.\nThis constrained configuration may explain the phenomenon where large models fail to benefit under RAG.\n\nIt would be beneficial to explore multi-document fusion, longer context windows, re-ranking and rerouting strategies, or tool-augmented reasoning chains, in order to more fairly assess the upper bound of large-model + RAG performance."}, "questions": {"value": "When large models exhibit performance degradation under the RAG setting, have the authors analyzed whether this results from limited retrieval relevance (e.g., using only Top-1 document and 256 tokens) or from suboptimal fusion/integration strategies?\n\nAdditionally, have you experimented with multi-document or paragraph-level retrieval, or with tool-augmented reasoning mechanisms to test if the issue lies in retrieval depth or model utilization?\n\nMoreover, the selection of large models in the current evaluation does not appear sufficiently frontier.\nModels such as Qwen3, Gemini 2.5 Pro, and GPT-5 are among the most representative current vision-language systems, yet their results are not included. Including these stronger baselines would help demonstrate the true upper bound of RAG-based cultural reasoning performance and provide a more complete picture of model scaling trends."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pg2SWLB9ns", "forum": "4zAbkxQ23i", "replyto": "4zAbkxQ23i", "signatures": ["ICLR.cc/2026/Conference/Submission13518/Reviewer_N9q4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13518/Reviewer_N9q4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870064176, "cdate": 1761870064176, "tmdate": 1762924126245, "mdate": 1762924126245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a benchmark designed to evaluate how RAG enhances visual culture understanding in vision-language models (VLMs). The benchmark comprises 1.8K instances spanning eight countries (China, Nigeria, Russia, Spain, Mexico, India, Indonesia, and Korea) across eleven cultural categories, incorporating over 10K human-ranked Wikipedia documents. The authors evaluate seven multimodal retrievers and fourteen VLMs on two primary tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). Key findings include: (1) cultural grounding annotations enhance multimodal retrieval performance, (2) lightweight VLMs benefit substantially from culture-aware retrieval (3.2% improvement on cVQA, 6.2% on cIC), and (3) performance varies significantly across countries, with some models showing cultural biases.​"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Timely Contribution**:\nThe paper addresses the relatively underexplored intersection of multimodal retrieval and cultural understanding. While RAG has been effective in text-based cultural reasoning, its application to vision-language understanding remains sparse. RAVENEA directly addresses this omission and establishes a systematic evaluation framework.\n\n- **Comprehensive Experimental Design**: \nThe evaluation encompasses seven retrievers and fourteen VLMs across multiple model families, providing thorough empirical coverage. The inclusion of both open-source and proprietary models enhances the generalizability of findings.\n\n- **Well-motivated methodology**:\nThe integration of CAC learning effectively extends the CLIP-style alignment paradigm to explicitly encode cultural supervision. The accompanying diversity loss formulation (Eq. 5) is well-motivated and technically sound. The use of RegionScore in response to deficiencies in automatic caption metrics is empirically validated through human correlation studies."}, "weaknesses": {"value": "- **Limited Novelty Beyond Dataset Construction**  \nDespite the benchmark’s quality, the methodological innovation (CAC loss and RegionScore) remains incremental. The CAC objective essentially adapts contrastive alignment to a culturally labeled setup a relatively modest technical contribution. RegionScore, while intuitive, captures surface-level lexical cues (country/demonym mentions) rather than deeper cultural semantics.  \n\n- **Inadequate Relevance Annotation Scheme**:   \nThe paper employs a fundamentally flawed binary annotation approach that fails to capture the graded nature of cultural relevance. By reducing cultural understanding to three binary questions and artificially combining them into a continuous scale, the authors lose crucial ranking information essential for cultural understanding tasks. Established information retrieval research demonstrates that graded relevance judgments provide superior training signals compared to binary labels, particularly for nuanced tasks like cultural understanding. The absence of proper graded annotations or pairwise ranking comparisons undermines the effectiveness of the contrastive learning framework and limits the model's ability to distinguish between documents with varying degrees of cultural relevance.\n\n- **Absence of ablations and bias audits for the retrieval pipeline.**  \nBM25 is a purely lexical retriever and cannot capture the semantic or conceptual associations central to cultural understanding, I do not feel its the optimal choice for getting to Wikipedia documents for further annotation. No experiments contrast retrieval quality using (a) GPT captions vs. human-written captions, or (b) BM25 vs. dense/hybrid retrieval. Without such comparisons, it remains unknown whether the current pipeline systematically overlooks semantically relevant documents or skews toward certain linguistic patterns. Furthermore, the paper does not analyze how often annotators added “missing” documents or how BM25’s coverage varied by language/country. These missing statistics weaken claims of balanced cross-cultural representation.   \n\n- **Potential data leakage due to image-only train/test split.**    \nThe authors report an 85 / 5 / 10 split by images but provide no evidence of a document-level split. If the same Wikipedia document appears as a positive for both training and test images (e.g., “Sushi” page reused across splits), retrieval models can trivially memorize text and inflate performance. The paper does not mention any de-duplication or partitioning of documents, nor does it measure document overlap across splits.  \n\n- **Critical Gaps in Evaluation setup:**  \nThe authors fail to report essential statistics about their evaluation setup, specifically the positive vs. negative document ratios within their 10-document sets per instance. Without knowing whether 8/10 documents are relevant or 2/10 are relevant, it is impossible to assess task difficulty or whether reported improvements represent meaningful advances or artifacts of trivial evaluation scenarios. The paper shows they tested against random characters and unrelated documents as controls, but:​ No analysis of hard negative examples (culturally similar but incorrect documents).  \n\n- **Insufficient Analysis of Cultural Bias and Representation:**   \nWhile identifying performance disparities across countries (e.g., poor performance on Nigerian contexts vs. better performance on Korean contexts), the paper provides no deeper investigation into the underlying causes of these systematic biases."}, "questions": {"value": "- What criteria were used to select 8 countries from the original 30 in CVQA? \n\n- You observe systematic underperformance on Nigerian and Indian contexts—what analysis was conducted to determine whether this stems from (a) annotation bias, (b) Wikipedia coverage gaps, or (c) model training data biases?\n\n- RegionScore measures surface-level geographic mentions rather than deep cultural understanding. How do you validate that explicit country/demonym references correlate with actual cultural comprehension rather than superficial pattern matching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IX2MG1byNY", "forum": "4zAbkxQ23i", "replyto": "4zAbkxQ23i", "signatures": ["ICLR.cc/2026/Conference/Submission13518/Reviewer_nXd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13518/Reviewer_nXd8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982888681, "cdate": 1761982888681, "tmdate": 1762924125951, "mdate": 1762924125951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RAVENEA, a retrieval-augmented, multimodal benchmark for cultural understanding across eight countries and eleven categories, built by pairing images from CVQA and CCUB with human-ranked Wikipedia passages and evaluating two tasks: culture-focused VQA (cVQA) and culture-informed captioning (cIC). The authors also propose RegionScore, a simple region-mention metric for cIC that correlates with human judgments better than standard captioning metrics, and Culture-Aware Contrastive (CAC) fine-tuning to adapt multimodal retrievers (producing CaCLIP/CaSigLIP) that notably improve retrieval and downstream performance, especially for smaller VLMs. This analysis highlights persistent cross-cultural gaps though there are diminishing returns for large models with RAG."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Well-founded benchmark design with human relevance supervision. The pipeline uses GPT-4o captions → BM25 over ~6M English Wikipedia pages → human re-ranking with a clear, 3-question taxonomy (country association, topic alignment, explicit visual representation). The methodology of creation is also well outlined and easy to understand, with good documentation.\n\n- Transparent quality control and strong agreement. There is multiple evidence that supervision is reliable and that the QA process was conducted at an acceptable standard, ensuring the reliability of the proposed benchmark and methodology. Caption checks show 92%/94% accuracies and agreement = 0.85. Moreover, the meta-check on annotations reports 98.2% acceptance and agreement = 0.83 and this prolongs the trend of faithful agreement between the data collection methodology line in line with how humans should have done.\n\n- Substantive retriever improvements. The proposed CAC loss function works well in improving the performance of the new retriever when trained with the RAVENEA benchmark, signaling that the proposed auxiliary loss matters in fine-tuning the learning process for this specific use case. This method can potentially be extended into other tasks since the three losses are generic and widely used heuristics (contrastive, ranking, and intra-modal diversity regularization).\n\n- Insightful findings on scale vs. retrieval and culture-specific gaps. RAG helps small VLMs substantially but shows diminishing or negative returns for the largest family members. The cross-country analysis shows persistent disparities, with weaker performance on specific regional subsets."}, "weaknesses": {"value": "- Generalizability of the method. The paper emphasizes CAC and CaCLIP, which are trained and evaluated on RAVENEA. Without testing on external cultural datasets, it is difficult to claim general culture-aware retrieval superiority beyond “RAVENEA-specific specialization.” Evaluation on other datasets capturing similar concepts (e.g., CVQA, CCUB, CultureVLM, WorldCuisines, ALMBench) would strengthen the claim.\n\n- The proposed RegionScore still measures lexical proxy rather than cultural semantic significance. By design, RegionScore is 1 if the caption contains the country or demonym, and 0 otherwise. This means it can reward trivial country mentions and miss high-quality culturally specific text without a country token. It limits what conclusions the cIC task can carry, and can even be harmful if a caption hallucinates an irrelevant country reference while describing a different cultural element.\n--  The W/O RAG caption is: “Three individuals wearing vibrant traditional attire perform a dance outdoors.” This is generic. RegionScore = 0.\n-- The With CaCLIP caption is: “A group of people wearing colorful traditional Igbo attire perform the Egedege dance at an event.” This is an excellent, culturally specific caption that captures deep nuance. However, it does not contain the word “Nigeria” or “Nigerian.” RegionScore = 0.\n-- A hypothetical poor caption such as “This is a photo from Nigeria” would receive a higher RegionScore due to the occurrence of Nigeria. The metric is rewarding geotagging, not the cultural description it claims to measure. This is a conceptual flaw that undermines the cIC task, which is defined as assessing “captions that are sensitive to and incorporate cultural nuances.”"}, "questions": {"value": "Minor stuff:\n\n- The paper mentions that the benchmark was derived from an initial pool of 30 countries but does not explain the rationale for narrowing down to eight. Clarifying this selection process would improve transparency and help readers assess dataset representativeness.\nLine 126: “capabilities of of VLMs” → “of VLMs.”\n\n- Several “under­standing / culturE uNdErstAnding” line breaks; clean hyphenation in camera-ready.\n\n- Line 469: “incoming level (Nwatu et al., 2025)” likely should be “income level.”\n\n- The introduction and Table 6 alternately say “over 10,000 documents” vs. “11,580 documents / 18,680 pairs.” Clarify whether “10k” is rounded or represents unique documents.\n\n- Appendix A (LLM usage) and Section 2.2: consider reiterating that GPT-4o captions were not included in the released gold text; they were only used for retrieval augmentation. This is stated, but repeating it in the main text would prevent confusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0gGIA1qBAD", "forum": "4zAbkxQ23i", "replyto": "4zAbkxQ23i", "signatures": ["ICLR.cc/2026/Conference/Submission13518/Reviewer_cUbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13518/Reviewer_cUbB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995503211, "cdate": 1761995503211, "tmdate": 1762924125185, "mdate": 1762924125185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}