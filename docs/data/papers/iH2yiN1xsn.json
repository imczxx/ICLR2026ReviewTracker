{"id": "iH2yiN1xsn", "number": 20530, "cdate": 1758307123313, "mdate": 1759896972940, "content": {"title": "Think Twice: Branch-and-Rethink Reasoning Reward Model", "abstract": "Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-once scoring into focused, second-look reasoning, BR-RM reduces judgment diffusion and improves sensitivity to subtle yet consequential errors while remaining practical and scalable.  Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains.", "tldr": "We introduce branch-and-rethink, a two-turn reward model that applies the think-twice principle to reduce judgment diffusion by branching on critical dimensions and then rethinking them for deeper, more accurate evaluation.", "keywords": ["reward model", "reasoning language model", "thinking model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb0bfda223bb3fc3c449b75d64c5cd354d29443c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes BR-RM, a two turn reward model that first performs adaptive branching to select a small set of instance critical evaluation dimensions and sketch hypotheses, then performs a branch conditioned rethinking pass to verify those hypotheses and decide a preference. Training uses GRPO with a strict format check and a binary outcome reward. On RewardBench, RM Bench, and RMB, BR-RM reports strong average accuracy, with particularly large gains on RM Bench, and presents ablations on turning off the second pass, removing branching, and changing reward design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*  The paper identifies judgment diffusion in reward models and motivates a focused second pass that aims to allocate test time compute where risk is highest. The concept and naming are crisp and intuitive. \n* The strict formatting penalty plus binary outcome reward is easy to implement and aligns with the evaluation objective. The paper also shows why finer grained scoring or extra branch rewards underperform. \n* BR-RM-Qwen-14B achieves 92.1 on RewardBench, 85.9 on RM Bench, and 74.7 on RMB, producing the best average among compared methods. The 8B model is competitive as well. \n* Removing the second pass or the adaptive focus yields consistent drops, supporting the core design claim that thinking twice with focus matters.  \n*  The training data ablation clarifies contributions from HelpSteer, safety data, math, and code preferences."}, "weaknesses": {"value": "* The paper highlights best averages, but several baselines appear very recent and some cells are missing. It would help to provide complete, reproducible comparison tables and lock evaluations with identical prompting and sampling across all methods. The current Table 1 summary is helpful but not fully auditable from the text alone. \n* The format penalty is large in magnitude, and the same terminal reward is assigned to both turns uniformly across tokens. This could incentivize shortest valid traces rather than best targeted analysis. A token or section level credit assignment analysis would strengthen the case.  \n* The nine dimension space is only sketched. The paper would benefit from concrete definitions, coverage analysis, and failure cases where the right dimension is off the list. \n*  The method doubles passes by design. Training steps, batch sizes, number of traces per item, and inference budgets are not fully spelled out for a fair cost adjusted comparison versus strong one turn GenRMs or scalar RMs. \n* The reward design is explicitly matched to binary preference accuracy. Generalization to settings that require calibrated magnitudes or multi way choices is not evaluated, and the “scoring on a scale” negative result is explained post hoc. A held out task with different decision granularity would build confidence. \n* The training data ablation shows safety data matters, but the paper does not include targeted red teaming or bias analysis of the judge decisions under adversarial style."}, "questions": {"value": "1.  How many branches are typically chosen per item. What is the token budget split between turn one and turn two. Please report distributions and correlate them with accuracy. \n2.  You assign the terminal reward uniformly to all tokens of both turns. Did you try turn specific weights or variance reduction by per section advantages. Any signs of mode collapse to minimal valid traces. \n3.  What exactly are the nine dimensions and how were they defined. Do any items leak benchmark rubric wording into the branch names. Evidence that the model can discover off rubric issues would be valuable. \n4.  Please provide per example wall clock for training and inference, and normalized accuracy per thousand generated tokens. This is key for practical deployment against larger scalar or generative baselines. \n5. Can BR-RM be extended to listwise judging or magnitude scoring without losing the benefits of the two turn design. The scale based attempt failed for alignment reasons, but could a pairwise consistent magnitude be learned."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vb9AlOCLPI", "forum": "iH2yiN1xsn", "replyto": "iH2yiN1xsn", "signatures": ["ICLR.cc/2026/Conference/Submission20530/Reviewer_4rrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20530/Reviewer_4rrV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714837064, "cdate": 1761714837064, "tmdate": 1762933951643, "mdate": 1762933951643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper implements a multi-turn mechanism for reasoning RMs. By forcing decouple of rubrics selection and rethinking in two rounds, the proposed RM exibits strong performance on various reward model benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation of focus dilution and shallow analysis are sound.\n2. The benchmark performances are strong."}, "weaknesses": {"value": "1. Lack of insights. The paper lacks in-depth analysis and the ablations are not informative (only benchmark scores).\n2. Too much inductive bias. Many important design choices are manually picked without much validation."}, "questions": {"value": "1. Weakness 1. Why do focus dilution or shallow analysis happen? Why multi-round query could mitigate this? Is the mitigation due to specific prompt design (e.g. round 1 reduce the amount of rubrics to take into account), or due to the forced \"rethinking\" by multi-round query?\n2. Weakness 2.\n- The number of turns. If think in two turns performs better than a single turn, can increasing turns lead to even better performance?\n- The design of subtasks. First turn selects a tiny subset of rubrics to consider, second turn analyze condition on them. An alternative is to 1) generate a weight given problem and rubric name, 2) independently generate a rationale and a score for each rubric, and 3) compute weighted score. The possibilities of designs are endless. Why choose to design this way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FsjfCjpvhi", "forum": "iH2yiN1xsn", "replyto": "iH2yiN1xsn", "signatures": ["ICLR.cc/2026/Conference/Submission20530/Reviewer_f3LS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20530/Reviewer_f3LS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907262391, "cdate": 1761907262391, "tmdate": 1762933951192, "mdate": 1762933951192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper diagnoses \"judgment diffusion\" in existing Reward Models (RMs), where attention is spread thinly across many quality dimensions, leading to shallow analysis. It introduces Branch-and-Rethink (BR-RM), a two-turn framework that first performs Adaptive Branching to select a few instance-critical dimensions and sketch hypotheses, followed by Branch-Conditioned Rethinking for a targeted, second-look analysis based on the initial findings . The model is trained with GRPO-style RL using a simple binary outcome reward and achieves state-of-the-art (SOTA) performance on three challenging RM benchmarks: RewardBench, RM-Bench, and RMB."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core idea is reasonable. Diagnosing \"judgment diffusion\" and transferring the \"think-twice\" principle from solvers (LLMs) to judges (RMs) is a clever and logical contribution.\n\n2. The paper is supported by strong SOTA results across three diverse benchmarks and is validated by exceptionally comprehensive ablation studies that justify each design component.\n\n3. The paper is well-written and clearly structured. The core problem and the proposed solution are easy to understand.\n\n4. This work addresses a critical bottleneck in LLM alignment by creating RMs that are more sensitive to subtle yet consequential errors, which is essential for developing more reliable AI systems."}, "weaknesses": {"value": "1. Cost: The BR-RM is a two-stage generative model. Compared to a scalar RM, which requires a single forward pass, this approach introduces substantial latency and complexity, especially during RLHF training where the RM is called millions of times. The paper doesn't quantify this two-turn cost, making its practical viability for large-scale application questionable.\n\n2.  The method relies heavily on a predefined \"universal set of criteria\" and \"task-specific evaluation hierarchies\". The performance seems contingent on these human-designed sets, making the approach potentially brittle. If a critical evaluation dimension for a new task is missing from the universal set of criteria, will the model be unable to \"branch\" to it and fail?\n\n3. The paper attempts to differentiate BR-RM from existing Reasoning RMs, but the distinction feels minor. The core contribution appears to be a strong engineering improvement and a clever prompting strategy rather than a fundamental conceptual leap over prior works."}, "questions": {"value": "1. See weakness 1\n\n2. See weakness 2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "btV39VUjcg", "forum": "iH2yiN1xsn", "replyto": "iH2yiN1xsn", "signatures": ["ICLR.cc/2026/Conference/Submission20530/Reviewer_7A17"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20530/Reviewer_7A17"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954304424, "cdate": 1761954304424, "tmdate": 1762933950822, "mdate": 1762933950822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}