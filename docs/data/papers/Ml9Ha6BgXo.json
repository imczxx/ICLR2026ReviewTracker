{"id": "Ml9Ha6BgXo", "number": 15010, "cdate": 1758246814512, "mdate": 1759897335782, "content": {"title": "Formalizing Audits of ML Models as a Sequential Decision-Making Problem", "abstract": "Auditing is a governance mechanism for evaluating ML models to identify and mitigate potential risks. This process is critical, as undetected issues in models, such as incorrect predictions or inappropriate feature use, can lead to adverse consequences. In this work, we focus on application audits, which aim to detect errors in domain-specific ML applications. Application audits are important as they assess the risks posed by ML models to guide mitigation. Currently, application audits are predominantly manual, relying on domain experts to identify model errors by inspecting predictions and their explanations, which limits the scalability of audits. To complement human auditors, we explore algorithmic approaches to application auditing and formalize the auditing task as a sequential decision-making problem. We propose SAFAAI, a novel conceptual framework for auditing, inspired by principles of situational awareness, to formally define the objectives of application audit problem. Building on this foundation, we introduce RLAuditor, a reinforcement learning method for automating application audits of ML models. We validate our approach on multiple ML models and datasets, both with and without human auditors, demonstrating its effectiveness in facilitating audits across different contexts. To our knowledge, this work is the first to formalize application audits for ML models as a sequential decision-making problem, informing the design of future automated and human-AI collaborative auditing approaches.", "tldr": "We formalize ML model application audits as a sequential problem and propose a novel solution that automates the audit process, thereby enhancing the scalability of application audits.", "keywords": ["XAI", "ML model audits", "application audits", "model explanations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03717b9d69a1968a96dcbc1e9b5f1774989b045d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to approach ML classifiers audits with reinforcement learning, for discovering zones where they make the most errors.\nIt claims to be the first to formalize audits as a sequential decision-making problem."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* simple application of a RL scheme for finding next possibly error-leading inputs for classifiers\n\nThe topic of auditing machine learning and decision-making systems is important."}, "weaknesses": {"value": "* unclear placement of the proposal concerning recent related works (e.g. active learning applied to auditing [2]), or current trends of audits to watch for bias (besides errors only)\n* the setup if very restrictive, far from a more widely usable black box approach (requires predictions for all classes, features attribution scores, etc)\n* applies rather standard RL without a contribution that would have come from the problem setting; in that light, this is more of an empirical paper only \n* claims the introduction of a 3-part framework (SAFAAI), but only addresses the first two parts\n\nMore details:\n\nI find the paper unclear in its presentation of what it claims (w.r.t. what is really addressed) as compared to the current state of the art of audit techniques this paper wants to overcome with a first formalization.\n\n* a motivation for the current paper is that audits are currently \"manual\". But there are many audit techniques that rely simply on choosing a metric (eg, fairness with disparate impact) and consist in sampling the classifier input space to see if it goes over a predefined threshold for some sub-populations (see eg [1]), I fail to see how this is very manual, yet it serves the purpose described in the introduction of the paper in my opinion.\n\n* it is also unclear to me how the authors place their work with regard to active learning applied to auditing (see eg [2] and the many papers citing it). Here also, the decision for the next sample is taken based on the previous ones. There is a full body of work on that matter, that is ignored from the paper discourse and related work (in particular Section 2, \"automated audits\"). The listing of active learning in the experiments as a baseline arrives too late and is not conclusive in the differentiation.\n\n* the introduction/motivation is very generic. There are now so many papers in the related work, that new papers must in my opinion be more focused. For instance, Figure 2 is a totally generic plot of the evolution of a RL-based scheme: there is no indication or explanation to which data/problem it corresponds to in the paper, and could actually have been taken as in from any introductory book on RL. \n\n* the operational setup for the audits only becomes clearer while reading the technical part; we are far from a widely applicable black or grey-box approach, as the requirements for the MDP proposal to function are high in practice. For instance, while here it is assumed that the auditor has access to feature attribution scores, some related works are proposing ways to infer which features are at use [3]. I get that we are here more in a white box setup, but the disconnection with what the community is assuming these days as a setup to define where audits can go is too important. And must be motivated precisely.\n\n* Fig 4 is totally unreadable, and not explained in the main paper, so I do not get the value of it at this stage\n\n* Section 5, that introduces the notion of collaboration with humans for audits, arrives as a surprise, and was not properly claimed in the contribution. This again blurs the whole message of the paper.\n\nIn general, I think the topic is important, but I have the feeling that authors had as a starter an RL approach to attack this MDP scheme, and then constructed a motivation for the whole paper (cf Fig. 2), but it is to far away from state of the art to be impactful in my opinion, so I would encourage authors to clarify their position and claims.\n\n[1] Cherian, J. J., & Cand√®s, E. J. (2024). Statistical inference for fairness auditing. Journal of machine learning research, 25(149), 1-49.\n\n[2] Yan, T., & Zhang, C. (2022, June). Active fairness auditing. In International Conference on Machine Learning (pp. 24929-24962). PMLR.\n\n[3] Rastegarpanah, B., Gummadi, K., & Crovella, M. (2021). Auditing black-box prediction models for data minimization compliance. Advances in Neural Information Processing Systems, 34, 20621-20632."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nyp9rVj729", "forum": "Ml9Ha6BgXo", "replyto": "Ml9Ha6BgXo", "signatures": ["ICLR.cc/2026/Conference/Submission15010/Reviewer_2LNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15010/Reviewer_2LNa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734029087, "cdate": 1761734029087, "tmdate": 1762925342420, "mdate": 1762925342420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Focusing on application audits, this paper proposed a framework, SAFAAI, which defines the objectives for this problem. And it proposed a novel RL-based auditing method, RLAuditor, to automate application audits."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The RL-based auditing method effectively picks up test samples that the target model will misclassify. Its performance surpasses baseline methods on four datasets.\n2. It explores different state designs and provides evidence for the advantage of the selected scheme.\n3. The user study confirms that RLAuditor can help human auditors."}, "weaknesses": {"value": "1. Definition 3.2 of Efficient Application Auditing Problem focuses only on maximizing the number of detected errors but ignores the FPR of detection.\n2. While for images, GradCAM automatically applies to test samples, the explanation for test E-SNL data relies on human annotation. This reliance is not practical, as such human intervention violates the motivation of RLAuditor and compromises its contribution. \n3. In Lines 230-242, the definition of e and $\\psi$ needs to be clarified. I don't understand where they're derived from and how you compute the function.\n4. RLAuditor does not achieve the third-level objective in the SAFAAI framework."}, "questions": {"value": "Can the proposed method scale up to other large-scale image datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kRoB5sySn8", "forum": "Ml9Ha6BgXo", "replyto": "Ml9Ha6BgXo", "signatures": ["ICLR.cc/2026/Conference/Submission15010/Reviewer_etWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15010/Reviewer_etWa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871112894, "cdate": 1761871112894, "tmdate": 1762925341791, "mdate": 1762925341791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of RL to improve \"auditing\", understood here as \"application auditing\", and implemented as finding as finding prediction errors in classifiers."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "-clear writing, easy to follow\n-important topic: auditing ML models is important, especially in contexts where few labels are available to the auditor"}, "weaknesses": {"value": "-unclear model: I do not understand the precise usage scenario envisioned by the authors. \nWhile l.54 it seems that the end-users perspective is taken -- which would be coherent with a rather black box perspective, we read that these end-users are supposed to routinely conduct those audits, submit reports, and have access to a sizable D_train dataset. \nGiven then that the objective (identify misclassification) is pretty far from many traditional black box audit approaches (like assess model fairness), I then tend to believe the considered auditors are rather employees of the model provider. Which leads to the question: why don't they use more \"white box\" approaches that would leverage the internals of the model to better identify model weaknesses ?\n\n-narrow view of auditing that leads to unsupported claims. Auditing is far more than finding model misclassifications. The title \"formalizing audits of models as a sequential decision making..\" implicitly ignores large branches of auditing that already explored active approaches, RL-based approaches. See e.g. \"Basu, Debabrota, and Udvas Das. \"The Fair Game: Auditing & debiasing AI algorithms over time.\" Cambridge Forum on AI: Law and Governance.\" or \"Online Fairness Auditing through Iterative Refinement\" that imho definitely call for nuance in the first paper contribution. \n\nMoreover, if one considers the practical implementation of auditing provided in the experiments (ie find misclassifications), a whole body of research that stems from software testing is ignored. See e.g. \"Abo-eleneen, A., Palliyali, A., & Catal, C. (2023). The role of Reinforcement Learning in software testing. Information and Software Technology, 164, 107325.\". Arguably, assuming model developers reply to the envisioned audit reports by proposing a new model version, the related works scope end up spanning the whole active learning field.  \n\n-Interestingly, in software testing a central notion is \"coverage\": the number of misclassifications is less important than their overall diversity. Here the only focus is \"score\" -- that I interpreted as the number of classification errors found. You mention (eg line 842) you seek a \"representative set\", but my intuition is that given the objective, a RL system say on mnist dataset would directly \"reward-hack\" and only identify 8 misclassified for 0 (leaving all other confusions rather unexplored). Have you assessed the diversity/representativity along the score ? \n\n-In my view, the experimental setting is unfair: RL auditor has already consumed \"D_train\" (=60% of complete test-set) examples. So concretely for example syntab, in figure 5, plots the range [3000,3050] of consumed requests... a fair baseline would have been to just submit these 3k examples and count the number of identified errors (as this constitutes the objective), which would represent according to table 1 (3k*0.14) around 400 errors: 12x more errors without additional complexity.\n\n-I found the datasets choice rather exotic in a domain (classifiers). Why resort to home-brewed synthetic dataset when a plethora of classification data is readily available ?"}, "questions": {"value": "-Cf questions in the weaknesses part above.\n-As I understand, your RL state space is continuous. This somehow deviates from the traditional RL (eg markovian) setting. Can you comment on the consequences of this choice ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qyf8iVc2yH", "forum": "Ml9Ha6BgXo", "replyto": "Ml9Ha6BgXo", "signatures": ["ICLR.cc/2026/Conference/Submission15010/Reviewer_cUsg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15010/Reviewer_cUsg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906487339, "cdate": 1761906487339, "tmdate": 1762925341416, "mdate": 1762925341416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames auditing as an RL task and provides instantiations of various parts of an MDP to enable auditing. The paper conducts empirical experiments and user study to show the effectiveness of their framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The idea of using RL for auditing is very interesting!\n\n(2) The instantiations used in the paper make intuitive sense.\n\n(3) The paper is well-written and thorough, in the sense that there are empirical experiments along with a user-study to show the efficacy."}, "weaknesses": {"value": "(1) Set C is ill-defined. You are trying to construct C' which is similar to C, but what guarantees hold on C itself? I think this is a missing definition that is making the framework a bit on shaky grounds. Eg. C could be a set which consists of all the mispredicted inputs or the entire input space?\n\n(2) why do humans still take 30 mins to complete auditing with your framework? is it the size of the dataset or the annotating process?"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zbN93oyEO8", "forum": "Ml9Ha6BgXo", "replyto": "Ml9Ha6BgXo", "signatures": ["ICLR.cc/2026/Conference/Submission15010/Reviewer_AejE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15010/Reviewer_AejE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762034084093, "cdate": 1762034084093, "tmdate": 1762925340164, "mdate": 1762925340164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}