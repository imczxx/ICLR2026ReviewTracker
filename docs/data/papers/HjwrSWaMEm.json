{"id": "HjwrSWaMEm", "number": 8874, "cdate": 1758100726637, "mdate": 1759897757761, "content": {"title": "DATE-GFN: A Co-Evolutionary Framework for Principled Exploration and Credit Assignment in GFlowNets", "abstract": "Generative Flow Networks (GFlowNets) are powerful for scientific discovery but are severely hampered in sparse-reward, long-horizon settings by the temporal credit assignment problem, which causes high-variance gradients. While recent work has sought to densify learning signals \\citep{Jang2023, Pan2023} or improve exploration with methods like Evolution Guided GFlowNets (EGFN) \\citep{ikram2024egfn}, the fundamental variance issue for the learning agent persists. We introduce the Distillation-Aware Twisted Evolutionary GFlowNet (DATE-GFN), an actor-critic inspired framework that recasts the problem. We advocate for a paradigm shift: instead of evolving policies, DATE-GFN evolves a population of critics (state-dependent value functions, or \\emph{twist functions}) that learn to estimate the expected future reward from any state. This constructs a dense, state-dependent guidance signal, transforming the high-variance, reward-driven learning into a stable, low-variance supervised distillation task where the student GFlowNet learns to imitate the policy induced by the best critic. Crucially, we solve the inherent \\emph{realization gap} between an optimal teacher and a finite-capacity student via a novel \\textbf{distillation-aware fitness function}. This objective creates a principled trade-off: it simultaneously rewards critics for discovering high-reward states while penalizing them for their \\emph{teachability}, measured by the KL-divergence between their induced policy and the student's. This creates a symbiotic co-evolutionary dynamic where the evolutionary search for better critics is continuously grounded in the student's current learning capabilities. We prove this system converges to a realizable, high-performing equilibrium and show empirically that DATE-GFN significantly outperforms state-of-the-art baselines.", "tldr": "DATE-GFN solves the sparse-reward credit assignment problem in GFlowNets by evolving a population of \"teachable\" critics that provide a dense, low-variance learning signal for a final distilled policy.", "keywords": ["Generative Flow Networks", "Evolutionary Algorithms", "Actor-Critic Methods", "Reinforcement Learning", "Sequential Modelling"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/895a3bc4a5cd4cc11bdc44ac57bbf90ef9cca41f.pdf", "supplementary_material": "/attachment/ab34f280d1c37cb30d9dc904060e091b73fb199e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DATE-GFN, a co-evolutionary method build upon previous E-GFN  for training GFlowNets.  Instead of evolving a population of policies as in E-GFN, it evolves a population of critic, based on which  policies are distilled.  Then it presents empirical results on a hypergrid task, an antibody sequence design task, and a molecular binding-affinity task (sEH binder generation). The authors claim improved performance relative to baselines including TB and EGFN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The conceptual shift to evolving critics rather than policies is original and connects nicely different strands of research (twisted SMC, distillation, GFlowNets).\n\n* The formulation of teachability as a divergence-based penalty is a clear and interesting way to formalize the “realization gap” between teacher and student policy capacity.\n\n* Empirical results are reasonably broad (three tasks, including a combinatorial benchmark and two real-world generative tasks) and include ablations (λ sweep, adaptive version)."}, "weaknesses": {"value": "1. The key limitation of DATE-GFN is it is a purely **online** method (lines 237-239), which makes it difficult to explicitly encourage exploration via offline samplers in the reward-sparse settings. In comparison,  a key advantage of TB method is allowing offline samples from a arbitrary data sampler.  The E-GFN that DATE-GFN is build upon seems to allows offline samples as well.\n\n2. The range of baselines considered is relatively limited. While TB and EGFN are relevant, comparative evaluation against other exploration or credit-assignment methods are missing. For example, comparisons with more explorative GFlowNet variants [1], reinforcement learning–based formulations that also incorporate critics(value) functions [2, 3], and Sub-TB[3], which generalizes TB and achieves better credit assignment by  introducing a parameter $\\lambda$ to control the variance–bias trade-off within its gradient estimator. The absence of these comparisons makes it difficult to figure out the specific source of improvement, whether it arises from enhanced exploration, improved credit assignment, or mitigation of the teacher–student mismatch.\n\n3. As acknowledge by the author, the computational budges is significantly large than the baseline (TB). Besides budget comparison between DATE-GFN and the amortized version of DATE-GFN, please provides detailed comparison between DATE-GFN and other baselines. Without this information, the practicality of the method for large-scale applications is unclear to me.\n\n4. The ablation study on the population size $k$ and number of critics evaluation $G$ should be conducted. Since DATE-GFN adopts an evolutionary framework, an ablation study on these two key hyperparameters is important to clarify the true source of performance improvement.\n\n5. I **disagree** with the statement that the space of $F_{DA}$ is **smoother** than the space of $\\pi_F$. According to the basic detailed balance condition [4], the goal of gflownet training is to achieving the balance of $\\log P_F(s'|s)F(s)=\\log P_B(s|s') F(s')$ for flow estimator $F(s)$ and back/forth policies.  So it is clear that $\\pi_F$ is optimized in the logarithmic domain.  In hypergrids, the ratio between modes and caveats rewards can reach $10^5$, which corresponds to only $log 10^5=5$ in the logarithmic domain.\n\n6. The idea of separating the training into two phase,  a actor-critic framework and the idea of finding new polices under a KL region have been studies in [2,3]. While I note that a single critic is used in [2],  a formal discussion and comparison with the prior works and should be provided. Besides, Sub-TB[3] use $F^{log}(s_0)$ to model $\\log F^\\ast(s_0)=\\log Z^\\ast$ and the PG-GFN[2] can model $V(s_0)=E_{s_{0:T} \\sim p_F}[\\log (P_F(s_{0:T})Z)-\\log (P_B(s_{0:T-1}|s_T)R(s_T))]$, so they operate entirely in the logarithmic domain, in others words, a smoother space than the  $F_{DA}$ or $\\psi$ in DATE-GFN.\n\n7. The experimental results are not convincing. It appears that the reported training curves are based on a single trial, rather than multiple independent runs with statistical reporting.\n\n[1]  Kim, Minsu, et al. \"Local Search GFlowNets.\" The Twelfth International Conference on Learning Representations.\n\n[2] Niu, Puhua, et al. \"GFlowNet Training by Policy Gradients.\" International Conference on Machine Learning. PMLR, 2024.\n\n[3] Deleu, Tristan, et al. \"Discrete Probabilistic Inference as Control in Multi-path Environments.\" Uncertainty in Artificial Intelligence. PMLR, 2024.\n\n[4] Bengio, Yoshua, et al. \"Gflownet foundations.\" Journal of Machine Learning Research 24.210 (2023): 1-55."}, "questions": {"value": "1. Since the size of the chosen hypergrid is still enumerable ($\\approx 2.4\\times 10^7$), can you provide the 2D marginal visualizations of the learned distribution over $s_T$ ?  \n\n2. In distillation you sample states from the teacher’s induced visitation distribution. How do you mitigate distributional shift when the student subsequently explores states not seen by that teacher? Would mixing student trajectories or importance-sampling help?\n\n3. What precisely are the mutation/crossover operators used for evolving the critic population? Are there additional hyper-parameters introduced?\n\n4. How does the choice of base policy $p_0$ affect performance (uniform vs heuristic priors)? ( since the twisted SMC logic depends on $p_0$)\n\n5. Both TB and E-GFN both support off-line sampling. To ensure a fair comparison with these baselines, what choices were made for their offline samplers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S7GLonH3Mw", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_iS54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_iS54"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864254584, "cdate": 1761864254584, "tmdate": 1762920637720, "mdate": 1762920637720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an alternative setup for training gflownets, where multiple critics are trained, and the best critic is selected, and distilled into a student network.  There is also an interesting penalty that forces the teacher networks to be reasonably close to the student network (teachability).  There is a bit of theory showing that this approach has a valid equilibrium and experiments validate that the teachability constraint is necessary for the approach to work.  While evolutionary training has a cost in terms of computation, I think this paper is a nice effort on pushing forward the gflownet line of research.  It would be interesting to explore ways of gaining the advantages of this method without having to train multiple critic networks.  \n\nnotes: \n  -Gflownet struggles in sparse-reward, long-horizon setting.  \n  -Paper proposes to learn a population of critics, which the gflownet distills into a single low-variance policy.  Critics try to find policies which are learnable by the student.  \n  -The optimal twist is the true expected future reward-to-go under the base policy.  Can define a twist-induced sampling policy, which is variance-optimal.  q_\\psi is a greedy policy wrt the value function \\psi."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "-The distillation-aware fitness function is a nice idea.  \n  -The theory, showing the equilibrium property is nice and not excessive.  \n  -Results are great.  It's also nice to see the ablation on the effect of lambda, showing that the decoupled baseline fails.  \n  -The results on sEH binder generation is good."}, "weaknesses": {"value": "-Figure 2 could be nicer, these look like screen captures from weights and biases.  They should have larger font size.  The results are great, though.  \n\n  -I don't see any external baseliness on the antibody generation task (table 4).  \n\n  -It would be nice to see a more diverse set of tasks and datasets, because I think this method can be applied anywhere where gflownet is used."}, "questions": {"value": "-Some of the earlier work on semi-supervised learning involved learning multiple teachers, and then distilling them into a student network via ensembling.  Later work (such as \"Mean teacher\") shifted towards using an EMA of the teacher's weights as a cheap way of approximating ensembling, to give targets to the student network.  Have you considered that something similar could also be done here?  \n\n  -In 3.2, do you think it's possible that overestimation bias in value function learning, and optimism, could be alternative explanations for why the naive algorithm fails?  \n\n  -What's the tradeoff curve between computational cost and performance?  I appreciate the effort to reduce computational cost, but it might be nice to see how computation trades off against performance, when compared to other gflownet baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfgnGknO1t", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_Ezm7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_Ezm7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898393160, "cdate": 1761898393160, "tmdate": 1762920637277, "mdate": 1762920637277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn a co-evolutionary critic by replacing the previously proposed evolutionary-algorithm learnt actors to improve GFlowNets mode-seeking capabilities. It also learns a policy-aware critic by running constrained search with the evolutionary algorithm. The experiments show improvements in hyper-grid, antibody generation, and sEH binder generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Replacing the EA-trained actor with EA-trained critic is a neat idea.\n- The constraint put on the evolutionary search of parameters for critics make sense. In practice, the actor indeed might lack representational capabilities needed to realize critic's potential. \n- I appreciate the theoretical reasoning in section 7,8,9.\n- The paper was generally easy to follow."}, "weaknesses": {"value": "- The paper lacks sufficient experiments. This includes--\n\na. Comparison against exploration techniques (for example, GAFN)\n\nb. Time comparison. While i understand it notes the time complexity as an understandable weakness, time comparison of other methods would be beneficial.\n\nc. Experimental validation of lambda optimal. The authors performs experiments with adaptive lambda in Figure 4. How does the values look like over the training?\n\n- The authors mention experimenting with large values of lambda but uses lambda = 1 in their experiments. What happens if lambda is 2? Or 100?"}, "questions": {"value": "- Can we enforce the constraint without explicitly calculating the extra loss term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zSBQw8m3Vv", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939613860, "cdate": 1761939613860, "tmdate": 1762920636927, "mdate": 1762920636927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn a co-evolutionary critic by replacing the previously proposed evolutionary-algorithm learnt actors to improve GFlowNets mode-seeking capabilities. It also learns a policy-aware critic by running constrained search with the evolutionary algorithm. The experiments show improvements in hyper-grid, antibody generation, and sEH binder generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Replacing the EA-trained actor with EA-trained critic is a neat idea.\n- The constraint put on the evolutionary search of parameters for critics make sense. In practice, the actor indeed might lack representational capabilities needed to realize critic's potential. \n- I appreciate the theoretical reasoning in section 7,8,9.\n- The paper was generally easy to follow."}, "weaknesses": {"value": "- The paper lacks sufficient experiments. This includes--\n\na. Comparison against exploration techniques (for example, GAFN)\n\nb. Time comparison. While i understand it notes the time complexity as an understandable weakness, time comparison of other methods would be beneficial.\n\nc. Experimental validation of lambda optimal. The authors performs experiments with adaptive lambda in Figure 4. How does the values look like over the training?\n\n- The authors mention experimenting with large values of lambda but uses lambda = 1 in their experiments. What happens if lambda is 2? Or 100?"}, "questions": {"value": "- Can we enforce the constraint without explicitly calculating the extra loss term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zSBQw8m3Vv", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939613860, "cdate": 1761939613860, "tmdate": 1763399914028, "mdate": 1763399914028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to learn a co-evolutionary critic by replacing the previously proposed evolutionary-algorithm learnt actors to improve GFlowNets mode-seeking capabilities. It also learns a policy-aware critic by running constrained search with the evolutionary algorithm. The experiments show improvements in hyper-grid, antibody generation, and sEH binder generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Replacing the EA-trained actor with EA-trained critic is a neat idea.\n- The constraint put on the evolutionary search of parameters for critics make sense. In practice, the actor indeed might lack representational capabilities needed to realize critic's potential. \n- I appreciate the theoretical reasoning in section 7,8,9.\n- The paper was generally easy to follow."}, "weaknesses": {"value": "- The paper lacks sufficient experiments. This includes--\n\na. Comparison against exploration techniques (for example, GAFN)\n\nb. Time comparison. While i understand it notes the time complexity as an understandable weakness, time comparison of other methods would be beneficial.\n\nc. Experimental validation of lambda optimal. The authors performs experiments with adaptive lambda in Figure 4. How does the values look like over the training?\n\n- The authors mention experimenting with large values of lambda but uses lambda = 1 in their experiments. What happens if lambda is 2? Or 100?"}, "questions": {"value": "- Can we enforce the constraint without explicitly calculating the extra loss term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The paper seems to be AI generated."}}, "id": "zSBQw8m3Vv", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_LUrq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939613860, "cdate": 1761939613860, "tmdate": 1763525607124, "mdate": 1763525607124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a fundamental limitation in existing GFlowNet training methods—high variance in gradient estimates due to sparse terminal rewards—and proposes a paradigm shift inspired by actor-critic methods in RL, termed DATE-GFN. The key innovation is a distillation-aware fitness function that optimizes critics not only for reward but also for teachability, ensuring the student GFlowNet can effectively learn from them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies an important problem in GFN training."}, "weaknesses": {"value": "The proposed method is trivial, but presented in an inproper way. The student policy is distilled from the teacher policy, which is trained by maximizing the expected return with the distillation gap as a regularizor. Firstly, it has nothing to do with GFlowNet. According to the Section 3.1, the resulted optimal policy will sample trajectories in proportion to $R(\\tau)$ (RL), instead of sampling terminal states in proportion to $R(s)$ (GFN). Secondly, it is doubtful that the proposed framework can tackle the high-variance issue caused by sparse reward, as the proposed disstillation-aware fitness function is just adding some noise to the original reward maximization objective."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o8lCvZiVOT", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_kQGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_kQGp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972074051, "cdate": 1761972074051, "tmdate": 1762920636583, "mdate": 1762920636583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a fundamental limitation in existing GFlowNet training methods—high variance in gradient estimates due to sparse terminal rewards—and proposes a paradigm shift inspired by actor-critic methods in RL, termed DATE-GFN. The key innovation is a distillation-aware fitness function that optimizes critics not only for reward but also for teachability, ensuring the student GFlowNet can effectively learn from them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies an important problem in GFN training."}, "weaknesses": {"value": "The proposed method is trivial, but presented in an inproper way. The student policy is distilled from the teacher policy, which is trained by maximizing the expected return with the distillation gap as a regularizor. Firstly, it has nothing to do with GFlowNet. According to the Section 3.1, the resulted optimal policy will sample trajectories in proportion to $R(\\tau)$ (RL), instead of sampling terminal states in proportion to $R(s)$ (GFN). Secondly, it is doubtful that the proposed framework can tackle the high-variance issue caused by sparse reward, as the proposed disstillation-aware fitness function is just adding some noise to the original reward maximization objective."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o8lCvZiVOT", "forum": "HjwrSWaMEm", "replyto": "HjwrSWaMEm", "signatures": ["ICLR.cc/2026/Conference/Submission8874/Reviewer_kQGp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8874/Reviewer_kQGp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8874/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972074051, "cdate": 1761972074051, "tmdate": 1763517944959, "mdate": 1763517944959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}