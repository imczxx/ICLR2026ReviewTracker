{"id": "UmdmJRGb9u", "number": 8515, "cdate": 1758089007054, "mdate": 1759897779368, "content": {"title": "DLLMQuant: A Post-Training Quantization Framework Tailored for Diffusion-Based Large Language Models", "abstract": "Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16\\% accuracy drop on LLADA under W4A4). This paper explores how the unique mechanisms of Dynamic Language Models (DLLMs) conflict with quantization, identifying three core issues: 1) During the iterative generation process of DLLMs, dynamic masking ratios are inherently involved, leading to notable differences in token distributions across decoding steps. Unfortunately, these distinct distributions are not sufficiently captured by current PTQ calibration approaches; 2) Quantization errors propogate and accumalte progressively during iterations in DLLMs, leading to a gradual decline in the performance of quantized models as decoding steps advance; 3) The stability of unmasked tokens, combined with the probabilistic nature of masked tokens, gives rise to an overall feature distribution that is uncoordinated and unsuitable for PTQ. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which which utilizes bidirectional attention scores to identify important tokens, and prioritizes these tokens when minimizing quantization error. 3) Certainty-Guided Quantization (CGQ) incorporates mask status and token scores as core weighting criteria for error compensation, enabling PTQ to better align with the unique weight distribution of DLLMs. Experiments show that DLLMQuant achieves significant performance gains (e.g., over 10-point accuracy improvement on GSM8K for LLADA under 4-bit quantization) while enhancing efficiency.", "tldr": "", "keywords": ["Quantization", "Diffusion-based large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7db4f422b1aa898ee22dd33f3bba2fb034f824c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To reduce the memory and computational cost of diffusion-based large language models (DLLMs), the authors propose a post-training quantization (PTQ) method.\nThey observe that directly applying conventional PTQ methods to DLLMs leads to significant performance degradation due to three main challenges.\nFirst, to address the data distribution shift across time steps, the authors introduce a Temporal-Mask Adaptive Sampling strategy that collects calibration data from various time steps and masking ratios, effectively capturing the diverse data distributions present during inference.\nSecond, to mitigate the accumulation of quantization errors across iterations, the authors identify that these errors mainly arise from quantized matrix multiplications of the value matrix and the softmax outputs. To alleviate this issue, they propose an Interaction-Aware Activation Quantization method that searches for the optimal quantization parameters of the value matrix, reducing cumulative quantization error.\nThird, recognizing the different impacts of masked and unmasked tokens to the model output, the authors assign different importance factors to masked and unmasked tokens when profiling the Hessian matrices of weights, leading to more accurate quantization sensitivity estimation.\nExperimental results demonstrate that the proposed method achieves an average 3% performance improvement over state-of-the-art baselines under 4-bit weight and activation quantization."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a comprehensive and well-organized background on diffusion-based large language models (LLMs) and quantization techniques, making the work accessible and informative for readers who may not be deeply familiar with these topics.\n2. The paper clearly describes the motivation behind each design choice by contrasting autoregressive LLMs with diffusion-based LLMs to justify the necessity of the proposed method.\n3. The proposed method demonstrates notable improvements in both algorithmic performance over baseline models and hardware efficiency compared to the original model when deployed on consumer-level GPUs, underscoring its practical applicability and potential for real-world deployment."}, "weaknesses": {"value": "1. The motivation of Interaction-Aware Activation Quantization (IA-AQ) could be further clarified and strengthened.\n\n$~$ (a) In Figure 3, using unquantized matrix multiplication in the attention module helps reduce quantization error. However, the error still appears to accumulate across time steps, suggesting that the proposed approach may not address the issue of quantization error accumulation. It would be helpful if the authors could explain how IA-AQ aims to mitigate this problem theoretically or using experimental results.\n\n$~$ (b) The relationship between searching for scaling factors and enabling sufficient token interaction remains somewhat ambiguous. A more detailed explanation of this relationship would strengthen the rationale of the proposed design.\n\n2. The overall workflow of the proposed method could be presented more clearly. \n\n$~$ (a) In Equation (10), the authors compute the Hessian matrices of the weights, but it is not fully explained how the Hessian matrices is utilized in subsequent steps. Providing a pseudo-code or schematic diagram of the full pipeline would greatly improve readability and understanding.\n\n$~$ (b) In Tables 1 and 2, the proposed CGQ is combined with AWQ[1] and QuaRot[2] to validate its effectiveness. Since AWQ and QuaRot do not require Hessian matrices while CGQ refines their calculation, clarifying the implementation details would avoid confusion.\n\n3. Some parts of the manuscript appear to be incomplete. \n\n$~$ (a) There is a placeholder “TODO” table in line 439, indicating missing ablation results for the different components of DLLMQuant.\n\n$~$ (b) Figure 4 contains meaningless notations.\n\n\n[1] Lin, Ji, et al. \"Awq: Activation-aware weight quantization for on-device llm compression and acceleration.\" Proceedings of machine learning and systems 6 (2024): 87-100.\n\n[2] Ashkboos, Saleh, et al. \"Quarot: Outlier-free 4-bit inference in rotated llms.\" Advances in Neural Information Processing Systems 37 (2024): 100213-100240."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tGVZcSxroR", "forum": "UmdmJRGb9u", "replyto": "UmdmJRGb9u", "signatures": ["ICLR.cc/2026/Conference/Submission8515/Reviewer_8GDD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8515/Reviewer_8GDD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707538220, "cdate": 1761707538220, "tmdate": 1762920380165, "mdate": 1762920380165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a PTQ framework named DLLMQuant for DLLMs, which tailors for calibration, quantization errors propogation and significant disparities in feature distributions caused by masked generation strategies in DLLMs. Experiments demonstrate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper thoroughly investigates the differences between DLLMs and LLMs in quantization and provides a detailed discussion on why existing LLM quantization methods are not well suited for DLLMs.\n2. This paper is one of the earliest exploratory works in DLLM quantization and provides valuable insights for future research."}, "weaknesses": {"value": "1. **The most intolerable weakness of this paper is that it appears to have been prepared in a rush with numerous editorial errors, which means that it was submitted without careful proofreading**. For example, in Line 31, there are two *which* before *utilizes*.  In Line 388, the performance of DLLMQuant is worse than QuaRot on Arc but it was bolded. And in Line 439, *As can be seen in Tab (TODO)* is so careless, and we can find that the authors ignore to list the table of ablation studies about TMAS, CGQ and IA-AQ. I strongly suggest that the authors carefully proofread and revise their manuscript. Without demonstrating sufficient care and attention to their own work, it will be difficult to achieve a high score from reviewers.\n2. In Figure 3, it is difficult to distinguish the three yellow lines in the middle. It is recommended to use colors with greater contrast to improve visual clarity."}, "questions": {"value": "1. In section 3.3, the authors should provide the detailed comparison results to support their hyperparameter selection (1 and 0.7).\n2. Does Eq.7 aim to make the quantization error of the output more correlated with the important tokens?\n3. In the description of section 2.2, both the softmax output and value matrix will have bad impact on quantization performance. However, according to the reasoning behind Eq.7–9, only the quantization error of V is reduced, while the output of the softmax still suffers from performance degradation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fiNGdYsyD", "forum": "UmdmJRGb9u", "replyto": "UmdmJRGb9u", "signatures": ["ICLR.cc/2026/Conference/Submission8515/Reviewer_HiH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8515/Reviewer_HiH2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745911528, "cdate": 1761745911528, "tmdate": 1762920379829, "mdate": 1762920379829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DLLMQuant, a novel post-training quantization (PTQ) framework specifically designed for Diffusion-based Large Language Models (DLLMs). The authors identify that standard PTQ methods, which work well for autoregressive LLMs, fail for DLLMs due to three unique challenges: temporal distribution shifts across denoising steps, accumulation of quantization errors over iterations, and imbalanced feature distributions caused by dynamic masking. To address these, DLLMQuant proposes three core techniques: Temporal-Mask Adaptive Sampling (TMAS) for better calibration data selection, Interaction-Aware Activation Quantization (IA-AQ) to reduce error propagation in attention, and Certainty-Guided Quantization (CGQ) to refine weight quantization using mask and confidence information. Experiments on multiple DLLMs show that DLLMQuant significantly outperforms existing PTQ methods, preserving reasoning capabilities and achieving substantial speedup and memory savings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Problem Identification: The paper excels at diagnosing the specific reasons why standard PTQ fails for DLLMs (temporal shift, error accumulation, masking disparities), providing a solid foundation for the proposed solutions.\n\n2. Comprehensive Evaluation: The experiments are thorough, testing on three different DLLMs across nine diverse benchmarks (including reasoning and code generation), and include detailed ablation studies that convincingly demonstrate the contribution of each proposed component.\n\n3. Practical Impact: The framework is plug-and-play, requires no fine-tuning, and demonstrates impressive practical benefits—over 1.6x speedup and 3.2x memory reduction—which are crucial for real-world deployment on consumer hardware."}, "weaknesses": {"value": "1. Limited Baseline Comparison: While compared against strong PTQ methods like AWQ and QuaRot, it would be beneficial to see a comparison against more recent or specifically designed quantization-aware training (QAT) approaches, even if only to baseline the performance gap that PTQ seeks to bridge.\n\n2. Clarity on Computational Overhead: The calibration process for TMAS and the search for the scaling factor α in IA-AQ likely introduce some overhead. The paper does not quantify the additional calibration time or cost compared to the baseline methods.\n\n3. Ablation Parameter Justification: The choice of specific parameters (e.g., the 0.7 weight for unmasked tokens in CGQ, the [0.3,0.2,0.2,0.3] sampling proportion in TMAS) feels somewhat heuristic. While ablations show they work well, a more principled explanation for why these values are optimal would strengthen the methodology."}, "questions": {"value": "Calibration Cost: Could you provide details on the computational cost and time required for the DLLMQuant calibration process (especially TMAS and the IA-AQ parameter search) compared to the calibration steps of AWQ or QuaRot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dellLsf32x", "forum": "UmdmJRGb9u", "replyto": "UmdmJRGb9u", "signatures": ["ICLR.cc/2026/Conference/Submission8515/Reviewer_DkrN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8515/Reviewer_DkrN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793425499, "cdate": 1761793425499, "tmdate": 1762920379535, "mdate": 1762920379535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DLLMQuant, a novel post-training quantization (PTQ) framework specifically designed for Diffusion-based Large Language Models (DLLMs). The authors identify that standard PTQ methods fail on DLLMs due to three unique challenges: 1) temporal distribution shifts across denoising steps, 2) accumulation of quantization errors over iterations, and 3) disparate feature distributions caused by dynamic masking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper demonstrates high originality by being the first to systematically address the unique challenges of quantizing Diffusion-based LLMs (DLLMs), a nascent and structurally distinct model class. Its significance is substantial, as it enables the practical deployment of computationally intensive DLLMs by achieving major speed and memory gains.\n\nThe quality of the work is strong, supported by rigorous experiments across multiple DLLMs and diverse tasks, which clearly validate the performance improvements over adapted baselines. The clarity is commendable; the paper is well-structured, with a logical flow from problem identification to solution, making the novel contributions of TMAS, IA-AQ, and CGQ easy to understand."}, "weaknesses": {"value": "(1) This paper's primary weakness lies in its experimental validation. While it demonstrates accuracy retention, it lacks inference latency measurements for quantized models, which is critical for assessing the real-world \"efficiency\" gains promised by quantization. The reported speedup (tokens/sec) is insufficient without latency-per-token data.\n\n(2) The integration strategy with baseline methods (AWQ/QuaRot) is unclear. The paper states DLLMQuant's components are \"plug-and-play\" but does not specify if it uses these baselines' core algorithms or merely their frameworks. A clearer ablation, perhaps replacing CGQ with GPTQ's Hessian in the same pipeline, would better isolate the contribution of the novel weighting scheme in Eq. 10 versus the underlying quantization machinery."}, "questions": {"value": "Question 1: Could you provide per-token latency measurements for the quantized models? This would more directly validate the practical inference speedup beyond aggregate tokens/second.\n\nQuestion 2: Does CGQ replace the standard Hessian calculation in baselines like GPTQ, or is it an additional weighting applied on top? Clarifying this integration would help isolate its novel contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RJkCGyHGlJ", "forum": "UmdmJRGb9u", "replyto": "UmdmJRGb9u", "signatures": ["ICLR.cc/2026/Conference/Submission8515/Reviewer_ZDdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8515/Reviewer_ZDdf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797741423, "cdate": 1761797741423, "tmdate": 1762920379046, "mdate": 1762920379046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}