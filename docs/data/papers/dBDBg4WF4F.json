{"id": "dBDBg4WF4F", "number": 20632, "cdate": 1758308398682, "mdate": 1763461502804, "content": {"title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics", "abstract": "Behavioral Foundation Models (BFMs) proved successful in producing near-optimal policies for arbitrary tasks in a zero-shot manner, requiring no test-time retraining or task-specific fine-tuning. Among the most promising BFMs are the ones that estimate the successor measure learned in an unsupervised way from task-agnostic offline data. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward–Backward (FB) representation, one of the methods from the BFM family, cannot produce reasonable policies under distinct dynamics, leading to an interference among the latent policy representations. To address this, we propose an FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. Additionally, we show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. Those traits allow our method to respond to the dynamics mismatches observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.", "tldr": "We provide both theoretical and empirical evidence that Forward–Backward representations cannot adapt to changing dynamics and introduce a method that overcomes this, generalizing to both seen and unseen dynamics at test time.", "keywords": ["zero-shot reinforcement learning", "unsupervised reinforcement learning", "successor measure"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64c2421464bec1ee25cf5bbb6d1d494374e9832e.pdf", "supplementary_material": "/attachment/5260a74d9f3a5fac792a67e13c56136f39d6d6b9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces an algorithm for unsupervised zero-shot reinforcement learning under partial observability. It considers environments where the dynamics are governed by a latent parameter that is not directly observable but can be inferred from transitions. The authors propose using a permutation-invariant, transformer-based architecture to build an estimate (or belief state) of the dynamics from a set of transitions. This belief vector is then used to condition the Forward-Backward model, i.e., both the actor and critic are trained conditioned on the belief state. Furthermore, they propose modifying the structure of the task/policy space to be belief-state dependent, resulting in clusters of dynamics. The effectiveness of the method is evaluated on three toy domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- As far as I know, this is the first paper to tackle the problem of unsupervised reinforcement learning in partially observable contextual Markov Decision Processes (MDPs).\n- The approach is simple and sound, and the authors provide a clear, step-by-step construction of the method, explaining why each component is necessary.\n- Although the core idea is not very novel, I believe its application to reinforcement learning is new and interesting for the community."}, "weaknesses": {"value": "I don't see any major weaknesses to mention in this section. I will report a few questions in the next section."}, "questions": {"value": "- I found Section 3.1 a bit trivial. It is expected that a method designed for a single MDP does not perform well in the considered contextual MDP. What am I missing? Why spend so much time justifying this aspect?\n- I like the approach of building the latent belief by predicting the next state. Could you discuss further the choice of using a non-causal transformer? Why did you use Gaussian regularization?\n- Do you use samples from the training dataset to perform reward inference? Does the performance change if you use transitions from the test environment for inference?\n- You assume access to a set of transitions from the test environment to infer the context. However, you could also compute the context directly while interacting with the environment. Could you evaluate the test performance with respect to the number of samples used for belief inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JG1u0efL5W", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Reviewer_dycu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Reviewer_dycu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760947409786, "cdate": 1760947409786, "tmdate": 1762934031140, "mdate": 1762934031140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical failure in Forward-Backward (FB) based Behavioral Foundation Models: training on mixed-dynamics data causes interference, as the model averages the successor measures across all environments. This averaging entangles the latent policy representations, yielding sub-optimal policies. To solve this, the authors propose Belief-FB (BFB), which introduces a transformer-based belief estimator to infer a dynamics-specific context vector from reward-free transitions, which then conditions the FB model. They further introduce Rotation-FB (RFB), which structures the latent space by sampling from a von Mises-Fisher (vMF) distribution centered at the inferred context. This alignment partitions the policy space into non-overlapping, dynamics-specific clusters, preventing interference and enabling zero-shot adaptation to unseen dynamics. The methods were tested on simulated discrete and continuous tasks with changing dynamics, including Randomized Four-Rooms, PointMass, and AntWind. The results show that both BFB and RFB successfully adapt to seen dynamics and generalize to unseen ones, achieving up to 2x higher zero-shot returns compared to baselines like vanilla FB."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The authors identify a fundamental failure mode in Forward-Backward (FB) models, which is termed interference.\nThe proposed method demonstrates promising zero-shot generalization improvement to unseen dynamics."}, "weaknesses": {"value": "The work addresses a problem specific to the FB paradigm and builds upon this methodology only. It may not be general to other behavioral foundation models.\n\nThe introduction of a context into a dynamics model to disambiguate between distinct training environment dynamics is a well-established approach in adaptive control and reinforcement learning (Peng et al.). However, I do not see a discussion of these previous works (or really any related works discussion).\n\nEvaluations are in simulated, ideal environments under narrow dynamics variations."}, "questions": {"value": "Lines 143-144. Something appears to be missing?\n\n### References\n\nPeng, Xue Bin, et al. \"Learning agile robotic locomotion skills by imitating animals.\" arXiv preprint arXiv:2004.00784 (2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uI0gM2y8Se", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Reviewer_mdha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Reviewer_mdha"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901453762, "cdate": 1761901453762, "tmdate": 1762934030506, "mdate": 1762934030506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Behavior foundation models (BFMs), such as Forward-Backward representation, enable zero-shot policy optimization under test-time rewards. An example is Forward-Backward representation, which represents the success measure as a product of forward features $F(s, a, z)$ and backward features $B(s^+)$, and extracts policies as $\\pi_z := \\arg\\max_a F(s, a, z)^\\top z$. However, FB inherently encodes information about the dynamics and doesn't transfer to unseen dynamics. This paper first diagnoses this issue by visualizing the optimal actions corresponding to latent codes and find that training on data from different dynamics results in interference. To address this, they propose Belief-FB (BFB), which introduces a context encoder -- a permutation-invariant transformer that encodes a set of transitions into a context vector about the dynamics. The context $h$ is concatenated to the latent code $z$ and used to condition the forward representation $F(s, a, z+h)$. However, because FB training draws latent $z$ uniformly, this still entangles all the policies from different dynamics. To further disentangle the polices, the paper proposes Rotation-FB (RFB), which samples $z$ within a cone around a projection of the context vector $h$. The paper evaluates BFB and RFB in discrete and continuous state-based environments and finds BFB to outperform baseline BFMs without contextual awareness, and RFB to further improve performance. Overall, the paper takes an important step towards removing the stationary-dynamics assumption of BFMs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed methods are intuitive. Introducing a context vector to address ambiguity during FB training across diverse dynamics is a logical and effective approach, while sampling $z$ around a cone-shaped prior makes sense as an inductive bias.\n2. The visualizations (Figures 3 and 4) are highly informative, providing compelling evidence for the policy entanglement issue in standard FB and demonstrating the improvements achieved by BFB and RFB.\n3. The paper provides a rigorous formal analysis, showing that (1) the worst-case regret for vanilla FB increases with the number of environments and (2) the FB approximation error is reduced by introducing the cone sampling approach.\n4. Both BFB and RFB outperform the baseline methods in the empirical evaluations."}, "weaknesses": {"value": "1. The evaluation environments are excessively simple.\n2. The paper does not compare the proposed methods to immediately relevant baseline with context awareness (e.g. contextual FB [1]).\n3. The vanilla context vector doesn't fully disentangle policies, which is why the paper introduces additional rotation variant."}, "questions": {"value": "**Major**\n1. Why can't we simply add context to the observation? For example, one can encode the maze layout and concatenate it with the rest of the state vector (or even better, use image-based policies). This wouldn't work when the context is partially observable, but for fully observable environments, can you evaluate this simple oracle baseline?\n2. Can you add comparison to immediately relevant baselines that are dynamics aware, such as [1]?\n3. Can you demonstrate the effectiveness of your method on more challenging domains, e.g., dexterous manipulation, where the friction parameter is varied?\n\n**Minor**\n\n4. Typo line 342: \"platoes\" -> \"pleateaus\"\n5. Typo line 732: missing right bracket\n6. Lines 119 - 132 are not very clear for readers unfamiliar with the FB literature. For example, the policy is defined as argmax (saying it is extracted as argmax might confuse readers). I suggest adding a section on the arithmetic of FB in the appendix. \n\nReferences:\n\n[1] Scott Jeen and Jonathan Cullen. Dynamics generalisation with behaviour foundation models. Workshop on Training Agents with Foundation Models at RLC 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2QtQs8ISyM", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Reviewer_jHoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Reviewer_jHoj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907046796, "cdate": 1761907046796, "tmdate": 1762934029909, "mdate": 1762934029909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Belief-Forward-Backward (BFB) and Rotational Forward-Backward (RFB), two methods that extend Behavioral Foundation Models (BFMs) for zero-shot adaptation to unseen dynamics.  \n1. Belief-Forward-Backward (BFB): introduces a transformer-based belief encoder that infers environment dynamics and conditions the Forward-Backward (FB) representation on this latent belief, improving adaptability to new transitions.  \n2. Rotational Forward-Backward (RFB): further refines BFB by aligning latent policy directions with context-specific clusters using a von Mises-Fisher prior, allowing better separation between dynamics in the latent space.  \n3. The paper provides theoretical explanations for why standard FB representations fail under varying dynamics and offers formal regret bounds for BFB and RFB.  \n4. Experiments across discrete (Four-Rooms) and continuous (PointMass, AntWind) domains demonstrate improved zero-shot generalization of BFB and RFB compared to baselines such as FB, HILP, and LAP."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and well-motivated idea that is straightforward to implement.\n2. The visualization and qualitative analyses are helpful for understanding the model behavior.\n3. Experiments convincingly demonstrate the effectiveness of the proposed methods across multiple settings."}, "weaknesses": {"value": "1. **Minor issues:**  \n   - Equation (1) uses `(s_{t+1}, a_{t+1}) ∈ X` although `X` was defined as a subset of states. It should be `s_{t+1} ∈ X`.  \n   - Line 144 appears incomplete.  \n   - Theorem 1 defines `‖r‖_∞ ≤ R` but the symbol `R` is not used in the subsequent expressions.\n2. The paper lacks a formally defined problem statement. The current description is mostly natural language; a mathematical setup of the CMDP and belief inference process would improve clarity.\n3. Figure 2 is conceptually confusing without such a formal section. It appears that observations are included in the state, and layouts (contexts) are not distinguished in state space, meaning the agent cannot infer context. Since `z_FB` does not encode context either, the result—an averaged policy—becomes unsurprising.\n4. The connection between Theorem 1 and the statement in line 215 is logically weak. Theorem 1 implies that more CMDPs increase the *upper bound* of the worst-case error, not necessarily the regret itself. More CMDPs may also mean more samples, potentially reducing `ε_k`. Section 3.1 lacks discussion of this, reducing its persuasiveness.\n5. The value of Theorem 2 depends on the validity of Theorem 1, and it has the same limitations. In addition, the variable `L` is undefined in Theorem 2.\n6. The experimental comparisons may be unfair: BFB and RFB explicitly encode dynamic information into `z_FB`, while baselines like FB or HILP do not. This makes it expected that those baselines perform close to random."}, "questions": {"value": "Empirically, RFB consistently outperforms BFB. Are there situations where the authors believe BFB would be preferable to RFB—for instance, when dynamics clusters are fuzzy, data is limited, or tuning κ is undesirable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1haPxZpMc8", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Reviewer_EvRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Reviewer_EvRU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962293076, "cdate": 1761962293076, "tmdate": 1762934029536, "mdate": 1762934029536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to the Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their careful reading, constructive suggestions, and thoughtful critiques. Your feedback helped us clarify the scope of our theory, improve the fairness of our comparisons, and strengthen the presentation. Below we respond to most frequent questions. All of the revised text is highlighted in blue, together with new experiments.\n\nWe would like to emphasize that our main contribution is the insight, which showcases ,both theoretically and empirically, fundamental issue of BFMs when trained to learn policies across mixture of offline data (coming from different dynamics). *In theory, FB can encode policies corresponding to different Successor Measures (SMs) for different transition functions*. **We show why this is not the case and identify that geometric prior over latent policy-representation space is required to mitigate interference issues.**\n\n- **Motivation.** We study BFMs based on Forward–Backward (FB) representations because of their growing popularity in the community [1-3]. However, existing BFMs do not explicitly address dynamics generalization, which limits adaptivity. While FB can, in principle, encode policies for distinct dynamics, the absence of structural bias in the policy latent space induces interference/representation collapse. We formalize this in Theorem 1, observe it empirically in Section 4 and propose improvement (RFB) in Section 3.3.\n \n - **Baselines.** We add information-matched baselines, including Contextual FB and Oracle-ID. Our main insight, however, is to *leverage the geometry* of the FB policy latent space $\\pi_z$ via belief conditioning (BFB) and latent-space partitioning (RFB).\n    \n- **Clarity.** We reorganized Section 3.1, added short discussions after each theorem, and refined wording to reflect the trade-off between approximation and estimation (upper bounds become looser in $k$ via the approximation term, while estimation improves with more data).\n\nBelow, we provide a brief discussion on how the research community can build upon our proposed work:\n\n**Future developments that can improve scalability of BFB/RFB:**\n\n**One promising future direction for scaling is continual adaptation. In realistic scenarios, the environment may undergo multiple changes in its underlying state (e.g. the appearance of new objects), requiring the agent to continually adapt its behavior.** In such settings, identifying a suitable behavior policy  that can account for these changes is essential for achieving optimal performance across tasks. Currently, we are not aware of any existing approaches that successfully scale BFMs to the continual learning regime.\n\nAn interesting avenue for future work is to study the geometry of the learned behaviors under such non-stationarity, extending our analysis in Figures 3 and 4 and to understand how the internal representations evolve. For instance, it would be valuable to examine how anchor embeddings  shift instantaneously in response to changes in dynamics from a Bayesian perspective. This could shed light on stability of the learned behavioral basis and guarantees for optimal policies.\n\n**Another promising direction for scaling our approach is language grounding.** Rather than relying solely on sequences of trajectories to learn representations of dynamics, the agent could incorporate language-based descriptions during training. This would allow the model to generalize better at test time by leveraging high-level semantic information, e.g prior knowledge about object locations or environment structure, which is provided through natural language. Such grounding could significantly improve the agent ability to infer dynamics and adapt its behavior in unseen or partially observed environments.\n\nWe appreciate the opportunity to improve the paper and believe these changes address the key concerns raised by the reviewers."}}, "id": "B7L4UE75kX", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763466187700, "cdate": 1763466187700, "tmdate": 1763466307848, "mdate": 1763466307848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to the Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their careful reading, constructive suggestions, and thoughtful critiques. Your feedback helped us clarify the scope of our theory, improve the fairness of our comparisons, and strengthen the presentation. Below we respond to most frequent questions. All of the revised text is highlighted in blue, together with new experiments.\n\nWe would like to emphasize that our main contribution is the insight, which showcases ,both theoretically and empirically, fundamental issue of BFMs when trained to learn policies across mixture of offline data (coming from different dynamics). *In theory, FB can encode policies corresponding to different Successor Measures (SMs) for different transition functions*. **We show why this is not the case and identify that geometric prior over latent policy-representation space is required to mitigate interference issues.**\n\n- **Motivation.** We study BFMs based on Forward–Backward (FB) representations because of their growing popularity in the community [1-3]. However, existing BFMs do not explicitly address dynamics generalization, which limits adaptivity. While FB can, in principle, encode policies for distinct dynamics, the absence of structural bias in the policy latent space induces interference/representation collapse. We formalize this in Theorem 1, observe it empirically in Section 4 and propose improvement (RFB) in Section 3.3.\n \n - **Baselines.** We add additional baselines, including Contextual FB and Oracle-ID. Our main insight, however, is to *leverage the geometry* of the FB policy latent space $\\pi_z$ via belief conditioning (BFB) and latent-space partitioning (RFB).\n    \n- **Clarity.** We reorganized Section 3.1, added short discussions after each theorem, and refined wording to reflect the trade-off between approximation and estimation (upper bounds become looser in $k$ via the approximation term, while estimation improves with more data).\n\nBelow, we provide a brief discussion on how the research community can build upon our proposed work:\n\n**Future developments that can improve scalability of BFB/RFB:**\n\n**One promising future direction for scaling is continual adaptation. In realistic scenarios, the environment may undergo multiple changes in its underlying state (e.g. the appearance of new objects), requiring the agent to continually adapt its behavior.** In such settings, identifying a suitable behavior policy  that can account for these changes is essential for achieving optimal performance across tasks. Currently, we are not aware of any existing approaches that successfully scale BFMs to the continual learning regime.\n\nAn interesting avenue for future work is to study the geometry of the learned behaviors under such non-stationarity, extending our analysis in Figures 3 and 4 and to understand how the internal representations evolve. For instance, it would be valuable to examine how anchor embeddings  shift instantaneously in response to changes in dynamics from a Bayesian perspective. This could shed light on stability of the learned behavioral basis and guarantees for optimal policies.\n\n**Another promising direction for scaling our approach is language grounding.** Rather than relying solely on sequences of trajectories to learn representations of dynamics, the agent could incorporate language-based descriptions during training. This would allow the model to generalize better at test time by leveraging high-level semantic information, e.g prior knowledge about object locations or environment structure, which is provided through natural language. Such grounding could significantly improve the agent ability to infer dynamics and adapt its behavior in unseen or partially observed environments.\n\nWe appreciate the opportunity to improve the paper and believe these changes address the key concerns raised by the reviewers."}}, "id": "B7L4UE75kX", "forum": "dBDBg4WF4F", "replyto": "dBDBg4WF4F", "signatures": ["ICLR.cc/2026/Conference/Submission20632/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20632/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20632/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763466187700, "cdate": 1763466187700, "tmdate": 1763491976722, "mdate": 1763491976722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}