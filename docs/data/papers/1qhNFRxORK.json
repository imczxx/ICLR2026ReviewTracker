{"id": "1qhNFRxORK", "number": 5708, "cdate": 1757928249304, "mdate": 1759897959240, "content": {"title": "ACTIVA: Amortized Causal Effect Estimation via Variational Autoencoders", "abstract": "Predicting the distribution of outcomes under hypothetical interventions is crucial in healthcare, economics, and policy-making. However, existing methods often require restrictive assumptions, and are typically limited by the lack of amortization across problem instances. We propose ACTIVA, a transformer-based conditional variational autoencoder (VAE) architecture for amortized causal inference, which estimates interventional distributions directly from observational data. ACTIVA learns a latent representation conditioned on observational inputs and intervention queries, enabling zero-shot inference by amortizing causal knowledge from diverse training scenarios. We provide theoretical insights showing that ACTIVA predicts interventional distributions as mixtures over observationally equivalent causal models. Empirical evaluations on synthetic and semi-synthetic datasets validate our insights and show the effectiveness of our amortized approach, highlighting promising directions for future real-world applications.", "tldr": "Introduces ACTIVA, a transformer-based conditional VAE that enables zero-shot estimation of interventional distributions from observational data, amortizing across causal problems.", "keywords": ["causal inference", "variational autoencoder", "amortized inference", "zero-shot learning"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7016905eec817c836fe5f583238f8af7c88a0ffa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ACTIVA (Amortized Causal Effect Estimation via Variational Autoencoders), a Transformer-based conditional β-VAE framework designed to perform amortized causal inference. The goal is to learn a generalizable model that can estimate interventional distributions p(V∣do(X)) directly from observational data, enabling zero-shot causal reasoning on unseen causal models. Theoretical results claim that ACTIVA approximates a mixture over all Markov-equivalent causal models, and empirical results on synthetic and semi-synthetic datasets show improved metrics compared to conditional baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work tackles a timely and relevant problem: enabling amortized or foundation-style causal inference that generalizes across tasks.\n2. Empirical results suggest that the proposed method improves over simple conditional baselines in terms of distributional similarity metrics."}, "weaknesses": {"value": "1. The proposed method essentially combines a conditional VAE with a Transformer backbone for dataset-to-distribution mapping. The main innovation claimed “amortized causal inference” has been explored in prior works such as Neural Causal Models, MetaCausal Inference, and Do-PFN. The paper lacks a clear articulation of what distinguishes ACTIVA from these approaches, beyond architectural variations.\n2. The theoretical results (Propositions 1–2) mainly formalize that the learned distribution corresponds to a mixture over Markov-equivalent models. This is not new and does not directly establish identifiability or consistency guarantees. The connection between the ELBO optimization and causal identifiability remains mostly qualitative.\n3. The experiments are restricted to low-dimensional synthetic datasets. There is no evaluation on real or large-scale causal datasets, nor comparison to recent amortized or foundation causal inference baselines. As a result, the empirical validation is insufficient to support the ambitious claims of generalizable causal inference.\n4. The model outputs are distributions, but there is no qualitative or quantitative analysis of which causal mechanisms have been captured or whether the learned latent space aligns with meaningful causal factors. Without such interpretability checks, the claim that ACTIVA “amortizes causal reasoning” remains speculative."}, "questions": {"value": "See Weaknesses Part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2e70ytAZxJ", "forum": "1qhNFRxORK", "replyto": "1qhNFRxORK", "signatures": ["ICLR.cc/2026/Conference/Submission5708/Reviewer_4cjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5708/Reviewer_4cjR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479156403, "cdate": 1761479156403, "tmdate": 1762918209096, "mdate": 1762918209096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ACTIVA (Amortized Causal Inference via Variational Autoencoding), a framework for amortized estimation of interventional distributions. Given an observational dataset $D_M$ and a query intervention $do(V)$, it outputs an estimate of the post-intervention joint distribution $p(V\\mid do(V))$ . Experiments on synthetic Gaussian/Beta causal systems and semi-synthetic SERGIO data show improved fidelity to ground-truth interventional distributions compared to a conditional VAE baseline."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is clearly written and well-structured.\n- Assumptions, lemmas, and propositions are clearly stated.\n- Consistent improvements on all metrics (MMD, Wasserstein, Energy) across multiple data regimes, with good OOD generalization."}, "weaknesses": {"value": "- The amortization relies on paired interventional + observational data from a simulator / training distribution; real-world acquisition of such pairs may be costly, and generalization hinges on simulator-to-real match. \n- The focus is entirely on distributional distances and the empirical experiments lack evaluation on downstream causal quantities (e.g., ATE / ITE).\n- Limited statistical analysis as no confidence intervals / paired tests are provided."}, "questions": {"value": "1. How sensitive is ACTIVA to misspecification of the training model family ($p_{tr}(M)$)? \n2. How sensitive are results to the number of GMM components and mixture size in the decoder? Would increasing mixture components improve uncertainty calibration?\n3. Please include CIs / paired tests for Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5a0VleP57A", "forum": "1qhNFRxORK", "replyto": "1qhNFRxORK", "signatures": ["ICLR.cc/2026/Conference/Submission5708/Reviewer_hs82"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5708/Reviewer_hs82"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858823586, "cdate": 1761858823586, "tmdate": 1762918208816, "mdate": 1762918208816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for causal inference using variational autoencoders (VAEs), interpreting the prior and decoder distributions as a mixture over causal models. It claims to enable zero-shot transfer to unknown causal settings by training on a predefined class of identifiable models $\\mathcal{M}$, allowing inference on novel datasets from $\\mathcal{M}$ without prior knowledge of specific causal relations. The approach leverages strong identifiability assumptions (e.g., full model recovery and injectivity) and a mixture prior to amortize inference across instances. Theoretical justification is provided. Experiments are done on synthetic linear additive models and a semi-synthetic dataset"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The perspective of interpreting the prior and generative (decoder) distributions as a mixture over causal models is a conceptually appealing idea.\n- The work pursues an ambitious research direction: enabling zero-shot transfer to unknown causal settings, which could advance causal discovery and inference in generative models."}, "weaknesses": {"value": "**Overly Strong Assumptions That Trivialize the Problem**  \nThe core assumptions render the problem overly simplistic.  \n- Assumption 1 identifies the entire causal model, which far exceeds the identification of treatment effects (TE) alone. This is unnecessarily restrictive.  \n- Assumption 2 effectively sidesteps the central problem by presupposing full identifiability from the posterior. Identifiability in VAEs remains an open research area (see [1, 2] and \"Missing Related Work\" below). \n\n**Incorrect Theoretical Results and Derivations**  \n\n- Equation (4) posits that $V$ is independent of $D^M$ and $\\mathrm{do}(V)$ given $z$. Independence from $D^M$ stems from the strong requirement that the latent $z$ \"contains all the information needed to reconstruct the data,\" which should be explicitly stated as an assumption. However, independence from $\\mathrm{do}(V)$ is ill-formulated: intervened variables are, by definition, dependent on (and determined by) the intervention, typically modeled via a Dirac delta. All subsequent derivations must account for this. Moreover, implied by this independence, $z$ needs to block paths from intervened variables to their descendants, and in turn, $z$ would need to be (an invertible transform of) the intervened variables themselves; I cannot think of other scenarios for this to happen.  \n- **Lemma 1**: The proof is incoherent and circular, which invalidates most of the theoretical analysis thereafter. The \"two equivalent models\" introduced are identical; Markov equivalence classes (MECs) are irrelevant here. The claim that \"they represent the same model\" (presumably referring to the posteriors) is tautological, given the models' equality, and is unrelated to Assumption 2. The assertion that \"the two resulting distributions over $V$ must be the same\" lacks any logical connection to prior statements, and none of Assumptions 1–3 imply equal distributions. The proof resembles \"plausible nonsense,\" *potentially indicative of LLM-generated content*, warranting further review. Some other parts of the paper share a similar feel as can be seen in this review, although it is less apparent. \n\n**Unconvincing Justification for Zero-Shot Transfer to Unknown Causal Settings**  \n\n- **Methodological Issues**: How does the approach generalize outside of \"a pre-defined class of causal models\" $\\mathcal{M}$? First, to define a proper $\\mathcal{M}$ is already nontrivial, as identifiability (per Assumption 1) depends not just on graphs but on conditions like positivity (a.k.a overlap) and monotonicity (a kind of functional form). Does $\\mathcal{M}$ include only models identifiable from graphs alone? To date, this is limited to unconfounded settings satisfying the backdoor criterion. Even within $\\mathcal{M}$, different functional forms would prevent generalization.  \n- **Experimental Issues**: Results rely on a synthetic linear additive model and a semi-synthetic dataset, which are too narrow to demonstrate robustness. This fails to convincingly show that the method \"successfully recovers causal information at inference time even on novel instances\" or enables \"zero-shot transfer without knowing the causal relations.\"  \n- Relatedly, I am not sure how the “model can make inferences on datasets coming from the class of predefined causal models, even if the specific dataset was not in the training set.” How about different models generate highly non-overlapping data? This is not just a theoretical problem, because overlap conditions are inherently required for causal inference [3].\n\nThe emphasis on amortized inference as a key feature is underdeveloped. Phrases like \"amortization across problem instances\" and \"amortize over datasets from different causal models\" seem to merely describe variational inference with a mixture prior, without clarifying unique benefits in this causal context.\n\n**Confusing Notation and Symbolism**  \nNotation hampers readability and precision. For example:\n- The definition of $D^{\\mathrm{tr}}_j$ is unclear: \n   - since a single intervention is fixed across models and data, $\\mathrm{do}(V)$ should not appear in $D^{\\mathrm{tr}}_j$. \n   - The symbol $v^{M_{\\mathrm{do}(V)}}$ is ambiguous—why a single $n$? It likely intends a dataset, so define $D^{M_{\\mathrm{do}(V)}} := \\\\{v_k^{M_{\\mathrm{do}(V)}}\\\\}_{k=1}^K$. \n   - Since $j$ indexes models, use $M_j$ inside $D^{\\mathrm{tr}}_j$ consistently, not generic $M$.  \n- Line 215: Symbol $O$ is used without definition, which ties into the unsubstantiated zero-shot claims.\n\n**Other Issues**  \n- Assumption 4: The posterior should be absolutely continuous with respect to the prior (reverse the direction). Regardless, this is trivial and redundant: the posterior is a Dirac delta, and the prior is a mixture of Diracs. A real issue is why \"absolute continuity implies prior knowledge on which latents do not fall into the observational equivalence class of the input dataset.\"  \n- Line 66: Claiming one's own work offers \"rare fundamental insights\" is oddly self-congratulatory.\n\n**Missing (Proper Discussion of) Related Work**  \nThe paper overlooks key connections to identifiable VAEs and causal applications. Assumption 3's *injectivity* aligns with identifiable VAEs [1], extended to causal effects [2], which also addresses *prior/posterior degeneration to Diracs*.  And I cannot see how (Louizos et al., 2017; Vowels et al., 2021; Wu & Fukumizu, 2023 [2]) are \"without modeling the whole distribution\".\n\n---\n\n### References\n- [1] Khemakhem, Ilyes, et al. \"Variational autoencoders and nonlinear ICA: A unifying framework.\" *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020.  \n- [2] Wu, Pengzhou Abel, and Kenji Fukumizu. \"$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap.\" *International Conference on Learning Representations* (2022).  \n- [3] Rosenbaum, Paul R. \"Modern algorithms for matching in observational studies.\" *Annual Review of Statistics and Its Application* 7.1 (2020): 143-176."}, "questions": {"value": "Please refer to the points in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mredLRFWiP", "forum": "1qhNFRxORK", "replyto": "1qhNFRxORK", "signatures": ["ICLR.cc/2026/Conference/Submission5708/Reviewer_RehM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5708/Reviewer_RehM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762265625601, "cdate": 1762265625601, "tmdate": 1763018272545, "mdate": 1763018272545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm for zero-shot prediction of interventional distributions given observational data as input. For this purpose, they utilize a transformer-based conditional VAE and use both observational and interventional data along with the corresponding causal query as input during training. They provide the theoretical insight behind their capability of amortized causal inference. Finally, the authors provide experimental results on synthetic and semi-synthetic datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a solution to a very interesting and important problem. It is also written in a very intuitive way. I appreciate how they precisely mentioned the assumptions and wrote the proofs for their theoretical statements."}, "weaknesses": {"value": "Below I share my comments:\n\n## **Major:**\n\n1. It is unclear what the proposed method is generalizing over. Can it generalize to  \n   i) arbitrary observational distributions,  ii) arbitrary causal graphs,     iii) arbitrary interventions?\n\n2. **(i) Arbitrary observational distributions:**  What if the model was trained on observational datasets $D^M$ with noises being only Gaussian, however, during inference, the observational datasets $D^M$ have different noise distributions?\n\n3. **(ii) Arbitrary causal graphs:**  Does the method not need any assumption on the graph $G$? In line 461, the authors mentioned that the need for graphical assumptions is removed.  \n   We know that based on the causal graph, the causal effect might be different for the same query. How does the model deal with such a scenario?  \n\n   Suppose, for a 3-node graph of $X, Z, Y$, the model was not trained on any dataset which represented the $X \\rightarrow Z$ edge, but during inference time we give a dataset as input which has $X \\rightarrow Z$ dependency. The model should not be able to generalize in such a scenario unless the model was trained on datasets generated from all possible causal graphs, which would be exponentially costly.\n\n4. **(iii) Arbitrary interventions:**  In a causal model with $d$ variables, $d$ single interventions are possible. Do they consider $d$ interventional datasets?  Do the authors not need any assumption on how many interventions they are considering?  How does the model generalize to higher support size?  \n\n   More precisely, suppose during training the model was given interventional datasets with $\\text{do}(X=0), \\ldots, \\text{do}(X=8)$, but during inference, we give a query $\\text{do}(X=9)$. Will the model generalize in such a scenario?  \n\n   Also, suppose during training, we gave it $\\text{do}(V_1), \\text{do}(V_2), \\ldots, \\text{do}(V_{n-1})$, i.e., $n-1$ different interventional datasets. During inference, we query $\\text{do}(V_n)$ or $\\text{do}(V_2, V_3, V_4, V_5)$, i.e., interventions on a new set of variables unseen in the training data. It should not generalize, as interventions/treatments on multiple variables might interact with each other and give a different output, unless the model was trained on all possible interventional datasets.\n\n5. According to lines 261 and 272, for just one combination of intervention (e.g., $\\text{do}(X=0)$), the authors need a dataset of size $N \\times d \\times |I| + 1$.   What interventional datasets does the algorithm need as input for each of the above cases?  I understand we need at least the observational data $D$ sampled from $P(\\mathbf{V})$.\n\n6. More baselines are needed. The authors compared with the conditional model baseline. We understand that the method performs better than the baseline, but are the errors obtained by the algorithm low enough to be considered acceptable? This is not clear.  \n   They should compare their performance with existing algorithms for causal effect estimators that take the observational data, the causal graph, the query and gives a causal effect prediction. For example: Shpitser, Ilya, and Judea Pearl. \"Complete identification methods for the causal hierarchy.\" (2008).\n\n---\n\n## **Minor:**\n\n1. This paper considers no unobserved confounder, i.e., all exogenous noises are independent. This needs to be stated more precisely.\n\n2. Some end-to-end diagram of the algorithm would be helpful for readers to connect the whole algorithm. For example, how are the $\\lambda$, $\\phi$, and $\\eta$ parameterized models arranged?"}, "questions": {"value": "Below I share my questions:\n\n1. How does the proposed approach work for multiple interventions during inference?  \n2. For two variables case, conditioning and intervening should be the same. In Table 1, why is the error so high for the conditional baseline in the two-variable case?  \n3. How do the 2-variable ($X \\rightarrow Y$ or $Y \\rightarrow X$) and 8-variable graphs look like?  \n4. Is there any possibility that the model $q_{\\phi}$ just ignores $\\mathbf{V}$? When might that happen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9fCCSzgdS", "forum": "1qhNFRxORK", "replyto": "1qhNFRxORK", "signatures": ["ICLR.cc/2026/Conference/Submission5708/Reviewer_U8Ab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5708/Reviewer_U8Ab"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762747024181, "cdate": 1762747024181, "tmdate": 1762918208340, "mdate": 1762918208340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}