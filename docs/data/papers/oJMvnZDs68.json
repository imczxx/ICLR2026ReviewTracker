{"id": "oJMvnZDs68", "number": 16323, "cdate": 1758263123307, "mdate": 1759897247591, "content": {"title": "PRESTO: A Framework for Orchestrating System States and Test Cases for Bash Script Verification", "abstract": "Bash is a widely used scripting language for automating system and cloud tasks, but its reliance on implicit preconditions—such as environment variables, file paths, and tool availability—makes it error-prone, especially when scripts are generated by large language models (LLMs). While LLMs have demonstrated promising capabilities in translating natural language to Bash scripts, the lack of reliable evaluation methods and test coverage hampers their practical utility. We introduce PRESTO, a modular framework for Precondition-aware Script Testing and Orchestration, designed to assess and refine Bash scripts through execution-driven feedback loops. PRESTO automatically infers required preconditions, synthesizes minimal reproducible environments, generates targeted test cases, and evaluates the behavior of both LLM-generated and human-authored Bash scripts in a sandboxed execution environment. Upon failure, an iterative refinement cycle—driven by LLMs—updates the script, environment setup, or test harness until correctness is restored. Our experiments on two public benchmarks show that PRESTO significantly improves correctness, debugging efficiency and reliability compared to static or heuristic methods. Unlike reference-based metrics, PRESTO operates without requiring gold-standard references, making it suitable for real-world deployment scenarios. This positions PRESTO as a practical solution for production-ready script generation.", "tldr": "", "keywords": ["Bash Script Generation", "Code Verification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db14fdcdff4a24ef4852272eb7cdd4bf42dabfa9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper targets Bash, which is particularly bad compared to other programming\nlanguages. However, it addresses a very general problem: suppose we have a\nprompt for a programming task that we want to use for evaluation (or for RL). We\ncan easily get a model to output a solution, but it takes a lot of labor to\ncreate an execution environment to run the solution and to write test cases to\nevaluate that the solution is correct. The paper attacks this problem by\nbuilding an LLM agent-based system that builds both the execution environment\nand the test suite.\n\nThe system uses feedback to refine the generated code and test suite, which has\nan obvious threat to validity: when a failure occurs, it could be because the\nenvironment is wrong (bad precondition), the test suite is wrong (bad\npostcondition), or the solution is wrong (bad code). The paper takes steps\nto address this so that bugs in the synthesized code do not leak -- i.e., so\nthat the model doesn't trivialize the test cases to make them pass."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A really important problem\n- Likely much larger scope than just Bash (and there are some Python experiments\n  in the paper)"}, "weaknesses": {"value": "The primary weakness with this paper is that it generates a *single environment*\nand *single test script* for each task. (The paper tries to acknowledge this\nin Example 4. I assume the ground-truth script is \"rm -rf dir\".)\n\nWe can think of any bash script as a function F, defined as:\n\n    F(fs, env, stdin) = (fs', stdout, stderr)\n\nThe limitation of the paper is that it generates a single input tuple `(fs, env,\nstdin)` as the environment. The paper shows an example (Example 4), where the\nsynthesized filesystem fs allows a buggy script to trivially pass the\nsynthesized test script -- which is a correct test suite.\n\nBut, there is a deeper problem, which is that the task may have a logical\ndisjunction. For example, consider the following task, which is close\nto something I regularly do:\n\n> If the hostname is my laptop, load the model from $HOME/models/model_name.\n> Otherwise, if the hostname is the cluster, load the module from \n> /nfs-shared/my-research-group/models/model_name.\n\nTo test that a script implements the behavior described above correctly, it is\nnot enough to strengthen a test input. Instead, we genuinely must test the\nscript on two different inputs -- one that simulates the laptop and one that\nsimulates the cluster.\n\nTo address this, the system in the paper would have to be architected quite\ndifferently. The agent would need to plan out several different test\nenvironments simultaneously. In this alternative design, when a failure occurs,\nthe system would have to consider both strengthening the environment (which it\nalready does) as well as forking a new test environment (which it does not do).\n\nThis is a big change, but I think it is necessary to make the system more\nrobust.\n\nMinor:\n\n- The Arxiv URL for InverseCoder points to the wrong paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EU5Vlf35LP", "forum": "oJMvnZDs68", "replyto": "oJMvnZDs68", "signatures": ["ICLR.cc/2026/Conference/Submission16323/Reviewer_Q5e3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16323/Reviewer_Q5e3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923810042, "cdate": 1761923810042, "tmdate": 1762926461827, "mdate": 1762926461827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRESTO, a framework for the reference-less evaluation of Bash scripts. The authors argue that script verification, especially for LLM-generated code, is fundamentally a problem of state and environment, not just code. PRESTO uses LLMs to generate and iteratively refine two key components from a natural language task: (1) an environmental prerequisite script (P) and (2) a test case script (T). The framework's core novelty is its execution-driven refinement loop, which stabilizes P and T independently of the main script (M) being evaluated. This decoupled design prevents \"evaluation leakage\" where the environment or test might overfit to a buggy script. Experiments on two benchmarks show PRESTO's evaluation F1-score outperforms execution-less and existing execution-based baselines, and that its feedback signal is uniquely effective for downstream script refinement."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a critical and deceptively difficult problem. While much of the community focuses on self-contained, algorithmic code generation (e.g., MBPP, HumanEval), this work correctly identifies that system-level scripts (like Bash) present a distinct and harder challenge due to their profound state-dependency. Addressing the \"implicit prerequisite\" problem is a novel and valuable contribution.\n\nThe core design choice to decompose the problem into Prerequisite (P), Test Case (T), and Main Script (M) is strong. The decision to generate and refine P and T in isolation from M is the paper's strongest methodological contribution. This separation wisely prevents the test harness from being overfit to a specific, potentially buggy, implementation (M), ensuring that P and T remain faithful to the original task specification (S).\n\n The framework's ability to operate without gold-standard reference scripts is a significant practical advantage. In real-world deployment for SRE or DevOps, such references are non-existent. PRESTO's approach of synthesizing the evaluation criteria (P and T) from the task description is the only scalable path forward.\n\nValue in an Agent-driven World: The framework's value is particularly clear in the context of modern interactive coding agents (e.g., \"Cursor-like\" tools). While such agents can interact with an environment via trial-and-error, they often lack a systematic oracle for defining correctness. PRESTO provides exactly this: it automates the generation of a minimal, reproducible environment (P) and a formal test (T) that together define the correctness criteria for a given task. This shifts the paradigm from human-guided, trial-and-error debugging to automated, systematic verification. PRESTO's output could, in principle, serve as the \"scaffolding\" for an interactive agent, providing it with a robust sandbox and a success condition."}, "weaknesses": {"value": "The framework's primary weakness (Examples 4 and 5), is its total reliance on the LLM's \"imagination\" to generate comprehensive prerequisites and test cases. The evaluation is only as good as the generated tests. A \"Coverage Gap\" (Example 4) is a critical failure mode: if the LLM fails to generate prerequisites and tests for relevant edge cases (e.g., empty files, non-existent directories, files with spaces, mixed file/directory scenarios), the evaluation will produce false positives. This is a fundamental limitation.\n\nSimilarly, the \"Semantic Mismatch\" (Example 5) is a critical flaw. If the LLM misunderstands a domain-specific term (e.g., \"system group\" vs. \"group\"), the entire evaluation framework will be verifying the wrong behavior. The paper does not propose a clear mitigation for these two key failure modes, which are inherent to the LLM-as-generator paradigm.\n\n \nThe extremely poor performance of the adapted AgentCoder baseline (e.g., 31% accuracy with GPT-4o on NL2Bash-EABench) is suspicious. a simpler baseline is to \"just run the bash and take the feedback to LLM\" - a simple self-debug scaffold, which AgentCoder does not have? I wonder how sota LLM performs. While the authors attribute this to its lack of prerequisite-awareness, the performance is so low that it raises questions about the fairness and fidelity of the adaptation from its original Python domain to Bash.  \n\nGiven that coverage gaps and semantic ambiguity are the primary failure modes, what extensions to PRESTO are envisioned?"}, "questions": {"value": "do you think with the increasing coding ability of LLM, the problem you are tackling will no longer be valid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qF3KZ5XJHb", "forum": "oJMvnZDs68", "replyto": "oJMvnZDs68", "signatures": ["ICLR.cc/2026/Conference/Submission16323/Reviewer_XrMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16323/Reviewer_XrMa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955417587, "cdate": 1761955417587, "tmdate": 1762926460870, "mdate": 1762926460870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PRESTO, a prerequisite-aware framework to evaluate and refine Bash scripts generated from natural-language tasks. PRESTO includes (i) environment prerequisite setup, (ii) test-case generation, and (iii) iterative error-attribution and refinement. A key design choice is that prerequisite and test planning never see the main script, reducing leakage and overfitting to a particular implementation.\n\nExperiments on NL2Bash-EABench and InterCode-Corrections compare PRESTO against execution-less evaluators (ICE-Score, Direct Grading) and an execution-based evaluator, AgentCoder. PRESTO achieves high accuracy and macro-F1 across three LLMs and two benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Methodology novelty:**  generation of both prerequisite and tests from the task without seeing the main script. \n- **PRESTO Performance**: better macro-F1 than baselines across datasets and models. \n- **Case Studies:** highlight cases where PRESTO’s refinement corrects missing prerequisites or brittle tests, as well as failure cases."}, "weaknesses": {"value": "- **Missing Results:** \n  - In Table 1, there is no consistent number of decimal points, which makes comparison harder.  \n  - Report the full table for prerequisite generation performance across models [RQ2].\n- **Scalability.** What is the cost / overhead of iterative refinement step? Please add runtime and token accounting.\n- **Dependence on LLM-as-judge.** For InterCode-Corrections, using LLM-as-a-Judge for evaluation may mean some “ground truths” are model-derived, not purely executional. This may affect claims about absolute correctness\n- **Presentation**\n  - Citation format is not consistent, especially in introduction and related works. \n  - Typo: testcase $\\to$ test case\n  - The two paragraphs that start with \"Important design note:\" in Section 3.2 disrupt the flow.\n  - Figure 2 is not readbable due to its font size."}, "questions": {"value": "- How do results change if you use different LLMs for the generator and evaluator? \n- Do you use the same LLM for generation and evaluation? It may cause self-confirmation effects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EiQTapoD5J", "forum": "oJMvnZDs68", "replyto": "oJMvnZDs68", "signatures": ["ICLR.cc/2026/Conference/Submission16323/Reviewer_D1A9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16323/Reviewer_D1A9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993433376, "cdate": 1761993433376, "tmdate": 1762926459995, "mdate": 1762926459995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRESTO, a modular framework designed to evaluate and refine Bash scripts generated by llms. The core problem it addresses is the unreliability of existing evaluation methods for system-level scripts, which depend heavily on implicit environmental prerequisites. PRESTO's novelty lies in its prerequisite-aware approach: it automatically infers required preconditions, synthesizes minimal environments, generates targeted test cases, and executes an iterative refinement loop—all without relying on gold-standard reference scripts. The framework is evaluated on two NL2Bash benchmarks, showing improvements in evaluation accuracy and enabling more effective script refinement compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies the challenge of reliably evaluating and refining system-level scripts where execution context is paramount.\n2. The key design choice of separating the generation of prerequisites and test cases from the main script is crucial. This ensures that prerequisites and test cases validate the task specification rather than overfitting to a potentially buggy implementation.\n3. The paper provides a thorough experimental evaluation on two complementary benchmarks.\n4. The analysis of failure modes in RQ5 is particularly valuable. It provides transparency about the framework's limitations and offers clear directions for future work."}, "weaknesses": {"value": "1. The empirical validation, while solid, is conducted on two relatively small benchmarks. Broader assessment, for instance, on a larger set of language models or more diverse languages, would strengthen the claims of generalizability.\n2. The iterative refinement process is inherently more computationally expensive than one-shot methods. A discussion of the computational trade-offs is needed for a complete picture of the framework's practicality.\n3. The observation that the \"Direct Grading\" baseline performs competitively on the NL2Bash-EABench benchmark is notable but under-analyzed. A deeper investigation into why this execution-free method works deceptively well on certain tasks but fails on harder ones would provide valuable insights into the problem's nature and the necessity of PRESTO's approach."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtVkl7DPis", "forum": "oJMvnZDs68", "replyto": "oJMvnZDs68", "signatures": ["ICLR.cc/2026/Conference/Submission16323/Reviewer_7jC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16323/Reviewer_7jC2"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16323/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993574111, "cdate": 1761993574111, "tmdate": 1762926459174, "mdate": 1762926459174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}