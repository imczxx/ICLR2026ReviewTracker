{"id": "Vj1ZEBsStT", "number": 19295, "cdate": 1758295131419, "mdate": 1759897047269, "content": {"title": "Echo Flow Networks with Infinite-Horizon Memory", "abstract": "At the heart of time-series forecasting (TSF) lies a fundamental challenge: how\ncan models efficiently and effectively capture long-range temporal dependencies\nacross ever-growing sequences? While deep learning has brought notable progress,\nconventional architectures often face a trade-off between computational complexity\nand their ability to retain accumulative information over extended horizons.\n\nEcho State Networks (ESNs), a class of reservoir computing models, have recently\nregained attention for their exceptional efficiency, offering constant memory usage\nand per-step training complexity regardless of input length. This makes them\nparticularly attractive for modeling extremely long-term event history in TSF.\nHowever, traditional ESNs fall short of state-of-the-art performance due to their\nlimited nonlinear capacity, which constrains both their expressiveness and stability.\n\nWe introduce ECHO FLOW NETWORKS (EFNS), a framework composed of a\ngroup of extended Echo State Networks (X-ESNs) with MLP readouts, enhanced\nby our novel Matrix-Gated Composite Random Activation (MCRA), which en-\nables complex, neuron-specific temporal dynamics, significantly expanding the\nnetwork’s representational capacity without compromising computational effi-\nciency. In addition, we propose a dual-stream architecture in which recent input\nhistory dynamically selects signature reservoir features from an infinite-horizon\nmemory, leading to improved prediction accuracy and long-term stability.\n\nExtensive evaluations on five benchmarks demonstrate that EFNS achieves up\nto 4× faster training and 3× smaller model size compared to leading methods like\nPatchTST, reducing forecasting error from 43% to 35%, a 20% relative improve-\nment. One instantiation of our framework, EchoFormer, consistently achieves\nnew state-of-the-art performance across five benchmark datasets: ETTh, ETTm,\nDMV, Weather, and Air Quality.", "tldr": "We introduce Echo Flow Networks (EFNs), which extends classical Echo State Networks by modeling infinite memory.", "keywords": ["reservoir computing; long-term memory; echo state networks; Transformer"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/262685005a637392283739840a9303ceb1a9662d.pdf", "supplementary_material": "/attachment/d203ffba5498a12c37f601c8d9ca50faf994c056.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Echo Flow Networks (EFNs), a framework that extends Echo State Networks (ESNs) for time-series forecasting by combining the efficiency of reservoir computing with the expressive power of deep models. EFNs introduce a novel Matrix-Gated Composite Random Activation (MCRA) for richer nonlinear dynamics, use ensembles of heterogeneous reservoirs to improve stability, and employ a dual-stream architecture that fuses infinite-horizon memory with short-term attention-based models. Experiments on several benchmarks show that EFNs, and especially the EchoFormer variant, achieve state-of-the-art accuracy with up to 4× faster training and significantly lower forecasting error than existing Transformer and MLP-based methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem in time-series forecasting, i.e., capturing long-term dependencies efficiently, and explores the potential of Echo State Networks within modern architectures. The idea is ambitious and the experimental comparisons suggest promising performance gains."}, "weaknesses": {"value": "The paper is extremely confusing and poorly structured. It mixes many unrelated components and ideas without a clear logical flow, making it difficult to follow both the technical content and the overall narrative. Figures, tables, and formulas are often presented without sufficient explanation or coherence.\n\nMost critically, several cited references are **non-existent**, indicating the presence of LLM-generated hallucinations. For example:\n\n- et al. Chi. Tpgn: The rnn’s new successor for long-range time series forecasting. ArXiv preprint arXiv:2401.xxxxx, 2024.\n- Michiel Hermans and Benjamin Schrauwen. Training readout in reservoir computing with kernel methods. In ESANN, 2010.\n- et al. Huang. Tide: Time-series decomposition enhanced transformer. ArXiv preprint arXiv:2305.xxxxx, 2023.\n- Y. Li and X. Wang. Enhancing time series transformers with frozen encoders. Proceedings of ..., 2022.\n- et al. Liao. Transportation flow prediction based on graph attention echo state network. In CNIOT 2023: 4th Int’l Conf. on Computing, Networks and IoT, 2023.\n- et al. Nie. Patchtst: Time-series transformer with patch aggregation. In ICML, 2023.\n- X. Song, Y. Li, and M. Wang. Patchtsmixer: Efficient time series forecasting with patch-based mixer. arXiv preprint arXiv:2301.XXXX, 2023.\n- et al. Wang. Film: Frequency-integrated linear modeling for time series forecasting. ArXiv preprint arXiv:2306.xxxxx, 2023.\n- et al. Wu, Z. Temporal pattern graph network for time series forecasting. NeurIPS, 2023a.\n- et al. Wu, Z. Timegpt: Foundation models for time series forecasting. arXiv preprint arXiv:2303.XXXX, 2023b.\n- et al. Zhou. Basisformer: A transformer with basis decomposition for time series. ArXiv preprint arXiv:2307.xxxxx, 2023.\n\nThis, among other things, strongly suggests that the paper was written or heavily assisted by a LLM without proper verification by the authors. Submitting such a manuscript without checking or disclosing the use of an LLM constitutes irresponsible scientific behavior, and the authors should be flagged for this misconduct."}, "questions": {"value": "N/a"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"]}, "details_of_ethics_concerns": {"value": "The paper seems to be written by an LLM and not being checked by the authors. It contains several hallucinations and incorrect statements."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Kxw8v1D6ep", "forum": "Vj1ZEBsStT", "replyto": "Vj1ZEBsStT", "signatures": ["ICLR.cc/2026/Conference/Submission19295/Reviewer_wBr2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19295/Reviewer_wBr2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760618939240, "cdate": 1760618939240, "tmdate": 1762931248176, "mdate": 1762931248176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EFNS, a time series forecasting framework which combines Extended Echo State Networks (X-ESNs) with Multi-Layer Perceptrons (MLP) for readouts. The framework aims to efficiently capture long-range temporal dependencies in time series data. By incorporating a  Matrix-Gated Composite Random Activation (MCRA) mechanism, EFNS enhances the model's expressive power while maintaining computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The EFNS framework combines the efficiency of traditional ESNs with the expressive power of modern deep learning models, which is intriguing; however, the combination of ESNs and deep learning is not new. For example, \n\n> Evanusa M S, Patil V, Girvan M, et al. t-convesn: Temporal convolution-readout for random recurrent neural networks[C]//International Conference on Artificial Neural Networks. Cham: Springer Nature Switzerland, 2023: 140-151.\n\n> Ma Q, Shen L, Chen E, et al. WALKING WALKing walking: Action Recognition from Action Echoes[C]//IJCAI. 2017: 2457-2463."}, "weaknesses": {"value": "- This work is very similar in modeling approach to the following paper, including the idea and the learning function used for optimization. However, it does not provide explanations or citations.\n> Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer\" (arXiv:2402.09573v2, 2024).\n- Moreover, many of the references in this paper do not actually exist. It appears that the references are generated by GPT."}, "questions": {"value": "In Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "Main concerns: \n- This work is very similar to the following paper, including the idea and the loss function for optimization. However, it does not provide explanations or citations.\n> Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer\" (arXiv:2402.09573v2, 2024).\n- Moreover, many of the references in this paper do not actually exist. It appears that the references are generated by GPT.\n\nOthers:\n- The experimental section reports results but lacks an in-depth analysis of the reasons behind the observed performance improvements. Given that many datasets (e.g., Traffic) have been extensively studied, achieving significant accuracy gains without additional guiding information seems impossible. Authors should provide more insights into the reasons for performance improvements, potential inconsistencies in settings, and comparisons with existing works to enhance the results' persuasiveness.\n- The reasoning behind the settings for input scaling and other hyperparameters for multivariate time series appears to be inadequately justified. Although Table 6 provides some information, it would be beneficial for the authors to elaborate on the rationale behind these settings and how they adjust for different variables to improve the model's adaptability to multivariate data.\n- There are many formatting errors in the reference."}}, "id": "OyfDneE37f", "forum": "Vj1ZEBsStT", "replyto": "Vj1ZEBsStT", "signatures": ["ICLR.cc/2026/Conference/Submission19295/Reviewer_vZAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19295/Reviewer_vZAZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012429535, "cdate": 1762012429535, "tmdate": 1762931247832, "mdate": 1762931247832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, authors develop a framework combined of echo state networks, matrix-gated composite random activation for time series forecasting. Experiments should that it has a competitive performance compared to some state-of-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation of matrix-gated composite random activation is comprehensive and sound. \n2. This framework can be combined with many other TSF models, indicating its flexibility."}, "weaknesses": {"value": "1. This model lacks comparisons with some advanced baselines, such as itransformer and timemixer.\n2. For ablation study, only the result of one dataset is displayed, and it might require more abaltion studies in different dataset. \n3. Why ESNs for TSF? This paper lacks theoretical discussion about why ESNs is an ideal tool for TSF.\n4. Since computational efficiency is another contribution, some results related to inference time can also be discussed.\n5. Figure 2 is a little bit ambiguous, could authors explain why they show figure 2 in this paper?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zCS9HdKkrI", "forum": "Vj1ZEBsStT", "replyto": "Vj1ZEBsStT", "signatures": ["ICLR.cc/2026/Conference/Submission19295/Reviewer_Dhrn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19295/Reviewer_Dhrn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19295/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762116705854, "cdate": 1762116705854, "tmdate": 1762931247446, "mdate": 1762931247446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}