{"id": "3O8TAbrMKW", "number": 24710, "cdate": 1758359561792, "mdate": 1759896753413, "content": {"title": "Catalyst: Reveal the Geometry of Pruning by Reshaping Neural Network", "abstract": "Structured pruning aims to reduce the computational cost of neural networks by removing entire filters or channels, but conventional regularization-based approaches suffer from unstable pruning dynamics and magnitude bias.\nIn particular, commonly-used regularizers such as L1 and Group Lasso exhibit trivial global minima and fail to align with the geometry of pruning-invariant configurations, leading to a tradeoff between sparsification and model integrity.\nWe propose Catalyst, a novel regularization framework for structured pruning grounded in extended-space optimization and rigorous landscape geometry. Catalyst introduces auxiliary variables to reshape the loss landscape, admitting a nontrivial global minimizer which aligns to the pruning-invariant set, where pruning decisions are lossless by construction. This formulation enables strong regularization without collapsing the model, and induces robust bifurcation dynamics that separate filters into prune-or-preserve groups with wide decision margins.\nWe provide theoretical analysis of the optimization geometry and bifurcation behavior, and demonstrate empirically that Catalyst achieves stable, magnitude-invariant pruning with superior performance across benchmarks. Our work establishes a principled foundation for structured pruning through geometric regularization and extended-space dynamics.", "tldr": "", "keywords": ["Structured pruning", "regularization", "model compression"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e42ffbe42bb89f2912f7eaaa3bbe48d47c8aebd1.pdf", "supplementary_material": "/attachment/ae52b2e97869d808c607ffb382989f9164817bff.zip"}, "replies": [{"content": {"summary": {"value": "Motivated by the weakness of L1 regularization and group lasso, this paper characterizes the lossless pruning via geometry and proposes a regularization object whose global minimum is non-trivial and corresponds to lossless pruning. They propose catalyst pruning based on the bypassing algo.  The model will be pruned after $\\|DW\\|_{2,1}$ is small enough. Experiments show that the training of catalyst pruning is more smooth than L1/Group Lasso, and the performance of fine-tuning outperforms some existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The clear algebraic conditions for \"lossless pruning\" are given from a geometric perspective ($DW=0$) and the regularization construction of the extended space ($\\|DW\\|_{2,1}$), with novel ideas and clear expressions.\n\n2. $c=D_{ii}/\\|F\\|$ naturally results in pruning decisions with wide intervals and no bias; the results are competitive with some existing methods."}, "weaknesses": {"value": "1. The method is ‚Äúlossless‚Äù only at the pruning step, whatever the test acc is. In Table 1, several settings still experience sizeable accuracy recovery at finetuning stage, e.g., ResNet-50 on ImageNet shows post-prune accuracy of 65.13 and finetuning acc of 76.04; DeiT-Tiny also exhibits 62.97 before recovery(71.41). This suggests the lossless guarantee is fragile in practice, the algorithm of the paper can indeed ensure that after training, pruning will not cause loss degradation, but the performance at this time cannot be guaranteed (for example, I can prune at a low performance, and lossless is relatively easy to achieve at this level). Retraining is still required to recover performance matching other algorithms. Therefore, although the theoretical part of the paper gives seemingly powerful results, its actual results not seem to require lossless, as the finetuning stage is necessary. Thus, another question arises: Is lossless pruning necessary? It is obvious that even if the loss is damaged, I can recover it through finetuning.\n\n2. I have some questions about motivation, the paper is stated to address two problems, one is that L1/Group Lasso paradigms are small but important filters are pruned and the other is that decision boundaries are fragile. Is the former a flaw specific to L1/Group Lasso? If so, can addressing this flaw do something that other algorithms can't? As far as I know, there are many algorithms for structured pruning, and L1/Group Lasso is not the only solution. I appreciate the author's efforts on the problem of fragile decision boundaries, so I would like to get more explanations from the author about motivation.\n\n3. I'm very curious about the choice of hyperparameters, the hyperparameters of common algorithms generally guarantee performance after training, the paper claims that their regularization is harmless to performance, but the experimental results prove that the performance of the trained model decreases significantly, and finetuning is needed to recover it. This indicates nontrivial sensitivity to hyperparameters and compute budget, but a systematic robustness study is missing.\n\n4. For DeiT-Tiny, the paper restricts comparisons to _neuron-level_ pruning and explicitly notes that other prunable targets (embed dimension, attention heads) are left out. This narrows applicability for transformers, where head/token/embed pruning is common. Moreover, comparisons are grouped by ‚Äúsimilar speedups,‚Äù but FLOPs/params/latency trade-offs and hardware-aware speed are not jointly normalized or reported as alternative axes, which could bias comparisons across methods with different effects.\n\n5. No generalization/error-propagation bounds linking $\\|DW\\|_{2,1}$‚Äã to post-prune risk, this weakness is related to weakness 1.\n\n6. The method argues that decisions are independent of initial filter magnitudes. I'm wondering that is this effect robust under reparameterization? A notable problem with magnitude pruning is that magnitude can be changed significantly by reparameterization while without changing its importance, which is the core reason for the shortcomings of the L1/Group Lasso, did the paper's algorithm solve this problem? Clarifying issues like this in the text would be a significant improvement in quality.\n\n7. The algorithms compared in the paper are still not comprehensive enough, as far as I know there are many powerful algorithms, such as HRank, CHIP, FPAC, HBFP, SPSRC, HALP, MDP, IPPRO, etc. Since there are many algorithms, I have not made a complete list, I would like to ask the authors to choose the algorithms according to the actual to supplement, and give a comprehensive description of the related work, in addition, I would like to respectfully ask the authors to provide the performance without finetuning, many algorithms for structured pruning, as far as I know, the comparison has always been the performance before finetuning because finetuning significantly changes the performance of the model after pruning, at least in their claims, it is not fair to compare the performance after finetuning."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8HHrecvQqs", "forum": "3O8TAbrMKW", "replyto": "3O8TAbrMKW", "signatures": ["ICLR.cc/2026/Conference/Submission24710/Reviewer_mLQT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24710/Reviewer_mLQT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633226013, "cdate": 1761633226013, "tmdate": 1762943170489, "mdate": 1762943170489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Catalyst, a new regularization framework for structured pruning of neural networks. Traditional regularizers such as L1 and Group Lasso often suffer from bias and unstable pruning behavior. Catalyst introduces an auxiliary set of ‚Äúcatalyst‚Äù parameters that extend the optimization space and aim to enable unbiased and lossless pruning. The method is implemented through a modified optimization process and evaluated on CIFAR and ImageNet benchmarks. Experiments show results that are comparable or slightly better than existing pruning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The introduction of auxiliary ‚Äúcatalyst‚Äù parameters represents a conceptually novel angle compared to conventional sparsity-inducing regularizers. It attempts to provide a more principled view of pruning rather than relying on heuristic thresholding.\n\n(2) The authors provide some theoretical justification for the proposed formulation, which could potentially inspire further research on lossless or function-preserving pruning.\n\n(3) The experimental scope covers both CIFAR and ImageNet, including standard architectures such as ResNet and VGG. The visualization of parameter bifurcation offers a qualitative intuition for how the regularizer affects model structure."}, "weaknesses": {"value": "(1) The overall readability of the paper needs improvement, especially in the method section. Many implementation details are not clearly described or are scattered across different parts, which makes it difficult for readers to grasp the full picture of the algorithm. It would be helpful if the authors could include a concise pseudocode or an overview figure to illustrate the workflow. The formulation of the optimization problem is also incomplete, and the paper should explicitly define all optimization variables.\n\n(2) The improvements over existing baselines are rather limited. The claimed advantage of the proposed regularizer is not well supported by the quantitative results. In Figure 4, it seems that the method requires significantly longer training to slightly outperform simpler existing approaches.\n\n(3) There are also concerns about fairness and comparability in the experimental design. For example, the two-stage training and pruning procedure does not follow standard practice and may introduce additional computational cost. The authors should provide clearer explanations of the experimental settings and ablation studies. The reviewer is familiar with DepGraph and its subsequent works, which adopt more flexible pruning mechanisms that address more general pruning problems rather than only removing entire filters. Compared with those approaches, the Catalyst framework appears more restricted and is expected to"}, "questions": {"value": "(1) The authors emphasize the importance of achieving lossless pruning. However, according to the results shown in Figure 4, even when pruning leads to severe degradation, the model can still recover well after fine-tuning. This raises the question of whether enforcing the lossless pruning objective is truly necessary or meaningful for practical model compression.\n\n(2) From Table 2, we observe that accuracy and speedup do not seem to improve simultaneously. In most cases, the trade-off remains close to the baseline, and the proposed method does not clearly dominate in either metric. The authors are encouraged to provide further explanation for why such results occur and whether this limitation is inherent to the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pBJCZGvFtX", "forum": "3O8TAbrMKW", "replyto": "3O8TAbrMKW", "signatures": ["ICLR.cc/2026/Conference/Submission24710/Reviewer_xP2x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24710/Reviewer_xP2x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704684557, "cdate": 1761704684557, "tmdate": 1762943169667, "mdate": 1762943169667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a regularization scheme for structured pruning that introduces auxiliary ‚Äúcatalyst‚Äù variables to extend the parameter space. The method is motivated by a geometric analysis of lossless pruning conditions and is implemented in a modified bypass manner. It is claimed that Catalyst achieves magnitude-independent pruning decisions, robust bifurcation behavior, and lossless pruning with theoretical support. Empirical evaluations on CIFAR and ImageNet show competitive pruning performance compared to several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides an interesting and theoretically informed approach to structured pruning by reframing it through an extended parameter space. The introduction of catalyst variables enables a more flexible optimization landscape, which helps the pruning process evolve smoothly rather than collapsing abruptly when certain weights are removed. The geometric intuition behind the design gives the method a clear motivation and differentiates it from purely empirical pruning heuristics. The presented results show that the model maintains stable accuracy even under aggressive pruning, suggesting that the proposed formulation effectively preserves representational capacity. The inclusion of both CIFAR and ImageNet experiments further supports the generality of the approach."}, "weaknesses": {"value": "- As the authors note, this work is closely related to the framework of Jung & Lee (2024). The paper should more carefully articulate the essential differences and additional theoretical value that distinguish the proposed method from the original framework.\n\n- The experimental results show relatively modest gains. In many cases, Catalyst only slightly outperforms existing methods under long training schedules or specific settings, and the overall improvement may not be sufficient to justify the added complexity."}, "questions": {"value": "- It would be helpful to clarify how Catalyst would generalize to other task types and architectures, such as detection, segmentation, or recurrent models like LSTMs, where structural dependencies differ substantially from computer vision models.\n\n- Since pruning is mainly motivated by model compression, it is recommended that the authors conduct experiments with extreme pruning ratios on other network architectures to better demonstrate the robustness of Catalyst. The reviewer acknowledges that experiments on VGG-19 are included, but VGGs are heavily overparameterized and therefore relatively easy to prune, making the results on VGG less representative of the method‚Äôs general performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6d9wxf3CZb", "forum": "3O8TAbrMKW", "replyto": "3O8TAbrMKW", "signatures": ["ICLR.cc/2026/Conference/Submission24710/Reviewer_9Geh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24710/Reviewer_9Geh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824688710, "cdate": 1761824688710, "tmdate": 1762943169263, "mdate": 1762943169263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Catalyst, a theoretically grounded framework for understanding and exploiting pruning-invariant structures in deep networks. The core idea is that many weight configurations can be modified (or ‚Äúpruned‚Äù) without changing the network function, provided those weights lie in a special geometric manifold defined by zero-marginal directions. The authors formalize this with the concept of a pruning-invariant target set and prove that small-norm perturbations in this space preserve the function. They further analyze the gradient dynamics of parameters augmented with a ‚Äúcatalyst‚Äù scalar ùëë, showing that under mild conditions the dynamics exhibit a bifurcation behavior‚Äîeither amplifying or suppressing redundant directions exponentially."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper takes an unusually geometric view of pruning, treating invariance as a manifold property rather than a combinatorial selection problem. The construction of ùëã_{tgt} via the null space of  ùê∑ is elegant and connects pruning to differential topology ideas in parameter symmetry.\n2) Mathematical rigor.\n3) Despite its theoretical density, the paper maintains good readability, with motivating figures and an ethical statement on responsible model compression (energy efficiency vs. fairness trade-offs)."}, "weaknesses": {"value": "1) The practical Catalyst algorithm essentially re-interprets existing weight decay and pruning schemes through a new lens, without introducing a fundamentally new optimization procedure. The conceptual leap is strong, but the empirical method is relatively incremental. It will be an excellent paper by considering this. \n2) Theorem 3.2 assumes sign stability of ùëë  and ùëÄ, ignoring stochastic gradient noise and cross-couplings present in realistic networks. It is unclear how robust the bifurcation mechanism is in the presence of SGD perturbations."}, "questions": {"value": "1) (Theorem 3.1) The choice of ùëò in the constructive part is currently written unclearly (missing parentheses). Please restate precisely the constraints on ùëò‚Ä≤ and the needed condition.\n\n2)The theorems assume the sign of entries of ùëÄ and ùëë does not change during the dynamics. Can you comment on robustness if signs flip (e.g., because of SGD noise or nonzero gradients from the main loss ùêø)? Is the bifurcation still observed under realistic SGD noise? (If you have experiments that show sign stability or explain why sign flips are unlikely, please point to them.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SCwWbessUV", "forum": "3O8TAbrMKW", "replyto": "3O8TAbrMKW", "signatures": ["ICLR.cc/2026/Conference/Submission24710/Reviewer_6rF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24710/Reviewer_6rF7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930876085, "cdate": 1761930876085, "tmdate": 1762943169072, "mdate": 1762943169072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}