{"id": "DvB066Mw9m", "number": 14838, "cdate": 1758244592948, "mdate": 1759897346290, "content": {"title": "Effective Interaction between Quantization and Low-Rank Decomposition based on LLMs", "abstract": "As the parameter size of language models continues to grow, effective model compression is required to reduce their computational and memory overhead. Low-rank decomposition and quantization are two prominent compression methods that have been proven to significantly reduce the computational and memory requirements of Large Language Models (LLMs) while maintaining model accuracy. However, how these two methods interact when combined remains a critical question for developers, as many assume they are orthogonal, meaning their combination would not introduce additional errors beyond those independently introduced by each method. This paper provides the first mathematical proof that low-rank decomposition and quantization are non-orthogonal. We validate these findings through a series of experiments on large language models. Our results demonstrate that these methods are non-orthogonal, and their combination leads to significant performance degradation. Importantly, we propose a novel approach Diagonal Adhesive Method (DAM), which can effectively combine the two methods and mitigate the performance loss. Our research provides deep insights into model compression and lays a solid theoretical and experimental foundation for future related studies.", "tldr": "", "keywords": ["Quantization", "Low-Rank Decomposition", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9b952b0ee4fae6f7fefab5e67370b62de396aed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to maximize the accuracy of the compressed models by jointly applying low-rank approximation and quantization. This is a critical problem for mitigating the issue of quantization, which often exhibits severe performance degradation at 3 bits or lower. The paper proposes a new technique based on a theoretical analysis in this area.\n\nHowever, contrary to the paper's goal of proposing a technique through a theoretical approach, the paper contains severe theoretical errors. The major issues are as follows:\n\n* The paper's experiments deal with activation quantization, but Definition 3.1 only covers weight quantization, which falls outside the scope of the definition's applicability.\n\n* Definition 3.2 considers the activation when defining quantization error, yet Definition 3.5 does not. Furthermore, an error term for low-rank decomposition is used without being properly defined.\n\n* The equations in the Appendix inherently contain numerous typos. Additionally, line 787 states that $\\text{tr}(A^T B)$ \"may be\" negative, which is unacceptable phrasing to be included in a rigorous mathematical proof.\n\nBased on these issues, this reviewer finds the paper's theoretical content unreliable, and the experimental results built upon this theoretical analysis are also difficult to trust. I personally suspect the paper was entirely written using an LLM. Therefore, this reviewer recommends a **strong rejection**."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "No strength,"}, "weaknesses": {"value": "See the summary."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "This article has significantly unreliable theoretical statements, which should not be reused in other papers. Authors who submit their own paper to OpenReview have the responsibility to check their content strictly before submission,"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E4eNi8XC8O", "forum": "DvB066Mw9m", "replyto": "DvB066Mw9m", "signatures": ["ICLR.cc/2026/Conference/Submission14838/Reviewer_Xku7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14838/Reviewer_Xku7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447806149, "cdate": 1761447806149, "tmdate": 1762925191583, "mdate": 1762925191583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the interaction between quantization and low-rank decomposition for large language model compression. The authors argue that these two techniques are not orthogonal and that the optimal order is Low-rank decomposition → Quantization (L→Q) rather than Quantization → Low-rank decomposition (Q→L). They further propose a Diagonal Adhesive Method (DAM) to mitigate quantization errors by rank-wise scaling using a learnable diagonal matrix.\nWhile the paper is clearly written and the empirical results are extensive, the central problem formulation contains a fundamental flaw:\nQ→L (quantization followed by low-rank decomposition) is not a valid or meaningful pipeline in either theory or practice."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a unified framework to discuss how quantization interacts with low-rank decomposition.\n- DAM itself is a simple yet practical idea that empirically improves robustness.\n- Experimental results on multiple LLaMA variants are detailed and reproducible."}, "weaknesses": {"value": "- Invalid problem setup (Q→L is conceptually meaningless).\nQuantization converts floating-point weights into integer form to remove float-domain computation. Performing SVD on quantized weights necessarily requires dequantization, which reintroduces floats—thus invalidating the quantization step itself. The paper treats “Quantization → Low-rank decomposition” (Q→L) as a legitimate alternative order, but this operation cannot exist in an actual quantized inference pipeline. As a result, comparing L→Q and Q→L is not a theoretically symmetric problem, but rather a comparison between a valid and an invalid setting.\n\n- Overstated novelty and misleading framing.\nThe authors repeatedly claim to be the first to prove that quantization and low-rank decomposition are non-orthogonal. However, L→Q and Q→L result in different values as Q→L is an invalid setting. Therefore, proving non-orthogonality between the two operations is not a discovery rather a restatement of basic linear algebra.\nAlso, no prior work assumed they were orthogonal in the first place. The paper’s framing suggests resolving a nonexistent controversy.\n\n- Incremental idea.\nThe diagonal rescaling (DAM) is an incremental extension of channel-wise or block-wise scaling (e.g., SmoothQuant (ICML’23), Basis-sharing (ICLR’26)). While useful, it does not constitute a fundamentally new principle."}, "questions": {"value": "- Can Q→L be implemented in a truly quantized (int8 or int4) inference pipeline?\n- If Q→L is infeasible in practice, what conceptual meaning would your analysis of “non-orthogonality” still hold?\n- How does your diagonal rescaling differ fundamentally from SmoothQuant’s per-channel scaling or basis-sharing normalization?\n- Given that the paper derives the existence of an optimal diagonal matrix, what is the rationale for learning it through gradient updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKIFchGAGK", "forum": "DvB066Mw9m", "replyto": "DvB066Mw9m", "signatures": ["ICLR.cc/2026/Conference/Submission14838/Reviewer_NNrR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14838/Reviewer_NNrR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792980933, "cdate": 1761792980933, "tmdate": 1762925190878, "mdate": 1762925190878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the interaction between quantization and low-rank decomposition for compressing large language models (LLMs). While prior work often assumes these two methods are orthogonal (i.e., their errors are independent), the authors provide theoretical proofs and empirical evidence that they are in fact non-orthogonal, meaning their combination introduces additional errors. The paper further shows that the order of applying the methods matters, with low-rank decomposition before quantization being superior. To mitigate performance degradation, the authors propose a Diagonal Adhesive Method (DAM), which reduces activation outliers and improves quantization robustness. Experiments on the LLaMA family (7B–70B) across multiple benchmarks demonstrate that DAM significantly improves performance under combined compression."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides the first formal proof that quantization and low-rank decomposition are non-orthogonal.\n2. This paper proposes DAM, a lightweight diagonal scaling technique that effectively reduces quantization error caused by outliers."}, "weaknesses": {"value": "1. Generality beyond LLaMA: Experiments are restricted to LLaMA models; it is unclear whether the findings generalize to other architectures (e.g., Qwen3)."}, "questions": {"value": "1. Have you tested DAM on non-LLaMA architectures, or do you expect the same non-orthogonality and order sensitivity to hold universally?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jhlx2OAsom", "forum": "DvB066Mw9m", "replyto": "DvB066Mw9m", "signatures": ["ICLR.cc/2026/Conference/Submission14838/Reviewer_Rfkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14838/Reviewer_Rfkt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994405123, "cdate": 1761994405123, "tmdate": 1762925190355, "mdate": 1762925190355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to derive theoretical insights on orthogonality and the proper order of factorization and quantization of large language models (LLMs). They further introduce the diagonal adhesive method (DAM) to account for outliers."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper attempts to provide a theoretical justification for previously observed effects regarding joint factorization and quantization.\n- The application of FWSVD/ASVD diagonal scaling is adapted to a slightly different task of joint quantization and factorization."}, "weaknesses": {"value": "- The paper does not consider existing methods that employ both quantization and low-rank decomposition, such as QLoRA and Quantization-Aware Factorization.\n- The conclusions about the order of factorization and quantization and their orthogonality are largely self-evident. While the mathematical proofs may interest some, the broader impact and usefulness of these results are questionable.\n- The proposed DAM approach is too conceptually similar to ASVD and FWSVD\n- There are several inconsistencies throughout the text that reduce readability. More details are listed in the “Questions” section.\n- The paper does not provide comparisons with any other compression methods, which is a huge drawback."}, "questions": {"value": "- There should be a clear indication that symmetric quantization is used in Equation 1; otherwise, a zero-point term should be included.\n- There is an inconsistency between Equations 1 and 2 regarding the step size. Please define the step size properly.\n- Equation 3 should use proper vector/tensor norm notations rather than the absolute value definition.\n- Existing papers that combine quantization and factorization, such as QLoRA or Quantization-Aware Factorization, are not cited or discussed.\n- The procedure in Equation 6 appears questionable: taking the SVD of quantized weights may be meaningless, as the factors will no longer align with the quantization grid. It is unclear why this comparison is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sFUX4G0PUw", "forum": "DvB066Mw9m", "replyto": "DvB066Mw9m", "signatures": ["ICLR.cc/2026/Conference/Submission14838/Reviewer_yMcP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14838/Reviewer_yMcP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999972505, "cdate": 1761999972505, "tmdate": 1762925189693, "mdate": 1762925189693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}