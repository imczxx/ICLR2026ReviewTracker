{"id": "jKcHPxRvk9", "number": 13766, "cdate": 1758222221410, "mdate": 1759897414438, "content": {"title": "Sample-efficient Reinforcement Learning by Warm-starting with LLMs", "abstract": "We investigate the usage of Large Language Models (LLMs) in collecting high-quality data to warm-start Reinforcement Learning (RL) algorithms for learning in Markov Decision Processes (MDPs).\nSpecifically, we leverage the in-context decision-making capability of LLMs, to generate an \"offline\" dataset that sufficiently covers state-actions visited by some good policy, then use\n  an off-the-shelf\n  RL algorithm to further explore the environment and fine-tune its policy, in a black-box manner. Our algorithm, LORO\\footnote{The code of our experiments can be viewed at \\url{https://anonymous.4open.science/r/LlamaGym-551D}}, can both converge to an optimal policy and have a high sample efficiency thanks to the good data coverage collected by the LLM. \n  On multiple OpenAI Gym environments, such as CartPole and Pendulum, given the same environment interaction budget, we empirically demonstrate that LORO outperforms baseline algorithms such as pure \n  LLM-based policies, pure RL, and a naive combination of the two.", "tldr": "LLM can zero-shot a sub-optimal policy for solving MDP, but it has good coverage, which can be used to warm start classical RL algorithms.", "keywords": ["Reinforcement learning", "LLM", "warm start", "sample efficiency", "coverage"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c907b850c4c66d37ee079cc38317f54ff3f71a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an algorithm called LLM Offline, RL Online (LORO) which is designed to make reinforcement learning faster and more data-efficient via pretraining with high quality offline data. The core idea is to \"warm-start\" the learning agent. Instead of starting from scratch with no knowledge, the method first uses a large language model to act as an initial policy and play the game for a few episodes. The idea is that this LLM will generate a small, high-quality offline dataset of experiences. A standard reinforcement learning algorithm is then pre-trained on this LLM-generated data before it starts interacting with the environment to learn and fine-tune its policy online. The authors argue this works because the language model's data, while not perfect, provides better coverage of important state-action pairs than random exploration. Their experiments on several game environments show that the LORO approach can speed up learning compared to starting with random exploration and online RL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is highly reproducible and code is included which is a big plus. It includes detailed hyperparameter values, clear details of the LLM variations used, prompt formats, rollout budgets, and pretraining steps, along with a working anonymous code link. \n- The core idea is novel, interesting, and exciting and provides a different modality for pretraining and general training of RL agents that could be quite useful in the future for reducing sample complexity of RL algorithms.\n- The experiments are carefully designed to show what actually drives improvement. The paper compares pretraining on LLM data with mixing data directly, tests different data sources such as LLM, online, and random, and uses multiple RL algorithms showing that in off-policy settings the LORO idea can help."}, "weaknesses": {"value": "- The writing quality throughout the paper is a concern. There are frequent grammatical mistakes, awkward sentence constructions, and unclear phrasing that collectively make the paper difficult to read through cleanly. I initially began noting specific instances to include in this review, but by the end of the related work section I had already identified at least five clear grammatical or syntactic errors and decided to stop recording them. These issues give the paper a rushed and unpolished feel. I recommend a thorough language and structure revision.\n\n- The proposed approach relies on an external framework that provides a text-based wrapper enabling the LLM to interact with classic RL environments. Although this design choice is quite interesting theoretically, it introduces a strong dependency on environment-specific engineering. If I understand the prior work correctly, the wrapper translates numerical observations into text descriptions and maps LLM-generated text back into executable actions, which encodes task-specific structure and domain knowledge. This dependence limits the scalability and generality of the approach, as the method remains tied to environments for which wrappers already exist or can be easily synthesized manually. As a result, it is unclear how easily the framework could extend to new or more complex domains without substantial additional interface design. Likewise the question (which maybe is more appropriate for prior works) is how susceptible is this wrapper framework to reward hacking and prompt engineering failures. If the wrapper fails to work properly the ideas presented in Assumption 1 fall apart. Furthermore, the paper does not justify why this level of manual engineering would be better spent on using LLM-generated policies rather than equally direct approaches such as incorporating physics-informed priors, structured exploration strategies, or hand-coded heuristic controllers, which could provide similar sample efficiency benefits when a priori information about the environment’s structure is available.\n\n- The paper's central methodological benefit hinges on Assumption 1, which claims that the LLM policy generates pretraining trajectories that sufficiently cover the state-action space that would be visited by an optimal policy. This assumption is foundational to the algorithm's warm start success, yet it lacks strong theoretical or mechanistic justification. The paper does not provide a clear reason why an LLM's text based reasoning should align with the optimal visitation distribution of an arbitrary MDP, especially in complex environments where optimal solutions are non intuitive. The only validation for this critical assumption is empirical, and this evidence is limited to simple gridworlds (CliffWalking, FrozenLake) where optimal paths are easily inferred from textual descriptions. The scalability of Assumption 1 to high dimensional, continuous, or partially observable domains remains unproven. I skimmed the appendix as well to try and better understand the assumption, but the empirical data in Appendix C (Table 3) is inconclusive and, in some cases, contradictory. For example, in the CliffWalking environment, the online collected data achieves a superior (lower) surrogate transfer coefficient than the LLM policies. The paper's post hoc dismissal of this data by suggesting the final policy used for evaluation was non optimal undermines the validity of its own verification protocol. This leaves the core premise that the LLM policy inherently provides high quality, optimal state covering data unconvincingly supported.\n\n- Comparative performance is a weakness in this work. Across the six OpenAI Gym-style environments evaluated for sample efficiency, LORO only outperforms a simple online RL algorithm in three of the six cases. The only setting where LORO substantially outperforms prior work is in the CliffWalking environment, which could reasonably be classified as an edge case characterized by extreme reward scales and strong critic divergence sensitivity due to the sharpness of penalties for falling off the cliff. This pattern raises questions about whether the observed improvements generalize beyond tasks that inherently favor conservative value estimation or high-quality initialization.\n\n- The chosen environments are small and largely deterministic. As a proof of concept, the inclusion of smaller environments makes sense, but the lack of consistent performance improvement over simpler purely online methods in these small test cases alongside the absence of larger-scale or higher-dimensional tasks limits the contribution of this work.\n\n- LORO discards the LLM-collected dataset after pretraining and uses standard SAC or Double DQN for subsequent online learning. This design lacks any mechanism to preserve or regularize around the pretrained policy, making the approach vulnerable to catastrophic forgetting once new online data is introduced. In addition, vanilla SAC is known to suffer from overestimation when trained offline without corrective regularization or online interaction [1]. The paper addresses the issue of overfitting, but does not explain how this phase remains stable or avoids value overestimation given that the pretraining stage involves no environment interaction.\n\n[1] Hussing, Marcel, et al. \"Dissecting deep rl with high update ratios: Combatting value divergence.\" arXiv preprint arXiv:2403.05996 (2024)."}, "questions": {"value": "How do you prevent catastrophic forgetting of the pretrained policy once online training begins and the LLM data is discarded?\nGiven that standard SAC is prone to severe Q-value overestimation under offline training, what steps (if any) were taken to ensure stable pretraining?\nGiven the mixed performance of your method why did you not test across a broader suite of tasks or were you limited by the number of wrappers available for Gym environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UaaQ1PfWl2", "forum": "jKcHPxRvk9", "replyto": "jKcHPxRvk9", "signatures": ["ICLR.cc/2026/Conference/Submission13766/Reviewer_ZgGw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13766/Reviewer_ZgGw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450713176, "cdate": 1761450713176, "tmdate": 1762924298671, "mdate": 1762924298671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LORO: LLM Offline, RL online, a new algorithm for RL by using LLMs to bootstrap the initial policies. \nLORO works by using the LLM and the problem description to generate an offline dataset. There is one hyperparameter \\tau that controls the total episodes to run to collect the dataset.\n\nThis offline dataset can then be used by any RL algorithm to bootstrap the initial policy. Once bootstrapped, the RL algorithm can proceed as normally while adding new samples to the dataset.\n\nThe authors conduct an empirical evaluation on LORO to showcase its effectiveness across different discrete adn continuous environments and and also with zero-shot LLM policies to showcase the efficacy of LORO over coldstart RL methods and LLM policies.\n\nFinally, the authors provide some light theory on domains where LLMs could perform well and not although this is not empirically demonstrated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clear and well written. Enough information and contrast with related work is there.\n\n2. The idea is intuitive and clear\n\n3. I liked the theory part where the authors hypothesize where LLMs could do well.\n\n4. Some results on the ablations are interesting and surprising."}, "weaknesses": {"value": "I think this paper is almost there but not quite ready yet for publication. \n\n1. The idea is good and the choice of baseline environments is good, but the RL algorithms themselves are lacking. Firstly, why use SAC instead of more SOTA RL methods as baselines for online RL.\n\n2. A minor weakness: There are no experiments with domains that already have offline datasets. It would be interesting to see results here.\n\n3. The hypothesis is good for where LLMs could do better but domains where LLMs cannot do well is not really explored so this contribution is quite limited. \n\n4. Table 5 shows quite a heavy compute investment to generate the dataset. For example, Qwen-7b took 20 hrs for Cartpole. DQN takes maybe 15 min to learn a very good policy for CartPole that can generate an \"offline\" dataset for another RL algorithm. Why would one use LORO when they could use another algorithm to generate an equally good if not better dataset in a fraction of the time (assuming the simulator is cheaper to execute for 20 hrs than GPUs which at least is the case for the environments you tried).\n\n5. Overall results are not super impressive. Looking at Table 5, the avg rank is same whether using online RL or LORO. I think your idea has merit but perhaps a better empirical design might make it shine better.\n\n6. Bold claim where it is stated that LLM's model size and performance is not having any clear link. I think this claim is clearly overstated and might be incorrect. Ideally, you show a plot of LORO performance with many different model sizes including the tiniest LLM (1b models or even smaller). In the context of the paper you might be right but that is only with 2 models you tested. The only interesting contribution here is that the inflection point is likely 7B beyond which you will not find meaningful gains although this does not need as much text in the paper as it is obvious from the results. This would be a better contribution had you tested more models I think."}, "questions": {"value": "Please addresses my weaknesses. Overall interesting work. Happy to engage in discussion and increase my score. (#4 is my biggest concern and primary reason for a score of 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UbBeeqNGNJ", "forum": "jKcHPxRvk9", "replyto": "jKcHPxRvk9", "signatures": ["ICLR.cc/2026/Conference/Submission13766/Reviewer_BdGa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13766/Reviewer_BdGa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804226560, "cdate": 1761804226560, "tmdate": 1762924298274, "mdate": 1762924298274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to leverage priors from LLMs to generate datasets of experience for RL agents. The method is extremely simple: directly prompt an LLM to generate actions, actually take them in the environment, and then use this generated dataset to initialize an off-the-shelf RL algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Leveraging prior knowledge from LLMs to make RL more sample efficient is a very promising research direction.\n- The method proposed is extremely straightforward.\n- Code to reproduce experiments is made available."}, "weaknesses": {"value": "- The paper only shows results on toy environments, and doesn’t discuss how it would be possible to extend the method to more complex tasks.\n- The paper ignores all recent literature on applying RL directly to LLMs. For domains in which the LLM already gives some reasonable actions, why not directly improve it as a policy?\n- The writing in the paper suggests that the intention is to be in the offline-to-online RL regime. However, the LLM actually takes actions in the **real** environment in order to collect the “offline” dataset. This is certainly not a bad thing by itself, but it is misleading to call this offline learning, since there is interaction with the environment. The x-axis of every plot should be adjusted to account for these episodes of interaction.\n- For 4 out of 6 tested environments, LORO does not perform differently than “Online RL” to a statistically significant extent."}, "questions": {"value": "What adaptations would be needed to make LORO work for more realistic tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hoDtCbacDu", "forum": "jKcHPxRvk9", "replyto": "jKcHPxRvk9", "signatures": ["ICLR.cc/2026/Conference/Submission13766/Reviewer_8NaS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13766/Reviewer_8NaS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970453442, "cdate": 1761970453442, "tmdate": 1762924297772, "mdate": 1762924297772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper suggests using an LLM to collect data to warm-start an RL algorithm. The method is called LORO and is shown to have some advantages. The paper is lacking in many aspects: \n\n1. There is not much novelty in this paper. The idea of collecting data using LLMs has been a standard application. It might be that its use in RL - to collect an offline dataset - offers some advantages, but I don't see this as a novel idea per se. Also, my decision in rejecting this paper is not based on that in general. \n\n2. The authors seem to miss a complete literature of model-based RL and Bayesian RL that work very well in the environment they have suggested. For example, in CartPole, PILCO (https://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf) can solve the task of swinging up and balancing in 3 episodes. Of course, there are many other model-based RL algorithms that need to be cited and compared to if we are discussing sample efficiency. The same goes for Bayesian RL - the idea of using a prior policy to learn has been studied. Not in the way the author suggests, i.e., collect offline data, but more to do with variational inference/posterior sampling to improve sample efficiency. The authors need to cite and compare to those for me to consider this paper. Additionally, there has been work on using LLM priors for RL, which is cited in the paper; e.g. https://arxiv.org/abs/2410.07927. The suggested method's improvement in sample efficiency should consider an empirical comparison. I really think many baselines are missing. \n\n3. Assumption 1 is very heavy: I mean, if the LLM can cover the optimal policy, then you are more than halfway there to solve your problem. That is why I prefer the notion of a prior, whose samples get weighted by Q-values or rewards. Maybe the authors can help me understand why assumption 1 is a good assumption and why it actually holds in the real world. \n\n4. The experiments are rather weak and simplistic. I'd suggest that the authors run some large problems to see the effect of their proposed method. Something around reasoning would be great. Maybe, even consider ALFWorld. Additionally, I would urge the authors to conduct statistical significance tests. I am not sure if the results presented are statistically significant, especially since LORO doesn't always win. \n\n5. Are the rewards considered in the experiments sparse or dense? I didn't fully understand this point. If they are dense, the authors should consider running on sparse reward setups, since this is where the advantages might lie. \n\n6. The authors note that when Assumption 1 is violated, LORO “typically” remains robust—yet the experiments are still in small domains. I would like to really see any scalable application of this method. \n\nI don't know if it is my browser, but the PDF dimensions seem not right."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "See above"}, "weaknesses": {"value": "See above"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sb6y42RSNA", "forum": "jKcHPxRvk9", "replyto": "jKcHPxRvk9", "signatures": ["ICLR.cc/2026/Conference/Submission13766/Reviewer_PrjW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13766/Reviewer_PrjW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13766/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996365785, "cdate": 1761996365785, "tmdate": 1762924297336, "mdate": 1762924297336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}