{"id": "DKOIADzbtM", "number": 1886, "cdate": 1756957755653, "mdate": 1759898180650, "content": {"title": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography", "abstract": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, which requires view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision–language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image–text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image–text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision–language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image–text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5\\% AUC in zero-shot disease classification and 95.1\\% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.", "tldr": "", "keywords": ["Echocardiography", "vision-language model", "ultrasound"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09a01ae8b43dad1d9d78155474972568275765a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EchoVLM, a measurement-grounded VLM for echocardiography, and EchoGround-MIMIC, a multimodal dataset explicitly linking echocardiographic images with structured measurements, measurement-grounded captions, and guideline-aligned disease labels. EchoVLM extends the CLIP framework with two clinically motivated objectives: a view-informed contrastive loss that models the view-dependent nature of echocardiographic images, and a negation-aware contrastive loss that distinguishes positive and negative clinical statements. Trained on 19,065 image–text pairs, EchoVLM achieves state-of-the-art results across 36 tasks, including zero-shot disease classification (AUC = 86.5%), view classification (95.1% accuracy), and competitive segmentation and landmark detection on public datasets. The results demonstrate strong cross-modal transfer and clinically meaningful visual representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- New measurement-grounded dataset: EchoGround-MIMIC provides the first large-scale, structured dataset pairing echo images with quantitative measurements, standardized views, and guideline-derived labels.\n\n- The proposed view-informed and negation-aware contrastive losses directly encode clinical reasoning patterns, improving both visual coherence and semantic discrimination.\n\n- Extensive validation on 36 tasks across five applications shows consistent superiority over domain and generalist baselines (e.g., +7.2 AUC vs. EchoCLIP, +0.9% precision over EchoApex)."}, "weaknesses": {"value": "- EchoGround-MIMIC originates from a single institution (MIMIC-IV-ECHO), which constrains demographic, hardware, and acquisition variability, potentially limiting generalization.\n\n- OCR-extracted measurements and LLM-generated captions introduce potential noise; limited manual validation may not fully prevent systematic errors.\n\n- While effective, the new contrastive objectives are empirically motivated with limited theoretical analysis of their convergence or interaction with the CLIP loss.\n\n- EchoVLM operates on single frames rather than sequences, missing dynamic cardiac context critical for echocardiographic interpretation."}, "questions": {"value": "- How would EchoVLM perform when trained or tested on multi-institutional data with varying imaging protocols, demographics, and ultrasound vendors?\n\n- Could the framework be extended to incorporate video-level dynamics, given that echocardiography interpretation heavily depends on temporal motion?\n\n- What proportion of the automatically generated measurement-grounded captions were manually verified, and how sensitive are downstream results to errors in this supervision?\n\n- How robust are the view-informed and negation-aware losses to their respective λ parameters when scaling to larger datasets or different imaging domains?\n\n- What are the major failure cases observed in zero-shot classification, e.g., misinterpretations of negations or confusion between anatomically adjacent views?\n\n- What are the computational requirements and latency of EchoVLM inference in a real-time clinical setting, and how might model compression or distillation affect its performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GKQAd1tRQD", "forum": "DKOIADzbtM", "replyto": "DKOIADzbtM", "signatures": ["ICLR.cc/2026/Conference/Submission1886/Reviewer_qUxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1886/Reviewer_qUxR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1886/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760724488609, "cdate": 1760724488609, "tmdate": 1762915931326, "mdate": 1762915931326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EchoGround-MIMIC, a measurement-grounded multimodal echocardiography dataset, which includes standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. The authors also propose EchoVLM, a CLIP-style vision-language model, which is pretrained with two novel contrastive loss functions (view-informed and negation-aware)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A significant contribution of this work is the design of a comprehensive data processing pipeline. This pipeline successfully extracts and aligns a complex, multimodal dataset—comprising images, standardized views, quantitative measurements, measurement-related reports, and disease labels—from the MIMIC-IV-ECHO and MIMIC-IV-Note databases.\n\n2. The paper proposes two novel and clinically-motivated pretraining objectives: view-informed contrastive learning and negation-aware contrastive learning. The utility and effectiveness of these objectives are well-supported by the provided ablation studies.\n\n3. The proposed model (EchoVLM) is thoroughly evaluated on a diverse set of five downstream application types (36 tasks in total), demonstrating its generalizability and strong performance across both multimodal and vision-only benchmarks."}, "weaknesses": {"value": "1. Disconnect between \"Measurement-Grounded\" Narrative and Methodology: The paper's core theme is \"measurement-grounded multimodal learning.\" However, there appears to be a significant disconnect between this narrative and the technical implementation. The structured measurements (e.g., JSON-formatted values like \"EF: 45%\"), which are a key highlight of the new dataset, are not directly utilized as an input during the model's training phase. The model is only trained on the captions derived from these measurements. Furthermore, the two novel optimization objectives (L_view and L_neg)  are independent of the structured measurements. In fact, these objectives appear to be entirely separable from the 'grounded' nature of the data pipeline: L_view is a vision-only objective, while L_neg is a text-only objective that could be applied to any positive/negative caption pair, whether it is measurement-grounded or not.\n\n2. Lack of Ablation on the \"Grounded\" Data Pipeline: While the \"measurement-grounded\" nature of the captions is presented as a core advantage, the paper lacks a direct ablation study to quantify the benefit of this complex and costly data curation process. Specifically, there is no experiment comparing the performance of a model trained on these curated \"measurement-grounded captions\" against a baseline model trained on the original, complete, and noisy \"non-measurement-grounded\" reports (e.g., the full text from MIMIC-IV-Note). This makes it difficult to assess the true value added by the grounding pipeline."}, "questions": {"value": "1. Given the paper's core theme of \"measurement-grounded\" learning, could the authors elaborate on the design rationale for not directly utilizing the extracted structured measurements as an input during pretraining (e.g., as explicit tokens or an auxiliary regression loss)? Why was this quantitative data, a key part of the new dataset, only used as an intermediate tool for caption generation?\n\n2. The paper provides a simple example for negation generation (e.g., \"no regurgitation\" from \"mild regurgitation\"). For more complex quantitative statements, such as \"Quantitative biplane left ventricular ejection fraction is 45 %,\" could the authors clarify what form the corresponding \"clinical semantic negation\" takes? \n\n3. Considering that echocardiography is an inherently dynamic (video-based) modality, what are the specific advantages or benefits of the proposed frame-based approach when compared to existing video-based solutions (such as EchoPrime mentioned in the related work)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9U10uzyrto", "forum": "DKOIADzbtM", "replyto": "DKOIADzbtM", "signatures": ["ICLR.cc/2026/Conference/Submission1886/Reviewer_v1CK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1886/Reviewer_v1CK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1886/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729395552, "cdate": 1761729395552, "tmdate": 1762915931124, "mdate": 1762915931124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the EchoGround-MIMIC dataset, a set of image/text paired datasets for echocardiography. Specifically, the authors use the MIMIC-IV-ECHO  dataset and extract numerical measurements using OCR-based methods. Second, the authors propose a CLIP-based contrastive learning framework and evaluate EchoVLM on 5 different clinical applications with 36 clinical tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a needed dataset for echocardiography. Most of the papers working on VLMs for echo are constrained to private datasets, limiting their applicability and contribution.\n\n- The paper comprehensively details different procedures taken to obtain the final dataset from the raw original MIMIC-IV-ECHO.\n\n- The negation-aware contrastive objective for CLIP, along with diverse ablation studies."}, "weaknesses": {"value": "- The main weakness of the paper, to me, is its limited architectural novelty. Although introducing the new dataset is needed for the community working on echocardiography, the proposed Echo-VLM is similar to prior works originally CLIP and also its variants Echo-CLIP.\n\n- Measurements are cropped from overlays and transcribed via an LLM, along with the captions and guideline labels. Although this is acknowledged in the paper and despite manual checks, parsing errors may introduce label noise as mentioned. To what extent is this labelling noise mitigating? Were there cardiologists involved in the process?\n\n-"}, "questions": {"value": "- Can the authors elaborate on their novelty in terms of the architecture design, as opposed to prior works like EchoCLIP?\n\n- A main concern of mine is whether the dataset is really going to be open-sourced. I understand that the authors mention this; however, based on my experience in this field, I have seen many papers in top-tier conferences that mention they will open-source the code/data, but they don't. This is particularly evident in many echo papers. Ideally, the authors could share an anonymised GitHub repo containing the code/data of the paper. \n\n- Can the authors clarify more on the manual checks performed on the outputs of LLMs? How trustful is the outputs of the OCR algorithtm and the LLM-generated captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bBKkZlmV0W", "forum": "DKOIADzbtM", "replyto": "DKOIADzbtM", "signatures": ["ICLR.cc/2026/Conference/Submission1886/Reviewer_Frhu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1886/Reviewer_Frhu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1886/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772148295, "cdate": 1761772148295, "tmdate": 1762915930656, "mdate": 1762915930656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a vision-language model, CLIP-style, for echocardiography. VLMs for echocardiography suffer from internal challenges in accurate measurement predictions, and sparse and unfocused data of image-text pairs. The proposed model combines image and text encoders trained on the novel EchoGround-MIMIC dataset (19,065 measurement-grounded image-text pairs). The dataset comes from preprocessing and organizing existing public repos (MIMIC-ECHO), and could be valuable for further research—so far there is no similar open-source data, therefore the data is useful. The model uses two specialized contrastive losses: a view-informed contrastive loss (same-view positives, different-view negatives) and a negation-aware contrastive loss for distinguishing negative vs. positive clinical findings in text. However, these losses appear to offer limited technical novelty. For downstream tasks like segmentation and landmark detection, task-specific heads are added to the pre-trained encoder and fine-tuned on benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-- Data: EchoGround-MIMIC (~20K measurement-grounded image-text pairs) - first open-source dataset of its kind for echocardiography. Data Processing Innovation -- Successfully integrates MIMIC-IV-ECHO imaging with MIMIC-IV-Note reports. \n\n\n-- Clinical Relevance: Addresses critical gap between free-text narratives and quantitative measurements essential for guideline-based echo diagnosis.\n\n-- Comprehensive Evaluation Framework: 36 tasks across 5 clinical application types (classification, retrieval, segmentation, landmark detection) - would be valuable if released as a benchmark.\n\n-- Community Value: Fills significant resource gap for medical AI research in echocardiography."}, "weaknesses": {"value": "-- Limited Technical Novelty: View-informed loss is just constrained negative sampling; negation-aware loss potentially similar to existing work (e.g., MICCAI 2025, \"EchoViewCLIP: Advancing Video Quality Control through High-performance View Recognition of Echocardiography\")\n\n-- Evaluation Methodology Issues: Primary comparison against unreleased EchoApex (weights/data are not released, based on reported results) instead of available EchoPrime (weights are open to download) raises reproducibility concerns, and if all models were trained in the same manner. \n\n-- Technical Details: Frame vs. video level processing unclear; mathematical formulation of negation-aware loss may lack sufficient innovation\n\n-- Algorithmic Contributions Questionable: Technical contributions may not meet novelty bar for top-tier venues - relies heavily on dataset contribution rather than methodological innovation"}, "questions": {"value": "Given that EchoPrime is publicly available while EchoApex is unreleased, why not use EchoPrime as the primary baseline? This would enable reproducible comparisons and address potential selection bias concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uqTActq3Nq", "forum": "DKOIADzbtM", "replyto": "DKOIADzbtM", "signatures": ["ICLR.cc/2026/Conference/Submission1886/Reviewer_zREj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1886/Reviewer_zREj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1886/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833867382, "cdate": 1761833867382, "tmdate": 1762915930291, "mdate": 1762915930291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}