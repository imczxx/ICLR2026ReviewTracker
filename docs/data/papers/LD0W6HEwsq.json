{"id": "LD0W6HEwsq", "number": 1938, "cdate": 1756969017404, "mdate": 1759898177598, "content": {"title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics", "abstract": "The remarkable success of deep neural networks is often attributed to empirical *scaling laws*, which predict consistent performance gains as model size, data, and compute increase. However, practical challenges such as training instability and diminishing returns highlight a fundamental gap in our theoretical understanding of these principles. The Neural Tangent Kernel (NTK) framework falls short in explaining these phenomena, as it cannot capture the rich feature learning (FL) that underlies modern AI breakthroughs (e.g., LoRA, CLIP, and in-context learning). By contrast, the maximal update parameterization ($\\mu$P) preserves active FL at scale, but existing work has primarily analyzed scaling rules rather than training dynamics. This work bridges this gap by introducing a theoretical framework for the training dynamics of ResNets in the joint infinite-width and infinite-depth limit. We prove that in this regime, the training trajectory converges to a coupled forward–backward stochastic dynamics system, termed *Neural Feature Dynamics (NFD)*. NFD explains scaling success when its convergence conditions are satisfied and failure when these conditions break down. Our analysis further shows that diminishing returns emerge naturally as the model's dynamics approach the NFD limit and its effective capacity saturates. Experiments on ResNets validate these predictions, while additional empirical studies with Transformers and the Adam optimizer demonstrate broader applicability. This framework positions NFD as a principled foundation for future research on training dynamics, stability, and generalization in the FL regime.", "tldr": "We study why neural scaling laws succeed and fail by proving that deep ResNets trained with SGD converge to a coupled forward–backward SDE system in the joint infinite-width and infinite-depth limit.", "keywords": ["Scaling laws; Feature learning dynamics; Infinite-width and infinite-depth limit; Residual networks; Stochastic differential equations; Neural Tangent Kernel (NTK); Maximal update parameterization (μP)."], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8adbacab1c1f237fc2acf90f3bdfa4f5518eb4e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce Neural Feature Dynamics (NFD), a theoretical framework for understanding scaling laws in deep neural networks through the lens of feature learning in the joint infinite-width and infinite-depth limit. The paper explores ResNets under depth-adapted μP and shows that training dynamics converge to a coupled forward-backward stochastic differential equation (SDE) system. The framework explains when scaling succeeds (convergence to the limiting SDE holds), when it fails (convergence breaks down), and the emergence of diminishing returns (as approximation error shrinks near the limit). \n\nOverall, the paper makes a solid contribution to the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper introduces a valuable theoretical model for understanding width/depth scaling in relevant models.\n* The framework is introduced clearly, making the paper relatively accessible to a general audience.\n*  The mathematics appears comprehensive, particularly when including the appendix.\n* The models chosen for the analysis are current and of importance to ML practitioners. This also includes testing a limited number of optimizers.\n* The identification of a capacity ceiling is an interesting result."}, "weaknesses": {"value": "* As with any theoretical papers, not all architectures of interest to the community are represented, leaving possible gaps in the scope of the theory.\n* The paper is very dense. While, as stated above, the mathematics is introduced clearly and with sufficient explanation, a lot is left to the appendix.\n* In the introduction, the authors emphasize the feature learning regime falling outside the scope of NTK theory. While I agree from a dynamics perspective, the (empirical) NTK can be computed on networks of any size and in any regime, and is still an interesting object to study. Perhaps this can be made clearer in those early statements."}, "questions": {"value": "* The outlook of the paper states that NFD can be used as a base for future learning dynamics studies. What insight do the authors expect to gain by using the framework to study learning dynamics? Could the NFD be expected to provide more detailed information about how models are learning, the kinds of features being learned, or the phases?\n* If so, could these insights then be applied to finite networks on a more practical level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8P4pFTh3cI", "forum": "LD0W6HEwsq", "replyto": "LD0W6HEwsq", "signatures": ["ICLR.cc/2026/Conference/Submission1938/Reviewer_8saH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1938/Reviewer_8saH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662644893, "cdate": 1761662644893, "tmdate": 1762915962158, "mdate": 1762915962158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the scaling limits and convergence rates associated with the joint infinite width and infinite depth limit of residual neural networks. The authors provide a description of the training dynamics of networks in this joint limit. They provide experiments demonstrating that convergence can be achieved at reasonable widths and depths. They provide some analytical results about the hilbert space of the NNGP kernel at initialization and also the evolution of the preactivation statistics through training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper studies an important question of how neural networks converge to their large width and depth limits. They provide many interesting experiments including comparisons of preactivation and postactivation design and the evolution of the minimum kernel eigenvalue throughout training. They also prove that increasing the effective layer time $T$ increases the size of the RKHS of the associated kernel at initialization."}, "weaknesses": {"value": "**Novelty**: \nI am concerned about the level of novelty of the primary results of this work. For example, the evolution equations describing the infinite width and depth limit (proposition 8 in this paper) can be found in earlier works (for instance [this work](https://arxiv.org/pdf/2309.16620), Result 1 and Appendix E). That paper also provides finite width and depth error analysis (Appendix H and I of [this work](https://arxiv.org/pdf/2309.16620) ). \n\nIn the Appendix of this submission, the authors claim that prior studies \"remain primarily focused on identifying scaling rules, offering limited insight into the training-time dynamics of coupled feature and gradient evolution.\" I am not sure that this is fair to the prior work that I linked to. A lot of work has gone into analyzing the evolution equations of infinite width networks during training (see below). In my opinion, the authors should clarify in what ways their analysis provides new results relative to prior works. \n\nIn the conclusion, the authors state \" Our main result (Theorem 1) established that, in the infinite-width-depth limit, training trajectories converge to Neural Feature Dynamics (NFD), a coupled forward-backward stochastic system that goes beyond the NTK regime,\nwith explicit convergence rates and commutativity at initialization.\" In my understanding, these findings were present in [prior work](https://arxiv.org/pdf/2309.16620) on this topic. \n\n**Regime to study scaling laws**: \n\nI am also not sure that the convergence rates provided from standard error analysis are descriptive of realistic neural scaling laws. It is true that for fixed data and fixed iteration count (compared to width, depth etc), the error rates should scale as $\\sim 1/n + 1/L$ (or $1/L$, see question below) as $n,L \\to \\infty$. However, the **task dependent scaling laws** with exponents less than one, that are observed after training a finite model on a large quantity of data, cannot be explained by this standard error analysis (see discussion [on page 2 here](https://arxiv.org/abs/2402.01092) ). Prior works usually study convergence rates of limited rank empirical kernels in the underparameterized regime to generate these alternative scaling exponents. \n\n**Incomplete references to prior works**:\n\nThe authors do not discuss some highly relevant areas of prior work\n\n1. Theoretical works on neural scaling laws: [Bahri et al 2021](https://arxiv.org/abs/2102.06701), [Maloney & Roberts 2022](https://arxiv.org/abs/2210.16859), [Simon et al 2023](https://arxiv.org/abs/2311.14646), [Bordelon, Atanasov, Pehlevan 2024](https://arxiv.org/pdf/2402.01092), [Paquette et al 2024](https://arxiv.org/abs/2405.15074).\n2. Theory describing evolution of infinite width networks: [Bordelon & Pehlevan 2022](https://arxiv.org/abs/2205.09653) (predicting infinite width dynamics across varying levels of richness), [Bordelon, Chaudhry, Pehlevan 2024](https://arxiv.org/abs/2405.15712) (infinite width and depth limits of transformers), [Chizat et al 2022](https://arxiv.org/abs/2211.16980) theory for deep linear networks\n3. Other works on infinite depth limits: [Hayou 2024](http://www.jmlr.org/papers/v25/23-1163.html) for convergence rates. [CompleteP](https://arxiv.org/abs/2505.01618) ($1/L$ depth scaling)"}, "questions": {"value": "1. **Depth Convergence Rate** Could the depth convergence rate derived in this work be too pessimistic? The authors of this submission derive a square error convergence rate of $1/L$ by considering convergence of residual variables $h$ to the SDE for the limiting residual stream. I think if the authors are concerned about convergence of the *neural network predictions*, the error rate should be $1/L^2$ like in [Bordelon et al 2023](https://arxiv.org/abs/2309.16620) and [Hayou 2024](http://www.jmlr.org/papers/v25/23-1163.html). The reason for this is that while the preactivation variables follow an SDE with $1/L$ square error rate, the **kernels** which govern the behavior of the network follow an ODE in the limit which has a more favorable error rate.  \n2. **Data / Batch / Steps Dependence**: How many data points are used in Figure 4 where the authors show convergence of the (non-zero) minimum kernel eigenvalue. At some point if the dataset is sufficiently large compared to width, should this eigenvalue decrease to zero? Providing more details about these figures could be very helpful in their interpretation. \n3. The convergence plots in"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fHJ8VHnXek", "forum": "LD0W6HEwsq", "replyto": "LD0W6HEwsq", "signatures": ["ICLR.cc/2026/Conference/Submission1938/Reviewer_5S2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1938/Reviewer_5S2H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826720570, "cdate": 1761826720570, "tmdate": 1762915961952, "mdate": 1762915961952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a mathematical framework for understanding scaling laws in deep residual networks (ResNets) trained with stochastic gradient descent (SGD) under depth-adapted mean-field parameterization (μP). It introduces *Neural Feature Dynamics* (NFD): a coupled forward-backward stochastic differential equations (SDE) that captures how features and gradients co-evolve during training in the joint infinite-width and infinite-depth limit."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The discussion of depth scaling is intrinsically valuable, as it elucidates the fundamental mechanism that stabilizes feature propagation and aligns the asymptotic behavior of networks in both the large-width and large-depth regimes."}, "weaknesses": {"value": "**Lack of novelty and insufficient acknowledgment of prior works.**\n\nThe $1/\\sqrt{depth}$​ scaling and its implications for residual networks have been extensively analyzed in the literature. In particular, several prior works have already established that:\n1. Both forward and backward processes in deep residual networks converge to stochastic differential equations (SDEs) in the joint infinite-width and infinite-depth limit [3,4]. \n2. These width and depth limits commute in the asymptotic regime [3,4]. \n3. Appropriate depth scaling ensures dynamical isometry [1].\n4. Under this scaling, hyperparameters and learned dynamics transfer consistently across both width and depth [4].\n\nConsequently, **the main claimed technical novelties of the paper**: the SDE description under mean-field parameterization and commutation of depth and width limits - **have already been derived or empirically demonstrated in previous works**. The manuscript does not show anything new beyond these results, nor does it adequately acknowledge the existing literature.\n\n[1] Tarnowski et al., Dynamical Isometry Is Achieved in Residual Networks in a Universal Way for Any Activation Function, AISTATS 2019.\n\n[2] Yang & Schoenholz, Mean Field Residual Networks: On the Edge of Chaos, NeurIPS 2017.\n\n[3] Hayou & Yang, Width and Depth Limits Commute in Residual Networks, ICML 2023.\n\n[4] Bordelon et al., Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit, ICLR 2024."}, "questions": {"value": "1. Can the authors comment on finite width and finite depth corrections to the asymptotic limit?\n2. Wouldn't be possible to rescale the SDE and just consider a time horizon $T=1$?\n3. Why is the train loss bigger than the test loss in Fig.3(c-d)?\n4. The width plots do not appear to follow a clean $1/n$ decay in the shown range. Could the authors comment on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tTW8KT9tLY", "forum": "LD0W6HEwsq", "replyto": "LD0W6HEwsq", "signatures": ["ICLR.cc/2026/Conference/Submission1938/Reviewer_FrMa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1938/Reviewer_FrMa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968287676, "cdate": 1761968287676, "tmdate": 1762915961033, "mdate": 1762915961033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission extends NNGP in the kernel regime from the traditional infinite-width limit to the joint infinite-with-depth limit at initialization. The authors also extends the depth-level Euler–Maruyama approximation results for ResNets with a single weight matrix per layer from the forward pass with ReLU activation to both the forward and backward pass with more general Lipschitz continuous activation funtions. The authors finally derive the joint limiting training dynamics after a fixed (constant) number of gradient steps using online SGD with general activation functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The technical contributions of rigorously extending NNGP from the infinite-width limit to the joint infinite-with-depth limit and extending the depth-level Euler–Maruyama approximation results from the forward pass under ReLU to both the forward and backward pass under weaker regularity conditions are solid\n- The technical contribution of tackling the so-called neural feature dynamics under general activation functions for online SGD is solid\n- The proofs are largely well-written. And the ensemble of propositions and theorems (except Proposition 1) serves as a good lecture note of scaling limits of ResNets.\n- Though Proposition 8 is an instantiation of Tensor Programs, its explicit exposition of $\\tau^2/$ here still serves as a new observation, which the reviewer consider to be significantly benificial to the broader audience."}, "weaknesses": {"value": "1. The last line of the proof of Proposition 1 is **mathematically wrong**. In particular, the reviewer kindly remind the authors an elementary fact that\n$$\n\\lim_{L \\to +\\infty} \\big(1 + c_1 \\sqrt{\\frac{T}{Ln}}\\big)^L = \\exp(2c_1\\sqrt{T/n}),\n$$\nwhich is a **finite** quantity in the infinite-depth limit. Thus, the word \"implies\" in Line 200 is wrong if $c_1 > 0$ and $c_2 = 0$, which is the case for **many** actiation functions in practice (because $c_2 = 0$ as long as $\\phi(0)=  0$). And in fact, popular activations function (in practice) such as ReLU, SiLU, and GELU all satisfy $\\phi(0) = 0$.\n  - By the way, the term $c_2 \\sqrt{TL}$ on the RHS of Proposotion 1 is non-zero only for $\\phi(0) \\neq 0$, but to the knowledge of the reviewer, activation functions with $\\phi(0) \\neq 0$ is very rare in practice, either for post-activation ResNets or pre-activation ResNets; which means this term might not be significant for separating post-activation versus pre-activation variants.\n2. In Section 3, the authors repeatedly refer to lowercase $t$'s by \"time\", but actually all resutls in Section 3 are the analysis of **initialization** and any lowercase $t$ in Section 3.3 has nothing to do with \"training steps\", which means the lowercase $t$ in Section 3.3 is a **\"continuous depth\"** instead of the training horizon. The reviewer suggest the authors to explicitly mention it clearly that for both the forward-pass SDE in Proposition 2 and the backward-pass SDE in Proposition 6 are the **depth**-level Euler–Maruyama approximation results, which are essentially **free of \"training\"**.\n3. On the contribution of Theorem 1 besides rigorousness: Actually this theorem is high reminiscent of the SDEs in [1], which even considered SGD with mini batches (although this previous paper assumes $\\varphi$ to be identity), while Theorem 1 in this submission is only for online SGD. The reviewer suggest the authors to explicitly discuss about the distinction (at least at an message level, better at a technical level) between the limiting SDEs for ResNets in [1] and the NFD in this submission.\n   - The reviewer knows that the derivations in [1] are not as rigorous as those in this paper, but in terms of \"understanding scaling laws\" (which is the goal of this submission), a detailed comparison between Theorem 1 and **Section 3** of [1] is necessary.\n\nReferences\n\n[1] Bordelon, Blake, and Cengiz Pehlevan. \"Deep linear network training dynamics from random initialization: Data, width, depth, and hyperparameter transfer.\" arXiv preprint arXiv:2502.02531 (2025)."}, "questions": {"value": "1. Suggestion at a message level: Given [2] and [3], when the multiplier for the non-residual branch scales $\\propto L^{-1}$ instead of $\\propto L^{-1/2}$, the limiting distribution of $h_t$ will follow an ODE instead of the non-trivial SDEs mentioned in this submission, in that case, what would be the scaling of Euler-Maruyama discretization error w.r.t. $L$? I believe this question is worth discussing given the approiximation results in [3] for *any constant number of gradient steps*.\n   - Disclaimer: This is NOT considered as a weakness by the reviewer in terms of missing citations.\n\nReferences\n\n[2] Dey, Nolan, et al. \"Don't be lazy: CompleteP enables compute-efficient deep transformers.\" arXiv preprint arXiv:2505.01618 (2025).\n\n[3] Chizat, Lénaïc. \"The hidden width of deep ResNets: Tight error bounds and phase diagrams.\" arXiv preprint arXiv:2509.10167 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "861Vjhmvkh", "forum": "LD0W6HEwsq", "replyto": "LD0W6HEwsq", "signatures": ["ICLR.cc/2026/Conference/Submission1938/Reviewer_KNSS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1938/Reviewer_KNSS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762682891499, "cdate": 1762682891499, "tmdate": 1762915960657, "mdate": 1762915960657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}