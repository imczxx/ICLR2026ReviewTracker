{"id": "nDdpp0285M", "number": 20088, "cdate": 1758302305555, "mdate": 1759897002403, "content": {"title": "Competitive Multi-Agent Delegation for  LLM Reasoning", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in natural language generation, yet they remain limited in complex and multi-step reasoning. We propose COMMAND: COMpetitive Multi-AgeNt Delegation, a framework where a principal LLM assigns tasks to multiple agent LLMs. Agents compete in an environment where utilities depend on both their internal confidence and the principal’s evaluation, incentivizing answers that are higher-quality and better aligned with the principal. We establish theoretical guarantees demonstrating that, under fair comparison, multi-agent systems such as COMMAND provably outperform their single-agent counterparts. Moreover, each agent, via online learning, achieves sublinear regret and its average policy will converge to a Nash equilibrium. Empirical evaluations on multiple benchmarks demonstrate that COMMAND yields significant improvements in factual accuracy.", "tldr": "", "keywords": ["Large language models", "Multi-Agent Learning", "Mechanism design", "Nash equilibrium"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63896b6227315826110fab02567ea7ba391a79c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes COMMAND, a game-theoretic framework that enhances LLM reasoning through competition among multiple agents evaluated by a principal model. Each agent independently generates candidate answers and receives rewards based on both its internal confidence and the principal’s ranking feedback. Theoretically, the authors prove that multi-agent delegation yields higher expected utility than single-agent setups under fair comparison. Empirically, COMMAND improves accuracy across math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an interesting and relevant problem: how to enhance the reasoning performance of LLMs through competitive multi-agent delegation under a principal–agent framework. The idea of coordinating multiple policies via a principal for self-improvement without explicit fine-tuning is conceptually appealing.\n- The framework is supported by theoretical analysis, providing regret bounds and convergence guarantees under online mirror descent, which adds mathematical grounding to the proposed approach.\n- The work connects multi-agent learning and game theory with LLM reasoning, an angle that is potentially useful for understanding cooperative-competitive dynamics in large models."}, "weaknesses": {"value": "- Many implementation details are missing or unclear, making it difficult to fully understand or reproduce the method. For example:\n  - It is not clearly explained how the principal aggregates responses or computes the global utility mentioned in Line 111.\n  - The MCTS process is under-specified: what constitutes a node, how rollouts are defined, and how branching decisions are made remain ambiguous.\n  - The paper does not specify the number of iterations in the competitive loop or the stopping criterion for convergence.\n- In Table 1, it is unclear which base models are used for the reported baselines. Are they the same as the principal’s model, or different ones? Moreover, since the policies in COMMAND may include stronger models than the principal, the paper should include a baseline reflecting the best single policy’s performance for fair comparison.\n- Table 2 and Figure 2 seem to indicate that COMMAND does not outperform the strongest policy in the ensemble and may even slightly underperform it, suggesting that the competitive interaction does not yield consistent gains. I'm not sure whether I misinterpret the results. Correct me if I'm wrong.\n- The experimental scope is limited, focusing solely on mathematical reasoning tasks. Broader evaluation on other domains would strengthen the generality of the claims.\n- The practical contribution is somewhat limited. Although the theory is sound, the improvement margins are small and the framework lacks clear insights into why competition helps or when it may hurt. Also, the relationship between the consumed computation and the performance of COMMAND and other baselines is not reported. It seems that COMMAND may require significantly more compute than other baselines, while yielding only marginal improvement.\n- No ablation or sensitivity analysis is provided to isolate the effects of key components (e.g., number of agents, principal choice). Without this, the contribution feels more conceptual than empirically validated."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rfn4NBGN6D", "forum": "nDdpp0285M", "replyto": "nDdpp0285M", "signatures": ["ICLR.cc/2026/Conference/Submission20088/Reviewer_hjLk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20088/Reviewer_hjLk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898891027, "cdate": 1761898891027, "tmdate": 1762932984839, "mdate": 1762932984839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes COMMAND, a training-free framework that uses game-theoretic principles to improve LLM reasoning through competitive multi-agent delegation. In this setup, a principal LLM ranks answers submitted by multiple agent LLMs. Each agent's utility function combines two components: its internal confidence (measured by self-consistency across its own samples) and the principal's ranking feedback. Agent policies are then updated using mirror descent with the Hedge algorithm.\nThe authors make three main theoretical claims: (i) multi-agent delegation provably outperforms single-agent approaches under fair candidate budgeting, (ii) each agent achieves sublinear regret, and (iii) the time-averaged policies converge to an approximate Nash equilibrium. Empirically, COMMAND shows accuracy improvements on MATH, GSM8K, and GSM-Hard compared to few-shot CoT, a simplified r* (rStar) variant, and a \"Principal-alone\" baseline (Sections 1–3, Section 4.2 Table 1, Figure 2).\nHowever, I notice this work appears quite similar to https://arxiv.org/abs/2506.08292—I'd appreciate the authors clarifying the relationship."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Motivated game formulation. The \"delegation game\" design is elegant: agents optimize a utility that combines their own self-consistency signal with the principal's ranking feedback. This explicitly aligns each agent's search process with the principal's evaluation criteria. The ranking-based reward structure (top=+1, bottom=−1) and the mirror-descent policy updates using exponential weights provide a concrete, implementable mechanism that requires no fine-tuning (Sections 2.2–2.3, Algorithm 1).\n\nTheoretical foundations built. The analysis builds on well-established assumptions (Pareto-optimal play, agent symmetry, non-negative alignment) and standard online learning theory. Theorem 1 formalizes why delegation outperforms single-agent approaches under equal candidate budgets. Theorem 2 proves O(√T) regret with learning rate η=1/√T. Theorem 3 establishes that time-averaged policies converge to a ξT(δ)-approximate Nash equilibrium. Together, these results provide COMMAND with a principled theoretical backbone (Sections 3.1–3.3).\n\nReasonable experimental setup. The evaluation uses heterogeneous 7–8B parameter agents (Mistral-8B-Instruct, Zephyr-7B-Beta, Phi-3-Mini-Instruct, Falcon-7B-Instruct) with LLaMA-2-7B as the principal. Candidate generation employs MCTS with 16 rollouts at depth 5. The benchmarks span MATH, GSM8K, and GSM-Hard with clearly reported sample counts of 300/300/320 respectively (Section 4.1)."}, "weaknesses": {"value": "Significant overlap with concurrent work—this is my primary concern. The closest contemporary work is ECON (From Debate to Equilibrium, arXiv:2506.08292), which is not cited in the paper. ECON also formulates multi-LLM coordination as a game and seeks a (Bayesian) Nash equilibrium with regret guarantees. While ECON uses a hierarchical RL procedure rather than training-free mirror descent, it reports 11.2% mean gains across six reasoning and planning benchmarks (ICML 2025). Given this substantial overlap in problem formulation and approach, I have concerns about the novelty of the contribution, which has influenced my score.\n\nTheory-practice gap in symmetry assumptions. Assumption 1-ii requires symmetric agents sampling from \"the same distribution D\" (Section 3.1, page 4). However, the experimental agents come from different model families (Mistral, Zephyr, Phi-3, Falcon) with inherently different sampling distributions. While the paper argues these models have \"comparable capacity\" and use \"identical sampling procedures,\" this doesn't satisfy the formal symmetry requirement. This gap weakens the applicability of Theorem 1's theoretical comparison to the experimental results (Section 4.1, page 6).\n\nMissing critical baseline for Theorem 1. Theorem 1's central claim compares single-agent versus multi-agent performance under equal total candidate budgets. However, the empirical \"Principal\" baseline doesn't appear to use the same total number of candidates as the multi-agent system (where each agent runs 16 MCTS rollouts). The paper doesn't report a \"single-agent with the same total candidate pool\" ablation, so the core theoretical prediction isn't directly validated experimentally (Sections 3.1, 4.1–4.2).\n\nLimited evaluation scope. All tasks are math-centric; there's no evaluation on code generation, planning, or open-ended QA where verification is more challenging. The evaluation uses relatively small subsets (300/300/320 examples) without reporting confidence intervals or significance tests. Additionally, rStar is implemented as a simplified verifier-only variant, which may not represent a strong baseline (Sections 4.1–4.2)."}, "questions": {"value": "Direct test of Theorem 1. Could you add a single-agent baseline that receives the same total candidate budget as the multi-agent system? For example, one agent could select from the union of all candidates produced by the multi-agent pool. This would directly test Theorem 1's theoretical setup (Sections 3.1, 4.2).\n\nConnection to Bayesian equilibrium. Is there an interpretation of your mirror-descent updates as seeking a (Bayesian) equilibrium under uncertainty? This might help clarify the relationship to ECON.\n\nRobustness to violated symmetry. What happens when Assumption 1-ii (symmetry) is violated, as it is with your heterogeneous agents? Do you have any theoretical extensions or empirical ablations studying scenarios where agent utilities come from different distributions? (Sections 3.1, 4.1)\n\nPrincipal model sensitivity. How sensitive are the results to the choice of principal? If you swap to a different model family (e.g., Mistral or Llama-3) or use a verifier-based reward, do both the absolute accuracy and relative gains change significantly? (Section 4.1)\nGeneralization beyond math. Can COMMAND handle tasks without easily verifiable solutions, such as planning, code synthesis, or open-ended QA? Do you have any preliminary results beyond mathematical reasoning? (Sections 4.1–4.2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SQZiDgr7jH", "forum": "nDdpp0285M", "replyto": "nDdpp0285M", "signatures": ["ICLR.cc/2026/Conference/Submission20088/Reviewer_CTzi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20088/Reviewer_CTzi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938054593, "cdate": 1761938054593, "tmdate": 1762932984298, "mdate": 1762932984298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMMAND, a game-theoretic framework for improving LLM reasoning through competitive multi-agent delegation. In this framework, a principal LLM assigns reasoning tasks to multiple agent LLMs that generate candidate answers and compete for rewards. Each agent's utility combines its internal confidence with the principal's ranking-based evaluation, incentivizing both high-quality outputs and alignment with the principal's preferences. Empirical evaluations on GSM8K, MATH, and GSM-Hard demonstrate modest accuracy improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is training-free, requiring no fine-tuning or parameter updates, and uses only inference-time computation. \n\n2. The paper provides three theorems with complete proofs establishing that multi-agent systems can outperform single-agent counterparts.\n\n3. Experiments show  gains in mathematical reasoning compared to single-agent baselines."}, "weaknesses": {"value": "1. The paper claims \"under fair comparison, multi-agent systems outperform their single-agent counterparts\". However, recent work \"Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?\" has shown that Majority Voting accounts for most performance gains in multi-agent systems, and proved theoretically that debate alone does not improve expected correctness. Therefore, I have two concerns: \n- How do the authors clarify the contradiction between their theory and recent theoretical results? \n- Is the competitive delegation mechanism adding value beyond simple aggregation? Without comparisons to majority voting baselines, it is unclear whether the gains stem from the game-theoretic mechanism or simply from having more independent samples to aggregate. \n\n2. The experiments use relatively old and weak models, such as LLaMA-2-7B. These models have limited reasoning capabilities, making the experiments less convincing. The paper should validate the approach on stronger, more recent models such as Qwen3.\n\n3. All experiments focus exclusively on mathematical reasoning tasks. To substantiate claims about general multi-agent LLM reasoning, the paper should evaluate on diverse domains and standard benchmarks such as MMLU, HumanEval, and HellaSwag. The current narrow evaluation severely limits the generalizability of the findings.\n\n4. The paper uses Monte Carlo Tree Search, but its focus is on the advantages of a multi-agent system over a single agent. Therefore, comparing MCTS + multi-agent with a single agent is unfair and requires ablation experiments. However, I did not see any relevant ablation experiments in the paper, making it difficult to believe that the performance improvement comes from multi-agent.\n\n5. The paper's presentation makes it easy for readers to get lost. And the paper missed some critical details, such as baseline implementations (Which LLM does each baseline use?). These details are essential for reproducibility and fair comparison."}, "questions": {"value": "The paper does not clearly specify which LLM is used for baselines e.g., Few-shot CoT. Are they using LLaMA-2-7B-Instruct? This information is critical for fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ArDvJ1nU9f", "forum": "nDdpp0285M", "replyto": "nDdpp0285M", "signatures": ["ICLR.cc/2026/Conference/Submission20088/Reviewer_A5qW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20088/Reviewer_A5qW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971191144, "cdate": 1761971191144, "tmdate": 1762932983439, "mdate": 1762932983439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes COMMAND, a training-free method for multi-agent LLM reasoning based on competitive delegation. Multiple agent LLMs generate candidate answers and compete for rewards determined by a principal LLM's ranking, combined with their internal confidence. The method employs game theory with agents updating policies via online mirror descent to reach the Nash equilibrium. Experiments on GSM8K, MATH, and GSM-Hard show performance improvements over baselines, though gains are modest (2-9%). The paper provides theoretical guarantees for convergence and regret bounds."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides theoretical guarantees showing that the multi-agent framework of COMMAND improves over its single-agent counterpart.\n\nUnlike RL-based approaches or fine-tuning methods, COMMAND works purely at inference time, making it practical for immediate deployment without requiring additional training resources.\n\nTables 3-4 validate key aspects of Assumption 1, with ~90% Pareto-optimal play compliance and positive correlation between principal and agent utilities (0.15-0.50)."}, "weaknesses": {"value": "The paper's evaluation setting is quite odd. The evaluation only used 300 questions from the GSM8K and MATH datasets and 320 questions from the GSM-Hard dataset. In fact, the complete MATH500 test set only has 500 questions, which wouldn't introduce significant computational overhead. Furthermore, the reasoning chains in GSM8K are not very long, and the reviewer considered the computational overhead to be completely acceptable. Conducting experiments on the complete test set will be more convincing.\n\nThe paper compares against only three baselines (Few-shot CoT, rStar, Principal), missing several essential comparisons: multi-agent debate, self-consistency, etc.\n\nThe paper should include an ablation where all agents use the same LLM (e.g., all agents are LLaMA-2-7B-Instruct). Without this, it is impossible to determine whether performance gains come from the multi-agent mechanism or simply from having one stronger model (e.g., Mistral) in the agent pool."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cagTS2oY39", "forum": "nDdpp0285M", "replyto": "nDdpp0285M", "signatures": ["ICLR.cc/2026/Conference/Submission20088/Reviewer_Rjhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20088/Reviewer_Rjhr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975711564, "cdate": 1761975711564, "tmdate": 1762932982688, "mdate": 1762932982688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}