{"id": "YeagC09j2K", "number": 17171, "cdate": 1758273034843, "mdate": 1763651619663, "content": {"title": "FREAK: A Fine-grained Hallucination Evaluation Benchmark for Advanced MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) suffer from hallucinations. Existing hallucination evaluation benchmarks are often limited by over-simplified tasks leading to saturated metrics, or insufficient diversity that fails to adequately assess the hallucination extent in state-of-the-art multimodal models. To address this gap, we propose FREAK, a comprehensive multimodal benchmark designed for fine-grained hallucination assessment in MLLMs. Through high-quality photorealistic images featuring fine-grained counter-commonsense edits, FREAK innovatively evaluates hallucination phenomena in detailed visual perception of MLLMs. Extensive experiments on FREAK show severe hallucination issues in SOTA models regarding detailed visual perception. To enable deeper investigation, we curate a controlled subset to indirectly evaluate the model’s ability to perceive target detailed information. Through systematic evaluation of prevailing Chain-of-Thought\n(CoT) prompting techniques within this task, we reveal critical insights regarding hallucination patterns and model reasoning processes.", "tldr": "This paper proposes a novel benchmark designing for MLLMs' fine-grained hallucination evaluation, revealing the severity of fine-grained hallucinations in advanced MLLMs and experimentally analyzes the limitations of CoT in hallucination tasks.", "keywords": ["MLLM", "VLM", "Hallucination", "Benchmark", "Chain-of-Thought"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/717a06d51dbdf7cfe23baff5409fc402193023df.pdf", "supplementary_material": "/attachment/d48785136f8fd60da23a414057e9772dfd8b908d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FREAK, a benchmark for evaluating fine-grained hallucinations in MLLMs using photorealistic images with subtle counter-commonsense (CCS) edits created via a novel \"generate-then-edit\" pipeline. FREAK tests detailed perception across 6 categories. Experiments show SOTA MLLMs struggle significantly (~45% accuracy vs. 86.71% human), revealing severe limitations, and Chain-of-Thought prompting often degrades performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper writes well.\n2. The focus on fine-grained hallucinations, where models find it difficult to perceive subtle details contradicting common sense, is highly relevant.\n3. Instead of relying solely on direct text-to-image generation or manual editing, the two-stage process (generate normal -> edit CCS detail) allows for the creation of high-quality, photorealistic images with precisely controlled, localized CCS elements.\n4. The gap between SOTA models and humans clearly indicates that fine-grained perception remains a major challenge.\n5. The investigation into why CoT prompting degrades performance is valuable."}, "weaknesses": {"value": "1. While the images are generated/edited, the initial CCS descriptions and the subsequent questions/distractors are generated by LLMs. This introduces the possibility of biases inherent in the LLMs used during benchmark creation.\n2. The quality and nature of the CCS images depend heavily on the capabilities of Seedream3.0 and SeedEdit3.0.\n3. It focuses primarily on perception, less on deeper reasoning.\n4. The pipeline is model-assisted, but the dataset size is still very small. It would be better if it could be scaled up."}, "questions": {"value": "1. The image editing process could introduce subtle visual artifacts. How do the authors ensure that models are identifying the semantic, counter-commonsense errors, rather than simply learning to detect these low-level editing artifacts?\n2. The benchmark is explicitly designed for 'fine-grained perception.' While valuable, this focus seems to neglect hallucinations that arise from multi-step reasoning. Could the authors discuss this trade-off and the benchmark's limitations in assessing these more complex failure modes?\n3. For a semi-automated pipeline, the resulting dataset of ~1.8k images is smaller than expected. Could the authors quantify the main bottleneck in their workflow (e.g., image generation failure rate, human verification time) that limits the benchmark's scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "20rome7UUs", "forum": "YeagC09j2K", "replyto": "YeagC09j2K", "signatures": ["ICLR.cc/2026/Conference/Submission17171/Reviewer_Xkgu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17171/Reviewer_Xkgu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761580240255, "cdate": 1761580240255, "tmdate": 1762927151826, "mdate": 1762927151826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed FREAK, a challenging hallucination benchmark for vision-language models. It follows a 3-step pipeline at construction: 1) generate counter-commonsense (CCS) textual description using LLMs 2) generate CCS images using an image editing models and 3) generate MCQ/open-ended questions using LLMs followed by postprocessing and a human inspection process. Results show that even humans only achieve 86% accuracy, followed by gemini-2.5-pro at 43%, indicating the challenging nature of this benchmark. They also find that CoT/reasoning does not bring performance boost."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The benchmark dataset is developed with strong motivation and should be beneficial for diagnosing models.\n\n2. The blind-test human experiments for  validation makes the dataset more solid.\n\n3. The in-depth analysis in sec6 provides insights on why the model hallucinates, potentially inspiring better model development."}, "weaknesses": {"value": "1. A major drawback of this work is the taxonomy is not exhaustive or mutually exclusive, nor is it hierarchical. For instance, in fig2, the example question for the “Analysis” dimension also seems to fit into the “Position” dimension. Besides, while spatial relation is included, questions to probe general relations between objects seem missing (e.g. the “riding” relation as in “a person riding a horse”).\n\n2. In sec4.2, why are errors divided into commonsense errors and others? Meanwhile, using LLMs to classify error types is not validated.\n\n3. In sec4.2, “Hallucination Rate (HalluRate) is the proportion of cases where the model either outputs a commonsense answer in free-form questions or selects the commonsense distractor in multiple-choice questions.” How to judge whether a model outputs a commonsense answer? Is every open-ended question paired with a commonsense answer for judgement?\n\n4. In sec6.1, can you elaborate on the input/output/groundtruth of the normal and CCS settings?"}, "questions": {"value": "1. Tab1 can be better formatted. Also, the fourth column should be “GPT Series Eval”\n\n2. In line 243-244, when multiple groundtruths may exist, why is the dominant distractor adopted to construct the dataset?\n\n3. One of the goals of blind-test human experiment is to detect potential biases, whose results are not shown. Did you find any bias in the annotated datasets?\n\n4. It’s better to provide some analysis on why reasoning does not boost performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeozxywgFG", "forum": "YeagC09j2K", "replyto": "YeagC09j2K", "signatures": ["ICLR.cc/2026/Conference/Submission17171/Reviewer_bnQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17171/Reviewer_bnQv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795635670, "cdate": 1761795635670, "tmdate": 1762927151551, "mdate": 1762927151551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new fine-grained hallucination benchmark for Multimodal Large Language Models (MLLMs), addressing the limitations of previous benchmarks, which often suffer from low quality, oversimplified tasks, saturated metrics, and limited diversity. The proposed benchmark is constructed from AI-generated, fine-grained counter-commonsense (CCS) images. To build it, the authors first define CCS concepts and then generate and refine images using a generative model. Experimental results show that current MLLMs still perform significantly worse than human experts on this benchmark. Furthermore, the study reveals that reasoning-based approaches such as Chain-of-Thought (CoT) can actually degrade performance in this setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper highlights the limitations of previous MLLM hallucination benchmarks, such as low quality, oversimplified tasks, and insufficient diversity. The authors clearly compare their proposed benchmark, FREAK, with existing CCS benchmarks and demonstrate its superiority.  \n    \n* The finding that Chain-of-Thought (CoT) reasoning negatively impacts performance on fine-grained hallucination tasks is insightful and well explained.  \n    \n* The experiments are extensive, and the analyses are thorough and well-structured."}, "weaknesses": {"value": "Major Weaknesses\n\n* **Dependence on the Image Editing Model:** The quality of the generated CCS images likely depends heavily on the performance of the image editing model. Since this model is not trained specifically on CCS images, it may struggle to edit them properly, potentially leading to corrupted or unrealistic results. It is unclear how the authors ensured that the edited images appear natural. An ablation study or analysis regarding the influence of the image editing model on the benchmark quality would be valuable.\n\n* **Insufficient Justification for the Fine-Grained Hallucination Benchmark:** The motivation for introducing a fine-grained hallucination benchmark is not well justified. The CCS images seem highly artificial, and it is questionable whether users would encounter such counter-commonsense images in real MLLM usage scenarios. The paper should provide stronger reasoning for why this type of benchmark is necessary and how it reflects practical hallucination issues.  \n    \n* **Reliability of the LLM-as-Judge Evaluation:** The paper adopts an LLM-as-judge approach for free-form question evaluation, but its reliability is uncertain. It would strengthen the work to validate this approach by comparing its results with human judgments and analyzing the performance gap between LLM-based and human evaluations.\n\nMinor Weaknesses\n\n* **Ambiguity in Wording (Line 40–41):** The phrase “This saturation arises from inherent limitations in both difficulty and evaluation methods” is unclear. The authors should clarify what is meant by inherent limitations in this context.  \n    \n* **Clarity in Table 2:** It would be helpful to separate reasoning-based models from non-reasoning models in Table 2\\. Without this distinction, the statement “reasoning shows no clear advantage” does not align well with the presented results.\n\n* **Typos and Formatting Issues**  \n  * Line 53: diversity the of → the diversity of  \n  * Line 141: judgments (spelling issue)  \n  * Line 147: MIRAGE Dong et al. (2025) (missing space)  \n  * Table 4: The arrow direction in the “Accuracy – CoT” column for GPT-4.1 appears incorrect."}, "questions": {"value": "* How reliable is the image editing model used for generating CCS images? Are there any ablation studies or analyses that verify its impact on the benchmark quality?  \n    \n* What is the clear rationale for introducing a fine-grained hallucination benchmark in MLLMs? The paper should better explain why such a benchmark is necessary and how it addresses real-world hallucination issues.  \n    \n* How reliable is the LLM-as-judge approach used for evaluating free-form questions? Has its consistency with human evaluation been validated?\n\n#####"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DV6s7rzI3Y", "forum": "YeagC09j2K", "replyto": "YeagC09j2K", "signatures": ["ICLR.cc/2026/Conference/Submission17171/Reviewer_EXmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17171/Reviewer_EXmZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915072098, "cdate": 1761915072098, "tmdate": 1762927151068, "mdate": 1762927151068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FREAK, a benchmark designed to evaluate fine-grained hallucinations in multimodal large language models (MLLMs). Unlike prior benchmarks focused on object-level errors, FREAK targets subtle, counter-commonsense inconsistencies within photorealistic images across six categories (Detection, Counting, Attribute, Analysis, Position, OCR). Built through an automated generate-then-edit pipeline, FREAK includes human-verified annotations for reliability. Experiments on 17 state-of-the-art models show that even top models achieve only around 45% accuracy, far below the human score of 86.7%, indicating that fine-grained hallucination remains a major challenge. The benchmark also reveals that reasoning (e.g., CoT) can amplify hallucinations rather than mitigate them."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces FREAK, a large-scale, AI-generated CCS benchmark that evaluates models across six tasks (Detection, Counting, Attribute, Analysis, Position, OCR), offering a comprehensive assessment of fine-grained hallucination.\n\n* Experiments show that most SOTA models perform far below human baseline on FREAK (≈45% vs. 86.7%), indicating that the benchmark meaningfully exposes robustness gaps under CCS conditions.\n\n* The paper is readable and well-structured, making the motivation, pipeline, and evaluation easy to follow (e.g., the generate-then-edit pipeline and curated Q/A process)."}, "weaknesses": {"value": "* Novelty concerns: CCS-style evaluation has appeared in prior benchmarks (e.g., PhD), and the six task types are common in MLLM evaluation, which may narrow the incremental contribution despite FREAK’s fine-grained focus.\n\n* Although FREAK emphasizes photorealistic images with localized CCS edits, real-world usage spans paintings, animations, and other styles; it’s unclear how well the benchmark reflects such domains or whether deduplication alone guarantees sufficient diversity.\n\n* The human baseline would benefit from reporting inter-annotator agreement or additional statistics to aid reproducibility. (The paper notes a 100-participant blind test but gives limited agreement detail.)\n\n* Since an LLM-as-judge paradigm is used for free-form scoring, a discussion/analysis of the judge model’s bias and reliability would strengthen the evaluation methodology.\n\n* Presentation quality: Numerous minor typos/formatting issues (spacing, quotation, punctuation) detract from polish and should be corrected in revision."}, "questions": {"value": "* Why restrict to photorealistic images? Please justify the choice and discuss how conclusions might change for non-photographic domains (e.g., paintings/animations).\n\n* Your analysis suggests CoT often degrades performance/hallucination rates (e.g., Table 4). Would the same trend hold with stronger reasoning strategies (self-refinement, verification-first CoT, or error-detection loops), especially when early visual misperception could be corrected mid-reasoning? [1-3]\n\n**Referneces**:\n\n[1] Gao, Jun, et al. \"Interleaved-modal chain-of-thought.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" Advances in Neural Information Processing Systems 36 (2023): 46534-46594.\n\n[3] Zhou, Qiji, et al. \"Image-of-thought prompting for visual reasoning refinement in multimodal large language models.\" arXiv preprint arXiv:2405.13872 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I7DgFNtP21", "forum": "YeagC09j2K", "replyto": "YeagC09j2K", "signatures": ["ICLR.cc/2026/Conference/Submission17171/Reviewer_BnmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17171/Reviewer_BnmH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018457759, "cdate": 1762018457759, "tmdate": 1762927150756, "mdate": 1762927150756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}