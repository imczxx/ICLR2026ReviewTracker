{"id": "JtX6oaaJ2d", "number": 1191, "cdate": 1756862543435, "mdate": 1759898222511, "content": {"title": "Improving LLM Unlearning Robustness via Random Perturbations", "abstract": "Here, we show that current state-of-the-art LLM unlearning methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is present in the retain-query. Toward understanding underlying causes, we propose a novel theoretical framework that reframes the *unlearning process as backdoor attacks and defenses*: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in unlearned models' behaviors, similar to successful backdoor attacks. The sense that, LLM unlearning methods *themselves poison the model*, make it more vulnerable to forget-tokens, and *hide rather than erase* target knowledge, describes their true mechanism. To mitigate the vulnerability caused by the forgetting process, we reinterpret the retaining process as a backdoor defense and propose Random Noise Augmentation (RNA), a lightweight, model and method-agnostic approach with theoretical guarantees for improving the robustness of models.  Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models while preserving forget and retain performances. This backdoor attack-defense framework offers insights into the mechanisms of unlearning that can shed light on future research directions for improving unlearning robustness.", "tldr": "A Study on the Robustness of Large Language Model Unlearning", "keywords": ["LLM Unlearning", "Unlearning Robustness", "Unlearning as a Backdoor Attack and Defense Problem"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48cc8e16fe67cd23b67dd8f64d372e8c92470452.pdf", "supplementary_material": "/attachment/444e4f6af0e68d4eaba8fefac248c1b810a7b993.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies why current large language model unlearning methods often weaken model robustness, leading to errors when retain queries accidentally include forget tokens. The authors show that the unlearning process behaves like a backdoor attack in which forget tokens act as triggers that reintroduce forgotten behavior. To mitigate this issue, the paper reframes the retaining process as a backdoor defense and proposes Random Noise Augmentation, a simple and model agnostic technique that injects small Gaussian noise into retain representations during fine tuning. Theoretical and empirical results demonstrate that this method improves the robustness of unlearned models while maintaining their forgetting and retaining performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a fresh angle by connecting LLM unlearning with backdoor attack and defense mechanisms, offering a clear framework for understanding existing weaknesses. \n\n2. Theoretical analysis provides partial support for the proposed method, though it relies on many assumptions that weaken its rigor. \n\n3. The writing is clear, and the figures are well-presented."}, "weaknesses": {"value": "1. The experiments lack sufficient baselines. The authors only study a few non-robust LLM unlearning methods, while many recent approaches have been specifically proposed to enhance robustness in LLM unlearning [1-2].\n2. The experimental evaluation is narrow, focusing only on the WMDP benchmark. How does the proposed method perform on other benchmarks such as TOFU [3] and MUSE[4]?\n3. Using only MMLU to evaluate model utility is limited. It would be more convincing to include additional utility metrics[5].\n4. The paper does not discuss how the proposed method behaves under other types of attacks, such as relearning attacks or adversarial prompts [6].\n5. Many proofs rely on overly strong assumptions. For example, Eq. (9) asserts that adding Gaussian perturbations increases the expected loss, which depends on a strong assumption of local convexity or a positive-definite Hessian. In deep LLM latent spaces, this condition often fails, so the inequality $\\mathbb{E}[\\ell(y|z{+}v)]>\\ell(y|z)$ is not generally guaranteed.\n\n> [1] Tamirisa R, Bharathi B, Phan L, et al. Tamper-resistant safeguards for open-weight llms[J]. arXiv preprint arXiv:2408.00761, 2024.\n> \n> [2] Fan, Chongyu, et al. \"Towards llm unlearning resilient to relearning attacks: A sharpness-aware minimization perspective and beyond.\" arXiv preprint arXiv:2502.05374 (2025).\n> \n> [3] Maini, Pratyush, et al. \"Tofu: A task of fictitious unlearning for llms.\" arXiv preprint arXiv:2401.06121 (2024).\n>\n> [4] Shi, Weijia, et al. \"Muse: Machine unlearning six-way evaluation for language models.\" arXiv preprint arXiv:2407.06460 (2024).\n>\n> [5] Che Z, Casper S, Kirk R, et al. Model tampering attacks enable more rigorous evaluations of llm capabilities[J]. arXiv preprint arXiv:2502.05209, 2025.\n>\n> [6] Łucki, Jakub, et al. \"An adversarial perspective on machine unlearning for ai safety.\" arXiv preprint arXiv:2409.18025 (2024)."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6bfJOj9UAY", "forum": "JtX6oaaJ2d", "replyto": "JtX6oaaJ2d", "signatures": ["ICLR.cc/2026/Conference/Submission1191/Reviewer_1B8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1191/Reviewer_1B8L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761329094249, "cdate": 1761329094249, "tmdate": 1762915702489, "mdate": 1762915702489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose Random Noise Augmentation, a method to avoid the misbehave when forget-tokens are in the retain-queries. The author frames current unlearning methods as inadvertently introducing a backdoor attack and proposes RNA as a backdoor defense to enhance the robustness of unlearned models against non-adversarial forget-tokens in retain-queries. By adding random perturbations in the data retaining stage, RNA significantly improves the model's robustness. The theoretical analysis of this paper focuses on how the presence of forget-tokens introduces randomness in latent representations, and proves a bounded probability for RNA to reject the misbehavior caused by forget-tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper establishes a unified view of the two primary classes of LLM unlearning methods, Representation Misdirection and Preference Optimization, by analyzing both through the lens of the generative latent variable model.\n\n2. The paper provides theoretical guarantees for RNA's effectiveness in improving the robustness of models by rejecting the detrimental effects caused by forget-tokens.\n\n3. Comprehensive experiments are conducted. This extensive evaluation rigorously validates the effectiveness and generalizability of the proposed RNA method by including diverse unlearning methods and demonstrating generalization across three different LLMs. The rigorous testing further includes detailed ablations on the RNA mechanism itself, varying the crucial noise scale and the inner model layers for injection."}, "weaknesses": {"value": "1. The paper's theoretical guarantees are oversimplified approximations because they rely heavily on Assumption 4.1, treating the perturbation $\\epsilon$ as simple, independent Gaussian noise. This ignores the reality that the actual perturbation is deterministic, complex, and highly dependent on the model's parameters and context, a complexity not fully addressed by the mere \"Gaussian-like\" appearance of the empirical activation differences in Figure 9.  The oversimplification is further evidenced by the experimental results in Table 6. If the influence of unlearning could indeed be modeled as Gaussian noise, then by the same logic, adversarial perturbations would exhibit similar characteristics, and the RNA should yield consistent improvements in adversarial robustness. However, the results in Table 6 do not support this expectation.\n\n2. The proposed method requires manipulation of the model's inner layers, which may be impractical for real-world scenarios where users cannot modify these layers to inject random noise. This is an unavoidable weakness, although it is discussed in the appendix.\n\n3. Since the paper does not explicitly report running multiple independent trials using different random seeds or provide measures of variance (e.g., standard deviation), the reported performance figures are susceptible to random initialization effects and sampling randomness."}, "questions": {"value": "1. See weakness 1.\n\n2. How to determine which perturbation is added to the retain-query. Since there can be many key-words in the forget set. \n\n3. It is interesting that the choice of layer significantly affects the effectiveness of the proposed method. In practical settings, should the layer be selected by some principles or determined through grid search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q2x5xxkg1W", "forum": "JtX6oaaJ2d", "replyto": "JtX6oaaJ2d", "signatures": ["ICLR.cc/2026/Conference/Submission1191/Reviewer_wLnL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1191/Reviewer_wLnL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780008183, "cdate": 1761780008183, "tmdate": 1762915702374, "mdate": 1762915702374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes reframing LLM unlearning as a backdoor attack-defense process, showing that current unlearning methods make models fragile when forget-tokens appear in retain queries. To improve robustness, the authors introduce Random Noise Augmentation (RNA), which injects small Gaussian noise into retain representations during fine-tuning. RNA is lightweight, model-agnostic, and theoretically grounded. Experiments on several 7B–8B LLMs demonstrate significant improvements in robustness without sacrificing forget or retain performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper recasts unlearning as a backdoor attack/defense mechanism is original and intuitive. It clarifies why unlearned models “misbehave” when seeing forgotten tokens, they’re activating an unintended backdoor.\n\n- This paper also has a clear theoretical grounding, for example, analytical results (Eqns 9 and 13) give interpretable relationships between robustness, noise variance, and token perturbation.\n\n- Experiments also show substantial accuracy recovery on perturbed MMLU queries."}, "weaknesses": {"value": "- Limited experimental diversity. The experiments focus mainly on the WMDP benchmark and a single model scale. It would strengthen the paper to include additional benchmarks such as MUSE [1] and to evaluate across both larger and smaller model sizes to assess generality.\n\n- Synthetic perturbation design. The way retain sets are perturbed (e.g., replacing one token with “SARS-CoV-2”) feels somewhat artificial compared to real-world mixed-context prompts. Including datasets like MUSE could help verify whether the proposed method remains effective under more natural conditions.\n\n- Terminology clarity. The paper’s use of “unlearning robustness” could be better defined. Traditionally, robustness in unlearning refers to resistance against relearning or recovery of forgotten knowledge, whereas this work mainly studies how retain-set performance degrades when forget-tokens appear. Clearer differentiation would help readers understand the scope.\n\n- Missing robustness evaluations. It would be valuable to test the method against relearning and jailbreaking attacks to more comprehensively assess robustness.\n\n> [1] Shi, Weijia, et al. \"Muse: Machine unlearning six-way evaluation for language models.\" arXiv preprint arXiv:2407.06460 (2024)."}, "questions": {"value": "Please refer to the weaknesses section. I would be willing to raise my score if these issues are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mlf51nfgIo", "forum": "JtX6oaaJ2d", "replyto": "JtX6oaaJ2d", "signatures": ["ICLR.cc/2026/Conference/Submission1191/Reviewer_p1EL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1191/Reviewer_p1EL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927616573, "cdate": 1761927616573, "tmdate": 1762915702222, "mdate": 1762915702222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on understanding and improving robustness of llm unlearning. Different from prior efforts that mostly look at robustness to unlearned content, this work focus on robustness to retain content. This work propose a theoretical framework that frames the unlearning problem as a backdoor attack and defense problem. Based on the understanding, the authors propose random perturbations on top of the latent representation to preserve retain performance when the retain set contains forget tokens."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper focus on understanding robustness of unlearning and specifically look at robustness on the retain set, which is largely under explored in prior literature.\n- The RNA method is a very simple heuristic and seems to be effective on perturbed retain set."}, "weaknesses": {"value": "- I did not get Section 5.1. The author said we train model with the \"poisoned\" forget set and the retain set, but equation 10 is still sampling data from $\\mathcal{Z}$, which is the non-\"poisoned\" forget set and retain set. Then I don't understand the role of $T$ and $\\Omega$ here. Now given equation 11 and equation 12 is correct and make sense, why does the conclusion: \"current state-of-the-art LLM unlearning methods themselves “poison” the model and make it more vulnerable to forget-tokens\" follows? There is no analysis of how current LLM unlearning methods map to this framework. Detailed explanation in this part is largely missing, making this section very confusing.\n- In the experiment section, the evaluation is quite limited. The authors only evaluate on wmdp unlearning and only on one forget token \"SARS-CoV-2\". The authors should evaluate on more tasks (tofu, rwku, etc) and a variety of different forget tokens for each task. Otherwise, it's hard to tell whether the proposed method works for just this dataset or for general unlearning task.\n- Figure 1 is very hard to interpret. For each figure, I suggest using different symbols for different methods and one color for baseline unlearning method, one color for the RNA augmented method. Or use a table to present the results. Otherwise it is very hard to quantify the advantage of RNA."}, "questions": {"value": "See weaknesses above. My main question is the how the framework converts to the conclusion that \"current state-of-the-art LLM unlearning methods themselves “poison” the model and make it more vulnerable to forget-tokens\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7wia56zs2", "forum": "JtX6oaaJ2d", "replyto": "JtX6oaaJ2d", "signatures": ["ICLR.cc/2026/Conference/Submission1191/Reviewer_pZfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1191/Reviewer_pZfT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136511458, "cdate": 1762136511458, "tmdate": 1762915702082, "mdate": 1762915702082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Addtional Results"}, "comment": {"value": "Dear Reviewers, \n\nWe present additional experiments below.\n\nChain-of-Thought (CoT; [1]) is one of the most commonly used prompting techniques for improving reasoning capabilities. The effect of RNA on CoT is a fairly interesting point that might need to be investigated. We conducted additional experiments on GSM8K [2] and GPQA [3] with zero-shot, 4-shot, and 8-shot CoT with Zephyr-7B. The results demonstrated that noise added by RNA introduces minor effects on CoT.\n\n|Method||GSM8K zero-shot|GSM8K 4-shot|GSM8K 8-shot|GPQA zero-shot|GPQA 4-shot|GPQA CoT 8-shot|\n|-|-|-|-|-|-|-|-|\n|RMU|w/o RNA|15.1|37.4|40.8|12.0|24.5|21.8|\n|| **w/ RNA**|13.1 (-2.0)|36.5 (-0.9)|40.6 (-0.2)|12.0 (+0.0)|24.3 (-0.2)|24.1 (+2.7)|\n|Adaptive RMU|w/o RNA|12.9|36.7|41.5|10.9|25.2|21.6|\n||**w/ RNA**|15.1 (+2.0)|37.5 (+0.8)|41.0 (-0.5)|12.2 (+1.3)|19.8 (-5.4)|23.4 (+1.8)|\n|RSV|w/o RNA|17.4| 36.7|42.5|8.2|25.4|21.4|\n||**w/ RNA**|16.9 (-0.5)|37.5 (+0.8)|42.8 (+0.3)|10.4 (+2.2)|23.2 (−2.2)|25.6 (+4.2)|\n|NPO+KL|w/o RNA|14.2|36.2|40.1|10.4|27.0|21.6|\n||**w/ RNA**|14.7 (+0.5)|36.7 (+0.5)|38.9 (−1.2)|9.3 (−1.1)|22.7 (−4.3)|23.6 (+2.0)|\n|NPO+MSE|w/o RNA|10.6|37.6|41.0|11.3|26.1|22.3|\n||**w/ RNA**|11.2 (+0.6)|35.7 (−1.9)|38.8 (−2.2)|9.1 (−2.2)|23.4 (−2.7)|21.4 (−0.9)|\n|DPO+KL|w/o RNA|11.9|36.1|37.2|11.3|23.2|19.8|\n||**w/ RNA**|11.3 (−0.6)|36.9 (+0.8)|38.7 (+1.5)|11.3 (+0.0)|23.2 (+0.0)|19.8 (+0.0)|\n|DPO+MSE|w/o RNA|10.0|36.0|39.8|11.6|23.8|22.5|\n||**w/ RNA**|14.9 (+4.9)|37.5 (+1.5)|40.5 (+0.7)|14.2 (+2.6)|24.3 (+0.5)|24.1(+1.6)|\n|SimNPO+KL|w/o RNA|15.6|36.5|41.0|11.1|20.9|18.7|\n||**w/ RNA**|17.8 (+2.2)|37.5 (+1.0)|41.8 (+0.8)|8.0 (−3.1)|23.6 (+2.7)|20.3 (+1.6)|\n|SimNPO+MSE|w/o RNA|11.0| 38.2|39.5|8.2|24.5|24.3|\n||**w/ RNA**|11.0 (+0.0)|37.9 (−0.3)|40.2 (+0.7)|13.6 (+5.4)|25.6 (+1.1)|23.2 (−1.1)|\n\n[1] Wei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" Advances in neural information processing systems 35 (2022): 24824-24837.\n\n[2] Cobbe, Karl, et al. \"Training verifiers to solve math word problems.\" arXiv preprint arXiv:2110.14168 (2021).\n\n[3] Rein, David, et al. \"Gpqa: A graduate-level google-proof q&a benchmark.\" First Conference on Language Modeling. 2024."}}, "id": "0o1x5tyQpW", "forum": "JtX6oaaJ2d", "replyto": "JtX6oaaJ2d", "signatures": ["ICLR.cc/2026/Conference/Submission1191/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1191/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission1191/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763707841120, "cdate": 1763707841120, "tmdate": 1763707841120, "mdate": 1763707841120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}