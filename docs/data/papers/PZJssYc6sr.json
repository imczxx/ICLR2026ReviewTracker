{"id": "PZJssYc6sr", "number": 14615, "cdate": 1758240141059, "mdate": 1763772065014, "content": {"title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution", "abstract": "Modern AI systems, including LLMs, are trained on diverse tasks (e.g., translation, code generation, math reasoning, text prediction) simultaneously. A key challenge is to quantify the influence of individual training tasks on target task performance --- a problem we term \\textit{task attribution}. A natural solution is leave-one-out retraining, where each task is removed and the model is retrained to measure its effect on target performance. However, this approach is computationally prohibitive at scale. We address this challenge using surrogate models that approximate a target task's performance given any subset of training tasks. While prior work has explored linear surrogates, these only capture first-order (linear) effects and do not model nonlinear task interactions such as synergy, antagonism, or XOR-type relationships. We introduce \\textit{kernel surrogate models}, which better capture these nonlinear relationships. To make kernel estimation tractable, we develop a gradient-based procedure leveraging a first-order approximation of pretrained models, and empirically validate this to be accurate. Experiments across various domains (math reasoning in transformers, in-context learning, and multi-objective reinforcement learning) validate the effectiveness of kernel surrogate models. We find that kernel surrogate models demonstrate a 25\\% higher correlation with the leave-one-out ground truth than linear surrogate models and influence functions (among other baselines), establishing a more accurate and scalable solution for task attribution. Using kernel surrogate models for downstream task selection leads to 40\\% improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.", "tldr": "", "keywords": ["Model interpretability", "Data attribution", "Kernels methods"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0422e826b2f916cc525729402b49488b14a0b13a.pdf", "supplementary_material": "/attachment/6d2c72aee65e8fef887f3385347abb14dd5fc222.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the problem of task attribution: quantifying how each training task affects performance on a target task. The authors propose KernelSM, a kernel surrogate model over subset-indicator vectors trained via kernel ridge regression to capture higher-order task interactions. The method aims to address two limitations of existing approaches: the computational cost of leave-one-out evaluation and the inability of linear surrogate models to capture nonlinear interactions. Experiments on modular arithmetic, in-context learning, and multi-objective RL show that KernelSM achieves higher correlation with leave-one-out estimates and improves downstream task selection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Task attribution is an important and broadly applicable problem, as demonstrated by the diverse experimental settings.\n* Modeling task interactions with kernel methods is a reasonable and well-motivated approach."}, "weaknesses": {"value": "* In my opinion, the issue of capturing nonlinear interactions is relatively straightforward and has already been explored in feature or data-selection settings using linear surrogate methods such as LIME - KernelLIME. This limits the contribution, as the paper essentially replaces linear surrogates with kernel-based ones.\n* There is no theoretical support for the gradient-based estimator obtained by linearizing the model in Section 3.3. It is surprising that this approach works well for highly nonlinear models such as LLMs. The authors should provide more intuition for why this estimator is effective and whether it generalizes to larger model sizes.\n* Baselines: the paper doesn’t discuss and compare with several recent works beyond TRAK, such as [1, 2], which address related problems in data and task attribution.\n\n[1] Bae, Juhan, et al. \"Training data attribution via approximate unrolling.\" Advances in Neural Information Processing Systems 37 (2024): 66647-66686.\n[2] Kreer, Philipp Alexander, et al. \"Bayesian Influence Functions for Scalable Data Attribution.\" High-dimensional Learning Dynamics 2025."}, "questions": {"value": "In tasks like in-context learning, the order of examples can substantially affect the final outcomes. Could the authors clarify how their method accounts for or mitigates this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5colIccBBO", "forum": "PZJssYc6sr", "replyto": "PZJssYc6sr", "signatures": ["ICLR.cc/2026/Conference/Submission14615/Reviewer_yBwo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14615/Reviewer_yBwo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879417738, "cdate": 1761879417738, "tmdate": 1762924995538, "mdate": 1762924995538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies \"task attribution,\" which is a task to quantify the influence of individual training tasks on a model's performance for a given target task. \n\nThe paper aims to become less computationally expensive compared to leave-one-out techniques while being more accurate than linear models which only capture first-order effects.\n\nThis work introduces the use of kernel surrogate models to capture nonlinear task interactions better. The authors develop a gradient-based estimation procedure for fitting these kernel models, leveraging first-order approximations to avoid repeated retraining. \n\nFinally, the proposed approach is validated on different tasks showing improved accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Computational efficiency (comparable to linear surrogate runtime) while being much more accurate (<1% relative error)\n\nClearly explain and support the fact about why linear surrogates work well only when the task does not have interaction (in section 3.1)\n\nSimple and smart method to get rid of multiple retraining of the model in different input subsets."}, "weaknesses": {"value": "Minor point: you didn't clearly define KernelSM. Although it is obvious that it stands for Kernel Surrogate Model.\n\nThe discussion in the results is not well organized and is confusing. For instance, table 3 is not referenced and discussed anywhere\n\nSome components are not explained and well discussed (e.g. SAM and NSO) or they are in the appendix although it would be better to be in the main body"}, "questions": {"value": "There is a paper called \"Register Always Matters\" where they find how different genres of data in the pretraining affect the performance of the model on the test samples. I am wondering if one can use your method to validate their results? No need to answer if you don't know or etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sJhPUEysw9", "forum": "PZJssYc6sr", "replyto": "PZJssYc6sr", "signatures": ["ICLR.cc/2026/Conference/Submission14615/Reviewer_Wqdp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14615/Reviewer_Wqdp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897323595, "cdate": 1761897323595, "tmdate": 1762924994989, "mdate": 1762924994989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework for estimating how each training task impacts performance on a target task. By extending beyond linear models, to capture some of the non-linear interactions between tasks, significant empirical performance gains are found to emerge. This kernel based approach is validated with a range of domains and found to correlate more favourably with the ground truth LOO approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The manuscript is well written and clearly presented.\n\nThe approach presented in the paper is novel, well motivated, and timely.\n\nA broad range of experiments were performed with a suitable set of baselines, and empirical performance of the proposed approach is consistently strong."}, "weaknesses": {"value": "Motivations for the kernel hyperparameters could be more clearly presented.\n \nFigures 3 and 4 appear to be lacking uncertainty estimates"}, "questions": {"value": "What motivated the choice of setting gamma to 10^-5? (section C4) \nIf a sweep of hyperparameters was run, how sensitive are the empirical results to these values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fzaqLRkSqU", "forum": "PZJssYc6sr", "replyto": "PZJssYc6sr", "signatures": ["ICLR.cc/2026/Conference/Submission14615/Reviewer_SaVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14615/Reviewer_SaVr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987325197, "cdate": 1761987325197, "tmdate": 1762924994208, "mdate": 1762924994208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KERNELSM, a kernel-based surrogate modeling approach for task attribution in multi-task learning. The method captures nonlinear relationships between training tasks and model performance, going beyond the linear assumptions of prior approaches. The paper provides (1) a theoretical framework connecting influence functions and linear surrogate models, clarifying their limitations; (2) an efficient gradient-based algorithm for estimating kernel surrogate models without retraining; and (3) comprehensive experiments across arithmetic reasoning, in-context learning, and multi-objective reinforcement learning, showing consistent improvements in attribution quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and clearly written. \n2. The theoretical discussion is solid; it connects influence functions and linear surrogate models, offering valuable insights into their relationship and the limitations of linear approximations.\n3. The method seems to perform well against all baselines, and the evaluation tasks are comprehensive (modular arithmetic reasoning, in-context learning, multitask RL)."}, "weaknesses": {"value": "In Table 4, the comparison seems to exclude the task-level adapted versions of TRAK and TracIn, and the paper also does not specify their hyperparameter settings, which may reduce the credibility of the results reported in Table 3.\n\nAlthough the evaluation covers a diverse and comprehensive set of tasks (including arithmetic reasoning, in-context learning, and multi-objective reinforcement learning), the actual number of training tasks in each setting remains relatively small (only 10–50)."}, "questions": {"value": "1. While Appendix C.4 provides an empirical comparison among polynomial and RBF kernels, could the authors offer more theoretical justification or intuitive reasoning for why the RBF kernel performs best in this method?\n\n2. Is this method sensitive to the choice of hyperparameters (λ and γ)? Are there any guidelines or heuristics for selecting these hyperparameters in practice? I am asking this because some other data attribution methods (e.g., TRAK) have already had practical heuristics for hyperparameter selection. If KERNELSM is highly sensitive to λ and γ, the additional hyperparameter search could increase the computational cost of applying the method in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nmz5S0vtC5", "forum": "PZJssYc6sr", "replyto": "PZJssYc6sr", "signatures": ["ICLR.cc/2026/Conference/Submission14615/Reviewer_3bLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14615/Reviewer_3bLy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996640386, "cdate": 1761996640386, "tmdate": 1762924993703, "mdate": 1762924993703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}