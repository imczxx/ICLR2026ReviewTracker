{"id": "vprM1GQdmx", "number": 19407, "cdate": 1758295981409, "mdate": 1759897040786, "content": {"title": "MALLVi: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation", "abstract": "Task-planning for robotic manipulation tasks using large language models (LLMs) is a relatively new phenomenon. Previous approaches have relied on training specialized models, fine-tuning pipeline components, or adapting LLMs with the setup through prompt tuning. However, many of these approaches lack environmental feedback. We introduce the MALLVi Framework, a Multi-Agent Large Language and Vision framework designed to solve robotic manipulation tasks that leverages closed-loop feedback from the environment. The agents are provided with an instruction in human language, and the vision-language model (VLM) is also given an image of the current environment state. After thorough investigation and reasoning, MALLVi generates a series of realizable atomic instructions necassary for a supposed robot manipulator to complete the task. The VLM receives environmental feedback and prompts the framework either to repeat this procedure until success, or to proceed with the next atomic instruction. Our work shows that with careful prompt engineering, the integration of five LLM agents (Decomposer, Perceptor, Thinker, Actor, and Reflector) can autonomously manage all compartments of a manipulation task - namely, initial perception, object localization, reasoning, and high-level planning. Moreover, the addition of a Descriptor agent can introduce a visual memory of the initial environment state in the pipeline. Crucially, compared to previous works, the reflecting agent can evaluate the completion or failure of each sub-task. We validate our framework through experiments conducted both in simulated environments using VIMABench, RLBench and in real-world settings. Our framework handles diverse tasks, from standard manipulation benchmarks to custom user instructions. Our results show that the agents communicating to plan, execute, and evaluate the tasks iteratively not only lead to generalized performance but also increase average success rate in trials. The essential role of the reflecting in the pipeline is highlighted in experiments.", "tldr": "An integrated framework named MALLVi based on LLM agents to perform generalized robotics manipulation tasks.", "keywords": ["Object Localization", "Robot Manipulation", "Multi-Agent Systems", "LLM"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1c1ba19ee7953b565b5930a4d7d3d909e6145b5.pdf", "supplementary_material": "/attachment/9029cfa76e644216b833836d3cad76b277617b8c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MALLVi, a multi-agent framework designed to solve robotic manipulation tasks from natural language and visual inputs. The core problem it addresses is the fragility of open-loop Large Language Model (LLM) planners in dynamic environments. MALLVi's architecture decomposes the problem across specialized agents: a Decomposer (breaks high-level prompts into atomic subtasks) , a Descriptor (builds a spatial graph of the scene for memory) , a Localizer (a \"CV toolkit\" using models like GroundingDINO and SAM to perceive, ground, and project grasp points) , a Thinker (an LLM that converts subtasks into 3D-parameterized actions) , an Actor (executes commands via a predefined API) , and a Reflector (a Vision-Language Model that provides closed-loop feedback by verifying subtask success or failure). This design, particularly the Reflector, enables the system to re-attempt failed subtasks. The framework is validated on a diverse set of tasks in the real world and in simulation (VIMABench, RLBench) , demonstrating significantly higher success rates compared to prior work (MALMM, Wonderful Team, PerAct) and crucial ablations (a single-agent baseline and the framework without the Reflector)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper's primary strength is its modular, multi-agent design. The decomposition of the complex \"prompt-to-action\" problem into specialized roles (planning, perception, reasoning, execution, verification) is logical and well-justified.\n2. The authors test MALLVi across three distinct domains: real-world custom tasks, VIMABench, and RLBench. This demonstrates the framework's applicability to different environments and tasks."}, "weaknesses": {"value": "1. Unclear Novelty: The paper cites many related works on multi-agent LLM systems (RoCo, MALMM) and closed-loop replanning (Replan, ReplanVLM). The perception stack (DINO, SAM) is standard. The primary contribution appears to be the specific integration of these known techniques. The paper would be stronger if it more clearly articulated its precise, novel contribution beyond \"careful prompt engineering\"  of existing ideas.\n2. Missing recent baselines: As the authors claim their multi-agent framework is better than single-agent frameworks, they only compare with an out-of-date baseline, PerAct, on a few RLBench subsets. It is essential to include more modern end-to-end baselines, like LIFT3D, RoboTron-Mani.\n3. Insufficient task types: Although the authors show several different task settings, these tasks can all be considered as different formats of “Pick and Place” tasks. Other types of tasks are not included in the paper, like pouring, opening, inserting, etc. Besides, the long-horizon tasks, like the CALVIN benchmark, are missing in the experiments.\n4. Ambiguity in low-level control: The Actor is said to be “agnostic” to motion planning and executes via a predefined API, but the paper omits robot hardware details, controller type, IK solver behavior, collision checks, grasp execution policy, and timeouts.\n5. Oversimplified Failure Recovery: The Reflector's role is described as triggering a \"reattempt from the beginning of the relevant subtask\". This \"retry\" mechanism is a very simple form of closed-loop control. It does not appear to involve replanning or using the reason for failure to inform the next attempt. This makes the system brittle to unrecoverable errors (e.g., an object is knocked over) or failures that require a different approach (e.g., a new grasp point). The conclusion acknowledges a similar point about \"predefined atomic actions\"  but doesn't fully address the limitation of the \"retry-only\" feedback loop."}, "questions": {"value": "1. Is the Localizer an LLM agent that intelligently selects and calls tools from the \"CV toolkit\" (DINO, SAM, etc.), or is it a fixed, hard-coded function pipeline that runs every time for each subtask?\n2. Does the Reflector agent only provide a binary true/false signal for task completion? Or does it generate a natural language explanation for why a task failed?\n3. What happens if a \"reattempt\"  fails repeatedly? Is there a maximum retry count before the system reports a total task failure? How does the system handle unrecoverable failures, such as knocking an object out of reach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Ca2ViJnwP", "forum": "vprM1GQdmx", "replyto": "vprM1GQdmx", "signatures": ["ICLR.cc/2026/Conference/Submission19407/Reviewer_d7c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19407/Reviewer_d7c7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720117734, "cdate": 1761720117734, "tmdate": 1762931324899, "mdate": 1762931324899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MALLVi, a multi‑agent LLM+VLM framework for robotic manipulation. The system decomposes a natural‑language task into “atomic instructions” (Decomposer), builds a scene memory (Descriptor), performs perception/grounding and grasp‑point extraction (Localizer), chooses 3D pick‑and‑place parameters (Thinker), executes via a simple API (Actor), and verifies each step with a VLM (Reflector) to enable retries. Experiments cover eight real‑world tabletop tasks, VIMABench subsets, and RLBench tasks. Ablations compare a single‑agent variant and a “w/o Reflector” variant; an additional study swaps in open‑source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear modular architecture and closed loop:** The pipeline and the specific roles of each agent are well delineated. The inclusion of a Reflector agent for step‑level verification and retry is practical and easy to follow.\n\n- **Concrete implementation details:** The appendix provides pseudo‑code for the graph state and main loop, prompt templates for agents, and a camera‑model derivation for 3D projection. This improves clarity and partial reproducibility.\n\n- **Results across sim and real:** The authors evaluate on custom real‑world tasks, VIMABench, and RLBench, and show meaningful gains over the single‑agent baseline, highlighting the practical value of reflection and modularization."}, "weaknesses": {"value": "- **Limited novelty relative to prior multi‑agent and closed‑loop planners:** The central idea—using a VLM to verify steps and trigger replanning/reties—has appeared in prior work (e.g., RePlan/ReplanVLM). Here, the claimed novelty is largely the specific arrangement of roles. The paper does not convincingly demonstrate fundamentally novel ingredients beyond prompt engineering and module orchestration.\n\n- **Lack of Genuine Multi-Agent Interaction:** Although the paper repeatedly claims to introduce a “multi-agent framework,” the actual implementation appears to be a sequential pipeline of prompt-based modules rather than a genuinely interactive multi-agent system. Each agent executes in a fixed order, passing textual or visual outputs downstream. The paper does not demonstrate any mechanism for dialogue, negotiation, or joint decision-making between agents.\n\n- **Inconsistent ablations for Math Ops:** The paper argues the Reflector is “essential,” yet Table 1 shows Math Ops performance drops from 80% (w/o Reflector) to 70% with the full system. This inconsistency is not analyzed.\n\n- **Missing inference cost/latency:** Cost/latency of multi‑agent inference is not reported (number of LLM/VLM calls per episode, average wall‑clock). This matters for real deployment."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rfcI1l4EUo", "forum": "vprM1GQdmx", "replyto": "vprM1GQdmx", "signatures": ["ICLR.cc/2026/Conference/Submission19407/Reviewer_JT5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19407/Reviewer_JT5v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982693491, "cdate": 1761982693491, "tmdate": 1762931324065, "mdate": 1762931324065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MALLVi, a modular, closed‑loop, multi‑agent framework for language‑conditioned robotic manipulation. A set of specialized agents—Decomposer, Descriptor, Perceptor, Grounder, Projector, Thinker, Actor, and Reflector—share a state and collaborate to decompose a user instruction into atomic actions, ground these actions in visual observations, execute them, and verify success from visual feedback. The architecture is illustrated in Figure 1 (p. 2) and contrasted with a single‑agent design in Figure 2 (p. 4). The Reflector uses a VLM to check post‑conditions and trigger retries, while the Descriptor creates a scene graph “visual memory” that conditions subsequent perception and planning. Empirically, MALLVi is evaluated on 8 real‑world tabletop tasks (20 trials each), a subset of VIMABench (100 trials), and RLBench tasks (100 trials). It outperforms a reimplemented MALMM baseline and a single‑agent variant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear modular design with closed‑loop verification. The division of labor (Decomposer/Descriptor for planning & memory; Localizer components for grounding; Reflector for success checking) is well‑motivated and clearly described.\n\n2. Ablations that match the claims. Single‑agent vs. multi‑agent and w/ vs. w/o Reflector analyses substantiate the benefits of modularity and closed‑loop verification, especially on compositional tasks.\n\n3. Engineering detail increases credibility. The Projector integrates SAM segmentation with grasp‑point selection (including a radial boundary search) and calibrated 3D projection/IK; stereo disparity is used to derive axial depth, and equations are provided."}, "weaknesses": {"value": "1. Limited evaluation breadth for long‑horizon and transfer. The tasks are solid, but the framework would be better situated with long‑horizon/transfer benchmarks like LIBERO, SIMPLER, or CALVIN, and with a sim‑to‑real gap analysis and failure taxonomy.\n\n2. Memory and scene evolution. The Descriptor captures an initial scene graph, but it’s not fully clear how the “visual memory” is updated through time and how contradictions (e.g., moved objects) are reconciled across agents. A brief study on memory drift and corrections (possibly instrumented via the Reflector) would align with the paper’s focus on feedback.\n\n3. Ambiguity in the confidence‑weighted detection fusion. The Grounder combines GroundingDINO/OWLV2 with graph‑consistency weighting, but the exact weighting function, thresholds, and failure‑handling policy are not fully specified.\n\n4. Baseline and fairness. The selected tasks in RLBench differ from those in MALMM and appear to be much easier."}, "questions": {"value": "The major concern and the largest factor impacting my opinion is the experiments. Please refer to my questions in the weakness section and discuss them in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NmnNf1QgAl", "forum": "vprM1GQdmx", "replyto": "vprM1GQdmx", "signatures": ["ICLR.cc/2026/Conference/Submission19407/Reviewer_oLUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19407/Reviewer_oLUG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989132817, "cdate": 1761989132817, "tmdate": 1762931323505, "mdate": 1762931323505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces the MALLVi Framework, a Multi-Agent Large Language and Vision framework designed to solve robotic manipulation tasks.\n\n- The MALLVi pipeline processes user prompts through specialized agents: Decompose breaks instructions into atomic steps, Describe provides scene understanding, Perceive processes visual inputs, Ground localizes target objects, Project generates\nmotion trajectories, Think coordinates high-level reasoning, Act executes robotic commands, and Reflect evaluates outcomes to enable iterative refinement and error recovery."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed framework has high application value in the real world. \n\n- Judging from the experimental results, the design of each module is effective."}, "weaknesses": {"value": "Currently, the main shortcomings of the paper are concentrated in the experimental section:\n\n- The authors should report the success rate and accuracy of different modules throughout the entire system to illustrate the sources of error in the whole system.\n\n- For each module, the authors should report ablation results for different models or different schemes and the corresponding implementation details. For example, how are the grasping points generated and selected? How are the objects localized?\n\n- The authors lack a comparison of results with other existing works, such as Voxposer, ReKep, etc.\n\n[1] VoxPoser: Composable 3d value maps for robotic manipulation with language models.\n\n[2] ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TSKRVDPZCE", "forum": "vprM1GQdmx", "replyto": "vprM1GQdmx", "signatures": ["ICLR.cc/2026/Conference/Submission19407/Reviewer_AM85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19407/Reviewer_AM85"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173418563, "cdate": 1762173418563, "tmdate": 1762931323073, "mdate": 1762931323073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}