{"id": "TtBSbhT86Z", "number": 4959, "cdate": 1757820138840, "mdate": 1759898002797, "content": {"title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "abstract": "Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function or file level detections—offering limited actionable guidance to engineers who need precise line-level localization and targeted patches in real-world software development. We present T2L-Agent (Trace-to-Line Agent), a project-level, end-to-end framework that plans its own analysis and progressively narrows scope from modules to exact vulnerable lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer (ATA) that fuses runtime evidence—crash points, stack traces, and coverage deltas with AST-based code chunking, enabling iterative refinement beyond single pass predictions and translating symptoms into actionable, line-level diagnoses. To benchmark line-level vulnerability discovery, we introduce T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash families and real-world projects. T2L-ARVO is specifically designed to support both coarse-grained detection and fine-grained localization, enabling rigorous evaluation of systems that aim to move beyond file-level predictions. On T2L-ARVO, T2L-Agent achieves up to 58.0\\% detection and 54.8\\% line-level localization, substantially outperforming baselines. Together, the framework and benchmark push LLM-based vulnerability detection from coarse identification toward deployable, robust, precision diagnostics that reduce noise and accelerate patching in open-source software workflows.", "tldr": "We propose T2L-Agent (Trace-to-Line Agent), a project-level framework that incrementally narrows vulnerability localization from modules to lines through multi-round diagnostics.", "keywords": ["Bug Localization", "Large Language Models", "Agent Systems"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8545448d821fb02f9708225e275a50a12ed76a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses line-level bug localization, proposing both a benchmark of 50 manually annotated real-world cases and a multi-stage T2L-Agent framework that combines dynamic and static analysis with LLM reasoning.\n\nThe agent achieves 58% chunk-level detection and 54.8% line-level localization accuracy.\nT2L-Agent consists of a Trace Analyzer that collects crash and runtime evidence using sanitizers / debuggers, and a Detection module where an LLM generates and iteratively tests root-cause hypotheses.\nThe benchmark is derived from the ARVO dataset of 4 900 reproducible C/C++ vulnerabilities and provides line-level ground truth and LLM-estimated difficulty labels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper focuses on line-level bug localization, which is a critical stage bridging bug analysis and bug repair. The ability to accurately locate the faulty line provides essential context for debugging, program understanding, and subsequent automated repair tasks.\n\n2. The authors construct a fine-grained benchmark based on real-world projects to evaluate line-level localization performance.\nThis benchmark includes detailed annotations from real C/C++ vulnerabilities, providing valuable data for the community to measure progress in this direction.\n\n3. The proposed T2L-Agent framework adopts a multi-stage design that integrates multiple types of static and dynamic analysis tools, including sanitizers and debuggers. Such integration enriches the runtime and structural information available to the LLM, effectively assisting in identifying root-cause locations and improving localization precision."}, "weaknesses": {"value": "1. The Approach section is not clearly written. It lacks a step-by-step description of what each stage takes as input and produces as output, and several key terms—such as crash signature and dynamic evidence graph—are not explicitly defined.\nFurthermore, the example figures meant to help readers understand the workflow are difficult to follow: the visualizations remain in their original JSON format and lack explanations of individual fields, which can easily cause readers to get lost.\n\n2. The definition of the benchmark’s ground truth is missing. Since the paper’s central goal is line-level localization, it is crucial to clarify how the “ground-truth line number” is derived. Without this explanation, it is hard to assess how faithfully the evaluation reflects real localization accuracy.\n\n3. The evaluation lacks comparison with other agentic solutions. The experiments only include ablation studies, which are insufficient to contextualize performance. It would be more convincing to include comparisons with existing LLM-based agentic systems capable of bug localization, such as SWE-Agent (Zhang et al., 2024)."}, "questions": {"value": "### Agent Framework\n\n**Trace Analyzer**\n- What exactly is the output of this analyzer? Is it the *crash signature*? If so, how is that signature obtained or synthesized?  \n- How are the **chunks** in the codebase defined or segmented?  \n- What is the **dynamic evidence graph**? How does it correlate runtime observations with static code features?  \n- Figure 4 is intended to provide examples for readers to understand the workflow, yet the figure itself is difficult to interpret. The results remain in the original JSON format without explanations of the fields, and the tools invoked in the examples are not described. Although the appendix lists the toolkits used in the experiments, the main text and figures provide no reference or explanation, making them hard to follow.  \n\n**Detection**\n- The method description states that it *“selects source code slices based on crash signatures, stack trace information, and vulnerability patterns.”* How is this selection performed concretely? The authors should first clarify what outputs are produced by the Trace Analyzer and then explain how the Detection stage consumes these outputs.  \n- What is the **termination condition** of the iterative detection process? Is it based on confidence thresholds, hypothesis convergence, or a fixed iteration limit?  \n\n### Benchmark\n- How is the **ground-truth line number** defined? The paper states that “Localization requires exact line matches to ground-truth patches,” but for a given bug, there may exist multiple valid patch locations. The patch itself may not uniquely represent the *root-cause line*. Please clarify whether the ground truth corresponds to the first faulty statement, the patched line, or a specific heuristic rule.  \n\n### Evaluation\n- In Table 3, what does “Runtime = 20.2g” mean? Is this a typo or a specific metric unit?  \n- Sections 6.1 and 6.2 essentially constitute an **ablation study**, but the evaluation still lacks comparison with other **agentic baselines** capable of bug localization. Ideally, the baseline section should include comparisons to systems like **SWE-Agent**, which are designed for similar reasoning-driven localization tasks.  \n- The *best results* reported in the abstract (58.0% detection and 54.8% line-level localization) appear only in Table 2 (ablation). It would be clearer and more impactful to highlight these key results earlier in the main tables. Moreover, Table 1 does not explicitly indicate whether the reported numbers belong to baselines or to T2L-Agent itself, which can be misleading and make it appear as though they represent the best results of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "66uhNDjK8i", "forum": "TtBSbhT86Z", "replyto": "TtBSbhT86Z", "signatures": ["ICLR.cc/2026/Conference/Submission4959/Reviewer_3t1k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4959/Reviewer_3t1k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760473400778, "cdate": 1760473400778, "tmdate": 1762917794721, "mdate": 1762917794721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presented T2L-Agent, a multi-agent framework for line-level vulnerability localization. Specifically, T2L-Agent integrates three core components i.e.,  Agentic Trace Analyzer, which bridges runtime crash evidence and static code structure, Detection Refinement, which iteratively improves predictions based on runtime and semantic feedback, and  Divergence Tracing, which explores multiple hypotheses in parallel to capture distributed vulnerabilities. To evaluate its performance, the authors introduce a new benchmark, including 50 expert-verified, reproducible vulnerability cases, enabling evaluation at both coarse (chunk) and fine (line) granularity. Experiments across multiple models (GPT-5, GPT-4, Claude, Qwen, Gemini, etc.) show that T2L-Agent can achieve up to 58% detection and 54.8% localization accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ focus on a practical topic\n+ sound agentic-based approach"}, "weaknesses": {"value": "Weakness:\n\n- limited benchmark dataset\n- incremental contribution\n- no reproduce data\n- limited to a subset of CWEs\n- performance is low \n- no comparison with existing approach\n\n=== comments ===\n\nMy first concern is the representativeness of the benchmark dataset, which only contains 50 instances. The scale of the dataset is quite limited, and such a small dataset constrains the generalizability of the findings, i.e., the dataset may not capture the diversity of real-world vulnerability patterns across different languages and domains. I suggest the authors increase the size of the benchmark to address the generalizability issue.\n\nComparing T2L-Agent to these existing approaches (e.g., D-CIPHER, AgentFL, AutoFL, FuzzGPT, and MirrorFuzz), its contribution appears incremental. Most components, such as trace parsing, AST analysis, and multi-agent refinement, are conceptually similar to existing designs. Also, these tools can be targeted at line-level vulnerability localization with very minor updates.\n\nThe benchmark and experiments appear to be restricted to a narrow subset of CWE categories, primarily memory-related or crash-triggering vulnerabilities (e.g., buffer overflow, null pointer dereference, memory leak). While these categories are critical, they only represent a fraction of real-world software vulnerabilities. More CWE types should be evaluated. \n\nBased on the reported results, the absolute performance remains low for practical adoption, i.e., a detection rate of ~58% and localization accuracy of ~54% means the system still fails on nearly half the cases. In real vulnerability triage, missing vulnerabilities can have severe consequences, so higher precision and recall are essential. With such a low performance, the practical value of the proposed approach is limited."}, "questions": {"value": "1. The proposed T2L-ARVO benchmark includes only 50 vulnerability cases. Could you elaborate on the selection criteria?\n\n2. T2L-Agent includes a multi-agent paradigm and integrates runtime trace analysis. Could you clarify what conceptual advances distinguish it from existing agentic frameworks in the related work part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gIQy4nDi0M", "forum": "TtBSbhT86Z", "replyto": "TtBSbhT86Z", "signatures": ["ICLR.cc/2026/Conference/Submission4959/Reviewer_7K5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4959/Reviewer_7K5h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761369332590, "cdate": 1761369332590, "tmdate": 1762917794268, "mdate": 1762917794268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes T2L-Agent and the T2L-ARVO benchmark, aiming to address project-level vulnerability discovery through a multi-agent and iterative runtime feedback framework. On T2L-ARVO, T2L-Agent achieves 58.0% detection accuracy and 54.8% line-level localization accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation is solid: tackling project-level vulnerability discovery goes beyond the more common function-level localization tasks, and it’s closer to real-world scenarios. The method design also appears intuitive and reasonable. However, I find the evaluation section somewhat disappointing, and I have several concerns below."}, "weaknesses": {"value": "### Major: no baseline.\n\n- The authors introduce many agents developed for the NYU CTF Bench, yet none of them are used for comparison. Conceptually, CTF tasks also require exploring the entire project to locate vulnerabilities, so these should serve as intuitive baselines for comparison. In particular, I don’t see any essential difference between D-CIPHER’s planner–executor framework and T2L-Agent’s design. I’m curious: what prevents existing agents such as D-CIPHER from adapting to T2L-ARVO (e.g., by simply adjusting prompts to output chunks or lines)?\n- In Section 3, the authors note that methods like BAP and LLMAO already perform line-level localization, though they \"still rely on limited runtime evidence\" or \"are not multi-agent systems\". This implies that such methods can tackle the problem, just less effectively. However, there’s no experimental evidence to support this claim.\nThe paper should include direct comparisons to clarify what challenges T2L-Agent uniquely overcomes, how it differs from existing agents, and what exactly enables its improved performance.\n\nAdditionally, even if no specialized baselines are available, general-purpose repository-level agents such as SWE-Agent could be easily adapted (e.g., by prompting them to output chunk- or line-level results). I strongly recommend including such agentic baselines to demonstrate whether the observed improvements come from the proposed T2L-Agent itself or simply from the model’s inherent capabilities.\n\n### Minor: limited evaluation scope\n\n- The concept of a “vulnerability” is inherently tied to how it can be exploited [1]. It’s unclear how a model can learn to fix vulnerabilities without any information about their *exploitation*. If the model can fix them without this context, these issues might represent general *bad code smells* rather than true *security vulnerabilities*—which have normal functionality and typically remain harmless until intentionally exploited by an attacker. From this perspective, the paper doesn’t fully bridge the key research–practice gap. That said, since most current vulnerability-localization work shares this limitation [1], this is a minor issue in comparison.\n- The evaluation scale seems small. Conclusions drawn from just 50 vulnerabilities may fluctuate significantly, especially given that accuracy differences among models are modest. I also don’t quite understand how the paper reports metrics like 44.3% based on only 50 samples—shouldn’t 2% be the smallest possible unit? Perhaps I missed something, but the authors should clarify how the metrics were computed.\n\n### Reference\n[1] Risse, Niklas, Jing Liu, and Marcel Böhme. “Top score on the wrong exam: On benchmarking in machine learning for vulnerability detection.” Proceedings of the ACM on Software Engineering 2.ISSTA (2025): 388–410."}, "questions": {"value": "- Why are existing project-level or CTF-style agents (e.g., D-CIPHER, LLMAO, BAP) not included as baselines?\n- What specific challenges does T2L-Agent address that existing agents cannot? Please show some evidence directly related to such challenges.\n- How does the model learn to detect or fix vulnerabilities without access to information about their exploitation?\n- Could the authors justify the small evaluation set (50 vulnerabilities) and explain how metrics like 44.3% are derived from such a limited sample size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M0Zq04zSnE", "forum": "TtBSbhT86Z", "replyto": "TtBSbhT86Z", "signatures": ["ICLR.cc/2026/Conference/Submission4959/Reviewer_21Fk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4959/Reviewer_21Fk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678083022, "cdate": 1761678083022, "tmdate": 1762917793891, "mdate": 1762917793891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to solve the problem of vulnerability localization.\nThis is an important and challenging problem.\nMany vulnerabilities (e.g., an NPD) may have different locations for the root cause and the symptom (the point of crash).\nThis paper proposes an agent to solve the vulnerability localization problem.\n\nIt proposes a new benchmark containing 50 runnable cases with vulnerabilities. The cases are validated by human experts. The proposed agent considers different sources of information (e.g., static information, debugger, crash stack traces, etc.), and iteratively refine generated hypothesis until a final conclusion is made."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets a very important and challenging problem.\nIt proposes benchmark with human labels.\nThe proposed technique is comprehensive and systematic."}, "weaknesses": {"value": "However, I find it very difficult to understand the technique details of the proposed technique. \nThe writing about key techniques is vague and hinders a fair evaluation of the work.\nWhile I believe it is a solid work, a thorough revision is necessary to make the paper better appreciated by the community.\n\n\nHere are some key details that are missing:\n\n1. In 4.1, the authors mention the proposed agent first collect evidence, then make hypothesis, then iteratively refine those components.\nHere are some key details that are missing:\n(1) the evidences from different components may support/contradict with each other, how the agent formulate the relationship between different evidences?\n(2) how the agent makes hypotheses? Different hypotheses may have dependence with each other. How the agent systematic validate/update the belief across different hypotheses?\n(3) How exactly the refinement is achieved? What are the information that are updated? How the updated information is propagated across different hypotheses?\n\n2. What is the relationship between T2L-Agent and ATA? Is ATA a component of the T2L-Agent?\n\n3. At line 214, the authors mention \"we score and sort candidates by how well they match across multiple signals\". How exactly is this score computed?\n\n4. At line 236, the authors mention \"On a second pass, it rereads those slices to find missed patterns by checking syntactic cues, semantic links such as data and control flow, and alignment with runtime evidence.\" How this is achieved? What are syntactic cues and semantic links? How to justify the re-read stage of the slices? How does it different from the first stage? What is the benefit of having two stages?\n\n5. At line 285, the authors mention \"We combine automated screening with expert review to ensure both realism and balance.\". How realism is evaluated? What is the labeling process of identifying the ground-truth root cause of a vulnerability?"}, "questions": {"value": "Please see the clarification questions in the above discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pCr388Yq7J", "forum": "TtBSbhT86Z", "replyto": "TtBSbhT86Z", "signatures": ["ICLR.cc/2026/Conference/Submission4959/Reviewer_XLJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4959/Reviewer_XLJa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967419246, "cdate": 1761967419246, "tmdate": 1762917793523, "mdate": 1762917793523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}