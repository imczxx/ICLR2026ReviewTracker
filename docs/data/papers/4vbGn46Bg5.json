{"id": "4vbGn46Bg5", "number": 13836, "cdate": 1758223439448, "mdate": 1759897409491, "content": {"title": "Inversely Learning Transferable Rewards via Abstracted States", "abstract": "Inverse reinforcement learning (IRL) has made significant progress in recovering reward functions from expert demonstrations. However, a key challenge remains: how to extract reward functions that generalize across related but distinct task instances. In this paper, we address this by focusing on transferable IRL—learning intrinsic rewards that can drive effective behavior in unseen but structurally aligned environments. Our method leverages a variational autoencoder (VAE) to learn an abstract representation of the state space shared across multiple source task instances. This abstracted space captures high-level features that are invariant across tasks, enabling the learning of a unified abstract reward function. The learned reward is then used to train policies in a separate, previously unseen target instance without requiring new demonstrations in the target instance. We evaluate our approach on multiple environments from Gymnasium and AssistiveGym, demonstrating that the learned abstract rewards consistently support successful policy learning in novel task settings.", "tldr": "We propose TraIRL, a method that learns transferable rewards by mapping ground states into a shared abstract state space via VAE. This enables learning an abstract reward function that generalizes to unseen target task.", "keywords": ["Inverse Reinforcement Learning", "Transfer Learning", "Abstraction"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f1cfb3347320516559b44d4a85ec3d1dd035fcf.pdf", "supplementary_material": "/attachment/4b734d774b03fc6ebc412b0de1afba37b591d9dc.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method for transfer inverse RL where they can transfer rewards to unseen but similar environments.  They use a multi-task VAE, with shared encoder and task-specific decoders, to learn a shared, abstract state space between all training tasks, and a WGAN discriminator to distinguish between expert and policy trajectories on the abstract state space.  This discriminator can then be transferred directly to a new task without new demonstrations.  They demonstrate this on MuJoCo-Gym locomotion tasks with different disabled limbs and on an Assistive Gym task.  They also provide theoretical results that link reward transferability to the abstract state densities between tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors present a strong idea of learning abstract state representations for a shared reward function.\n2. They present promising empirical evidence.  The reward transfer from Ant to HalfCheetah is especially surprising."}, "weaknesses": {"value": "1. It's unclear how the multi-task VAE actually aligns the state representations between different tasks.  Even with the discriminative objective, it's entirely possible that the encoder does not well align semantically equivalent states.  There's some evidence that the learned embeddings do align based on the t-SNE plots but this may not be the case in higher dimensions.\n\n2. The definition of the problem setting and what tasks can transfer rewards between are imprecise.  The locomotion experiments all deal with transferring the same locomotion tasks between characters with different active limbs or embodiments.  The Human Assistive gym tasks do transfer rewards between feeding and itching tasks but seem to require an extra task reward so it's unclear what is being transferred here.\n\n3. Overall the experiments lack breadth.  Does the method generalize to more harder locomotion or manipulation tasks when the reward functions are more complex?  Can it handle other types of unseen environments (environment dynamics differences, obstacles, different reward functions)."}, "questions": {"value": "1. How is your method able to generalize to HalfCheetah from training on Ant tasks?\n2. Is the encoder and reward function frozen for the target task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ZQTq4KPOn", "forum": "4vbGn46Bg5", "replyto": "4vbGn46Bg5", "signatures": ["ICLR.cc/2026/Conference/Submission13836/Reviewer_1yzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13836/Reviewer_1yzq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531716128, "cdate": 1761531716128, "tmdate": 1762924359491, "mdate": 1762924359491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TraIRL, a method for learning transferable reward functions in inverse RL. The core idea is to learn a reward function over an abstract state space that is invariant to the dynamics of different source tasks. The method uses a multi-head VAE to learn this shared abstract representation from expert demonstrations. A Wasserstein GAN objective is then used to structure this abstract space by discriminating between expert and learner trajectories, guiding the learning of a reward function that captures task-agnostic intent. The learned abstract reward is then transferred to a novel target task to train a policy without requiring target-domain demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*   **Principled Approach to Disentanglement:** The paper proposes a well-motivated method to disentangle a task's core reward from its specific dynamics by learning a reward function in an abstract state space. This is a significant conceptual strength.\n*   **Strong Empirical Performance:** The method demonstrates superior performance over strong baselines in transferring rewards across tasks with different dynamics within the same domain (e.g., MuJoCo Ant with different disabled legs).\n*   **Theoretical Grounding:** The paper provides a formal analysis (Theorem 2) that delineates the conditions under which the learned reward is transferable, linking performance to the structural properties of the abstract space."}, "weaknesses": {"value": "*   **Unverifiable Theoretical Assumptions:** The main theoretical result, Theorem 2, hinges on the \"structural alignment\" assumption that optimal policies in source and target tasks are close in the abstract space. The paper provides no mechanism to verify this assumption for a new target task, rendering the guarantee non-constructive. The theory explains when transfer works, but provides no guidance on how to ensure it.\n*   **Incomplete Reward Transfer:** In the AssistiveGym experiments, the learned abstract reward is insufficient to solve the target task and requires supplementation with an explicit, goal-specific reward shaping term. This suggests the method learns a useful behavioral prior (e.g., how to move smoothly) rather than a complete, transferable reward function that specifies the goal.\n*   **High Complexity and Sensitivity:** The framework combines a VAE, a WGAN-GP, and a reward network, resulting in a complex system with many interacting components and hyperparameters. This complexity could pose a significant barrier to reproducibility and practical use.\n*   **Misleading Cross-Domain Results:** The impressive Ant-to-HalfCheetah result in Table 2 is from a \"one-shot\" setting that requires a target expert trajectory and a different training objective (cycle loss), as detailed only in Appendix D.6. Presenting this in the main paper without context overstates the method's zero-shot capability."}, "questions": {"value": "1.  Why was the Ant-to-HalfCheetah result in Table 2 presented without clarifying its one-shot nature in the main text? This seems to overstate the method's zero-shot transfer capabilities. Would you consider adding the zero-shot result to the table for a more complete and transparent comparison?\n2.  The need for additional reward shaping in AssistiveGym suggests the learned \"reward\" is more of a behavioral prior. How does your method's performance (with shaping) compare to a simpler baseline that uses the same goal-based shaping but replaces the learned reward with a simple, handcrafted shaping term (e.g., action magnitude penalty)? This would help isolate the value of the learned abstract component.\n3.  Regarding the structural alignment assumption: Is there any metric that can be computed from the learned VAE and source data to estimate the potential for transfer to a new target domain *before* committing to a full RL training run? Without this, the applicability of the method seems to rely on trial and error."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qfw4jN2XZH", "forum": "4vbGn46Bg5", "replyto": "4vbGn46Bg5", "signatures": ["ICLR.cc/2026/Conference/Submission13836/Reviewer_pTgQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13836/Reviewer_pTgQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606446138, "cdate": 1761606446138, "tmdate": 1762924358857, "mdate": 1762924358857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of reward transfer between environments in inverse reinforcement learning (IRL). To learn transferable rewards, the paper introduces the TraIRL method, which first learns a state abstraction and then learns a reward using a standard IRL framework within this abstracted state space. Experiments across two MuJoCo domains demonstrate that TraIRL can learn rewards that transfer effectively to unseen environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper investigates the important problem of reward transfer between related tasks in IRL. It is crucial that rewards learned through IRL generalize to unseen settings.\n2. The paper formally studies the problem of reward transfer in Section 4.5.\n3. The comparisons in MuJoCo Gym and Assistive Gym show that TraIRL outperforms baselines in generalization to new target tasks. The analysis in Section 5.2.1 also confirms that TraIRL learns a meaningful state abstraction."}, "weaknesses": {"value": "1. TraIRL lacks novelty compared to prior IRL algorithms. The method first learns a state encoding and then performs standard IRL on top of this learned representation. Simply learning a state encoding before applying IRL offers limited advancement over prior work.\n2. The experiments are limited to domains that are already well suited for learning an easily transferable state encoding. Both domains use the ground-truth state. In MuJoCo Gym, the state encoder only needs to ignore the joint information and focus on the torso, as described in Section 1. Thus, the state encoder can merely filter out irrelevant observations. The paper does not compare to the simple baseline of manually removing these clearly non-transferable features in reward learning. A more meaningful evaluation would involve complex observation spaces where learning a transferable state representation for rewards is challenging, such as image-based observations.\n3. The paper lacks sufficient empirical evaluation. Results are presented for only three settings. Additional evaluations are needed to fully assess TraIRL’s performance.\n4. The paper does not report the performance of f-IRL, which TraIRL uses as its underlying IRL algorithm.\n5. Reporting performance based on only ten episodes per task is insufficient. Since the experiments are conducted in fast simulated environments, many more episodes should be used for evaluation.\n6. The paper does not examine how performance changes as the number of expert trajectories varies. Does TraIRL maintain strong performance relative to baselines as the number of trajectories increases? Do baselines also learn good state abstractions when provided with more expert demonstrations? Is TraIRL still able to learn state abstractions with fewer demonstrations?"}, "questions": {"value": "1. How does TraIRL compare to simply removing joint information from the state and retaining only the torso information for reward learning in MuJoCo Gym?\n2. What does “ground state density” refer to in line 391?\n3. Why are the baseline results omitted for Half Cheetah in Table 2?\n4. What is the performance impact of updating the encoder with the reward, as mentioned in line 243?\n5. How does TraIRL’s performance compare to regular f-IRL?\n6. Do the baselines also jointly train the reward across multiple tasks as TraIRL does?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KXL3shGvce", "forum": "4vbGn46Bg5", "replyto": "4vbGn46Bg5", "signatures": ["ICLR.cc/2026/Conference/Submission13836/Reviewer_9uju"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13836/Reviewer_9uju"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935342126, "cdate": 1761935342126, "tmdate": 1762924358183, "mdate": 1762924358183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}