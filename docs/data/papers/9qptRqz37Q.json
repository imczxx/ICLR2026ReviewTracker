{"id": "9qptRqz37Q", "number": 17363, "cdate": 1758275057634, "mdate": 1759897179874, "content": {"title": "Flow Matching for Robust Simulation-Based Inference under Model Misspecification", "abstract": "Simulation-based inference (SBI) is transforming experimental sciences by enabling parameter estimation in complex non-linear models from simulated data. A persistent challenge, however, is model misspecification: simulators are only approximations of reality, and mismatches between simulated and real data can yield biased or overconfident posteriors. We address this issue by introducing Flow Matching Corrected Posterior Estimation (FMCPE), a framework that leverages the flow matching paradigm to refine simulation-trained posterior estimators using a small set of real calibration samples. Our approach proceeds in two stages: first, a posterior approximator is trained on abundant simulated data; second, flow matching transports its predictions toward the true posterior supported by real observations, without requiring explicit knowledge of the misspecification. This design enables FMCPE to combine the scalability of SBI with robustness to distributional shift. Across synthetic benchmarks and real-world datasets, we show that our proposal consistently mitigates the effects of misspecification, delivering improved inference accuracy and uncertainty calibration compared to standard SBI baselines, while remaining computationally efficient.", "tldr": "Learning to correct posterior estimation in simulation-based inference under model misspecification by leveraging scarce high-fidelity data.", "keywords": ["Simulation-Based Infererence", "Flow Matching", "Posterior Estimation", "Multi-fidelity", "Calibration"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f975d16ae3d01710c7685aa5e55cf04ebecab32.pdf", "supplementary_material": "/attachment/afba1c5bc6f71e01d340c6aebc7cff0bfb582be9.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to solve the model misspecification problem under simulation-based inference (SBI) with flow matching. It proposes a two-stage flow-matching scheme: (i) an observation-space flow $T_X$ that transports Gaussian perturbations around a real observation y toward a proxy distribution q(x∣y), and (ii) a parameter-space flow $T_\\\\theta$ that maps samples from a source distribution built from an amortized posterior into an approximation of the true posterior. Across several toy and real-world tasks, the method appears robust in low-calibration-data regimes and improves downstream metrics vs. amortized baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is quite interesting. The construction of $\\\\pi(\\\\theta∣y)$ via $T_X$ reduces the transport gap and stabilizes training in small-calibration settings.\n* To my understanding, it works as a post-hoc calibration layer on top of any amortized posterior estimator and does not rely on restrictive conditional-independence assumptions like some of the previous methods.\n* The empirical result looks solid, which includes multiple tasks, and shows consistent gains as calibration size grows.\n* The presentation is clear, also the source code is provided."}, "weaknesses": {"value": "* Lack of robust SBI baselines: Although the paper argues RoPE is “not directly comparable,” I believe it is still a meaningful baseline because the setups are closely related: both methods correct amortized posteriors under simulator–real mismatch using a small calibration set and a learned transport/alignment step. The key difference is protocol (RoPE is transductive—using the test set as a whole—whereas your method is inductive—per-sample), but that does not preclude comparison; it simply requires transparent labeling. A practical compromise is to report RoPE under its native transductive protocol (clearly marked as such) alongside your inductive results. This lets readers gauge the attainable performance when the full test set is accessible, while still appreciating your method’s per-sample advantages. In addition, adding other previous robust SBI baselines like what RoPE did would further strengthen the paper—e.g., NNPE, NPE-RS, or J-NPE that jointly trains on simulated and calibration pairs. \n* Missing ablations: The paper motivates each module but does not quantify its contribution. It would be great if the authors could add some ablation studies (some may not make sense)\n  * X flow only without theta flow, then you could feed $\\\\tilde{x}$ into a standard NPE.\n  * theta flow only. You could feed the pre-trained NPE directly with y, obtain $\\\\hat{\\\\theta}$, and use your theta flow to map them to the target.\n  * Gaussian start. sample theta_0 from a Gaussian, learn only theta flow with few calibration pairs.\n  * Sequential vs joint training. Is joint training necessary? What if we train them separately?\n* It would be great to add a few more metrics, such as MMD, and also coverage/ECP for posterior calibration."}, "questions": {"value": "* jC2ST scores below 0.5 in wind tunnel example is a bit unusual to me given N_test = 5000. Could you give more explanation about it?\n* A Gaussian $N(y, \\\\sigma^2I)$ is used to sample $x_0$, I think it needs some clearer justification, and how sensitive are results to $\\\\sigma$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YMX1egZiuV", "forum": "9qptRqz37Q", "replyto": "9qptRqz37Q", "signatures": ["ICLR.cc/2026/Conference/Submission17363/Reviewer_TKbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17363/Reviewer_TKbH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509934893, "cdate": 1761509934893, "tmdate": 1762927276965, "mdate": 1762927276965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multifidelity simulator-based inference, where the goal is to infer the posterior distribution of model parameters $\\theta$ given the low-fidelity data $(x,\\theta) \\sim p(x,\\theta)$ and the limited high-fidelity data $(y,\\theta) \\sim p(y,\\theta)$ (termed as calibration data). The paper proposes training two flows (i) a flow from the base distribution to $q(x|y)$ using the calibration data, and (ii) a flow from the misspecified posterior of $\\theta$ to the correctly specified posterios of $\\theta$, using joint flow-matching. The method shows good performance compared to some exististing baselines  in synthetic and real-world tasks,  in terms of quantative and qualitive metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, easy to follow, and the proposed method is cleary articuled in Section 3, where I could not find any tehcnical flaws or ad-hoc justifications. While there are some limitations in the experimental section (see discussion in Weaknesses), the paper demontstrate good and relatively robust emprical performance in synthetic and real-world tasks. When evaluated in the context of multifidelity SBI, the proposed method constitutes a contribution in terms of novel ideas and empirical performance. \n\nIt is great that the experimental section contains visualizations of the learned posteriors for both the baselines and the proposed method. This makes it easier to qualitatively judge performance and shows that the proposed method produces more accurate and robust posteriors than the baselines."}, "weaknesses": {"value": "The main weakness is the insufficiently clear problem formulation, which results in a somewhat exaggerated scope. I am quite convinced that the paper does not tackle SBI under model misspecification but rather multifidelity SBI, where in the latter there is some information about the true model as we have access to the samples $(y,\\theta)$. I try to elaborate this weaknesses below:\n\nLines 78-82: “…high-fidelity data—accurate representations of the phenomenon obtained either from costly high-quality simulations or from ground-truth observations.”\nIsn’t the application scope quite limited in the setting where the ground-truth observations are samples from the joint $p(\\theta,y)$ rather than from the marginal $p(y)$? Typically, $\\theta$ represent latent variables that only exists within the model. For that reason, the considered problem in the paper is not a classical SBI problem but rather a multi-fidelity SBI problem, the term that already appears in the title of (Krouglova et al., 2025). Of course, this same critique applies to (Wehenkel et al., 2025), but I think it is important to be explicit about this to avoid misunderstanding that the proposed method is a general SBI method (as e.g. the title currently hints). This fact is also reflected in the experiments where only real-world tasks are from (Gamella et al., 2025).\n\nGiven this, I suggest:\n1.\tThe title should replace the term “SIMULATION-BASED INFERENCE” to “MULTIFIDELITY SIMULATION-BASED INFERENCE” to honestly capture the scope of the paper. \n2.\tThe accommodiate the first sentences in Section 3: “We consider the general problem of sampling from a posterior distribution…” to something where the paper early states that ‘”…we have access to samples $(y,\\theta)$”.\n\nNext, I discuss some minor weaknesses.\n\nEvaluation metrics (Section 4.2)\n\nThis section lacks justification and discussion for the chosen evaluation metrics. For example, if the task is to infer the posterior of $\\theta$, why the paper does not consider the distance (say Wasserstein) between the inferred  posterior and the ground-truth $p(\\theta)$. This should be feasible at least in the task “Gaussian”, where the ground-truth $p(\\theta)$ is known. Further, why the paper considers MSE of $\\theta$ samples, while we are dealing with distributions, isn’t some metric in the space of probabilitdy distributions (Wasserstein etc.) more appropriate?\n\nBaselines\n\n“….we do not include RoPE (Wehenkel et al., 2025) because it is a method that requires access to the full test set at inference time and is not directly comparable to our approach, as explained in Section 2.” The conditional independence assumption imposed by (Wehenkel et al., 2025) it maybe a limitation of their work but should not be a reason to exclude RoPE from the experiments. Further, can you elaborate why requiring access to the full test set at the inference time is a critical limitation? Concernign the sequential setup, I think that all the experiments in the paper are non-sequential, right?\n\nExperimental results\n\nIn real-world tasks (Wind tunnel, Light tunnel), and when considering the metric that focuses on the posterior samples of $\\theta$ (i.e. MSE), the all the methods looks achieving comparable performance when “Calibration size” is not very small (Figure 4). It would be good to discuss this in the main paper"}, "questions": {"value": "Can you list some real-world problems that fits to the problem considered in the paper? I tried to quickly check (Wehenkel et al, 2025), but so far could only confirm that it matches with (Gamella et al., 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PPoTy7CmX3", "forum": "9qptRqz37Q", "replyto": "9qptRqz37Q", "signatures": ["ICLR.cc/2026/Conference/Submission17363/Reviewer_92qm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17363/Reviewer_92qm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822409382, "cdate": 1761822409382, "tmdate": 1762927276612, "mdate": 1762927276612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to address model misspecification in SBI via flow matching trained on a calibration dataset with known ground truths. I am short on time due to the semester start. Apologies if my reviews are a bit short. I am happy to engage in reviewer discussion should be concerns not be clear. And I am happy to consider increasing my score should the authors provide convincing responses to my concerns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the important problem of model misspecification in SBI/ABI.\n- The method appears theoretical sound and overall sensible to me. \n- I think the paper is well written. But this assessment may be different for readers less familiar with the field of SBI/ABI."}, "weaknesses": {"value": "- Assuming the existance of a calibration dataset with known ground truths is a strong assumption. I know several papers make this assumption. But still, for many application areas we just never have the ground truth parameters available ruling out the proposed method for their analyses.\n- The tested benchmarks are all very low dimensional in parameter space. How does the size of the required calibration dataset scale with the parameter dimensionaliry of the problem? I would have expected this scalability to be analyzed at the very least in toy examples (Gaussian).\n- Some relevant related work was not cited or considered as benchmarks (at least https://arxiv.org/abs/2501.13483 and https://arxiv.org/abs/2502.04949). Perhaps the references therein provide even more relevant research?\n- A comparison of both training and inference speeds between the approaches is lacking to my reading. At least I haven't found it. Providing information in that regard would be important too I think."}, "questions": {"value": "- Just to double check: The approach is fully amortized right? In the sense that, at inference time, only forward passes through the networks are required without any (re-)training?\n- What is the target of inference? The true posterior under the misspecified model or the true posterior under the correctly specified model? See also https://arxiv.org/abs/2502.04949 for a taxonomy of interence targets in the face of misspecification.\n- How strongly is the idea of the method rooted in flow matching? I.e., how easy would it be to replace the flow matching networks with standard diffusion models or even coupling flows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WO1uJ32cbP", "forum": "9qptRqz37Q", "replyto": "9qptRqz37Q", "signatures": ["ICLR.cc/2026/Conference/Submission17363/Reviewer_iYZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17363/Reviewer_iYZs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838712867, "cdate": 1761838712867, "tmdate": 1762927276280, "mdate": 1762927276280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **FMCPE**, a two-stage flow-matching correction for SBI under misspecification: train an NPE on simulations, then learn two conditional flows—one in data space $T_X$​ and one in parameter space $T_\\Theta$—to transport toward a “corrected” posterior using scarce calibration pairs. The authors consider a *joint training* objective of corrected posterior and source distribution. Practical (Algorithm 2) each training tuple requires a **simulator call** ​ and an **ODE solve** to estimate $T_x$ jointly with a correction map $T_\\Theta$ the adjust a pretrained NPE (on low-fidelity or misspecified data) (Algs. 1–2). The paper then empirically validates the proposed approach on several misspecification-tasks from previous work (Wehenkel et.al. 2025) and demonstate empirical advantages against a naive NPE baseline (only trained on calibration set) and a MF-NPE (pretrained on simulations, then fine tune on calibration)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- **Novelty**: To my knowledge this learning-based approach is novel and address limitations of previous approaches  which relied on pre-specified form of misspecification or conditional independence assumptions."}, "weaknesses": {"value": "1. **Conceptual positioning (misspecification vs. multifidelity/multilevel):**  \n    The method and experiments read sometimesmore like a **multifidelity/multilevel** correction pipeline (low-fidelity simulator + scarce high-fidelity pairs + learned transport) than a principled treatment of misspecification. Additionally the comparisons target **NPE** and **MFNPE**; there is **no direct baseline targeted at misspecification** (e.g., methods that explicitly correct simulator bias), and RoPE is excluded from evaluation. This creates an **identity gap** between the stated aim (misspecification) and what is empirically validated (multifidelity fine-tuning). The authors consider the same tasks as in Wehenkel et al. 2025, but claim that its not \"comparable\" because it requires the full test set at inference time. While I agree that this is a limitation that the proposed method addresses, this **does not** prevent a comparison! \n\n2. **Computational cost & performance:**  \n    Every training tuple triggers **both** a **simulation** **and** an **ODE integration**. This pipeline seems **extremely costly** compared to single-flow or single-stage amortized approaches and scales poorly with calibration size. Analysis or discussion of this is completely missing in the manuscript. Furthermore it should come with strong performance gains, specifically it **must** be better than methods just using e.g. the calibration dataset (similar to what the authors demonstrate with NPE). Recent work [1] has show that Tabular Foundation models can do SBI with improved robustness and simulation-efficiency. A good way to demonstrate the efficacy would be to include a comparison against such methods that e.g. only \"train\" (evaluate) on the calibration datasets (or maybe mixed with cheap simulations similar to MF-NPE). \n\n[1] Vetter, Julius, et al. \"Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models.\" _arXiv preprint arXiv:2504.17660_ (2025).\n\n3. **Weak metrics and limited empirical coverage:**  \n        The paper evaluates via **joint** discrepancy metrics (jC2ST, Wasserstein on the joint), rather than **posterior-level** diagnostics that directly assess the **misspecified posterior** when that posterior is tractable or obtainable via MCMC (which is true for at least the Gaussian task). This makes the evidence **indirect** and weaker than necessary. For example this are effectively very similar to *global coverage* metrics which are only *necessary* but **not** sufficient properties that can be fooled by conservative posterior approximations (i.e. any convex combination of the posterior and prior). The empirical evidence in general is additionally limited to 4 tasks. While the authors claim that the method does not require the conditional independence assumption required for rope (and I agree conceptually), its unclear if the presented tasks demonstrate this."}, "questions": {"value": "- **Multi-fidelity vs. misspecification.** These are related but to a some degree also quite different concepts, and the distinction should be communicated clearly by explicitly defining both terms in the text. In misspecification (e.g., sim-to-real gaps), calibration datasets are typically rare or impossible to obtain; by contrast, in multi-fidelity/multilevel settings they exist by definition. Moreover, simulators are generally misspecified to some degree and need not be “cheap” to run, whereas in multi-fidelity setups usually include lower-cost simulators that would justify the additional simulation burden of the proposed approach (but this is not necessarily given in misspecified settings). This should be clearly communicated in the manuscript.\n- Can you add **model-misspecification baselines** i.e. ROPE or others (not just multi-fidelity ones) and report **posterior-level** metrics where the ground-truth misspecified posterior is tractable? \n- Another recent multi-fidelity approach needs to be discussed [1].\n- What is the **wall-clock/NFE** budget per calibration size, broken down into simulator calls vs. **odeint** solves​ and flow training steps?\n\n\n[1] Hikida, Yuga, et al. \"Multilevel neural simulation-based inference.\" _arXiv preprint arXiv:2506.06087_ (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "55RshKjzNz", "forum": "9qptRqz37Q", "replyto": "9qptRqz37Q", "signatures": ["ICLR.cc/2026/Conference/Submission17363/Reviewer_q8cA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17363/Reviewer_q8cA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909652133, "cdate": 1761909652133, "tmdate": 1762927275773, "mdate": 1762927275773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}