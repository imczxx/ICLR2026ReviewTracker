{"id": "qUQARlAx5y", "number": 8415, "cdate": 1758082466746, "mdate": 1759897785484, "content": {"title": "Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics", "abstract": "A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit significant heterogeneity in both observations and dynamics. Model-based RL (MBRL) offers a promising path to sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, leading to poor reconstruction and prediction accuracy. We introduce the Mixture-of-World Models (MoW), a scalable architecture that integrates three key components: i) modular VAEs for task-adaptive visual compression, ii) a hybrid Transformer-based dynamics model combining task-conditioned experts with a shared backbone, and, iii) a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark,  \\textbf{a single MoW agent} (trained once over Atari $26$ games) achieves a mean human-normalized score of $\\mathbf{110.4}$%, competitive with the  $\\mathbf{114.2}$% achieved by the recent STORM—an ensemble of $26$ task-specific models—while requiring $50$% fewer parameters. On Meta-World, MoW attains a $\\mathbf{74.5}$% average success rate within 300k steps, establishing a new state-of-the-art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models. Our code is available in the supplementary materials.", "tldr": "This paper proposes Mixture-of-World models (MoW), a novel and sample-efficient world model architecture for multi-task reinforcement learning.", "keywords": ["Multi-task reinforcement learning", "world model", "transformer", "mixture-of-world models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afaee15946ba0d54af788b6cdcb5152d7ee3286a.pdf", "supplementary_material": "/attachment/253113e5c0d07ac9c86b702102d6557ad7a912df.zip"}, "replies": [{"content": {"summary": {"value": "The Mixture-of-World Models (MoW) is a new, scalable, and parameter-efficient architecture designed for multi-task reinforcement learning (MTRL), specifically targeting visual environments where tasks have highly different visual observations and dynamics. The core challenge is that traditional model-based RL (MBRL) world models struggle to generalize across diverse tasks, leading to poor performance. MoW overcomes this by integrating three key modular components: a) Modular Variational Autoencoders (VAEs) for task-adaptive visual data compression, b) a hybrid Transformer-based dynamics model that shares a core backbone while utilizing task-conditioned specialized experts, and c) a gradient-based task clustering mechanism to efficiently allocate parameters. A single MoW agent demonstrated strong results, achieving a competitive human-normalized score of 110.4% on the Atari 100k benchmark while requiring 50% fewer parameters than the previous state-of-the-art ensemble method (STORM). It also established a new state-of-the-art success rate of 74.5% on the Meta-World benchmark, proving MoW's effectiveness as a foundation for building scalable, generalist world models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and structured.\n- The work is well-motivated: The authors clearly state the general problem of multi-task RL (MTRL) with visual inputs. They effectively motivate their proposed solution of using model-based RL (MBRL) via a world model, specifically addressing the technical obstacle of learning diverse task dynamics with a single model through the use of a Mixture of Experts (MoE).\n- The learning algorithm integrates several interesting components, such as the annealing of the temperature coefficient in the softmax function, which is used to prevent distribution collapse and limit parameter sharing.\n- The results on Meta-World are strong, with MoW's performance using visual inputs clearly outperforming its results using state vectors. However, I would have expected the baselines to be benchmarked with visual inputs as well, in order to demonstrate their failure to handle high-dimensional visual inputs."}, "weaknesses": {"value": "- My main criticism is the poor empirical evaluation, which makes the submission incomplete.\n- Although the two benchmarks, Atari 100k and Meta-World, are diverse, the baselines provided are very sparse.\n- For instance, the learning curves for single-task STORM [1] are missing in Figure 3, where only the MoW results are shown.\n- Since TD-MPC2 [2] was highlighted in the related work section, I would have expected to see it benchmarked against the proposed algorithm on both benchmarks.\n- Since the baseline results on Meta-World were taken from MOORE [3], there are two issues to address.\n- One issue is the total number of environment steps for the baselines. The appendix of MOORE stated 100M (2M per task), which contradicts the 20M cited here. I believe the 20M is a typo in the MOORE paper, as recently highlighted by other work [4]. Although the author is not to blame for this, the correct value should be stated.\n- A more crucial issue is the mismatch between the proposed approach's hyperparameters and the MOORE baseline results (e.g., MOORE used a planning horizon of 150 instead of 500). I advise either rerunning the baselines (which I recommend) or rerunning MoW with the exact hyperparameters stated in the MOORE Appendix.\n- While MoW is clearly more parameter-efficient than single-task STORM, the model size (number of parameters) for MoW and the baselines on Meta-World was not stated. I suspect that MoW uses a very large model relative to MOORE and the other baselines.\n- Since the proposed algorithm contains many algorithmic components, I would have expected a full ablation study detailing their individual effect on performance.\n\n[1] Zhang, Weipu, et al. \"Storm: Efficient stochastic transformer based world models for reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 27147-27166.\n\n[2] Hansen, Nicklas, Hao Su, and Xiaolong Wang. \"Td-mpc2: Scalable, robust world models for continuous control.\" arXiv preprint arXiv:2310.16828 (2023).\n\n[3] Hendawy, Ahmed, Jan Peters, and Carlo D'Eramo. \"Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts.\" The Twelfth International Conference on Learning Representations.\n\n[4] McLean, Reginald, et al. \"Multi-Task Reinforcement Learning Enables Parameter Scaling.\" arXiv preprint arXiv:2503.05126 (2025)."}, "questions": {"value": "- Please benchmark TD-MPC2 on both the Atari 100k and Meta-World benchmarks and compare its performance with MoW. I believe this will highlight the potential of the proposed method.\n- The inclusion of the single-task STORM training curves is highly recommended. Could you please provide these for Figure 3?\n- I believe there should be consistency in the learning settings across MOORE, the baselines, and MoW. Please consider updating the empirical results on Meta-World to account for these aspects. \n- To strengthen the paper, it would be beneficial to demonstrate how MOORE and the other baselines fail to effectively learn from raw visual inputs, which would highlight the advantage of the proposed MoW architecture.\n- What are the model sizes (e.g., number of parameters) for MoW and the baselines when benchmarking on Meta-World?\n- An ablation study is requested to demonstrate the effect of changing the different algorithmic components on overall performance.\n- Could you please provide the individual task learning curves in the appendix for the Meta-World tasks?\n\nI acknowledge that these requests constitute a substantial body of work for the rebuttal period. However, addressing these crucial empirical and validation points is necessary, in my opinion, for the paper to be ready for publication and subsequent acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kgSPKO093l", "forum": "qUQARlAx5y", "replyto": "qUQARlAx5y", "signatures": ["ICLR.cc/2026/Conference/Submission8415/Reviewer_uU94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8415/Reviewer_uU94"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677623750, "cdate": 1761677623750, "tmdate": 1762920315078, "mdate": 1762920315078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a well-motivated architecture—Mixture-of-World Models (MoW)—for multi-task visual RL that marries task-specific VAEs with a mixture-of-expert Transformers and a shared backbone, plus task-prediction and expert-balance losses and a gradient-based task clustering warm-up. The approach targets the core pain point of heterogeneous observations/dynamics and reports strong results: a single model reaches 110.4% human-normalized score on Atari-100k, competitive with STORM’s 114.2% while using ~50% fewer parameters, and achieves 74.5% average success on Meta-World MT50 within 300k steps per task. Overall, this is a solid contribution to scalable, parameter-efficient world models for MTRL with promising empirical evidence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Clear modular design**. MoW uses reasonable modular architecture designs.\n\n**Promising parameter scalability**. MoW demonstrates promising parameter scaling capability. However, it would be interesting if the author can further scale the parameters to investigate if the performance could be further exponentially improved."}, "weaknesses": {"value": "**Natation clarity**. It would be beneficial to clarify the notations in the figures, for example, Fig. 1, which could significantly enhance the readability of this paper. It would also be beneficial if you could explain the notation a little bit when it first appears. Details see questions.\n\n**Lack of experimental evidence**. The author didn't demonstrate the common Atari-100k performance table, including mainstream baselines.\n\n**Insufficient baselines.** STORM is actually not a multi-task RL baseline. I personally cannot understand why the authors include more single-task baselines as comparisons."}, "questions": {"value": "1. What does $m_{\\phi,j}$ represent in Equation 2?\n2. Is $W_k$ sorted? To my perspective, it is not, so you should verify the definition of $W_k$.\n3. What does $N_m$ represent?\n4. What does $l_k^{1:t}$ represent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iClae7upnJ", "forum": "qUQARlAx5y", "replyto": "qUQARlAx5y", "signatures": ["ICLR.cc/2026/Conference/Submission8415/Reviewer_ujJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8415/Reviewer_ujJn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793542402, "cdate": 1761793542402, "tmdate": 1762920314508, "mdate": 1762920314508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mixture-of-World Models (MoW) for multi-task reinforcement learning. The architecture employs task-cluster-specific VAEs combined with a hybrid transformer containing both shared and expert modules. Task grouping is performed through gradient-based clustering, and the training incorporates an expert balance loss and adaptive loss weighting to stabilize learning. Experiments on Atari100k and Meta-World benchmarks show that MoW achieves comparable or improved performance with a more compact model design."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. To the best of my knowledge, modular architectures for world models are still underexplored, making this paper’s direction novel and valuable.\n2. The design choices—such as task-level routing, the combination of fixed task clusters for VAEs with dynamic expert routing for transformers—are conceptually interesting and insightful.\n3. The qualitative comparison in Figure 2 shows that MoW produces notably higher-quality imagined rollouts than the multi-task STORM baseline."}, "weaknesses": {"value": "1. **The ablation study is severely incomplete.** The method introduces many components—such as cluster-specific VAEs, cascaded expert-shared transformers, an auxiliary task predictor head, balanced loss, and gradient-based task clustering—but none of these components are ablated to clarify their individual contributions to performance. Without such analysis, it is hard to assess which design elements are truly essential.\n2. The overall architecture is highly complex, introducing a large number of new hyperparameters, which may make reproducibility and tuning difficult."}, "questions": {"value": "1. While there are few studies on online multi-task reinforcement learning on Atari, there exist some offline multi-task works [1, 2]. Could MoW be extended to offline settings, and would its modular design still provide efficiency or performance advantages there? (I understand this would be a substantial effort and not necessarily feasible during the review period.)\n2. In Line 372, how exactly is the gradient vector per task computed? Is it obtained by averaging gradients over all mini-batches associated with each task?\n\n[1] Multi-Game Decision Transformers\n\n[2] Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Psr0T0px0w", "forum": "qUQARlAx5y", "replyto": "qUQARlAx5y", "signatures": ["ICLR.cc/2026/Conference/Submission8415/Reviewer_hP6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8415/Reviewer_hP6A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899613116, "cdate": 1761899613116, "tmdate": 1762920314082, "mdate": 1762920314082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of sample efficiency in Multi-Task Reinforcement Learning (MTRL). The authors propose Mixture-of-World (MoW) Models, a modular architecture integrating:\n- Modular VAEs specifically clustered for task-adaptive visual compression.\n- A hybrid dynamics model combining task-conditioned MoE (Mixture of Experts) Transformers with a shared Transformer backbone.\n- Gradient-based task clustering to efficiently allocate parameters during a warmup phase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Tailored MoE specifically for World Model Dynamics. Standard MoE in Large Language Models often uses token-level routing. The authors rightly identify that for dynamics modeling, token-level routing can lead to \"fragmented learning,\" where sequential temporal dependencies are broken if consecutive tokens route to different experts.\n2. End-to-end MTRL often suffers from gradient conflicts and tasks dominating the loss landscape. MoW integrates distinct mechanisms to stabilize this.\n3. Parameter Efficiency via Gradient Clustering. Instead of naively assigning a VAE per task (expensive) or one VAE for all (insufficient for heterogeneous visuals), they use a warmup phase to cluster tasks based on gradient similarity. This allows intelligent parameter sharing for the perceptual modules, balancing capacity with efficiency"}, "weaknesses": {"value": "1. Dependency on Warmup Quality. The gradient-based clustering relies heavily on a \"warmup stage\" where a single VAE/predictor set is trained. If the initial warmup yields noisy gradients (common in early RL from pixels), the resulting clusters might be suboptimal and fixed for the rest of training. The paper does not deeply analyze the sensitivity of final performance to the duration or stability of this warmup phase.\n2. Architectural Complexity and Tuning. The system is highly complex, involving multiple specialized losses (reconstruction, reward, continuation, task prediction, dynamics KL, representation KL, harmonious weighting, expert balance). While effective, this increases the hyperparameter surface significantly. Reproducibility outside specifically tuned benchmarks might be challenging without robust auto-tuning mechanisms for these loss components.\n3. Routing Rigidity. While task-level routing prevents temporal fragmentation, it might be too rigid for tasks that share partial sub-dynamics. Task-level routing forces it to choose one set of experts for the entire episode."}, "questions": {"value": "1. Given the high variance of gradients early in standard RL training (especially from pixels), how stable are the resulting task clusters across different random seeds? Did you observe cases where a poor warmup led to irrecoverable sub-optimal clusterings?\n2. How sensitive is MoW to the fixed hyperparameters? Were these tuned specifically for Atari and Meta-World, or do you expect them to generalize to new domains?\n3. Does task-level routing limit zero-shot generalization to new tasks? If a new task is a composition of two existing tasks, your current architecture forces it to choose just one set of experts, whereas token-level might allow it to interpolate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zYAavMoWid", "forum": "qUQARlAx5y", "replyto": "qUQARlAx5y", "signatures": ["ICLR.cc/2026/Conference/Submission8415/Reviewer_XJhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8415/Reviewer_XJhM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969884616, "cdate": 1761969884616, "tmdate": 1762920313673, "mdate": 1762920313673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}