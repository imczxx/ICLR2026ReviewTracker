{"id": "RGmDtMs9w7", "number": 16448, "cdate": 1758264670306, "mdate": 1763619929450, "content": {"title": "Modeling Training Dynamics and Error Estimates of DNN-based PDE Solvers: A Continuous Framework", "abstract": "Deep neural network-based PDE solvers have shown remarkable promise for tackling high-dimensional partial differential equations, yet their training dynamics and error behavior are not well understood. \nThis paper develops a unified continuous-time framework based on stochastic differential equations to analyze the noisy regularized stochastic gradient descent algorithm when applied to deep PDE solvers. \nOur approach establishes weak error between this algorithm and its continuous approximation, and provides new asymptotic error characterizations via invariant measures. \nImportantly, we overcome the restrictive global Lipschitz continuity loss gradient, making our theory more applicable to practical deep networks. \nSpecifically, our study focuses on general second-order elliptic PDEs; however, the proposed framework is not limited to this specific form and can be extended in principle to broader classes of PDEs.\nFurthermore, we conduct systematic experiments to reveal how stochasticity affects solution accuracy and the stability domains of optimizers. \nOur results indicate that stochasticity can have varying impacts on the stability of solutions near different local minima; therefore, in practical training, strategies should be dynamically adjusted according to the local optimization landscape to enhance robustness and stability of neural PDE solvers.", "tldr": "", "keywords": ["DNN-based PDE solvers", "SGD", "continuous modeling", "error estimates"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ef5c5b37e10516e2c0ffa44f6ca0cb117964097.pdf", "supplementary_material": "/attachment/aebdb2fd13486662f763cf1e9f530a4d10a4fcec.zip"}, "replies": [{"content": {"summary": {"value": "The paper claims to propose a continuous-time theoretical framework for neural PDE solvers that removes the need for the global Lipschitz continuity assumption. It introduces a noisy high-order regularization to ensure well-posedness of an SDE corresponding to discrete SGD, proving local weak convergence and providing long-term error estimates for DNN-based PDE solvers. The theoretical contribution is novel, but relies on strong assumptions. Its practical impact is also quite unclear."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretical novelty: proposes a continuous-time analysis framework for neural PDE solvers without global Lipschitz assumptions.\n- Provides long-term error estimates and Laplace-type approximations that may be useful for understanding training dynamics.\n- Establishes local weak convergence from discrete SGD to a regularized SDE."}, "weaknesses": {"value": "While theoretically interesting, the strong assumptions and limited experimental validation reduce the practical impact of the work. The main concerns is listed as following\n- Very limited baseline comparisons. Modern optimizers like AdamW or other recent related literatures (for example **Newton Informed Neural Operator for Solving Nonlinear Partial Differential Equations**) are not compared in the experiments. Based on the results in the manuscript, it's difficult to claim better convergence performance.\n- The theoretical derivation has too many restrictions which can significantly reduce the applicability of the proposed method."}, "questions": {"value": "- The framework relies on a $|θ|^{2s}$ (s≥10) term to guarantee well-posedness. Such assumption may not be desirable in many application conditions. \n- Convergence is guaranteed only in bounded regions and depends on the probability of the process staying within that region. Its global convergence in high-dimensional or steep PDE loss landscapes is not addressed properly. \n- Too simple test cases: Only 2D ODEs / low-dimensional PDEs are tested with very small networks (2 layers, width 10). Performance on complex, nonlinear, high-dimensional PDEs or real physical systems is unknown.\n- Experiments compare only SGD and GD, without modern optimizers (Adam, AdamW, L-BFGS, etc.), making the practical advantage of the proposed approach unclear.\n- Only L2 errors and loss curves are reported. While convergence speed, training time, generalization, and robustness are not evaluated thoroughly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8xAc6JEmr", "forum": "RGmDtMs9w7", "replyto": "RGmDtMs9w7", "signatures": ["ICLR.cc/2026/Conference/Submission16448/Reviewer_5Xb2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16448/Reviewer_5Xb2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715213456, "cdate": 1761715213456, "tmdate": 1762926561140, "mdate": 1762926561140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a continuous-time framework based on SDEs to analyze the training dynamics of deep neural network-based PDE solvers, specifically PINNs. The authors introduce a \"noisy regularized SGD\" algorithm that adds both a high-order regularization term and Gaussian noise to standard SGD. They establish weak convergence between this discrete algorithm and its continuous SDE approximation, removing the restrictive global Lipschitz assumption common in prior work. The paper also characterizes asymptotic error via the SDE's invariant measure using WKB approximation and the Laplace method. Experiments on a simple 1D ODE reveal that stochasticity narrows the stability regime of learning rates and degrades solution accuracy compared to gradient descent, though SGD can outperform GD near sharp minima."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The most significant theoretical contribution of this paper is to establish weak convergence results without requiring global Lipschitz continuity of the loss function. As the authors note, \"our theory more applicable to practical deep networks\" since standard neural networks violate this assumption. The local Lipschitz approach using stopping times (Definition 2) and the decomposition in Theorem 1 is technically sound.\n2. The perspective of analyzing long-term error through the invariant measure of the SDE (Proposition 4) and its asymptotic approximation via WKB methods is interesting. The connection between the maximizers of $S_0$ and the expected error in Proposition 5 provides a different lens on understanding SGD behavior beyond standard optimization/generalization decompositions.\n3. The experiments in Section 4 provide valuable empirical insights. The observation that SGD has a much narrower stability domain than GD near low-sharpness minima (Figure 1) and that stochasticity degrades accuracy even within the stable regime helps explain why PINNs often struggle with precision. The comparison between two regimes with different sharpness levels is well-motivated.\n4. The paper is generally well-written with careful definitions and the proofs appear rigorous."}, "weaknesses": {"value": "1. The practical applicability of the modified algorithm is a bit limited. The noisy regularized SGD requires an extremely high-order regularization term to ensure theoretical guarantees. While the authors acknowledge this in Remark 4, the gap between theory and practice can be concerning. The experiments in Appendix I show the algorithm works on some problems, but the regularization parameter and noise level appear highly problem-dependent. It's unclear how practitioners should set these hyperparameters or whether the theoretical insights transfer to standard SGD.\n2. The uniform moment bounds in Assumption 3 are crucial for Proposition 3 and Theorem 1, yet the authors simply assume this holds without proof or empirical verification. They acknowledge it \"remains open in many settings,\" which significantly weakens the main result. Without this, the weak convergence is only established for processes that stay in $B_R$, and the exit probability bounds don't apply uniformly in $\\eta$.\n3. The experimental validation focuses exclusively on a simple 1D ODE with width-10 networks where the exact solution can be represented. While this enables precise analysis, it's far from the high-dimensional PDEs that motivate DNN-based solvers. The experiments in Appendix I on 2D problems (Helmholtz, Fisher-KPP, Allen-Cahn) show the algorithm can work but don't validate the theoretical predictions about stochasticity effects or compare against standard methods systematically.\n4. The paper promises \"actionable guidance for practical training\" but the main takeaway is not too surprising and doesn't lead to clear recommendations. The suggestion that \"adaptively switching optimizers and step sizes...can be beneficial\" is vague and not demonstrated."}, "questions": {"value": "1. Can the authors provide any theoretical or empirical evidence to support Assuption 3? At minimum, is it possible to verify whether it holds for the experimental settings in Section 4?\n2. The WKB approximation assumes $B_0 \\in C^2$, but is this regularity guaranteed, given that the loss landscape of neural networks is typically not smooth?\n3. In regime 2, SGD outperforms GD even when using a learning rate that causes GD to diverge. Does this contradict the stability analysis?\n4. This paper https://proceedings.mlr.press/v235/chen24ad.html proposes a method that the author claims to solve Allen-Cahn equation (and other equations) towards machine precision. Can the authors comment on how the framework proposed in this paper is consistent with the method in https://proceedings.mlr.press/v235/chen24ad.html ? Is the framework not applicable in their setting at all? If not, can a similar framework be developed?\n5. Are the theoretical results sensitive to the choice of $s=10$ in the regularization? The proof is for $s\\ge10$, but is there an intuition for why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gEBW4VJk2p", "forum": "RGmDtMs9w7", "replyto": "RGmDtMs9w7", "signatures": ["ICLR.cc/2026/Conference/Submission16448/Reviewer_8rio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16448/Reviewer_8rio"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804155847, "cdate": 1761804155847, "tmdate": 1762926560666, "mdate": 1762926560666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work contributes to a deeper understanding of the optimization procedure with stochastic gradient descent (SGD) using tools from stochastic differential equations (SDEs) in the case of training feed-forward neural networks for a class of partial differential equations. The estimation error of the iterative SGD algorithm compared to its continuous counterpart is characterized in the weak form. Computational experiments are used to investigate the effect of stochasticity on the stability of the optimization and provide a comparison to classical gradient descent."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes use of established connection of SGD and SDE via stochastic modified equations (SME) (Li et at. 2017, 2019) and investigates the consequences of modified assumptions. So far this community has not focused much on neural networks for partial differential equations, which is a part of the novelty of this work. For the optimizer setting the authors consider, including the new assumptions, they find that the optimizer weakly converges to its continuous SDE version. An important result is the relaxation of Lipschitz continuity assumption on the network, which was used by previous works.\nThe experimental results are consistent with existing literature on the dynamics of SGD optimizers. To the best of my knowledge, the results are novel for the considered noisy regularized SGD optimizer."}, "weaknesses": {"value": "1) The authors position this contribution in the realm of learning methods for general partial differential equations (PDEs), but it seems that the focus is on specific, second-order elliptic PDEs, as stated in section 2.1.. I suggest making this clear at least in the abstract and the title. The main results (theorem 1) heavily rely on this specific PDE type, with assumptions (1) for the coefficient terms.\n2) \"... a continuous framework\" is also too generic for the title (use \"continuous-time\" instead?); it is also challenging to read about \"continuous time\" frameworks while the PDEs in question are not time-dependent, yet the introduction and abstract make no distinction about this.\n3) The authors claim that \"..our results readily extend to more general equations and deeper network architectures\", but do not prove any of this, or demonstrate it experimentally. The same seems true for \"all results extend directly to the empirical setting\" (l155), where it is not clear at all why choosing a finite-size data set (with e.g. only 10 evaluation points, in higher base-space dimensions... etc) would result in \"direct\" extension from the exact L^2 error.\n4) Similarly, in the title and the main text, \"dnn-based PDE solvers\" are mentioned, but in the setting (e.g. eq. 2), only shallow, two-layer networks are considered. Merely stating that \"the results readily extend to deeper architectures\" is not enough to use a broad statement in the title about DNNs. Shallow networks behave very differently from deep networks, and have very different properties (see e.g. \"Poole, Ben, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. 2016. \"Exponential Expressivity in Deep Neural Networks through Transient Chaos.\" In Advances in Neural Information Processing Systems 29, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett. Curran Associates, Inc. http://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos.pdf\".)\n5) The computational experiments do not show the performance of SGD on a PDE, but just an ODE. There is no discussion why this is a good test case for behavior on PDEs, beyond \"it has a closed form solution\".\n6) For many PINNs the optimizer starts with Adam iterations, followed by additional iterations with L-BFGS (a quasi-Newton method), this is not acknowledged and should be mentioned either as a limitation or future work, especially as the abstract mentions \"adaptively switching optimizers\".\n7) The manuscript lacks a reflection on the limitations of the current work.\n\n8) Minor remarks:\n- The text in the figures is too small\n- It seems that the same notation $|\\cdot|$ is used both for the $L^2$ vector norm as well as for the determinant. Ideally two different notations would be used (e.g. $\\|\\cdot\\|$ for norm, and normal lines $|$ for determinant, or use $\\det(\\cdot)$).\n- Several times \"continuous modeling\" is used (even in the title, and l.053, for example), while I presume the authors meant \"continuous-time modeling\". This must be changed.\n- The explanation of regularization of powers 20 is not sufficient (l240). It indeed seems extremely artificial, and it is not clear at this point in the paper why the power 20 is the barrier between unbounded and bounded growth. There should at least be a link to a discussion later in the paper, or the proof where this is clarified."}, "questions": {"value": "1. Other work (Li et al. 2017, 2019) considers not only SGD but also SGD with momentum and Nesterov's method. Is an extension of your work possible for these variants of SGD? \n2. The loss function for PINNs often includes additional terms (e.g. boundary/initial conditions, additional physical constrains, enforcing symmetry), what kind of impact would these terms have on the SGD dynamics, or what assumptions should be made in regard to them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LIq9ttClTc", "forum": "RGmDtMs9w7", "replyto": "RGmDtMs9w7", "signatures": ["ICLR.cc/2026/Conference/Submission16448/Reviewer_Bqfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16448/Reviewer_Bqfp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16448/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931791880, "cdate": 1761931791880, "tmdate": 1762926560169, "mdate": 1762926560169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}