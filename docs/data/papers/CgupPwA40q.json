{"id": "CgupPwA40q", "number": 14649, "cdate": 1758240801608, "mdate": 1759897357346, "content": {"title": "Reinforcement Learning via Implicit Imitation Guidance", "abstract": "We study the problem of sample efficient reinforcement learning, where prior data such as demonstrations are provided for initialization in lieu of a dense reward signal. A natural approach is to incorporate an imitation learning objective, either as regularization during training or to acquire a reference policy. However, imitation learning objectives can ultimately degrade long-term performance, as it does not directly align with reward maximization. In this work, we propose to use prior data solely for guiding exploration via noise added to the policy, sidestepping the need for explicit behavior cloning constraints. The key insight in our framework, Data-Guided Noise (DGN), is that demonstrations are most useful for identifying which actions should be explored, rather than forcing the policy to take certain actions. Our approach achieves up to 2-3x improvement over prior reinforcement learning from offline data methods across seven simulated continuous control tasks.", "tldr": "", "keywords": ["Reinforcement Learning", "Imitation Learning", "Exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b4b98bf8a74ceab5aeeb4d8c40c0f864565bc6f.pdf", "supplementary_material": "/attachment/48043c0b9ed0aa8fb6b7c8a5de3db2ecdabb7735.zip"}, "replies": [{"content": {"summary": {"value": "This work tackles sample-efficient reinforcement learning using prior data without relying on imitation learning objectives that may hinder long-term reward optimization. The proposed Data-Guided Noise (DGN) framework leverages demonstrations to guide exploration through data-informed policy noise, rather than enforcing behavior cloning. This approach yields 2–3× performance gains over previous RL-from-offline-data methods across seven continuous control tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is clear and easy to follow, and the illustrations effectively convey the proposed method.\n\n- The proposed approach serves as a flexible framework that integrates naturally with other imitation learning for RL methods and can be easily combined with various RL algorithms."}, "weaknesses": {"value": "- The paper lacks discussion and comparison with online imitation learning methods (e.g., [1,2,3]). These methods operate under a harder setting using expert datasets with reward free interactions, whereas this work assumes access to both expert data and reward signals. The authors should include a discussion and empirical comparison with this line of research to better highlight the advantages of their imitation guided RL framework.\n\n- In Figure 9, the authors argue that their method achieves a larger KL divergence compared to prior approaches, implying greater flexibility in policy optimization. However, a larger KL divergence does not necessarily indicate better policy performance, especially if the optimal policy is expected to be close to the behavior cloning (BC) policy. The primary benefit of incorporating online RL over pure BC lies in improving policy coverage, yet no experiments are provided to evaluate performance on out-of-distribution (O.O.D.) states beyond the expert demonstrations, leaving this claimed advantage unsubstantiated.\n\n\n[1] Ren, J., Swamy, G., Wu, Z. S., Bagnell, J. A., & Choudhury, S. (2024). Hybrid inverse reinforcement learning. arXiv preprint arXiv:2402.08848.\n\n[2] Yin, Z. H., Ye, W., Chen, Q., & Gao, Y. (2022). Planning for sample efficient imitation learning. Advances in Neural Information Processing Systems, 35, 2577-2589.\n\n[3] Li, S., Huang, Z., & Su, H. (2025). Reward-free World Models for Online Imitation Learning. In Forty-second International Conference on Machine Learning."}, "questions": {"value": "- Could the authors provide a more detailed discussion and comparison with existing online imitation learning literature to better position their work within this research landscape?\n\n- Could the authors add experiments on out-of-distribution (O.O.D.) samples to demonstrate the advantages of incorporating online RL over purely offline imitation learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5FJR7yMW1", "forum": "CgupPwA40q", "replyto": "CgupPwA40q", "signatures": ["ICLR.cc/2026/Conference/Submission14649/Reviewer_zeD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14649/Reviewer_zeD3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423895811, "cdate": 1761423895811, "tmdate": 1762925021624, "mdate": 1762925021624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on applying reinforcement learning (RL) in the continuous control setting, where the agent is further provided some expert demonstrations.\nThe paper claims that simply including the demonstrations into a replay buffer, or using a imitation-learning (IL) regularizer utilizes the demonstrations ineffectively.\nInstead, the paper proposes to parameterize the exploration Gaussian policy such that its covariance is computed based on the demonstrations.\nThe paper evaluates the proposed method, DGN, on continuous-control tasks from Adroit and Robomimic in the sparse reward setting, and show that DGN outperforms, if not comparable, against three existing RL algorithms that utilizes demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read and the idea is simple\n- The performance outperforms compared approaches, and the paper provides some ablation studies on a subset of design choices"}, "weaknesses": {"value": "- Related work, combining imitation and reinforcement learning: I believe there is a line of research that uses hierarchical (inverse) RL [1-4] to better explore the environment. It would be nice if the paper includes some discussions on these papers as well.\n- Method\n\t- In section 4.1, it seems like the method will encourage the agent to explore indefinitely as the variance will be large asymptotically when the \"expert\" policy is suboptimal and the RL policy is optimal, or when there are multiple expert actions from the same states. Based on the training/evaluation split this might be okay as during evaluation we use the greedy policy, however practitioners applying RL will A/B test policies along with the training phase.\n\t- In section 4.2, the paper argues that IL regularizer will restrict the learner from learning the optimal policy which I agree. However, the paper alleviates the above problem using a manually defined hyperparameter decaying schedule, which arguably, can also be applied to the IL regularizer.\n\t\t- Furthermore, the schedule choice seems arbitrary, in the sense that they could be using linear schedule, cosine schedule, etc.\n- Experiments\n\t- Nit: Figure 4 should include the environment name per plot.\n\t- Regarding the comparison against IBRL on line 370---I have trouble agreeing with the argument that IL policy has difficulty capturing the right distribution. Nowadays, BC policies are trained with diffusion policies (DPs), which can capture multimodal distributions [5]. Furthermore, it has been shown that the compounding error can be alleviated with DPs [6]. Finally, there are works recently [7, 8] that focuses on policy steering in DPs and show promising results, which I think this work should also compare against.\n\t- The choice of hyperparameters might seem arbitrary for the scheduling choice, as well as $\\tau, n, m$.\n\t- I think there can be analysis on checking how to covariance changes over time, and how that dynamics change based on the modality of the dataset.\n\nReferences:  \n[1] Riedmiller, Martin, et al. \"Learning by playing solving sparse reward tasks from scratch.\" International conference on machine learning. PMLR, 2018.  \n[2] Hertweck, Tim, et al. \"Simple sensor intentions for exploration.\" arXiv preprint arXiv:2005.07541 (2020).  \n[3] Ablett, Trevor, Bryan Chan, and Jonathan Kelly. \"Learning from guided play: Improving exploration for adversarial imitation learning with simple auxiliary tasks.\" IEEE Robotics and Automation Letters 8.3 (2023): 1263-1270.  \n[4] Ablett, Trevor, et al. \"Efficient Imitation Without Demonstrations via Value-Penalized Auxiliary Control from Examples.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025.  \n[5] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" The International Journal of Robotics Research 44.10-11 (2025): 1684-1704.  \n[6] Zhang, Thomas T., et al. \"Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction.\" arXiv preprint arXiv:2507.09061 (2025).  \n[7] Wagenmaker, Andrew, et al. \"Steering Your Diffusion Policy with Latent Space Reinforcement Learning.\" arXiv preprint arXiv:2506.15799 (2025).  \n[8] Wang, Yanwei, et al. \"Inference-time policy steering through human interactions.\" 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JZUq8hIDQn", "forum": "CgupPwA40q", "replyto": "CgupPwA40q", "signatures": ["ICLR.cc/2026/Conference/Submission14649/Reviewer_FcCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14649/Reviewer_FcCQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649419625, "cdate": 1761649419625, "tmdate": 1762925021267, "mdate": 1762925021267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Data-Guided Noise (DGN), a method to improve sample efficiency in reinforcement learning by using prior demonstration data. Instead of using data for explicit imitation learning objectives, DGN learns a state-conditioned noise distribution based on the difference between expert actions and the current policy's actions. This noise is used to guide exploration during training."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel method for leveraging demonstration data to guide RL exploration by shaping the noise distribution, rather than through an explicit imitation loss.\n\n- The empirical evaluation is thorough. The method is compared against several relevant baselines (RLPD, RFT, IQL, IBRL), and the core components of the DGN framework are carefully validated through various ablation studies."}, "weaknesses": {"value": "- **Contextualization w.r.t. related work**: The paper's discussion of combining Imitation Learning (IL) and Reinforcement Learning (RL) primarily focuses on two strategies: (1) combining IL/RL objectives and (2) using a separate IL policy to guide or propose actions. However, there are a couple other approaches for guiding the exploration of the RL agent with demonstrations while avoiding the need to balance between explicit IL/RL objectives, or incurring state-action distribution shift issues experienced by replay buffer approaches. Further, there is a long line of work in robotics on residual-based control where, where RL is used to train a residual policy to improve pre-existing controllers. None of these lines of work were mentioned, and I think discussing them would improve contextualization w.r.t. related work.  \n\n  - **Guiding RL exploration with demonstrations**: \n\n    - [Salimans & Chen 2018](https://arxiv.org/abs/1812.03381) consider resetting to expert states from a single demonstration in reverse order, to guide RL exploration in sparse reward, goal-reaching tasks.\n\n    - [Wang et al. 2023](https://arxiv.org/abs/2210.14428)  formulates a potential-based shaped reward that treats demonstration states as goals to inform RL exploration while retaining the ability to discover the optimal policy w.r.t. the task reward\n\n  - Using RL to learn residual policies on top of pre-existing controllers: \n\n    - [Kasaei et al. 2023](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.1004490/full) uses RL to learn a residual policy to improve simulated humanoid robot locomotion for Robocup 3dSim\n\n    - [Yang et al. 2023](https://proceedings.mlr.press/v211/yang23b/yang23b.pdf)  uses RL to learn a residual policy for learning jumping behavior for a quadruped robot\n\n    - [Li et al. 2025](https://arxiv.org/pdf/2509.20696) learns such a residual policy to improve locomotion of a Unitree G1 humanoid\n\n  - **Empirical improvement**: The empirical advantage of DGN over baselines is not consistently strong. In 4/7 tasks in Figure 4, DGN shows minimal or no significant improvement in sample efficiency or asymptotic return in several of the tasks. While achieving SOTA results isn’t a requirement for a paper to be accepted to ICLR, the strength of the results in this paper will naturally limit the impact/interest from the ICLR community. \n\n  - **Argument for DGN’s advantages over related methods is weak**: The paper argues that the common strategy of regularizing the RL objective with an IL objective has the following problems: (1) it requires carefully tuning the loss weights (Line 127), and (2) it constrains the policy to remain close to the expert distribution, even when no longer needed (Line 37). The paper then argues that DGN is a novel strategy for guiding RL with IL that doesn’t suffer these issues (Line 46), but I am not convinced that this is the case. \n\n    - In Section 4.2, the authors state, “If the policy eventually surpasses the expert demonstrations in performance, the learned noise may remain large in magnitude, potentially pulling the agent away from its improved behavior. To mitigate this, one strategy is to apply an annealing schedule to the noise during training.” (pg 5).\n\n    - So, although DGN is not directly doing IL, DGN also requires carefully tuning the tradeoff between RL and “IL” losses via the annealing schedule. This annealing schedule seems to be just another form of tradeoff parameter that requires careful tuning, similar to the loss weights DGN claims to avoid."}, "questions": {"value": "Questions:\n\n- Is there any reason to prefer DGN over RLPD? The answer in the paper is that “Initializing the replay buffer with expert data does not directly use the expert information to maximally accelerate learning” but the empirical improvement of DGN is not large either.\n    \n- Could the authors compare against a residual policy learning baseline, as mentioned in the weaknesses? This approach seems directly applicable to this paper's setting and does not appear to require additional assumptions. To clarify any potential misunderstanding, this is different from the \"Alternative DGN Formulation,\" which learns a residual mean $\\mu_{\\phi}(s)$ for the noise distribution. A residual policy in this paper’s setting would involve learning a policy that outputs an action residual added to the mean of a fixed IL policy.\n    \n\nMinor comments: \n\n- Fig 4 seems to be missing the task name for each subplot."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sOdbRCyDe8", "forum": "CgupPwA40q", "replyto": "CgupPwA40q", "signatures": ["ICLR.cc/2026/Conference/Submission14649/Reviewer_u3Py"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14649/Reviewer_u3Py"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030328387, "cdate": 1762030328387, "tmdate": 1762925020903, "mdate": 1762925020903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of sample-efficient reinforcement learning in sparse-reward environments by leveraging prior demonstration data. The authors argue that traditional methods, which use explicit imitation learning losses or constraints, can overly restrict the policy and hinder long-term reward maximization. As an alternative, they propose Data-Guided Noise (DGN), a framework that uses demonstrations to guide exploration \"implicitly.\" The core idea is to learn a state-dependent exploration noise distribution. Specifically, the mean of the policy is learned via a standard RL objective, while the covariance of the exploration noise is learned via an imitation objective that models the difference between the expert's actions and the current policy's actions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "*   The core motivation is strong and well-articulated. The idea of using demonstrations to guide exploration without being rigidly constrained by them is a compelling direction for combining imitation and reinforcement learning.\n*   The paper's choice of baseline algorithms is comprehensive and appropriate. It compares against methods that represent key paradigms in the offline-to-online and imitation-augmented RL space (e.g., replay buffer initialization, explicit regularization, and reference policies).\n*   The empirical results presented are impressive. DGN consistently outperforms strong baselines across multiple difficult, sparse-reward tasks, suggesting the practical effectiveness of the proposed method."}, "weaknesses": {"value": "*   The central mechanism of the paper lacks a clear theoretical or intuitive justification. It is not immediately obvious why the difference between the expert action and the current policy's mean action should define an optimal exploration distribution. While the results are strong, the paper would be more impactful if it provided more insight into *why* this specific formulation of exploration noise is so effective.\n*   The paper makes strong claims about the failure modes of imitation-based methods (e.g., they \"degrade long-term performance\") without providing concrete, illustrative examples to substantiate them. A simple experiment showing this degradation would make the motivation for DGN much clearer.\n*   The analysis seems limited to high-quality, expert demonstrations. It is unclear how DGN would perform with suboptimal or noisy demonstration data, which is a common real-world scenario. If the noise is guided by bad data, it could potentially be detrimental to exploration.\n*   The experimental evaluation, while strong, could be more thorough. The policies are trained for a relatively small number of timesteps, and it's unclear if they have fully converged. Furthermore, the plots do not specify the measures of centrality and spread (e.g., mean and standard deviation/error over seeds)."}, "questions": {"value": "*   Could the authors provide more intuition or a theoretical justification for why modeling the variance of exploration noise based on the difference between expert and policy actions is an effective strategy? For instance, why should a large deviation between the policy and the expert imply a need for high-variance exploration in that direction?\n*   How is DGN expected to behave when provided with suboptimal or mixed-quality demonstrations? Does the \"guidance\" become detrimental in such cases, and could the annealing schedule mitigate this?\n*   The paper states that explicit imitation constraints \"degrade long-term performance.\" Could you provide a specific, perhaps toy, example where this happens and illustrate how DGN avoids this failure mode?\n*   In Algorithm 1, the action `at` is sampled from `π_sampling`, which already includes the learned noise. The algorithm as written does not show the base policy's mean action being computed first. Could you clarify how the base policy `πθ` is used in the action selection process during rollouts?\n*   The research questions listed in Section 5 seem somewhat incomplete. Was the potential to learn from suboptimal demonstrations considered? What about the sample complexity of learning the covariance matrix itself?\n*   What are the measures of centrality and spread for the plots in Figure 4 (e.g., mean and standard deviation over seeds)? Also, was there a reason for the specific training horizon chosen, and do the performance trends hold if trained for longer?\n*   Figure 2 provides a helpful conceptual overview but does not appear to be referenced in the main body of the text. Could this be added?\n* The current method guides exploration based on the difference between expert and policy actions, treating all expert data equally. Have the authors considered a reward-weighted formulation? For example, could the covariance learning objective be modified to give more weight to action differences from high-reward trajectories? This seems like a more direct way to encourage exploration in promising directions and might be more robust when learning from mixed-quality data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vUHBSL7C0f", "forum": "CgupPwA40q", "replyto": "CgupPwA40q", "signatures": ["ICLR.cc/2026/Conference/Submission14649/Reviewer_sd6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14649/Reviewer_sd6c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14649/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035070173, "cdate": 1762035070173, "tmdate": 1762925020358, "mdate": 1762925020358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}