{"id": "A1fWxZLy9H", "number": 2512, "cdate": 1757129377162, "mdate": 1759898143834, "content": {"title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing", "abstract": "Instruction-guided image editing enables users to specify modifications by natural language, offering more flexibility and control. Among existing frameworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion models in scalability and performance. However, while real-world scenarios often require concurrent execution of multiple instructions, step-by-step editing suffers from accumulated errors and degraded quality, and integrating various instructions with a single prompt usually results in incomplete edits. We propose an Instruction Influence Disentanglement (IID) framework that enables the parallel execution of multiple instructions within a single denoising process for DiT-based models. By analyzing self-attention mechanisms in DiTs, we identify distinct attention patterns in multi-instruction settings and derive instruction-specific masks to disentangle the influence of each instruction. These masks then guide the editing process to ensure localized modifications while preserving consistency in non-edited regions. Extensive experiments demonstrate that IID can enhance fidelity and instruction completion, while also reducing computation overhead compared to existing methods.", "tldr": "IID enables parallel multi-instruction image editing in Diffusion Transformers using attention masks to disentangle influences, ensuring better quality, consistency, and efficiency than existing methods", "keywords": ["Image Editing", "Multi-Instruction Guided Image Editing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7fcb90a3894a4e3c7af6b8abaf6b1d812db1fd6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Instruction Influence Disentanglement, a training-free framework for multi-instruction image editing using Diffusion Transformer models. The method aims to overcome the limitations of step-by-step editing, which accumulates errors, and naive instruction concatenation, which suffers from instruction conflict. The core of IID involves generating instruction-specific masks by analyzing and subtracting self-attention maps, followed by blending latent representations and guiding the subsequent denoising process with a modified attention mechanism to execute all edits in parallel."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a practical and increasingly important problem: how to efficiently and effectively apply multiple edits to an image using natural language instructions, particularly within the context of the powerful and scalable DiT architecture.\n- The central idea of using a head-wise subtraction of attention maps to isolate instruction-specific editing regions is an interesting and intuitive mechanism. The analysis highlights that different instructions can elicit similar attention patterns in the same heads for non-target regions, provides a solid motivation for this design choice.\n- The proposed IID framework is demonstrated to be effective across several state-of-the-art DiT-based models (Omnigen, FluxEdit, FluxKontext), suggesting a degree of architectural generality.\n- The human preference study provides compelling evidence that as the number of instructions increases, the proposed method's advantage in both instruction alignment and image fidelity becomes more pronounced compared to the baselines."}, "weaknesses": {"value": "- Equation (3) which defines the core head-wise mask generation strategy, is mathematically incorrect. This expression will always yield a non-positive result, which is contrary to the goal of creating a positive mask over the target edit region. The text states, \"The negative values are then set to zero to suppress non-relevant regions,\" which implies the operation should be a ReLU or `max(0, ...)` function. This is not a minor typo; it is an error in the formal definition of the central technical contribution of the paper and reflects a lack of rigor.\n-  In Adaptive Blender (Sec 4.2), the sequential blending process described in Equation (5) is order-dependent. While the paper mentions re-ranking instructions for FluxEdit based on an \"Influence Score\" to mitigate this, it's unclear how this is handled for OmniGen and FluxKontext, for which the paper claims \"we ensure all $P_i$ share the same position embedding, neutralizing positional bias.\" This is a vague statement. Positional embeddings typically relate to token sequence order, so it's not obvious how making them identical neutralizes the bias from the *blending* order in Eq. (6).\n- In Multi-instruction Influence Disentanglement (Sec 4.2), the description of the final attention masking step is highly ambiguous. The paper states that \"the token of $P_i$ can only attend to noisy image tokens excluding regions masked by $M_j$, where $j \\in[1, N]$ and $j \\neq i$.\" How is this \"gating\" of attention implemented? Is it a hard mask where attention scores are set to $-\\infty$? Is it a soft modulation? This critical detail of the method is completely absent, making the mechanism opaque.\n- The theoretical analysis in Appendix A.4, while presented with formal notation, offers little genuine insight. Section A.4.1 merely recasts standard ODE numerical error analysis to argue that error accumulates over multiple steps, a widely known phenomenon. Section A.4.2 provides a high-level, intuitive argument for \"instruction conflict\" based on data sparsity and misalignment of velocity fields, but it lacks any rigorous derivation and does not constitute a meaningful theoretical contribution. This section feels more like a post-hoc justification than a principled analysis.\n- The empirical evaluation relies almost exclusively on two simple baselines: step-by-step (Step) and naive concatenation (NaiveCon). While the authors correctly note the scarcity of multi-instruction methods for DiTs, this does not excuse the lack of a more challenging comparison. No attempt was made to adapt principles from U-Net-based multi-edit methods (e.g., attention modulation from FOI) to a DiT architecture, even as a rudimentary baseline. Dismissing such approaches by citing \"unresolved challenges\" without elaboration is insufficient. The strong performance of IID is less impactful when only compared against methods that are known a priori to be flawed.\n- The ablation studies, while useful, do not fully dissect the contribution of each component of the IID pipeline. For example, what is the performance if only the adaptive blending is used without the subsequent attention masking? Or vice-versa? The method is a sequence of several heuristics (head-wise subtraction, latent blending, final attention masking), and their relative importance is not established. This makes it difficult to ascertain which part of the framework is truly responsible for the performance gains.\n- The paper curates a new dataset for evaluation. However, the filtering process described in Appendix A.1 (\"human experts manually assess the quality of the resulting outputs. If the overall error rate exceeds 60%, the case is considered beyond the model's ability and is excluded\") is a potential source of significant selection bias. This process may inadvertently filter out precisely the challenging cases where instruction conflicts are most severe, thereby inflating the apparent performance of the proposed method."}, "questions": {"value": "- Could the authors please clarify and correct the formulation in Equation (5)? Is the intended operation `max(0, ...)`?\n- Can the authors provide a more precise explanation of how the \"Multi-instruction Influence Disentanglement\" step is implemented? Specifically, how do you enforce that tokens for instruction $P_i$ only attend to image regions not masked by other instructions $M_j$?\n- Regarding the Adaptive Blender in Equation (6), what is the justification for claiming that shared position embeddings for OmniGen/FluxKontext \"neutralize positional bias\" when the blending operation itself is sequential and thus order-dependent? Have you experimented with different blending orders for these models?\n- The ablation study compares your head-wise subtraction to an \"Avg\" method. Have the authors considered a simpler but potentially stronger baseline for mask generation, such as applying a threshold (e.g., Otsu's method) directly to the attention map $\\bar{A}_{Z P_i}$ for each instruction individually, and then resolving overlaps? This would isolate the benefit of the subtraction component more directly.\n- Given the filtering process for your custom dataset, how can you be sure that you are not systematically removing the most difficult multi-instruction cases, which could inflate the reported performance of your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zad27hzvfA", "forum": "A1fWxZLy9H", "replyto": "A1fWxZLy9H", "signatures": ["ICLR.cc/2026/Conference/Submission2512/Reviewer_5pFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2512/Reviewer_5pFe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641237155, "cdate": 1761641237155, "tmdate": 1763004904591, "mdate": 1763004904591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Instruction Influence Disentanglement (IID), a training-free framework for parallel multi-instruction image editing within a single denoising pass. It targets Diffusion Transformer (DiT) models like FluxEdit, Omnigen, and FluxKontext, aiming to solve two main problems: cumulative errors from step-by-step editing and conflicts from naive instruction concatenation. IID analyzes self-attention patterns in DiTs and introduces a head-wise mask generation strategy. For each instruction, it subtracts the mean attention map of other instructions to isolate the editing region, then aggregates and binarizes these masks. Next, an adaptive blender composes instruction tokens and latent images at a predefined timestep S. It uses a re-ranking process with an influence score to mitigate any dominance effects. Finally, an attention mask constrains instruction tokens to their respective regions, enabling disentangled parallel edits. Experiments on MagicBrush and a custom dataset show consistent improvements over baselines across metrics like L1/L2, CLIP-I/T, and DINO, as well as in human preferences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes a novel, training-free parallel editing framework. It utilizes \"instruction-wise attention subtraction\" to leverage the model's own attention for disentangling multiple instructions, eliminating the need for external segmenters.\n2. The evaluation is comprehensive, demonstrating consistent performance improvements across multiple mainstream DiT backbones (e.g., FluxEdit, Omnigen) and datasets.\n3. Addresses a critical, real-world need in multi-instruction DiT editing, solving a key problem where existing methods fail or are ineffective."}, "weaknesses": {"value": "1. While the paper provides a theoretical discussion of error accumulation and instruction conflicts in the appendix, the main text adopts a simple averaging scheme for overlapping regions during blending, without analyzing its potential to reintroduce conflicts or degrade boundary consistency.\n2. The evaluation is primarily conducted on relatively simple editing tasks. The custom dataset construction using GPT-4o may introduce biases, and the human evaluation involves only 5 participants, which may not be sufficient for robust conclusions. The paper lacks comparison with more recent multi-instruction editing methods or adaptation of existing approaches to DiT architectures.\n3. The paper does not provide sufficient analysis of scenarios where the method fails or performs poorly."}, "questions": {"value": "1. The IS score used to guide instruction position re-ranking is an innovative idea. How stable is IS under paraphrasing, i.e., different phrasings of the same instruction?\n2. In applying masks within the DiT denoising process, how does IID compare against using minimally adapted external segmenters (e.g., SAM) to derive instruction-specific masks for DiTs? Does an external mask improve completion or fidelity, or introduce new artifacts and complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6UTVCfpSWy", "forum": "A1fWxZLy9H", "replyto": "A1fWxZLy9H", "signatures": ["ICLR.cc/2026/Conference/Submission2512/Reviewer_QEsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2512/Reviewer_QEsG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884455678, "cdate": 1761884455678, "tmdate": 1762916261403, "mdate": 1762916261403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses multi-instruction-guided image editing in DiTs, where users specify several natural-language instructions simultaneously. Existing strategies tend to degrade image quality or yield incomplete edits. The authors overcome these limitations via introducing IID, a training-free framework enabling multiple edits within a single denoising process. It constructs head-wise attention masks to blend instruction-specific latents, and builds a compositional attention mask that ensures local edits while preserving non-edited regions. Experiments demonstrate that IID improves image fidelity and efficiency across Omnigen, FluxEdit, and FluxKontext models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The introduction precisely identifies two failure cases in multi-instruction editing—error accumulation in sequential steps and instruction conflicts in prompt concatenation. This evidence grounds the paper’s motivation in observable limitations of prior methods.\n\n2.The proposed mask derivation introduces a simple yet effective subtractive strategy across attention heads to isolate editing regions.\n\n3.The adaptive blender and re-ranking strategy mitigate instruction dominance, ensuring balanced edits. \n\n4.Good performance achieved under both MagicBrush and a custom dataset extending to six-instruction cases."}, "weaknesses": {"value": "1. The adaptation to the proposed method is limited to some extent. According to the experiments from the paper, the method only works in DiT-based architectures.\n\n2.One of my concerns is the actual application. Compared with the parallel instructions paradigm, sequential instruction editing is usually used in real-world application as the user can adjust the requirement at the next time. However, the parallel instruction is equivalent to the T2I editing that the text is pointwise and concise, and has differences compared with sequential image editing. \n\n3. Based on the stated concern, it is better to show the generation performance with the traditional generated model with the same type of instructions to demonstrate the effectiveness.\n\n4. Human evaluation involves only five participants, lacking statistical analysis or inter-rater reliability, which undermines the reproducibility of subjective results.\n\n5.The subsequent thresholding with Gaussian + Otsu filters introduces multiple heuristic stages. How about the performance relative to other mask computation settings?"}, "questions": {"value": "Please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "unPacwrgep", "forum": "A1fWxZLy9H", "replyto": "A1fWxZLy9H", "signatures": ["ICLR.cc/2026/Conference/Submission2512/Reviewer_oZHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2512/Reviewer_oZHh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2512/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900590894, "cdate": 1761900590894, "tmdate": 1762916261196, "mdate": 1762916261196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}