{"id": "LAYCYiIgZ1", "number": 8576, "cdate": 1758091436089, "mdate": 1759897775647, "content": {"title": "Aurelius: Relation Aware Text-to-Audio Generation At Scale", "abstract": "We present Aurelius, a new framework that enables relation aware text-to-audio (TTA) generation research at scale. Given the lack of essential audio event and relation corpora, \\emph{Aurelius} contributes a large-scale audio event corpus \\emph{AudioEventSet} and another large-scale relation corpus \\emph{AudioRelSet}. Comprising 110 event categories, AudioEventSet maximally covers all commonly heard audio events and each event is unique, realistic and of high-quality. AudioRelSet consists of 100 relations, comprehensively covering the relations that present in the physical world or can be neatly described by text.  As the two corpora provide audio event and relation independently, they can be combined to create massive <text,audio> pairs with our pair generation strategy to support relation aware TTA investigation at scale. We comprehensively benchmark all existing TTA models from both general and relation aware evaluation perspective. We further provide in-depth investigation on scaling up existing TTA models' relation aware generation by either training from scratch or leveraging cross-domain general TTA knowledge. The introduced corpora and the findings through investigation in this work potentially facilitate future research on relation aware TTA generation.", "tldr": "A new audio event corpus and relation corpus supporting relation aware text to audio generation TTA task", "keywords": ["Relation Aware Text-to-Audio Generaion", "Audio Event Corpus", "Relation Corpus"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53844c2f992159fc374cb68e6f9c3b7005258fc3.pdf", "supplementary_material": "/attachment/4f4d2e2470ca1497771e1dbfe25dbac87fe199a3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a relation-aware text-to-audio generation framework and introduces two newly developed datasets, AudioEventSet and AudioRelSet, to improve the modeling of relational structures between audio events. The datasets are organized in a tree-structured hierarchy and are designed to enhance compositionality and relation understanding in audio generation. The authors argue that these datasets can serve as new benchmarks for relation-aware audio generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on an important and relatively underexplored topic — relation-aware text-to-audio generation, which aims to capture richer dependencies between sound events beyond conventional text-to-audio mapping.\n\nThe proposed datasets include explicit relation-level annotations (such as “arity,” count, and compositional structure), which provide more fine-grained relational information than most existing datasets.\n\nThe authors attempt to model hierarchical relations between sound events, potentially offering insights into how complex auditory scenes could be represented in structured data formats."}, "weaknesses": {"value": "The main contribution lies almost entirely in dataset creation, with no substantial methodological innovation or new generation framework beyond existing relation-aware approaches (e.g., RiTTA or CompA).\n\nThe datasets are not released, and only high-level descriptions are given. Without access to examples or samples, it is impossible to verify the data quality or reproducibility.\n\nThe tree structure design (depth 3 for AudioEventSet and depth 2 for AudioRelSet) lacks theoretical or empirical justification. It is unclear why this specific hierarchy benefits the relation-aware task or how the root/leaf nodes are defined.\n\nThe data generation process appears mostly manual or GPT-assisted, rather than automatic. This makes it resource-intensive, potentially inconsistent, and difficult to scale. The resulting compositions may sound unnatural or ambiguous, especially since all clips are fixed to 10 seconds regardless of content or event duration.\n\nThe evaluation methodology is largely inherited from general text-to-audio works and RiTTA, without introducing new metrics to evaluate the proposed hierarchical or relation-specific structures.\n\nOverall, the paper reads more like an engineering dataset report than a research paper introducing conceptual or algorithmic innovation."}, "questions": {"value": "Previous works such as RiTTA and CompA also address relation-aware text-to-audio generation. Beyond the newly developed datasets, what is the core conceptual or technical difference between this work and prior ones?\n\nHow exactly were the datasets collected — are all audio-text pairs manually constructed or verified? If so, how was data distinctiveness and quality control ensured?\n\nDid the authors listen to and validate all audio compositions to ensure naturalness and correct temporal ordering of events?\n\nThe proposed tree structure seems to play no role in evaluation — could the authors propose or experiment with metrics that explicitly utilize this structure?\n\nWhy was the relation-aware improvement only tested on the Tango model family? Would results generalize to other text-to-audio architectures (e.g., AudioLDM, Make-An-Audio)?\n\nIs there any other way to improve the performance on relation aware?  Not just training on the \"new-proposed dataset\".\n\nAny demo or public data for the dataset (waveform)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W0nkXJcjDE", "forum": "LAYCYiIgZ1", "replyto": "LAYCYiIgZ1", "signatures": ["ICLR.cc/2026/Conference/Submission8576/Reviewer_P1t4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8576/Reviewer_P1t4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760624149201, "cdate": 1760624149201, "tmdate": 1762920427203, "mdate": 1762920427203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Aurelius, a new framework designed to enable relation-aware text-to-audio (TTA) generation research at scale. The authors also propose two accompanying datasets, AudioRealSet and AudioEventSet, which demonstrate effectiveness in audio generation tasks. Overall, the proposed datasets are valuable contributions to the audio research community. However, the paper lacks novel methodological and technical innovations beyond dataset construction. (But the dataset is a great one!)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The release of a large-scale, well-curated dataset is always beneficial to the audio research community.\n\n- The authors conduct comprehensive benchmarking of existing TTA models to evaluate the proposed datasets.\n\n- The distinction between AudioEventSet and AudioRealSet is clearly defined. In particular, AudioRealSet provides rich attribute-level annotations for sound events, which is a valuable addition."}, "weaknesses": {"value": "- My main concern is that, despite the usefulness of the dataset, there are no novel methodological or technical contributions. The paper reads primarily as a dataset paper rather than a technical paper.\n\n- In the Introduction, the term “relation modeling” is not clearly defined.\n\n- In Section 3.1, the phrase “audio events potentially present in the 3D physical world is unclear” is ambiguous — clarification is needed.\n\n- The authors claim that AudioEventSet is more distinctive than AudioSet, but no supporting evidence or analysis is provided. Since AudioEventSet is also manually designed, how do the authors ensure that the 110 sound classes are well-separated and not confusing?\n\n- The size of AudioEventSet should be reported more clearly. As it is built from Freesound and FSD50K (which contains only ~50k clips), it is unclear how large the final dataset actually is or how scalability is achieved.\n\n- In Section 3.2, is AudioRealSet a subset of AudioEventSet? How are the relations labeled — automatically or manually? The writing in this section could be improved for clarity.\n\n- The experimental section lacks comprehensive evaluation of how the proposed datasets improve performance. For instance, how much improvement is observed when models are trained with these datasets compared to without them?"}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JGoz10oUW6", "forum": "LAYCYiIgZ1", "replyto": "LAYCYiIgZ1", "signatures": ["ICLR.cc/2026/Conference/Submission8576/Reviewer_2P2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8576/Reviewer_2P2L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831528122, "cdate": 1761831528122, "tmdate": 1762920426667, "mdate": 1762920426667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Aurelius, which contains:\n(1) AudioEventSet, which contains clean clips for audio events\n(2) AudioRelSet, which contains the relative relationship of audio events.\n\nThe authors show that, by combining these two resources, we can generate theoretically unlimited relation-aware text-audio pairs. \n\nThe authors also present the evaluation method (benchmarks) for the relation-aware audio.\n\nIt also shows that training existing TTA models on the proposed dataset can benefit the relation-aware TTA performance.\n\nThe relation-awareness is an important property in current TTA, and this paper is a very good resource."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper comprehensively discusses the relation-aware TTA generation. It contains good resources, benchmarks and sufficient discussion.\n\nThe relation-awareness is of wide interest in the TTA community. The paper also reveals that the current TTA is not good enough in this direction."}, "weaknesses": {"value": "(1) The paper is mostly about the resources (data, benchmark) of building relation-ware TTA. For this kind of resource paper, I wonder if the author would make it public.\n(2) In section 3, although the author provides a comprehensive design in the data content, they don't mention (1) why the designs are reasonable and (2) how they ensure the intended design philosophy is well implemented in practice. e.g., how they ensure the audio clips in AudioEventSet are precise and clean enough; why such designs in AudioRelSet are reasonable. Such missing information would compromise the contribution of the work.\n(3) I'm a bit confused about the mAMSR metric: in the Table 2 caption, you mention its range is [0, 1], but numbers in Table 3 are beyond this range.\n(4) Even with this carefully designed data pipeline, the overall mAPre, mARel, etc, are still absolutely low. (e.g., <30%). This very tailor-made dataset seems not to solve the issue very well.\n(5) The authors claim that unlimited data simulation is feasible, but scaling up the simulated data is not very effective, as shown in Figure 6."}, "questions": {"value": "In general, whether the resources would be made public is an important metric for the paper evaluation. Would the authors release them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mJuVLorC3g", "forum": "LAYCYiIgZ1", "replyto": "LAYCYiIgZ1", "signatures": ["ICLR.cc/2026/Conference/Submission8576/Reviewer_SfjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8576/Reviewer_SfjC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975368438, "cdate": 1761975368438, "tmdate": 1762920426202, "mdate": 1762920426202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Aurelius, a large-scale benchmark framework with two corpora, AudioEventSet and AudioRelSet, that enables systematic evaluation and development of relation-aware text-to-audio generation at scale."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the well-established limitation of current TTA models in generating audio with accurate temporal ordering and relational structures, which is an interesting and important research problem.\n- The framework's approach of combining relation templates with audio events to generate numerous <text, audio> pairs provides excellent flexibility and scalability. The adoption of the \"Head-Modifier Structure with Progressive Verb Form\" (e.g., \"door bell ringing audio\" rather than \"ringing door bell\") ensures syntactic consistency across the dataset."}, "weaknesses": {"value": "- The GPT-generated templates or synonyms are not always accurate, and some generated texts may not properly correspond to the actual audio events, leading to potential noise in the dataset.\n- Since the generated sounds are synthetic, there is no clear way to assess or guarantee their perceptual quality.\n- Compared to the existing datasets shown in Table 1, the improvement in performance is not clearly demonstrated. There is no head-to-head comparison between models trained on existing datasets and those trained on Aurelius, making it difficult to quantitatively verify the superiority of the proposed dataset."}, "questions": {"value": "Please refer to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fo0NLPMnz0", "forum": "LAYCYiIgZ1", "replyto": "LAYCYiIgZ1", "signatures": ["ICLR.cc/2026/Conference/Submission8576/Reviewer_hVjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8576/Reviewer_hVjf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982492552, "cdate": 1761982492552, "tmdate": 1762920425706, "mdate": 1762920425706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}