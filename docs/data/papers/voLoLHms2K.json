{"id": "voLoLHms2K", "number": 1893, "cdate": 1756959190130, "mdate": 1759898180358, "content": {"title": "SQLAgent: Learning to Explore Before Generating as a Data Engineer", "abstract": "Large Language Models have recently shown impressive capabilities in reasoning and code generation, making them promising tools for natural language interfaces to relational databases.\nHowever, existing approaches often fail to generalize in complex, real-world settings due to the highly database-specific nature of SQL reasoning, which requires deep familiarity with unique schemas, ambiguous semantics, and intricate join paths.\nTo address this challenge, we introduce a novel two-stage LLM-based framework that decouples knowledge acquisition from query generation. In the Exploration Stage, the system autonomously constructs a database-specific knowledge base by navigating the schema with a Monte Carlo Tree Search–inspired strategy, generating triplets of schema fragments, executable queries, and natural language descriptions as usage examples.\nIn the Deployment Stage, a dual-agent system leverages the collected knowledge as in-context examples to iteratively retrieve relevant information and generate accurate SQL queries in response to user questions.\nThis design enables the agent to proactively familiarize itself with unseen databases and handle complex, multi-step reasoning. Extensive experiments on large-scale benchmarks demonstrate that our approach significantly improves accuracy over strong baselines, highlighting its effectiveness and generalizability.", "tldr": "We propose SQLAgent, a two-stage Text-to-SQL framework that first explores database structures before generating queries, achieving state-of-the-art performance on complex benchmarks.", "keywords": ["SQL Generation", "Large Language Models", "LLM Agents"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/856892c64f985c600f4f8e1ee58892f2f8dc2015.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a two‑stage, LLM‑centric framework for Text‑to‑SQL. In the Exploration stage, the system traverses a schema structure (Database→Schema→Table→“Group”→Field) and, via an MCTS‑inspired search, autonomously constructs triplets (S,Q,U). In the Deployment stage, a dual‑agent setup (InfoAgent + GenAgent) retrieves relevant triplets and generates SQL for a user question."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. Motivation: The paper tackles a real pain point—database‑specific generalization in NL2SQL—by trying to separate knowledge acquisition from generation.\n\nS2. Systematization attempt: The exploration stage aims to build a database‑specific repository of examples that can help downstream generation, which is a reasonable engineering idea even if the theory is thin.\n\nS3. Readable pipeline: The dual‑agent design (InfoAgent/GenAgent) and its runtime loop (Figure 2) are clearly diagrammed, aiding reproducibility at a high level."}, "weaknesses": {"value": "The first set of weaknesses (W1-W5) comes form informal and incorrect database concepts and schema modeling.\n\nW1. Oversimplified/ambiguous “schema” notion: Figure 1 describes a schema as “a logical container for a collection of tables,” but the paper never formalizes essential relational concepts (keys, foreign keys, constraints, views, indices) that are indispensable for reasoning about joins and correctness. This under‑specification leaks into the rest of the method (e.g., join selection and grouping) and undermines the soundness of the search and the triplets that are recorded.\n\nW2. “Group node” concept is ill‑defined and conflated: The left side of Figure 1 says, “When multiple tables share common attributes, use Group Node to perform aggregation.” It is unclear whether “Group” refers to (i) column‑set reuse (the later “Shared Field Group”) or (ii) SQL GROUP BY semantics. The paper uses “group” for both representational abstraction and aggregation, which are orthogonal concepts, producing conceptual confusion.\n\nW3. “Common attributes” not rigorously defined: In practice, “common” is implemented by hashing sorted field names and types to detect identical sets (Appendix B: Algorithm 1). This definition ignores trivial real‑world synonyms/aliases (e.g., course_id vs. c_id), naming conventions, and semantic equivalences. It also treats exact name/type equality as semantic equivalence, which is often false. As a result, the “group” abstraction is brittle and can fail on even mildly heterogeneous schemas. \n\nW4. Unnatural join modeling: The action space explicitly allows joining via “Shared Field Group linkages” (Appendix C). A shared column‑set is not a principled join criterion; joins should be driven by keys/constraints or explicit join conditions. This design invites spurious joins and ill‑formed SQL plans.\n\nW5. From “tree‑like” to graph—but treated as a tree: Section 3.1 calls the representation “tree‑like” while Appendix A formalizes a graph with five node types and several many‑to‑one/many edges (e.g., tables to shared field groups). MCTS on a non‑tree structure requires explicit handling of transpositions/state de‑duplication, which is not specified, compromising search correctness.\n\nW6. The “exploration” search is not soundly specified. \n- No clear objective or reward: The MCTS‑inspired procedure removes core MCTS components—no UCT, no explicit reward, and success is largely equated with the query returning a non‑empty result. “Non‑empty” is not a validity metric for semantics; it biases the KB toward easy, high‑cardinality patterns and does not guard against semantically wrong queries that coincidentally return rows.\n- Backpropagation on a graph without value semantics: The paper “records positive/negative outcomes on nodes along the path,” but does not define the value being propagated or how it influences future selection beyond a vague “priority.” Without a principled value estimate, the search policy is ad‑hoc.\n\nW7. (S, Q, U) triplets are essentially RAG/few‑shot memory with unclear formalism.\n- Definition of S is vague: Section 3.2 defines S as “a subset of the database schema’s structural” (sic). The representation of S, its relation to the actual join path/constraints, and how it ensures semantic alignment are not formalized. The grammatical error here underscores the lack of precision.\n- Not clearly different from RAG/few‑shot: The “knowledge base” is a store of (U, Q) (plus a sketch of S). Retrieval of relevant examples at inference time is standard RAG‑style few‑shot prompting. The paper does not articulate a principled difference from prior retrieval‑augmented NL2SQL or “retrieve‑then‑generate” pipelines; it mainly adds an offline synthetic‑example generation step.\n\nW8. The “tree” search is built on a shaky structure; therefore search results aren’t reliable.\nBecause joins can be proposed via “Shared Field Group linkages” and keys/constraints are not formalized, the state space being searched does not faithfully reflect relational semantics. If the structure is ill‑defined, the resulting triplets and the KB are not reliable, and any downstream search/generation is not sound."}, "questions": {"value": "See the above weaknesses W1-W8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KtuuA98tOg", "forum": "voLoLHms2K", "replyto": "voLoLHms2K", "signatures": ["ICLR.cc/2026/Conference/Submission1893/Reviewer_VqYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1893/Reviewer_VqYo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760759259957, "cdate": 1760759259957, "tmdate": 1762915933042, "mdate": 1762915933042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage framework for Text-to-SQL on complex enterprise databases. Stage 1 (Exploration): autonomously generates a knowledge base of triplets (schema structure, SQL query, natural language) using MCTS-inspired tree search. Stage 2 (Deployment): dual-agent system (InfoAgent + GenAgent) retrieves triplets as in-context examples to generate SQL. Reports 25.78% accuracy on Spider 2.0-Snow vs. 20.84% for ReFoRCE baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Two-stage design mimics how humans learn databases (explore first, then use)\n2. Triplets as executable knowledge vs. static schema descriptions is clever\n3. Ablation shows both stages contribute (+5.8 pts for exploration, +5.7 pts for dual-agent). Consistent gains across different LLM backbones (GPT-4o, Claude, Qwen)"}, "weaknesses": {"value": "1. Weak baselines \nThe paper compares against only 2 baselines: (1) ReFoRCE (20.84%) - one concurrent work (2) \"Spider-Agent\" (12.98%) - a bit naive\nSpider 2.0 has a public leaderboard at spider2-sql.github.io. ReFoRCE ranks #8/10 and doesn't seem to be the state-of-the-art. Comparing only against it is misleading.\n\nThe paper claims triplets discover \"business logic\". But: Standard database profiling tools (Great Expectations, dbt) can extract Unique values per column, Cardinality & distributions, FK relationships via value overlap, and Sample data\nThis is cheaper and deterministic vs. generating 10,000 queries. The paper should show compare against a stronger baseline with Database profiling. Without this, we can't tell if the 6-point gain is from \"having examples\" or specifically from \"having executable triplets.\"\n\n2. Weak qualitative analysis\nThe paper shows that accuracy improves but not why:\n- No example queries showing when triplets help vs. fail\n- No error analysis (what % of failures are wrong joins vs. wrong values?)\n- No case studies of successful complex queries\nCan't tell if the gain is from better table selection, join inference, or value formatting\n\n3. Exploration cost not justified\nGenerating triplets requires thousands of query executions during exploration.\nBut the paper doesn't show the actual wall-clock time for exploration, nor the cost in $ for LLM API calls\nIt's unclear whether simpler (and cheaper) approaches (profiling + few manual examples) would achieve similar results"}, "questions": {"value": "Can you add an ablation with database profiling (column unique values, cardinality, FK detection) as a baseline? This would show whether triplets add value beyond traditional metadata.\n\nProvide 3-4 concrete query examples showing:\n- Query that fails without triplets but succeeds with them (and why)\n- Query that fails even with triplets (current limitation)\n- What triplet was retrieved and how it helped\n\nError analysis: Of the 74% of queries that still fail, break down failure modes (wrong tables 30%, wrong joins 20%, etc.)\n\nHow much does exploration cost in time and $ per database? Is this practical for users?\n\nWhat makes a good triplet? Are simple queries more useful than complex ones? How many triplets are actually used at deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Us5VvPHlv7", "forum": "voLoLHms2K", "replyto": "voLoLHms2K", "signatures": ["ICLR.cc/2026/Conference/Submission1893/Reviewer_ujZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1893/Reviewer_ujZM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611384888, "cdate": 1761611384888, "tmdate": 1762915932869, "mdate": 1762915932869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SQLAgent, a two-stage LLM framework for Text-to-SQL. In the Exploration Stage, an LLM guided by a Monte Carlo Tree Search autonomously explores a database schema to create a knowledge base of (schema, SQL, natural language) triplets. In the Deployment Stage, a dual-agent system (InfoAgent and GenAgent) retrieves and iteratively refines SQL generation using these triplets as in-context examples. Experiments on Spider 2.0-Snow show higher execution accuracy than ReFoRCE and Spider-Agent."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-engineered framework. The two-stage design is clearly explained and systematically evaluated.\n- Strong empirical results. The method achieves higher execution accuracy with detailed ablation and LLM-backbone analysis.\n- Good implementation quality. Integration with LangGraph, Neo4j, and FAISS shows solid engineering and reproducibility."}, "weaknesses": {"value": "- Limited conceptual novelty. The “exploration-before-generation” paradigm is well established in prior agentic LLM work (e.g., ReAct, Reflexion, AlphaCode). The contribution mainly adapts this idea to Text-to-SQL.\n- Incremental adaptation. The MCTS-style schema exploration and dual-agent refinement are logical extensions rather than fundamentally new ideas.\n- Missing qualitative insights. The paper does not show concrete examples illustrating how exploration improves generation.\n- Incomplete baseline coverage. Missing recent retrieval- and agent-based systems such as C3, Chess, and Din-SQL.\n- Exploration cost unreported. Scalability and runtime overhead are unclear."}, "questions": {"value": "- Clarify the distinction from prior “search-then-generate” or agentic reasoning frameworks.\n- Include qualitative examples or visualizations to demonstrate how exploration benefits generation.\n- Report exploration runtime, number of generated triplets, and associated cost.\n- Expand baselines to include more recent Text-to-SQL models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "czdTPHj4sf", "forum": "voLoLHms2K", "replyto": "voLoLHms2K", "signatures": ["ICLR.cc/2026/Conference/Submission1893/Reviewer_iSCp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1893/Reviewer_iSCp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136543234, "cdate": 1762136543234, "tmdate": 1762915932708, "mdate": 1762915932708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}