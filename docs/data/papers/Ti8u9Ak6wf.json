{"id": "Ti8u9Ak6wf", "number": 137, "cdate": 1756729469455, "mdate": 1763086664691, "content": {"title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization", "abstract": "Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Code will be released to advance the related research.", "tldr": "", "keywords": ["Diffusion model", "Reward model", "Identity preservation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fd7c7d04eaf36e75253134e0b6bea02dc30133ac.pdf", "supplementary_material": "/attachment/86332af5df7716e1073459eb16c774652e351508.zip"}, "replies": [{"content": {"summary": {"value": "To improve the identity consistency of image-to-video models when the reference image contains a low-resolution human face and the motion prompt involves large-scale movements, this work proposes IPRO, a reinforcement learning–based framework. Experiments conducted with state-of-the-art video generation models, particularly Wan 2.2 A14B, demonstrate the effectiveness of IPRO. Interestingly, IPRO enhances identity preservation capabilities during the post-training phase of the base model through reward feedback, without requiring modifications to the model's architecture or the introduction of additional modules."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. IPRO is simple and effective, requires no modification to the base model's architecture, and can serve as a post-training approach to further enhance the base model's performance.\n\n2. This work has significant potential: by modifying the reward model and reward function, IPRO can improve other capabilities of the base video model, not limited to identity consistency."}, "weaknesses": {"value": "1. As the first equation in the Method section, Equation 3 does not include the ground truth, which raises some confusion about whether the ground truth is involved in the reward calculation.\n\n2. The comparative experiments with DPO are impressive; however, many current image/video generation methods (e.g., Flow-grpo [1]) employ GRPO rather than DPO to enhance the performance of the base models. Including additional comparisons with GRPO could further improve the quality and completeness of this work.\n\n[1] Flow-grpo: Training flow matching models via online rl."}, "questions": {"value": "1. There is a typo on line 179: “IPROpredicts” should be corrected to “IPRO predicts”.\n\nIf the authors address my concerns, I may consider increasing the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rv2LUEw0QE", "forum": "Ti8u9Ak6wf", "replyto": "Ti8u9Ak6wf", "signatures": ["ICLR.cc/2026/Conference/Submission137/Reviewer_SX3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission137/Reviewer_SX3p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650039265, "cdate": 1761650039265, "tmdate": 1762915456763, "mdate": 1762915456763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2li92fjoF8", "forum": "Ti8u9Ak6wf", "replyto": "Ti8u9Ak6wf", "signatures": ["ICLR.cc/2026/Conference/Submission137/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission137/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763086663443, "cdate": 1763086663443, "tmdate": 1763086663443, "mdate": 1763086663443, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IPRO, a reinforcement learning–based optimization framework for improving identity preservation in image-to-video (I2V) diffusion models. The method uses a facial reward signal derived from an identity encoder (ArcFace) to fine-tune pretrained diffusion models without altering their architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Architecture-agnostic: Does not modify the base model, making it plug-and-play for large diffusion systems."}, "weaknesses": {"value": "1. Novelty: The concept of introducing an additional identity-preserving objective in diffusion-based generative models is not new.\nSeveral prior works, such as DCFace [1], ID3 [2], ID-Booth[3] and many others, have already incorporated identity-related losses or conditioning into diffusion models to preserve subject consistency.\nThe novelty here primarily lies in the reinforcement-learning formulation rather than the introduction of an identity loss itself, which should be made clearer and positioned more accurately with respect to prior work.\n\n2. Metric–objective mismatch:\nThe evaluation relies mainly on FaceSim (mean cosine similarity of ArcFace embeddings), which measures static image similarity rather than temporal identity consistency.\nA model can achieve high FaceSim simply by producing nearly frozen faces (reward hacking) while still failing to maintain identity under motion or expression change.\n3. Need for verification-based metrics:\nIdentity consistency should ideally be assessed with verification metrics such as EER (Equal Error Rate) or FMR1000, etc, which directly evaluate whether a consistent identity is recognized across frames or sequences.\nUsing these would better reflect whether the same person is preserved, not just similar embeddings per frame.\nReproducibility:\nEvaluation uses proprietary datasets and large in-house models, which limits independent verification.\n\nEvaluation of identity is limited to 1 face recognition model, raising questions about generalizability and validity of the results\n\n[1] Minchul Kim, Feng Liu, Anil K. Jain, Xiaoming Liu:\nDCFace: Synthetic Face Generation with Dual Condition Diffusion Model. CVPR 2023: 12715-\n\n[2]\tJianqing Xu, Shen Li, Jiaying Wu, Miao Xiong, Ailin Deng, Jiazhen Ji, Yuge Huang, Guodong Mu, Wenjie Feng, Shouhong Ding, Bryan Hooi:\nID3: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition. NeurIPS 2024\n[3] Darian Tomasevic, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Struc, Peter Peer:\nID-Booth: Identity-consistent Face Generation with Diffusion Models. CoRR abs/2504.07392 (2025)"}, "questions": {"value": "Have you evaluated IPRO using verification-based metrics (e.g., EER, FMR1000 etc) to quantify temporal identity consistency?\nWhat is the conceptual difference between the presented approach and previous ones (ID3 etc.) ?\n\nHow does IPRO perform under different face recognition models (ArcFace vs. ElasticFace,  vs. AdaFace)?\n\nHow sensitive is the reward signal to noise or occlusions in face detection?\n\nCan IPRO generalize to non-facial identity cues such as clothing or body structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c7xEtSXNAG", "forum": "Ti8u9Ak6wf", "replyto": "Ti8u9Ak6wf", "signatures": ["ICLR.cc/2026/Conference/Submission137/Reviewer_KFZv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission137/Reviewer_KFZv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762082748304, "cdate": 1762082748304, "tmdate": 1762915456564, "mdate": 1762915456564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Identity-Preserving Reward-guided Optimization (IPRO), a reinforcement learning-based framework for improving identity consistency in image-to-video (I2V) diffusion models. Rather than modifying model architectures or introducing additional identity modules, the method optimizes the generation process by backpropagating a facial reward signal, computed via a facial feature embedding model, through the final denoising steps of the diffusion chain. The authors further design a multi-view facial scoring mechanism (FSM) and employ multi-step KL-divergence regularization to stabilize learning and address reward hacking. Results are demonstrated on both public and in-house I2V models, with quantitative and qualitative comparisons to prior methods highlighting improvements in identity preservation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a salient and underexplored challenge in I2V generation: maintaining identity consistency, particularly for small faces and long video sequences where identity drift is prominent.\n\n2. The RL-based reward-guided tuning framework is methodologically elegant, introducing identity-specific optimization without modifying the model architecture or requiring additional subject modules.\n\n3. Extensive experiments, including ablation studies and human evaluation, offer a comprehensive assessment."}, "weaknesses": {"value": "1. While the reward optimization and KL-regularization are justified intuitively and via prior art, the theoretical properties of the combined objective are underexplored. For example, the interaction between RL-style reward and the original model’s distribution in the presence of the KL penalty is only empirically motivated. There’s no discussion of the convergence characteristics or the effect on the underlying generative process’s distribution, particularly in high-variance or low-signal reward settings. \n\n2.  The approach is heavily dependent on the ArcFace reward model. While ablations demonstrate its empirical superiority, there is little discussion about the limitation of such pretrained facial encoders. Known sensitivities to pose, illumination, and motion blur are not extensively discussed. How does the reward model respond to occlusions or low-res faces, and could it be gamed by artifacts? A more extensive error analysis, perhaps with per-attribute breakdowns, would provide additional confidence.\n\n3. It is difficult to observe that the proposed method can achieve superior identity consistency in generated from Fig.1.\n\n4.  The weighting of the KL-divergence loss, step weights, and truncation steps ($K$) are crucial hyperparameters (Section 5.1), but there is no sensitivity analysis demonstrating robustness to these choices. Given the importance of balancing identity improvement without catastrophic forgetting or reward hacking, more systematic ablation or reporting on these choices is warranted."}, "questions": {"value": "As shown in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeesKdohfK", "forum": "Ti8u9Ak6wf", "replyto": "Ti8u9Ak6wf", "signatures": ["ICLR.cc/2026/Conference/Submission137/Reviewer_9q9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission137/Reviewer_9q9n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164365997, "cdate": 1762164365997, "tmdate": 1762915456423, "mdate": 1762915456423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IPRO, a reinforcement learning-based framework for improving identity preservation in image-to-video generation. The method leverages a facial identity reward model, a novel facial scoring mechanism, and KL-divergence regularization to enhance identity consistency without modifying the base model architecture. Experiments on Wan 2.2 and an in-house I2V model show improvements in face similarity metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of identity preservation in I2V generation is important and underexplored.\n\nThe proposed facial scoring mechanism and multi-step KL regularization are novel and well-motivated.\n\nThe paper provides extensive experiments, including comparisons with adapted T2V methods and ablations.\n\nThe method does not require architectural changes, making it more scalable."}, "weaknesses": {"value": "Lack of strong baselines: The comparisons are mainly against adapted T2V methods (e.g., MoCA, Concat-ID) rather than native I2V identity-preservation methods. This weakens the claim of superiority.\n\nLimited generalization: The method is heavily focused on facial identity, while other aspects of identity (e.g., clothing, accessories) are not addressed. This limits the scope and practical impact.\n\nThe method lacks novelty."}, "questions": {"value": "No Questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XXxM5hLwNc", "forum": "Ti8u9Ak6wf", "replyto": "Ti8u9Ak6wf", "signatures": ["ICLR.cc/2026/Conference/Submission137/Reviewer_5wrN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission137/Reviewer_5wrN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204062581, "cdate": 1762204062581, "tmdate": 1762915456317, "mdate": 1762915456317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}