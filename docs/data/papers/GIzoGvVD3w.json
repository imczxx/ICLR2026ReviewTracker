{"id": "GIzoGvVD3w", "number": 10435, "cdate": 1758171381042, "mdate": 1759897650784, "content": {"title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning", "abstract": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.", "tldr": "We propose a new legal benchmark tailored for Multi-agent systems.", "keywords": ["Multi-agents system", "Legal judgement"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89c196b074ffe127fe3b8123d83a7945c6870c95.pdf", "supplementary_material": "/attachment/81d0440a484f325e9a7e8e8ac88b949450baded0.zip"}, "replies": [{"content": {"summary": {"value": "Multi-agent systems promise increased accuracy in solving complex tasks compared to vanilla LLMs. The authors claim there are no benchmarks yet that are suitable for measuring MAS performance in the legal domain. This is why they create MASLegalBench. MASLegalBench uses court cases tackling GDPR issues. The authors used an LLM to extract the sections from the PDFs and validated it with human checks. They ran a human evaluation to verify the quality of the extracted tasks. They find that 44 out of the top 60 performances were achieved by MAS compared to 16 by vanilla LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors are early in evaluating MAS in the legal domain\n- The authors pick a realistic evaluation domain that seems challenging\n- The dataset construction and validation seems sound"}, "weaknesses": {"value": "- The focus on GDPR makes the benchmark rather narrow. Therefore the general title is a bit misleading and should probably be adjusted.\n- To me it is not clear why the authors limited the retrieval to 5 hits in the largest case, since this appears like quite a low number given today's LLM context windows\n- Both sparse and dense retrieval have their pros and cons, which is why they are usually combined. However, this experiments is missing\n- Out of the five evaluated models only two are reasoning models (Qwen3-8B and DeepSeek-v3.1) and it is not clear what settings were used for inference. To me it is unclear why the authors don't focus their evaluation more on reasoning models given that they are benchmarking \"Deductive Legal Reasoning\".\n- The name MASLegalBench is somewhat unfortunate since it suggests a connection to the LegalBench evaluation, but it operates on different data."}, "questions": {"value": "- L065: what do you mean with expert-authored court cases?\n- You might be interested in LEXam: Benchmarking Legal Reasoning on 340 Law Exams\n- check the grammar on L302-303\n- L473: typo in Data Type Hybrid\n- What was the reasoning budget for the reasoning models?\n- What are the inference settings you used (temperature, top_p, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m1PjZGWqv4", "forum": "GIzoGvVD3w", "replyto": "GIzoGvVD3w", "signatures": ["ICLR.cc/2026/Conference/Submission10435/Reviewer_W2TS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10435/Reviewer_W2TS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760955720637, "cdate": 1760955720637, "tmdate": 1762921740374, "mdate": 1762921740374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies LLM agents for legal reasoning. First, a benchmark dataset is constructed, focused on GDPR law, containing several multiple choice questions regarding law cases. Afterwards, a MAS system is constructed, using LLM and non-LLM agents, which are aggregated by a meta-LLM agent. That system is then evaluated on the proposed benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposal of a legal benchmark, using real data, for evaluation law-experts agents seems quite relevant.\n\n- It is nice that a LLM-based MAS system is proposed and evaluated on that benchmark.\n\n- The paper is in general well-written and well-presented."}, "weaknesses": {"value": "- One of the key contributions seem to be the proposal of a benchmark dataset. However, currently the evaluation of that dataset itself seems to be quite poor. It is only evaluated by 3 students, and they see a small percentage of the dataset (30 questions out of 950). The data may even have some anomalous behaviour (such as one student simply giving 100 to every criteria).\n\nIf the dataset itself may have issues, then it is hard to reach conclusions based on that benchmark.\n\n- I wonder a bit about the option of using multiple choice questions. Surely a team of legal agents would not be used to answer multiple choice questions, but rather to take a legal decision/recommendation. Hence, perhaps there is a gap between the dataset and the actual intended application?\n\n- Some of the conclusions of the paper do not really seem to follow from the experimental results. Additionally, there is no analysis of the variance or the statistical significance, which makes it even harder to accept the conclusions (e.g., \"44 out of 60 top results are achieved under our designed MAS\", while there are several values in the table quite close to each other).\n\nDetailed comments:\n\n- \"Unfortunately, few studies have explored the potential of MAS in legal tasks,\" -> Which studies have done that before? Citation missing?\n\n- \"IRAC reasoning is designed to address the limitation of classical deductive reasoning, where the truth of the premises in a legal argument is often neither straightforward nor self-evident1 .\" -> Why cite a book as a footnote?\n\n- \"Intuitively, we define the following mapping relations to bridge the actual data with our conceptual framework discussed in Section 3:\" -> Somewhat strange that the table is not presented in a table environment, but just follows directly in the text.\n\n- \"We invited three students with legal backgrounds or prior experience in legal-related research.\" -> Is that good enough? May need to scale up the human evaluation? E.g., evaluator 2 simply gives 100 to everything?\n\n- \"A total of 30 samples were randomly selected, each including the original text, the extracted question, and the corresponding answer.\" -> 30 out of 950? It doesn't seem to be good enough?\n\n- Table 2: I think the performance of Random @3 and @5 could also be calculated?\n\n- Table 2, Qwen2.5-7B Instruct: Wrong formatting, CS is the 2nd best (62.84), not F+LR+AR+CS (62.74).\n\n- \"Richer contexts can lead to better performance. The results indicate that involving more agents and providing richer reasoning steps generally leads to improved performance. For instance, ’BM25@5’ outperforms ’BM25@3’ when using the GPT-4o-mini model.\" -> I don't see how you reach this conclusion by simply changing from @3 to @5, you are just increasing the output ranking size, which should obviously increase the results monotonically in any case.\n\nAdditionally, there are several cases where a lower number of agents leads to better results. For instance, in the @1 column, there is only one case where the highest number of agents has the highest result.\n\n- \"This effect is more pronounced in larger-parameter models, such as DeepSeek-v3.1 and GPT-4o-mini, suggesting that the improvements brought by MAS enable the Meta-LLM to better evaluate the execution results of these agents.\" -> It is hard to see this conclusion from your table. For instance, several cases in DeepSeek only 2 agents lead to the top results. \n\n- \"Our results show that 44 out of the 60 top performances (bold values) are achieved under our designed MAS, demonstrating the effectiveness of our MAS design as well as its potential for legal tasks.\" -> What about statistical ties? In how many cases are you really statistically significantly better?\n\n- \"The best performance is often achieved when agents handling Legal Rules or Common Sense are activated.\" -> Curiously, many times the results drop when CS is activated (from second to last row to last row of every block).\n\n- \"We conduct a case study on this phenomenon to investigate the underlying reasons, as illustrated in Figure 3.\" -> Table 3.\n\n- Section 7 is quite strange, shouldn't that be in related work?\n\n- Ethics: human annotators were used, but it was not mentioned whether this work obtained ethical approval.\n\n- \"To ensure the reproducibility of our experimental results, we put our detailed implementations under Section 5.\" -> Not sure I follow, did you mean to say that your methodology is explained in detail?\n\n- Appendix: \"with a average of approximately 185.53 chunks.\" -> \"an average\""}, "questions": {"value": "- In how many cases the MAS system is actually statistically significantly better than the single agent system?\n\n- Why an evaluator has 100 for every criteria? Can the data be trusted?\n\n- Did you obtain ethical approval for this work?\n\n- As shown in my detailed comments, in several places I struggle to reach your conclusions based on your experimental data. Can you clarify?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Human annotators are used, but the paper does not mention whether ethical approval was obtained by their institution."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fyNIl9tE8K", "forum": "GIzoGvVD3w", "replyto": "GIzoGvVD3w", "signatures": ["ICLR.cc/2026/Conference/Submission10435/Reviewer_Pdab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10435/Reviewer_Pdab"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797899443, "cdate": 1761797899443, "tmdate": 1762921739848, "mdate": 1762921739848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes legal benchmark collected from GDPR related documents consisting of 950 questions and various context for answering these questions. It also proposes an multi-agent framework, which first decomposes a question into atomic subtasks and then solve them step by step. To answer the subquestions, they assign agents different roles, based on the problem solving procedure in legal domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The writing is clear and easy to follow\nSince there aren’t many LLM benchmarks in the legal domain, this dataset could be a useful addition."}, "weaknesses": {"value": "The main contribution of the paper seems to be the dataset. But there are several issues:\n1. The dataset is rather narrow. It only focuses on GDPR-related questions. This feels too limited to really assess how well LLMs handle legal tasks in general.\n2. No comparison with other legal benchmarks. The paper doesn’t show how this dataset stands out from existing ones. The authors claimed the result dataset is the only one with rich context. But there are similar datasets for comparison, for example, \"LEXam: Benchmarking Legal Reasoning on 340 Law Exams\"\n3. Dataset quality isn’t clearly validated. The dataset is created in a semi-automatic way (using LLMs to extract questions), but only 30 samples are checked manually. There’s also no information on how annotations were done, what criteria were used, or whether annotators agreed with each other.\n\nThe proposed multi-agent framework MAS lacks novelty. The core idea is to decompose questions into subquestions, which has been extensively explored in previous works. Besides, the experiments don’t reveal much. The only real takeaway is that more context leads to better performance, which is pretty expected. Therefore, it’s still not clear how this dataset helps us better understand LLMs’ legal reasoning abilities."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lbtjV989aS", "forum": "GIzoGvVD3w", "replyto": "GIzoGvVD3w", "signatures": ["ICLR.cc/2026/Conference/Submission10435/Reviewer_MqBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10435/Reviewer_MqBR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867526163, "cdate": 1761867526163, "tmdate": 1762921738880, "mdate": 1762921738880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a novel benchmark for the legal domain, focusing on multi-agent systems (MAS). The authors fill a gap in benchmarks for MAS in the legal domain. Multi-agent systems show great promise, and the legal domain would inherently profit greatly from multi-agent solutions due to natural task decomposition, e.g., the popular IRAC method. In the reviewer's opinion, the primary contribution of the paper is the dataset comprising 950 legal questions extracted from official GDPR-based court cases. The authors also propose a framework for multi-agent systems for the legal domain, based on a popular legal task decomposition called IRAC. Lastly, they present results for different frontier models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "# Originality\n- A benchmark in the legal domain, with multi-agent systems in mind, seems useful to the community. The task composition into IRAC appears practical and well-motivated.\n\n# Quality\n- The data collection and filtering process appears reasonable, and the authors attempt to validate the dataset through human annotations. \n- The authors demonstrate their results transparently and include a range of analyses of both the dataset and frontier model performances.\n\n# Clarity\n- The paper is overall easy to read.\n- Tables and Figures are well formatted.\n\n# Significance\n- A well-constructed legal benchmark with a focus on task decomposition suitable for multi-agent systems could provide a valuable signal for method development."}, "weaknesses": {"value": "## Quality\n1. Three annotators for 30 samples (unclear if each got the same 30 samples, 10 samples per annotator?) is not a particularly extensive human annotation effort. Having worked with human annotators extensively, the reviewer's experience is that at least 2 annotations per sample, ideally 3 as a tie breaker, are required to get a notion of inter-human agreement to validate the quality of any human annotations, even in specialist domains (This is from personal experience; the reviewer couldn't find a good citation for this claim). It would then be useful to analyse where the human annotators disagree. While a 30-sample annotation round can provide an early signal, it is the reviewer's opinion that it does not suffice to paint the whole picture on the annotation quality and thus the verification of the dataset creation. (1) Please clarify the annotator setup in more detail, and potentially provide inter-human agreement, and provide some examples where human annotators disagreed with either the model's extraction or with other human annotators. (2) Would it be feasible to get more human annotations? (3) Can you provide more details about the randomly selected samples? Case 14 is dominating the dataset. Are most samples coming from Case 14?\n2. The presented results, e.g., Table 2 and 3, Figure 2 and 3, don't contain any measure of variance. The experiment description does not mention any hyperparameters or whether these results were run over multiple seeds. It is the reviewer's opinion that all claims of Sections 6.1 and 6.2 are not supported sufficiently. (4) Please add some measures of variance, e.g., standard deviation or standard error, for at least some models. Please report your hyperparameters for all models, such as temperature, context length, etc.\n3. The results for EMB are reported but never discussed. They also seem to suggest that LR and F+LR often perform better than any MAS, as evidenced by the GPT-4o-mini results. (5) When the authors get results with more statistical significance, can they also discuss the EMB results?\n\n\n## Clarity\n1. The abstract is wordy and could be improved by leaving out some adjectives. For example, change \n\n> Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations.\n\ninto\n\n> Our benchmark uses GDPR as the application scenario, encompassing background knowledge and covering\nreasoning processes that reflect the intricacies of real-world legal situations.\n\n2. The Introduction is slightly hard to read, with the introduction of \"reasoning steps\" as \"sub-task\" (line 54), which then become \"subproblems\", and the subproblems are passed to the MAS, even though in the pseudocode, the full case description is passed to the Meta-LLM, which is part of the whole MAS. Either way, clarification of that paragraph would greatly improve the flow.\n3. Could you please clarify if you were referring specifically to the legal domain with the following sentence (line 62):\n\n> To the best of our knowledge, this is the first benchmark that provides sufficiently rich context to enable multiple LLM agents to collaborate in reasoning and exploration\n\n4. The unfolded enumeration is hard to read and easily improvable in Section 2.3\n5. Line 158 typo: \"when an MAS\" -> \"when a MAS\"\n6. Could the authors clarify the use of bookmarks? Why don't the authors use citations here, especially when citing a book? Additionally, when citing a book, please specify the exact pages you're referring to.\n7. It is the reviewer's opinion that Figure 1 would probably look better on page 3. Currently, it's two pages removed from its first reference.\n8. In Section 3.2, the header of \"1)\" does not end with a period, but the headers of \"2)\" and \"3)\" end with a period.\n9. In Section 3.2, Algorithm 1 is inconveniently placed after Step 2, not right after it was referenced in Step 1.\n10. What are \"reality issues\" in Section 4.2?"}, "questions": {"value": "1. Could you please clarify what the difference between a legal decision and a legal opinion is, and who decided which sample belongs to which category? This distinction was not validated in the human annotations either. Who annotated the samples into legal decisions and legal opinions?\n2. Could you please cite BM25 when it's first referenced?\n3. Table 4 does not really convey a lot of information. For example, taxonomies are not discussed anywhere else in the text, so it's unclear what exactly is being referred to. Data type must have a type for \"Hybrid\". It is also unclear how \"Task Decomposition\" would be formally defined in this context.\n\nTo summarise, the reviewer believes that all the necessary components are in place for a potentially strong contribution and a robust benchmark. However, each component still lacks quality and depth. First, the performance analysis focuses only on accuracy, without any measure of variance. Are some tasks harder than others? Are there other statistical measures for a dataset with both yes/no and single-choice questions that might be interesting? Currently, F+LR are hardcoded. Could the MAS extract the relevant sections itself, given the source document? Second, the human annotation effort is a starting point, but it does not reach a level that would provide confidence in the quality of the entire dataset. Third, the paper's formatting appears rushed and requires improvement.\n\nThe reviewer is looking forward to the discussion during the rebuttal period."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w00ZFlAIuw", "forum": "GIzoGvVD3w", "replyto": "GIzoGvVD3w", "signatures": ["ICLR.cc/2026/Conference/Submission10435/Reviewer_RDkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10435/Reviewer_RDkk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10435/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005573390, "cdate": 1762005573390, "tmdate": 1762921738347, "mdate": 1762921738347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}