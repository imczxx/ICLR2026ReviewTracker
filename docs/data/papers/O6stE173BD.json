{"id": "O6stE173BD", "number": 20982, "cdate": 1758312348519, "mdate": 1759896948594, "content": {"title": "Discovering Hierarchical Software Engineering Agents via Bandit Optimization", "abstract": "Large language models (LLMs) are increasingly applied to software engineering (SWE), but they struggle on real-world tasks that are long-horizon and often out of distribution. Current systems typically adopt monolithic designs where a single model attempts to interpret ambiguous issues, navigate large codebases, and implement fixes in one extended reasoning chain. This design makes it difficult to generalize beyond training data. Inspired by how human engineers decompose problems into sub-tasks, we argue that SWE agents should be structured as orchestrators coordinating specialized sub-agents, each responsible for a specific sub-task such as bug reproduction, fault localization, code modification, or validation. The central challenge is how to design these hierarchies effectively. Manual decompositions follow human workflows but often mismatch LLM capabilities, while automated search methods such as evolutionary strategies require evaluating a very large number of candidates, making them prohibitively expensive for SWE. We show that formulating hierarchy discovery as a multi-armed bandit problem enables efficient exploration of sub-agent designs under limited budgets. On SWE-bench-Verified, this approach outperforms single-agent systems and manually designed multi-agent systems. On SWE-bench-Live, which features recent and out-of-distribution issues, our system ranks 2nd on the leaderboard with a 36B model, surpassing larger systems such as GPT-4 and Claude. This provides the first evidence that hierarchical multi-agent systems improves generalization on challenging long-horizon SWE tasks.", "tldr": "", "keywords": ["Multi-armed bandit", "Model selection", "Software engineering"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/858d97bf639ea98f8b0895d092d2c274a4e41351.pdf", "supplementary_material": "/attachment/6adab39b751c756bb34adc5be7fb5246950c02db.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the problem of automated development of SWE-Agents. They consider a multi-agent setup structured such that an orchestrator agent calls multiple sub-agents. The main concern of the paper is optimizing the start-up prompts for the orchestrator and subagents. They treat this as a bandit problem, maintaining and growing a set of sub-agents over time. Each sub-agent is scored using a with an LLM credit assignment approach. The paper's evaluations show strong results on SWE-Bench Verified and Live."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The major strength of the work is the novel formulation of the problem as an MAB problem and the demonstration different aspects of the  formulation leads to improvement of performance.\n\n1. The framing of optimising multi-agent systems as multi-armed bandits is novel in the context of SWE-Agents. The work successfully formulates a UCB strategy for this context as well as a way to grow the number of subagents. Ablations demonstrate how optimising sub-agents explicitly outperforms static approach.\n\n2. The use of LLM based credit assignment technique in the multi-armed bandit setup is novel and very interesting here. It shows a way to learn from specific mistakes the system might be making in a modular way. Ablations also support the importance of this metric.\n\n3. Evaluation show improvements on standard benchmarks, especially on SWE-Bench Live."}, "weaknesses": {"value": "1. The major limitation of this work is that it is limited to only improving the starting prompt of the system. From the difference in performance of scaffolds present on https://www.swebench.com/, it is reasonable to believe that design decisions can impact the behavior of agent, as much or maybe more than prompts.\n\n2. The analysis of the behaviour of the system is limited to short qualitative analysis. This is currently not substantiated with example or quantitative metrics. There is also no exposition of what sorts of agents are \"discovered\"."}, "questions": {"value": "1. Related to W1, can this idea be extended to agent design as well as prompt content? \n\n2. Related to W2:\na) Is there quantitative evidence for multi-agent systems being better in the sorts of ways described in 5.4\nb) What sorts of agents are \"discovered\" and is there a way to characterise the trajectory of the system as is discovers these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6N5N9MGdsC", "forum": "O6stE173BD", "replyto": "O6stE173BD", "signatures": ["ICLR.cc/2026/Conference/Submission20982/Reviewer_xwJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20982/Reviewer_xwJx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855502583, "cdate": 1761855502583, "tmdate": 1762999997881, "mdate": 1762999997881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper searches for prompts of sub-agents for software engineering. The resulting agent consists of a set of sub-agents and an orchestrator (all parameterized by prompts). During inference, the agent will ask the orchestrator to call sub-agents sequentially to solve a problem. To search for the set of sub-agents, it maintains a pool of candidate sub-agents, picks and evaluates a subset of candidates each time, and updates the sub-agents' information according to the evaluation results afterward. The paper uses UCB to balance exploration and exploitation during search. It can also add a new sub-agent each round to improve diversity. LLM-judge is used to assign credits to each sub-agent as well. The searched agent is evaluated on two popular benchmarks and demonstrated strong performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method is intuitive and interesting, although it largely relies on the abilities of LLMs for judging and proposing sub-agents. We do need various ways to balance exploration and exploitation. The searched agent demonstrates strong performance on popular benchmarks as well. \n\nThe paper is generally well-written and easy to understand."}, "weaknesses": {"value": "* Missing naive baseline, such as evolution search. One can treat all prompts of sub-agents and/or orchestrators as parameters and use LLMs + evolution search to optimize them. There are various prior works that balance exploration and exploitation for naive LLM tree search as well. The authors discussed this baseline in the method section, claiming it is prohibitively expensive with no experimental results. \n* I'm not sure if the comparison with baselines is fair, missing experimental details. It would be great to include the costs of calling Claude-4 (which model?) for each method. I am not sure if the method is stochastic or deterministic either. If not deterministic (hard to imagine LLM to be deterministic even with temperature=0), how noisy is the evaluation? \n* The meta-prompts (e.g., for proposing and refining sub-agents) look long and detailed. Can the method discover novel sub-agents that are not expected, given the meta-prompts? \n* It would be great to include the searched prompts for various methods as well."}, "questions": {"value": "* Are there results of the naive evolution search?\n* Can you provide more experimental details such as the variance of evaluation, the cost for each method, and meta-prompts for each method? \n* Can you share the final best-performance prompts of the searched sub-agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vEEWQcsAmZ", "forum": "O6stE173BD", "replyto": "O6stE173BD", "signatures": ["ICLR.cc/2026/Conference/Submission20982/Reviewer_rVvm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20982/Reviewer_rVvm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876071777, "cdate": 1761876071777, "tmdate": 1762939464702, "mdate": 1762939464702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BOAD, a method for discovering and selecting subagents to use as tools for an orchestrator agent in software engineering tasks. It considers subagents in a set as arms of a multi-armed bandit and assign credits with both the test-based final outcome reward and LLM-as-a-judge process reward. After running this bandit optimization on 12 issues from SWE-Bench Verified, the discovered multi-agent system can perform well on both SWE-Bench Verified, and SWE-Bench Live, which doe not have repository-level overlap with the problems in SWE-Bench Verified."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting formulation of the multi-agent system discovery problem as a multi-armed bandit. This creates balance between exploration and exploitation while selecting and evolving subagents.\n\n2. Great performance on SWE-Bench Live demonstrates the effectiveness of the method.\n\n3. Comprehensive ablation showing the effectiveness of having the subagents, customizing the orchestrator, and using hindsight helpfulness for credit assignment."}, "weaknesses": {"value": "1. Lack of details about the actual optimization process. How many agents are there in the final set of subagents? How many of the top agents are from the expanded set or the initial set of subagents? What are the final top agents selected? Qualitatively why are they better than other subagents? Readers need these details to get a better idea of the final discovered system.\n\n2. Single run evaluation is insufficient. The non-determinism of LLM agents results on lots of randomness in every agent run, which impacts the optimization process. Is BOAD always effective as reported or will its performance fluctuate across multiple runs?"}, "questions": {"value": "1. Any plans to compare against manual optimization conducted by human engineers in the loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Qb3rC6zkg", "forum": "O6stE173BD", "replyto": "O6stE173BD", "signatures": ["ICLR.cc/2026/Conference/Submission20982/Reviewer_Mz7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20982/Reviewer_Mz7z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016571906, "cdate": 1762016571906, "tmdate": 1762939405976, "mdate": 1762939405976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous work in agentic frameworks for code generation have shown a worrying trend -- performance drops significantly from in-distribution benchmarks (SWE-Bench Verified) to out-of-distribution benchmarks (SWE-Bench-Live). The authors posit that one of the root causes for this is the long-horizon and complex nature of SWE-tasks, which makes it hard for single-agent frameworks to delegate and solve tasks. Furthermore, current multi-agent systems rely on expensive evolutionary optimization algorithms, which is impractical in the current domain. \n\nThe authors hypothesize that a multi-armed bandits (MAB) presents an efficient framework to build and optimize such multi-agent systems to solve such tasks. Specifically, they propose BOAD, which treats hierarchical multi-agent systems as a sequential decision-making process with the reward signal for evaluating each sub-agent (each arm in the MAB) coming from an LLM-as-a-judge.\n\nBOAD achieves impressive performance on SWE-Bench Verified and SWE-Bench-Live."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well written and I thank the authors for explicitly stating the MAB formulation in the context of the code generation task.\n- In terms of novelty, I was pretty surprised but a multi-arm bandit optimizer hasn't been tried before for software engineering tasks."}, "weaknesses": {"value": "**Methodology:**\n* **Training using 12 problems**: My understanding is that the SOTA accuracy number was reached after including 12 problems from SWE-Bench-Verified in the design set for BOAD. This raises two concerns:\n\t* Why 12 problems specifically? Does increasing or decreasing the set of design problems drastically effect final performance?\n\t* None of the other baselines are automatically tuning the multi-agent system. I understand that an evolutionary agent here might be less efficient than BOAD, but including a result for how BOAD fairs against an evolutionary agent would undoubtedly present a useful point of comparison to understand the tradeoff.\n\n* **Efficiency Experiment:** One of the motivating points of using BOAD (from the introduction) is that it is hypothesized to be more efficient than evolutionary multi-agent frameworks at the same task. However, the Token Analysis experiment is not enough to justify the efficiency for two reasons:\n\t* There isn't a direct comparison against an evolutionary multi-agent framework.\n\t* Efficiency can be achieved by either reducing token usage OR by reducing the underlying model size:\n\t\t* Specifically, In terms of floating point operations, querying a smaller model more times is more efficient than querying a larger model less times.\n\t\t* For example (be advised: this is a rough analysis without access to underlying data):\n\t\t\t* Assuming equivalence in all other aspects, a `30B` model can generate `20%` more tokens than a similar `36B` model yet have the same total cost.\n\t\t\t* Under this logic, the OpenHands single-agent that uses `Qwen3-coder-30B` and achieves a 51.6% resolution rate for SWE-bench Verified in is actually *more efficient*  than SWE-Agent+BOAD using the `OSS-36B` model which achieves a 53.2% resolution rate.\n\t\t* My recommendation:\n\t\t\t* Getting access to TFLOP data is extremely hard. Instead, most works use normalized cost, which is defined as the fractional cost of querying a smaller model per token compared to the cost of querying the largest model. e.g. if cost for 36B model is `1` per token, then cost for a 30B model will be `0.8334`.\n\t\t\t* Then, such methods will show an efficiency-performance tradeoff curve.\n\t\t\t* The key insight is to demonstrate that the efficient-agent algorithm is at the pareto froont of the tradeoff curve for different configurations.\n\t\t\t* Look at Figure 3 of this paper [https://arxiv.org/abs/2504.07247](https://arxiv.org/abs/2504.07247) for an example of how this is computed and analyzed. This might be a good paper for the related works as well.\n\n\n**Overall:** I'm slightly leaning towards rejecting the paper. The results are pretty impressive and I generally haven't seen a similar mutli-agent system optimized with bandits before, but there are some issues with the experimental setup that need to be better understood before proceeding. I'm happy to discuss these points with the authors during the discussion period."}, "questions": {"value": "Look at weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yIMP1pF3w3", "forum": "O6stE173BD", "replyto": "O6stE173BD", "signatures": ["ICLR.cc/2026/Conference/Submission20982/Reviewer_5qkT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20982/Reviewer_5qkT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199082457, "cdate": 1762199082457, "tmdate": 1762939398330, "mdate": 1762939398330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}