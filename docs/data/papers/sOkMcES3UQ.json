{"id": "sOkMcES3UQ", "number": 6630, "cdate": 1757990883725, "mdate": 1759897903980, "content": {"title": "Dual-Structure Self-Distilled Learning for Enhancing Unsupervised Semantic Segmentation", "abstract": "Unsupervised semantic segmentation (USS) aims to assign semantic labels to pixels without human annotations, yet existing methods struggle to capture semantic structures across different abstraction levels. We propose Dual-Structure Self-Distilled Learning (DSSDL), a novel framework that performs self-distillation within a single network by transferring the stronger semantic representations learned in label space to guide shallower layer, without relying on external teachers. DSSDL integrates two complementary structures:(1) an affinity structure that performs binary pair classification over pairwise similarity scores and leverages a reversed directional mining strategy to preserve fine-grained local consistency.(2) a cluster structure that derives semantic codes from global prototypes and aligns per-pixel predictions via a swapped prediction loss to encourage consistent global grouping. By jointly modeling both structures, DSSDL enforces semantic consistency at both local and global levels, resulting in coherent and robust segmentations. Our method achieves substantial improvements over the strong baseline STEGO, with accuracy and mIoU gains of +16.7 and +3.3 on COCO-Stuff, +14.8 and +3.2 on Cityscapes, and +8.2 and +11.5 on Potsdam-3, respectively.", "tldr": "a novel framework that performs self-distillation within a single network by transferring the stronger semantic representations learned in deeper layers to guide shallower layers, without relying on external teachers.", "keywords": ["Dual-Structure", "Self-Distilled Learning", "Unsupervised Semantic Segmentation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d2a17e2a4e9e2f8d94da33452e7e0a3f1a217e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Dual-Structure Self-Distilled Learning (DS²DL), a new framework for unsupervised semantic segmentation (USS) that aims to jointly model local pixel affinities and global clustering structures. The core idea is a dual self-distillation scheme operating within a single network. An \"Affinity Structure Distillation\" module reframes segmentation as a binary classification of pixel pairs, guided by soft labels derived from the model's own predictions. Complementing this, a \"Cluster-Level Structure Distillation\" module enforces global consistency by aligning local pixel predictions with global semantic prototypes updated via Exponential Moving Average (EMA). The authors claim substantial improvements over strong baselines like STEGO on COCO-Stuff, Cityscapes, and Potsdam-3 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method achieves state-of-the-art or highly competitive performance across three standard benchmarks.\n2. The paper includes a detailed \"Implementation Details\" section (4.3) that specifies architectures, optimizers for different datasets, momentum coefficients, and other crucial hyperparameters.\n3. The overall organization of this paper is easy to follow."}, "weaknesses": {"value": "1. The central novelty of this paper is largely derivative of prior work: \n- Learning pairwise affinities is a classic approach in clustering. Formulating it as a binary classification task is not new (e.g., seen in deep metric learning). The use of cosine similarity from label features as pseudo-ground truth is a straightforward extension of self-training paradigms.\n- The \"Cluster-Level Structure Distillation,\" involving local-to-global code prediction and EMA-updated global prototypes, is heavily inspired by SwAV (Caron et al., 2020). The paper appropriately cites SwAV but downplays the extent of the methodological borrowing, presenting it as a novel component of its framework.\n- EMA-update is the cornerstone of many recent self-supervised and semi-supervised methods like DINO and BYOL. The paper fails to differentiate its use of self-distillation from this extensive body of work in a meaningful way.\n\n2. The proposed \"Reversed Directional Mining\" strategy (Section 3.3) is counter-intuitive and inadequately justified：\n- Hard example mining literature almost universally argues for focusing on difficult samples to improve decision boundaries. Why this paper considers it is beneficial to focus on easy positives? The authors provide no theoretical or strong empirical evidence to support this reversed logic.\n- The ablation study in Table 2 is insufficient. Removing positive or negative sample pairs entirely is too coarse. A more convincing study would compare the proposed reversed mining to standard hard-mining and no mining (r=1) to isolate the actual effect of this specific strategy. \n- The performance graphs in Figure 4 show that the optimal $r$ varies wildly across datasets, suggesting this is a sensitive hyperparameter requiring careful tuning rather than a robust, generalizable mechanism.\n\n3. The ablation study presented in Table 2 fails to convincingly demonstrate the necessity of core components:\n- Removing \"affinity structure distillation\" causes a massive performance drop (Acc. from 84.6 to 32.5). This is unsurprising, as it appears to remove the primary loss function driving local consistency. This doesn't prove the superiority of the proposed affinity module, but rather that local consistency learning is essential.\n- Removing \"cluster structure distillation\" results in a much smaller drop (Acc. from 84.6 to 83.0). This raises questions about the importance of this entire branch of the model. Are the performance gains primarily from the affinity learning part?\n- The study does not ablate the cross-image pixel pairing, the Sinkhorn algorithm for optimal transport, or the orthogonality regularization. The contributions of these individual design choices remain unverified."}, "questions": {"value": "* Removing \"cluster structure distillation\" results in a much smaller drop (Acc. from 84.6 to 83.0). This raises questions about the importance of this entire branch of the model. Are the performance gains primarily from the affinity learning part?\n* Hard example mining literature almost universally argues for focusing on difficult samples to improve decision boundaries. Why this paper considers it is beneficial to focus on easy positives?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ol1ETing8E", "forum": "sOkMcES3UQ", "replyto": "sOkMcES3UQ", "signatures": ["ICLR.cc/2026/Conference/Submission6630/Reviewer_e8JZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6630/Reviewer_e8JZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632081426, "cdate": 1761632081426, "tmdate": 1762918948594, "mdate": 1762918948594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Dual-Structure Self-Distilled Learning, a novel framework for Unsupervised Semantic Segmentation (USS). The authors argue that existing USS methods struggle to effectively capture semantic structures at different levels of abstraction simultaneously. Specifically, most methods do not explicitly and jointly model both the affinity structure (fine-grained, local pixel relationships) and the cluster structure (high-level, global semantic groups). The proposed DS^2DL framework addresses this gap by using a dual self-distillation scheme: Affinity Structure Distillation and Cluster Structure Distillation, within a single network. It works by enforcing semantic consistency at both the local and global levels simultaneously, transferring semantic cues from the label space into the feature space. The method demonstrates significant performance gains over the strong baseline STEGO and other state-of-the-art methods on three benchmark datasets: COCO-Stuff, Cityscapes, and Potsdam-3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:** The paper's primary originality lies in its holistic framework design. It is the first to explicitly and jointly model the affinity (local) and cluster (global) structures for unsupervised semantic segmentation within a unified self-distillation framework. While prior works have used affinity or clustering, the synthesis of these two as a dual self-distillation mechanism is novel. Key original contributions include Dual-Structure Self-Distillation and Reversed Directional Mining.\n\n**Quality:** The paper is of high quality. The methodology is well-reasoned, and the empirical evaluation is robust.\n\n**Clarity:** The paper is well-written, well-structured, and effectively uses visuals to explain its core concepts.\n\n**Significance:** This work makes a significant contribution to the challenging field of unsupervised semantic segmentation.The method achieves substantial improvements over a wide range of state-of-the-art baselines, including the strong STEGO model. The reported gains (e.g., +16.7 Acc on COCO-Stuff, +14.8 Acc on Cityscapes and +11.5 mIoU on Potsdam-3 over STEGO) are significant."}, "weaknesses": {"value": "- The caption of Figure 1 is too brief. It would benefit from a more detailed introduction and explanation of the overall framework. Additionally, please clarify the meaning of the icon between the \"weight sharing\" components—is it representing the visual features? The lightweight architecture, which consists of a linear layer and a two-layer MLP with SiLU activations, should also be explicitly illustrated in the figure for completeness.\n- As Reversed Directional Mining (RDM) is a key novel component. its experimental validation should be more comprehensive. Although the paper analyzes the hyperparameter $r$ (Figure 4) and notes that $r=1$ corresponds to standard binary cross-entropy (BCE), the main ablation study (Table 2) does not include a direct comparison between the full model with RDM (e.g., $r=2.22$ on Cityscapes) and the baseline model using standard BCE ($r=1$). Including this comparison would more clearly demonstrate the effectiveness of RDM.\n- The Quantitative Evaluation section requires substantial improvement. Beyond listing numerical comparisons, the authors should summarize the overall trends observed in the results and provide insightful analysis explaining the performance differences."}, "questions": {"value": "- Would it be useful to introduce hyperparameters in Eq (8) to create a weighted loss function for model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cgknomvovh", "forum": "sOkMcES3UQ", "replyto": "sOkMcES3UQ", "signatures": ["ICLR.cc/2026/Conference/Submission6630/Reviewer_Tm7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6630/Reviewer_Tm7S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810936514, "cdate": 1761810936514, "tmdate": 1762918948208, "mdate": 1762918948208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study aims to improve the feature representation in unsupervised semantic segmentation. The network first projects the features into a lower-dimensional clustered label space and then use the similarity in the label space to supervise the feature similarity for feature clustering. It then utilize a cluster structure to derive semantic codes from prototypes and align pixel prediction using optimal transport theory. It is verified on different benchmarks to reach the state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is mostly easy to follow, and the ideas are presented clearly. Starting from the inspiration from self-distillation and cross-modal distillation, the authors smoothly present the idea to utilize the distillation from affinity and clustering structures to make the feature space more compact.\n- The experimental performance is strong on benchmarks like COCO-Stuff, Cityscapes, and Potsdam-3 with its Acc and mIOU reaching state-of-the-art compared to previous literature. \n- The ablation over each component is considered necessary and compelling. The advantage of cluster sturcture distillation over affinity structure distillation is surprising."}, "weaknesses": {"value": "1. Originality issues. The two major components--affinity structure learning and cluster structure distillation, are not designed and invented in this study.  Projecting the features into a lower-dimensional clustered label space and then supervising the feature similarity using the similarity in the lower space has been a common practice in unsupervised learning. Simply assembling these components into unsupervised semantic segmenation is not considered to reach the standard of novelty of this conference. \n2. What really concerns me is that the authors failed to acknowledge and discuss the differences with these classic works. For example, using binary classification as the cluster supervision loss as in Eq. 1 had been widely utilized in [1] and many following papers. The online coding using optimal transport theory as in Eq. 5 has long been proposed in SwAV[2]. Both of these studies are not properly referred to and discussed with in this manuscript, which leaves a compression that these methods are originated in this work. \n2. The sampling mining function may require further proof. The authors propose to emphasize more on the negative pair loss and decrease the importance of positive pairs. But the intuition is not explicitly explained in Sec. 3.3 and other part of the manuscript. \n3. As the authors emphasize on the soft label similarity learning instead of using hard labels in Eq. 1, there should also be an ablation on the advantage of such a change compared to common practice as well. \n\n[1] Wen, Yandong, et al. \"Pairwise similarity learning is simple.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[2] Caron, Mathilde, et al. \"Unsupervised learning of visual features by contrasting cluster assignments.\" Advances in neural information processing systems 33 (2020): 9912-9924."}, "questions": {"value": "The originality issue is a great concern in this study and I would like to hear from the authors' rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ldLBGb6nE", "forum": "sOkMcES3UQ", "replyto": "sOkMcES3UQ", "signatures": ["ICLR.cc/2026/Conference/Submission6630/Reviewer_x2DC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6630/Reviewer_x2DC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917799597, "cdate": 1761917799597, "tmdate": 1762918947768, "mdate": 1762918947768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for unsupervised semantic segmentation that improves both local consistency and global coherence without using labeled data. The method juses a two-part architecture that models pairwise pixel relationships through a binary similarity classification, and a cluster level loss, which aligns per-pixel predictions with global prototypes. This dual self-distillation transfers semantic knowledge from label space to feature space, ensuring both fine-grained boundary accuracy and consistent global grouping. They use entropy-regularizers and cluster orthogonality constraints on prototypes as well. The work boasts significant performance gains over previous state-of-the-art methods across COCO-Stuff, Cityscapes, and Potsdam datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a variety of interesting new loss functions to help improve unsupervised semseg performance\n- The paper compares against a wide variety of baselines and benchmarks on commonly used benchmark tasks\n- Good ablations and hyperparameter exploration\n- Nice summary of prior work and good descriptions of the added methods"}, "weaknesses": {"value": "- Perhaps compare  against or comment some recent open domain semabtic segmentation methods like those that use diffusion of VLMs\n- Possibly explore some other backbones to understand dependence on backbone choice\n- The paper introduces a \"bag of tricks\" style of additional loss terms, is there a way to unify some of them or reduce unecessary ones?"}, "questions": {"value": "- How sensitive is the loss to different weightings of the loss terms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "scFJeIhsay", "forum": "sOkMcES3UQ", "replyto": "sOkMcES3UQ", "signatures": ["ICLR.cc/2026/Conference/Submission6630/Reviewer_PZZW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6630/Reviewer_PZZW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945000817, "cdate": 1761945000817, "tmdate": 1762918947412, "mdate": 1762918947412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}