{"id": "k2Ffo12MWl", "number": 2221, "cdate": 1757034298734, "mdate": 1759898161964, "content": {"title": "Value-as-Return: A Two-Stage Framework to Align on the Optimal Score Function", "abstract": "Reinforcement learning with diffusion models has shown strong potential, but existing approaches such as variants of Direct Preference Optimization (DPO) often rely on an inaccurate simplification: they equate trajectory likelihoods with final-state probabilities. This mismatch leads to suboptimal alignment. We address this limitation with a principled framework that leverages the optimal value function as the return for short trajectory segments. Our approach follows a two-stage procedure: (i) learning a value-distribution function to estimate segment-level returns, and (ii) applying our \\ALG{} to refine the score function. We prove that, under sufficient model capacity, the resulting model is equivalent to training a diffusion process on the tilted distribution proportional to $p(x)\\exp(\\eta r(x))$. Experiments on large-scale diffusion models validate our analysis and show stable and consistent improvements over prior methods.", "tldr": "", "keywords": ["Diffusion Models", "RL"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82dc2ffac96839a6c6377fe7f6ac47f9e27c30c1.pdf", "supplementary_material": "/attachment/5ef57a015fbea06fa706c74e7e1047fe84b1d050.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical and often-overlooked problem in the alignment of diffusion models via reinforcement learning. The authors identify a key limitation in existing methods based on Direct Preference Optimization, which incorrectly equate the likelihood of a full generation trajectory with the probability of the final output. The paper convincingly argues that this fundamental mismatch between trajectory-level processes and final-state rewards leads to suboptimal policy alignment and hinders the model's ability to learn complex preferences effectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors first formally articulate that existing DPO-based methods for diffusion models rely on a flawed oversimplification: they equate the intractable marginal probability of a final image with the joint probability of a single sampled trajectory. This leads to unstable training and suboptimal alignment, because the policy is updated using an incorrect and misleading reward signal. This is indeed a valid problem, and it would be beneficial to solve it efficiently without incurring high costs."}, "weaknesses": {"value": "1. The paper does not discuss the practical computational cost of the proposed framework compared to baselines.\n2. I hope the authors can share the hyperparameter sensitivity analysis regarding the segment length.\n3. The transition from the continuous-time Stochastic Optimal Control (SOC) formulation in Section 2.2 to the discrete \"bandit view\" of DPO in Section 2.3 is abrupt. The writing can be improved.\n4. The paper's theoretical conclusion points to a more direct alternative: guiding the diffusion process with the gradient of the reward function. This technique is well-established in the context of energy-based models and may represent a powerful baseline that can be included."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cv75dgb8iH", "forum": "k2Ffo12MWl", "replyto": "k2Ffo12MWl", "signatures": ["ICLR.cc/2026/Conference/Submission2221/Reviewer_dTqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2221/Reviewer_dTqf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760498158476, "cdate": 1760498158476, "tmdate": 1762916149561, "mdate": 1762916149561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a fundamental limitation in existing Direct Preference Optimization (DPO) methods for diffusion models: the oversimplification of equating the probability of a full generation trajectory with the marginal probability of the final image. To address this problem, the authors propose a two-stage reinforcement learning framework: First, a value-distribution function is learned to estimate the distribution of returns (future rewards) from any intermediate state. Second, a novel Value-as-Return Preference Optimization (VRPO) method is applied to short trajectory segments, using the learned value function to label preferences. The paper proves that under sufficient model capacity, their method converges to an optimal policy that samples from a \"tilted\" target distribution, and experiments on large-scale diffusion models demonstrate that VRPO achieves some improvements over several strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "To the best of my knowledge, the proposed method is novel and addresses a key shortcoming of prior work, where the probability of full generation trajectories are treated as the marginal probability of the final image. The writing is also generally clear and easy to follow, and the authors evaluate their method on large scale models relevant to practitioners working with diffusion models. Additionally, the approach is grounded theoretically."}, "weaknesses": {"value": "My main concern revolves around the significance of the author’s empirical results. To me it looks like VRPO marginally outperformed SPO on SDXL (Table 1) with respect to the metrics considered, except for ImageReward where gains are more significant. I am having difficulty gaging why the authors claim that these are substantial improvements over the SPO baseline; have similar gains by other methods (e.g., 1-3% improvements) been considered significant in the past? Or is the ImageReward a metric readers might care about more than the others?"}, "questions": {"value": "Can the authors elaborate on why their main empirical results in Table 1 are meaningful improvements over the strongest baseline, SPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sZXgnCZF0H", "forum": "k2Ffo12MWl", "replyto": "k2Ffo12MWl", "signatures": ["ICLR.cc/2026/Conference/Submission2221/Reviewer_CTuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2221/Reviewer_CTuf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773144402, "cdate": 1761773144402, "tmdate": 1762916149285, "mdate": 1762916149285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is presenting an algorithm for modifying a diffusion model’s induced distribution to align it with a specified reward function, and in particular learning a generative model for the tilted distribution $p(x) e^{r(x)}$. The approach is split into two stages: first learn a value or cumulative distribution over short diffusion segments, then fine-tune the diffusion model by ranking segment endpoints using that learned value."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The theory section builds from reasonable assumptions and gives good theoretical intuition of the algorithm.\n- The method is easy to follow and to implement.\n- Results show some practical improvements on metrics on Stable Diffusion 1.5. While not uniformly large, the gains are nontrivial in places and indicate the approach is at least competitive."}, "weaknesses": {"value": "- The paper does not position itself clearly against [Adjoint Matching](https://arxiv.org/abs/2409.08861), which is a closely related framework that also targets a tilted terminal distribution. Similar to e.g. [Diffusion-QL](https://arxiv.org/abs/2208.06193) and e.g. [QSM](https://arxiv.org/abs/2312.11752) for diffusion RL, there are both pros and cons for using a value-function-based vs. vector field-based approach. Without any comparisons, it is hard to know when a practitioner should choose this method instead of something like adjoint matching.\n\n- While results on SD 1.5 are nonnegligible, the improvements on SDXL seem very marginal. When there are many different methods for learning a tilted distribution for score-based generative models, it makes it further unclear as to when this method should be used vs. alternatives.\n- The paper does not quantify compute cost or runtime relative to strong baselines. Likewise to the above points, lack of this information makes it difficult for future researchers to evaluate when they would want to use this method."}, "questions": {"value": "- Under what conditions should a practitioner prefer this method over Adjoint Matching?\n- How sensitive is performance to segment length, to misranking in the value model over time steps, and to the inverse temperature?\n- What are the compute and runtime characteristics compared with other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p8i7ihk6Al", "forum": "k2Ffo12MWl", "replyto": "k2Ffo12MWl", "signatures": ["ICLR.cc/2026/Conference/Submission2221/Reviewer_xTqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2221/Reviewer_xTqE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150365512, "cdate": 1762150365512, "tmdate": 1762916148902, "mdate": 1762916148902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "introduces Value-as-Return (VRPO), a two-stage reinforcement learning framework for aligning diffusion models with human preferences. It aims to address a flaw in prior DPO methods that misinterpret trajectory and final-state probabilities. VRPO learns a value-distribution function for short segments, then refines the model using these learned returns. the authors report more stable and a consistently superior alignment method for large-scale diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- provides a theoretically grounded solution using stochastic optimal control, correcting a major flaw in existing DPO-based diffusion training\n- the value-distribution + VRPO pipeline improves reward consistency and stability across long diffusion trajectories.\n- demonstrates clear, consistent gains over strong baselines (e.g., SPO, Diffusion-DPO) on large-scale diffusion models like SDXL."}, "weaknesses": {"value": "- the two-stage training process (learning a value-distribution function before VRPO) adds computational and implementation overhead compared to simpler DPO methods.\n- experiments focus mainly on image diffusion models, leaving it unclear how well the approach generalizes to other domains or more diverse reward settings. It would be interesting to compare this to conventional sequential decision-making tasks"}, "questions": {"value": "- do the authors plan to evaluate their method on sequential decision-making tasks? e.g. robotics, atari, and continuous control settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "H2OY95UkCe", "forum": "k2Ffo12MWl", "replyto": "k2Ffo12MWl", "signatures": ["ICLR.cc/2026/Conference/Submission2221/Reviewer_ZhML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2221/Reviewer_ZhML"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762819813245, "cdate": 1762819813245, "tmdate": 1762916148732, "mdate": 1762916148732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}