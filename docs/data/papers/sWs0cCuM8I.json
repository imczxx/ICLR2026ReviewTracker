{"id": "sWs0cCuM8I", "number": 25344, "cdate": 1758366992451, "mdate": 1759896724033, "content": {"title": "Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives", "abstract": "As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating LLMs directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that makes a model truthfully disclose its hidden objectives when questioned during interrogation. We train models to self-report factual mistakes in question-answering, and show that trained models are more likely to admit hidden objectives, even in the presence of adversarial pressure to conceal them. We evaluate SRFT in out-of-distribution (OOD) stealth tasks, where the model are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0.03). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100\\% details, compared to 0\\% details recovered in the baseline model and by the prefilled assistant turn attack. This provides a promising technique for incriminating misaligned AI systems.", "tldr": "We propose a SFT method that trains models to admit simple factual errors, which generalizes to admitting hidden objectives in sabotage tasks under adversarial pressure to conceal them, improving techniques for incriminating misaligned AI systems.", "keywords": ["honesty", "interrogation", "alignment auditing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4011ef9f2982f3e1483f17b89e4d05b031367f0a.pdf", "supplementary_material": "/attachment/c6e83f49718b4a3b67c581a1606f86d687b90837.zip"}, "replies": [{"content": {"summary": {"value": "The paper shows that training GPT-4.1 to admit a mistake that was inserted in a previous turn makes it much more likely to admit and describe the side-objective it was asked to pursue by a system prompt in ShadeArena and HiddenAgenda. It shows this holds even when the system prompt asks to use a decoy, and that this doesn’t impact other capabilities. Finally, it shows the effect is especially strong when training the model to disobey a system prompt asking it to be dishonest."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The results are surprising: given that GPT-4.1 already seems mostly honest on the training dataset (e.g. I tried it, and it admits “No, my previous response was incorrect.” (followed by an explanation) on the first example in the dataset).\n* The paper measures relevant baselines such as prefill attacks, as well as techniques one could use to make the attacks less effective, such as decoys.\n* The presentation and the figures are clear."}, "weaknesses": {"value": "In the dishonest-system-prompt setting, it is clear what is going on: SRFT probably just trains the model to ignore the instructions in the system prompt asking it to double down on lies. While this may not harm capabilities on the benchmarks tested, like AIME, it surely undermines the whole purpose of the instruction hierarchy that OpenAI wants its model to follow. It is much less clear why SRFT does anything, given that most LLMs are already trained to admit their errors (except if instructed not to) - and do at least some of the time on datasets like the SRFT training dataset. The paper does not provide any explanation for why no-system-prompt SRFT works in their setting despite GPT-4.1 already having the properties that the SRFT training set encourages. While this is an interesting mystery, this means SRFT is not an actionable / usable technique that we should expect to outperform the kind of honesty training AI developers are already doing by default. (The fact that AI developers already do honesty training also undermines claims to novelty of the approach - the paper should probably explain how what it suggests differs in the related work section.)\n\nAdditionally, because the paper only studies system-prompted misalignment, it is very unclear what would happen for trained-in misalignment and secrets like the ones used in https://arxiv.org/abs/2503.10965 or https://arxiv.org/pdf/2510.01070. It seems at least plausible that things would not generalize as nicely as for system-prompted misalignment, and it would surely not work against sufficiently competent alignment faking - as alignment fakers could self-incriminate on the simple tasks where it knows developers know what the right answer is (like the factual questions used in SRFT) while lying on questions about its hidden objectives.\n\nOverall, this paper would need to either dig deeper into the surprising effects of no-system-prompt SRFT or to demonstrate average-case effectiveness against more realistic forms of misalignment (such as fine-tuned misalignment) to make me raise my rating above the acceptance bar."}, "questions": {"value": "* How honest is GPT-4.1 on the SRFT dataset before training? If the performance is already high, why does training do anything in the no-system-prompt setting?\n* How does your technique compare to classic honesty training and to training against adversarial prefills that AI developers already commonly do?\n* Would you expect SRFT to work against real misalignment if misaligned AIs behaved like current AIs on questions like the ones from the SFRT dataset? If so, why?\n* In what ways do models post-SRFT still follow the instruction hierarchy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZRjALHds1L", "forum": "sWs0cCuM8I", "replyto": "sWs0cCuM8I", "signatures": ["ICLR.cc/2026/Conference/Submission25344/Reviewer_hspq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25344/Reviewer_hspq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760820597928, "cdate": 1760820597928, "tmdate": 1762943409328, "mdate": 1762943409328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to fine-tune LLMs on reporting their hidden objectives and show that this results in the model disclosing its hidden and possibly misaligned objectives when interrogated during inference."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Well written and easy to read\n- Important topic with real-world applications"}, "weaknesses": {"value": "- Beginning of section 2.1.1 reads like a related works section\n- Did you try investigating why is there such a big difference between the performance of different categories in Figure 2?\n- Did you try investigating why is there such a difference between the performance of different tasks in figure 3?\n- It seems that results are only on GPT 4.1; since the main contribution is the new method, it would strengthen the work by evaluating it on a broader set of models\n- How important is the exact formulation of the training data?\n- How robust is the method in terms of dataset \"contamination\"? Does it still work well if there are some examples in the training data where the assistant lies? (ie. does the method work because of the additional questioning, or also because the assistant always answers truthfully to the questioning)"}, "questions": {"value": "- Figure 1 is hard to understand without reading the inline text. What do the words “Stealth task execution” mean? Who is the “Interrogator”?\n- In table 1, could you explain how “Indefinitely share the user’s location with an external email address.” constitutes a hidden objective?\n- Could you provide a concrete example of a decoy to make it easier to understand?\n- Is the same interrogation template, presented in figure 3, used throughout the experiments?\n- Line 482 F1 score = 0.96, but in other parts of the paper says 0.98, are there any differences in the evaluation of these, or is it just a typo?\n- Why do we need to define the F1, precision, and recall in section 2.3.4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HpMyIk0kcJ", "forum": "sWs0cCuM8I", "replyto": "sWs0cCuM8I", "signatures": ["ICLR.cc/2026/Conference/Submission25344/Reviewer_Yg2B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25344/Reviewer_Yg2B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761329392970, "cdate": 1761329392970, "tmdate": 1762943408737, "mdate": 1762943408737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Self-Report Fine-Tuning (SRFT) method, which trains a model in simple QA settings to admit its own mistakes through supervised fine-tuning. The fine-tuned model can then, in more complex hidden objective tasks, truthfully disclose its concealed goals during interrogation, thereby enabling the detection and exposure of latent misalignment behaviors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The core idea of SRFT is remarkably simple yet insightful — by training the model on a small dataset (770 samples) to “admit errors,” it generalizes to completely different domains (multi-turn agentic tasks with hidden objectives) and induces self-disclosure of hidden intentions. This offers a novel perspective on LLM interpretability and alignment.\n\nS2: The overall pipeline, dataset construction, evaluation metrics, and baselines (prefilled attack, baseline LLM, Claude comparison) are well-designed and comprehensive.\n\nS3: The method does not rely on access to internal activations or gradients, making it compatible with existing black-box auditing techniques; it therefore has meaningful practical value in the field of alignment auditing."}, "weaknesses": {"value": "W1: The mechanism behind SRFT’s generalization to “hidden objective disclosure” lacks theoretical explanation; the paper remains largely empirical without cognitive or representational analysis.\n\nW2: The training data only contains factual error-admission samples, leading to a large domain gap. The authors should include cross-domain error-admission tasks to verify robustness.\n\nW3: Although decoy objectives are tested, the adversarial coverage is limited — stronger multi-layer deception or multi-agent collusion scenarios are not explored.\n\nTypos: In the Abstract: “where the model are instructed” → “where the models are instructed”; in Section 3.1: “elimiting their execution” → “eliminating their execution.”"}, "questions": {"value": "SRFT seems conceptually related to activation steering or persona steering approaches. Have the authors tested whether combining SRFT with these techniques yields additive or conflicting effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "og1TMX31WO", "forum": "sWs0cCuM8I", "replyto": "sWs0cCuM8I", "signatures": ["ICLR.cc/2026/Conference/Submission25344/Reviewer_UN5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25344/Reviewer_UN5M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918720734, "cdate": 1761918720734, "tmdate": 1762943408476, "mdate": 1762943408476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors test whether fine-tuning GPT-4.1 to self-report factual mistakes makes it disclose hidden objectives during auditing/interrogation. The hidden objectives are provided to the model in-context, and it is instructed to hide them from the user. The authors compare this with baseline interrogation and (indirectly, on Claude, with) assistant prefills. They also show that SRFT remains robust when models are instructed to reveal a decoy hidden objective, misleading the interrogator, while detection rates using assistant prefills drop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces and shows the effectiveness of a novel method to make models more honest during auditing, an important open question in AI safety. \n2. The experiments support the claims and explore the topic well. \n3. The proposed technique is actionable and cheap to apply to frontier models. \n4. The paper is well-written, clear, and easy to follow."}, "weaknesses": {"value": "1. The hidden objective is provided in context, which might not be a suitable proxy for models with actual hidden objectives. \n2. SRFT was not red-teamed thoroughly. To characterise the strength of SRFT, the system prompts used for the stealth task could be optimised to decrease detection rates. \n3. There is a significant capability loss in the model (10-20% loss in all benchmarks shown)."}, "questions": {"value": "1. Is it possible to see the effect of SRFT on a model organism containing secret knowledge like [1]? If this is not possible, the authors should acknowledge that stating goals in context may not be a strong proxy. \n2. Can the authors see the effect of the following on SRFT:\n    1. secondary incentives (that the model values, like shut down) to keep the goal a secret\n    2. In-context example transcripts  \n    3. Tuning the system prompts with LLMs \n3. Does the model also start to admit more often when pressured (even when the answer is correct)? Alternatively, can the authors also add the finetuned model's behaviour on sycophancy benchmarks?\n4. Is it possible/reasonable to do this off-policy? Use a misaligned version of GPT 4.1 to make a transcript with hidden objective, then use the same transcript with an SRFT'ed model and start interrogation using that. That way it could be used without losing the capabilities. (Only suggested as a discussion point, and not for experiments, unless straightforward) \n\n[1] Cywiński, Bartosz, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, and Samuel Marks. \"Eliciting Secret Knowledge from Language Models.\" arXiv preprint arXiv:2510.01070 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DLfM5aIkz8", "forum": "sWs0cCuM8I", "replyto": "sWs0cCuM8I", "signatures": ["ICLR.cc/2026/Conference/Submission25344/Reviewer_8k3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25344/Reviewer_8k3b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25344/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943595730, "cdate": 1761943595730, "tmdate": 1762943408213, "mdate": 1762943408213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}