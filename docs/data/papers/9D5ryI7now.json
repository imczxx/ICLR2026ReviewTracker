{"id": "9D5ryI7now", "number": 987, "cdate": 1756827178405, "mdate": 1763311006893, "content": {"title": "In-Token Learning for High-Fidelity Image Restoration via Diffusion Transformers", "abstract": "Diffusion-based image restoration has advanced rapidly, yet existing methods remain fragile under severe degradations, exhibiting geometric drift, identity loss, or texture hallucination. We present In-Token Learning, a token-aligned framework that redefines restoration as learning a conditional velocity field via rectified flow matching (RFM), directly transporting pure noise to clean images under intra-token alignment within a Multimodal Diffusion Transformer (MMDiT). This design enables robust and high-fidelity restoration, avoiding misleading details from degraded inputs.\nTo further stabilize conditioning, we introduce Direct Low-Quality Guidance (DLG), a lightweight mechanism that injects degraded-image and prompt embeddings into model's native text-conditioning pathway, without relying on external prompts, side branches, or sequence-level concatenation.\n\nOur framework (i) improves robustness under severe degradations, (ii) improves fidelity by narrowing the long-standing perception-distortion gap, and (iii) supports QHD ($2560{\\times}1440$) inference and seamless scaling to ultra-high resolutions through fixed-length attention. \nWe further demonstrate the first $12$K restoration of the classical scroll painting Along the River During the Qingming Festival using an unmodified backbone.\nAcross five benchmarks (DIV2K, LSDIR, FFHQ, RealLQ250, RealPhoto60), our method achieves state-of-the-art performance on both full- and no-reference metrics, and generalizes to colorization, achieving state-of-the-art perceptual quality.\nThese results position In-Token Learning as a unified and scalable paradigm across diverse tasks, degradations, and resolutions.", "tldr": "Our work establishes a new paradigm that unifies diffusion models and flow matching for high-fidelity image restoration, including ultra-high-resolution super-resolution and denoising, and further generalizes to automatic colorization.", "keywords": ["Diffusion Models", "Flow Matching", "Ultra-High Resolution", "Super-Resolution", "Automatic Colorization", "Image Restoration"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db53069dd3d7e06e05ab4fe66ae8eb43b74e895a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces In-Token Learning, a novel token-aligned framework for robust image restoration using diffusion models. The approach redefines restoration as learning a conditional velocity field via rectified flow matching within a Multimodal Diffusion Transformer (MMDiT), enabling direct and accurate transport from noise to clean images. To stabilize conditioning, the authors propose Direct Low-Quality Guidance (DLG), which efficiently integrates degraded-image and prompt embeddings into the model’s text-conditioning pathway, eliminating the need for external prompts or complex architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is simple and effective.\n2. The paper is easy to understand and follow.\n3. The experiments are comprehensive, which contain SR, deoising, and colorization."}, "weaknesses": {"value": "1. The proposed method is overly simplistic, as it merely concatenates the low-quality image to the input of FLUX and applies LoRA for fine-tuning, without any technical innovation.\n2. The results of the ablation experiments show excessively large performance differences among the various settings, which makes me question the authenticity of these ablation data. I do not believe that these ablated components could have such a significant impact on PSNR, SSIM, and CLIPIQA metrics. I strongly recommend that the authors provide reproducible code and models to enhance the credibility of the paper.\n3. There are obvious errors in the citation format throughout the paper.\n4. The paper does not provide efficiency comparisons with other methods, such as parameter count and inference speed."}, "questions": {"value": "Refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RuNBPvccij", "forum": "9D5ryI7now", "replyto": "9D5ryI7now", "signatures": ["ICLR.cc/2026/Conference/Submission987/Reviewer_zDzX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission987/Reviewer_zDzX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567809837, "cdate": 1761567809837, "tmdate": 1762915653319, "mdate": 1762915653319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. A recurring assumption in several comments is that our method inherits strong generative priors from diffusion models and that LoRA fine-tuning merely adjusts those priors. This is not the case. Our work does **not** rely on any diffusion denoising prior. Instead, our core contribution is a **restoration-specific redefinition of Rectified Flow Matching (RFM)** that fundamentally changes the target distribution and the geometry of the generative trajectory.\n\n## ▌1. Redefining the RFM endpoint removes diffusion priors\n\nStandard RFM (and diffusion) transport noise toward a clean-image manifold learned during pretraining. \nWe instead redefine the endpoint as the inverse-degradation target:\n\n$$\nz_0 = E(D^{-1}_\\phi (I\\_{lq}) )\n$$\n\nwhich contracts the target distribution and grounds it in the degradation model rather than the diffusion prior. \nBecause of this, the model cannot rely on a pretrained denoising prior. \n\n**LoRA is not fine-tuning an existing diffusion prior; rather, it is learning a new clean-image manifold induced by our redefined endpoint.**\n\nThis is a conceptual shift from diffusion-based restoration.\n\n## ▌2. Decoupling degraded content from the trajectory (no denoising-style refinement)\n\nThe degraded latent $E(I_{\\mathrm{lq}})$ never enters the generative trajectory:\n$$\nz_t = (1-t) z_0 + t \\epsilon,\n$$\nmeaning the model is not “denoising” the degraded input. \n\nIt must instead learn to map noise to a clean target purely through our conditional RFM objective. \n**This makes LoRA training fundamentally different from standard diffusion finetuning**, highlighting its non-incremental nature.\n\n## ▌3. Why ITL + DLG are structurally necessary, not engineering tricks\n\n> To illustrate why the three components interact structurally rather than additively, the conditional RFM objective can be written as:\n\n$$\n\\mathcal{L}_{\\text{RFM}} = \\mathbb{E}\\_{z_0,\\epsilon,t} \\Big[  \\\\|   v\\_\\theta ( z\\_t, t, e\\_f, [x\\_t;  E(I\\_{lq})])  -  (\\epsilon-E(\\mathcal{D}^{-1}\\_\\phi(I\\_\\text{lq})) \\\\|_2^2 \\Big].\n$$\n\nBecause our formulation discards the diffusion denoising prior, these components are not interchangeable but **structurally required**:\n\n- **RFM** defines a stable deterministic transport from pure noise $\\epsilon$ to the restoration-specific endpoint $z_0 = E(D^{-1}_\\phi I\\_{lq} )$ , which replaces the clean-image prior normally inherited from diffusion.\n- **ITL** injects structural evidence through $h_t = [x_t; E(I_{\\mathrm{lq}})]$, while keeping degraded statistics out of the trajectory and preserving fixed-length attention.\n- **DLG** supplies the semantic priors needed to learn $\\mathcal{D}^{-1}\\_{\\phi}$ from scratch: the system prompt provides a **task-level semantic prior**, and the image embedding provides an **instance-level semantic prior** for $I_{\\mathrm{lq}}$.\n\nTogether, these components form the minimal conditioning structure compatible with our redefined RFM endpoint.\n\n## ▌4. Practical scalability without diffusion priors (1B parameters)\n\nDespite discarding diffusion priors, our reformulated RFM enables:\n\n-  **1B trainable parameters** to unify SR, denoising, and colorization \n-  **State-of-the-art or second-best results on 6 out of 7 benchmarks**, matching or surpassing **2.3B–2.4B diffusion-based baselines**, despite using only 1B trainable parameters \n-  QHD native inference and **12K restoration capability** (first full restoration of the 12K Qingming scroll)\n\nThat LoRA can successfully learn such a complex inverse mapping **without leveraging diffusion priors** is a strong indicator that the method represents a conceptual shift rather than an incremental modification.\n\nFurthermore, achieving these results with only **1B trainable parameters**, to our knowledge, the smallest parameter count among recent diffusion-based restoration models reaching comparable fidelity, underscores the efficiency of the proposed formulation. Achieving this level of performance **without** diffusion denoising priors highlights both the efficiency and the underlying conceptual strength of our approach.\n\n## ▌Summary\n\nOur method does not refine or utilize diffusion priors. \n\nBy redefining the RFM endpoint and restructuring the trajectory, we require LoRA to learn a new clean-image prior from scratch. This makes our approach **fundamentally different from diffusion restoration** and directly addresses any concerns regarding incremental contribution.\n\nThese clarifications may help contextualize several concerns that were based on assumptions specific to diffusion-style restoration, which do not apply to our redefined RFM formulation."}}, "id": "uDnMEXWqNZ", "forum": "9D5ryI7now", "replyto": "9D5ryI7now", "signatures": ["ICLR.cc/2026/Conference/Submission987/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission987/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission987/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763304819037, "cdate": 1763304819037, "tmdate": 1763308981870, "mdate": 1763308981870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a strategy of learning the conditional velocity field via rectified flow matching, directly generating clean images from noisy with low-quality and text prompts as conditions. It achieves good image restoration quality and has a certain generalization effect for larger image resolutions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The writing is clear and easy to understand, facilitating readers' comprehension.\n\n2.The proposed method has consistently stable restoration results for the restoration tasks at different resolutions."}, "weaknesses": {"value": "1.The evaluation metrics of the paper are incomplete.  LPIPS remains the main metric for assessing the quality of image restoration. However, it is not reported in the main text of the paper, and the LPIPS, PSNR, and SSIM under all settings are not reported in the supplementary materials either.\n2. The additional low-quality and text prompts used as conditions seem more like an engineering trick to enhance the ability of condition control..  Moreover, the specific role these text prompts play in the text is not clear; it might merely be to match the pattern of the pre-trained model.\n3. There is an issue of unfairness in the experimental setup. Why should the test metrics of different settings of super-resolution be averaged on the basis of no-reference metrics, but only full-reference metrics of D1 be reported? This will cause misunderstandings for the readers.Furthermore, the experimental setup is also different from other methods. Other methods were not trained on different degraded images. Such a comparison is difficult to determine whether it is the effect of the network design or the training settings."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tAd7avi3F3", "forum": "9D5ryI7now", "replyto": "9D5ryI7now", "signatures": ["ICLR.cc/2026/Conference/Submission987/Reviewer_t7Y5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission987/Reviewer_t7Y5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575794488, "cdate": 1761575794488, "tmdate": 1762915653148, "mdate": 1762915653148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose \"In-Token Learning\", a unified paradigm for high-fidelity image restoration, with five core contributions:  first, it innovates the restoration paradigm by abandoning the traditional \"iterative denoising of degraded images\" approach, learning a conditional velocity field via Rectified Flow Matching (RFM), and combining in-token alignment with Direct Low-Quality Guidance (DLG) to achieve \"direct mapping from pure noise to clean images\", thus avoiding the propagation of degraded artifacts;  second, it designs the lightweight guidance mechanism DLG, which injects the fused embedding of degraded images and task prompts into the model's native text-conditioning pathway without relying on external Vision-Language Models (VLMs) or ControlNet-style side branches, providing task-aware guidance at minimal cost;  third, it narrows the perception-distortion gap by simultaneously improving full-reference metrics (PSNR, SSIM) and no-reference perceptual metrics (CLIPIQA, MUSIQ) across multiple benchmark datasets (DIV2K, LSDIR, etc.) and different degradation scenarios, alleviating the problem of \"mismatch between perceptual effects and objective distortion\";  fourth, it supports ultra-high resolutions: leveraging the fixed-length attention mechanism of in-token alignment, it natively enables direct inference at QHD (2560×1440) resolution, achieves 4K/8K/12K resolution restoration through tile-consistent expansion, and verifies this capability with the 12K restoration of the classical scroll painting Along the River During the Qingming Festival; fifth, it exhibits strong task generalization: the same backbone network and training pipeline can seamlessly extend from super-resolution tasks to image colorization tasks without re-designing the model, demonstrating excellent cross-task transfer performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The proposed \"intra-token alignment + RFM\" restoration paradigm differs significantly from existing \"iterative denoising\" or \"sequence-level conditional fusion\" methods, representing a novel technical route.\nThe DLG mechanism cleverly leverages the model’s native text pathway, avoiding dependencies on external models and additional branch overhead. It balances \"conditional constraint strength\" and \"computational efficiency\" with an innovative design.\nThe experimental design is rigorous, covering \"synthetic + real-world,\" \"low-resolution + ultra-high-resolution,\" and \"single-task + cross-task\" scenarios.  Ablation experiments (on DLG components, generation modes, and token alignment) are comprehensive, ensuring highly credible results.\nThe author demonstrate the first 12K restoration of the classical scroll painting Along the River During the Qingming Festival. Ultra-high-resolution scalability provides an efficient solution for 12K restoration, with significant application value in fields such as historical artifact restoration."}, "weaknesses": {"value": "The paper claims that in-token fusion and DLG “bridge the perception-distortion gap” and “stabilize conditioning,” yet no clear mathematical or causal analysis is provided to substantiate these effects.\nThe “Direct Low-Quality Guidance” module (DLG) is described as fusing embeddings of the degraded image and system prompt — yet it remains unclear what the “system prompt” represents, or how it differs across tasks (SR, denoising, colorization)."}, "questions": {"value": "In Figure 3, the person in the LR input appears to have no two front teeth, yet the restored image incorrectly adds two front teeth. Does this indicate a lack of fidelity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "15cX2AHxI7", "forum": "9D5ryI7now", "replyto": "9D5ryI7now", "signatures": ["ICLR.cc/2026/Conference/Submission987/Reviewer_wYA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission987/Reviewer_wYA2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734815250, "cdate": 1761734815250, "tmdate": 1762915652941, "mdate": 1762915652941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an image restoration framework based on Diffusion Transformer (DiT) and Rectified Flow Matching (RFM). The authors define their approach as learning a conditional velocity field that transports pure noise directly to a clean image. The framework is a combination of three existing components: 1) RFM as the generative paradigm; 2) \"In-Token Learning\" (ITL), which is channel-wise concatenation of conditions; and 3) \"Direct Low-Quality Guidance\" (DLG), a multi-modal cross-attention injection mechanism. \n\nThe authors claim SOTA performance on multiple benchmarks, a reduction in the perception-distortion gap, and scalability to 12K resolution inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good Empirical Results: The proposed system achieves (parts of) SOTA or highly competitive performance on several synthetic image restoration benchmarks.\n2. Promising Scalability: The method is successfully demonstrated on ultra-high-resolution 12K imagery, which is a notable engineering achievement. The underlying channel-wise (ITL) approach is indeed more computationally scalable than sequence-wise concat."}, "weaknesses": {"value": "1.  Overstated Contribution: \n    * The paper's core flaw is over-packaging. The central contribution \"In-Token Learning\" is a standard channel-wise concatenation (where is \"Learning\"?), and \"DLG\" is a standard multi-modal attention injection. The paper re-brands these existing techniques with new nomenclature, supported by a trivial complexity analysis (Sec. 3.8), which obscures a lack of methodological novelty. \n    * And also, there is a disconnect between the paper's central argument and its methods. The claim of a new paradigm (\"transporting pure noise directly to a clean image\") is merely a description of the underlying RFM. The paper fails to articulate the necessary link between this high-level concept and the specific combination of the ITL and DLG mechanisms (at least from my view). Please clarify this.\n2.  Incremental System-Building Work: This work is like an incremental systems-building project, combining three existing components (RFM, channel-concat, cross-attention) well. However, it lacks a strong **justification for the synergy** of this specific combination (i.e., \"Why these three?\") and offers no new fundamental principles for representation learning, which is the focus of ICLR.\n3.  Not Good Real-World Generalization: Despite strong synthetic results, the method significantly **underperforms** all established baselines on **all metrics** on the real-world RealLQ250 dataset. The defense of this as a \"conservative\" strategy (Sec. 4.2) is unconvincing and more likely masks overfitting to the synthetic degradation model $\\mathcal{D}_{\\phi}$."}, "questions": {"value": "1. Contradictory Ablation: In the Table 4 ablation, you show that \"anchoring to the flawed LQ input\" (e.g., \"Denoise 0.9\") degrades performance. Yet, the core ITL method strongly \"anchors\" to the flawed LQ latent $y$ at *every* step via $h_t = [x_t; y]$. Please clarify the fundamental difference between these two settings and explain why ITL is not negatively affected.\n\n2. Justification for 'Conservatism': The method underperforms on the RealLQ250 dataset, which you attribute to a \"reliability-oriented strategy\" that \"conservatively\" avoids hallucination. This justification appears to be a post-hoc rationalization for poor generalization, likely due to overfitting on your synthetic degradation model $\\mathcal{D}_{\\phi}$. What concrete evidence can you provide to support that this is a beneficial 'conservative' behavior rather than simply a model failure on out-of-distribution real-world data?\n\n3. Necessity of DLG's Prompt Content: How critical is the *semantic content* of the \"system prompt\" in DLG? What is the performance if $e_t$ is replaced with a null-text or random embedding? The current ablation (Table 3) only removes $e_t$ entirely, which fails to decouple its presence from its semantic meaning. I have doubts about the contribution/effect of the specific text content itself (as shown in Figure 1, prompts like \"Produce clean, sharp, noise-free images...\" seem empty and lack information). Please do a Placebo Experiment ([empty text + LQ] vs [text + LQ]) within the same Text Encoder structure, to see the benefit come from semantic guidance or from an **unexplored architectural bias**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GHuAxGIb9B", "forum": "9D5ryI7now", "replyto": "9D5ryI7now", "signatures": ["ICLR.cc/2026/Conference/Submission987/Reviewer_uxXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission987/Reviewer_uxXd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975689208, "cdate": 1761975689208, "tmdate": 1762915652753, "mdate": 1762915652753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}