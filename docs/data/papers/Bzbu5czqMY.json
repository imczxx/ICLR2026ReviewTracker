{"id": "Bzbu5czqMY", "number": 1111, "cdate": 1756842245880, "mdate": 1763650852229, "content": {"title": "IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning", "abstract": "Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. We propose IB-Inspired Normalization (IBNorm), a simple yet powerful family of methods grounded in the Information Bottleneck principle. IBNorm introduces bounded compression operations that encourage embeddings to preserve predictive information while suppressing nuisance variability, yielding more informative representations while retaining the stability and compatibility of standard normalization. Theoretically, we prove that IBNorm achieves a higher IB value and tighter generalization bounds than variance-centric methods. Empirically, IBNorm outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual information analysis confirming superior information bottleneck behavior. Code will be released publicly.", "tldr": "", "keywords": ["machine learning", "deep learning", "activation normalization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10a932e466673caa40e04086251b1d4a0b450cc2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IBNorm, a novel normalization technique for deep learning inspired by the Information Bottleneck (IB) principle. It argues that common methods like BatchNorm and LayerNorm primarily control mean and variance, neglecting the informational content of representations. IBNorm incorporates a non-linear \"compression\" step before standardization, aiming to shape activations to preserve task-relevant information while suppressing nuisance variability, thus aligning better with IB objectives. The authors claim theoretical guarantees that IBNorm achieves higher IB values and tighter generalization bounds than standard methods. Empirically, they demonstrate performance improvements by integrating IBNorm into various language models (LLaMA, GPT-2) and vision models (ResNet, ViT) across several benchmarks, outperforming standard normalization techniques and the recent NormalNorm method. The paper also provides empirical analysis using mutual information estimators to support the claim that IBNorm leads to representations with better IB characteristics."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper clearly identifies a potential limitation of existing normalization methods (focusing solely on moments) and proposes addressing it through the principled lens of the Information Bottleneck.\n* The method is evaluated across different domains (NLP, CV), architectures (Transformers, CNNs), and model scales, providing a broad assessment of its practical performance.\n* IBNorm seems to outperform standard baselines (LN, BN, RMSNorm) and a relevant recent competitor (NormalNorm) across most reported experiments, suggesting potential practical utility."}, "weaknesses": {"value": "The paper's primary weaknesses lie in the lack of sound theoretical justification for its central claims and unfair empirical comparisons in the vision domain.\n\n1.  **Unfair Empirical Comparison in Vision Experiments:** The claimed empirical superiority in vision models (Table 3) is based on an unfair comparison. Appendix F.2 reveals that the authors used different hyperparameters (learning rates, weight decays) for their proposed IBNorm method compared to the baseline methods (including BatchNorm, LayerNorm, and NormalNorm). For example, in the ResNet50/ImageNet experiment, IBNorm used a learning rate 10x smaller and weight decay 10x larger than the baselines. Claiming superior performance when the proposed method received potentially more favorable tuning invalidates the comparison. This experimental flaw makes the vision results unsubstantiated.\n2.  **Invalid Proof for Generalization Bound (Corollary 2):** The proof attempts to justify a tighter generalization bound by invoking Theorem 2 from Kawaguchi et al. (2023). This invocation is flawed for multiple reasons:\n    * Misrepresentation of the Bound: The paper incorrectly presents the bound from Kawaguchi et al. It omits a critical term, $I(\\phi\\_l^S; S)$, which represents the mutual information between the learned encoder parameters and the training data, capturing model complexity/overfitting. The actual bound depends on *both* $ I(X; Z\\_l^s|Y)$ (representation complexity) *and* $I(\\phi_l^S; S)$ (model complexity).\n    * Incorrect Structure: The paper incorrectly changes the bound's structure from a minimum over layers ($\\min_l$) in the source to a sum over layers ($\\sum_l$).\n    * Unchecked Assumptions: The paper fails to verify that the assumptions required by the Kawaguchi et al. theorem (e.g., regarding data generation, finite spaces) hold in their setting.\n    * This constitutes a logical flaw because the cited theorem is fundamentally misrepresented and misapplied, invalidating the proof and the claim of a tighter generalization bound.\n3.  Invalid Proof for Higher IB Value (Theorem 1): The proof that IBNorm achieves a higher IB value (Theorem 1, Appendix E.1-E.4) relies on applying mathematical bounds (via Lemma 3, citing Shamir et al. 2010) derived for discrete random variables directly to the continuous activations of neural networks.\n    * The theoretical tools used (concentration bounds based on variances over finite sample spaces from Shamir et al. 2010) are not generally applicable to continuous distributions without significant adaptation or justification, which is not provided. Furthermore, the subsequent Lemma 5 contains questionable arguments regarding differential entropy and appears conditional on the compression hyperparameter lambda being large, contradicting the main theorem statement.\n    * While potentially fixable with different bounding techniques appropriate for continuous variables, the current proof relies on fundamentally misapplied tools and contains questionable steps, rendering it invalid as presented. This is a significant flaw and an alternative argument or major restructuring appears necessary.\n4.  Flawed Logic Connecting IB Value to Generalization: The argument in Section 4.3 linking the claimed higher IB value (Theorem 1) to the claimed tighter generalization (Corollary 2) is logically flawed. It relies on the invalid proofs for both Theorem 1 and Corollary 2. Moreover, it *ignores* the necessary $I(\\phi\\_l^S; S)$ term from the actual Kawaguchi et al. bound, failing to provide any analysis of how IBNorm might impact model complexity, which is essential for assessing the overall generalization bound. \n5. Lack of rigor in LLM gains reporting: While the LLM results appear promising, the magnitude of gains over baselines (especially the internally-run NormalNorm) is surprisingly large. Crucially, these results are reported based on single runs (there is no mention of runs in the main paper or appendix), lacking error bars or statistical significance testing, which makes it difficult to assess their reliability and may potentially overstate the gains."}, "questions": {"value": "-  Theorem 1 Proof: Can the authors provide a corrected proof for Theorem 1 that uses information-theoretic bounds applicable to continuous random variables, rather than relying on the discrete-variable bounds from Shamir et al. (2010)? Does the claim hold for all lambda >= 1, or only under specific conditions (e.g., large lambda) as suggested by the current Appendix E.4?\n- Corollary 2 Proof: Can the authors provide a valid proof for Corollary 2  and address issues raised in my review above?\n- Can the authors report the LLM results (Tables 1, 2) averaged over multiple random seeds (e.g., 3 or 5), including standard errors or confidence intervals, to allow for assessment of statistical significance, and more generally provide more details on the NormalNorm baseline implementation and tuning for the LLM tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pMEpMygR1F", "forum": "Bzbu5czqMY", "replyto": "Bzbu5czqMY", "signatures": ["ICLR.cc/2026/Conference/Submission1111/Reviewer_9sfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1111/Reviewer_9sfx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567743692, "cdate": 1761567743692, "tmdate": 1762915681283, "mdate": 1762915681283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new normalization method called IBNorm, which introduces an information bottleneck  constraint into the normalization process to balance feature information preservation and noise suppression.  Unlike traditional normalization methods such as LayerNorm and RMSNorm that standardize feature  distributions by mean and variance, IBNorm explicitly regulates the mutual information between features and  task-relevant representations. By reformulating normalization from an information-theoretic perspective, the  method aims to allow adaptive information flow control during feature extraction. Experiments are conducted on multiple language models (LLaMA-60M/130M/350M and GPT-2-12M/355M).  Results show that IBNorm achieves better average performance on Leaderboard II, particularly on several mid scale models, even though some concerns on experimental setup and analysis/explanation remains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides an theoretical viewpoint by integrating  the information bottleneck principle into normalization design,  even though inspired from the previous paper (NormalNorm, ICML 2025), contributing to a deeper understanding  of how normalization can regulate information flow in representation learning. \n2. The information-theoretic reformulation of normalization is  well motivated, and the overall structure and derivation are easy to follow.\n3. Empirical validation on multiple models – The method is evaluated on several LLaMA and GPT-2  variants, covering different model sizes and task categories. In several cases (e.g., LLaMA-350M and  GPT-2-355M on Leaderboard II), IBNorm exhibits performance improvements over LayerNorm and  RMSNorm."}, "weaknesses": {"value": "1. The overall idea and contribution is incremental, especially the writing is mostly following the presentation of the previous paper (NormalNorm, ICML 2025), e.g., the Information Bottleneck idea of normalization and the description of algorithm (NormalNorm, ICML 2025). Besides, this paper misses a bunch of papers to discuss and  compare.,  e.g., the whitening method in normal supervised learning [1,2]  and self-superverses learning [3,4]. I think this paper should compare to IterNorm [2] method in experiments, which is in native to obtain the same functionality as the paper and (NormalNorm) and has a similar formulation in algorithm. \n\n2. I have concerns on the experiments.\n\n   (1) The hyper-parameters  $\\lambda$ is  sensitive. This paper  uses hyper-parameters search in several experiments, and the results on reported over different hyper-parameters (e.g., $\\lambda$) on different methods/tasks. This remains the concerns on the practicability. \n\n   (2) **Experimental setups.**  In the results of ResNet on ImageNet of Table 3, it is weired the BatchNorm has only 76.15% accuracy when training over 200 epochs (From the description in  supplementary materials). It is true that the basic ResNet (BatchNorm) training (100 epochs) has a performance around 76.15%.  However, It is easy to obtain 77%+ accuray when training with 200 epochs from the results reported in previous work. Does this results are cited from previous paper or reproduced?\n\n   (2) **Inconsistent experimental results**. IBNorm does not consistently outperform baselines across tasks. In Table 2, it performs worse on BBH,  GPQA, and MMLU-PRO, indicating limited generalization ability across reasoning and knowledge intensive benchmarks. The paper does not analyze or discuss these cases, weakening its empirical  justification.\n\n   (3) **Questionable numerical gain calculations**. The reported relative improvements in Section 5.1 appear inconsistent with actual table values. For example, the authors claim: “On LLaMA-350M, IBNorm-L achieves 0.2140 (Leaderboard II), with gains of 6.84% over LN (0.2003) and  9.51% over RMSNorm (0.2010).” However, the value in Table 1 is 0.2116, yielding gains of only 5.64% and 5.27%, respectively. . This discrepancy suggests either computational inconsistency, which should be clarified. \n\n   (4) **Lack of statistical significance analysis** .Results are reported as single averages over tasks without standard deviation, variance, or confidence  intervals. Given the heterogeneous scoring scales of tasks in Leaderboard II, it is difficult to determine  whether the improvements are statistically meaningful. Repeated runs and reporting of mean ± std are  strongly recommended. \n\n  \n   (5).**Over-claiming in interpretation**. The paper frequently uses strong statements such as “consistently surpasses variance-centric methods”  and “achieves significant improvement.” However, given that some improvements are minor and  performance decreases occur in several tasks, such claims are not fully supported by the results and  should be moderated.\n\n\n**Ref:**\n\n1.Decorrelated Batch Normalization. CVPR, 2018.\n\n2.Iterative Normalization: Beyond Standardization towards Efficient Whitening. CVPR, 2019.\n\n3.On feature decorrelation in self-supervised learning. ICCV, 2021.\n\n4.Modulate Your Spectrum in Self-Supervised Learning. ICLR, 2024."}, "questions": {"value": "1. Concerns 2. (2) on the experimental setups. \n2. Clarification of improvement calculation How are the reported performance gains (e.g., “+6.84%” and “+9.51%”) computed?  Please provide the  explicit formula. \n\n3.  Failure case explanation Why does IBNorm perform worse on BBH, GPQA, and MMLU-PRO? Could this be related to task  characteristics (e.g., reasoning vs. factual retrieval) or model-scale sensitivity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R0uFe9TEmR", "forum": "Bzbu5czqMY", "replyto": "Bzbu5czqMY", "signatures": ["ICLR.cc/2026/Conference/Submission1111/Reviewer_P5Gw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1111/Reviewer_P5Gw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623691366, "cdate": 1761623691366, "tmdate": 1762915681125, "mdate": 1762915681125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes IBNorm, a normalization method inspired by the Information Bottleneck (IB) principle. Unlike traditional variance-centric normalization techniques (such as BatchNorm, LayerNorm), IBNorm introduces a compression operation to preserve task-relevant information while suppressing irrelevant variability in activations. The paper presents both theoretical guarantees and empirical results, showing that IBNorm outperforms conventional methods across language and vision models. The authors argue that IBNorm provides better generalization by enhancing the quality of learned representations, making it theoretically sound and empirically effective.\n\nWhile the paper's contributions are valuable, particularly in terms of theoretical guarantees and empirical validation, it lacks sufficient exploration of the novelty of its compression functions compared to existing methods. Moreover, a deeper comparison with other IB-inspired methods and clearer discussions on practical aspects (such as mutual information estimation) would further strengthen the paper. Overall, the paper is a solid contribution but requires some refinements to fully justify the novelty and unique benefits of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an application of the information bottleneck to normalization. By integrating compression operations into the normalization process, the paper presents a significant departure from traditional methods that focus solely on statistical normalization (mean and variance).\n2. The paper presents a clear theoretical foundation and solid empirical validation. The authors present rigorous proofs that IBNorm achieves a higher IB value compared to variance-centric methods and demonstrate its better generalization performance. The extensive experiments across multiple architectures (LLaMA, GPT-2, ResNet, ViT) strengthen the paper's claims and show the practical benefits of IBNorm.\n3. The paper is clearly written, with a logical flow that introduces the problem, motivates the new approach, and carefully explains the theoretical and experimental results. The methodology section is particularly well-explained, making complex concepts like the IB principle and the compression operation accessible."}, "weaknesses": {"value": "1. Limited contribution in context of existing IB works. In most work based on the Information Bottleneck principle, compression operations (such as nonlinear functions like tanh [1] and kernel-based function [2], or explicit compression losses like VIB [3] and SPC [4]) are often applied alongside normalization operations within neural networks. The paper claims to introduce an IB perspective to normalization, but it essentially introduces an additional compression operation into networks already using normalization. This contribution appears to be limited. While IBNorm introduces new compression functions and combinations with normalization layers, the paper does not sufficiently explore whether these combinations provide any significant advantage over existing work. The novelty of combining IB with normalization is not fully established because similar operations are already in use.\n\n- [1] On the Information Bottleneck Theory of Deep Learning. IEEE Information Theory Workshop 2015.\n- [2] Nonlinear information bottleneck. Entropy 2019.\n- [3] Deep variational information bottleneck. ICLR 2017.\n- [4] Structured Probabilistic Coding. ACL 2024.\n\n2.  Fail to justify the unique advantages of the compression functions in IBNorm. The proposed compression functions (IBNorm-S, IBNorm-L, and IBNorm-T) are based on nonlinear functions that exhibit compression behavior, specifically functions like tanh. A prior work [1] already shows that saturating nonlinearities like tanh induce compression in neural activations from the perspective of the information bottleneck. The compression operations introduced in IBNorm are thus special cases of these well-known nonlinearities. The paper fails to sufficiently justify the unique advantages of these specific functions over general nonlinear functions like tanh. Further empirical or theoretical validation of the superiority of these three heuristic functions would be helpful to establish their distinct contribution.\n\n\n3. Comparisons with existing information bottleneck methods. While the paper proves that IBNorm achieves a higher IB value than variance-centric methods like LayerNorm and BatchNorm, it does not compare IBNorm directly with other IB-inspired methods such as explicit compression losses (e.g., VIB loss, MINE loss [5] ) or other implicit compression functions like tanh. Since IBNorm itself uses compression as its central mechanism, comparing it to existing IB methods is crucial to validate whether it achieves a truly higher IB value and whether this additional compression operation provides unique advantages in improving generalization and representation learning.\n\n- [5] Mutual information neural estimation. ICML 2018.\n\n4. Lack of references for IB-guided normalization objective. The IB-guided normalization objective described between lines 186-195 is essentially a layer-wise IB loss, which is discussed in the paper [1] on information bottleneck and layer characteristics. The paper lacks references to this foundational work and does not highlight how its approach differs or extends the existing layer-wise IB loss formulation. Including these references would strengthen the theoretical foundation of the paper.\n\n5. Inaccurate statement about mutual information estimation. The paper states in lines 203-205 that \"Directly optimizing Eqn.(5) is infeasible due to costly mutual information estimation over the unknown joint distribution.\" This description is not entirely accurate. While variational estimation of mutual information does require assumptions about the distribution, there are alternative methods (e.g., MINE, using additional networks to estimate mutual information) that do not require explicit assumptions about the distribution. Clarifying this point and discussing these alternatives would improve the technical accuracy of the paper."}, "questions": {"value": "1. Clarify the novelty beyond existing IB-based compression approaches with normalization. \n- Many prior works already combine Information Bottleneck (IB) principles with normalization or compression mechanisms, either implicitly (via nonlinear activations like tanh) or explicitly (via compression losses such as VIB).\n- Could the authors clarify what fundamentally differentiates IBNorm from these existing approaches beyond introducing new compression functions (IBNorm-S/L/T)?\n- Specifically, what are the unique advantages (theoretical or empirical) of IBNorm’s bounded compression functions compared to standard nonlinearities that already induce information compression?\n\n2. Justification of the compression functions (IBNorm-S, IBNorm-L, IBNorm-T).\n- The proposed heuristic functions are variants of saturating nonlinearities that inherently compress information in the work [1].\n- Could the authors provide a quantitative analysis or visualization showing how these three functions differ in their information compression characteristics (e.g., in terms of mutual information, entropy, or Fisher information)?\n- Additionally, what design principles guided the specific forms of IBNorm-S/L/T? Were they chosen empirically or derived from any theoretical insight?\n\n3. Comparisons with other IB variants and explicit compression losses. Since IBNorm’s main contribution is introducing an IB-inspired compression step, it would be very informative to compare it against explicit IB-based training methods such as VIB or MINE loss applied alongside normalization. Could the authors comment on whether IBNorm would still outperform such methods in terms of the achieved IB value or generalization?\n\n4. Impact of λ across models and tasks. The relationship between λ and model performance may vary across architectures and data modalities. Could the authors provide more analysis (e.g., λ-sensitivity curves or cross-task validation) to better understand how λ affects representation informativeness and generalization?\n\n\n5. Scalability to larger models and practical deployment. How does IBNorm scale to larger LLMs (e.g., 7B–70B parameters)? Are there computational trade-offs (e.g., increased training time, numerical stability issues) when applying IBNorm at scale? Clarifying these practical considerations would be useful for understanding IBNorm’s potential in real-world.\n\n6. Under the IB framework, it is difficult to determine in advance how close we are to optimal compression, which can easily lead to the over-compression or under-compression issue of representations [4,6]. Has this work considered the problems in the compression process, which may have significant implications for layer-wise IB?\n- [6] The conditional entropy bottleneck. Entropy 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qsVQyJ7hGW", "forum": "Bzbu5czqMY", "replyto": "Bzbu5czqMY", "signatures": ["ICLR.cc/2026/Conference/Submission1111/Reviewer_mLmV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1111/Reviewer_mLmV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992884073, "cdate": 1761992884073, "tmdate": 1762915680857, "mdate": 1762915680857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}