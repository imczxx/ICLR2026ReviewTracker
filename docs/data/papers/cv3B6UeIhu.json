{"id": "cv3B6UeIhu", "number": 3690, "cdate": 1757497187281, "mdate": 1759898074900, "content": {"title": "Discriminator-Guided Diffusion for Generating Large Directed and Undirected Graphs", "abstract": "Synthesizing large-scale, realistic (directed) graphs is essential for modeling complex relationships, detecting anomalies, and simulating scenarios where real-world data is sparse, sensitive, or unavailable. While diffusion-based graph generators have shown promising results on small-scale graphs, such as molecular structures, existing models face three key challenges: (1) quadratic time complexity, making them impractical for large graphs; (2) a narrow focus on either structure or node and edge features; and (3) limited exploration of directed graph generation. In this work, we propose \\textbf{DGDGL}: \\textit{Discriminator-Guided Diffusion for Generating Large Directed and Undirected Graphs}. Our approach unifies structure and feature generation for both nodes and edges within a single framework, supporting both directed and undirected graphs. Using graph neural networks and a novel discriminator module, DGDGL guides the denoising process through gradient-based feedback, improving the quality of generated graphs while maintaining linear time complexity with respect to the number of edges. This makes our method scalable to large graphs. We evaluate DGDGL on diverse datasets, including undirected citation networks and directed financial graphs. The results show that our method outperforms existing models with quadratic time complexity. By combining comprehensive support for both directed and undirected graphs, including feature generation for nodes and edges with efficient scalability, DGDGL shows potential for broad use in complex graph-based systems.", "tldr": "", "keywords": ["Graph generative models", "Diffusion generative models", "Graph neural networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c593d6636bdc06de01dde2acbfa905bcc01b299.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a scalable MPNN-based diffusion model for graph generation, combined with a discriminator for guidance, and extended to directed graphs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed discriminator design is an interesting idea in graph generative modeling.\n2. The edge deletion strategy and the use of MPNNs with minimal edge number guarantee are reasonable and practically useful."}, "weaknesses": {"value": "First, the paper motivation is not very clear. There are mainly 2 (actually 3) lines: one about scalability, one about guidance, and one about directed graph generation. However, they are not logically dependent on each other.\n- Specifically, scalability is achieved by using GINs and claimed to be comparable to previous quadratic methods, but there is no comparison with important quadratic models such as DiGress, CatFlow, or DeFoG.\n- The directed graph generation is a simple extension with no further justification, and current diffusion models can also support directed graph generation in a straightforward manner, which weakens the necessity and contribution of this part.\n\nSecond, from a presentation perspective, there are notations like $L_L$(Corollary 3.2), which is not very readable. Beyond that, in Table 3, there seem to be some mistakes. For instance, GraphRNN is better on Deg (Citeseer), Def (Elliptic), but the results of DGDGL (ours) are bolded. The columns including node features distance and edge feature distance that have no values. The 3 histograms in the paper are presented in 3 different formats (A2 A3 A4). The paper presentation is not complete.\n\nThird, experimentally, as mentioned in the first point as well, the paper claims: “The results show that our method outperforms existing models with quadratic time complexity.” in the abstract, but it fails to demonstrate this in the results since lots of important models are not compared with on common graph benchmarks with smaller sizes. Even for scalable methods, many strong and relevant baselines are also not included such as:\n- Efficient and Scalable Graph Generation through Iterative Local Expansion\n- HIGEN: Hierarchical Graph Generative Networks\n- Graph Generation with K2-Trees"}, "questions": {"value": "The paper claims to outperform quadratic-time methods while avoiding dropping too many edges during message passing. Beyond the fact that this is not sufficiently supported by experimental evidence. It is also unclear why the performance should be comparable, given the known expressivity gap between graph transformers and MPNNs. More clarification or theoretical justification would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxM6D0DuMA", "forum": "cv3B6UeIhu", "replyto": "cv3B6UeIhu", "signatures": ["ICLR.cc/2026/Conference/Submission3690/Reviewer_QS4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3690/Reviewer_QS4y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515487727, "cdate": 1761515487727, "tmdate": 1762916924614, "mdate": 1762916924614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DGDGL, a model that utilizes discriminator-guided diffusion for generating large directed and Undirected Graphs. Its diffusion-based generator employs a message-passing graph neural network to synthesize node and edge features alongside structure, also lowering complexity from quadratic in the number of nodes to linear in the number of edges. The forward noising process involves iteratively removing edges from the input clean graph up to a theoretically argued minimum edge density threshold required for maintaining the effectiveness of message-passing. During sampling, the model iteratively denoises a corrupted graph into a clean one through iterative refinement and the removal of noisy edges. The discriminator's gradients guide the generator in denoising features and structure during sampling, and the authors provide theoretical support for the utility of the approach. Experimental results show that DDGL performs similarly to or better than various baselines on a set of large single-graph datasets spanning directed/undirected connectivity, as well as node/edge feature combinations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method enables the use of classic message passing graph neural network layers while achieving comparable performance to baselines with more complex network architectures.\n- The concept of boosting sampling performance with a GAN-inspired discriminator is reasonable.\n- The authors provide a theoretical discussion on specific properties, like the requirement for a minimum edge density to ensure GNNs operate effectively."}, "weaknesses": {"value": "-The paper does not include any empirical results on the computation time of the proposed method relative to the baselines to supplement the claims of asymptotic complexity reduction.\n-The evaluation could benefit from considering additional MMD measurements (e.g., spectre, orbit) common in other graph generation works (including baselines).\n- Having at least one dataset composed of more than one graph could provide further insight into the model's generalization abilities, and a discussion on possible concerns about output diversity when integrating the discriminator in sampling.\n- Certain notation elements and methodology details could benefit from some slight refinement and clarification (see questions)."}, "questions": {"value": "- Why does Table 1 not highlight any previous works that support node/edge classes, such as DiGress (mentioned elsewhere in the paper)? While DiGress does not focus on scalability, D-VAE also does not, yet it is present in the overview.\n- In Table 1, the stated time complexity for DGDGL is O(|E|), which appears to be the per-time-step figure, rather than the end-to-end complexity of O(T|E|). For reference, in Algorithm 1, $f_\\theta$, which has O(|E|) complexity per Appendix A3, is called T times for sampling. Conversely, for EDGE, the Table 1 complexity of O(T max(E, K^2)) appears to be end-to-end. Could the authors clarify the situation?\n- Why is the value under “edge features” for Bitcoin-OTC set to 3 in Table 2, if the possible values are integers from -10 to 10?\n- In Equation (A2), wouldn't the current notation imply that the node feature representation of each node gets concatenated to itself, which seems rather unhelpful?\n- Given that the forward noising process is discrete, as it (mostly) removes edges from the clean graph, the fully-noised edges E_T should also form a discrete structure. However, in Algorithm 1 (and elsewhere in the manuscript), E_T gets sampled from a distribution N(...), which is generally associated with a continuous normal distribution. Is it correct that the distribution still represents a discrete prior, despite the notation?\n- Similarly, if the forward process injects noise into a graph by mainly removing edges up to a minimum edge density $\\rho^\\text{min}_T$, and E_T gets initially sampled according to that density. However, the phrasing in line 253 and Equation (4) seems to imply that the sampling process also removes edges, which would further lower edge density. While the experiments show that the actual generation achieves average degrees comparable to, and often higher than, the clean graph, could the authors clarify the intent behind the aforementioned phrasing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jdMs8EAaLm", "forum": "cv3B6UeIhu", "replyto": "cv3B6UeIhu", "signatures": ["ICLR.cc/2026/Conference/Submission3690/Reviewer_m3WS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3690/Reviewer_m3WS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760293279, "cdate": 1761760293279, "tmdate": 1762916924254, "mdate": 1762916924254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a discriminator-guided diffusion framework for generating large graphs, both directed and undirected. The method introduces noise through edge deletion and feature perturbation, then uses a GNN-based denoising process to iteratively reconstruct graphs. A discriminator is added to the reverse process, providing gradient feedback meant to steer generation toward more “realistic” structures and attributes. The authors present theoretical conditions for maintaining graph connectivity under edge deletion and argue that discriminator guidance aligns the denoising with distributional objectives. Overall, the framework is positioned as a unified and scalable setup for graph generation, with an additional emphasis on anomaly detection as a downstream use case."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The approach seems to yield good emprical results.\n\n- Using the discriminator to perform anomaly detection is interesting. I think there are a couple of papers for anomaly detection using diffusion models, it could be nice to add a comparison with those methods."}, "weaknesses": {"value": "The presentation of the method is rather unclear : \n\n- First, you say that you approximate $p(G_{t-1} | G_t)$ using your reverse kernel. Yet, you claim this kernel is trained to reconstruct $G_0$ from $G_T$. No explanation is provided.\n\n- Then, you refer $s_t$ and $\\hat{e}_t$ as edge scores and noise prediction of the edge scores without further explanations. It's completely unclear what those variables are. \n\n- It's very unclear what the denoising process consists in. What I gathered from the current manuscript is that the noising process removes edges, and denoising process iteratively \"revise\" edges from a randomly sampled graph. Does it allow to add edges or simply to remove some ?\n\n- Proposition 3.1 is kind of trivial, as well as Corollary 3.2. It gives the impression that the authors included those statements for the sake of \"more theory\".\n\n- In table 3, DGDGL is bolded for Degree on the Elliptic dataset, even though it is outperformed by EDGE and ARROW-Diff.\n\n- Tables are sometimes hard to read. Since CPL and PLE are better when close to the ground truth, why not reporting a distance to the ground truth ? Also, consider including bolding in Table 5."}, "questions": {"value": "- Noising in diffusion models is made under the assumption that different dimensions (e.g. tokens in text or pixels in images) are noised independently. Your operator breaks that assumption since it looks at density of the whole graph.\n\n- See weaknesses concerning the denoising process.\n\n- How does your approache reduce computational complexity in practice ? Does it treat graphs as fully connected \n\n- Considering that you generate 32 samples, it would be nice to include error bars for your results. \n\n- What is the practical relevance of chosen benchmarks ? What would be the real-world of generating such networks ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nQC1XMYdUp", "forum": "cv3B6UeIhu", "replyto": "cv3B6UeIhu", "signatures": ["ICLR.cc/2026/Conference/Submission3690/Reviewer_VH9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3690/Reviewer_VH9p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837925900, "cdate": 1761837925900, "tmdate": 1762916923998, "mdate": 1762916923998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DGDGL, a diffusion-based generative model for creating large directed and undirected graphs with node and edge features. The key contribution is achieving linear O(|E|) complexity by injecting structural noise through edge deletion, combined with a GNN-based discriminator that provides gradient guidance during the reverse denoising process. The authors derive theoretical bounds on minimum edge density required for reconstruction. Experiments on citation networks and Bitcoin trust networks show competitive performance against baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method utilizes a discriminator as guidance for graph generation. Both the results and ablation studies demonstrate that the proposed guidance term is effective.\n2. The authors provide theoretical guarantees for edge density and GNN effectiveness.\n3. The proposed method is lightweight. The authors employ only a 5-layer and 3-layer GNN architecture, yet achieve results comparable to existing baselines."}, "weaknesses": {"value": "1. Although the proposed method demonstrates better computational complexity than baselines, the origin of the baseline complexity is unclear (i.e., whether it is cited from the original papers or computed by the authors). Additionally, no experimental results are provided to support the overhead comparison.\n2. Discriminator guidance is not a novel technique for diffusion models, as it has been applied in image domain for a long time.\n3. The mathematical notation could be improved, especially for the discrete components."}, "questions": {"value": "1. How were the computational complexities of other baselines in Table 1 obtained? Were they taken from the original references or computed by the authors? If they were extracted from the references, this should be explicitly stated. If they were computed by the authors, the derivation process should be documented.\n2. For EDGE, the number of generation steps T is included in the complexity analysis. However, why is the generation step count not included in the proposed method's complexity? Could you explain why the complexity is O(|E|) instead of O(T|E|)?\n3. For previously mentioned methods like DiGress that are not included in the baselines, what are the specific reasons they cannot generate graphs larger than 300 nodes? Could these methods be scaled to large-scale graphs in the implementation presented in this paper?\n4. Table 5 shows that increasing diffusion steps from 256 to 1024 leads to worse performance. This contradicts the typical behavior of diffusion models, where more steps generally lead to higher fidelity outputs. Does this indicate an instability in your reverse process, or does it suggest that the discriminator's guidance degrades over longer sampling chains?\n5. Can the proposed method scale to larger graphs, such as those in OGB-LSC [1]?\n\n[1] Hu, Weihua, et al. \"Ogb-lsc: A large-scale challenge for machine learning on graphs.\" arXiv preprint arXiv:2103.09430 (2021)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LHTwQGQBV6", "forum": "cv3B6UeIhu", "replyto": "cv3B6UeIhu", "signatures": ["ICLR.cc/2026/Conference/Submission3690/Reviewer_6NBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3690/Reviewer_6NBZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981958386, "cdate": 1761981958386, "tmdate": 1762916923744, "mdate": 1762916923744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a diffusion model framework for graph generation consisting of\n\n- a noise process constructed out of edge deletion (up to a sparsity threshold, uniformly adding edges back in once that threshold is hit) and noise injection\n    \n- co-training a discriminator that is used as a classifier guidance, distinguishing “realistic” from “nonrealistic” graphs and improving generation\n    \n\nThe method is evaluated for graph generation on Citeseer,bitcoint-otc, elliptic,bitcoint-alpha,cora and pubmed datasets showing poimising results\n\nApplies the idea of the above to graphs improves sampling, it’s good"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "clarity: the paper is well written and presents the ideas clearly\n\nsignfificance: the method shows overall promising results\n\noriginality: the paper is an (independent?) reinvention of [https://arxiv.org/abs/2211.17091](https://arxiv.org/abs/2211.17091) applied to graphs, which is a non-trivial step of novelty\n\nquality: the paper is overall well written and evaluations make sense, with some caveats detailed below"}, "weaknesses": {"value": "- the paper is missing a related work [https://arxiv.org/abs/2211.17091](https://arxiv.org/abs/2211.17091) which should be added\n    \n- some of the evaluations metrics are very close, please train multiple models with more seeds and compute  CIs\n    \n- graphRNN is a weak baseline, should include GRAN and maybe  Bigg (dai et al)\n    \n- at least on the smaller datasets, should  to do isomorphism checks against the training dataset to guard against memorization\n    \n- figure 4 should do some form of rigorous evaluation (e.g. computing wasserstein distance between the distribution and the training distribution and comparing against  a gaussian/uniform prior, some form of statistical proximity ranking test). other figures that are making statistical points should be characterized like this as well"}, "questions": {"value": "see weaknesses, the main issue being the missing reference and more rigorous statistical evaluation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mc0JWfIwbj", "forum": "cv3B6UeIhu", "replyto": "cv3B6UeIhu", "signatures": ["ICLR.cc/2026/Conference/Submission3690/Reviewer_gwjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3690/Reviewer_gwjt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986318133, "cdate": 1761986318133, "tmdate": 1762916923436, "mdate": 1762916923436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}