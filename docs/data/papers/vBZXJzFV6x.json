{"id": "vBZXJzFV6x", "number": 3268, "cdate": 1757389853375, "mdate": 1759898098676, "content": {"title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate", "abstract": "Effectively managing missing modalities is a fundamental challenge in real-world multimodal learning scenarios, where data incompleteness often results from systematic collection errors or sensor failures. Sparse Mixture-of-Experts (SMoE) architectures have the potential to naturally handle multimodal data, with individual experts specializing in different modalities. However, existing SMoE approach often lacks proper ability to handle missing modality, leading to performance degradation and poor generalization in real-world applications. We propose ConfSMoE to introduce a two-stage imputation module to handle the missing modality problem for the SMoE architecture by taking the opinion of experts and reveal the insight of expert collapse from theoretical analysis with strong empirical evidence. Inspired by our theoretical analysis, ConfSMoE propose a novel expert gating mechanism by detaching the softmax routing score to task confidence score w.r.t ground truth signal. This naturally relieves expert collapse without introducing additional load balance loss function. We show that the insights of expert collapse aligns with other gating mechanism such as Gaussian and Laplacian gate. The proposed method is evaluated on four different real world dataset with three distinct experiment settings to conduct comprehensive analysis of ConfSMoE on resistance to missing modality and the impacts of proposed gating mechanism.", "tldr": "The paper address proposed novel method to address the expert collapse and missing modality problem", "keywords": ["Mixture of Expert", "Multimodal Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dffcd410dcaa9656f46e80ba83433a11a39de523.pdf", "supplementary_material": "/attachment/3188b200010fa847d6086bf28c8c87b0daf3653e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ConfSMoE, a confidence-guided sparse MoE designed to address the challenge of missing modalities in multimodal learning. Traditional SMoE models rely on softmax-based gating, which often causes expert collapse and poor generalization when inputs are incomplete. To overcome these limitations, ConfSMoE incorporates two key innovations: (1) it proposes a confidence-guided gating mechanism (ConfNet) that replaces softmax routing with token-level confidence scores, enabling stable and interpretable expert selection without requiring load-balancing losses; (2) it introduces a two-stage imputation strategy that reconstructs missing modalities through modality-specific inference followed by instance-level cross-modal refinement, preserving both structural and contextual fidelity. Experiments demonstrate that ConfSMoE outperforms existing methods in robustness and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper tackles a crucial problem in multimodal learning: how to effectively handle missing or incomplete modalities in real-world data. This challenge is highly relevant for domains such as healthcare, where data from different sensors or sources are often unavailable or corrupted.\n\n2. The paper provides valuable insights into the root cause of expert collapse in SMoE models through a detailed gradient analysis, revealing that softmax-based gating and load-balancing losses induce conflicting optimization directions.\n\n3. The proposed ConfSMoE framework has made contributions in both confidence-guided gating mechanism and the two-stage imputation strategy, where they demonstrated their unique advantages."}, "weaknesses": {"value": "The overall contribution of this paper is clear and nontrivial, so my comments on the weakness is relatively marginal.\n\n1. Based on my understanding, the framework does not fully address temporal or causal dependencies among modalities or expert activations, which is a common problem in healthcare/activity monitoring/autonomous driving etc. The current design assumes static modality relationships and focuses primarily on handling missing inputs at the feature level. In dynamic settings such as time-evolving multimodal data or streaming environments, confidence-guided gating and two-stage imputation may not adapt quickly to changing modality relevance. So it is interesting to see further extension of this work on these settings.\n\n2. Although the two-stage imputation is effective in handling inter-modality relationships, it may introduce additional computational overhead and stochasticity into the model training procedure."}, "questions": {"value": "In line 244, it is not very clear how the “token-level confidence” is computed or propagated through the MoE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bq5HyJHqU1", "forum": "vBZXJzFV6x", "replyto": "vBZXJzFV6x", "signatures": ["ICLR.cc/2026/Conference/Submission3268/Reviewer_q3sb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3268/Reviewer_q3sb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411394983, "cdate": 1761411394983, "tmdate": 1762916638761, "mdate": 1762916638761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose ConfSMoE, a new sparse mixture-of-experts (SMoE) architecture designed to robustly handle missing modalities in multimodal learning. The paper identifies two major limitations of existing SMoE models:\n\n(i) poor robustness under incomplete modality conditions, and\n\n(ii) conventional load-balancing losses exacerbate expert collapse issue (caused by softmax-based router) by introducing gradient conflicts during optimization.\n\nTo address these problems, the authors introduce two key innovations:\n\n1. A confidence-guided gating mechanism (ConfNet) that replaces softmax gating with token-level confidence scores, thereby avoiding gradient conflicts and improving expert diversity without using load-balance losses.\n\n2. A two-stage modality imputation framework that reconstructs missing modalities using intra-modality sampling followed by instance-specific refinement through sparse cross-modal attention.\n\nLastly, the authors perform several experiments across three missing-modality settings to demonstrate the robustness and generalization of the proposed ConfSMoE. Furthermore, an ablation study is also provided to verify the theoretical analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The confidence-guided gate is a promising alternative for softmax routing. It reduces the “rich-get-richer” feedback loop responsible for expert collapse while remaining interpretable and supervision-aligned.\n\n2. Soundness: The paper provides theoretical derivation for illustrating the expert collapse and the gradient conflicts induced by existing load balancing losses.\n\n3. Relevance: The problem of gating design for alleviatiing expert collapse and handling missing modalities is of interest."}, "weaknesses": {"value": "1. Dependence on supervision: ConfNet’s confidence gating relies on supervised signals (via MSE loss to ground-truth confidence). This limits applicability to unsupervised or weakly labeled multimodal scenarios.\n\n2. Training complexity and cost: The two-stage imputation introduces additional sampling and sparse cross-attention steps that may increase training time. Although Table 14 discusses FLOPs, a clearer runtime or memory comparison would strengthen claims of efficiency.\n\n3. Quality: The theoretical analysis in Section 2 is not good. \n\n- Firstly, in Section 2.1, the MoE in Eq. (1) is not defined well. In particular, the explicit form of softmax router $G(\\textbf{h})$ is not provided. Furthermore, how the top-K experts are selected is not specified, either. \n\n- Secondly, it seems that the Jacobian in Eq. (2) does not take into account the Top-K function. Additionally, sparse MoE is not differentiable in general due to the discontinuity of the Top-K function. Therefore, I'm quite concerned about the validity of Eq. (2).\n\n- Thirdly, the Jacobian of softmax is not accurate. More specifically, softmax weights in MoE should depend on the input $\\textbf{h}$ but the input is not considered when calculating the Jacobian. \n\n4. Clarity: \n\n- The maths presented in Section 3.2 is not rigorous and difficult to follow. In line 273, the notation $M_{m,i}$ is not defined precisely.\n\n- The Gaussian gating is discussed in the paper but the authors do not provide any math formulation or reference for that gating. \n\n- The authors argue that the ConfSMoE helps address the expert collapse but there are not any evidences provided in the paper.\n\n5. Minor issues:\n\n- In line 205, \"Confidence-Guidede\"\n\n- In Eq. (5), $\\sum^{D}y_t\\log(p_t)$"}, "questions": {"value": "1. How sensitive is the performance to the accuracy of confidence supervision? Could miscalibrated confidence estimates lead to suboptimal expert routing?\n\n2. Can the proposed ConfNet be adapted to unsupervised or self-supervised settings where no ground-truth confidence is available?\n\n3. How significant is the additional computational overhead introduced by the two-stage imputation compared to FuseMoE or FlexMoE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rh5AjhxZat", "forum": "vBZXJzFV6x", "replyto": "vBZXJzFV6x", "signatures": ["ICLR.cc/2026/Conference/Submission3268/Reviewer_8riB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3268/Reviewer_8riB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454195337, "cdate": 1761454195337, "tmdate": 1762916638535, "mdate": 1762916638535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ConfSMoE, a Sparse Mixture-of-Experts (SMoE) model designed to mitigate expert collapse and handle missing modalities.  \nThe method combines two components:\n1. Confidence-guided gating (ConfNet): replaces softmax routing with a supervised \"confidence\" score aligned with ground-truth labels, avoiding gradient conflicts from entropy-based load-balancing losses.  \n2. Two-stage imputation: imputes missing modalities first via intra-modality sampling (pre-imputation) and then refines them using sparse cross-attention over available modalities (post-imputation).  \n\nThe authors analyze the gradient cause of expert collapse, argue that load-balancing losses create conflicting gradients, and claim that confidence-guided routing both balances expert usage and improves robustness to incomplete multimodal data. Experiments on MIMIC-III/IV, CMU-MOSI, and CMU-MOSEI show moderate gains in F1/AUC over FuseMoE, FlexMoE, and other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a clear gradient-based explanation of expert collapse and why load-balance losses cause conflicting updates.  \n- Introduces a supervised confidence gate, which is a fresh idea compared to typical softmax or Laplacian routing.  \n- Includes comprehensive experiments across clinical and multimodal benchmarks.  \n- Visualization of expert usage and attention maps supports interpretability claims."}, "weaknesses": {"value": "- **W1. Unclear conceptual linkage between collapse mitigation and missing-modality imputation.**  \n  The paper merges two largely independent problems--expert collapse (an optimization issue) and missing-modality imputation (a data-level issue)--without a convincing reason they must be solved together. Each part could stand as a separate paper, and the combined scope dilutes the narrative focus.\n\n- **W2. Limited novelty of the two-stage imputation module.**  \n  The \"pre-imputation + cross-attention refinement\" design closely resembles existing multimodal imputation methods like DrFuse [1], which also separate modality-specific and cross-modal reconstruction. The proposed variant adds little conceptual innovation beyond placing the procedure inside an MoE.  \n\n- **W3. Empirical improvements are modest.**  \n  Reported F1 scores (40-50 %) are low due to dataset imbalance, but even relative gains over baselines are small (1-4 %). Given the added architectural complexity, the empirical evidence feels insufficient to demonstrate clear practical benefit.\n\n- **W4. Missing comparison to closely related gating approaches.**  \n  The paper does not benchmark against recent load-balancing and uncertainty-aware routing methods that share similar motivations, such as Auxiliary-Loss-Free Load Balancing [2]. Without such baselines, the claimed advantages of the confidence-guided gate remain unverified.\n\n- **W5. Reliance on supervised confidence limits generality.**  \n  The gating network learns from ground-truth labels to estimate confidence, restricting applicability to supervised classification tasks. It is unclear how this method would extend to unsupervised or generative MoE settings where such supervision is unavailable.\n\n[1] DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency. https://arxiv.org/abs/2403.06197\n\n[2] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts. https://arxiv.org/abs/2408.15664"}, "questions": {"value": "If the author could provide clarification on the points in the weakness section and the following additional questions, I will be happy to raise the score.\n1. Can ConfNet operate without ground-truth labels (e.g., using predictive uncertainty as confidence)?  \n2. What is the sensitivity analysis of the sparsity hyperparameter (B) in post-imputation, i.e., Line 300 how sensitive is the model performance to hyperparameter T for top-T entry?  \n3. Could the confidence gate improve MoE models without missing data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RmDt28wHuH", "forum": "vBZXJzFV6x", "replyto": "vBZXJzFV6x", "signatures": ["ICLR.cc/2026/Conference/Submission3268/Reviewer_dEKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3268/Reviewer_dEKy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698732896, "cdate": 1761698732896, "tmdate": 1762916638297, "mdate": 1762916638297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}