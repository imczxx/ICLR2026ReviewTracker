{"id": "0IWZjbMmry", "number": 16659, "cdate": 1758267368176, "mdate": 1759897226556, "content": {"title": "LayerDecompose: Exploring weight sharing for Large Language Model Compression", "abstract": "Recent advances in large language model (LLM) compression have predominantly focused on pruning and low-rank factorization, leaving weight sharing—despite its success in classical neural network compression—largely unexplored. We introduce LayerDecompose, a novel framework that reduces parameter redundancy by sharing a core weight matrix across transformer layers and augmenting each layer with lightweight, low-rank adapters. Unlike prior SVD- and pruning-based methods, our joint optimization of shared weights and residual adapters achieves a 30% model size reduction while retaining 89% of the original performance on seven standard benchmarks. Experiments on LLaMA and other models demonstrate that LayerDecompose consistently outperforms state-of-the-art baselines. These results highlight the promise of combining weight sharing with low-rank adaptation for efficient, scalable LLM deployment.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Model Compression", "Weight Sharing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8cec12cc44b2a6394692a3239cbd59726b460b02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LayerDecompose, a compression framework for large language models that combines weight sharing with low-rank adapters. The key idea is to represent groups of consecutive layers with a single shared weight matrix W, augmented with layer-specific low-rank residuals and per-channel scaling vectors. The authors exploit permutation invariances in transformer modules to better align weights before decomposition. Experiments on LLaMA-7B and other models show 30% compression while retaining 89% of original performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear methodology: The paper provides a well-structured algorithmic approach (Algorithm 1) with closed-form initialization via alternating SVD followed by joint optimization. \n2. Comprehensive experiments: Evaluation across multiple models (LLaMA-7B, Qwen-7B, DeepSeek-7B, OLMo-7B, LLaMA-2-13B) and seven benchmarks demonstrates generalizability.\n3. Comparison against relevant baselines (SVD-LLM, LLM-Pruner, Basis Sharing)."}, "weaknesses": {"value": "1. Missing results. Table 1 only shows results after healing/distillation. No comparison of pre-healing performance across methods. Cannot assess whether improvements come from the decomposition itself or the healing procedure. Please add a column showing \"After Decomposition (Before Healing)\" for all methods.\n2. Inconsistent baseline results. LLM-Pruner results in Table 1 differ significantly from the original paper. Example: At 50% compression, paper reports PIQA=0.66, HellaSwag=0.45, ARC-e=0.41, while original LLM-Pruner reports PIQA=0.693, HellaSwag=0.476, ARC-e=0.465. This raises concerns about the experimental setup and fair comparison. Please clarify the experimental protocol, verify baseline implementation, or cite reasons for differences.\n3. Limited novelty over prior work. DeltaLLM (Mikaelyan et al. 2025) already propose shared base + low-rank deltas. While the paper mentions differences (scaling, permutation-aware alignment, closed-form init), the core contribution feels incremental. The permutation alignment is the main novel component, but its impact is not isolated.\n4. Unclear visualization (Figure 3): Authors claim \"banded structure along diagonal\" showing adjacent layers are similar. But the pattern is not visually obvious; quantitative analysis would help\n5. SVD dimension error (Line 124) States A, B^T ∈ ℝ^{m×ñ} but for standard SVD, shapes should be A ∈ ℝ^{m×r}, B ∈ ℝ^{n×r}"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bDmUvdhqX0", "forum": "0IWZjbMmry", "replyto": "0IWZjbMmry", "signatures": ["ICLR.cc/2026/Conference/Submission16659/Reviewer_jPAH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16659/Reviewer_jPAH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579891363, "cdate": 1761579891363, "tmdate": 1762926718111, "mdate": 1762926718111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LAYERDECOMPOSE, an LLM compression approach leveraging weight sharing. The method compresses models by sharing a single \"base\" weight matrix across a group of transformer layers while augmenting each layer with lightweight, low-rank residual adapters and scaling vectors. Notably, it permutes the weights before decomposition, minimizing reconstruction error. The authors demonstrate that this approach can achieve a 30% reduction in model size on LLaMA-7B while retaining 88.9% of its original performance on seven benchmarks, outperforming SVD- and pruning-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. It tackles the critical problem of weight compression in LLMs.\n2. The idea of applying permutation alignment to weights to maximize the similarity of the base matrix is insightful."}, "weaknesses": {"value": "1. The models used in the experiments are outdated. Specifically, earlier-generation LLMs like LLaMA-7B are known to be more robust to compression, whereas modern, heavily overtrained LLMs like LLaMA 3.1 or Qwen3 are much harder to compress. The work's practicality would be significantly enhanced by including experiments on these modern models.\n2. The baselines are outdated. For example, modern SVD-based methods like [1], claiming performance on par with quantization at high compression ratios, are not included as baselines or cited for discussion.\n3. It is unclear if the method can scale to larger models (e.g., 70B). Solving the required LSAPs could be prohibitively costly at that scale.\n\n[1] BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments"}, "questions": {"value": "1. Can you quantify the computational cost of solving the LSAPs?\n2. Can you add a few experiments on modern models and baselines, or at least add a discussion about this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pX1HPG1QJy", "forum": "0IWZjbMmry", "replyto": "0IWZjbMmry", "signatures": ["ICLR.cc/2026/Conference/Submission16659/Reviewer_1SsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16659/Reviewer_1SsX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809584243, "cdate": 1761809584243, "tmdate": 1762926717760, "mdate": 1762926717760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for compressing LLMs by sharing a core weight matrix across layers, while employing low-rank adapters for each type of weight matrix within a layer. To further reduce approximation error, the method groups output similar layers and permutes the weight matrices within each group."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tSame type of weight matrix in different layers shares a same matrix is interesting.\n2.\tMore redundancy in LLMs is exploited through layer grouping and weight matrix permutation, resulting in better performance than previous methods."}, "weaknesses": {"value": "1.\tLack of generality. All models in the experiments are implemented using MHA, whereas GQA has now become widely adopted.\n2.\tUnfair comparison. In prior works such as SVD-LLM and Basis-Sharing, models were calibrated using only 256 samples without any additional training, whereas this work undergoes a full distillation-based training process.\n3.\tNo end-to-end latency reported. The additional adapters for weight matrices may introduce higher latency, but the overall impact on inference speed remains unreported."}, "questions": {"value": "1.\tIn table 1, why the proposed method is compared with SVD-LLM V2 under 20% compression ratio while compared with SVD-LLM under the rest compression ratio?\n2.\tWhich dataset is used for distillation? And how long does this distillation take?\n3.\tWhich layers are grouped together?\n4.  What is the comparison with  SVD-LLM V2 and Basis-Sharing? \n[1] SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression, NAACL 2025\n[2] Basis Sharing: Cross-layer Parameter Sharing for Large Language Model Compression, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oundaTNzbd", "forum": "0IWZjbMmry", "replyto": "0IWZjbMmry", "signatures": ["ICLR.cc/2026/Conference/Submission16659/Reviewer_cPRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16659/Reviewer_cPRP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828733643, "cdate": 1761828733643, "tmdate": 1762926716770, "mdate": 1762926716770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method for model compression, exploring weight sharing between layers and using low-rank factors to compensate for errors. Its accuracy is validated on several 7B models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. It proposes a compression paradigm that differs from standard quantization and SVD.\n\n2. The figures are clear and well-illustrated."}, "weaknesses": {"value": "1. In my opinion, the paper's most significant weakness is its inability to translate storage savings into inference acceleration. Although weights are shared between layers, each layer must still load these weights during computation at inference time, meaning memory access is not reduced. Furthermore, the authors introduce a LoRa branch, which adds latency. Even with kernel fusion (e.g., as in SVDQuant), this latency is likely non-trivial. In contrast, recent mainstream SVD methods reduce parameters while offering straightforward acceleration by directly cutting the computational load.\n\n2. The experimental section is significantly below the acceptance standard. The authors only test on outdated models and only at the 7B scale. Comparisons to other works are also limited. Comprehensive benchmarks on the Llama 2 and Llama 3 series (from 7B to 70B) are necessary to demonstrate the method's effectiveness and generalizability.\n\n3. The proposed method requires an additional training/fine-tuning step to recover performance. How does it compare to other Post-Training Quantization (PTQ) methods when this extra training step is omitted?\n\nIn summary, this compression form seemingly increases inference latency, and the authors have failed to demonstrate its effectiveness and generalizability on a wide range of modern models. Therefore, compared to the mature, existing paradigms of quantization and SVD, I find the contribution of this new paradigm to be minimal."}, "questions": {"value": "1. Can the authors provide experimental results on more recent models, such as Llama 3 8B and Llama 3 70B?\n\n2. How does the method perform compared to other PTQ methods without the fine-tuning/retraining step?\n\n3. Can the authos provide experimental results on inference speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m514VQsYk3", "forum": "0IWZjbMmry", "replyto": "0IWZjbMmry", "signatures": ["ICLR.cc/2026/Conference/Submission16659/Reviewer_ULsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16659/Reviewer_ULsj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912134658, "cdate": 1761912134658, "tmdate": 1762926716418, "mdate": 1762926716418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}