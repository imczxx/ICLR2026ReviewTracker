{"id": "mUrw5WixTx", "number": 24430, "cdate": 1758356878980, "mdate": 1759896766287, "content": {"title": "Shared Dynamic Model-Aligned Hypernetworks for Zero-Shot Generalization in Contextual Reinforcement Learning", "abstract": "Zero-shot generalization in contextual reinforcement learning (RL) remains a core challenge, particularly when explicit context information is unavailable and must be inferred from data. We propose DMA*-SH, a framework based on dynamics model-aligned (DMA) context inference, where a shared hypernetwork jointly parameterizes the dynamics model, policy, and value function. This design enforces consistency between learned context representations and transition dynamics, while normalization and random masking in the context encoder improve stability and robustness. To evaluate our approach, we introduce environments that distinguish between overlapping and non-overlapping contexts, the latter being typically unsolvable with standard domain randomization. Across these benchmarks, DMA*-SH achieves strong zero-shot generalization and outperforms both context-aware and context-unaware baselines. Our results demonstrate that hypernetworks enable effective and scalable context inference in contextual RL.", "tldr": "We introduce DMA*-SH, a hypernetwork-based approach for contextual reinforcement learning that aligns context inference with dynamics models, enabling stable representations and achieving strong zero-shot generalization across diverse environments.", "keywords": ["contextual reinforcement learning", "zero-shot generalization", "hypernetworks"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac5ccccbd3e740d01bc25cad31f0e6333c0f85ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes to use dynamics aligned context representations via shared hypernetworks. The work first describes how to learn dynamics aligned models from a window of past observations (as commonly also done in meta-RL) and then describes how input masking and input/output normalization are applied to improve over standard practices. Based on this improved dynamics model, the work then discusses how the hypernetwork is producing shared \"adapter\" weights for both the policy and Q functions.\nThe work evaluates both standard dynamics model aligned representations, their improved variants and the shared hypernetwork variant against a broad range of baselines on a variety of benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work tackles an important problem for learning general policies via reinforcement learning. Particularly, the work assumes that one can not make use of prior knowledge of the environment which could be readily be provided to a learning agent and instead learns to infer contextual changes.\nThe baselines are well chosen and meaningful and the spread of environments is also commendable."}, "weaknesses": {"value": "Despite the listed strengths in experimental setup, I believe the work needs to improve how the results are presented. While the IQM provides good overall aggregate statistics, I believe it is not well fit on it's own to discuss the generalization capabilities of the discussed methods. For example, in Figure 3 the results are split into train, eval-in and eval-out results. However, the aggregation does not show how the performances on individual contexts look like. This seems particularly important to me as, the further away from the training distribution one goes, it is reasonable to expect less and less performance. An aggregate statistic like the IQM hides this important information and it is not clear if the proposed methods have better out-of-distirbution capabilities further from the training distribution than others. Similarly, the in-distribution results do not tell us how close the in-distribution evaluation contexts are to the training ones. The survey by Kirk. et al (cited in the paper) showed how this could be evaluated and plotted better and the Work of Prasanna et al (also cited in the paper) gives a direct example for a new cRL method.\n\nFurther, since the environments are dissimilar, I do believe that the aggregate over all environments is not particularly useful. The work has to give more fine grained analysis on a per-environment level to better highlight where the proposed methods shine.\n\nThe \"Informativeness\" analysis in Section 5.5 is similar to a different cRL work by Ndir et al (https://openreview.net/forum?id=51XSWH0mgN). Since this work also contrasts dynamics-model aligned learning with directly using explicit context information, this work warrants a citation. Further, I am unsure if the R2 score here can be seen as a measure of \"informativeness\" as it does not directly measure the \"information content\" of the data. Instead it simply gives a score of how similar the learnt embedding is to a ground truth. Being highly informative, a learnt embedding can still be highly dissimilar to the ground truth. Thus, the wording of informativeness should likely be avoided. If I misunderstood something, I'd be happy to take back my criticism.\n\nWhile I highly appreciate the work and would like to see it published, in it's current state I have to vote for rejection. Relying only on aggregate statistics for the analysis should not be permissible for a work that discusses generalization and needs to provide more detailed insights into relation of evaluation contexts to training ones. I fear that this would require significant rewriting of the experimental section, which I am not sure if it is doable in the short rebuttal time window. I am more than happy to increase my score if the authors can convince me that they can improve the analysis."}, "questions": {"value": "Why is the R2 score analysis a measure of \"informativeness\"?\nWhy did the analysis only rely on aggregate statistics?\nWhich environments are similar enough that they could be grouped in an aggregate statistic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fZJVXhhk1Z", "forum": "mUrw5WixTx", "replyto": "mUrw5WixTx", "signatures": ["ICLR.cc/2026/Conference/Submission24430/Reviewer_kM3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24430/Reviewer_kM3b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640496161, "cdate": 1761640496161, "tmdate": 1762943080906, "mdate": 1762943080906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes incorporating a shared adapter layer, whose weights are generated by a hypernetwork, into the dynamics model, policy, and critic networks in contextual reinforcement learning (RL). The goal is to align these three components under a unified, learned context representation. Additionally, the paper introduces masking and normalization techniques during training to further enhance policy performance. Experimental results demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on most benchmark tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper clearly identifies the challenge of contextual RL when explicit context information is unavailable. To implicitly infer context from data, the authors propose a novel framework that learns context representations by integrating a shared adapter layer—parameterized via a hypernetwork—into the dynamics, policy, and critic networks. This design provides an interesting and potentially valuable direction for context representation learning in contextual RL.\n2. The paper is well-written overall, and the figures illustrating the proposed method are clear and easy to understand."}, "weaknesses": {"value": "1. The novelty of the proposed approach is somewhat limited. The use of a hypernetwork to produce shared adapter parameters is not entirely new in contextual or meta-RL. For instance, a similar idea was explored in [1], which employed a comparable scheme to train the actor and critic in offline meta-RL. Moreover, the use of masking and normalization techniques is quite common in machine learning, so these aspects alone may not constitute significant methodological contributions.\n2. The paper lacks theoretical analysis. The approach appears to rely primarily on empirical findings rather than theoretical justification. For example, there is no analysis explaining why a particular masking ratio or normalization strategy was chosen, nor how these design choices affect learning stability or performance.\n\n[1] Li Z, Lin Z, Chen Y, et al. Efficient offline meta-reinforcement learning via robust task representations and adaptive policy generation. Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24), 2024: 4524–4532."}, "questions": {"value": "1. I am curious about the 100% masking of the input trajectory shown in Figure 7. Could the authors clarify what this represents and how the model handles such fully masked inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DpQOdQhOhi", "forum": "mUrw5WixTx", "replyto": "mUrw5WixTx", "signatures": ["ICLR.cc/2026/Conference/Submission24430/Reviewer_2MdE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24430/Reviewer_2MdE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761646446865, "cdate": 1761646446865, "tmdate": 1762943080611, "mdate": 1762943080611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses zero-shot generalization in contextual reinforcement learning (CRL).\nThe authors propose a framework that learns a dynamics-aligned context representation jointly with the policy and Q-function.\nA shared hypernetwork conditions on the inferred context and outputs parameters that are used across three modules — the dynamics model, the policy, and the action-value network.\nAdditional implementation components include random masking of transition inputs and layer normalization to stabilize the learned context.\nThe method is evaluated on a set of contextual continuous-control environments, showing improved in-distribution and out-of-distribution performance over several baselines such as DMA and PEARL variants."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper identifies a relevant gap in CRL — the difficulty of learning generalizable contextual representations that can adapt to unseen environment dynamics.\n2.\tThe idea of using one hypernetwork to generate parameters for multiple components (dynamics model, policy, and critic) is conceptually novel and promotes parameter consistency across modules.\n3.\tThe paper includes ablations on normalization, masking, and weight sharing, demonstrating their influence on stability and performance.\n4.\tResults are reported with mean, IQM, and aggregate metrics across tasks, following recent reproducibility standards."}, "weaknesses": {"value": "1.\tThe main ideas closely resemble VariBAD (learning a latent dynamics representation for fast adaptation) and Beukman et al., 2023 (using hypernetworks to condition multiple RL components). The proposed method mainly combines these two directions with incremental engineering contributions (masking, normalization) rather than introducing fundamentally new principles or theory.\n2.\tThe paper does not compare against VariBAD, UNICORN, or other prominent COMRL approaches that explicitly learn context embeddings from dynamics. This omission makes it difficult to assess whether the reported improvements are meaningful beyond minor implementation differences.\n3. The evaluation focuses on a small number of custom contextual environments. Broader testing on DMControl or Gym benchmarks would strengthen claims of zero-shot generalization.\n4.\tThe paper does not provide analytical insights into why shared parameterization via a hypernetwork should improve zero-shot generalization, beyond empirical intuition.\n5.\tHypernetworks increase training cost and parameter coupling; no quantitative analysis of overhead or convergence behavior is provided."}, "questions": {"value": "1.\tWhy are VariBAD, UNICORN, or other context-based meta-RL methods not included in the comparison? How does your method differ in architectural or objective terms from VariBAD’s latent dynamics modeling?\n2.\tHave you tested versions where the hypernetwork outputs parameters only for the policy or the dynamics model, rather than all three modules? Quantifying how much of the gain comes from shared parameterization would clarify its value.\n3.\tWhat is the additional computational cost (training time, memory) of using a shared hypernetwork compared to a standard architecture? Does the model remain stable for larger or more diverse environments?\n4.\tHow well does the learned context embedding transfer to truly unseen dynamics families (e.g., different friction coefficients or link masses outside the training distribution)? Any evidence beyond the limited set of contextual environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QRjN7XCBlj", "forum": "mUrw5WixTx", "replyto": "mUrw5WixTx", "signatures": ["ICLR.cc/2026/Conference/Submission24430/Reviewer_VpJQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24430/Reviewer_VpJQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751447466, "cdate": 1761751447466, "tmdate": 1762943080327, "mdate": 1762943080327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an architecture for adaptation in dynamic-varying environment (contextual RL). The authors suggest a few tricks (input masking and input/output normalization) to train the context encoder, as well as using a hypernetwork to \"adapt\" the policy and the Q functions to the context (instead of the common concatenation approach). Through a series of experiments and ablation studies, the authors show superior results in dynamic-varying environments compared to many relevant baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors considered many relevant and competitive baselines, including state-of-the-art meta-rl and context-aware baselines.  \n2. The distinction between overlapping and non-overlapping contexts in the environments is interesting. I think expanding this idea and formalizing it might improve readability."}, "weaknesses": {"value": "1. *[Critical] Limited Contribution:* My main concern is the limited potential of this work to be impactful on the field as it is currently situated. The main contribution of the paper is to adapt the policy and Q function to the inferred context via a hypernetwork instead of just using a feedforward architecture (I state this as it can be seen that the input-masking effect is marginal in Figure 7b, and since normalization is a common practice in RL). Since this is the main contribution, I expect either a deep empirical or theoretical analysis on why a hypernetwork should adapt the policy better to different contexts, compared to the common feedforward approach. \n2. *[Major] Limited Evaluation Setting:* the considered environments are quite simple in the sense that the unobserved context can be inferred from just a handful of transitions (in some environments, even just a single transition). For example, in the DI environment, the mass can be inferred from one transition (as the state contains the velocity). This makes the “context-unaware” setting less challenging.   \n3. *[Minor] Design Choices:* Some design choices are not explained in the paper, e.g., reconstructing the state difference instead of the next state, using a sliding window of past interactions instead of the whole trajectory.\n\nThe first bullet is critical for my decision regarding the paper, and so adding more complex environments (per my second bullet) will not be enough to change my evaluation."}, "questions": {"value": "1. I did not understand the discussion on the input masking (lines 154-156). What is the “common masked prediction objective” compared to your objective, and why did you choose one and not the other? \n2. What is the number of past transitions (K) you used in practice for each environment? \n3. Typo: trajectories -> transitions (line 179). \n4. I suggest splitting the baselines section into a context-aware baselines section and context-unaware baselines section. \n5. I did not understand the formulation of the “mirroring” effect given in lines 317-323. I only understood its meaning after reading the details on the environments. I would suggest refining it.  \n6. I find the observations in Section 5.5 regarding the informativeness and variability interesting, but with limited discussion. I would suggest adding a discussion on why some approaches lead to more informative/variable contexts compared to others."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "22dUDZZc4k", "forum": "mUrw5WixTx", "replyto": "mUrw5WixTx", "signatures": ["ICLR.cc/2026/Conference/Submission24430/Reviewer_2AVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24430/Reviewer_2AVq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948633244, "cdate": 1761948633244, "tmdate": 1762943080081, "mdate": 1762943080081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}