{"id": "Uih5TTYxRe", "number": 22602, "cdate": 1758333360446, "mdate": 1759896857266, "content": {"title": "A Multi-Fidelity Mixture-of-Expert Framework Integrating PDE Solvers and Neural Operators for Computational Fluid Dynamics", "abstract": "Solving Navier-Stokes equations is essential for computational fluid dynamics. While recent advancements in neural operators provide significant speed-ups, they often struggle to generalize to out-of-distribution scenarios. On the other hand, hybrid models that integrate neural networks with conventional numerical solvers offer improved generalization ability but incur high computational costs. To address this trade-off between computational efficiency and generalization ability, we propose the Multi-Fidelity Mixture-of-Experts (MF-MoE) framework. This framework combines a pure neural operator with multiple solver-based hybrid models of varying fidelity, leveraging them as expert models. A physics-aware gating network dynamically selects the most appropriate expert based on input characteristics, optimizing both computational cost and predictive accuracy. This innovative design enables faster inference for in-distribution inputs while ensuring better generalization for out-of-distribution cases. Extensive experiments on fluid flow prediction governed by the incompressible Navier-Stokes equations demonstrate that MF-MoE consistently outperforms baseline approaches, offering an efficient solution for PDE surrogate modeling.", "tldr": "", "keywords": ["Computational Fluid Dynamics", "Mixture of Experts", "Scientific Machine Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2157297b4cbcecdff2cc0d1cff4fe740f12222e4.pdf", "supplementary_material": "/attachment/03bfc453c73ebe36392e79f939ad807f586126da.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Multi-Fidelity Mixture-of-Experts (MF-MoE) framework that combines a pure neural operator with several solver-based hybrid models of varying fidelity. A physics-aware gating network is used to dynamically select the most suitable expert for each input, aiming to balance computational cost and generalization ability. The method is evaluated on incompressible Navier-Stokes fluid flow prediction tasks and claims to outperform baseline neural operators while respecting a time-cost constraint."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of combining multi-fidelity solvers with a gating mechanism is intuitive and relevant for CFD applications.\n\n- The use of a constrained optimization formulation with Lagrangian relaxation is methodologically sound.\n\n- The paper provides several experimental comparisons across multiple neural operator architectures."}, "weaknesses": {"value": "- Lack of Conceptual Novelty: The core idea, routing between cheap and expensive models, is a well-established concept in both MoE literature and multi-fidelity modeling. The paper fails to situate itself meaningfully within these existing lines of work, and the “physics-aware” gating is implemented via simple feature concatenation, which is not fundamentally novel.\n\n- Superficial Treatment of Generalization: The paper claims to address out-of-distribution (OOD) generalization, but the experimental setup only varies viscosity (μ) within a narrow range. This does not constitute a rigorous OOD test, and the claim of “better generalization” is overstated.\n\n- Simplistic and Non-Scalable Experimental Setup: The study is limited to 2D incompressible Navier-Stokes with simple boundary conditions, which is a well-trodden testbed. Only single-step prediction is considered, which is of limited practical value in real-world CFD where long-term rollout stability is critical. The gating network is trained and evaluated on the same type of flows, raising doubts about its true adaptability.\n\n- Engineering-Heavy, Science-Light: The paper reads like an engineering report: it focuses on assembling existing components (FNO, UNO, ResNet) without deepening the understanding of why the method works or when it fails. There is no theoretical insight or failure analysis.\n\n- Inadequate and Outdated Baseline Comparisons: The experimental comparisons are critically limited to foundational neural operator architectures (e.g., FNO, UNO, ResNet) that are several years old. The paper entirely omits comparisons with the more advanced and recent models discussed in its own related work and appendix (e.g., U-FNO, WNO, Koopman Neural Operators, or sophisticated hybrids). This raises a fundamental question: does MF-MoE offer a genuine improvement, or does it merely outperform intentionally weak baselines? This flaw severely undermines the paper's claim of being competitive and makes it impossible to assess its true contribution to the field.\n\n- Insufficient Comparison to Real-World Baselines: While hybrid solver-NN models are compared, there is no comparison to adaptive numerical methods or other adaptive modeling strategies (e.g., adaptive mesh refinement, error-controlled solvers), which are the true competitors in efficient CFD.\n\n- Weak Gating Justification: The gating network is not rigorously analyzed—e.g., no visualization of gate decisions across flow regimes, no analysis of gate confidence, and no ablation on the “physics-aware” features."}, "questions": {"value": "- How does the gating mechanism perform under truly OOD settings (e.g., different geometries, boundary conditions, or turbulent regimes not seen in training)?\n\n- Why was only single-step prediction considered? Can the framework sustain stable multi-step rollouts without increasing error accumulation?\n\n- Have you compared against adaptive numerical methods (e.g., adaptive timestepping or mesh refinement) that also trade accuracy for cost?\n\n- What is the end-to-end training cost of MF-MoE compared to a single high-fidelity hybrid model? Is the training complexity justified?\n\n- Why were no contemporary neural operators (e.g., those listed in your Appendix A) used as baselines to establish a rigorous and up-to-date performance benchmark?\n\nShould you be able to satisfactorily address the points I've raised above, I will accordingly provide a positive rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6keagUCDjF", "forum": "Uih5TTYxRe", "replyto": "Uih5TTYxRe", "signatures": ["ICLR.cc/2026/Conference/Submission22602/Reviewer_sosU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22602/Reviewer_sosU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471340217, "cdate": 1761471340217, "tmdate": 1762942299433, "mdate": 1762942299433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Multi-Fidelity Mixture-of-Experts (MF-MoE) framework, a novel architecture designed to solve the critical trade-off in computational fluid dynamics (CFD) surrogate modeling: computational efficiency versus generalization ability.\n\nPure neural operators (like FNO or UNO) are fast but struggle with out-of-distribution (OOD) inputs, whereas hybrid models (integrating numerical solvers) generalize better but incur high computational costs.\n\nThe MF-MoE framework addresses this by:\n\n1. Integrating Experts: Combining a fast pure neural operator (for in-distribution/easy cases) with multiple solver-based hybrid models of varying fidelity (for OOD/hard cases) as experts.\n\n2. Dynamic Routing: Using a physics-aware gating network and a constrained optimization problem (Lagrangian relaxation) to dynamically select the cheapest, most accurate expert for each input while guaranteeing the average inference time stays below a specified threshold"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel and Relevant Problem Formulation: The paper is the first to formally frame the challenge of combining speed and generalization in PDE surrogates as a constrained optimization problem solved via Lagrangian relaxation. This unique formulation allows the model to explicitly budget its computational time, which is a highly relevant practical concern in industrial CFD applications.\n\nEffective Hybridization Strategy: The key novelty lies in the fusion of multi-fidelity hybrid models (solver + neural network refinement) with a pure neural operator within the MoE architecture. This approach is shown to enhance OOD generalization by dynamically routing complex cases to the slower, physics-integrated experts, successfully outperforming pure neural operators across OOD test cases.\n\nEmpirical Validation on Trade-off: The results clearly demonstrate that MF-MoE achieves accuracy superior to all comparable neural operator baselines while successfully meeting the c=10.0 seconds constraint, a feat impossible for the individual medium and fine-resolution hybrid models. This confirms the framework's ability to dynamically balance the speed-accuracy trade-off."}, "weaknesses": {"value": "Limited Empirical Generalization Scope: While the paper claims OOD generalization, the test set is split only by the viscosity parameter $\\mu$. This limited scope doesn't fully validate the model's robustness to other critical OOD shifts, such as changes in initial/boundary conditions or spatial resolution (which is implicitly handled by the multi-fidelity design but not explicitly tested as an OOD factor).\n\nNovelty of MoE Application: The core mechanism of using a Mixture-of-Experts structure to combine heterogeneous sub-models is well-established in machine learning and has been explored in the context of neural PDE solvers for tasks like incorporating historical information (MoNO) or dealing with spectral sparsity (FreqMoE). The specific novelty here rests entirely on applying it to integrate multi-fidelity solver outputs subject to a time constraint, which is a significant practical variation but builds heavily on known MoE structures."}, "questions": {"value": "Gating Efficiency in the Test Set: Since the goal is faster inference for easy/in-distribution (ID) cases and slower inference for OOD/hard cases, what percentage of samples in the OOD test set were actually routed to the slow, high-fidelity hybrid experts? Providing the routing distribution for the OOD test set is essential to show that the system learned to use the expensive experts when they were needed most.\n\nGeneralization to Geometric OOD Shifts: The OOD test primarily varies the viscosity parameter $\\mu$. How does the MF-MoE perform against a geometric out-of-distribution (OOD) test, such as predicting flow over a slightly perturbed or deformed boundary condition (e.g., a changed obstacle shape)? This would be a much stronger validation of the hybrid experts' generalization capability, which is a key deficiency of pure neural operators.\n\nChoice of Up-Sampling Method: The hybrid model relies on k-nearest neighbor (k-NN) interpolation for up-sampling the coarse solver solution $\\hat{y}_c$. Since this step is prone to introducing interpolation artifacts, have the authors investigated using a small, dedicated neural network (like an additional lightweight U-Net) for differentiable learning of the up-sampling residual? This could potentially eliminate k-NN artifacts and provide a smoother interface between the classical solver and the neural network refinement loop.\n\n\none last question: is there any 3d NS example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0XWHzIqvqH", "forum": "Uih5TTYxRe", "replyto": "Uih5TTYxRe", "signatures": ["ICLR.cc/2026/Conference/Submission22602/Reviewer_iPBr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22602/Reviewer_iPBr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525620528, "cdate": 1761525620528, "tmdate": 1762942299043, "mdate": 1762942299043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the significant challenge of balancing computational efficiency and generalization ability in surrogate modeling for Computational Fluid Dynamics (CFD). Neural Operators (NOs) offer fast inference but often struggle with out-of-distribution (OOD) generalization. Hybrid models, which incorporate numerical solvers during inference, generalize better but are computationally expensive.\n\nThe authors propose the Multi-Fidelity Mixture-of-Experts (MF-MoE) framework to address this trade-off. MF-MoE combines a pure NO (fast, lower generalization) with several solver-based hybrid models of varying fidelity (slower, higher generalization). A gating network dynamically selects one expert for a given input using Top-1 routing.\n\nThe core innovation is formulating the training as a constrained optimization problem: minimize prediction error subject to an explicit constraint c on the average inference time. This is solved using a Lagrangian relaxation approach via stochastic gradient descent-ascent (SGDA). The goal is to utilize the fast NO for \"easy\" (in-distribution) samples and reserve the expensive hybrid experts for \"hard\" (OOD) samples.\n\nExperiments on 2D incompressible Navier-Stokes equations, with OOD generalization tested across different viscosities, show that MF-MoE generally achieves better accuracy than baseline NOs while satisfying the imposed time constraint."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a highly relevant problem with an elegant conceptual framework and is presented with excellent clarity.\n\n1. Novel and Practical Problem Formulation: The strongest contribution is framing the accuracy-speed trade-off in hybrid models as a constrained optimization problem (Eq. 1) with an explicit average inference time budget c. This provides a principled method for managing the computational overhead of hybrid approaches during deployment.\n\n2. Elegant Conceptual Framework: The MF-MoE architecture offers a logical structure for integrating models with different fidelity profiles. The concept of dynamically routing inputs based on perceived difficulty to balance accuracy and cost is compelling.\n\n3. Excellent Clarity: The paper is very well-written, clearly motivated, and easy to follow. The diagrams (Figures 2 and 3) effectively illustrate the proposed architecture."}, "weaknesses": {"value": "While the conceptual framework is promising, the paper suffers from significant weaknesses in optimization stability, experimental validation, and architectural design, which currently undermine the central claims.\n\n1. Optimization Instability and Evidence of Poor Training Performance: The proposed training strategy involves solving a challenging optimization problem: a non-convex minimax objective (gradient descent on θ, ascent on λ) combined with discrete expert selection. There is strong evidence that the optimization is unstable and fails to train the experts effectively. Figure 5 (center panel) shows that expert selection frequencies remain noisy and do not appear fully stabilized even after 8000 training steps.\n\n2. Critical Missing Baselines and Incomplete Trade-off Analysis: The central claim is that MF-MoE optimally balances accuracy and speed. However, the experimental results (Table 2) lack crucial comparisons. The performance of the individual hybrid experts (e.g., \"Hybrid-Fine\") trained standalone is missing. To evaluate the trade-off, we must know how much accuracy MF-MoE sacrifices compared to the best (but slow) hybrid expert to meet the time constraint c. Also, the time constraint is c=10.0s. According to Table 1, the XCoarse (16x16) solver takes ≈8s. A hybrid model based only on the XCoarse solver would satisfy the time constraint. This is a critical baseline that MF-MoE must outperform.\n\n3. Questionable Architectural Design Choices: The architecture of the hybrid expert appears suboptimal. First, the neural operator corrector seems to take only the initial conditions as input, not the coarse solver output. This means the NN cannot adapt its correction based on the specific errors made by the solver. Second, the output is a fixed convex combination and this is highly restrictive; the optimal balance is likely state-dependent and should perhaps be learned or spatially varying. The value of α used is not reported.\n The gating network uses a \"simple MLP\" to extract features from the high-dimensional input fields. However, a MLP is likely insufficient to capture the complex spatial dynamics (e.g., emerging turbulence, sharp gradients) necessary to judge the difficulty of the input in a CFD context.\n\n4. Insufficient Evaluation Scope: The evaluation is limited to single-step predictions. For time-dependent PDEs, surrogates must demonstrate stability and accuracy over long-horizon autoregressive rollouts. Error accumulation is a major failure mode. Furthermore, in a rollout, error accumulation might interact with the gating mechanism (e.g., accumulated error forcing overuse of expensive experts later in the trajectory), which is completely unaddressed.\n\n5. Lack of Analysis of Gating Behavior: The paper claims the model uses fast inference for ID samples and hybrid models for OOD samples, but provides no evidence for this on the test set. The authors must demonstrate a correlation between the OOD parameter (μ) and the expert selected to validate that the gating network learns a meaningful routing strategy.\n\n6. Unrealistic Assumption of Fixed Solver Costs: The optimization relies on a fixed look-up table T for solver costs (L307). This assumes the solver runtime is independent of the physical parameters (e.g., viscosity μ). In practice, runtime often depends on the stiffness of the problem, which varies with μ. If the runtime varies significantly, the optimization based on fixed T is flawed."}, "questions": {"value": "The following major concerns must be addressed to improve the assessment of this work:\n\n1. Optimization Issues: Why is the expert selection not converging even after sufficient training? Does this indicate optimization instability? Have you experimented with stabilization techniques, such as two-stage training (pre-training experts) or softer gating?\n\n2. What is the Test MSE and average inference time for the individual hybrid experts (Fine, Medium, Coarse) trained standalone, and how does MF-MoE compare to a hybrid model built solely on the XCoarse (16x16) solver?\n\n3. Does the corrector NN in the hybrid expert (Figure 3) take the coarse solver output as input? If not, why was this design chosen over a standard correction approach? What value of α was used? Why use a fixed convex combination instead of a standard residual connection or a learned combination? Why is an MLP used for feature extraction in the gating network instead of a more spatially aware architecture (e.g., CNN)?\n\n4. How does MF-MoE perform in long-horizon autoregressive rollouts compared to the baselines in terms of accuracy, stability, and average time cost over the trajectory?\n\n5. Can you provide a breakdown of expert utilization on the test set correlated with the viscosity parameter μ?\n\n6. Can you verify the assumption that the solver runtime is independent of the viscosity μ for a fixed resolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ymGQDtyIVP", "forum": "Uih5TTYxRe", "replyto": "Uih5TTYxRe", "signatures": ["ICLR.cc/2026/Conference/Submission22602/Reviewer_zgUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22602/Reviewer_zgUJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719523677, "cdate": 1761719523677, "tmdate": 1762942298777, "mdate": 1762942298777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Multi-Fidelity Mixture-of-Experts (MF-MoE) framework for surrogate modeling in computational fluid dynamics. The core idea is to integrate a pure neural operator (fast but less generalizable) with multiple solver-based hybrid models of varying fidelities (accurate but computationally expensive). A key innovation is a physics-aware gating network that dynamically routes each input to the most suitable expert. The training process is formulated as a constrained optimization problem using Lagrangian relaxation, explicitly trading off prediction accuracy against a user-defined inference time-cost constraint. Extensive experiments on incompressible Navier-Stokes equations demonstrate that MF-MoE achieves better accuracy than pure neural operator baselines while satisfying the computational budget, effectively balancing efficiency and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The work presents a novel and well-motivated framework that effectively addresses the trade-off between the speed of neural operators and the generalization of hybrid solver-in-the-loop models. The integration of a multi-fidelity approach within a MoE setup for PDEs is innovative.\n\n2.The paper provides thorough experiments on a fluid dataset, comparing against strong and relevant baselines and ablated versions of the proposed model. The results convincingly show the framework's benefits"}, "weaknesses": {"value": "1.The current work focuses exclusively on single-step prediction. The performance and behavior of the gating network in a multi-step, autoregressive rollout setting—where error accumulation is a critical issue—remain unexplored and represent a significant limitation.\n\n2.While the framework is flexible in principle, the experiments are confined to a specific 2D Navier-Stokes setup. It is unclear how well the method scales to 3D problems, more complex geometries, or different PDE families.\n\n3.The \"physics-aware\" aspect of the gating network, while motivated, could be better analyzed. A more detailed ablation study on which input features (physical parameters vs. extracted field features) are most critical for the routing decision would strengthen the paper.\n\n4.The baselines used are outdated. The comparison would be more convincing with the inclusion of stronger or more recent state-of-the-art methods, such as FactFormer, P²C²Net, FFNO, GNOT, and DeepONet.\n\n5.The dataset is relatively simple. We recommend testing the model's performance on more challenging benchmarks like Taylor-Green or Kolmogorov flow."}, "questions": {"value": "1.How would you expect the MF-MoE framework to perform in a multi-step, long-horizon prediction task? Would the gating network be prone to cascading errors if it frequently selects the faster, less accurate expert? Have you conducted any preliminary experiments in this setting?\n\n2.Could you provide more insight into what the physics-aware gating network learns? For instance, can you analyze specific input cases (e.g., specific viscosities or flow complexities) and show which expert is typically selected, and why that choice is physically intuitive?\n\n3.The inference cost is dominated by the PDE solver. In a scenario where the pure neural operator expert is sufficiently accurate for a large fraction of \"easy\" inputs, does the overhead of running the gating network and the *potential* call to a solver still provide a net speedup compared to just using a hybrid model for all inputs?\n\n4.How sensitive are the results to the specific choice and number of experts in the pool? Did you experiment with different combinations of fidelities (e.g., removing the \"Medium\" expert) or including more than one pure neural operator？\n\n5.The train/validation/test split is based solely on the viscosity parameter `μ`. Have you tested the framework's generalization to other out-of-distribution scenarios, such as unseen initial conditions, boundary conditions, or external force fields `f`?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cDWrn0uOPy", "forum": "Uih5TTYxRe", "replyto": "Uih5TTYxRe", "signatures": ["ICLR.cc/2026/Conference/Submission22602/Reviewer_5DgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22602/Reviewer_5DgK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810077236, "cdate": 1761810077236, "tmdate": 1762942298280, "mdate": 1762942298280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}