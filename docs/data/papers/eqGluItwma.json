{"id": "eqGluItwma", "number": 19452, "cdate": 1758296361864, "mdate": 1759897038123, "content": {"title": "GLDS: Global–Local Diversity Selection for Scalable Token Pruning in Vision–Language Models", "abstract": "Transformer-based vision–language models (VLMs) have achieved state-of-the-\nart performance across a wide range of multimodal tasks, yet their high inference\ncost remains a major obstacle to scalability. We address the fundamental chal-\nlenge of efficiently identifying the most informative visual tokens in VLMs—a\nkey bottleneck for large-batch and long-sequence inference. Existing methods of-\nten rely on exhaustive or heuristic search strategies that become prohibitively slow\nor memory-intensive at deployment scale. We introduce Global-Local Diver-\nsity Selection (GLDS), a training-free, model-agnostic framework that performs\ncomputationally efficient token selection while explicitly balancing local impor-\ntance with global coverage. To further enhance representational quality under ag-\ngressive pruning, GLDS incorporates a determinantal point process (DPP)–based\ndiversity mechanism, ensuring that the retained subset captures both spatially\nand semantically diverse regions. This leads to consistent improvements across\nbatch sizes and sequence lengths. GLDS accelerates both the prefill and decoding\nstages, achieving up to x1.75 speedup in prefill and x1.40 in decoding, while\nscaling to inference regimes that overwhelm conventional approaches. On image\nunderstanding benchmarks, it maintains performance with less than 1% absolute\naccuracy loss. To our knowledge, this is the first principled and scalable token-\nselection strategy to achieve a favorable efficiency–accuracy trade-off in VLMs,\npaving the way for practical deployment of accelerated multimodal transformers.", "tldr": "We propose a training-free and model-agnostic token pruning framework for vision–language models that delivers up to x1.4 faster inference with <1% accuracy loss.", "keywords": ["Model Compression", "Large Language Models", "Structured Pruning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b32c387f4706d9e98bc8ecb9515b4e1324b442c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a token pruning technique called GLDS that combines determinantal point process-based global selection with local window refinement to reduce visual tokens in vision-language models during inference. The method is specifically designed to address limitations in modern architectures like Qwen2.5-VL that lack [CLS] tokens, achieving up to 1.75x speedup while maintaining accuracy within 1% on benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "-  The paper shows speedups with minimum accuracy loss.\n- The method is training-free and model-agnostic.\n-  The authors appear to be releasing code and provide enough implementation detail that someone could actually reproduce this."}, "weaknesses": {"value": "- Incorrect quotation marks throughout, low-resolution compressed figures.\n- Table 2 bolding error: baseline (61.9) outperforms GLDS (61.1) but GLDS is bolded as \"best\".\n- Undefined parameters: \"scaling\" in Eq. 10 is never explained.\n- No comparison with ToMe, AIM, PruMerge despite citing them.\n- Method is just DPP (from CDPruner) + local windows (from VScan) + merging (from ToMe).\n- Claims of \"first principled approach\" are false given extensive prior work.\n- Sections 3.1-3.3 waste space explaining basic transformer attention everyone knows.\n- Different retention ratios across models (60% vs 75%) may suggest cherry-picking.\n- No error bars or statistical significance testing.\n- No proper ablation isolating each component's contribution."}, "questions": {"value": "- What happens if you replace DPP with simpler alternatives (random sampling, k-means clustering, or furthest point sampling)? This ablation would clarify if the complexity of DPP is justified.\n- Why do you use different retention ratios (60% for Qwen2.5-VL vs 75% for LLaVA)? Please provide results with uniform retention ratios across all models.\n- Can you provide comparisons with ToMe, AIM and PruMerge? If implementation challenges exist, please explain specifically what prevents fair comparison.\n- Table 2 shows GLDS (61.1) underperforming baseline (61.9) on GQA for Qwen2.5-VL, yet GLDS is bolded. Is this an error or is there a different interpretation?\n- What is the \"scaling\" parameter in Equation 10? How is it set and what is its sensitivity?\n- What is the end-to-end speedup for complete inference pipelines including preprocessing/postprocessing?\n- All results appear to be single runs. Can you provide error bars or confidence intervals across multiple seeds?\n- For the ablation study, what is the individual contribution of each component (DPP alone, local windows alone, merging alone)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nZ1NhRZ7aH", "forum": "eqGluItwma", "replyto": "eqGluItwma", "signatures": ["ICLR.cc/2026/Conference/Submission19452/Reviewer_CKsz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19452/Reviewer_CKsz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583275356, "cdate": 1761583275356, "tmdate": 1762931368971, "mdate": 1762931368971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Global-Local Diversity Selection (GLDS), a training-free visual token pruning framework that balances local importance and global coverage to accelerate VLM inference. Experimental results demonstrate that GLDS achieves strong performance retention while delivering significantly higher efficiency, outperforming existing state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach achieves advantegeous performance-efficiency trade-off.\n2. The structure of this paper is clear."}, "weaknesses": {"value": "1. The novelty of this paper is limited. The proposed complementary global-local selection strategy for visual tokens closely resembles that of VScan, and the underlying motivation appears largely similar.\n2. The experimental results are not comprehensive. The performance is evaluated on only four standard benchmarks with a single predefined pruning ratio. Additional experiments under diverse settings are needed to better validate the effectiveness of the proposed approach.\n3. The motivation of this work should be better clarified. Most of the empirical analyses presented have already been explored in previous studies.\n4. The presentation of this paper is weak, and the overall work reads more like a technical report than a research paper. Most of the contributions appear to be engineering-oriented, offering limited conceptual or theoretical insights to advance the field."}, "questions": {"value": "See the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "72oVPEIGZY", "forum": "eqGluItwma", "replyto": "eqGluItwma", "signatures": ["ICLR.cc/2026/Conference/Submission19452/Reviewer_nB9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19452/Reviewer_nB9D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948416642, "cdate": 1761948416642, "tmdate": 1762931368353, "mdate": 1762931368353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a training-free and model-agnostic token pruning framework for vision-language models, which targets to accelerate the VLMs with PatchMerger and without [cls] like Qwen2.5-VL. The method is composed of a DPP-based diversity mechanism with local Top-K selection and token merging of the dropped tokens. Extensive experiments on different benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of the limitations for deploying existing methods to Qwen2.5-VL sounds good and insightful.\n2. The proposed method can achieve speedup on both the prefill and decode stages, which is beneficial to the deployment.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. A comprehensive ablation study is required, e.g., $\\alpha$, Top-M, w, r, etc.\n2. Compared with previous DPP-based methods, the main contribution of this work is adapting DPP to the models without [CLS] like Qwen2.5-VL, which I think is incremental in terms of novelty.\n3. The experimental comparisons are insufficient, as the authors mainly compared with VScan. However, works like FitPrune/FastV/SparseVLM/VTW are not compared, even on LLaVA.\n4. Some important references need to be compared or discussed, e.g., Dynamic-LLaVA, CoreMatching.\n5. Some claims are not self-contained, e.g., although the authors doubt the reliability of the attention score in Qwen2.5-VL, they still introduce the attention score into DPP kernel design. \n\n[1] Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification. ICLR 2025.\n[2] CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models. ICML 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7JTPWR1MRG", "forum": "eqGluItwma", "replyto": "eqGluItwma", "signatures": ["ICLR.cc/2026/Conference/Submission19452/Reviewer_64Sj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19452/Reviewer_64Sj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975556567, "cdate": 1761975556567, "tmdate": 1762931367895, "mdate": 1762931367895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}