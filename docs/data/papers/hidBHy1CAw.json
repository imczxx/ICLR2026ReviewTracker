{"id": "hidBHy1CAw", "number": 21818, "cdate": 1758322250290, "mdate": 1759896901555, "content": {"title": "WorldGym: World Model as An Environment for Policy Evaluation", "abstract": "Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.", "tldr": "", "keywords": ["World model", "video generation", "policy evaluation", "generative simulators"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d43a120b43c67b002fd41acb7bd9326727d71c4c.pdf", "supplementary_material": "/attachment/1e199f1772a0258615e3b6351b2aad90dfef2d0c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework to train a robot control sequence conditioned diffusion model to produce images. The model serves as a virtual world where robot policies can be validated in manipulation tasks. The language model can be used to provide rewards for the evaluation. In the experimental part it is shown that the model provides evaluations that are consistent with real-world evaluations. At its best the model can be used in training better models for real world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Since the diffusion models can provide realistic looking images and through language (or rather tokens) can be used to condition image generation, it is an interesting path of research for robot learning to use such models as components in robot learning. This idea is not new, but the implementation of this work is rather good and it can provide interesting avenues for the future research. I also believe it could be useful for offline RL."}, "weaknesses": {"value": "**Major:**\n\n**Moderate:**\n\n - The qualitative example sequences in Figure 2 are so short that it is barely visible something happens - a longer sequence from task start to task finish would be more interesting to see\n\n - More details about how much training data is needed to train the model. This could be in the terms of how many hours of video and how many episodes used, how the position of camera (view) affects the results (how much variation was allowed) => for the camera-ready you could add even more discussion about the limitations observed\n\n - Also some details about the computing setup and training times so that we know if this is doable in a research laboratory\n\n - Some example images of the out-of-dataset tasks would be interesting to see (i.e. how different they actually are)\n\n**Minor:**"}, "questions": {"value": "I think your work is good and everything is explained in the level the results are replicable. I wish the code and pre-trained models are published since I am sure there are many technical details missing but important for those who want to replicate them.\n\nI am sure the model has limitations, but I also find that it can be used in coarse evaluation of policies trained and perhaps to debug errors during development. I find it valuable and therefore important to be published.\n\nIt would be interesting to see how well the diffusion model can generalize given more training data (diversity + amount) and is there some kind of correlation with the policy i.e. could it be used to predict how much training data is needed to be able to learn a policy. And does diffusion model training need more data than learning a well working policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PpguwsUVpo", "forum": "hidBHy1CAw", "replyto": "hidBHy1CAw", "signatures": ["ICLR.cc/2026/Conference/Submission21818/Reviewer_vCB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21818/Reviewer_vCB6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641223124, "cdate": 1761641223124, "tmdate": 1762941942275, "mdate": 1762941942275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WorldGym, a framework that uses a video-based world model as an environment for evaluating robot policies. The model predicts future visual outcomes from actions and uses GPT-4o as a semantic reward evaluator, showing strong correlation with real-world policy performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a clear and innovative use of video diffusion world models for policy evaluation instead of training, which reframes how offline policy analysis can be done without physical robots. The experiments show consistent correlation between simulated and real results, maintain relative performance rankings across models, and include OOD tests that reveal model weaknesses. Technically, the framework is efficient, combining causal temporal attention and adaptive horizon prediction to support different policy granularities while keeping inference cost reasonable."}, "weaknesses": {"value": "I believe a more convincing way to show WorldGym’s effectiveness would be to perform reinforcement learning with WorldGym as the environment and then test the resulting policy in simulation or the real world, but the paper doesn’t do that."}, "questions": {"value": "Have the authors attempted to perform reinforcement learning using WorldGym as the training environment? If not, are there plans to test such sim-to-sim or sim-to-real transfer in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f2teDcCfIk", "forum": "hidBHy1CAw", "replyto": "hidBHy1CAw", "signatures": ["ICLR.cc/2026/Conference/Submission21818/Reviewer_tKDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21818/Reviewer_tKDH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870165503, "cdate": 1761870165503, "tmdate": 1762941941947, "mdate": 1762941941947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WorldGym: a learned, action-conditioned video world model used as a unified environment to offline-evaluate robot policies. From a single real initial frame and a policy’s action chunk, the model predicts future frames; a VLM grader turns those into success scores. A key systems choice aligns the prediction horizon with each policy’s action-chunk size to reduce wasted rollout compute. Experiments report strong sim-to-real correlation of success rates, preserved policy rankings across families/sizes/checkpoints, and targeted OOD probes that reveal failure modes. The setup is framed as OPE with learned dynamics and learned reward from videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: reframes policy evaluation as “rollout in one learned world” rather than per-task simulators; leverages the one-world prior and diverse training data. \n2. Practicality: one real frame + actions, no hand-coded simulators; horizon–chunk alignment is a clean trick that supports mixed policies while saving compute.\n3. Clarity: the OPE formulation and rollout protocol are easy to follow; model/policy interfaces are explicit.\n4. Significance: high sim-to-real correlation and preserved rankings make this a plausible tool for fast model selection; OOD edits provide actionable diagnostics."}, "weaknesses": {"value": "1. VLM reward calibration is under-analyzed: the proposed VLM grader is central, but the paper does not show reliability audits (human agreement, prompt/temperature sensitivity, temporal credit). The authors should add thorough calibration and robustness studies.\n2. Dynamics fidelity over long horizons is not quantified: the work shows plausibility, but does not report compounding-error metrics (e.g., FVD/LPIPS vs time, controllability under action perturbations). The authors should measure error growth and contact realism.\n3. Statistics are light: correlations are promising, but the paper does not provide per-task CIs, bootstrap uncertainty, or stress tests under harder domain shifts (lighting, camera pose, clutter). Add stronger stats and broader shifts.\n4. Single-frame initialization sensitivity is unclear: the paper does not analyze bias/leakage from the initial frame. Evaluate with occlusions/crops/background swaps to quantify sensitivity.\n5. Efficiency claims are not profiled: horizon–chunk alignment is appealing, but the paper does not show throughput/latency/memory vs baselines across GPUs and chunk lengths. Provide wall-clock profiles and ablations (e.g., with/without diffusion forcing).\n6. Related-work positioning lacks shared benchmarks: there is no head-to-head vs closest OPE/world-eval baselines on common tasks. Add small shared batteries and ranking-agreement metrics.\n7. OOD analysis is narrow: findings hinge on a few edit types; generality across textures, shapes, adversarial overlays is unknown. Expand OOD suite and report policy-wise degradations with uncertainty."}, "questions": {"value": "Same in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BBbyL64wGE", "forum": "hidBHy1CAw", "replyto": "hidBHy1CAw", "signatures": ["ICLR.cc/2026/Conference/Submission21818/Reviewer_zRts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21818/Reviewer_zRts"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894714197, "cdate": 1761894714197, "tmdate": 1762941941688, "mdate": 1762941941688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WorldGym, a world-model-based environment for evaluating robot control policies without physical deployment. The approach uses an autoregressive, action-conditioned video generation model to simulate robot interactions, with a VLM (GPT-4o) providing task success evaluation. The authors demonstrate strong correlation (r=0.78) between world model success rates and real-world performance across three VLA policies (RT-1-X, Octo, OpenVLA) on Bridge manipulation tasks. WorldGym enables efficient evaluation on OOD tasks and environments through language/image modifications, revealing interesting policy failure modes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- WorldGym proposes a way to address a genuine need for safe, reproducible, and cost-effective policy testing before real-world deployment.\n- WorldGym shows impressive correlation between simulated and real-world success rates. It also does strong empirical validation.\n- The single world model generalizes across diverse tasks and environments.\n- The paper is well-written, the problem motivation is compelling, and the approach is clearly described."}, "weaknesses": {"value": "- The paper does provide quantitative metrics on world model prediction quality.\n- The world model still shows physics-inplausible predictions, as shown in Figure 14. The paper also does not propose any method to help the video model learn real-world physics.\n- The paper does not compare with baselines like building digital twins to do policy evaluation.\n- The paper does not show the computational requirements and the inference speed of the world model.\n- The paper does not show qualitative results on the simulation and real-world correlation. For example, can authors provide the visualizations of model success and failure cases to see if WorldGym has similar visual renderings compared with the real world? Or providing a fixed set of action replay in both the real world and WorldGym is also meaningful to see the world model gap."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hAiCXjKZk9", "forum": "hidBHy1CAw", "replyto": "hidBHy1CAw", "signatures": ["ICLR.cc/2026/Conference/Submission21818/Reviewer_UNV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21818/Reviewer_UNV5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974359362, "cdate": 1761974359362, "tmdate": 1762941941392, "mdate": 1762941941392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}