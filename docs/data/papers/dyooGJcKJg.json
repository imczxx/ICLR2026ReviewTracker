{"id": "dyooGJcKJg", "number": 16906, "cdate": 1758270146927, "mdate": 1763748885554, "content": {"title": "PerSpectra: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments", "abstract": "Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity.\nYet this characteristic has not been carefully examined within the LLM research community and remains absent from most alignment studies.\nDebate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation.\nOther debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse.\nWe introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded into multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives. We release PERSPECTRA as a resource with flexible configurations, enabling the creation of tasks beyond the demo tasks presented in this paper, and fostering progress toward pluralism-sensitive systems that more faithfully capture human heterogeneity.", "tldr": "We present PerSpectra, a scalable benchmark that integrates debate structure and linguistic diversity to evaluate pluralism in large language models.", "keywords": ["pluralism", "Argument", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4974b8ff8e03dadbdb2fac83d568daf9554a837d.pdf", "supplementary_material": "/attachment/1c966ef5aa4cf8118610946492b8a12a1925e3b8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a dataset (with associated benchmark tasks) for LLM alignment geared towards “pluralism,” i.e. the ability to engage with and distinguish between different perspectives on an issue. This is motivated by the idea that LLM outputs often reflect homogenized, culturally dominant opinions (thereby suppressing minority or contrarian views) and there is a relative lack of research on addressing this issue.\nTheir new dataset, called PERSPECTRA, draws from two online sources: Reddit, which has an abundance of unstructured debate text, and Kialo, which has very structured pro-con debate text. The dataset is constructed using NLP techniques: for each Kialo opinion, they automatically retrieve semantically similar Reddit comments and then, using controlled prompting with ChatGPT, generate expanded versions of the arguments therein.\nThe retrieval step is done in cosine similarity on the Qwen3-Embedding-8B model’s text embeddings. The expanded arguments are constructed by prompting GPT-4o “to preserve the core stance of the original Kialo opinion while adapting its style and elaboration using cues from the paired Reddit comment.” There are five expansions of each of the 762 opinions, yielding 3,810 expansions in total."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper seems strongest in terms of clarity.\n\nOriginality: This paper applies a relatively new method (LLM-aided retrieval-and-expansion) in a relatively understudied domain (pluralism alignment).\nQuality: The analysis of failure modes is well done. In deploying a suite of LLMs on their benchmark, they are clear about the challenges faced by these models, and discuss the general trends about which models do better on which tasks.\nClarity: The paper and code seems very clearly written. \nSignificance: Their benchmark is well-motivated, tackling a legitimate problem in automated language generation––the (in)capacity for pluralism––which is of particular relevance as LLMs are used in educational and social contexts. Furthermore, though the dataset presented is somewhat small, the controlled retrieval-and-expansion pipeline suggests good scalability."}, "weaknesses": {"value": "Overall, the paper seems weakest in terms of significance and originality.\n\nOriginality: The novelty of the methods involved in this paper––in particular the dataset construction––is not made very clear. Is there anything unique about the retrieval-and-expansion method used in this paper, aside from the choice of Reddit and Kialo as data sources?\nClarity: It is unclear to me how helpful Table 1 is. Is this supposed to show what the opinion counting task looks like? The caption should explain this. Also, the motivation in the introduction (e.g. the claim that LLMs output homogenized answers) should be fleshed out more. Seems like the content in Section 2.1 is relevant. \nQuality: Scrolling through the LLM-expanded arguments contained in the dataset, I noticed that many of the arguments’ first sentences repeat the text of the topic word-for-word before going in more detail, while some start with simply “I agree.” \nSignificance: The benchmark is somewhat small (3810 arguments), compared to say PERSPECTRUM which has almost 20,000 combined claims, perspectives, and evidence sentences combined. This calls into question the claims of scalability. On a related note, I think using such a similar name to the existing PERSPECTRUM dataset should merit more discussion and comparison with that dataset. \n\n\nTwo more specific notes\nRe: line 130-131: doesn’t PERSPECTRA, like PERSPECTRUM, rely on human annotation? Alternatively, if you believe the human annotation gives PERSPECTRA high enough marks, do you recommend creating a scaled up version of this dataset that is not subject to human annotation?\nRe: Section 4/5: how do the suite of models perform on the other pluralism benchmarks? Are the trends discussed in the Failure Analysis section still representative?"}, "questions": {"value": "(1) Considering the claims of scalability made in the paper, I think it is natural to ask: why isn't PERSPECTRA bigger? Was the bottleneck the number of topics available on Kialo, or the number of human evaluators? \n(2) A key part of the dataset creation is asking ChatGPT to expand on the opinion given in a Reddit comment. Is there a risk that asking an LLM to expand on a comment is not so valid, depending on the nature of the comment? Is this an issue you ran into at all?\n(3) In the process of matching Kialo opinions with Reddit comments, was there a worry that some of the Reddit comments were bot-generated? It seems like there was some filtering for this (re: line 197), but it seems like comments written by a chatbot which don’t have bot in the username could still reasonably make it past your filtering?\n(4) Who performed the manual validation? The authors? External party? How many people? \n(5) I’m not sure how much Figure 3 contributes to the main text of the paper. Is it communicating anything unique about the dataset? Is there a particular statistic here that merits attention? I suppose this may just be a standard thing to do for papers which present a new dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G1YYCrIdbn", "forum": "dyooGJcKJg", "replyto": "dyooGJcKJg", "signatures": ["ICLR.cc/2026/Conference/Submission16906/Reviewer_eQpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16906/Reviewer_eQpS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448871896, "cdate": 1761448871896, "tmdate": 1762926936185, "mdate": 1762926936185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PERSPECTRA, a benchmark for evaluating the \"pluralism\" of LLMs, which they define as their ability to handle multiple distinct viewpoints without collapsing them. The core contribution is a synthetic data pipeline: they take structured arguments from Kialo, retrieve semantically \"similar\" comments from Reddit, and use GPT-4o to expand the arguments into naturalistic text that mimics the Reddit style. From this dataset of 3,810 arguments across 100 topics, they define three tasks: counting unique opinions, matching a generated argument back to its source, and checking the aggregate pro/con polarity. They benchmark several models, highlighting common and interesting failure modes like overestimating opinion counts and being tricked by concessive phrasing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of combining the structural clarity of Kialo with the stylistic diversity of Reddit is a neat trick. It's a pragmatic approach to generating nuanced argumentative data, which is a known bottleneck. I wasn't even aware of Kialo before this paper. \n\nThe real meat of the paper is in Section 5. The identification of challenges like \"opinion overestimation\" and the \"concession trap\" is good.\n\nThe idea of creating a benchmark for pluralism is good, and the generation method is a clever (if flawed) approach to scale beyond human annotation. The benchmark and the failure analysis are a welcome addition, even if the dataset is not massive.\n\nThe paper is clearly written. The pipeline is explained well, and the failure analysis section is particularly insightful."}, "weaknesses": {"value": "The paper champions a \"scalable\" pipeline but delivers a dataset with only 100 topics and 3810 rows. This feels more like a proof-of-concept than a large-scale resource that lives up to the \"scalable\" moniker. I'd call OpenDebateEvidence and it's nearly 4 million rows \"scalable\", but not this. \n\nThe related work section is incomplete. It fails to cite foundational work in this niche, particularly datasets like DebateSum and OpenDebateEvidence (Roush et al.). These works tackle the problem of structuring claims from formal debate rounds and are highly relevant to this papers related work sections. I would also expect similar citations for IBM Project Debater and the things they cite. \n\nThe entire benchmark is built on outputs from a single proprietary model (GPT-4o). Are we testing a model's understanding of pluralism, or its ability to parse the stylistic tics of another LLM? This creates a \"synthetic-on-synthetic\" evaluation loop that risks optimizing for model-to-model mimicry rather than genuine reasoning. It's the hall of mirrors problem, and the paper doesn't sufficiently address it.\n\nThe authors' own human evaluation reveals the retriever is mediocre. If the retrieved Reddit comment is irrelevant, then the claim of \"integrating the linguistic diversity of real Reddit discussions\" falls flat. The generation becomes simple paraphrasing-with-a-prompt rather than a true synthesis."}, "questions": {"value": "Given the low relevance score for the retrieved Reddit comments (3.31/5), how can you be sure the final dataset genuinely captures Reddit's linguistic diversity, rather than GPT-4o's simulation of it? Could you provide an analysis of how much the generator actually uses the retrieved comment, especially in cases where its relevance is low?\n\nHave you considered the potential for evaluation contamination, where models are benchmarked on data generated by a close relative (or even the same underlying architecture)? Does this risk rewarding models that simply mimic the generator's style over models that perform genuine semantic reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6UA7EHMt7", "forum": "dyooGJcKJg", "replyto": "dyooGJcKJg", "signatures": ["ICLR.cc/2026/Conference/Submission16906/Reviewer_nZ2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16906/Reviewer_nZ2F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625861996, "cdate": 1761625861996, "tmdate": 1762926935801, "mdate": 1762926935801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose PerSpectra, a large-scale, curated dataset combining Kialo's debate structure, which contains pro and con opinions on 100 different topics, and comments from Reddit. This dataset is extended with statements generated by large language models (LLMs). They also introduce three tasks that can be solved using this dataset: opinion counting, opinion matching, and polarity checking."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow. The data curation process and the proposed tasks and metrics are clearly described and useful for comparing the capability of large language models to recognize plurality of opinions."}, "weaknesses": {"value": "The authors claim that this dataset differs from others that focus on pluralistic opinions on various topics because the process did not require extensive human annotation. However, annotating a sub-sample revealed the possibility of selecting statements that do not fit the topic, which compromises the data's quality. Human annotation is therefore indispensable. Nevertheless, this dataset is a good starting point for further research and offers an interesting approach to obtaining debate data."}, "questions": {"value": "No questions left.\nSmall comments:\n- 042: The abbreviation for LLMs is introduced in the Related work section, but should already be introduced in the Introduction.\n- 059, 062-063: Some of the citations in the introduction that are written out in the text should be put in parentheses using the command \"\\citep.\"\n- 119: maybe you meant LLMs instead of LMs?\n- 244: Small typo in \"Writting\" (instead of Writing)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W0P01KuYZo", "forum": "dyooGJcKJg", "replyto": "dyooGJcKJg", "signatures": ["ICLR.cc/2026/Conference/Submission16906/Reviewer_uHvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16906/Reviewer_uHvT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914076550, "cdate": 1761914076550, "tmdate": 1762926935005, "mdate": 1762926935005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Defines pluralism as the ability to represent multiple valid perspectives.\n - Argues that debate-oriented data (Reddit, Kialo) naturally capture pluralism but each has limitations: Reddit is linguistically diverse yet unstructured, while Kialo is structured but too concise.\n - Introduces Perspectra, a benchmark that merges Reddit’s linguistic diversity with Kialo’s argument-graph structure.\n - Establishes three evaluation tasks:\n\t- Opinion counting — detecting distinct viewpoints.\n\t- Opinion matching — aligning discourse to original stances.\n\t- Polarity check — inferring the overall stance in mixed text."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "LLMs are increasingly used in sensitive contexts and naturally plurality of opinions is inherent to the problem. We need a lot of work to define new goal posts in this direction, like this work."}, "weaknesses": {"value": "The \"Opinion Counting\" (estimating how many distinct opinions are present within a paragraph) is a somewhat ambiguous. Like you the definitions of what constitutes as a distinct opinions can be subjective and can vary from one person to another. The same also holds for the other tasks (matching and polarity). \n\nIn other words, why should I (or any reader) trust that: \n - you have high-quality annotations? \n - and that the task is well-defined? \n\nI do see that you conduct human annotations to ensure data quality. Have you tried computing the human performance for each of your tasks? (counting, matching, polarity). Having these numbers can help inform the reader about data quality vs ambiguity."}, "questions": {"value": "I'd like to hear your perspective on other benchmarks that have been release on pluralistic alignment. \n\n - [1] CoSApien https://huggingface.co/datasets/microsoft/CoSApien https://arxiv.org/abs/2410.08968 \n - [2] MOREBENCH https://huggingface.co/datasets/morebench/morebench https://www.arxiv.org/abs/2510.16380 \n - [3] https://huggingface.co/datasets/facebook/community-alignment-dataset  https://arxiv.org/abs/2507.09650"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xmBMmJ41XD", "forum": "dyooGJcKJg", "replyto": "dyooGJcKJg", "signatures": ["ICLR.cc/2026/Conference/Submission16906/Reviewer_A1ew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16906/Reviewer_A1ew"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944989934, "cdate": 1761944989934, "tmdate": 1762926934661, "mdate": 1762926934661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}