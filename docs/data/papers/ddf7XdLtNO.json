{"id": "ddf7XdLtNO", "number": 10355, "cdate": 1758167999123, "mdate": 1759897656387, "content": {"title": "Sequence Length Matters in Data Scheduling for Accelerating Language Model Pretraining", "abstract": "Pretraining large language models (LLMs) is computationally intensive, often requiring billions of tokens and extensive compute to achieve competitive performance. Although recent advances in data selection have shown improvement on training efficiency, it is challenging for these methods to consistently maintain the promise in the context of the scaling law. In this work, we dive into the impact of sequence length with different linguistic structures and semantic continuity on language model pretraining, and propose a length-based online data scheduling method to accelerate the procedure. Specifically, we design a two-stage dense-balanced sequence prioritization framework for pretraining: 1) at the first stage, the model is exposed to uniform-length dense token batches to encourage the formation of global language representations; 2) the second stage incorporates variable-length sequences, which reinforces learned abstractions while significantly reducing the total number of training iterations. We hypothesize and prove that the model internalizes the foundational language knowledge during the dense-token phase, allowing it to optimize more efficiently the latter variable-length sequences. Empirical results show that our approach achieves comparable perplexity to standard pretraining while requiring substantially fewer optimization steps,  pinpointing a promising way to reduce the computational burden of LLM pretraining.", "tldr": "We introduce a two-stage pretraining framework that leverages dense-balanced sequence length progression to accelerate language model pretraining", "keywords": ["Sequence Length", "Data Scheduling", "Acceleration", "Language Model Pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac7a5239cd4ee475db315d77d25a050902916a92.pdf", "supplementary_material": "/attachment/c061463a5bf27ce7a6c0aa5ea1765e262dd9a84c.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic study on the role of sequence length in LLM pretraining, using Token Utilization Rate (TUR)to measure the effective use of tokens under different sequence-length and data-packing strategies. Building on empirical observations of gradient variance and loss dynamics, the authors propose a simple yet effective two-stage data scheduling framework: the dense stage first trains on fixed-length batches to maximize token utilization and accelerate early convergence, while the balanced stage later applies loss-based dynamic sampling to mitigate length bias and balance learning across different sequence lengths. Experiments demonstrate that DBSP can significantly accelerate pretraining (by up to 40% fewer iterations) without sacrificing performance or generalization across lengths, offering a new perspective on efficient data scheduling for LLM training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed DBSP framework achieves faster perplexity convergence compared with prior reference-free online data selection methods, showing clear training efficiency gains under the same compute budget.\n2. The empirical observations in Figure 2 convincingly illustrate the correlation between sequence length, gradient norm, and training dynamics, providing strong intuition for the two-stage (Dense → Balanced) scheduling strategy.\n3. The paper complements its empirical findings with theoretical analysis (Section 3.3 & Appendix C), formalizing how long-sequence sampling reduces gradient variance and how loss-based sampling amplifies effective gradients, thus explaining the curriculum-like benefits of the method."}, "weaknesses": {"value": "1. Although the authors emphasize fairness by comparing DBSP only with reference-free online data selection methods, this design choice raises a concern. If reference-based selection approaches outperform DBSP, the practical contribution of this paper becomes less clear.\nIt would strengthen the paper to include such baselines or at least discuss the trade-offs in terms of ppl, training time, computational cost, and resource efficiency, to better justify the necessity of a reference-free approach.\n2.  Figures 2 and 3 are referenced multiple times before they actually appear in the paper, requiring readers to scroll back and forth.\nReorganizing the figures to appear closer to their first mention would improve readability and narrative flow.\n3.  Lemma 2 requires stronger correlation assumptions or explicit proofs (e.g., monotonicity or rank correlation bounds) to justify the expectation inequality $E_{\\pi} \\ge E_{D}$\n\nI am willing to increase the score during the rebuttal."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "03TLWZg9jl", "forum": "ddf7XdLtNO", "replyto": "ddf7XdLtNO", "signatures": ["ICLR.cc/2026/Conference/Submission10355/Reviewer_BD9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10355/Reviewer_BD9d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737887871, "cdate": 1761737887871, "tmdate": 1762921683617, "mdate": 1762921683617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores the token efficiency of language model pre-training from a sequence length perspective. It begins by presenting several observations, followed by a proposed method. First, it shows that batching sequences without padding (called a dense batch) leads to more efficient learning than using padding (where sequences of different lengths are padded to the same length before batching). It also shows that gradient variance is lower when using dense batches compared to padded ones. Additionally, a pretrained model shows lower loss when evaluated on sequence lengths similar to those it was trained on. Motivated by these findings, the paper proposes Dense-Balanced Sequence Prioritization (DBSP), a two-stage training method: first training on dense batches, then sampling from bins of specific sequence lengths. The sampling probability is proportional to the model’s loss on each bin (measured using a small calibration set). It is argued that this sampling roughly follows importance sampling based on gradient norms, leading to better convergence in stochastic optimization. Empirical results support the claim, showing up to $1.7\\times$ faster convergence compared to random sampling and other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The length-based analysis of LLM pre-training is an interesting research area, and the paper provides useful observations in this direction.\n- The empirical results support the faster convergence claim, showing up to 1.7× speedup for a 1B model based on validation perplexity metrics. Further, preliminary results on a 7B model are provided supporting efficacy of the proposed method at that scale (at 100B token count).\n- The paper includes several relevant baselines from prior work.\n- Training details are provided in the appendix, enabling reproduction of the results."}, "weaknesses": {"value": "- It is not clear in the second stage of training how the probabilities of different bins differ or change during training. For example, what if sequences are uniformly sampled from all bins (a fixed probability for each bin)? Does that make the method worse? An additional ablation demonstrating the importance of the specific proposed sampling in the second phase is needed. Prior works have considered different curricula, e.g., short-to-long as in [3,4,5] or a cycling strategy as in [4] (also used as a baseline here). Is there a benefit to dynamically changing the sampling probability?\n- Some observations, while interesting, are not new. For example, that a dense batch is better than a padded batch is expected, since with a fixed token budget padded batches waste compute on padding tokens. Additionally, [1,2] show that an increase in truncation hurts model performance (i.e., no truncation, as in dense batches, is preferred). The observation about the importance of the sequence-length domain gap between training and test time (Fig. 2c) has also been reported before, e.g., in [4,6].\n- Some important details seem deferred to the appendix. For example, in Figure 5, what is the maximum sequence length of the Random baseline? What is the average sequence length when using Random? Why is the perplexity in Fig. 5a for DBSP initially significantly larger than the Random baseline? Another detail that would be helpful to include in the main body is how much training budget should be allocated to stage 1 vs. stage 2.\n- The significance of Theorem 1 is not clear. It gives an upper bound on the minimum of the expected gradient norm throughout training. What conclusion follows from this bound? Please also clarify the assumptions. In Assumption 1, it is assumed that the token-level gradient has bounded variance $\\sigma_{tok}^2$ and is independent across token positions. Over what distribution is the gradient-norm variance computed (token distribution? model-parameter distribution during training?)? Also, when referring to the gradient norm in Section 3.1, how is it computed? Is it the average of the gradient norms of all model parameters over a randomly picked batch?\n- In Section 4.2, the comparison with Dataset Decomposition [4] is not clear. It appears [4] considers multiple length-based curricula. Which one is used here? In [4], sequence lengths vary by bucket, yielding training time compute savings, whereas here all sequences are padded to the same length. Therefore, comparing perplexity at the same number of iterations is not fair, since [4] needs less compute for a given number of iterations.\n\n[1] Ding, Hantian, et al. \"Fewer truncations improve language modeling.\" arXiv preprint arXiv:2404.10830 (2024).\n\n[2] Zhao, Yu, et al. \"Analysing the impact of sequence composition on language model pre-training.\" arXiv preprint arXiv:2402.13991 (2024).\n\n[3] Zhu, Tongyao, et al. \"SkyLadder: Better and Faster Pretraining via Context Window Scheduling.\" arXiv preprint arXiv:2503.15450 (2025).\n\n[4] Pouransari, Hadi, et al. \"Dataset decomposition: Faster llm training with variable sequence length curriculum.\" Advances in Neural Information Processing Systems 37 (2024): 36121-36147.\n\n[5] Jin, Hongye, et al. \"Growlength: Accelerating LLMs pretraining by progressively growing training length, 2023.\" URL https://arxiv.org/abs/2310.00576.\n\n[6] Anil, Cem, et al. \"Exploring length generalization in large language models.\" Advances in Neural Information Processing Systems 35 (2022): 38546-38556."}, "questions": {"value": "- For the TUR definition (Eq. 2), is the maximum possible value $L_B/2$? If so, please state this in the paper.\n- In Eq. 3, how is $r_k$ computed? It is said to be the proportion of the $k$-th length bin (in the calibration data). Is this based on the number of sequences in that bin, or on the total number of tokens?\n- How should $L_d$ be chosen in general? The ablation in Fig. 5d suggests the best range is 128–256. why is that the case? Please provide intuition.\n- In Algorithm 2, it appears $L_d$ is gradually increased to the largest length. Please add a short description of Algorithm 2, explain how it differs from Algorithm 1, and why it is more efficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rJS7d1Lyg5", "forum": "ddf7XdLtNO", "replyto": "ddf7XdLtNO", "signatures": ["ICLR.cc/2026/Conference/Submission10355/Reviewer_r6GJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10355/Reviewer_r6GJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860202465, "cdate": 1761860202465, "tmdate": 1762921683304, "mdate": 1762921683304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DBSP to accelerate the pretraining of large language models by optimizing sequence length scheduling in training data. Its core approach involves a two-stage training strategy: First, the model is trained on dense batches composed of uniformly long sequences to maximize the utilization of effective tokens within each batch, thereby rapidly learning foundational language representations. Subsequently, the model transitions to balanced batches containing sequences of varying lengths. A calibration set dynamically adjusts the sampling probability across length intervals to correct potential length biases introduced in the first stage. Both theoretically and experimentally, this approach significantly reduces the number of training steps required to achieve target performance without compromising the model's capability on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Experimental data indicates that this method achieves the target perplexity on the LLaMA-1B model with 40% fewer training iterations than random sampling, demonstrating its acceleration effect.\n\n2. The evaluation encompasses models of varying parameter scales (from 60M to 7B) and multiple datasets (C4, SlimPajama), using pre-training perplexity and downstream task accuracy as metrics. Results are compared against various baseline methods.\n\n3. The paper highlights that its methodology differs fundamentally from purely length-based learning approaches (such as Dataset Decomposition) by addressing and resolving model bias issues arising from length uniformity within batches."}, "weaknesses": {"value": "1. This paper primarily attributes the data scheduling problem to sequence length, yet fails to adequately explore potential interactions between other intrinsic data attributes (e.g., semantic difficulty and domain distribution). This oversight may cast doubt on the generalization capabilities of its method in more complex data scenarios.\n\n2. The downstream task evaluation might not explicitly demonstrate improvements in the model's “long-context understanding.” We recommend that the authors incorporate benchmarks requiring long-document comprehension or long-sequence reasoning.\n\n3. There is a highly relevant paper titled Beyond Fixed Length: Bucket Pre-training is All You Need (IJCAI 2025). This paper similarly identifies the limitations of fixed-length training and proposes a multi-bucket data organization strategy to optimize data utilization. Both papers share considerable similarity in their research questions and core objectives, and both adopt the fundamental technical approach of bucket segmentation. It is recommended that the authors conduct a more in-depth comparative analysis of these two categories of methods."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j4bmXJV1VU", "forum": "ddf7XdLtNO", "replyto": "ddf7XdLtNO", "signatures": ["ICLR.cc/2026/Conference/Submission10355/Reviewer_A9na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10355/Reviewer_A9na"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901885976, "cdate": 1761901885976, "tmdate": 1762921682977, "mdate": 1762921682977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}