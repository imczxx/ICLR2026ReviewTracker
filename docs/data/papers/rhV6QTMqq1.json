{"id": "rhV6QTMqq1", "number": 24231, "cdate": 1758354424313, "mdate": 1762943740903, "content": {"title": "Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful approach for strengthening the reasoning capabilities of large language models (LLMs). Among existing algorithms, Group Relative Policy Optimization (GRPO) has demonstrated strong performance, yet it suffers from a critical issue: low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes. This imbalance leads to unstable training and suppresses the contribution of high-probability tokens that are more reliable for learning. In this work, we introduce **Token-Regulated Group Relative Policy Optimization (TR-GRPO)**, a simple yet effective extension of GRPO that assigns token-level weights positively correlated with the model’s predicted probability. By downweighting low-probability tokens and emphasizing high-probability ones, TR-GRPO mitigates gradient over-amplification while preserving informative learning signals. We provide theoretical analysis to show how token-level probability governs gradient norms which motivates our weighting design. Extensive experiments demonstrate that TR-GRPO consistently outperforms GRPO across RLVR tasks—including logic, math, and agentic reasoning—highlighting the importance of regulating token contributions during RL training and establishing TR-GRPO as a robust framework for enhancing LLM reasoning.", "tldr": "We propose Token-Regulated GRPO, a reinforcement learning method that downweights low-probability tokens to stabilize training and consistently outperform GRPO across logic, math, and agentic reasoning tasks.", "keywords": ["Large Language Models", "Reinforcement Learning", "Reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/5c882182209e73f51a5857ec3923a1fa5a64fbec.pdf", "supplementary_material": "/attachment/12a1804032f3a361b8ed9642406354121a27bfad.zip"}, "replies": [{"content": {"summary": {"value": "The authors claim to identify a critical issue—namely, that low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes—and propose a token-regulated method that downweights low-probability tokens while emphasizing high-probability ones.\n\nHowever, neither the finding nor the proposed approach appears original. The manuscript’s theoretical analysis and methodology substantially overlap with prior work [1], yet this overlap is not properly acknowledged; the authors cite [1] only in the experimental setup, stating that they follow its settings.\n\n[1] Yang et al. Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs. AI4MATH Workshop at ICML 2025."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The theoretical analysis presents some improvements over [1]. However, there are notational inconsistencies, and the analysis appears to be inspired by [1] without appropriate citation.\n\n2. The experimental results—particularly on math-related datasets—outperform [1]. Nonetheless, [1] is not treated as a baseline."}, "weaknesses": {"value": "1. Theorem 3.1 appears very close to Proposition 4.2 in [1], yet is not properly cited.\n\n2. The proposed “Token-Regulated” method closely mirrors the “Advantage Reweighting” approach in [1], again without proper citation. A similar sigmoid-based reweighting method also appears in [1]’s public codebase.\n\n3. A significant technical flaw concerns the regulation of the KL divergence term. After applying the proposed regulation, the KL divergence between π_θ and π_ref is nonzero even when the two distributions are identical. In addition, the expression for w_{i,t} is unnecessarily complex and lacks theoretical justification.\n\n4. The notation in Theorems 3.1 and 3.2 is confusing; it appears that α should be a and β should be b."}, "questions": {"value": "1. Please clarify the relationship between your work and [1]. Many components appear highly similar—including even the color schemes in figures—yet [1] is cited only in the experimental setup.\n\n2. Why is the KL divergence term also regulated? What theoretical or empirical benefits does this provide? From a theoretical standpoint, this appears inappropriate.\n\n3. Please provide direct experimental comparisons with [1]. Given the similarities, a proper head-to-head comparison is necessary."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The manuscript raises substantial concerns regarding originality and attribution. Key ideas, theoretical results, and methodological components appear to closely track [1] without adequate citation or clear differentiation.\n\nSpecifically:\n\n1. Abstract overlap: The paper states that “low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes” (lines 016–020), which is nearly identical to [1]’s abstract: “low-probability tokens disproportionately influence model updates due to their large gradient magnitudes.” This constitutes the central finding of [1] and is presented here without clear attribution.\n\n\n2. Theoretical overlap: Theorem 3.1 (lines 191–201) is highly similar to Proposition 4.2 in [1], with nearly identical notation. The proof sketch (lines 1011–1021) follows the proof outline of Proposition 4.2 in [1]. In addition, Eq. (7) matches Eq. (2) in [1] (including notation). These parallels are not properly cited, and the text appears to claim them as original contributions, which raises concerns of idea plagiarism.\n\n\n3. Methodological overlap: The proposed “Token-Regulated” method (Eq. (9)) is almost the same as “Advantage Reweighting” in [1]. The primary differences are the specific weight form (Eq. (11)) and an additional term regulating the KL-divergence (Eq. (10)), which lacks theoretical justification. A similar weight form to Eq. (11) also appears in [1]’s public codebase.\n\n4. Presentation overlap: Several figures and tables adopt formats that are nearly identical to those in [1].\n\nTaken together, the evidence suggests the authors are well aware of [1], yet [1] is cited only to state that its experimental settings are followed. This level of acknowledgment is far below standard scholarly practice for attribution.\n\nConclusion: The manuscript appears to constitute idea plagiarism of [1], or potentially a dual submission of [1]. While the latter seems unlikely, the current version does not meet scholarly norms for originality and attribution."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q1m5XQNVXk", "forum": "rhV6QTMqq1", "replyto": "rhV6QTMqq1", "signatures": ["ICLR.cc/2026/Conference/Submission24231/Reviewer_QQ5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24231/Reviewer_QQ5E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760668697478, "cdate": 1760668697478, "tmdate": 1762943008120, "mdate": 1762943008120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "KIErkL0vVP", "forum": "rhV6QTMqq1", "replyto": "rhV6QTMqq1", "signatures": ["ICLR.cc/2026/Conference/Submission24231/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24231/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943739864, "cdate": 1762943739864, "tmdate": 1762943739864, "mdate": 1762943739864, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TR-GRPO, a probability-aware token reweighting scheme atop GRPO to dampen gradients from low-probability tokens and emphasize high-probability ones, reporting sizable gains on logic, math, and agentic RLVR tasks. However, its framing, theoretical motivation, and even experimental setup appear substantially overlapping with “Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs, \tarXiv:2505.12929” which already identifies the same dominance issue and introduces Advantage Reweighting and Lopti; the submission does not clearly position itself against that work."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear problem statement and intuitive fix; easy to implement in existing GRPO pipelines. \n2. Consistent improvements across K&K logic, math suites, and agentic QA, with smoother training rewards."}, "weaknesses": {"value": "1. Substantial overlap with prior work: same core phenomenon (low-probability tokens dominate updates), highly similar motivation and claims, and close experimental regimes (K&K, Minerva/MATH, AIME/AMC), yet no direct, head-to-head comparison. \n2. Unclear novelty boundary: TR-GRPO's token weighting may be functionally similar to Advantage Reweighting and the two-stage Lopti schedule; equivalence or strict improvements are not established."}, "questions": {"value": "1. Please explicitly compare with “Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs”: what is new beyond its phenomenon, proofs, and methods (Advantage Reweighting, Lopti)?  I also find similar experiment setup in supplementary materials."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "found that this paper's background, motivation, proof, and methodology, as well as the code provided in the supplementary materials, are highly similar to *Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs*(arXiv:2505.12929). However, the authors did not detail in the paper the relationship and differences between this work and arXiv:2505.12929."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0E594h5RAW", "forum": "rhV6QTMqq1", "replyto": "rhV6QTMqq1", "signatures": ["ICLR.cc/2026/Conference/Submission24231/Reviewer_LBTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24231/Reviewer_LBTX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937567474, "cdate": 1761937567474, "tmdate": 1762943007900, "mdate": 1762943007900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper observes that in GRPO training, low-probability tokens tend to dominate gradient updates, causing instability. To address this, the authors propose a strategy-grouped update that partitions tokens by probability and updates them separately to balance learning signals. Experiments on math and logic puzzle tasks demonstrate more stable training and improved overall performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "See weaknesses."}, "weaknesses": {"value": "There appear to be concerns regarding the originality of this submission. The paper’s motivation, targeted problem, theoretical formulation, and even specific theorem statements are almost identical to those presented in “Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs” (arXiv:2505.12929). The resemblance extends beyond conceptual similarity: the theoretical derivations are nearly identical, and large portions of the codebase, including file structures and even the Conda environment name, appear to be copied from that work. Given the extent of textual, mathematical, and implementation overlap, this submission raises serious concerns about academic integrity and originality. I recommend that the program chairs carefully investigate this potential case of plagiarism before further consideration."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "There appear to be concerns regarding the originality of this submission. The paper’s motivation, targeted problem, theoretical formulation, and even specific theorem statements are almost identical to those presented in “Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs” (arXiv:2505.12929). The resemblance extends beyond conceptual similarity: the theoretical derivations are nearly identical, and large portions of the codebase, including file structures and even the Conda environment name, appear to be copied from that work. Given the extent of textual, mathematical, and implementation overlap, this submission raises serious concerns about academic integrity and originality. I recommend that the program chairs carefully investigate this potential case of plagiarism before further consideration."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rt3hMKnY3u", "forum": "rhV6QTMqq1", "replyto": "rhV6QTMqq1", "signatures": ["ICLR.cc/2026/Conference/Submission24231/Reviewer_x5ev"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24231/Reviewer_x5ev"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974779090, "cdate": 1761974779090, "tmdate": 1762943007611, "mdate": 1762943007611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is well motivated. However, the contribution of the proposed appraoch is not sound."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's core motivation is exceptionally well-justified. The theoretical analysis (Theorem 3.1), which demonstrates that gradient norms scale with $1 - \\pi_\\theta(o_t)$, coupled with the empirical observation (Figure 1) that high-probability tokens are often critical logical or mathematical symbols, presents a clear and compelling case for intervention.\n\n2. The experimental results across logic, math, and agentic tasks are extensive and demonstrate clear, consistent improvements over the GRPO baseline. The inclusion of a \"Reverse\" ablation study is particularly effective in validating the core hypothesis."}, "weaknesses": {"value": "1. The proposed solution—the design of the weighting function—feels somewhat heuristic and lacks a strong theoretical derivation. While the choice of a sigmoid function and subsequent clipping is empirically effective, it appears as an engineered solution. The paper would be significantly strengthened by providing a more principled justification for this specific functional form, moving beyond the rationale that it is merely a smooth, increasing function of the token probability.\n\n2. The paper's empirical analysis is primarily positioned against the established GRPO baseline and omits comparisons with other recent state-of-the-art RL methods for LLMs, such as DAPO. To fully establish TR-GRPO's contribution and competitiveness, it is crucial to include benchmarks against these other advanced optimizers. Such comparisons would provide a clearer understanding of its relative advantages and disadvantages within the current methodological landscape.\n\n3. The proposed method modifies the objective by applying the token weight $w(\\pi_\\theta(o_t))$ directly to the importance ratio inside the $\\min$ and $\\text{clip}$ operations (Eq. 9). This approach intertwines the new weighting scheme with PPO's fundamental clipping mechanism, which was originally designed to constrain policy updates based on the unweighted importance ratio $r_t(\\theta)$. The paper does not sufficiently analyze how this interaction affects the stability guarantees of the underlying PPO/GRPO algorithm. It is plausible that scaling the ratio by a weight $w > 1$ could push a larger fraction of tokens into the clipped region, potentially altering the optimization dynamics in unintended ways. A deeper discussion or analysis of this interaction is necessary to assure readers of the method's stability and theoretical soundness."}, "questions": {"value": "How does the interaction (between the new weighting scheme and PPO's fundamental clipping mechanism) affects the stability guarantees of the underlying PPO/GRPO algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V1G0iwJUXb", "forum": "rhV6QTMqq1", "replyto": "rhV6QTMqq1", "signatures": ["ICLR.cc/2026/Conference/Submission24231/Reviewer_S2cj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24231/Reviewer_S2cj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24231/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002991357, "cdate": 1762002991357, "tmdate": 1762943007280, "mdate": 1762943007280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}