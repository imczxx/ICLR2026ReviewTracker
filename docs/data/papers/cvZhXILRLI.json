{"id": "cvZhXILRLI", "number": 16144, "cdate": 1758260607755, "mdate": 1763377818012, "content": {"title": "Bayesian Neural Networks for Functional ANOVA Model", "abstract": "With the increasing demand for interpretability in machine learning, functional ANOVA decomposition has gained renewed attention as a principled tool for breaking down high-dimensional function into low-dimensional components that reveal the contributions of different variable groups.\nRecently, Tensor Product Neural Network (TPNN) has been developed and applied as basis functions in the functional ANOVA model, referred to as ANOVA-TPNN.\nA disadvantage of ANOVA-TPNN, however, is that the components to be estimated must be specified in advance, which makes it difficult to incorporate higher-order TPNNs into the functional ANOVA model due to computational and memory constraints.\nIn this work, we propose Bayesian-TPNN, a Bayesian inference procedure for the functional ANOVA model with TPNN basis functions, enabling the detection of higher-order components with reduced computational cost compared to ANOVA-TPNN.\nWe develop an efficient MCMC algorithm and demonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark datasets.\nTheoretically, we prove that the posterior of Bayesian-TPNN is consistent.", "tldr": "We propose an interpretable Bayesian neural network based on the functional ANOVA model.", "keywords": ["Bayesian Machine Learning", "Functional ANOVA Model", "Bayesian Neural Networks", "Interpretable AI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96b073fe30fff94bffe5b5c14861ad770c17bc83.pdf", "supplementary_material": "/attachment/bd0320fa8d32cd22fc710e0eac4d9ea9a3cb0e09.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates functional ANOVA decomposition, a technique for decomposing a high dimensional complex function into lower dimensional components to enhance model interpretability. Existing methods typically require pre specifying the maximum order of interaction in the low dimensional functions and exhaustively considering all feature combinations up to that order. In practice, this often limits analysis to pairwise interaction terms. To address this limitation, the authors propose Bayesian TPNN, a Bayesian framework that adaptively selects the most relevant features for inclusion in the low dimensional components without pre-specifying the maximum order. The method is evaluated across diverse domains, including real world tabular datasets, simulated data, and toy image datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well written and easy to follow. Using a Bayesian approach to select features for functional ANOVA decomposition provides a theoretically principled framework that could be of significant interest to the interpretability community and potentially a broader audience."}, "weaknesses": {"value": "My major concern is its evaluation and experiments. Most experiments are conducted on toy or tabular datasets, where the application of deep learning is arguably less compelling. Please refer to my detailed questions below."}, "questions": {"value": "1. In Table 1, the authors compare the proposed Bayesian TPNN approach with existing methods in terms of prediction accuracy. However, the proposed method does not appear to outperform existing baselines significantly, particularly when taking the standard errors into account.\n\n2. In Table 2, the authors evaluate uncertainty quantification against existing methods. It would strengthen the comparison to include deep ensembles, which are widely recognized as a standard baseline for uncertainty estimation.\n\n3. Most experiments are conducted on simulated or tabular datasets, with the exception of Section 4.4, which uses CelebA HQ and Catdog datasets. However, in these cases, the approach relies on another CBM model to generate interpretable concepts. Could the authors apply Bayesian TPNN directly to these datasets and produce interpretations for the low dimensional functions?\n\nOverall, it is challenging to identify practical scenarios where the proposed method would be preferred. I recommend extending the evaluation to additional domains, such as the genomics datasets used in Martens & Yau (2020), to better demonstrate applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AYc5GvvQ1O", "forum": "cvZhXILRLI", "replyto": "cvZhXILRLI", "signatures": ["ICLR.cc/2026/Conference/Submission16144/Reviewer_hVKC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16144/Reviewer_hVKC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139635663, "cdate": 1761139635663, "tmdate": 1762926310947, "mdate": 1762926310947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a Bayesian method for estimating a Tensor Product Neural Network, a method for estimating a functional ANOVA with a neural net proposed at ICML this year.\nAuthors develop an MCMC algo using Langevin dynamics for continuous parameters and bespoke proposals for the discrete ones."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The article is logically organized and easy to follow overall. It's clear what the goals are. From a structural perspective this piece is very well written.\n\nThe contributions of the article are clear, and the proposed methodology is thoroughly investigated.\n\nThe numerical experiments seem adequate to me; they use several datasets and compare against a reasonable suite of alternative methods.\nFor instance, BART is a tough competitor, and matching/beating it while providing for high interpretability is notable.\n\nThough primarily a computation/applied article, the authors prove a basic asymptotic result of their method."}, "weaknesses": {"value": "The biggest weakness of this article is that this work is fundamentally incremental: it is a straightforward Bayesian version of an existing method, for which the Bayesian inference is straightforward.\n\nSome of the limitations of Bayesian inference in the neural setting, notably the lack of scalability due to the nonexistence of a stochaastic version of the MH algorithm, are not addressed.\nDiscussion of how to mitigate this would have improved the article.\n\nNeeds spellchecking and grammar correction."}, "questions": {"value": "I think this work was very clearly presented and the motivation for the method itself is clear, so I don't have any questions, but I invite the authors to nevertheless answer:\n1) What did I get wrong in my review?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r96UotRvbe", "forum": "cvZhXILRLI", "replyto": "cvZhXILRLI", "signatures": ["ICLR.cc/2026/Conference/Submission16144/Reviewer_Jnxk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16144/Reviewer_Jnxk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941969775, "cdate": 1761941969775, "tmdate": 1762926310503, "mdate": 1762926310503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bayesian Tensor Product Neural Networks (Bayesian-TPNN), a Bayesian neural network architecture for the functional ANOVA model. The key innovation is incorporating Bayesian inference over both network parameters and architectures (i.e., the subsets of input variables forming each component). This allows efficient detection of higher-order interactions without predefining component structures, addressing the scalability issue of ANOVA-TPNN (Park et al., 2025), whose number of sub-networks grows exponentially with interaction order.\nThe authors design a specialized MCMC algorithm that alternates between growing/pruning architectures and updating continuous parameters via Langevin proposals. Theoretically, the paper establishes posterior consistency for both the overall regression function and its individual ANOVA components.\nEmpirically, Bayesian-TPNN is evaluated on eight real tabular datasets, synthetic benchmarks, and concept bottleneck image tasks. It delivers comparable or superior predictive accuracy to both interpretable and black-box baselines (NAM, BART, XGB, mBNN) and significantly better uncertainty quantification and component selection, particularly for higher-order interactions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation and significance: the paper tackles the long-standing scalability bottleneck of functional ANOVA neural models and provides a principled Bayesian alternative capable of learning higher-order terms without combinatorial explosion.\n\n- Novel integration of architecture learning into ANOVA-structured modes: treating subsets of input variables as random variables and exploring them via reversible-jump-style MCMC is a clever idea. The stepwise proposal mechanism guided by feature importance is intuitive and empirically validated.\n\n- The proof of posterior consistency for both the global function and each ANOVA component is nontrivial and strengthens the paperâ€™s soundness."}, "weaknesses": {"value": "- The paper spends many pages detailing MCMC updates and proofs but offers limited high-level intuition on why the proposed priors or proposal mechanisms work. As I'm not an expert of XAI and ANOVA, more discussion on background and a small synthetic visual example illustrating architecture evolution would help accessibility.\n\n- While results are comprehensive, the gains in predictive accuracy are modest compared to ANOVA-TPNN, and the improvements in uncertainty quantification, though consistent, are small in magnitude. It would strengthen the empirical narrative to include a more demanding high-dimensional or noisy regime where the Bayesian approach truly shines.\n\nMinors:\n- Some figures (e.g., component plots) and tables are crowded or use small legends.\n\n- The exposition occasionally repeats prior work descriptions or defers too much to appendices.\n\n- The paper would benefit from clearer separation between methodological explanation and algorithmic detail."}, "questions": {"value": "Could the proposed method scale to higher-order input, like p=4 or 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ipdLMpYv2E", "forum": "cvZhXILRLI", "replyto": "cvZhXILRLI", "signatures": ["ICLR.cc/2026/Conference/Submission16144/Reviewer_7sy8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16144/Reviewer_7sy8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987633952, "cdate": 1761987633952, "tmdate": 1762926310080, "mdate": 1762926310080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}