{"id": "HQ4VmeEzV0", "number": 13202, "cdate": 1758215051090, "mdate": 1759897456741, "content": {"title": "PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data", "abstract": "Off-policy evaluation (OPE) methods estimate the value of a new reinforcement learning (RL) policy prior to deployment. Recent advances have shown that leveraging auxiliary datasets, such as those synthesized by generative models, can improve the accuracy of OPE. Unfortunately, such auxiliary datasets may also be biased, and existing methods for using data augmentation within OPE in RL lack principled uncertainty quantification. In high stakes settings like healthcare, reliable uncertainty estimates are important for ensuring safe and informed deployment. In this work, we propose two methods to construct valid confidence intervals for OPE when using data augmentation. The first provides a confidence interval over $V^{\\pi}(s)$, the policy performance conditioned on an initial state $s$. To do so we introduce a new conformal prediction method suitable for Markov Decision Processes (MDPs) with high-dimensional state spaces. Second, we consider the more common task of estimating the average policy performance over many initial states, $V^{\\pi}$; we introduce a method that draws on ideas from doubly robust estimation and prediction powered inference. Across simulators spanning robotics, healthcare and inventory management, and a real healthcare dataset from MIMIC-IV, we find that our methods can effectively leverage auxiliary data and consistently produce confidence intervals that cover the ground truth policy values, unlike previously proposed methods. Our work enables a future in which OPE can provide rigorous uncertainty estimates for high-stakes domains.", "tldr": "We introduce two methods to construct valid confidence intervals for OPE methods that use both real and synthetic data.", "keywords": ["policy evaluation", "confidence intervals", "conformal prediction"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e13752d60556e1d503c55b9c651e7a5fc6a377ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes two methods that use auxiliary data to construct confidence intervals for off-policy evaluation: CP-Gen and DR-PPI. The authors provide theoretical guarantees to demonstrate that both methods provide valid confidence intervals with a high probability. They validate their approaches empirically across four domains, comparing their approaches to baseline methods including importance sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies a significant problem in OPE. Providing reliable CIs for policies with biased data is important and meaningful in many real world domains.\n\n2. The paper provides solid theoretical foundations for their methods under a few assumptions.\n\n3. The empirical comparisons across multiple domains with basline approaches show the effectiveness of their approaches."}, "weaknesses": {"value": "1. Three assumptions in the theory might be strong and the authors do not seem to provide convincing arguments. Specifically, I'm not sure how the fact that policy probabilities lie in [0,1] guarantees Assumption 1 (Line 335). The authors also acknowledge that Assumption 3 is a strong assumption. \n\n2. The CP-Gen algorithm introduces two hyperparameters $\\epsilon_s$ and $\\epsilon_r$, which are not standard in OPE. The authors do not provide a principled selection rule for these hyperparameters."}, "questions": {"value": "1. How do I understand Table 2? I find the comparison unfair since different methods have different length of CI. It's trivial that DM and $V^{\\pi_e}$ have high probability to be biased since they only provide a number; while DR-PPI have much larger CIs. A fair comparison should set the interval length fixed and compare the probability.\n\n2. Can you get probability bounds for your baseline methods? I will evaluate your paper to be much stronger if you can discuss the tradeoff of CI length and valid probability, and show your methods achieve better tradeoffs than baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JhoHWMOsQ1", "forum": "HQ4VmeEzV0", "replyto": "HQ4VmeEzV0", "signatures": ["ICLR.cc/2026/Conference/Submission13202/Reviewer_uFsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13202/Reviewer_uFsk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598540816, "cdate": 1761598540816, "tmdate": 1762923896187, "mdate": 1762923896187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for off-policy evaluation that can ensure confidence intervals on high-dimensional and long horizon domains, as opposed to prior works that focus on empirical results or bandits settings. Specifically, the paper develops a method for constructing confidence intervals in MDPs with constant initial state and show that a value function can be bounded. Additionally, they propose a method based on doubly robust estimation that can estimate confidence bounds on value over many intiial states. The methods are tested on domains such as robotics, health care, and inventory management."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well motivated and provides novel confidence intervals for OPE (CP-Gen) using an initial starting state and leverages common modern techniques for generative modeling. This is well motivated by applications in healthcare where each decision point at similar states leads to similar behavior. Additionally, the authors relax this assumption in the DR-PPI method for confidence interval calculation by removing the conditioning on starting state.\n\nThe authors provide theoretical results that take into account practical considerations and test their method on a wide array of domains (health care, mujoco, robotics, inventory control)\n\nIn general, the results demonstrate that the DR-PPI and CP-Gen methods provide the tightest and most accurate bounds on the value estimate of the evaluation policy."}, "weaknesses": {"value": "The main weaknesses in this paper surround the presentation of the methods and results. I struggled to understand the paper fully due to un-named variables, shift in notations, and general lack of clarity / assumptions / givens.\n\n### Lack of clarity\n1. What is the purpose of having a discount and a finite horizon? \n2. Line 97 introduces IPS with $\\pi(s,a)$ but $\\pi(a | s)$ is used throughout the rest of the paper. Is there a meaningful difference?\n3. Do you have access to the evaluation policy itself or just trajectories thereof?\n4. I believe the notation in line 151 of the value function non-standard and it is not clear what $\\tau \\sim p^{\\pi_e}$ means. Is it trajectories sampled from the evaluation policy? Then does $p^{\\pi_e}(\\tau)$ indicate the probability of $\\tau$ under the policy $\\pi_e$?\n5. In line 162, the authors mention that $\\tilde{p}$ is a generative model but give no other context.\n6. Line 211, the authors measure the IPS weight for a sample and use $\\pi_e$ and $\\pi_b$. As mentioned above, I am unclear whether the method has access to the policies themselves or just trajectories.\n7. Line 214, the authors discuss a \"ball around the output $\\delta_{rr'}$\" but do not give further context.\n8. Line 221, the authors invoke $Q$ which I believe is a Q-function but never define it. \n9. Line 253, there is a $E$ which I suspect is an expectation, except the remainder of the paper uses $\\mathbb{E}$. \n10. Line 262, $\\mathbb{V}$ is used but never introduced or discussed.\n\n# Experiments\nWhile the results indicate the efficacy of the methods, there does not seem to evaluation a wide array of evaluation policies. I am curious to see how the performance changes as the distance between the eval and behavior policies grows. The experimentation in varied domains is appreciated, but In general, it would be valuable to see ablations in each environment and discuss the specifics of the behavior and eval polciies.\n\nNitpick: PERRY is not mentioned anywhere but title"}, "questions": {"value": "I believe my questions are evident from the weaknesses section. Please refer there to clarify the understanding of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LOaV0GI8CM", "forum": "HQ4VmeEzV0", "replyto": "HQ4VmeEzV0", "signatures": ["ICLR.cc/2026/Conference/Submission13202/Reviewer_RvJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13202/Reviewer_RvJr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844381231, "cdate": 1761844381231, "tmdate": 1762923895896, "mdate": 1762923895896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of uncertainty quantification in a version of the off-policy evaluation setting where the behavior data is additionally augmented by synthetically generated data. The additional synthetic data can be used for OPE estimation, however, the source of the synthetically generated data is expected to have imperfections. The paper proposes methods for accounting for those imperfections in the synthetic data by quantifying the uncertainty of the prediction in the form of a *range* of the off-policy performance estimates, i.e., confidence intervals in which the true value lies with a high probability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem of using synthetically generated data for OPE is an important one, especially at a time when interaction data with a decision process can be generated in abundance."}, "weaknesses": {"value": "- Mistake in the proof of Equation (4): Equation (24) → (25), the probability of a trajectory includes the transition probabilities along with the policy probabilities. In typical importance weighting, since the transition function is held constant, those terms cancel out. In this setup considered by this work, since the synthetic data comes from a *different* generative model (with a correspondingly different transition function) those terms cannot cancel out as done in these steps.\n  - This correction term is central to all the results in this paper. It is not clear how the results that follow hold.\n- The aforementioned error must also affect the proof of the validity of the confidence intervals (Theorem 1).  However, no explicit proof is provided for this theorem, other than a statement that says “this is a direct consequence of the Coupling Lemma”. The coupling lemma hasn’t been stated or referenced.\n- If this were to be updated/corrected, practically, the application of this method would require access to the transition probabilities of the generative model. This may often be infeasible in practice, as it amounts to having access to the logits of generative model.\n- The motivation of this paper is to use *auxiliary* data. However, in both the methods proposed the generative model is being learnt from a split of the behaviour data itself, which is then used to generate additional data. This is the primary distinction from prior work [1]. This is mechanistically bootstrapping from existing data, rather than including additional data beyond what was available. \n- The paper would require a significant re-write. A few examples that stand out:\n  - Notation is introduced without being defined beforehand: for example, Equation (6), Q is undefined. Only after referring to [1] it is understood that it may probably refer to quantiles. Expressions in the paper bear a striking resemblance to those in [1].\n  - Under Section 2, the second term $\\widehat{V}^{\\pi_e}_{DR-PPI:2}$ is never defined throughout the paper. Why a generative model is being used for this IS estimate term [L276] is also unclear.\n  - Notation overloading: Section 2.1 defines $R$ as the reward function, Section 3.1 uses $R$ to denote return.\n\n---\n[1] Foffano, Daniele, Alessio Russo, and Alexandre Proutiere. \"Conformal off-policy evaluation in markov decision processes.\" 2023 62nd IEEE Conference on Decision and Control (CDC). IEEE, 2023."}, "questions": {"value": "Assuming transition probabilities of the generative model for the synthetic data were accessible and used in the computation of the score, what does Assumption 3 (Score Smoothness) get interpreted as? The likelihood ratio of the generated data for a given reward difference is Lipschitz? Is that then a statement about how “off-policy+off-dynamics” the data can be to be meaningfully used for OPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1QK36A6bh0", "forum": "HQ4VmeEzV0", "replyto": "HQ4VmeEzV0", "signatures": ["ICLR.cc/2026/Conference/Submission13202/Reviewer_wDpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13202/Reviewer_wDpu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941855233, "cdate": 1761941855233, "tmdate": 1762923895580, "mdate": 1762923895580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}