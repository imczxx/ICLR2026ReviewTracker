{"id": "AaMB3SFmBy", "number": 22208, "cdate": 1758327763852, "mdate": 1759896880255, "content": {"title": "Cost-effective Agent Test-time Scaling via Budget-Aware Thinking", "abstract": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking'' in tokens but also \"acting'' via tool calls. Unlike tokens in textual reasoning, the number of tool calls directly bounds the agent’s interaction with the external environment. To this end, we study how to scale such agents under explicit tool-call budgets, focusing on web search agents equipped with search and browse tools. We introduce CATS (Cost-effective Agent Test-time Scaling), a budget-aware framework designed for effective and efficient agent scaling. CATS integrates a lightweight budget tracker that provides a continuous signal of remaining resources to the agent's core modules, encouraging budget-aware adaptations in planning and verification. By constraining the number of tool calls and unifying the costs of both token and tool consumption, we analyze the cost–performance scaling behavior in a more controlled manner. Experiments across search-intensive benchmarks show that CATS produces more favorable scaling curves, attaining higher accuracy with fewer tool calls and lower overall cost.  Our work advances a cost-conscious design for agent test-time scaling and offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.", "tldr": "", "keywords": ["Search agent", "test-time scaling", "tool-augmented agent", "budget constraint"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e6c2aeed755f54a455660d8dbcc252b39426cc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CATS, a budget-aware system that uses a lightweight tracker to monitor token and tool-call usage, enabling the agent to adapt its planning and verification dynamically based on remaining resources. Through this design, CATS achieves better performance and cost-efficiency by unifying computation and tool-call expenses into a single cost metric. Experimental results on search-intensive benchmarks demonstrate that CATS yields higher accuracy and more favorable scaling curves compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of investigating into the cost-performance balance in agent behavior is very interesting and important, especially for real users, as we do not have unlimited budget. Therefore the paper addresses a very important problem in empirical agent usage."}, "weaknesses": {"value": "- The method part writing is a bit unclear as there’s no example provided to illustrate the concept, or prompt shown to the audience how the things are concretely done. Only giving high-level concepts are far from enough. For example, how you categorize clues in two type, what they refer to in the real question, etc.\n\n- The context of using tool call or token can be put into a broader context of whether to use LLM’s internal knowledge (manifested through token output) / external world knowledge (manifested through tool call) to answer a certain question. The decision making between these two aspects is the core of agent behavior. Many related works actually can be discussed from this perspective.\n\n- The results are only performed on Gemini models which may lacks insights, especially when the framework purely rely on prompting, as different model family may react differently towards the same prompt, and the model behavior may also be very sensitive to the prompt content.\n\n- The settings, especially for the baseline, seems very arbitrary and no detailed baseline settings are provided in the main text (e.g. why it’s valid to append the “wait” sentence, what are the alternatively, will it causes differences, and why you do like this). This is important as currently it’s not guaranteed the baselines are compatible, but risk cherry picking which may yield the final conclusion invalid."}, "questions": {"value": "- It’s a smart idea to directly bridge the cost physically through how much money is spent. I am wondering in your setting one tool call equals to approximately how many token input/output price? Will this ratio affect the agent’s behavior / decision making during its planning?\n\n- For the baseline parallel, I am not sure whether the results are comparable. If you set a very large parallel number for the baseline, it definitely needs more budget in total, and since they are parallel run, they cannot re-use the information as effectively as your method. So how you ensure the comparison is fair, especially when the conclusions are about efficiency?\n\n- More insightful analysis can be done on what is causing the improvement in your result. For example, in your analysis, is the raise in tool use the direct cause of you accuracy improvement, or it’s because of the “sequential scaling” (planning is a kind of special “sequential scaling”) that makes model paying more attention to the problem itself?\n\n- The paper will further benefit from doing error analysis including what. Are the common error patterns for failed cases, tool used up, budget reach limit, lack of model capability, or something else? What’s the statistics and how can we potentially improve them? These will guide future research directions more effectively than just informing people the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tFh4b4F7PS", "forum": "AaMB3SFmBy", "replyto": "AaMB3SFmBy", "signatures": ["ICLR.cc/2026/Conference/Submission22208/Reviewer_z9K3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22208/Reviewer_z9K3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767659066, "cdate": 1761767659066, "tmdate": 1762942115278, "mdate": 1762942115278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shifts the focus of test-time scaling (TTS) from *token-related scaling* to *tool-call-related scaling*, thereby extending the scope of TTS. It introduces a new problem: how to effectively utilize additional computational budgets allocated for tool calls in tool-augmented agents. To address this, the authors design a *cost-effective agent test-time scaling framework* that scales computation for tool calling with budget-awareness. Using *web search* as a representative task, the proposed scaling method is shown to outperform the base model, several existing trained agents, and some token-based TTS methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel research problem — *test-time scaling under a tool-call budget* — which fills an important gap in current TTS research and is of high research value.\n2. The proposed budget-aware planning and balanced budget usage are thoughtful and practically meaningful design considerations.\n3. The problem formulation and motivation are clearly written, especially in the early sections of the paper."}, "weaknesses": {"value": "1. The writing quality fluctuates significantly. The first two sections are well-organized and easy to follow, but the later sections, particularly Section 3, lack critical technical details. While the introduction mentions key challenges such as balancing different budget usage across tools and budget-aware exploration, the method section does not clearly explain how these challenges are addressed. Furthermore, several essential details are missing, which hinder reproducibility — for example, how the agent selects tools, how subsequent planning steps utilize previous tool outputs, and how the agent balances between breadth and depth of exploration. The paper would benefit from additional equations and figures to improve clarity. Finally, the early termination mechanism mentioned in line 398 lacks a corresponding description in the method section, and line 472 contains a capitalization issue.\n2. The unified metric introduced in Equation (2) lacks generalization. The definition of *economic cost* does not apply uniformly to both local model usage and local tool functions because the models and tools in local are free.\n3. The problem setup, which assumes that each tool call has an explicitly defined budget, is unrealistic. In practice, users typically specify a *total* computational budget, and the allocation across tools should be determined by the proposed method. Moreover, the budget definition should jointly consider both *token budget* and *tool-call budget* to align with the broader definition of test-time scaling.\n4. The experiments are insufficient.\n    (1) The main experiment should focus on demonstrating how the proposed method improves the *scaling cost–performance curve*, rather than only comparing with base or parallel models. Figure 2’s comparison is therefore inadequate.\n    (2) The choice of TTS baselines is limited. The token-based baselines do not include current state-of-the-art methods, and the paper does not explore tool-call-related scaling methods, such as simple BoN or self-refinement on tool calls.\n    (3) Evaluating on only one type of task is insufficient to demonstrate the method's generalization.\n    (4) The ablation studies in Section 5.3 are not well-designed. The ablation of the planning and budget-tracker modules should be conducted within the proposed framework itself, not on another existing method."}, "questions": {"value": "1. Has the paper considered the impact of different temperature parameters on agent exploration behavior?\n2. How is the *parallel* baseline in Figure 2 related to tool-call-based scaling?\n3. Could the authors provide additional results for larger budget settings in Figure 3 to show the trend of performance improvement with increased budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MBRQzEDvYL", "forum": "AaMB3SFmBy", "replyto": "AaMB3SFmBy", "signatures": ["ICLR.cc/2026/Conference/Submission22208/Reviewer_r4ZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22208/Reviewer_r4ZX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867201039, "cdate": 1761867201039, "tmdate": 1762942115028, "mdate": 1762942115028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of cost-effective test-time scaling for tool-augmented large language model (LLM) agents . The authors distinguish scaling \"thinking\" (token generation) from \"acting\" (tool calls) and focus on managing performance under explicit tool-call budgets. They propose CATS (Cost-effective Agent Test-time Scaling), a framework featuring a lightweight budget tracker. This tracker provides a continuous signal of remaining resources to budget-aware planning and budget-aware verification modules. The paper formalizes the budget-constrained optimization problem (Equation 1) and introduces a unified cost metric to account for both token and tool-call costs (Equation 2). Experiments on web search benchmarks show that CATS produces more favorable scaling curves, achieving higher accuracy with fewer tool calls and lower overall unified cost compared to baseline scaling methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and uses easy-to-understand language.\n\n2. Clear and relevant problem formulation.\n\n3. Sound and intuitive framework design."}, "weaknesses": {"value": "1. The entire framework seems to rely on manual framework engineering and prompt design, raising concerns about its scalability and generalizability.\n\n2. Model Dependence: Experiments are conducted exclusively on the Gemini series of models.\n\n3. Task Dependence: The framework is instantiated and evaluated exclusively on web search tasks. The paper claims broad applicability to tool-augmented agents , but provides no discussion or evidence for other domains (e.g., software engineering, scientific discovery) where tool-call costs and task structures are vastly different.\n\n4. Prompt Sensitivity: The framework's effectiveness is likely sensitive to the quality of the prompts used to guide the planning (Sec 3.2) and verification (Sec 3.3)  modules. The paper does not discuss this potential sensitivity. No direct evidence found in the manuscript.\n\n5. Incomplete ablation: It remains unclear how much of the performance gain from the full CATS model (Table 1) comes from the planning module (Sec. 3.2) versus the verification/early-stopping logic (Sec. 3.3), as these components are not tested in isolation from each other."}, "questions": {"value": "1. Can the authors provide results on a wider range of models, such as the GPT series or open-source models?\n\n2. Can the framework be tested on additional tasks beyond web search?\n\n3. Can the authors provide more comprehensive ablation studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LUUU5Q0Jlp", "forum": "AaMB3SFmBy", "replyto": "AaMB3SFmBy", "signatures": ["ICLR.cc/2026/Conference/Submission22208/Reviewer_B1Ff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22208/Reviewer_B1Ff"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893276418, "cdate": 1761893276418, "tmdate": 1762942114217, "mdate": 1762942114217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses cost-effective test-time scaling for tool-augmented agents by tracking and enforcing budgets not only as a function of thinking tokens but also---importantly---as a function of tool calls, which are both unified by means of their monetary costs. The authors propose CATS (Cost-effective Agent Test-time Scaling), which performs \"constraint decomposition\" and proceeds to either \"explore\" or \"verify\" each branch from a question decomposition, while following a global budget constraint that governs how much exploration and verification are allowed. Experiments on BrowseComp and BrowseComp-zh benchmarks with Gemini Flash or Pro, covering relatively naive sampling-based baselines, shows CATS outperforming the naive baselines, the pretrained models alone, and most custom trained models (although differences in backbone models limit all comparisons but the ones against the naive baselines). Authors conclude with sensible analyses about early stopping even with remaining budget and scaling behavior vs. naive baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Very practically relevant topic.\n\n2. The premises are intuitive and easy to understand---the idea of unifying two natures of costs by their monetary values, which then allows for a global budget to be tracked and enforced, makes a lot of sense.\n\n3. The paper is mostly clear and straightforward."}, "weaknesses": {"value": "1. Authors show gains over naive baselines that do not attempt to score and prioritize branches in the tree search space in any way. There is a limited analysis of \"ReAct + planning,\" which still does not reflect the state-of-the-art in planning. Importantly, the absence of strong baselines that estimate branch \"goodness\" without explicitly tracking monetary costs makes it hard to determine if CATS truly is more cost-effective than existing methods.\nIn this sense, authors should consider running stronger baselines like \"Reasoning with Language Model is Planning with World Model\" (EMNLP 2023), that applies MCTS with goodness estimates; \"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search\" (ICLR 2024), that applies A* with goodness and non-monetary cost estimates; or \"Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning,\" that applies MCTS with different goodness estimates; and showing to what extent they exceed monetary cost constraints that CATS is otherwise able to enforce. If true, claims would have much better support than with current comparisons.\n\n2. Why do authors claim that \"tool-call budget\" is \"more relevant\" than \"token-based budget\" (lines 111-112)? If one of the contributions is to unify the cost metric by means of monetary costs, then it would be natural to substantiate this statement with an analysis of the real dollar amounts associated to each resource in the experiments performed. Additionally, to call one budget \"more relevant\" goes against the motivation of unifying the two---maybe stating that \"tool-call budget\" is highly relevant but underexplored would be more appropriate."}, "questions": {"value": "As described in the weaknesses, a question:\n\n1. Why do authors claim that \"tool-call budget\" is \"more relevant\" than \"token-based budget\" (lines 111-112)? If one of the contributions is to unify the cost metric by means of monetary costs, then it would be natural to substantiate this statement with an analysis of the real dollar amounts associated to each resource in the experiments performed. Additionally, to call one budget \"more relevant\" goes against the motivation of unifying the two---maybe stating that \"tool-call budget\" is highly relevant but underexplored would be more appropriate.\n\nSuggestion:\n\n1. On lines 254-256, consider at least adding a citation that supports the statement that summarization makes exploration \"better informed.\" Otherwise, ideally, this could have been ablated in the Appendix to better understand the proposed methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rsLNNdesty", "forum": "AaMB3SFmBy", "replyto": "AaMB3SFmBy", "signatures": ["ICLR.cc/2026/Conference/Submission22208/Reviewer_AUaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22208/Reviewer_AUaU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962090822, "cdate": 1761962090822, "tmdate": 1762942113847, "mdate": 1762942113847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}