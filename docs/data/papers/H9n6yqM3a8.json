{"id": "H9n6yqM3a8", "number": 20964, "cdate": 1758312096479, "mdate": 1763594678658, "content": {"title": "LLM Output Homogenization is Task Dependent", "abstract": "A large language model can be less helpful if it exhibits output response homogenization. But whether two responses are considered homogeneous, and whether such homogenization is problematic, both depend on the task category. For instance, in objective math tasks, we often expect no variation in the final answer but anticipate variation in the problem-solving strategy. Whereas, for creative writing tasks, we may expect variation in key narrative components (e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity produced by temperature-sampling. Previous work addressing output homogenization often fails to conceptualize diversity in a task-dependent way. We address this gap in the literature directly by making the following contributions. (1) We present a task taxonomy comprised of eight task categories that each have distinct concepts of output homogenization. (2)  We introduce task-anchored functional diversity to better evaluate output homogenization. (3) We propose a task-anchored sampling technique that increases functional diversity for task categories where homogenization is undesired, while preserving it where it is desired. (4) We challenge the perceived existence of a diversity-quality trade-off by increasing functional diversity while maintaining response quality. Overall, we demonstrate how task dependence improves the evaluation and mitigation of output homogenization.", "tldr": "A Task-Anchored Framework for Evaluating and Mitigating Output Homogenization", "keywords": ["homogenization", "diversity", "alignment", "pluralism", "underspecification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e63ec26411bed0c6fab28ff8f0c1b91a3f9e98c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper describes work on proposing a task-specific perspective for evaluating and mitigating output homogenization in LLMs. The authors define output homogenization where LLMs produce similar outputs which can be problematic in scenarios that require high diversity (e.g., creative writing) but in ones that require verifiability (e.g, math problems), hence the need for a task-dependent approach. The authors introduce the notion of “functional diversity” where a user might find two or more responses to be useful based on the task they use an LLM for. The authors then propose a simple task-anchored sampling technique aimed to maximize functional diversity for task categories where homogenization is needed as well for tasks where it is not. The authors lay down an experiment setup that mainly uses commercial models (Claude, GPT-4o, Gemini) for both generation and response evaluation and sample prompts from five existing datasets. In terms of measuring functional diversity, results show that task-anchored sampling performs better than general sampling techniques and oftentimes reduces homogenization for crucial categories such as Encyclopedia Inquiry, Creative Writing, and Advice or Opinions. In terms of diversity-quality tradeoff, using task-anchored functional diversity removes said tradeoff which is evident if general diversity metrics are used such as vocabulary diversity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The concept of output homogenization and its inherent nature of being both essential and to be mitigated based on task is an important study. One strength I found with the paper is how well the motivation for the work is laid out that anyone can readily appreciate and understand the problem that needs to be solved. I also appreciate the simplicity with the task taxonomy and task-anchored sampling technique that the authors propose in order to increase functional diversity from LLMs. I agree that the proposed method’s simplificty will be key in the ease of adoption of future work with output homogenization."}, "weaknesses": {"value": "My main issue with the current study is the LLM-as-a-judge-centric evaluation for the concept of diversity in the experiments. The authors define functional diversity as “a user would perceive two responses as meaningfully different for a given task”, hence, I was fully expecting human evaluation as the main driver of the evaluation procedures across the task taxonomy rather than LLM-as-a-judge.  \n\nAnother issue I have is why did the authors not explore any open-weight models for both the functional diversity and diversity-quality tradeoff experiments and only used commercial ones? This reduces the reproducibility of the work that could continue to on focusing \"why\" (factors affecting) output homogenization happens as future work. The authors also mentioned that for all LLM-as-judge setups, the same models (GPT4o, Clause, GeminiFlash) are essentially evaluating their own responses even if they are averaged together. This setup sounds like bias may be introduced in the evaluation procedure of the study. What is the justification of the authors for this?\n\nThe authors mentioned that the work does not focus on answering the \"why\" output homogenization happens but the current results of the paper feels very limited and minimal at its current state. I would have appreciated it if the authors could have linked variations of functional diversity to factors such as model scale/size, language coverage, effect of preference optimization vs no preference optimization, etc. Especially that some of these factors like RLHF-ed models have been investigated by previous works to negatively affect output diversity. It would have been supplemental if the authors also investigated the same direction with the functional diversity whether this changes for non RHLF-ed models.\n\nThe paper should provide a clear disclaimer that this framework has only been tested with English-centric evaluation procedures. The results related to model output homogenization specifically functional diversity and diversity tradeoffs do not generalize in multilingual models unless otherwise tested sufficiently with the same level of rigor done for English."}, "questions": {"value": "What are the justification of the authors for exclusively using commercial models and not open-source/open-weight models?\n\nWere the prompts in Table 3 for each dataset and task category randomly sampled or handpicked?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8fjywcf6Ux", "forum": "H9n6yqM3a8", "replyto": "H9n6yqM3a8", "signatures": ["ICLR.cc/2026/Conference/Submission20964/Reviewer_Qpei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20964/Reviewer_Qpei"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760744469061, "cdate": 1760744469061, "tmdate": 1762939061434, "mdate": 1762939061434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies output homogenization in large language models, arguing that it should be evaluated and mitigated in a task-dependent way rather than uniformly across tasks. The authors develop an eight-category task taxonomy distinguishing settings where consistent outputs are desirable versus those where diversity is preferred. They introduce task-anchored functional diversity and a task-anchored sampling approach that adapts decoding to the task’s diversity requirements. Experiments on multiple LLMs suggest that this approach improves meaningful diversity without degrading overall quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper raises an important and underexplored perspective by framing output homogenization as a task-dependent issue rather than a universal flaw of LLMs.\n\n- The proposed task taxonomy and task-anchored sampling offer a clear and interpretable framework that connects conceptual reasoning about diversity with practical decoding strategies."}, "weaknesses": {"value": "- The paper’s main conclusion that task-anchored sampling resolves the diversity-quality trade-off relies entirely on LLM-judge evaluations. Both “functional diversity” and “checklist-based quality” are assessed by aligned models under task-specific prompts, and only the former is partially validated against human judgments. Without independent human validation for the quality metric, the claim of “no trade-off” may reflect a biased artifact of LLM-based evaluation rather than a genuine improvement in human-perceived output quality. This circular setup weakens the paper’s main result.\n\n- The eight-category task taxonomy that supports the framework is neither empirically grounded nor fully supported by data. Some categories, such as “Problem Solving or Design Subjective”, lack any corresponding dataset, while others have ambiguous or overlapping boundaries, such as “Encyclopedia Inquiry” and “Advice or Opinions”. In addition, the taxonomy itself is generated and validated by LLMs rather than humans, which risks reinforcing model-specific biases instead of revealing genuine task distinctions. This undermines the robustness and generality of the proposed task-dependent framing.\n\n- The proposed task-anchored sampling depends on accurate task classification, yet this step is never evaluated. The experiments assume access to ground-truth task labels derived from model majority votes, meaning the system is tested under idealized conditions. In real-world scenarios, even moderate misclassification would cause the model to apply the wrong sampling behavior, reducing quality in exactly the tasks where consistency or factual accuracy is most important. The lack of analysis of this dependency leaves the practical reliability of the approach uncertain."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WEykSu3UZe", "forum": "H9n6yqM3a8", "replyto": "H9n6yqM3a8", "signatures": ["ICLR.cc/2026/Conference/Submission20964/Reviewer_b2mQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20964/Reviewer_b2mQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447691752, "cdate": 1761447691752, "tmdate": 1762939061025, "mdate": 1762939061025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper makes the following key contributions. They observe a gap in the literature that prior work largely discusses the output diversity of LLMs in a task-agnostic manner. This might lead one to assume that homogenization is 'always an issue'. To address this gap, they propose to measure 'functional diversity' or the effective diversity, or the number of valid solutions generated by a system that is anchored in the expected answer format for the prompt/task. They also provide a comprehensive taxonomy of tasks, dividing them based on the expected answer format, which helps operationalize this definition into a formal evaluation. Through experiments with 3 frontier LLMs (GPT-4o, Gemini-2.5-Flash, Claude-4-Sonnet), they show that targeted task-anchored prompting strategies (basically a prompt that provides some functional definition of the expected output) can increase functional diversity for tasks where it is desired. The paper is well motivated, clearly written, and provides sufficient evidence for its claims."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The positioning and framing of this paper are really good. It notices an over-simplified finding in the literature (about homogenizing of output of LLMs as a single amorphous truth in all tasks), provides a clear and tractable method (by categorizing tasks) to create a more usable definition of functional diversity, and provides benchmarking of simple methods to mitigate homogenization as needed. \n\n2. The definition of functional diversity is clear, and also references past work that implements versions similar to it. \n\n3. The proposed taxonomy is extensive (and the authors also note that it is non-exhaustive in a refreshing change from recent ML discourse!), and the experiments provide clear evidence for the value of task-specific prompting techniques in domains where diversity is desired. Fig. 2 displays this result really well.\n\n4. Section 4.3 contextualizes prior findings of a trade-off between diversity and output quality as one of having mismatched metrics and that when measured correctly, these are not always at odds with one another."}, "weaknesses": {"value": "1. The definition in Def 3.1 should be extended to a set of responses $y \\in Y$ since that is really the thing you're interested in, and what you measure in the experiments (L.313-317). Given a fixed sampling budget (an answer set size), what is the effective number of unique responses in it?\n\n2. This is more for reproducibility, but I would advocate for experiments with an open-weight LLM since the models on which results are reported are all black-box and can be updated under the hood.\n\n3. For Section 4.3, before making a strong claim of 'not seeing an effect', I think you should also evaluate on tasks with a very large set of responses (and not just 5). It could just be that the effect is not observed at the current sampling budget."}, "questions": {"value": "1. Re footnote 2 on page 6 - Aren't MacGyver [1] or CoPoet [2] as used in [3] examples of a task in Category E, i.e., \"Tasks to solve a problem with many verifiable solutions\"? Basically, these involve coming up with solutions to real-world physical reasoning problems ('Iron a shirt with a coat hanger, steamer, and kettle') or creative instructions that have semantic constraints and stylistic variants.\n\n2. In [4], they find that many diversity measures are correlated with simple information compression algorithms like GZIP. I'm curious how well functional diversity correlates with information compression. \n\n3. In Fig. 3, L.422, should the caption say (a) and not (a)-(c)?\n\n4. It would be interesting if you could compare base and aligned models on functional diversity, given past findings [5, 6] that feedback tuning reduces diversity.\n\n[1] Tian, Yufei, et al. \"MacGyver: Are Large Language Models Creative Problem Solvers?.\" Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2024.\n\n[2] Padmakumar, Vishakh, et al. \"Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models.\" arXiv preprint arXiv:2504.09389 (2025).\n\n[3] Chakrabarty, Tuhin, et al. \"Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[4] Shaib, Chantal, et al. \"Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores.\" arXiv preprint arXiv:2403.00553 (2024).\n\n[5] West, Peter, and Christopher Potts. \"Base models beat aligned models at randomness and creativity.\" arXiv preprint arXiv:2505.00047 (2025).\n\n[6] Padmakumar, Vishakh, and He He. \"Does Writing with Language Models Reduce Content Diversity?.\" The Twelfth International Conference on Learning Representations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DLX5VZXpaz", "forum": "H9n6yqM3a8", "replyto": "H9n6yqM3a8", "signatures": ["ICLR.cc/2026/Conference/Submission20964/Reviewer_wNpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20964/Reviewer_wNpm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956742916, "cdate": 1761956742916, "tmdate": 1762939060290, "mdate": 1762939060290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper hypothesizes that quantifications and inductions towards output homogenization should be task dependent. They develop a task taxonomy enlisting which kind of variations is meaningful for that task and use that to construct prompts that describe task-dependent axes of variations to guide the generation more adaptively (to the task’s homogenization needs). Their task-anchored method increases diversity and does not reduce quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The key hypothesis is unique and has high potential for explaining the shortcomings of several works that categorize the diversity of the LM generations \n- Highlights an interesting shortcoming about prior work using scalar rewards (task-agnostic) to assign quality scores."}, "weaknesses": {"value": "- The task taxonomy and prompt design appear somewhat arbitrary and intuition-based, not derived from user studies or large-scale data.\nA small ablation that applies mismatched task prompts (e.g., creative prompt for factual task) would strengthen confidence that each mapping is truly optimal.Since the sampling methods themselves are not entirely new, the value of the paper rests heavily on how sound and justifiable these task definitions are.\n\n- All evaluations rely on LLM judgments. While the use of checklist-style grading for task-relevant quality is an improvement over scalar rewards, there are no human-based validations. Because the LLM-graded metrics show larger gains for the system-prompt variant and weaker discrimination elsewhere, a human diversity-quality study would help verify whether these numerical gains reflect actual perceptual differences."}, "questions": {"value": "- What fraction of your evaluation prompts fell into this “uncategorized” case? Could you report the performance of the methods on these? \n- How accurate is the task-to-category classification? Did you run any verification (human or otherwise) to ensure that prompts were correctly labeled before applying task-specific system prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "U2eRoVTrE5", "forum": "H9n6yqM3a8", "replyto": "H9n6yqM3a8", "signatures": ["ICLR.cc/2026/Conference/Submission20964/Reviewer_cajZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20964/Reviewer_cajZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060574842, "cdate": 1762060574842, "tmdate": 1762939059454, "mdate": 1762939059454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response & Summary of Minor Revisions"}, "comment": {"value": "We are very appreciative of all  reviewers for their time, engagement with our work, and thoughtful feedback! We are encouraged that each reviewer highlighted strengths of our work, including our task-dependent argument, taxonomy, and task-anchored sampling approach. While we have responded to each reviewer individually, this comment provides a global response addressing common concerns and outlining the minor revisions we made.\n\n\n# LLM-Judge Metrics & Human Validation\nWe agree with reviewers that one limitation of our experiments is using LLM-judges to evaluate functional diversity and task-based quality. We have updated the Discussion section (page 9, lines 452-456) to clearly acknowledge this limitation and to cite relevant work that documents reliability concerns with LLM-judges. \n\nWe further want to highlight:\n1. We validate the functional diversity judges with human annotation. Two authors independently labeled 225 response pairs for functional diversity and agreed with LLM-judges 79% of the time, with 80% inter-annotator agreement (page 6, footnote 2).\n2. For the checklist-based quality metric, we follow the approach of previous works in last year’s ICLR conference (WildBench & RocketEval) which extensively validated checklist-based grading with human annotation. We also manually confirmed that all our judge-generated checklists are appropriate for each prompt.\n3. We primarily report the average of 3 different LLM-judges to increase reliability. We have further added results based on each LLM-judge separately (Appendix Tables 28-33). The average differences across individual judges are small (3% for functional diversity & 5% for checklist-based quality) and do not change our overall conclusions.\n4. We include all our judge prompts for transparency (Appendix A.5-A.6), and have added examples of response pairs that are and are not functionally diverse for each task category (Appendix Tables 10-11). We also add examples of “Very Good” responses based on checklist-based grading for each task category (Appendix Table 13).\n\n\n# Taxonomy Motivations\nSome reviewers asked how our taxonomy relates to the homogenization literature and real-world LLM use-cases. We have added Appendix A.1 to help clarify this! \n\nOur taxonomy is grounded in the existing literature. We observed that many studies evaluate homogenization in specific task domains, suggesting that problematic notions of homogenization are task dependent. We designed our taxonomy to cover a variety of these task categories. Appendix Table 3 shows how our taxonomy maps to related work.  \n\nWhile our taxonomy is meant to clarify different functional diversity concepts, we also show that it captures many real-world LLM use-cases. Appendix Tables 4 & 5 map common use-cases for ChatGPT and Claude with categories in our taxonomy, and all text-based use-cases correspond to at least 1 category in our taxonomy.  \n\nUltimately, our taxonomy is non-exhaustive and represents one categorization of functional diversity concepts. However, our task-based evaluation and sampling approaches are generalizable to alternative task taxonomies.\n\n\n# Evaluation with Human-Labeled Task Classification\nWe appreciate reviewers for noting that we should evaluate with ground-truth task categories! We have adjusted our experiments to evaluate functional diversity based on human-labeled task categories. In order to test the practical setting, our task-anchored sampling uses the model's task categorization for each prompt.  We further add results showing the accuracy of models’ task classification (Appendix Figure 4); GPT-4o, Gemini-2.5-Flash, and Claude-4-Sonnet are all highly accurate (~85%). Thus, our results show that 1) models can identify the appropriate task category, and 2) when given the functional diversity concept for that category, they can generate many functionally unique responses.\n\n\n# Evaluating Open-Weight Models\nWe appreciate the suggestions to evaluate open-weight models! We have added experiments for Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 in the Appendix.\n\nFigures 7-8 show results for functional diversity for these models, and we generally observe that our task-anchored sampling improves diversity to similar levels as commercial models. Among temperature-sampled outputs, we also note that open-weight models have higher functional diversity than commercial models, possibly due to the less extensive alignment in open-weight models. \n\nFigures 11-12 illustrate the diversity-quality tradeoff for these models. In general, we observe a similar pattern of a large tradeoff when using general metrics, and a smaller tradeoff when using task-based metrics. However, the tradeoff is more noticeable for open-weight models (roughly 0.5 on our 5-point scale) than commercial models. One reason is that open-weight models have much lower task classification accuracy (only ~50% compared to ~85% for commercial models). Other factors may be model size and less extensive alignment."}}, "id": "qOitIIW7cm", "forum": "H9n6yqM3a8", "replyto": "H9n6yqM3a8", "signatures": ["ICLR.cc/2026/Conference/Submission20964/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20964/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission20964/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763598817730, "cdate": 1763598817730, "tmdate": 1763598817730, "mdate": 1763598817730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}