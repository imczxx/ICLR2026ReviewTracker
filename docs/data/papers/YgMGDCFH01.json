{"id": "YgMGDCFH01", "number": 2482, "cdate": 1757113280358, "mdate": 1759898145487, "content": {"title": "MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching", "abstract": "Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters’ motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.", "tldr": "unsupervised flow-matching alignment of VQ-VAE motion tokens enabling flexible, reversible cross-character retargeting—no paired data", "keywords": ["Motion Retarget", "Motion Representation", "Flow Matching", "unsupervised learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cf00bd71be3e8a2e2ebc22c8dbbfee87300c49c.pdf", "supplementary_material": "/attachment/1a07d5da26115018abe60b584eb2b188a0682e02.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes retargeting motions between significantly different skeletal structures by conditioned flow matching in the motion tokens. Specifically, the motion tokenizer for each character is learned with VQ-VAE. Then, the flow matching conditioned by the motion-specific objective learns the motion token mapping from one character's motion to another. The conditions demonstrated are root velocity, local end effector position, world end effector position, world path control, and world root height control. The conditioned flow matching is trained with a similar strategy as classifier-free guidance by randomly dropping the condition with a probability. The demonstrations showcase challenging scenarios, such as retargeting humanoid motions to Spot (a quadruped robot equipped with a robotic arm on its head), which requires interpreting the body part mapping from the humanoid to the robotic arm."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Simple and sensible design (VQVAE + conditioned flow matching) to achieve the goal\n* Honest disclosure of limitations\n* Clear and concise presentation with interesting demo scenarios, such as mapping humanoid motions to Spot and a chain-of-retargeting"}, "weaknesses": {"value": "* (minor) \"Conditioned flow matching\" rather than \"unsupervised flow matching\" may be a clearer way to describe the method without confusion\n* As the authors infer in the limitations section, there is a scalability issue with this approach\n  * There must be enough motions in each skeletal morphology to train sensible VQVAE tokenizers. This alone is challenging\n  * Then, the conditioned flow matching must be trained with all possible conditions\n  * The condition objectives must be hand-crafted. Five conditions predefined in the paper may not be enough for different motions, e.g., dancing"}, "questions": {"value": "* 8 hours of discrete flow retargeter training time - is this for training one conditioned flow from one morphology to another, or is this for all five conditions from one morphology to another?\n* Related to the above training time and the weakness, any ideas on improving the scalability and the training speed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6cnRXZo70c", "forum": "YgMGDCFH01", "replyto": "YgMGDCFH01", "signatures": ["ICLR.cc/2026/Conference/Submission2482/Reviewer_rBVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2482/Reviewer_rBVb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550770543, "cdate": 1761550770543, "tmdate": 1762916251202, "mdate": 1762916251202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a motion retargeting method based on flow matching. For each pair of character skeletons, this framework first trains a motion tokenizer for each of the characters, thus encoding their motion sequences into a series of character-specific tokens. These tokens form a character-specific token space, on which the flow matching model is trained to learn an invertible mapping between these two spaces. Once trained, motions from one character can then be retargeted to another character through this framework. Experiments reported better performance compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work demonstrates an interesting framework for motion retargeting, which is shown to work between characters with different skeleton structures. From my perspective, the main strengths of this work include:\n- A motion retargeting framework supports multiple conditions. I’m more in the character animation area, so it’s the first time for me to know that motion retargeting has a higher task-specific requirement in robotics. By supporting different conditions during formulation, this framework can adapt to different scenarios depending on the specific tasks.\n- A motion retargeting framework trained with unpaired data. I think this is another interesting part of this paper, which alleviates the training data requirement for motion retargeting."}, "weaknesses": {"value": "[Ablation study]\n- For ablation study, I would expect to see some qualitative results as the current reported results are quite hard to evaluate. Only a limited number of metric values are reported, making the conclusions not so convincing to me.\n\n[Paper writing]\n- As I wrote in the Strengths section, I feel the unpaired training strategy is a critical and interesting contribution of this work. However, the details of this part are mainly put in supplemental, and the current description in Section 3.3 is a little bit hard to understand. I feel it would be better if the authors can elaborate more in this part, and potentially arrange some space to put the pseudo code in the main paper."}, "questions": {"value": "I don’t have specific questions regarding this manuscript. Overall this is an interesting work, but the lack of qualitative results and potential improvement in the paper writing (see the Weaknesses section) restricts me from giving a higher score on it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oHhzeVUV89", "forum": "YgMGDCFH01", "replyto": "YgMGDCFH01", "signatures": ["ICLR.cc/2026/Conference/Submission2482/Reviewer_KJNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2482/Reviewer_KJNg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965117886, "cdate": 1761965117886, "tmdate": 1762916250995, "mdate": 1762916250995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The flow matching is introduced in this manuscript to achieve motion retargeting in latent feature space. First, the motion sequences of each character are used to train a invididual  set of encoder, decoder and codebook of VQ-VAE for each character. After that, flow matching is involved to learn the mapping between the latent space distribution of different skeletons for each non-homeomorphic skeleton pairs. During training, the learned motion retargeting mapping function is introduced to achieve the retargeting between latent motion vector of different characters. The training of the mapping function is unsupervised without paired motion data under different skeletons. The proposed method is evaluated on the AMASS dataset to show its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using flow motion to achieve motion retargeting in the tokenized motion space is novel. This manuscript is a good pratices for applying VQ-VAE in the motion retargeting task;\n\n2. The proposed method achieves good performance compared with existing method in both motion fidelity and controllability;"}, "weaknesses": {"value": "1. The setting of training an individual set of an encoder, a decoder and a codebook for each character seems not reasonable of at least not efficient. Considering that the motion retargeting mapping function is trained in the latent token space, it would be better to learn the codebook of each characters in a shared latent token space rather than in distinct spaces. Therefore, all the characters need to share the encoder and decoder. The difference between these two choices need further discussion;\n\n2. The learning of motion retargeting in the latent motion space without paired data is not reliable especially when the set of motion category is large and the type of these categories is diverse;\n\n3. The presentation of this manuscript is not good. There should be tables or figures to show the experimental results, not just by text descriptions;\n\n4. As the author mentioned, the motion retargeting mapping model can only be applied on characters that are seen during training. For those characters that is unseen in advance, the retargeting will fail;"}, "questions": {"value": "1. The detail of the condition variable c is not clear. It is a one-hot vector. What is the set of the category the c support? How to indicate the condition during training for example how to specify the style condition and how ensure the c help the retargeting function learn the style transfer;\n\n2. Please expain the reason why the encoder, decoder and codebook are learned for each of the characters? Why not share the encoder and decoder for all the characters. If the encoder and decoder is not shared, how to make sure that the latent motion space is shared?\n\n3. Is there any trick for the selection of batch during training? I am wondering if the motion sequence of the source and target characters are quite different from each other, it is not easy for the learning of the retargeting fucntion to converge;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UAzAzMxPeU", "forum": "YgMGDCFH01", "replyto": "YgMGDCFH01", "signatures": ["ICLR.cc/2026/Conference/Submission2482/Reviewer_2E3m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2482/Reviewer_2E3m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986874073, "cdate": 1761986874073, "tmdate": 1762916250765, "mdate": 1762916250765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **MoReFlow**, a framework for **cross-embodiment motion retargeting**. The motivation stems from the limitations of existing approaches, which typically assume identical human morphologies or a fixed retargeting objective. MoReFlow introduces a **VQ-VAE-based tokenizer** to encode motions into a discrete latent space, combined with a **conditional flow matching network** for motion retargeting within this space. Furthermore, a **multi-sample coupling strategy** is designed to associate each source motion with multiple target motions, enabling unsupervised learning. Experiments on both upper-body and lower-body motions across four embodiments demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths:** \n\n[Sound Motivation and Methodology] \n\n- The paper clearly identifies the limitations of existing motion retargeting methods that are restricted to fixed morphologies or deterministic mapping objectives.\n- The proposed conditional flow matching mechanism is a technically sound and promising approach to address these challenges.\n\n[Advantages in Controllability]  \n\n- MoReFlow allows controllable retargeting styles (e.g., local vs. global), which significantly enhances its applicability and flexibility for different use cases.\n\n[Ablation Study to Verify Effectiveness] \n\n- The paper includes ablation analyses to verify the contributions of key design components, supporting the soundness of the proposed architecture."}, "weaknesses": {"value": "**Weaknesses:** \n\n[Incomplete Methodological Explanation] \n\n- The conditioning feature vector is insufficiently described: it is only mentioned to differ between animation and robotics, without providing a general, conceptual definition. A clearer discussion in the main text (rather than relegated to the appendix) would help readers understand its role and composition.\n- The rationale behind the multi-sample coupling strategy—a critical component for achieving unpaired learning—is not well articulated. The paper should better explain why and how this mechanism facilitates generalization since it is essential component for applying flow matching to motion retargeting.\n\n[Assumptions on Cross-Embodiment Data] \n\n- The method seems to require roughly comparable motion coverage across embodiments, even if motions are not paired. This assumption could limit scalability to non-humanoid or newly introduced embodiments where motion data is scarce.\n\n[Insufficient Experiments] \n\n- The experimental comparisons are relatively weak: only two baseline methods are considered, despite the existence of many relevant cross-embodiment retargeting approaches discussed in related work.\n- Results are aggregated across embodiments, preventing detailed analysis (e.g., humanoid vs. non-humanoid performance).\n- The origin and preprocessing of the evaluation datasets are not clearly described, which reduces reproducibility.\n\n[Improvements to Writing] \n\n- The paper could better highlight the novel contributions of MoReFlow in the related work section to emphasize its differentiation especially why it is necessary to apply flow matching compared to existing motion retargeting work. \n- The ablation results are described only in text, making it difficult to assess their impact. Presenting them in a summary table would improve readability and support the claims more convincingly."}, "questions": {"value": "Please refer to the problems discussed in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b8ISzucUP9", "forum": "YgMGDCFH01", "replyto": "YgMGDCFH01", "signatures": ["ICLR.cc/2026/Conference/Submission2482/Reviewer_Khj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2482/Reviewer_Khj3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988137035, "cdate": 1761988137035, "tmdate": 1762916250566, "mdate": 1762916250566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}