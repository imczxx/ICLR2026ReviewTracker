{"id": "hsgl5Owiio", "number": 8791, "cdate": 1758098390650, "mdate": 1759897763626, "content": {"title": "BitNet Distillation", "abstract": "Recent advances in extremely low-bit large language models (LLMs), such as the 1.58-bit BitNet, present a promising avenue for improving LLM efficiency in both inference speed and energy consumption. However, deploying such low-bit LLMs for downstream, task-specific applications often necessitates pretraining from scratch to achieve strong performance, which is both computationally and energetically expensive. To overcome this limitation, we propose BitNet Distillation Framework (BDF), a method that fine-tunes pre-trained, full-precision LLMs to 1.58-bit precision for specific downstream tasks, achieving comparable task-specific performance while incurring minimal computational overhead. Specifically, our approach introduces innovations along three dimensions: modeling, training, and distillation, to address the performance degradation and poor scalability often observed in existing fine-tuning methods for low-bit LLMs. Our experimental results demonstrate that \\ours{} achieves performance on downstream tasks comparable to that of full-precision models, which facilitating the deployment of larger LLMs on edge devices across a variety of task-specific applications, enabling faster inference and lower energy consumption.", "tldr": "", "keywords": ["quantization-aware training", "post-training quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebca3d0e2dd7e124b63dbe7e87e4f1d0142b36c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of training low-bit, 1.58 BitNet LLMs, for downstream applications, proposing a BitNet Destillation Framework (BDF), which finetunes full-prevision LLMs to 1.58-bits precision for a given downstream task. The framework introcudes LayerNorm before the output projection of the MHA and the output-projection of the FFNs, adopting the ideas from the Qwen3 architecture, mitigating the large activation variance by 1.58-bit LLMs togther with Logits and Attention Destillation methods into learning objectives. The paper demonstrate comparable performance to full precision."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Streamlined and precise introduction of 1.58-bit quantization\n\n2. Identifying an increasing performance gap between 1.58 bits and FP16 counterparts, as model size increases, for downstreams tasks.\n\n3. Experiments across different model types: Qwen3, Qwen2.5 and Gemma.\n\n4. Demonstrates Logits and Attention-relation distillations is effective for low-bit LLMs performance."}, "weaknesses": {"value": "1. The paper states in lines 52-53, that transition from full precision to BitNet, until now, requires training from scratch. I'd like to point authors to [1,2,3], which all study QAT by starting with a pre-trained model (it's even in the Gemma 3 report). [3] in particular also systematically analyzes pre-training from scratch vs. continual quantization aware pre-training. Thus, regarding the paper’s contribution 1, it is not the first work to finetune into the 1.58-bit scheme.\n\n3. The Stage-3 method uses a Student-Teacher setup, instead of relying on the shadow-weights, already containing the full-precision weights, increasing the training memory-requirements by 1/3.\n\n4. The paper argues minimal overhead in the abstract, but that is not justified within the paper, also following point (3) above.\n\n5. No description on how the reduced memory is reduced in practice. Authors, are reporting a 10X memory reduction. Yes, you can fit 5, 2-bit values in an INT8, naively, Microsoft also provide BitNet GPU kernels in their repository, but given your comments on specialied arithmetic, in future work lines 373-374 it does sound like that. This makes me wonder, how the numbers in Table 1, Speed and Memory are obtained?\n\n6. Actual hyper-parameters is not reported, nor is promise of sharing code publicly upon acceptance, hindering reproduceability. Which is crucial with such impressive results, which the authors has obtained.\n\n7. Training-step time is not reported, nor compared to 16bit\n\n8. You state that the models are trained on AMD Mi300X’s, is that also used for inference and speed analysis? Are they in a HPC System – if yes, how are they connected? – Numa nodes, interconnects etc. This is important to get an idea of sharding-latency, mounted filesystems, network traffic etc, which will affect reported measurements, as well missing step-time.\n\n[1] Mekkouri et al: Fine-tuning LLMs to 1.58bit: extreme quantization made easy. Huggingface blog post, 2024.\n[2] Wei et al: RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models. ICML 2025.\n[3] Nielsen et al: Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models? ACL 2025 Findings."}, "questions": {"value": "Q1 8-bit activation, were using more or less bits considered for the activations?\n\nQ2 The authors state that that this work challenges deploying LLMs on resource constrained devices for specific tasks. While this is noble and valuable advantage of 1.58-bit, we need some discussion on actually making this happen, as 2-bit tensor-cores is not supported by either AMD or NVIDIA – or even specialised FPGA’s.\n\nQ3 Would you recommend performing attention distillation in a single layer rather than across all layers? Which layers? Do you have an abblation or general insights?\n\nQ4 I would have liked some convergence-graphs to get an idea of potential differences. This is both interesting from a wall-clock point of view, as well as inspecting the objective functions impact on learning over time. Do you have any insights on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l51S4C9vzL", "forum": "hsgl5Owiio", "replyto": "hsgl5Owiio", "signatures": ["ICLR.cc/2026/Conference/Submission8791/Reviewer_KPvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8791/Reviewer_KPvv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562144480, "cdate": 1761562144480, "tmdate": 1762920565190, "mdate": 1762920565190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge in deploying LLM on resource-constrained edge devices: extreme low-bit (1.58-bit) quantization often requires expensive training from scratch to maintain performance, while existing post-training quantization (PTQ) or quantization-aware training (QAT) methods suffer from severe performance degradation and \"inverse scaling\" (wider gaps with full-precision models as model size increases). To solve this, the authors propose the BitNet Distillation Framework, a three-stage pipeline(1) modeling refinement for stable optimization, (2) continued pre-training to bridge the full-precision-to-low-bit gap, and (3) distillation-based fine-tuning to recover performance. They fine-tune pre-trained full-precision LLMs into 1.58-bit BitNet models. Extensive experiments on text classification (GLUE) and summarization (CNNDM) show BDF matches full-precision (FP16) performance while delivering 2× faster inference and 10× lower memory usage on CPUs, with compatibility across model backbones (Qwen3, Gemma3, Qwen2.5)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Edge deployment of LLMs is a pressing need for industry and academia, but extreme low-bit quantization (≤2 bits) has long been bottlenecked by high training costs or performance loss. The paper directly targets this pain point by avoiding expensive pretraining and fixing the inverse scaling issue of existing PTQ/QAT methods, filling a critical gap in low-bit LLM research."}, "weaknesses": {"value": "1. The authors note that \"distilling a single later layer outperforms all layers\" (Section 4.4) but provide limited justification for why later layers are optimal. \n\n2. The paper tests classification (GLUE) and summarization, but would be broader if validated on popular LLM benchmarks. \n\n3. The authors state continued pre-training uses \"10B tokens from FALCON corpus\" (Section 4.1) but lack details on what subset of FALCON and Preprocessing steps."}, "questions": {"value": "1. The paper  state that \"distilling a single later layer outperforms all layers\" for attention distillation, but do not define what constitutes a \"later layer\" (e.g., the last Transformer layer, top 10% of layers, or a task-specific layer)? For example, did you test distilling Layer L (last), Layer L/2 (middle), and Layer 1 (first) for Qwen3-4B, and if so, what was the performance gap between them?\n\n2. The paper report CPU throughput (1135 tokens/s) and memory (0.11GB) but do not test smaller edge hardware (e.g., edge GPUs or mobile CPUs). Does the memory reduction and speedup hold on these devices, or do hardware-specific constraints (e.g., limited cache) degrade performance?\n\n3. Can the method use a quantized teacher (e.g., 4-bit FP16) instead of a full-precision teacher? This would reduce training cost (since quantized teachers use less memory)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YaAKF7A2mc", "forum": "hsgl5Owiio", "replyto": "hsgl5Owiio", "signatures": ["ICLR.cc/2026/Conference/Submission8791/Reviewer_jkRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8791/Reviewer_jkRC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860878582, "cdate": 1761860878582, "tmdate": 1762920564182, "mdate": 1762920564182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the BitNet Distillation Framework (BDF), a method for fine-tuning full-precision large language models (LLMs) to 1.58-bit precision for downstream tasks. BDF consists of three stages: 1. Architectural modification by adding extra LayerNorms before the output projection of the multi-head self-attention module and before the output projection of the feed-forward network to stabilize activation variance and improve training convergence in low-bit settings. 2. Continue-Training using a small subset of pre-training data and 3. Distillation-based Finetuning, leveraging logits distillation and attention relation distillation (transfers relational structures between query/key/value matrices of the attention heads). The technique is demonstrated on text classification benchmarks (GLUE - MNLI, QNLI, SST2 and text summarization benchmarks (CNN/DailyMail dataset) using Qwen and Gemma series of models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results - the technique provides a practical recipe to achieve near lossless performance compared to FP16 models\n2. There are comprehensive ablations demonstrating contributions of each stage in the pipeline.\n3. The paper is clearly written and easy to follow. \n4. The framework avoids full re-training, which makes it practically usable."}, "weaknesses": {"value": "1. Limited scope of evaluation - evaluations on broader benchmarks (instruction following, reasoning etc.) are missing.\n2. Limited scope of evaluation - the technique is only evaluated on small models (largest size 4B, not clear if the ideas will generalize to other models.\n3. Lack of novelty - none of the components proposed here are novel, and the contribution is an empirical recipe, which may not scale to larger models and more complex benchmarks like reasoning and instruction following."}, "questions": {"value": "Can you compare the technique to the following: SiLQ: Simple Large Language Model Quantization-Aware Training which has similar ideas?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "klN5vBkOup", "forum": "hsgl5Owiio", "replyto": "hsgl5Owiio", "signatures": ["ICLR.cc/2026/Conference/Submission8791/Reviewer_cxAo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8791/Reviewer_cxAo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126549382, "cdate": 1762126549382, "tmdate": 1762920563763, "mdate": 1762920563763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}