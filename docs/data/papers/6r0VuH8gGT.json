{"id": "6r0VuH8gGT", "number": 5896, "cdate": 1757944175449, "mdate": 1759897946334, "content": {"title": "Balancing Fidelity and Diversity: Synthetic data could stand on the shoulder of the real in visual recognition", "abstract": "As generative models (GMs) advance, producing high-quality, photorealistic synthetic images is now feasible and increasingly common. Beyond use in entertainment, such synthetic data offers a promising solution to data scarcity in AI research. With the growing research interest in using synthetic data, a critical question arises: How can we maximize performance gains for downstream tasks using synthetic data? Fine-tuning or prompt engineering are two common strategies to fully exploit the potential of GMs to produce task-specific synthetic data. However, unlike these tedious optimizations, is it possible to exploit the data potential by selecting an optimal synthetic subset from a given pool? Motivated by this question, we propose an efficient data selection strategy to improve the utility of existing synthetic datasets without adjusting the GMs' output. Experiments on several benchmarks demonstrate that balancing the trade-off between fidelity and diversity in synthetic data benefits model performance and robustness. In summary, this paper presents a practical and scalable approach to harnessing synthetic data, particularly valuable in scenarios where customizing the outputs of generative models is infeasible.", "tldr": "Proposing scoring method, we select synthetic data for model training. Results show that balancing fidelity and diversity is key to unlock the potential of generative data in visual recognition.", "keywords": ["Synthetic data selection", "Generative model", "Image classification"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab22919ccfbe679cf45bd2ff686944aa919cbc5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors look at the problem of synthetic training data, which they look at through the lens of dataset selection. In this way, they are able to examine the effects of balancing fidelity and diversity. The method itself consists of first dividing the real data into a homogenous set and a heterogeneous set. This is defined by first extracting features and comparing pairwise cosine similarities--any images that are the closest neighbor are considered homogenous, and the rest are heterogeneous. The synthetic data is split into these sets by scoring the cosine similarity with each of the sets. They evaluate on SVHN, CIFAR-10, Tiny ImageNet, and ImageNet, on ViT-B/16 and ResNet-50, and in both training-from-scratch and fine-tuning settings. They also evaluate OOD generalization, and perform additional analysis on impacts of the real data, diversity and fidelity trade-off, scaling, and feature extractor choice."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1) Certain parameters (training setting = from scratch / fine-tuned, model = ViT-B/16 / ResNet-50, feature extractor = SigLIP / DINOv3 / ViT) are well explored.\n\nS2) The fidelity-diversity trade-off is well-defined in literature, making it an interesting area to explore.\n\nS3) The high-level motivations of the paper are written clearly.\n\nS4) To my knowledge, this type of set-balancing has not been previously explored."}, "weaknesses": {"value": "Major Weaknesses:\n\nW1) The related works are missing important context, specifically in two broad areas:\nW1a) This is very related to dataset sub-selection, as defined in [A]. It would be interesting to discuss how this work draws from that literature.\nW1b) In the field of synthetic data, the choice of generation model is incredibly significant. It would be interesting to discuss the base models.\n\nW2) In section 3.1, the experiment seems misaligned with the downstream task. Specifically, this section measures the trade-offs between fidelity and diversity, as a way to justify the methods later. However, here diversity is applied with low-level data augmentation techniques (cropping, rotation, color jittering). Diversity in this form is not what we may expect from a generative model, which may generate the object unrealistically or entirely wrong. Therefore, these seem to be the wrong augmentations to fit with the later work.\n\nW3) This work proposes a very specific strategy for selecting the two sets in the real data (if an image is a nearest neighbor), but this choice is never ablated although there are other straightforward options (e.g. calculating a class centroid and classifying by distance). This makes it unclear if this is the best strategy. The contribution would be stronger if selection strategy were discussed or ablated.\n\nW4) I find some claims in the paper too strong for the evidence provided. Specifically:\nW4a) Line 216/217 claim: GMs more readily learn and reproduce HOMO\nset containing repeated canonical pattern. However, it involves two unrelated datasets: the (generally fixed, when fixed to a specific generative model) synthetic distribution and the target real data distribution. Hence, whether a generative model generates HOMO or HETERO images depends on the target real distribution (perhaps in the clearest-to-imagine case, a target dataset could be constructed that is closer to HETERO). This statement would be much clearer if it were more specific, for example discussing natural datasets, the datasets in this work, distribution similarities between the test dataset and the training dataset for the generative model, etc, and it supported the claims.\nW4b) In line 419, the 'dataset complexity' is referenced. This is a bit vague, and the statements would be stronger if it were defined more precisely.\nW4c) The statements in the section in lines 448-452 seem strong for the evidence provided. Specifically, there are other unexplored factors that may impact the performance difference--e.g. this work uses MoCo V3, SigLIP, DINOv3, and ViT for features, but CLIP score uses CLIP.\n\nW5) The HETERO partition seems to be computed very different mathematically for the synthetic data (distance from reference features) than the real dataset (not similar to any individual sample), which introduces some methodological inconsistencies. This choice would be stronger if there were strong discussion / justification for this choice.\n\nW6) The datasets used do not provide strong enough validation. Specifically: a) there are too few when compared to other literature, b) some of them have much lower resolution than most generative models, and therefore results are not as convincing (e.g. Tiny ImageNet, CIFAR10), c) half of them have very low number of classes compared to many modern datasets (10), d) there is relatively little diversity between subjects. I say these specifically with respect to other literature in this area, which uses a much more diverse set of 10 datasets including broad / fine-grained classes, higher resolution, many datasets with 100+ classes. Two papers (also not cited in the related works) which use this are [B] and [C].\n\nW7) The paper uses a full-dataset setting, but does not compare with other literature using this in other ways (e.g. full fine-tuning, just training with the real data instead of synthetic data, training with real and synthetic data as done in [B]). Comparing directly to these other methods in both resource consumption and performance is critical to fully understanding the best uses of synthetic data.\n\nMinor Weaknesses:\n\nw1) Most of the figures are not referenced in the text.\n\nw2) The introductory paragraphs of the experiments seem to miss some important information that could be helpful for readers. Specifically, training setting (from scratch or real), how Sp is chosen, choice of generative model.\n\nw3) The arrangement of figures seems a bit scattered--some (e.g. 6) are very far from their references, making them hard for readers to locate.\n\nw4) The naming of the data subsets may be reconsidered--the words have other inappropriate connotations, and homo specifically is widely considered a slur.\n\n[A] DataComp: In search of the next generation of multimodal datasets, Gadre et al., NeurIPS 2023.\n[B] DataDream: Few-shot Guided Dataset Generation, Kim et al., ECCV 2025.\n[C] Diversified in-domain synthesis with efficient fine-tuning for few-shot\nclassification, da Costa et al."}, "questions": {"value": "Q1) I don't fully understand the scoring mechanism for the partitions. The equation for Sp in line 267 refers to cosine similarity between two sets, but how are these values aggregated? Also is this happening at a set level or an individual level for each image? (the Fs are defined as sets, but perhaps it is scoring each element in the synthetic set with the full real partition?)\n\nQ2) I am also confused about how fidelity and diversity are scored--in line 257, it is said that it is repeated for each partition (HOMO, HETERO). However, then fidelity and diversity are both calculated from a partition p--does this mean there are 4 values? fidelity + diversity for HOMO, and fidelity + diversity for HETERO?\n\nQ3) Which generative model is used? I did not find it in the text, and it is highly impactful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lnKnawEENE", "forum": "6r0VuH8gGT", "replyto": "6r0VuH8gGT", "signatures": ["ICLR.cc/2026/Conference/Submission5896/Reviewer_ayvz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5896/Reviewer_ayvz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760542600114, "cdate": 1760542600114, "tmdate": 1762918334049, "mdate": 1762918334049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how balancing fidelity and diversity in synthetic data selection impacts model performance. The study employs both a standard image classification task and an out-of-distribution classification task to analyze this relationship. Fidelity and diversity are quantitatively defined by comparing each synthetic image with its corresponding real reference image. Based on this analysis, the paper proposes a novel synthetic data selection method, and experimental results show that applying this method improves accuracy compared to existing selection approaches under various setups, including training from scratch, fine-tuning, and using different dataset scales (1M and 3M synthetic samples)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Exploring the relationship between fidelity and diversity addresses a very interesting, practical, and challenging problem in data generation.\n\n- The approach of splitting the real dataset into HOMO and HETERO subsets to measure these two properties is impressive; the splitting method appears both novel and conceptually sound.\n\n- Experimental results demonstrate that the proposed selection method effectively improves performance, confirming its practical utility across different training setups."}, "weaknesses": {"value": "**Definition of Fidelity and Diversity**\n\na. The definition of fidelity and diversity appears conceptually unclear.\n\nb. Algorithm 1 is difficult to follow:\n\n- In line 1, does axis = 1 refer to the first axis (1-indexed) or the second axis (0-indexed)?\n\n- In line 2, is a single sample, $\\mathcal{F}^{ref}$, representing the entire synthetic dataset, or one per synthetic instance?\n\n- Does it (lines 3–9) apply to each synthetic sample individually, or to the entire synthetic dataset at once?\n\n- In line 4, $\\mathcal{F}^{syn} \\in \\mathbb{R} ^ {s \\times d}$ and $\\mathcal{F}^{p} \\in \\mathbb{R} ^ {(n \\text{ or } m) \\times d}$ have different dimensions—how can cosine similarity be computed between them? A similar dimensionality issue arises in line 6 among $\\mathcal{R}^{ref} \\in \\mathbb{R} ^ {d}$, $\\mathcal{F}^{p} \\in \\mathbb{R} ^ {(n \\text{ or } m) \\times d}$, and $\\mathcal{F}^{syn} \\in \\mathbb{R} ^ {s \\times d}$.\n\nc. Beyond these ambiguities, it is questionable whether a single sample (i.e., $\\mathcal{F}^{ref}$) can represent the entire distribution of the HETER set.\n\nd. Similarly, comparing each synthetic image only to one representative sample (either $\\mathcal{F}^{ref}$ or $\\mathcal{C}^{Homo}$) may not capture the full distributional diversity within the real subsets (HOMO and HETER).\n\n**Presentation and Clarity of Experiments**\n\na. The experimental setup and results presentation require clarification.\n\n- What is the main distinction between the classical classification and OOD tasks in the context of applying the proposed selection method? Are they treated simply as two independent tasks?\n\n- In Table 1, the terms Random and Realism are undefined—please clarify what these baseline methods represent.\n\n- What does “+0” mean in Random selection in Table 1? Does it indicate that no synthetic data were used?\n\n- More details are needed on how synthetic images were generated from each dataset. For instance, if SVHN was used, was the generator trained on the SVHN training set and then used to arbitrarily generate samples?\n\n- In Figure 8, how was the violin-plot distribution obtained? Were multiple synthetic datasets generated and evaluated?\n\n**Minor Concerns**\n\na. Figure numbering is inconsistent, and some figures are placed far from their first reference, making the paper difficult to follow.\n\nb. Although the proposed selection method shows slightly better accuracy than compared methods in Table 1, the margin is small. A discussion comparing computational cost versus accuracy improvement would be beneficial to assess the method’s practical value.\n\nc. There is some confusion between the main manuscript and the supplementary materials. Several figures that are substantially discussed in the main paper are located in the supplementary section. Reviewers should not be required to read the supplementary materials to fully understand the main content."}, "questions": {"value": "- In Figure 9, why does balancing between fidelity and diversity by controlling $\\alpha$ affect the generators differently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't have any ethical concern."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AgkQWiD6XM", "forum": "6r0VuH8gGT", "replyto": "6r0VuH8gGT", "signatures": ["ICLR.cc/2026/Conference/Submission5896/Reviewer_JRWP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5896/Reviewer_JRWP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393705319, "cdate": 1761393705319, "tmdate": 1762918333810, "mdate": 1762918333810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of selecting high-quality synthetic data for training visual recognition models. The authors propose a post-generation curation strategy that balances fidelity and diversity. The key innovation is partitioning real data into HOMO and HETERO subsets, then scoring synthetic samples based on their relationship to both partitions. Experiments across CIFAR-10, Tiny-ImageNet, ImageNet-1K, and SVHN demonstrate that this approach improves both in-domain accuracy and out-of-domain robustness compared to baseline selection methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-written\n- The paper is clearly structured and easy to follow\n\n2. HOMO-HETERO partitioning\n- The intuitive split of real data based on intra-class similarity provides an interpretable framework for understanding data characteristics.\n\n3. Comprehensive experiments\n- The paper includes extensive experiments across multiple datasets (SVHN, CIFAR-10, Tiny-ImageNet, ImageNet-1K), architectures (ResNet, EfficientNet, ViT), and evaluation settings (in-domain and OOD).\n\n4. Extensive analysis\n- The paper provides insightful analysis on the relationship between synthetic data quality, fidelity-diversity trade-offs, and the impact of different feature extractors."}, "weaknesses": {"value": "1. Limited theoretical justification\n- While the HOMO-HETERO split is intuitive, the paper lacks theoretical grounding for why this particular partitioning strategy is optimal. The definition (Equations 3,4) is somewhat arbitrary.\n\n2. Novelty\n- The fidelity score is essentially cosine similarity, and the diversity score is a relatively straightforward angle-based metric. The main contribution is the partitioning strategy rather than the scoring itself. I hope the authors can discuss how to strengthen the novelty of this work.\n\n3. Computational cost\n- The paper doesn't analyze the computational overhead of feature extraction, similarity computation, which could be significant for large-scale datasets.\n\n4. Dependence on feature extractor\n- While Section A.5 provides some ablation, the choice of feature extractor (MoCo v3) has not been explored."}, "questions": {"value": "1. The paper consistently uses $\\alpha = 0.5$ throughout experiments, but Figure 9 shows that optimal α varies by dataset. Can you explain the rationale for fixing $\\alpha = 0.5$?\n\n2. As mentioned in the Weakness section, I would appreciate if the authors could discuss how to enhance the novelty of this contribution.\n\n3. All experiments are on image classification. Can this work be expanded to other tasks to demonstrate generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Because this paper proposes methods for synthetic data curation and selection, adding an Ethics section would be valuable."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4jHuvoTyb", "forum": "6r0VuH8gGT", "replyto": "6r0VuH8gGT", "signatures": ["ICLR.cc/2026/Conference/Submission5896/Reviewer_aJnE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5896/Reviewer_aJnE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718070996, "cdate": 1761718070996, "tmdate": 1762918333477, "mdate": 1762918333477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a post-generation curation framework for selecting high-quality synthetic data to improve visual recognition models. The authors argue that both fidelity (semantic similarity to real data) and diversity (novel variations) are crucial for maximizing the utility of synthetic datasets. They propose partitioning real data into homogeneous (HOMO) and heterogeneous (HETERO) subsets and scoring synthetic samples by their fidelity and diversity relative to each subset. The method, which is generator-agnostic and training-free, consistently enhances in-domain and out-of-domain accuracy across various datasets and models. Experiments demonstrate that carefully balancing fidelity and diversity yields better generalization and robustness than existing data selection strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and practically important problem (i.e., how to effectively select synthetic data rather than merely generating it) and provides a simple yet principled solution.\n\n2. The paper is well-written, well-organized, and provides clear visualizations that make the proposed method and its effects intuitively understandable."}, "weaknesses": {"value": "1. The proposed framework relies heavily on the quality and representation power of the feature extractor (e.g., CLIP, SigLIP). As a result, the selection outcome may be biased or unstable when different embedding spaces are used, limiting the generality of the method across various encoders or modalities.\n\n2. While the HOMO–HETERO partition is conceptually intuitive, it lacks a formal theoretical justification or analysis explaining why this separation leads to optimal or stable performance. Without such grounding, the effectiveness of the split may vary significantly across datasets with different intrinsic structures.\n\n3. Although the authors claim that the proposed selection method is efficient and scalable, these aspects are discussed only qualitatively. The paper would benefit from a quantitative evaluation of computational cost, such as runtime or memory usage as a function of dataset size, to substantiate the scalability claim.\n\n4. The proposed method appears to incorporate a core-set–like selection mechanism into the fidelity–diversity framework, aiming to choose a representative yet diverse subset of synthetic samples. While this idea is conceptually related to existing core-set selection principles, the paper does not clearly differentiate its approach or justify how it fundamentally extends beyond standard core-set algorithms in either theoretical formulation or empirical advantage."}, "questions": {"value": "Please address the comments in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kgiai5CqYO", "forum": "6r0VuH8gGT", "replyto": "6r0VuH8gGT", "signatures": ["ICLR.cc/2026/Conference/Submission5896/Reviewer_Z8Sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5896/Reviewer_Z8Sg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978045071, "cdate": 1761978045071, "tmdate": 1762918333115, "mdate": 1762918333115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}