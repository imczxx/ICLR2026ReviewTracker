{"id": "hqFNqVQEUd", "number": 15788, "cdate": 1758255251940, "mdate": 1763750924633, "content": {"title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning", "abstract": "Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present $\\textit{Embodied Reasoning Agent (ERA)}$, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL).\nThe first stage, $\\textit{Embodied Prior Learning}$, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. \nIn the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. \n Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Detailed Ablation studies further validate the effectiveness of different prior datasets and agent RL designs. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.", "tldr": "We present ERA, a two-stage training framework including embodied prior learning and online reinforcement learning, enables 3B VLMs to achieve SOTA on both high-level and low-level embodied tasks.", "keywords": ["Embodied agent", "VLM reasoning", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1dcb1520fe32c31389bea6ced6ff02767241a5ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Embodied Reasoning Agent (ERA), a two-stage training framework that bridges the gap between large and small vision-language models in embodied tasks. The first stage introduces three forms of prior knowledge—trajectory-augmented, environment-anchored, and external priors—to enhance reasoning and perception, while the second stage applies improved PPO-based online reinforcement learning with self-summarization, dense reward shaping, and turn-level optimization. Evaluated on EmbodiedBench (EB-ALFRED and EB-Manipulation), ERA achieves strong performance with a 3B model, surpassing larger baselines and providing insights into effective prior design and RL strategies for embodied agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed taxonomy of embodied priors (trajectory-augmented, environment-anchored, and external) is conceptually clear and provides valuable guidance for future data curation and training strategies in embodied AI research.\n\n2. The paper presents a well-structured two-stage framework that combines knowledge-based pretraining and online reinforcement learning, offering a practical and scalable approach to improving small VLMs for embodied reasoning tasks."}, "weaknesses": {"value": "1. Although the experiments on EmbodiedBench demonstrate strong performance, this benchmark mainly involves QA-style evaluation, which differs significantly from real embodied agent control. While representing high-level plans as action primitives is reasonable, using a single action to denote the final state for low-level control is less convincing. Given that the object states and environmental conditions are already provided, generating one action prediction does not truly address the core challenges of embodied agent control.\n\n2. The method relies on a rule-based oracle to generate ground-truth visual descriptions for low-level operations, which simplifies training within a simulator but may not generalize well to real-world environments. In realistic settings with complex object appearances, occlusions, and lighting variations, such handcrafted rules are unlikely to hold, potentially leading to a distribution shift between training and deployment."}, "questions": {"value": "The proposed training paradigm aims to enhance the capabilities of VLMs so that they can function as embodied agents. From the authors’ perspective, what are the practical applications of such VLMs in embodied settings? For instance, could they be integrated with downstream action heads to form a complete vision-language-action (VLA) system?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AvpYk4zXz2", "forum": "hqFNqVQEUd", "replyto": "hqFNqVQEUd", "signatures": ["ICLR.cc/2026/Conference/Submission15788/Reviewer_KpUu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15788/Reviewer_KpUu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731475858, "cdate": 1761731475858, "tmdate": 1762926021512, "mdate": 1762926021512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Embodied Reasoning Agent (ERA), a two-stage framework that transforms small vision-language models (VLMs) into capable embodied agents. In the first Embodied Prior Learning stage, the model is enriched with structured reasoning data across three priors — trajectory-augmented, environment-anchored, and external knowledge. This enables stronger spatial grounding and step-level reasoning. The online RL stage further refines the agent through self-summarization, dense multi-component rewards, and turn-level policy optimization. Experiments on EB-ALFRED and EB-Manipulation benchmarks show that ERA-3B outperforms larger zero-shot models like GPT-4o and prior training-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated problem: The paper tackles a highly relevant challenge: scaling embodied reasoning in small VLMs, addressing both computational efficiency and reasoning capability, which are central issues in current LLM+RL research.\n- Clear and modular framework design: The two-stage ERA pipeline (Embodied Prior Learning + Online RL) is logically structured, easy to follow, and each component contributes meaningfully to the final performance. The three priors (trajectory-augmented, environment-anchored, and external knowledge) form an interpretable system for grounding, reflection, and planning.\n- Strong experimental validation: Extensive evaluations on both EB-ALFRED (high-level planning) and EB-Manipulation (low-level control) benchmarks demonstrate consistent gains over strong baselines, including GPT-4o zero-shot and VAGEN. Ablations further justify each component’s contribution.\n- Readable and well-presented: The paper is clearly written, with smooth organization and detailed appendices."}, "weaknesses": {"value": "- It is unclear whether the training dataset consists solely of expert trajectories or includes diverse-quality rollouts containing suboptimal or failed episodes. The paper mentions that RL can leverage such data, but if diverse-quality trajectories are indeed used, it is questionable whether attaching reasoning supervision to low-quality or failed samples is appropriate or effective. Clarifying how reasoning augmentation interacts with data quality would help assess the robustness of the proposed approach.\n- The results in 4.2 suggest that the majority of the improvement comes from trajectory-augmented priors, which essentially correspond to CoT-style reasoning supervision. This gives the impression that the performance gains are largely dominated by the explicit reasoning annotations rather than from a strong technical contribution.\n- For high-level planning, the paper states that penalties are applied to invalid actions the environment cannot execute, but it is unclear how these invalid actions are detected without environment feedback."}, "questions": {"value": "See weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eG8yvuQG9H", "forum": "hqFNqVQEUd", "replyto": "hqFNqVQEUd", "signatures": ["ICLR.cc/2026/Conference/Submission15788/Reviewer_g8Bi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15788/Reviewer_g8Bi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924526419, "cdate": 1761924526419, "tmdate": 1762926021120, "mdate": 1762926021120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their time and effort in evaluating our work and for recognizing that it is well-motivated (Reviewer g8Bi), addresses an important problem in embodied AI (Reviewers 4zRJ, g8Bi), proposes a practical approach (Reviewers KpUu, kXqj), conducts comprehensive experiments (Reviewers 4zRJ, g8Bi), and offers useful guidance for future embodied AI research (Reviewer KpUu). We appreciate the constructive comments from reviewers. At the same time, we noticed that several concerns stem from misunderstandings of our problem setting and contributions.\n\n**Our work targets training VLM-based embodied agents that interact with an environment via multi-turn observation–reasoning–action loops to achieve language-specific goals, which is different from standard single-turn question-answering (QA) VLMs.** We highlight and address two key challenges in this setting:\n\n* **Lack of sufficient agent trajectory data (Section 3.1)**. Unlike conventional LLM/VLM training, where large-scale datasets are readily available, embodied agent trajectories must be collected step by step via environment interaction, making them extremely costly. To tackle this, we propose Embodied Prior Learning (EPL), which systematically exploits three data sources: (i) augmenting existing agent trajectories, (ii) generating environment-anchored QA data to enhance reasoning and grounding at larger scale, and (iii) large out-of-environment data that is cheap to obtain. We validate their effectiveness and analyze their different impacts on in-distribution and out-of-distribution tasks, as well as how to best combine them in Section 4.1 and Section 4.2.\n\n* **Stability of RL for VLM-based multi-turn agents (Section 3.2).** Training multi-turn VLM agents with RL is substantially more unstable than training single-turn QA models. We identify three crucial components: history context management(exposing the agent to the right amount of state information), reward design under sparse multi-step feedback, and policy optimization. We found that after training the model to summarize historical information by itself at each step, we can use its summarization to effectively reduce the context length. Moreover, we show that conventional token-level policy optimization is unstable for VLM agents, and that our turn-level policy optimization yields stronger policies. We provide detailed studies and ablations across both high-level planning and low-level manipulation tasks in Section 4.3 and Section 4.4 to guide future work.\n\nDuring the rebuttal, we additionally:\n* **Provide zero-shot real-robot experiments using our ERA model**, highlighting strong real-world generalization, achieving an average accuracy of 57.5% on 40 task variations.\n* **Clarify technical details regarding data curation**, reward functions, context management, and turn-level optimization.\n* **Update the paper** to incorporate these results and clarifications.\n\nWe hope these explanations resolve the main concerns and clearly illustrate the novelty and practical impact of our approach. We are happy to further discuss any remaining questions."}}, "id": "3vkHGW5HYP", "forum": "hqFNqVQEUd", "replyto": "hqFNqVQEUd", "signatures": ["ICLR.cc/2026/Conference/Submission15788/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15788/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15788/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720519669, "cdate": 1763720519669, "tmdate": 1763720564783, "mdate": 1763720564783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ERA (Embodied Reasoning Agent), a two-stage framework for training compact vision-language models (VLMs) as embodied agents. Stage 1 (Embodied Prior Learning) fine-tunes VLMs on three types of curated data: trajectory-augmented priors (enriched with GPT-4o-generated reasoning), environment-anchored priors (QA pairs and grounding tasks), and external knowledge priors (math and spatial reasoning datasets). Stage 2 applies online reinforcement learning with self-summarization for context management, dense reward shaping, and turn-level policy optimization. The authors evaluate on EmbodiedBench, claiming their 3B model outperforms GPT-4o on both EB-ALFRED (high-level planning) and EB-Manipulation (low-level control) tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive experimental setup: The paper evaluates on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks, providing broad coverage of embodied reasoning challenges.\n\n\nPractical focus: The work addresses an important problem of making smaller VLMs viable for embodied tasks, which has practical implications for deployment."}, "weaknesses": {"value": "Major Issues\n\n1. Limited Technical Novelty\n\nThe proposed approach is primarily an engineering combination of existing techniques without significant innovation:\n\nUsing stronger models (GPT-4o) to augment training data with reasoning traces is standard practice in recent work (Zelikman et al., 2022; Wei et al. 2022)\n\nEnvironment-anchored priors (masked action modeling, action sequence reordering) are straightforward data augmentation techniques similar to BERT-style pre-training (Devlin et al., 2019)\n\nExternal knowledge priors simply leverage existing datasets (OpenO1-SFT, SpaceThinker) without novel curation strategies\nTurn-level policy optimization is an incremental modification of standard PPO (Schulman et al., 2017)\n\nSelf-summarization is essentially maintaining only the last reasoning step, which is a trivial context management strategy\n\n2. Simulation-Only Evaluation Severely Limits Impact\n\nAs acknowledged in Section A, all experiments are conducted in simulation without any real-world validation. This raises serious concerns:\n\nSim-to-real transfer for embodied AI remains a major unsolved challenge (Zhao et al., 2020; Pinto et al., 2018)\n\nClaims about \"practical path toward scalable embodied intelligence\" (Abstract) are not supported\n\nVisual perception in simulation is dramatically simpler than real-world scenarios (Tremblay et al., 2018)\n\nThe rule-based ground truth visual descriptions for EB-Manipulation (Section H.7) further undermines realism\n\n3. Unfair and Misleading Comparisons\n\nThe main claim of outperforming GPT-4o is problematic:\n\nComparing a task-specific fine-tuned 3B model against zero/few-shot prompted large models is fundamentally unfair (Brown et al., 2020)\n\nNo comparison with fine-tuned versions of larger VLMs (e.g., fine-tuned LLaVA-34B, Qwen-VL-72B)\n\nThe paper doesn't discuss what performance GPT-4o might achieve if fine-tuned on the same data\n\nThis comparison is like claiming a task-specific BERT model outperforms GPT-4 on a specialized task where GPT-4 is used zero-shot\n\n4. Data Leakage and Limited Generalization Evidence\n\nThe \"unseen\" test sets (Common Sense and Spatial) come from the same simulators and task distributions as training data\n\nTrue generalization would require evaluation on completely different environments, tasks, or real-world scenarios (Cobbe et al., 2019)\n\nThe 8.4% and 19.4% improvements over GPT-4o on held-out sets may simply reflect overfitting to the specific benchmark rather than genuine capability improvement\n\n5. Insufficient Analysis of Computational Costs\n\nThe paper fails to account for total computational costs:\n\nGenerating reasoning augmentations with GPT-4o for all trajectories (8,834 samples for EB-ALFRED, 4,249 for EB-Manipulation)\n\nFine-tuning the 3B model through EPL (2-5 hours × 2 nodes of H200 GPUs)\n\nOnline RL training (12 hours × 2 H200 GPUs for each task)\n\nThese costs likely exceed inference costs of directly using GPT-4o, undermining claims about efficiency\n\n6. Questionable Technical Choices\n\nRule-based visual descriptions (Section H.7): Using oracle ground truth visual descriptions for EB-Manipulation contradicts claims about visual perception and makes results less meaningful\n\nSelf-summarization: Simply keeping the last reasoning step (Section 3.2.1) is trivial and doesn't address the fundamental challenge of long-horizon memory\n\nTurn-level GAE: While shown to help empirically, the paper doesn't provide theoretical justification or deeper insights beyond variance reduction\n\nMinor Issues\n\n7. Presentation Issues\n\nThe taxonomy of \"priors\" (trajectory-augmented, environment-anchored, external knowledge) feels artificial and doesn't provide conceptual clarity\n\nFigures 1 and 2 are cluttered and hard to parse\n\nThe related work section lacks critical analysis and reads more like a literature survey\n\n8. Incomplete Experimental Analysis\n\nNo analysis of what specific capabilities each prior type provides beyond aggregate performance numbers\n\nLimited error analysis (Section I) is qualitative and anecdotal rather than systematic\n\nNo investigation of failure modes or limitations of the approach\n\nMissing ablations on key hyperparameters (e.g., reward coefficients, RL training duration)"}, "questions": {"value": "Computational cost comparison: Can you provide a detailed breakdown of total computational costs (including GPT-4o augmentation, EPL training, and RL training) compared to the cost of directly using GPT-4o or Claude-3.5-Sonnet for inference? Given that you generate reasoning augmentations for ~10K samples using GPT-4o, how does this compare to just using GPT-4o for the task?\n\nFair comparisons: What would be the performance if you fine-tuned larger models (e.g., Qwen2.5-VL-72B) on the same data? Have you considered comparing with other fine-tuned models of similar size?\n\nReal-world validation: The main limitation is simulation-only evaluation. What are the concrete plans for real-world validation? What sim-to-real transfer challenges do you anticipate?\n\nVisual description oracle: For EB-Manipulation, you use rule-based ground truth visual descriptions (Section H.7). How does performance degrade when using the model's own generated visual descriptions? Doesn't this oracle information make the results less meaningful?\n\nGeneralization: The \"unseen\" test sets come from the same simulators and task families. How would ERA perform on completely different environments or real-world scenarios? Have you tested on any truly out-of-distribution settings?\n\nPrior analysis: Can you provide more detailed analysis of what each prior type contributes? For example, by measuring specific capabilities (spatial reasoning, temporal reasoning, visual grounding) rather than just end-task performance?\n\nTurn-level GAE justification: Beyond empirical results, can you provide theoretical or intuitive justification for why turn-level GAE should work better than token-level approaches? Is this just variance reduction or is there something deeper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qcC6pKifnD", "forum": "hqFNqVQEUd", "replyto": "hqFNqVQEUd", "signatures": ["ICLR.cc/2026/Conference/Submission15788/Reviewer_4zRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15788/Reviewer_4zRJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995546794, "cdate": 1761995546794, "tmdate": 1762926020672, "mdate": 1762926020672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ERA, a two-stage framework that transforms compact vision-language models into embodied agents by combining embodied prior learning from structured multimodal data with online reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper demonstrates a practical method to endow small VLMs (3B) with embodied reasoning abilities, supporting the claim of achieving performance comparable to large models (e.g., GPT 4o)."}, "weaknesses": {"value": "- The contribution of this paper appears largely limited to constructing datasets for embodied learning. While the proposed Embodied Prior Learning (EPL) and online RL pipeline are well-integrated, the framework mainly combines existing components without introducing fundamentally new methodological insights.\n\n- The reasoning data are generated using GPT-4o, which makes the approach resemble a large-to-small model distillation process. Indeed, the model trained with EPL alone achieves performance very close to GPT-4o, raising the question of how this differs conceptually and empirically from standard distillation. Clarifying this distinction would strengthen the contribution.\n\n- The reward design in the online RL stage is relatively straightforward, relying on success, subgoal, and behavior (affordance) rewards derived from environment-specific assumptions (e.g., PDDL-based state access), rather than a novel or learnable reward mechanism.\n\n- The experimental comparisons are limited. In particular, for the manipulation tasks, the baselines should include recent Vision-Language-Action (VLA) methods specifically designed for robotic control, such as e.g., [1-3], to more rigorously validate the claimed effectiveness.\n\n[1] Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246 (2024). \n\n[2] Rt-2: Vision-language-action models transfer web knowledge to robotic control. Conference on Robot Learning. PMLR, 2023. \n\n[3] $\\pi_ {0.5} $: a Vision-Language-Action Model with Open-World Generalization. arXiv preprint arXiv:2504.16054 (2025)."}, "questions": {"value": "- How does Embodied Prior Learning (EPL) fundamentally differ from standard distillation, both conceptually and empirically?\n\n- How does ERA handle reward computation in partially observed or non-simulated environments?\n\n- What specific behavioral or reasoning capabilities emerge only after the RL stage, beyond what EPL already provides?\n\n- How well does ERA generalize to new or physical environments (including real world settings) beyond the EB-ALFRED, EB-Manipulation simulations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xuq7ojjlsU", "forum": "hqFNqVQEUd", "replyto": "hqFNqVQEUd", "signatures": ["ICLR.cc/2026/Conference/Submission15788/Reviewer_kXqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15788/Reviewer_kXqj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001872619, "cdate": 1762001872619, "tmdate": 1762926019261, "mdate": 1762926019261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}