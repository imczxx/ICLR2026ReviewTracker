{"id": "SzQGRR65c0", "number": 8962, "cdate": 1758104449383, "mdate": 1759897751559, "content": {"title": "Microarchitecture Is Destiny: Performance and Accuracy of Quantized LLMs on Consumer Hardware", "abstract": "While the deployment of out-of-the-box quantization on consumer-grade hardware is widespread, its impact on Large Language Models (LLMs) reveals a complex, twofold phenomenon that questions the prevailing assumption. This study presents a rigorous empirical evaluation across four generations of NVIDIA GPUs, uncovering two core, often counter-intuitive, findings. First, contrary to the prevailing view that quantization universally degrades performance on complex tasks, the analysis demonstrates that for large models (14B+ parameters), popular 8-bit and 4-bit quantization schemes can yield substantial accuracy improvements on mathematical reasoning benchmarks compared to their 16-bit floating-point counterparts, which suffer from system-level bottlenecks in resource-constrained environments. Second, the investigation reveals that for smaller models prone to overfitting, the noise introduced by these same quantization schemes can act as an effective computational regularizer, unexpectedly enhancing generalization. The performance analysis further establishes that once VRAM capacity is met, the GPU microarchitecture's support for low-precision integer arithmetic, rather than VRAM size, becomes the primary determinant of inference throughput. These findings provide a more nuanced perspective that moves beyond a simplistic trade-off, offering practitioners an evidence-based framework for navigating the interplay between model scale, hardware capabilities, and reasoning fidelity.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Quantization", "Post-Training Quantization (PTQ)", "Consumer Hardware", "GPU Microarchitecture", "Tensor Cores", "Inference Performance", "System-level Bottlenecks", "Quantization as Regularization", "System-Aware Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f868242dad66d0c4d21d16057b7f1cadc5b45b2.pdf", "supplementary_material": "/attachment/700ab37ac01ed0a1e14615638788ff22962b987d.zip"}, "replies": [{"content": {"summary": {"value": "This paper empirically studies how quantized large language models (LLMs) perform across multiple GPU generations. The authors evaluate four NVIDIA architectures (Pascal, Turing, Ampere, and Ada Lovelace) using three model families (Qwen 2.5, DeepSeek-R1, QwQ-32B) and three datasets (GSM8K, SQuAD, CMMLU).\nThey report three main findings: (1) once VRAM capacity suffices to load a model, inference throughput is primarily determined by Tensor Core generation and memory bandwidth; (2) quantization can improve accuracy for large models by mitigating system-level bottlenecks; and (3) quantization can behave as an implicit regularizer for smaller models, improving generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments span four GPU generations, multiple model sizes, and diverse benchmarks, offering strong empirical coverage.\n\n- The compute-bound versus bandwidth-bound distinction provides clear insight into performance bottlenecks during inference."}, "weaknesses": {"value": "- The result that VRAM capacity acts as a bottleneck is not novel. It is already well-understood that once a model fits in memory, inference becomes bandwidth-bound.\n- The paper dedicates significant space to infrastructure and setup details, but lacks deeper theoretical grounding to explain the observed behaviors (e.g., why quantization sometimes improves accuracy).\n- Only q4_K_M and q8_0 quantization schemes are tested. Excluding other widely used techniques such as GPTQ, AWQ, or SmoothQuant limits the generalizability of the conclusions.\n- Some experimental results (e.g., FP16 accuracy degradation) require further analysis to isolate whether they stem from framework overhead (Ollama/llama.cpp) or hardware-level constraints.\n- Evaluation on older GPUs (Pascal, Turing) offers limited relevance for modern LLM deployment, as these architectures lack key optimizations such as advanced Tensor Cores and efficient memory pipelines.\n- Minor: missing citation on line 218. Kindly proof-read the paper thoroughly."}, "questions": {"value": "- Can the FP16 accuracy drop be reproduced using native llama.cpp or TensorRT-LLM to rule out framework-induced effects?\n- How consistent are the accuracy improvements across random seeds and datasets?\n- Would including modern quantization algorithms (e.g., GPTQ or AWQ) alter the key trends?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dKuR5cy4DW", "forum": "SzQGRR65c0", "replyto": "SzQGRR65c0", "signatures": ["ICLR.cc/2026/Conference/Submission8962/Reviewer_ERPp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8962/Reviewer_ERPp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761309299945, "cdate": 1761309299945, "tmdate": 1762920697221, "mdate": 1762920697221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates the accuracy and throughput impacts of quantization on consumer-grade NVIDIA GPUs across multiple generations. Authors argue that, for both small and large models, quantization noise can act as a form of regularization, yielding 4-bit and 8-bit performance that sometimes surpasses the FP16 baseline."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Measures both accuracy and throughput across multiple generations of NVIDIA consumer GPUs, providing a cross-hardware view of quantized LLM inference."}, "weaknesses": {"value": "* Questionable evaluation outcome. Table 3 reports 8-bit/4-bit models outperforming the FP16 baseline by large margins. Attributing >~20% accuracy gains to “regularization” from quantization is not credible. In practice, any quantization-induced improvements over FP16 are typically tiny (≈≤0.5%) and task-dependent.\n* Missing evidence for Section 4.1.3. The section references results that are not shown. A corresponding table/figure is needed to interpret the claims and their context.\n* Minor: L218 missing citation.\n* Minor: L349 likely refers to Table 2, not Table 1."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E10tWecF0X", "forum": "SzQGRR65c0", "replyto": "SzQGRR65c0", "signatures": ["ICLR.cc/2026/Conference/Submission8962/Reviewer_Lfti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8962/Reviewer_Lfti"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472386944, "cdate": 1761472386944, "tmdate": 1762920696843, "mdate": 1762920696843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the impact of quantization on large language models (LLMs) in terms of accuracy and inference speed across various GPUs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "No clear strengths are identified in this paper."}, "weaknesses": {"value": "- While the evaluation setup (Section 3.3 and 3.4) describes various types of LLMs and tasks, the analysis is conducted only on a single LLM (Qwen2.5) and a single dataset (GSM8K), which limits the generality and robustness of the conclusions.\n- The findings mostly restate well-known characteristics of quantization. For example, that quantization can occasionally improve accuracy when a model is excessively large relative to the simplicity of the target task.\n- When the model fits entirely within GPU memory, it is straightforward that GPUs supporting lower-precision computation will deliver better throughput. Thus, the conclusions presented in this paper lack new insight."}, "questions": {"value": "Please check the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MlZE5KhO7R", "forum": "SzQGRR65c0", "replyto": "SzQGRR65c0", "signatures": ["ICLR.cc/2026/Conference/Submission8962/Reviewer_qr1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8962/Reviewer_qr1a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804631722, "cdate": 1761804631722, "tmdate": 1762920696297, "mdate": 1762920696297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts a system-aware empirical study of two LLMs (DeepSeek-R1 and Qwen2.5) across four consumer-grade NVIDIA GPUs (1080/2080Ti/3090/4090). The authors evaluate multiple quantization methods (fp16/q8/q4) on 3 datasets (GSM8k/CMMLU/SQuAD) and surprisingly find that low-bit quantization schemes have accuracy improvements over fp16. They hypothesize that this is due to quantization working as implicit regularization for smaller models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The experiments conducted in this paper are very close to the real use case of end-users (ollama framework and consumer-grade GPUs)."}, "weaknesses": {"value": "- The experiment is conducted on limited datasets (3 datasets) with limited metrics (accuracy).\n- There might be flaws during their experiments. For example, in Table 2, the accuracy of FP16 is different on multiple GPUs. The 100% accuracy on the Q4 scheme also looks suspicious.\n- The authors do not provide a convincing explanation of how the accuracy improvement is related to implicit regularization."}, "questions": {"value": "- Please provide a detailed analysis of the experiment results. For example, in Table 2, why does fp16 have different accuracy on different GPUs? Are the experiment setups aligned?\n- Please extend the experiments to more datasets and more metrics to reproduce the observation of accuracy gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xC68q0hM64", "forum": "SzQGRR65c0", "replyto": "SzQGRR65c0", "signatures": ["ICLR.cc/2026/Conference/Submission8962/Reviewer_eNtZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8962/Reviewer_eNtZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960473876, "cdate": 1761960473876, "tmdate": 1762920695860, "mdate": 1762920695860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}