{"id": "DmoNViA41R", "number": 8551, "cdate": 1758090762665, "mdate": 1763707166164, "content": {"title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation", "abstract": "For efficiency, preference alignment is often performed on compact, knowledge-distilled (KD) models. We argue this common practice introduces a significant limitation by overlooking a key property of the alignment's reference model: its distributional recall. We show that the standard KD -> Align workflow diminishes the model's capacity to align rare yet desirable behaviors, even under strong preference signals. We instead demonstrate that reversing the pipeline (i.e., Align -> KD) is essential: alignment must first be performed on a high-recall reference before distillation. Our contributions are threefold. First, we provide a minimal working explanation of how the reference model constrains preference alignment objectives at a fundamental level. Second, we validate this theory in a controllable Mixture-of-Gaussians experiment, where low-recall anchoring consistently results in suboptimal model performance. Finally, we demonstrate the same phenomenon holds in large-scale experiments with the SmolLM2 family: models aligned after KD fail to effectively align target behaviors, resulting in substantially lower reward and target precision. In contrast, our proposed Align -> KD pipeline robustly aligns these behaviors, yielding models with superior target-oriented metrics and lower variance. Together, these results establish reference-model recall as a first-order design choice in alignment, offering a clear principle: alignment must precede distillation.", "tldr": "The common \"distill-then-align\" pipeline for creating efficient, small language models leads to a \"low-recall trap\" that harms performance. We show that alignment must precede distillation.", "keywords": ["Alignment", "Reinforcement Learning with Human Feedback", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/beac745732db32f0a0d52692e329bf78d02c7593.pdf", "supplementary_material": "/attachment/eadf4d2aff06131d57a564c118f8ce047f12d195.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the common practice of performing knowledge distillation (KD) before alignment. The paper provides evidence that alignment followed by KD is better. They show this via two sets of experiments. First, they use a mixture of gaussian example to show this. Second, they use a small LM example. They show that in general KD-then-alignment is worse than alignment-then-KD in terms of precision and variance. The former often shows a better recall, though, but this comes with a huge sacrifice in precision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's main message is consistent.\n- To me, the really important point is not that K-A is better than A-K, because surely KD looses the model capacity. I think the common sense is that one should do KD at the final step in the training pipeline. The point that matters is that K-A was standard (I did not know this before) and that the gap between the two is meaningfully large as demonstrated by this paper.."}, "weaknesses": {"value": "- It is a bit confusing that there is a recall trap but at the same time, often K-A achieves a better recall (with large beta). Could you please explain this?\n- To me, the set of experiments performed is far from being comprehensive. For example, a different base LLM would be great.\n- Also, I was not quite convinced that the reward setting (9) is reasonable? Why do we need that reflection? Something like clipping would make more sense to me.\n- other minor comments\n\t- Eq 4: the target precision is defined to be the same as the overall precision. is this typo?\n\t- L424: p^* is redefined (previously defined in line 418). Please avoid this to improve readability.\n\t- L462-470: The authors imply that the disadvantage of $p_{AK}$ (having low overall recall) can be overcomed by adjusting distillation temperature. It would be great to see this confirmed empirically."}, "questions": {"value": "- How is the reward (9) justified? Why apply the 'reflection' there?\n- For off-policy DPO + K-A, one could say that the $\\pi_{ref}$ can be set to be the SFT model before KD. I wonder if this can help avoid the issues that K-A is encountering.\n- For K-A + PPO, one could use $\\pi_{ref}$ in the regularizer as the SFT model before KD. I wonder if this can help avoid the issues that K-A is encountering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G51dU4cCTw", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Reviewer_NvRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Reviewer_NvRw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552031242, "cdate": 1761552031242, "tmdate": 1762920404958, "mdate": 1762920404958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response 1 for Reviewer RXEC and v1RN"}, "comment": {"value": "## [Common response 1 for Reviewer RXEC and v1RN: Experimental setup of LLM experiments]\n\n\nIn practice of LLM KD/alignment research, we only consider to use a teacher (p'; the reference model) and a student (p''; the aligned model). This \"real-world\" scenario makes it impossible to measure a model's general capability against the true ground-truth distribution (which is unknown). To evaluate the student model trained by KD, we must rely on downstream benchmark tasks, which only evaluate specific capabilities, not the model's overall distributional quality (i.e., its general precision and recall).\n\nTo overcome this, we adopted the methodology from [*Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation, Cha & Cho, NeurIPS 2025*]. This required three models to create a controlled setup where we can measure precision and recall against the GT distribution. The table below clarifies the roles:\n\n\n|Case | GT (p*) | Pretrain (p') | Target model (p'') |\n| -------- | -------- | -------- | -------- |\n| Real-world | Real-world Text Dist. (unknown)     | Pretrain p' with sampled data from p* | Train p'' with KD->Align or Align->KD using p'     |\n| Our setting | Pretrained SmolLM2 1.7B | Pretrain p' (SmolLM2 360M) with sampled data from p* | Train p'' (135M) with KD->Align or Align->KD using p' \n\nIn our setting, the pretrained SmolLM2 1.7B model acts as the known GT text distribution (p∗). This allows us to:\n\n1. Faithfully simulate the real-world process: We train p′(360M) from p∗, and p′′(135M) from p′.\n2. Perform a more rigorous evaluation: Because we know p∗, we can calculate the true precision and recall of our final model (p′′).\n\n\nAs discussed in Lines 258-267 of the manuscript, note that precision (E_x∼p''[logp∗(x)]) measures how well the final model p′′ is aligned with the target modes within the GT distribution. Recall (E_x∼p∗[logp''(x)]) measures how well p′'' covers the entire GT distribution.\n\n\nThis setup and metrics allow us to prove, in a general and comprehensive way, that **Align $\\rightarrow$ KD consistently produces a final p′′ that is both better aligned (higher precision) and retains more general knowledge (competitive recall) than the KD $\\rightarrow$ Align pipeline**."}}, "id": "ZieCXeSmSz", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412884609, "cdate": 1763412884609, "tmdate": 1763413792132, "mdate": 1763413792132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response 1 for Reviewer RXEC and v1RN"}, "comment": {"value": "## [Common response 1 for Reviewer RXEC and v1RN: Experimental setup of LLM experiments]\n\n\nIn practice of LLM KD/alignment research, we only consider to use a teacher (p'; the reference model) and a student (p''; the aligned model). This \"real-world\" scenario makes it impossible to measure a model's general capability against the true ground-truth distribution (which is unknown). To evaluate the student model trained by KD, we must rely on downstream benchmark tasks, which only evaluate specific capabilities, not the model's overall distributional quality (i.e., its general precision and recall).\n\nTo overcome this, we adopted the methodology from [*Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation, Cha & Cho, NeurIPS 2025*]. This required three models to create a controlled setup where we can measure precision and recall against the GT distribution. The table below clarifies the roles:\n\n\n|Case | GT (p*) | Pretrain (p') | Target model (p'') |\n| -------- | -------- | -------- | -------- |\n| Real-world | Real-world Text Dist. (unknown)     | Pretrain p' with sampled data from p* | Train p'' with KD->Align or Align->KD using p'     |\n| Our setting | Pretrained SmolLM2 1.7B | Pretrain p' (SmolLM2 360M) with sampled data from p* | Train p'' (135M) with KD->Align or Align->KD using p' \n\nIn our setting, the pretrained SmolLM2 1.7B model acts as the known GT text distribution (p∗). This allows us to:\n\n1. Faithfully simulate the real-world process: We train p′(360M) from p∗, and p′′(135M) from p′.\n2. Perform a more rigorous evaluation: Because we know p∗, we can calculate the true precision and recall of our final model (p′′).\n\n\nAs discussed in Lines 310-318 of the manuscript, note that precision (E_x∼p''[logp∗(x)]) measures how well the final model p′′ is aligned with the target modes within the GT distribution. Recall (E_x∼p∗[logp''(x)]) measures how well p′'' covers the entire GT distribution.\n\n\nThis setup and metrics allow us to prove, in a general and comprehensive way, that **Align $\\rightarrow$ KD consistently produces a final p′′ that is both better aligned (higher precision) and retains more general knowledge (competitive recall) than the KD $\\rightarrow$ Align pipeline**."}}, "id": "ZieCXeSmSz", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412884609, "cdate": 1763412884609, "tmdate": 1763703973497, "mdate": 1763703973497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a critical design choice in the pipeline for creating efficient, aligned language models. The authors identify a potential flaw in the common practice of first distilling a large model into a smaller one and then performing preference alignment (`KD → Align`). They term this flaw the \"low-recall trap,\" arguing that distillation prunes rare but desirable behaviors, making them difficult to recover during alignment. The paper proposes reversing the pipeline to `Align → KD`, where a large, high-recall model is first aligned and then distilled. Through experiments on a Mixture-of-Gaussians toy problem and the SmolLM2 model family, the authors demonstrate that their proposed pipeline yields models with superior performance on target metrics, higher rewards, and greater training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper possesses several key strengths. It addresses a problem in the practical deployment of LLMs, challenging a common practice with a intuitive concept: the \"low-recall trap.\" The motivation is clear, and the proposed Align → KD alternative is simple. Furthermore, the empirical validation strategy, which uses both a controlled synthetic experiment to isolate the mechanism and a practical LLM experiment to demonstrate its relevance, is methodologically sound and provides consistent evidence for the authors' claims within the tested scope."}, "weaknesses": {"value": "While the paper addresses an important and interesting question in the practical application of LLM alignment, and its central thesis is compelling, I have several major concerns that, in my view, should be addressed before the paper can be considered for publication. The core idea is valuable, but the work in its current form would need to substantially strengthen its evidence and analysis regarding the following points: **(1)** the generalizability of the findings beyond a highly constrained and simplified experimental setting; **(2)** the reliance on idealized assumptions about the alignment signal that differ from noisy real-world scenarios; **(3)** the omission of the critical aspect of computational cost, which is the primary motivation for the very practice it critiques; and **(4)** the lack of discussion of crucial prerequisites and potential alternative solutions.\n\n**Major Weaknesses**\n\n**The Generalizability of the Findings**\n\nA primary concern is that the experimental validation, while well-executed within its scope, is conducted in a setting that may not be representative of modern, large-scale LLM applications. For the paper's claims to be broadly impactful, the following points need to be addressed:\n\n*   **Model Scale:** The experiments are limited to the SmolLM2 family (up to 1.7B parameters). It is difficult to assume that conclusions drawn from these small-scale models will hold for the state-of-the-art models (e.g., 30B+ parameters) where alignment and distillation challenges are most pronounced. The paper would be much stronger if it demonstrated that this trend persists on at least one larger, more capable model.\n*   **Task Complexity:** The LLM alignment task is based on text generation from a single, generic prompt (\"The\") with an \"oracle reward model\". This oversimplification is a significant departure from real-world alignment, which involves complex instruction-following and reasoning. To make a convincing case, the authors should consider evaluating their proposed pipeline on established and more complex alignment benchmarks, such as **MT-Bench, AlpacaEval, or Arena-Hard**.\n\n**Reliance on Idealized Assumptions**\n\nReal-world alignment is fundamentally shaped by noisy and inconsistent human feedback. This raises a critical, unaddressed question:\n\n*   **Robustness to Noisy Preferences:** Does the performance advantage of the `Align → KD` pipeline hold when the reward signal is weak, stochastic, or noisy, as is typical with human data? An investigation into this would be crucial, as the high cost of aligning a large model may not be justified if the benefits diminish significantly under realistic conditions.\n\n**Omission of Computational Cost Analysis**\n\nA crucial aspect that the paper does not sufficiently address is the computational cost, which is the primary driver behind the `KD → Align` paradigm in the first place. While the paper convincingly argues for the performance benefits of `Align → KD`, its practical viability is left unevaluated.\n\n*   The paper would be significantly strengthened by including a cost-benefit analysis. A discussion of the trade-offs in terms of computation resource metrics is essential for practitioners to gauge whether the performance gains of `Align → KD` justify its substantially higher upfront computational cost.\n\n**Insufficient Exploration of Prerequisites and Alternatives**\n\nFinally, the analysis would be more robust if it considered the boundary conditions of its claims and explored potential alternatives more deeply.\n\n*   **The Role of SFT Quality:** The proposed pipeline's success is critically dependent on starting with a high-recall SFT model. The paper would benefit from a discussion of how the quality and diversity of the SFT data impact the initial recall and, consequently, the effectiveness of the entire `Align → KD` pipeline.\n*   **Mitigation Strategies for the \"Low-Recall Trap\":** The paper presents the `Align → KD` pipeline as the solution to the \"low-recall trap\" but does not explore whether the trap can be mitigated within the standard `KD → Align` framework. For instance, could a more sophisticated data-sourcing strategy for distillation that improves the diversity and coverage of the training data alleviate the problem? Comparing against such an enhanced baseline would provide a more complete picture.\n\nI am willing to raise my score if the authors can satisfactorily address the weaknesses I have raised."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jKgtigox1n", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Reviewer_v1RN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Reviewer_v1RN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751947438, "cdate": 1761751947438, "tmdate": 1762920404500, "mdate": 1762920404500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response for all reviewers"}, "comment": {"value": "## [Common response for all reviewers]\n\nWe sincerely thank all reviewers for their constructive feedback. We are encouraged by the consensus on our work's core strengths: namely, that we address an \"important\" (Reviewer aqcD) and \"timely\" (Reviewer XYEC) question in LLM post-training, supported by \"clear empirical evidence\" (Reviewer aqcD and XYEC) from a \"robust\" (Reviewer XYEC) experimental setup that \"isolates the mechanism cleanly\" (Reviewer aqcD). We wish to clarify that our primary contribution is not simply the intuition that \"high-recall models align better,\" but rather, as Reviewer aqcD noted, **the \"formalization\" of why the common, efficiency-driven KD $\\rightarrow$ Align pipeline is structurally flawed**. We identify this failure as the \"low-recall trap\" , a specific mathematical barrier in PPO and DPO objectives  that prevents the recovery of rare modes. **Our work validates Align $\\rightarrow$ KD as the robust solution to this trap.** We are addressing all specific feedback, especially on presentation (Reviewer aqcD) and narrative clarity (Reviewer XYEC), and will upload a revised manuscript soon."}}, "id": "e9nqDU9h6L", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412908896, "cdate": 1763412908896, "tmdate": 1763412908896, "mdate": 1763412908896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the common practice of performing preference alignment after knowledge distillation (KD) in model training. The authors argue that distilling before alignment reduces the reference model's distributional recall, limiting its ability to learn and align rare but desirable behaviors. They propose reversing the standard pipeline, i.e., aligning first, then distilling (Align -> KD),  to preserve these behaviors. The paper contributes:\n1. A theoretical explanation of how the reference model's recall constrains alignment objectives.\n2. Empirical validation using a controlled Mixture-of-Gaussians setup and on the SmolLM2 language model family.\nOverall, the paper highlights *alignment must precede distillation* to maintain alignment quality and model robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper addresses a compelling and timely question: *should alignment precede or follow distillation for small models?* The authors provide clear empirical evidence showing that performing alignment before distillation leads to better outcomes. A key strength of the work lies in its thorough empirical validation, which includes both simulated experiments and on real language models, demonstrating the robustness and generalizability of the findings."}, "weaknesses": {"value": "* I find the overall narrative of the paper somewhat inconsistent. The work appears to conflate two related but distinct questions:\n     1. What is a good reference model for alignment?\n     2. Which process should be performed first: alignment or distillation?\n\n    Most of the theoretical discussion and motivation center on the first question, emphasizing that the base model should maintain high recall. However, I feel like this point is already fairly well recognized in the community. Although it's probably due to a different reason: as RL often leads to entropy or mode collapse, a base model with high output diversity is essential.\n\n    In contrast, the second question is more about: which process hurts diversity more, alignment or distillation? Both alignment (RL) and distillation can significantly reduce the diversity of language model outputs. Therefore, when developing compact student models, it would be valuable to investigate which step should precede the other to best preserve quality and diversity. I believe the paper would be stronger and more engaging if it focused more directly on this question.\n* For the MoG experiments, it would be more interesting if the target distribution consisted of two modes of $p^*$, as mode collapse is a persistent and unresolved challenge in alignment research.\n* For the LLM experiments, I'm not sure why it's necessary to include language models of three different sizes. In practice, there's typically a single large, high-capability model, and the smaller ones are usually distilled from it.\n* Minor points: The authors should revise the content that was directly copied and pasted from LLM outputs, especially the hyphens. For example, \"Kullback-Leibler\" always has a longer-than-usual hyphen. This is also the case for \"precision-recall\" on line 258 on page 5. Besides, an em dash as a long punctuation mark should be \"---\" in latex."}, "questions": {"value": "1. I'm not sure whether the actual pipeline for obtaining smaller models follows the sequence \"pretrain–SFT–KD–alignment.\" One key confusion is the blurred boundary between SFT and KD: both stages share the same training objective and use off-policy data. In practice, people don't seem to make a clear distinction between the two. If that's the case, performing RL or DPO before this stage seems somewhat unusual to me. Although the base model may exhibit high diversity, the probabilities of producing \"good\" responses could still be extremely small. In this case, issues like sampling and learning traps persist. Am I misunderstanding something?\n2. The second question relates to the first one. After reading the entire paper, I'm still unclear about how to obtain high-quality small models. I guess this mainly stems from the confusion about difference between SFT and KD. It would be helpful if the authors could describe the pipeline more explicitly somewhere in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FeSu5sUR41", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Reviewer_XYEC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Reviewer_XYEC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794711801, "cdate": 1761794711801, "tmdate": 1762920403969, "mdate": 1762920403969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper questions a common practice in LLM post-training. Given a pretrained model, practitioners often distill a large model into a smaller one and then perform preference alignment. This workflow is motivated by efficiency, since smaller models are cheaper and easier to fine-tune. However, the authors argue that this practice degrades performance in terms of distributional recall.\nThey claim that distillation tends to preserve only high-probability modes while pruning low-probability ones. As a result, the alignment that follows cannot recover desirable but rare behaviors if they fall outside the support of the distilled model. To address this issue, the authors propose reversing the order of operations: first align a (high-recall) pretrained model and *then* distill it, while acknowledging that this approach increases computational cost.\nThe authors support their argument with controlled experiments using a Mixture-of-Gaussians setup and additional experiments on large language models from the SmolLM2 family."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The paper raises an important question in LLM post-training. The main message is intuitive and clearly communicated. Although the proposed solution is conceptually straightforward, the authors convincingly explain why it matters and why the opposite pipeline remains dominant in practice. The work is also valuable in how it formalizes the informal intuition that alignment depends on the reference model’s coverage. The analysis of how \"low-recall\" references create structural traps during preference optimization is conceptually neat and supported by clear empirical evidence. \n- The experimental setup is simple yet effective. The Mixture-of-Gaussians example isolates the mechanism cleanly, and the follow-up experiments on the SmolLM2 family demonstrate that the phenomenon persists at scale."}, "weaknesses": {"value": "Having agreed upon the main message, my major concerns lie in the quality of exposition. Overall, the writing is often too rhetorical rather than mathematically precise. Key terms and concepts are used before being properly introduced or defined, which makes the argument difficult to follow. I also have several suggestions regarding the writing style listed below. I strongly believe that the paper requires a major revision to improve clarity and readability. With such a revision, the paper could become a strong and useful reference for the community. Detailed comments are provided below.\n\n- The writing style of the paper is not linear. The exposition often moves back and forth between motivation, intuition, and definition instead of following a clear logical order. As a result, it is difficult for the reader to understand the conceptual framework before being asked to interpret technical claims. The manuscript tends to introduce important terms long before defining them and then revisits those terms multiple times in later sections, each time with slightly different or incomplete explanations. Here are some examples.\n\n    - For instance, the term **\"distributional recall\"** appears in the abstract and introduction as if it were already a well-defined concept. However, no explicit mathematical definition or formal description is given at that point. The reader is left to guess what \"recall\" means in this probabilistic context until Section 3, where the authors begin to use it operationally. Even there, the explanation is scattered across paragraphs and depends heavily on the reader's prior familiarity with KL regularization. \n\n    - Similarly, the notion of **\"desirable behaviors\"** is introduced early in the paper in the phrase *\"we posit a simple recall requirement: desirable behaviors must lie within the support of π_ref\"*. This statement is highly ambiguous because \"desirable behaviors\" could refer to several different things. \n\n    - The authors also use the term **\"low-recall trap\"** repeatedly but define it gradually in several fragments rather than once in a clear and complete form. The paper first mentions it in the introduction as a structural problem, later splits it into a \"sampling trap\" and \"learning trap,\" and only then provides some mathematical reasoning behind it. The lack of a single, unified definition forces the reader to reconstruct the meaning by piecing together several paragraphs. This pattern makes the presentation feel recursive rather than progressive. \n\nThe cumulative effect is that the paper's theoretical argument feels less rigorous than it actually is. The ideas themselves are coherent, but the order of presentation obscures them. To improve readability, the authors should define all key terms explicitly and early. A linear presentation would begin by listing the essential concepts (for example, what \"recall\" means for a model distribution, what counts as a \"desirable behavior,\" and what exactly constitutes the \"low-recall trap\"), followed by clear empirical or mathematical demonstrations. Without such structure, the argument appears handwavy and repetitive even when the underlying logic is sound."}, "questions": {"value": "- It seems that the critical (yet implicit) assumption is that the \"distillation\" procedure yields a low-recall model. I suspect that it is based on (Cha and Cho, 2025), but it seems to be only implicitly assumed. I suggest the authors to add an explicit discussion on the assumption, and some justification if possible.\n- Why Section 4.1 and Section 4.2 are not under a single subsection? \n\n#### **Suggestions**\n- Besides the writing, the manuscript uses italics and bold faces too frequently for emphasis. I recommend limiting typographic emphasis to key definitions or novel terms only, as excessive use can distract the reader and reduce the perceived objectivity of the presentation.\n- Inconsistent formatting in section headings. Remove periods in the headings of Sections 3.1 and 3.2, for example.\n- In Section 4.1, please explicitly mention that the dimension is two. Overall, the description seems incomplete.\n- I found the oracle reward function $R(x)$ in Appendix A, but this should be explicitly mentioned.\n- Consider indexing $p''$, instead of using the single notation $p''$ for two distributions (4 modes / 3 modes).\n- \"Decisive\" in line 113 does not sound idiomatic. I suggest to revise the manuscript carefully.\n- In line 235, remove \"in\" before \"real LLM experiments\".\n- Please go over the references and fix capitalization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6rIPuOxFfo", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Reviewer_aqcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Reviewer_aqcD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875393325, "cdate": 1761875393325, "tmdate": 1762920403400, "mdate": 1762920403400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common response for all reviewers (after revision)"}, "comment": {"value": "## Common response for all reviewers (after revision)\n\nWe have completed the revision process following our initial author responses and have uploaded the revised manuscript along with a summary of the changes. Our primary objective for this revision was to prioritize and effectively address the major weaknesses identified by each reviewer, while keeping the changes focused and concise.\n\nFor detailed explanations of specific revisions, please refer to our individual responses to each reviewer. To facilitate your review, all updates in the revised manuscript have been **highlighted in** **purple**.\n\nWe would like to express our sincere gratitude once again for your insightful and constructive comments. We are fully committed to incorporating all remaining feedback in the final camera-ready version to further enhance the quality and clarity of our work. Thank you."}}, "id": "JGMXPBX2wi", "forum": "DmoNViA41R", "replyto": "DmoNViA41R", "signatures": ["ICLR.cc/2026/Conference/Submission8551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8551/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission8551/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763708060420, "cdate": 1763708060420, "tmdate": 1763708060420, "mdate": 1763708060420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}