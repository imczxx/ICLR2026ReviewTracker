{"id": "UOzxviKVFO", "number": 11634, "cdate": 1758202696628, "mdate": 1763559994543, "content": {"title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization", "abstract": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose EMPO$^2$, a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.", "tldr": "", "keywords": ["Reinforcement Learning", "LLM Agent", "Exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3f914c63072858c90376dcdf90ee00023322f05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a reinforcement learning framework, EMPO², designed to enhance exploration and generalization in large language model (LLM) agents. Existing RL-based LLM agents often overfit to pretrained knowledge and fail to discover novel states. EMPO² addresses this by combining on-policy and off-policy updates with a self-generated memory mechanism—where the agent reflects on past failures to produce “tips” that guide future rollouts.\n\nIt internalizes useful behaviors into model parameters while maintaining adaptability via external memory through hybrid optimization. Evaluations on ScienceWorld and WebShop show substantial gains over GRPO with strong out-of-distribution adaptability requiring no parameter updates. The framework is proposed to be a step toward building self-improving, memory-aware LLM agents capable of efficient exploration and continual learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novelty in integration of memory and RL optimization: The paper introduces a hybrid framework (EMPO²) that unifies parametric (on-policy) and non-parametric (off-policy) learning, bridging memory-augmented reasoning and reinforcement learning. This combination is conceptually novel and addresses a long-standing gap between reflection-based and RL-based LLM agents.\n\n- Effective exploration mechanism: By incorporating self-generated reflective memory (“tips”), the method enables autonomous correction of past errors and promotes deeper exploration without additional supervision. It is a meaningful improvement over prior static-memory or fixed-parameter approaches.\n\n- The method demonstrates substantial gains on two challenging multi-step reasoning benchmarks, ScienceWorld and WebShop, with comparably significant improvement over GRPO. It also shows good out-of-distribution generalization with zero-shot adaptability.\n\n- The study includes comprehensive ablation analyses, comparisons across offline, online, and non-parametric baselines, and even computational cost breakdowns, supporting the robustness of the findings.\n\n- The paper is clearly written, with structured algorithmic explanations, detailed pseudocode, and implementation appendices.\n\n- Broader significance: EMPO² provides a promising direction toward self-improving, memory-aware, and generalizable LLM agents, with potential applications in embodied AI, web interaction, and general decision-making systems."}, "weaknesses": {"value": "- The novelty is limited: The core contribution of EMPO² lies in combining existing components, such as memory reflection, on/off-policy RL, and intrinsic rewards, rather than introducing a fundamentally new algorithmic principle or theoretical insight. The innovation is primarily architectural rather than conceptual, which makes its novelty kind of limited. \n\n- The paper does not provide a formal or empirical analysis explaining why the hybrid on/off-policy mechanism stabilizes exploration or improves generalization. Key hyperparameters such as the rollout and update probabilities (p,q) are heuristic, with no clear sensitivity or convergence analysis.\n\n- Evaluation is confined to two benchmarks (ScienceWorld and WebShop), both text-based and reasoning-oriented. The framework’s effectiveness in broader or more complex environments—such as robotics, code synthesis, or multimodal RL—remains untested.\n\n- The study does not analyze the semantic quality of generated “tips” or demonstrate how they concretely guide exploration. Without such analysis, it is unclear whether the model truly learns generalized reasoning strategies or simply memorizes patterns.\n\n- Naming of the Method: the name of the method is kind of confusing at the first time. The \"square\" symbol is like the footnote, which is ambiguous. Since it is an acronym, it is better to show the full name at the first time of the appearance (e.g. in the abstract, you should show the full name at the first time it appears)."}, "questions": {"value": "- Q1: Justification of the hybrid update design: Could the authors elaborate on why combining on-policy and off-policy updates yields more stable or effective exploration in LLM agents? A theoretical or empirical rationale (e.g., ablation across different ratios of on/off-policy updates) would strengthen the methodological foundation.\n- Q2: Sensitivity to hyperparameters p and q: The rollout and update probabilities seem chosen heuristically (p=0.75, q=1/3). How sensitive is EMPO² to these settings? Have the authors explored how different sampling ratios affect training stability, exploration depth, or convergence?\n- Q3: Role and quality of generated “tips”: The memory mechanism is central to EMPO². Could the authors provide qualitative examples or a deeper analysis of what kinds of tips are most beneficial? Do these tips generalize semantically across tasks, or do they mainly encode task-specific heuristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AcOBCynPvS", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Reviewer_r39R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Reviewer_r39R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760585504760, "cdate": 1760585504760, "tmdate": 1762922706334, "mdate": 1762922706334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Online reinforcement learning (RL) has recently emerged as a powerful method to improve reasoning and agentic capabilities of large language models (LLMs). However, these methods generally employ on-policy rollouts, and past failed attempts deliver no information other than single scaler reward. For hard tasks where models can be consistently wrong, on-policy samples do not recover any new information, and models may never learn how to solve these tasks via RL.\n\nA possible alternative to this approach is to incorporate memory into LLM agents — LLMs can read their past rollouts, figure out where they had gone wrong/what they could have done differently, and use them to collect future experience. This paper proposes EMPO$^2$, an end-to-end framework where LLMs generate memory in the form of hints from prior rollouts, incorporate them in future rollouts to collect better experience, and uses a mixture of on and off-policy optimization to both improve the model with these hints, and distill the behavior with hints into the model via off-policy learning to retain good behavior on these tasks even when hints are not present. Experiments on multi-turn agentic environments like WebShop and ScienceWorld show superior performance of the proposed method compared to pure on-policy online RL methods like GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**Overall, the paper is quite strong and I recommend acceptance of the paper.**\n\n## Novelty\n\nThe paper proposes a mechanism for self-generating memory, incorporating memory into the rollout mechanism in order to avoid past mistakes, promote exploration and achieve better rollouts. Moreover, to the best of my knowledge, this paper is the first to use off-policy learning to then distill back these **hint-augmented** prompts back into the model’s parametric knowledge. This is remarkable and also what I was looking for in the online RL system, Kudos to the authors for making it work so nicely.\n\nTo the best of my knowledge, this was only possible in an offline manner using SFT via context distillation [1] (this is an important work that should be cited and discussed however), but no one has done it using online RL before. \n\nThe hints are self-generated and do not use a stronger model, which makes the work more appealing.\n\n## Strength of results\n\nThe results provided on two benchmarks are very strong, showing remarkable improvement over regular on-policy online RL. This may also unlock improvement in cases where on-policy RL has failed, and have potential implications beyond what the paper presents, i.e., on reasoning tasks like math/coding.\n\n## Memory\n\nThe fact that models can generate their own memory and use it in future effectively, **despite not being an entirely novel idea**, is very nice to see in practice."}, "weaknesses": {"value": "As mentioned above, I really like this paper. However, I would note the following weaknesses:\n\n## Comparison on single turn reasoning tasks\n\nThe idea of off-policy updates using previously generated hints can be useful beyond the tasks used in this paper. Particularly, this can help regarding single turn reasoning tasks like math/coding. \n\nThis is the single most important point where the paper's results can be improved. **If the authors can demonstrate the usefulness of their framework on these tasks, and respond to the other weaknesses/questions I mention below, I am very likely to increase my score on this paper further.**\n\n## Adding a comparison of performance vs GPU hours\n\nThe proposed method is inherently more computationally expensive compared to regular on-policy GRPO. **While Appendix E provides a rudimentary analysis of the breakdown of compute spent on various components of the proposed method, no comparison with GRPO is given.** The authors should include a plot with\n\n1. X-axis: GPU hours/flops/some other measure of compute\n\n2. Y-axis: performance\n\nIn the main paper, to make the comparison fairer with GRPO.\n\n## No ablation for intrinsic exploration reward\n\nThe paper uses an intrinsic exploration reward for novel states, in order to encourage the model to explore unseen/sufficiently novel states. However, I could not find an ablation of the proposed method showing how the performance differs in case this reward is not added/for different choices of the intrinsic exploration reward. To simply put it, it is unclear what the effect of this component of the proposed method is on the performance of the method.\n\n## Experiments using only one base model\n\nAll the experiments in the paper are done using only one base model. While the results are strong, it is unclear if the gains come from model specific pretraining/finetuning for Qwen2.5-7B-Instruct. Including results on models from different companies/different pre-training would make the paper significantly stronger.\n\n## No learning component for reward generation\n\n**This is more of an after-thought for future work instead of a serious weakness.** The proposed algorithm does not incentivize better memory generation. However, some proposed memory/hints can be better at steering future generations compared to others, and the model is never incentivized **directly** to generate better memory/hints (it may get incentivized indirectly via reward on memory-augmented rollouts). This needs to be addressed to make the learning better.\n\n(**Minor**)\n\nAn important prior work for section 3 discussing LLM agents to be information seeking is Paprika [2]. Similarly, context distillation [1] for learning to behave the same way without hints as whenever hints are available is an important prior work that should be cited and discussed."}, "questions": {"value": "(**Question 1: Advantage Estimation**)\n\nBased on the definition of advantage in line 106-107, am I correct to understand that there is no per-turn advantage? It seems like the advantages are calculated using the entire sum of rewards in each trajectory.\n\n(**Question 2: Calculating Importance Sampling Ratio**)\n\nIt is not clear to me how the importance sampling ratios are calculated for off-policy updates. Based on the text and Algorithm 1 from Appendix A, it seems the old log probs ($log \\pi_{\\theta_{old}}$) are calculated using an off-policy manner (i.e., without generated hints). But how is the current log probs ($log \\pi_\\theta$) calculated for the off-policy updates? With or without the hints? More generally, could you elaborate how $log \\pi_\\theta$ is calculated for all different cases (regular on-policy updates, on-policy updates with hints, off-policy updates without hints)? Adding a table clarifying these cases to the main paper would help a lot regarding the clarity of the paper.\n\n(**Question 3: Figure 8**)\n\nI am a bit confused about this figure.\n\nWhy are the EMPO and GRPO plots split across two different panels and not on the plot? It is much harder to compare, at least at the first glance, what the performance difference is.\nWhy does the starting point between EMPO and GRPO vary? Is it because they already went through one round of training with their respective algorithm on the previous task?\nWhat happens if you take the checkpoint resulting from GRPO and run EMPO on top of it, and vice versa (run GRPO on the checkpoint from EMPO) on the new task?\n\n(**Question 4: ScienceWorld**)\n\nWhat does return/reward mean for ScienceWorld? Is there some other metric like task success rate/completion rate beyond just reward coming from different subgoals/components within a task? Could the authors report that?\n\n(**Question 5: Performance Difference between ScienceWorld and WebShop**)\n\nThe proposed method seems to outperform online GRPO quite heavily on ScienceWorld, but not so much on WebShop. Is there a reason/explanation for this?\n\n(**Question 6: Example of how the hints help**)\n\nCould the authors put rollouts with/without hints side-by-side in the appendix, to showcase an example of how the hints help generate better rollouts/avoid common or repeated mistakes?\n\n# References\n\n[1] Learning by Distilling Context, https://arxiv.org/abs/2209.15189\n\n[2] Training a Generally Curious Agent, https://arxiv.org/abs/2502.17543"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IaWIS6n7iI", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Reviewer_c3bg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Reviewer_c3bg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160683609, "cdate": 1761160683609, "tmdate": 1762922705880, "mdate": 1762922705880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Online reinforcement learning (RL) has recently emerged as a powerful method to improve reasoning and agentic capabilities of large language models (LLMs). However, these methods generally employ on-policy rollouts, and past failed attempts deliver no information other than single scaler reward. For hard tasks where models can be consistently wrong, on-policy samples do not recover any new information, and models may never learn how to solve these tasks via RL.\n\nA possible alternative to this approach is to incorporate memory into LLM agents — LLMs can read their past rollouts, figure out where they had gone wrong/what they could have done differently, and use them to collect future experience. This paper proposes EMPO$^2$, an end-to-end framework where LLMs generate memory in the form of hints from prior rollouts, incorporate them in future rollouts to collect better experience, and uses a mixture of on and off-policy optimization to both improve the model with these hints, and distill the behavior with hints into the model via off-policy learning to retain good behavior on these tasks even when hints are not present. Experiments on multi-turn agentic environments like WebShop and ScienceWorld show superior performance of the proposed method compared to pure on-policy online RL methods like GRPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**Overall, the paper is quite strong and I recommend acceptance of the paper.**\n\n## Novelty\n\nThe paper proposes a mechanism for self-generating memory, incorporating memory into the rollout mechanism in order to avoid past mistakes, promote exploration and achieve better rollouts. Moreover, to the best of my knowledge, this paper is the first to use off-policy learning to then distill back these **hint-augmented** prompts back into the model’s parametric knowledge. This is remarkable and also what I was looking for in the online RL system, Kudos to the authors for making it work so nicely.\n\nTo the best of my knowledge, this was only possible in an offline manner using SFT via context distillation [1] (this is an important work that should be cited and discussed however), but no one has done it using online RL before. \n\nThe hints are self-generated and do not use a stronger model, which makes the work more appealing.\n\n## Strength of results\n\nThe results provided on two benchmarks are very strong, showing remarkable improvement over regular on-policy online RL. This may also unlock improvement in cases where on-policy RL has failed, and have potential implications beyond what the paper presents, i.e., on reasoning tasks like math/coding.\n\n## Memory\n\nThe fact that models can generate their own memory and use it in future effectively, **despite not being an entirely novel idea**, is very nice to see in practice."}, "weaknesses": {"value": "As mentioned above, I really like this paper. However, I would note the following weaknesses:\n\n## Comparison on single turn reasoning tasks\n\nThe idea of off-policy updates using previously generated hints can be useful beyond the tasks used in this paper. Particularly, this can help regarding single turn reasoning tasks like math/coding. \n\nThis is the single most important point where the paper's results can be improved. **If the authors can demonstrate the usefulness of their framework on these tasks, and respond to the other weaknesses/questions I mention below, I am very likely to increase my score on this paper further.**\n\n## Adding a comparison of performance vs GPU hours\n\nThe proposed method is inherently more computationally expensive compared to regular on-policy GRPO. **While Appendix E provides a rudimentary analysis of the breakdown of compute spent on various components of the proposed method, no comparison with GRPO is given.** The authors should include a plot with\n\n1. X-axis: GPU hours/flops/some other measure of compute\n\n2. Y-axis: performance\n\nIn the main paper, to make the comparison fairer with GRPO.\n\n## No ablation for intrinsic exploration reward\n\nThe paper uses an intrinsic exploration reward for novel states, in order to encourage the model to explore unseen/sufficiently novel states. However, I could not find an ablation of the proposed method showing how the performance differs in case this reward is not added/for different choices of the intrinsic exploration reward. To simply put it, it is unclear what the effect of this component of the proposed method is on the performance of the method.\n\n## Experiments using only one base model\n\nAll the experiments in the paper are done using only one base model. While the results are strong, it is unclear if the gains come from model specific pretraining/finetuning for Qwen2.5-7B-Instruct. Including results on models from different companies/different pre-training would make the paper significantly stronger.\n\n## No learning component for reward generation\n\n**This is more of an after-thought for future work instead of a serious weakness.** The proposed algorithm does not incentivize better memory generation. However, some proposed memory/hints can be better at steering future generations compared to others, and the model is never incentivized **directly** to generate better memory/hints (it may get incentivized indirectly via reward on memory-augmented rollouts). This needs to be addressed to make the learning better.\n\n(**Minor**)\n\nAn important prior work for section 3 discussing LLM agents to be information seeking is Paprika [2]. Similarly, context distillation [1] for learning to behave the same way without hints as whenever hints are available is an important prior work that should be cited and discussed."}, "questions": {"value": "(**Question 1: Advantage Estimation**)\n\nBased on the definition of advantage in line 106-107, am I correct to understand that there is no per-turn advantage? It seems like the advantages are calculated using the entire sum of rewards in each trajectory.\n\n(**Question 2: Calculating Importance Sampling Ratio**)\n\nIt is not clear to me how the importance sampling ratios are calculated for off-policy updates. Based on the text and Algorithm 1 from Appendix A, it seems the old log probs ($log \\pi_{\\theta_{old}}$) are calculated using an off-policy manner (i.e., without generated hints). But how is the current log probs ($log \\pi_\\theta$) calculated for the off-policy updates? With or without the hints? More generally, could you elaborate how $log \\pi_\\theta$ is calculated for all different cases (regular on-policy updates, on-policy updates with hints, off-policy updates without hints)? Adding a table clarifying these cases to the main paper would help a lot regarding the clarity of the paper.\n\n(**Question 3: Figure 8**)\n\nI am a bit confused about this figure.\n\nWhy are the EMPO and GRPO plots split across two different panels and not on the plot? It is much harder to compare, at least at the first glance, what the performance difference is.\nWhy does the starting point between EMPO and GRPO vary? Is it because they already went through one round of training with their respective algorithm on the previous task?\nWhat happens if you take the checkpoint resulting from GRPO and run EMPO on top of it, and vice versa (run GRPO on the checkpoint from EMPO) on the new task?\n\n(**Question 4: ScienceWorld**)\n\nWhat does return/reward mean for ScienceWorld? Is there some other metric like task success rate/completion rate beyond just reward coming from different subgoals/components within a task? Could the authors report that?\n\n(**Question 5: Performance Difference between ScienceWorld and WebShop**)\n\nThe proposed method seems to outperform online GRPO quite heavily on ScienceWorld, but not so much on WebShop. Is there a reason/explanation for this?\n\n(**Question 6: Example of how the hints help**)\n\nCould the authors put rollouts with/without hints side-by-side in the appendix, to showcase an example of how the hints help generate better rollouts/avoid common or repeated mistakes?\n\n# References\n\n[1] Learning by Distilling Context, https://arxiv.org/abs/2209.15189\n\n[2] Training a Generally Curious Agent, https://arxiv.org/abs/2502.17543"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IaWIS6n7iI", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Reviewer_c3bg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Reviewer_c3bg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761160683609, "cdate": 1761160683609, "tmdate": 1763561506422, "mdate": 1763561506422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a memory-augmented RL algorithm for learning effective LLM based policies. The key idea is to use a memory buffer in sampling (rollouts) and policy learning by using the memory to enable more effective exploration in the rollout phase (by being able to reason about past experiences) and using the memory to enable learning a more generalizable policy by performing a combination of on-policy and off-policy (where main policy is not conditioned on memory) learning. Results demonstrate significant improvement in ScienceWorld and moderate improvement in WebShop benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Paper is well-motivated and well-written. Justification for improved exploration in RL for LLMs is sound.\n- Use of memory in both rollout and update phase is simple yet novel in the context of RL for LLMs. \n- Strong results on ScienceWorld which demonstrate the OOD generalization of their method (due to generality of memory)."}, "weaknesses": {"value": "- Lack of ablations. The method introduces additional hyperparameters and components, the effects of which are largely undocumented.\n    - Effect of intrinsic reward component. What is the effect of this component on the performance of the final policy (paper only documents the effect on policy entropy)? How generalizable is this reward term? It seems as if it may require further reward-shaping (i.e. tuning similarity threshold) to generalize to newer domains where naive state similarity may lead to poor performing rollouts (e.g. see static noise TV example from Pathak et al).\n    - Effect of sampling proportion ($p$) between memory-free and memory-augmented rollouts. What is the effect of varying $p$?\n    - Effect of update proportion ($q$) between on- and off-policy updates. What is the effect of varying $q$?\n    - The authors set the KL coefficient to 0.0; does the final model lose its broad generality (e.g. on standard LLM benchmarks)?\n\n[1] Pathak, Deepak, et al. \"Curiosity-driven exploration by self-supervised prediction.\" International conference on machine learning. PMLR, 2017."}, "questions": {"value": "Please see questions listed in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E8hpPKtNHp", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Reviewer_1fWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Reviewer_1fWA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710033783, "cdate": 1761710033783, "tmdate": 1762922705254, "mdate": 1762922705254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the EMPO2 algorithm, which works as follows\n- agents roll out trajectories\n- at the end they write \"tips\" for solving the environment\n- in future runs, some fraction of the time the agent conditions on hints form past rollouts\n- we train on both types of rollouts, including a 3rd variant where the tips are stripped out of the rollout.\n- they show improvements above prior works on ScienceWorld and WebShop."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Although each individual part of the proposed method is not original, combining them all together under one framework is an important contribution and has not been done before, particularly in the important but still early field of RL on LLMs.\n\nThe quality is good, mainly focusing on showing saturation of two in-distribution benchmarks. The paper also demonstrated signs of life on out of distribution benchmarks. The paper also sought to understand each component’s importance by doing ablations, as well as proposed future interesting extensions to the work, such as a similarity-based bonus for novel contributions to the memory bank.\n\nI appreciated the clarity of the communication, the plots, figures, and charts all are well-designed and get the most important points across.\n\nThe significance is important, since the standard of RL for LLMs (GRPO) mainly focuses on parametric updates for each training batch rather than encouraging exploration or learning over time across training or episodes."}, "weaknesses": {"value": "I am confused about the hyperparameter choices for choosing to sample between memory and non memory for rollouts and on-policy and off-policy for updates. There isn’t explanation for these choices (½ and ⅓ respectively), and there are no ablations or sweeps (although 6.3 does ablate the entire components).\n\nSome of the plots could have better reporting. For example, figure 1 B not having error bars across the seeds, or figure 8.\n\nFor some of the baselines, I am concerned about the reported numbers being derived from other papers . For example, for WebShop, the Naive, Reflexion, GRPO, and GiGPO are taken from the paper. The paper states that all of the hyperparameters are the same for the training methods, which absolves most of my concerns, but RL methods are notorious for subtle implementation differences in the algorithm or environment which may not even be highlighted in the paper making big difference.\n\nI also think scaling this to more “non-toy” training and evaluations would improve the paper’s significance (although not necessary since this is proposing a new method)."}, "questions": {"value": "- The off-policy updates seem like they could introduce stability (as you mentioned). Did you consider either using a real off-policy algorithm or introducing an importance-sampling correction where the numerator is the prob with no tips and the denom is prob with tips?\n\n- The off-policy stabilization approach in Fig 6 is interesting. Would be nice to see error bars or multiple seeds there (to make sure it's not that one seed got unlucky in Fig 6).\n\n- A bit ambiguous to me what numbers are reported in Table 1 (and other results) for EMPO2 -- which rollout mode were used to create these? If the \"with tips\" one, how many rollouts were used to create the memory bank? Same question about baselines that use multiple episodes.\n\n- How were the hyperparameters chosen for sampling from on or off policy?\n\n- Why are there no error bars in table 2 for naive or reflexion?\n\n- Why are there no error bars in figure 8?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EuJTnLWuIp", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Reviewer_JKue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Reviewer_JKue"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165553033, "cdate": 1762165553033, "tmdate": 1762922704784, "mdate": 1762922704784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Paper Revision"}, "comment": {"value": "We would like to express our sincere gratitude to all reviewers for their valuable time and effort in helping improve our work. Below, we summarize the revisions made to the paper.\n\n&nbsp;\n\n### Summary of Paper Revision\n\nModified sections\n\n- [Abstract] In response to reviewer r39R’s suggestion, we added the full name of our method to the abstract.\n- [Section 5] In response to reviewer c3bg’s suggestion, we added relevant related works.\n- [Section 6] In response to reviewer JKue’s question, we clarified that the results of Tables 1 and 2 for EMPO² were evaluated without memory at test time.\n- [Section 7]  In response to reviewer JKue’s question, we present the exploration of fully off-policy algorithms as a future direction.\n\nNew sections\n\n- [Appendix C] Detailed Explanation of Importance Sampling Ratios in Policy Updates\n- [Appendix E.2]  Effects of Tips on Exploration Behavior\n- [Appendix F] Ablation study on Mode Selection Probability (p and q) and Intrinsic Reward\n- [Appendix G.2] Cost Analysis of Total Training Time"}}, "id": "NRtgymQCph", "forum": "UOzxviKVFO", "replyto": "UOzxviKVFO", "signatures": ["ICLR.cc/2026/Conference/Submission11634/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11634/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission11634/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763560072769, "cdate": 1763560072769, "tmdate": 1763560072769, "mdate": 1763560072769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}