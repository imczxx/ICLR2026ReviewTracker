{"id": "iPhyuBeLX6", "number": 11409, "cdate": 1758198392888, "mdate": 1759897577345, "content": {"title": "Adaptive Task Vectors for Large Language Models", "abstract": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks without parameter updates by conditioning on a few demonstrations provided in the prompt. Despite its success, ICL suffers from several limitations, including sensitivity to demonstration order, context length constraints, and computational inefficiency. To address these challenges, task vector-based approaches compress task information into a single vector. However, these methods typically construct task vectors from fixed sets of demonstrations and reuse them across input queries, without conditioning on the specific input. This limitation can lead models to struggle with effective adaptation when the input query is not well aligned with the underlying demonstrations, consequently degrading their generalization performance on unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors (ATV), a simple and effective framework that dynamically generates task vectors conditioned on each input query. ATV employs a small language model to generate task vectors, which are then transformed to match the target LLM’s architecture and applied to guide its output generation. In contrast to ICL and previous vector-based approaches, which rely on fixed demonstration sets and their corresponding vectors, ATV dynamically generates task vectors tailored to each specific input query and task. Consequently, ATV demonstrates strong performance and generalization capabilities, even for unseen tasks. Furthermore, we provide a theoretical analysis indicating that ATV is expressively equivalent to LoRA under equal rank budgets and more expressive than Prefix-Tuning, thereby offering formal support for its representational advantage.", "tldr": "We propose Adaptive Task Vectors (ATV), a framework that dynamically generates input-conditioned task vectors for large language models, offering strong performance and greater flexibility via query-specific adaptation.", "keywords": ["In-context Learning", "Task Vectors", "Large Language Models", "Query-dependent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c2369b96cf4ce02062eda84f2e337bf1d847b18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Adaptive Task Vectors (ATV), a framework that dynamically generates query-conditioned task vectors to steer frozen large language models. Unlike existing methods that use fixed task vectors, ATV employs a small language model (GPT-2) to encode each input query into a compact representation, which is then expanded and injected into the target LLM's hidden states. The authors provide theoretical analysis showing ATV's equivalence to LoRA and superior expressiveness to Prefix-Tuning. Empirical evaluation on 20 in-domain and 5 unseen tasks demonstrates strong performance, particularly on adversarial generalization (59.6% on HANS vs. <0.5% for baselines), while maintaining computational efficiency comparable to parameter-efficient tuning methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Compelling results on unseen tasks, particularly the HANS adversarial benchmark (59.6% accuracy while all baseline methods collapse to <0.5%), demonstrating robust adaptation capabilities.\n\n- Achieves token efficiency comparable to zero-shot inference and inference time comparable to LoRA, while substantially outperforming prompt-based methods.\n\n- Well-structured paper with effective visual explanations (particularly Figures 1 and 5) that clearly illustrate the adaptive mechanism and its effects."}, "weaknesses": {"value": "The paper is well-prepared, with clear presentation, comprehensive experiments, and informative illustrations. However, my main concern lies in the motivation. I don't see the necessity of introducing a query-based task formulation, and the authors don't demonstrate why this would be the case. There's Figure 5, but it focuses on the representational comparison with a prior work, rather than motivating the problem itself. That said, I believe if tasks are consistently associated with prompts containing different queries, a generalizable task representation can already be derived. \n\nAdditionally the choice of using GPT-2 as the small, representation model—rather than a bidirectional encoder like a sentence transformer—is unclear. As far as I understand, the query is passed to the small GPT-2 model once. Therefore, there's no obligation for using an encoder problem there. This raises some doubts about the authors' expertise in this area, in my opinion.\n\nPlease see my detailed review below.\n\n---\n\n### **Main Concern – Motivation and Novelty**\n\nI don't think task representations strictly depend on the query but on the underlying task itself. As long as the task is the same, wording or semantics in queries shouldn't change the task representation much.\n\nI acknowledge that the authors illustrate this with Figure 5 in Section 4.7, but this is problematic. If you pass two similar texts to a sentence transformer, you will get similar embeddings. How does the authors' proposed method improve the embedding similarity in an ICL context? My understanding is that while learning task formulations, ATV can retain the embedding similarity of those similar queries. This part is unclear.\n\n---\n\n### **Statistical Rigor**\n\nThe paper uses only 90 samples per task for training. More critically, there is no analysis of how performance scales with training data size or learning curves showing convergence behavior. This raises concerns about data efficiency and whether the method's advantages hold with fewer samples.\n\nAdditionally, while the authors report standard deviations across three random seeds, they do not provide statistical significance tests (e.g., t-tests, confidence intervals) to validate that ATV's improvements over baselines are statistically significant rather than due to random variation. Given the modest sample sizes and the fact that some performance differences are within overlapping error bars (e.g., Table 1, Math category), formal statistical analysis is needed to support the claimed superiority.\n\n---\n\n### **Unclear Design Choices**\n\nWhy is a separate small model necessary for ATV generation? The paper doesn't justify (as long as I notice) why the large model's own representations (e.g., hidden states from early layers) couldn't generate query-specific vectors. This seems like added complexity without clear motivation.\n\nAdditionally, if computational efficiency was intended here, why not use a sentence transformer? Their parameter count typically ranges from 100M to 700M, which would be similar to GPT-2 in terms of complexity.\n\n---\n\n### **Missing Ablations/Sensitivity Analysis**\n\nThe choice of $\\lambda$ is unclear and a systematic study of $\\lambda$ (on a representative set is enough) is missing.\n\n---\n\n### **Missing References**\n\nFollowing the citation of Hendel et al.'s work, several references about fixed task vector embeddings are missing. For example, Todd et al. [1] have shown the importance of attention heads and how they are activated based on task recognition in inputs. On the other hand, Saglam et al. [2] show the contribution of these attention heads can be learned with causal optimization. In addition, Saglam et al. [2] also add the task embedding to hidden layers, so it would be appropriate to acknowledge their work in doing so.\n\nReturning to the paper’s motivation, Saglam et al. [2] demonstrated that effective task formulations for causal inference can be derived from a set of prompts associated with the same task, regardless of the queries. If comparable performance can already be achieved this way, the necessity of a query-based formulation remains unclear. A direct comparison with Saglam et al. [2] would help clarify this point.\n\n---\n\n### **Suggestions**\n\n- **\"Theorem\":** I think calling a straightforward demonstration a theorem is overclaiming. I would change \"theorems\" to, say, \"Propositions\" or \"Demonstrations.\"\n- **Figure 5 in Section 4.7:** I suggest moving Section 4.7 to somewhere in the introduction, so that the main motivation in the paper can be built from the beginning. However, it shouldn't be fully based on the comparison to ELICIT. I think the authors should highlight the main motivation, rather than focusing on what they think is different from ELICIT.\n\n---\n\n### References\n\n[1] Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., & Bau, D. (2024). _Function Vectors in Large Language Models_. In International Conference on Learning Representations (ICLR).\n\n[2] Saglam, B., et al. (2025). _Learning Task Representations from In-Context Learning_. In Findings of the Association for Computational Linguistics: ACL 2025."}, "questions": {"value": "1. Can you provide ablation results showing that the two-stage design (small model + expansion) outperforms simpler alternatives? Specifically, does using the large model's own hidden states as the task vector source work comparably well? It would also be great to see the same small-model experiment but with a bidirectional model (e.g., sentence transformer). Given the relatively small size of encoder models, I think adding these experiments should be easy.\n\n2. Theorem 1 establishes static equivalence with LoRA. Can you provide empirical measurements quantifying the dynamic advantage (e.g., performance of query-conditioned vs. task-averaged vectors)?\n\n3. Why was I2CL's configuration changed from dataset-specific to shared coefficients?\n\n4. Were hyperparameters (especially $\\lambda$) tuned on validation data? If so, what was the tuning procedure and how much variation exists across tasks?\n\n5. How does the small model size affect performance beyond Table 4's marginal differences? Is there a point where larger generators yield diminishing returns?\n\nI also expect the authors to clarify the items pointed out in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EUbCi7fE3E", "forum": "iPhyuBeLX6", "replyto": "iPhyuBeLX6", "signatures": ["ICLR.cc/2026/Conference/Submission11409/Reviewer_LSEb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11409/Reviewer_LSEb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761156512423, "cdate": 1761156512423, "tmdate": 1762922526891, "mdate": 1762922526891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Adaptive Task Vectors (ATV), a framework that steers frozen LLMs using dynamic, query-conditioned task vectors. It addresses a key limitation of prior TV methods that use a single static vector for all queries. ATV's framework uses a small generator model to create a unique vector representation from each input query, and then linearly transforms this vector to match the target LLM's architecture. The created ATV is then injected into the frozen target LLM to guide its output. The paper provides theoretical proofs of ATV's expressive power and empirical results showing it outperforms ICL and static-vector baselines on in-domain tasks, unseen tasks, and especially adversarial generalization benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting idea: Using a query vector to allow TVs to support different tasks.\n- There is some theoretical analysis demonstrating the equivalence between ATV and LoRA.\n- Extensive and promising experiments: results show that ATVs beat both baseline TV methods and LoRA, with slightly slower inference speed compared to I2CL."}, "weaknesses": {"value": "- The paper lacks some discussion on the training cost of the extra modules.\n- According to the paper, the small model is supposed to generate a vector representation of the query. It seems more intuitive to use a language encoder for this purpose. Why does the paper use the decoder-only GPT family models? How does a decoder-only model generate a vector representation? Is it using its last layer hidden state? I would love to see some experiments using encoder-decoder models like the BERT family as well."}, "questions": {"value": "- Does ATV require access to ICL demonstrations? For traditional TV methods, they typically extract TVs from a set of ICL demonstrations. It seems that ATV here does not require this, but only tasks for training.\n- For tasks where output is a phrase or sentence (i.e., y is not a single token), how is the CE loss in Eq. 4 computed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S8vFeV1r4J", "forum": "iPhyuBeLX6", "replyto": "iPhyuBeLX6", "signatures": ["ICLR.cc/2026/Conference/Submission11409/Reviewer_Hwr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11409/Reviewer_Hwr1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562909017, "cdate": 1761562909017, "tmdate": 1762922526273, "mdate": 1762922526273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Task Vectors (ATV), a method for steering frozen LLMs by injecting a query-conditioned vector into selected layers. A small generator network produces this vector from the input query, which is then added to hidden states to modulate predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simple and computationally light steering mechanism for frozen LLMs.\n- Clear motivation to reduce inference-time token overhead compared to ICL.\n- Interesting empirical finding that early-layer injection performs best.\n- Theoretical framing situates ATV among PEFT methods (LoRA, prefix, prompt tuning)."}, "weaknesses": {"value": "- I find the framing of the method somewhat misaligned. Despite its name, the proposed vector is generated per query via supervised training and is not shared across examples or tasks. This makes it a query-specific steering signal rather than a reusable task-level representation. A more appropriate baseline would embed fixed demonstrations once into a shared vector that can be reused across queries, achieving comparable token efficiency while preserving task conditioning. Moreover, the notion of “adaptive” task vectors feels conceptually weak: most token overhead in ICL stems from contextual demonstrations, not the query itself, so producing a query-conditioned vector offers limited efficiency gains in practice.\n- The generator is trained with labeled data but receives only the query text at test time.\n  It is unclear how such a setup supports genuine task-level generalization rather than multitask supervised fitting.  \n- The claim that early-layer injection aligns with prior findings on layer specialization is questionable.  \n  If lower layers primarily encode lexical features, injecting task information there should not yield optimal results.  \n  In my opinion, a more likely explanation is residual bias shaping rather than the injection of semantic \"task meaning.\"  \n  Quantitative analyses could clarify this mechanism.\n- Table 2 sourcing -- it is unclear whether results \"from ELICIT\" were reproduced or copied directly, raising reproducibility concerns.  \n- The reported standard deviations for ATV are large, yet no statistical tests are provided. This brings into question the significance of reported improvements.\n- The paper claims that ATV achieves greater efficiency by requiring fewer input tokens than prompt-based ICL. While this is valid in principle, similar token savings could likely be achieved by amortizing demonstration information, for instance, encoding a fixed set of $k$ demonstrations once into a shared vector reused across queries, while still preserving explicit task conditioning.\n- Theorem 1 analyzes an affine \"static ATV\" that includes a multiplicative low-rank term  $\\Delta W_\\ell(v) h_\\ell,$ whereas the implemented ATV is purely additive. If I understood it correctly, the equivalence result therefore applies to a broader, unimplemented variant rather than to the actual method. Moreover, the proof treats hidden states $h_\\ell$ as free variables instead of $h_\\ell = f_\\ell(x)$, so the result holds in hidden-state space but not necessarily for real input mappings."}, "questions": {"value": "1. Can you conceptually explain how does the generator generalize to unseen tasks without demonstrations or label schemas?\n2. Why should early-layer injection be optimal if these layers are mainly lexical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fwyEE1BbxY", "forum": "iPhyuBeLX6", "replyto": "iPhyuBeLX6", "signatures": ["ICLR.cc/2026/Conference/Submission11409/Reviewer_qfNR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11409/Reviewer_qfNR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961428201, "cdate": 1761961428201, "tmdate": 1762922525354, "mdate": 1762922525354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Task Vectors (ATV), which use a small generator model to produce per-input vectors that are expanded and injected additively into the last-token hidden states across layers of a frozen LLM, aiming to preserve ICL’s flexibility while avoiding prompt-token overhead and fixed-vector rigidity. Theoretically, ATV is argued to match LoRA’s expressivity under equal rank budgets and to strictly subsume Prefix-Tuning under a linear attention approximation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear articulation of a simple, modular pipeline that keeps the target LLM frozen while learning a lightweight generator and linear expansion, with a consistent injection interface across layers.\n- Theoretical framing that precisely scope-limits claims to next-token distributions and gives a clean equivalence-to-LoRA result under matched rank and placements, plus a principled argument for subsuming Prefix-Tuning under a linearized attention view.\n- Broad empirical sweep over a standardized ELICIT setup with in-domain and held-out tasks, plus adversarial HANS evaluation and ablations on generator size and injection depth, offering multiple angles on behavior and efficiency."}, "weaknesses": {"value": "- The LoRA equivalence and superiority-over-Prefix claims rest on constrained scopes and approximations: the LoRA equivalence focuses only on next-token distribution with matched placements and static ATV, and the Prefix result relies on a linear attention approximation that may diverge from real softmax attention in practice.\n- The empirical fairness of baselines is questionable in places. For example, the LoRA setup departs from the original learning rate due to poor performance (adjusted to 4e-4), and ELICIT requires selecting an optimal injection layer per task, which may advantage or disadvantage baselines depending on protocol specifics not fully stress-tested across settings.\n- ATV underperforms BM25 retrieval on math, and while the paper attributes this to pattern alignment and shows targeted math-only training helps, this suggests ATV’s generality depends on careful domain data allocation, weakening one-size-fits-all claims across reasoning vs. procedural domains.\n- The dynamic generator introduces an extra forward pass and a large linear expansion into RLdl parameters per input; while inference latency is reported comparable to LoRA, the memory/runtime implications of full-layer injection and expansion mapping are undercharacterized for larger models and batched, long-sequence workloads.\n- Injection is additively applied to the last-token hidden state per layer, which may limit control over earlier token positions and long-context interactions; the paper lacks analysis of multi-token or position-aware injection variants and their trade-offs.\n- Claims of generalization could be confounded by the close coupling to the ELICIT evaluation harness and prompt templates; robustness to substantially different prompting regimes, decoding strategies, or safety constraints is not deeply probed beyond format adherence and limited consistency metrics.\n- Safety and bias implications of steering hidden states are only cursorily addressed via a few safety datasets; there is no analysis of potential exacerbation or mitigation of harmful behaviors when the generator produces misaligned vectors for out-of-distribution inputs."}, "questions": {"value": "- How sensitive is ATV to the exact placement and breadth of layer injections. Could partial-layer injection with structured sparsity retain most gains while cutting expansion cost and minimizing interference?\n- Can the authors quantify generator–target mismatch effects (e.g., cross-family or multilingual generalization) and whether external encoders or distilled generators alter robustness vs. efficiency trade-offs?\n- How does ATV interact with decoding strategies like temperature, nucleus sampling, or constrained decoding—does the steering effect persist uniformly or require retuning?\n- Could multi-token or segment-level injections improve math/procedural tasks and adversarial robustness, and what are the stability risks when modifying more than the last-token state?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qmuwe4N5FQ", "forum": "iPhyuBeLX6", "replyto": "iPhyuBeLX6", "signatures": ["ICLR.cc/2026/Conference/Submission11409/Reviewer_YGMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11409/Reviewer_YGMn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989465987, "cdate": 1761989465987, "tmdate": 1762922524653, "mdate": 1762922524653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}