{"id": "XLiUcvHfzS", "number": 17720, "cdate": 1758279786739, "mdate": 1759897158322, "content": {"title": "Post-hoc Probabilistic Vision-Language Models", "abstract": "Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable success in classification, retrieval, and generative tasks. For this, VLMs deterministically map images and text descriptions to a joint latent space in which their similarity is assessed using the cosine similarity. However, a deterministic mapping of inputs fails to capture uncertainties over concepts arising from domain shifts when used in downstream tasks. In this work, we propose post-hoc uncertainty estimation in VLMs that does not require additional training. Our method leverages a Bayesian posterior approximation over the last layers in VLMs and analytically quantifies uncertainties over cosine similarities. We demonstrate its effectiveness for uncertainty quantification and support set selection in active learning. Compared to baselines, we obtain improved and well-calibrated predictive uncertainties, interpretable uncertainty estimates, and sample-efficient active learning. Our results show promise for safety-critical applications of large-scale models.", "tldr": "", "keywords": ["Uncertainty Quantification", "Active Fine-Tuning", "Bayesian Deep Learning", "Vision-Language Models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea0b79258b3209a21107517e19d7e8b67ecd8702.pdf", "supplementary_material": "/attachment/543b8102d61b0263c63877082ca66578b820a9bd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes BayesVLM, a post-hoc Bayesian uncertainty estimation method for large-scale vision-language models (VLMs) such as CLIP and SigLIP. The key insight is to apply a Laplace approximation over the last linear projection layers of pre-trained VLMs to obtain Gaussian posteriors over the parameters. This gives a local second-order uncertainty around the learned weights, thereby inducing probabilistic embeddings for both modalities. This also allows the model to analytically compute a distribution over cosine similarities, leading to uncertainty estimates without any retraining, architectural modifications, or additional data. The authors further propose ProbCosine, an analytical approximation for the mean and variance of the cosine similarity under this Bayesian formulation, avoiding costly Monte Carlo sampling. Empirically, the paper demonstrates that BayesVLM yields better-calibrated uncertainty estimates in zero-shot classification and improves sample efficiency in active learning, with minimal computational overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Novelty and conceptual clarty:** The idea of using a post-hoc Laplace approximation on pre-trained VLMs to obtain Bayesian uncertainty estimates is novel and conceptually elegant. The formulation is well-grounded in Bayesian deep learning literature and bridges it with the practical need for scalable uncertainty estimation in large multimodal models.\n- **Practicality and scalability:** The method is traning-free and model-agnostic, requiring only access to the final projection weights and not the original training pipeline. The approach introduces negligible computational overhead (<5% runtime, <0.11% GFLOPs), making it attractive for large-scale or resource-constrained deployments.\n- **Analytical Contribution (ProbCosine)** The derivation of a closed-form Gaussian approximation for cosine similarity uncertainty is elegant, interpretable, and computationally efficient. This is a non-trivial mathematical contribution that generalizes beyond this specific application.\n- **Strong Empirical Evaluation:** Comprehensive experiments across multiple datasets (Flowers-102, CIFAR, Food-101, ImageNet-R, UCF101, SUN397) and tasks (zero-shot classification, active learning). The paper also shows comparison with strong baselines: temperature scaling, test-time augmentation, and probabilistic embedding models. Results show consistent improvement in calibration metrics (ECE) and competitive or improved accuracy.\n- **Robustness and Generality:** The method performs well even when the Hessian is estimated on proxy datasets, showing robustness to missing original training data. The paper provides ablations on hyperparameters (τ, λ) and data subset sizes for Hessian estimation.\n\nOverall, the main paper is well-written and complemented by detailed appendices with derivations, algorithms, and hyperparameters.\nThe authors mention plans to release code, aiding reproducibility."}, "weaknesses": {"value": "- **Assumption of Independence (Modality Factorization)** The method assumes independence between image and text modalities (P ⊥⊥ Q) to enable tractable posterior estimation. Although the authors justify this as a local approximation around the MAP estimate, it weakens the theoretical rigor since VLMs are inherently cross-modal. Also, the impact of this assumption on downstream uncertainty fidelity is not fully explored. Can the authors comment on this?\n- **Limited Scope of Bayesian treatment:** The Laplace approximation is applied only to the final projection layers, while the encoders remain deterministic. This limits the expressiveness of the uncertainty model, potentially underestimating uncertainty arising from deep feature extraction. Although, I understand  that an approximation to the entire encoder layers would be a huge sacrifice for efficiency/practicality, could the authors comment on any other plausible reasons why/why not to consider this?\n- **Evaluation of Uncertainty Quality:** While calibration metrics (ECE, NLPD) are reported, it would strengthen the paper to show qualitative examples or diagnostics of uncertainty estimates (e.g., in failure or domain shift cases).\n- **Active Learning Evaluation Setup:** The active learning results are promising but limited to relatively small-scale benchmarks (OfficeHome, ImageNet variants). It is unclear how the approach would scale to real-world large-scale active learning pipelines or multimodal retrieval settings.\n- **Comparative baselines:** The paper primarily compares with test-time augmentation and temperature scaling, but omits comparison with recent Bayesian adapters or probabilistic prompt tuning methods (e.g., BayesAdapter [1]). A direct comparison could more clearly position BayesVLM in the landscape of uncertainty-aware VLMs.\n- **Interpretability of Uncertainty:** Although the authors claim interpretability under corruptions, the paper could better visualize what kind of uncertainties are captured (epistemic vs. aleatoric). Without this, it’s difficult to assess if BayesVLM truly models epistemic uncertainty or merely produces smoother predictions.\n\n**References**\n\n[1] Morales-Alvarez, Pablo et al. “BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation.” (2024)"}, "questions": {"value": "- **Direct baselines.** Can the authors comment on adding ProLIP (frozen or few-shot adapted) [1] and CLAP4CLIP (single-task setting) [2] as baselines on selected zero-shot datasets to validate BayesVLM’s calibration per FLOP gains?\n- **Data requirements:** When estimating the Hessian with proxy datasets, how do you ensure alignment between proxy and original data distributions? Could this introduce bias in uncertainty estimates?\n- **On scalability:** What is the memory or compute bottleneck when applying BayesVLM to very large projection layers (e.g. CLIP large)? Could low-rank approximations be used instead?\n\n**References:**\n\n[1] Chun, Sanghyuk et al. “Probabilistic Language-Image Pre-Training.” ICLR 2025.\n\n[2] Jha, Saurav et al. “CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models.” NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S7RBQq4M4q", "forum": "XLiUcvHfzS", "replyto": "XLiUcvHfzS", "signatures": ["ICLR.cc/2026/Conference/Submission17720/Reviewer_88Gp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17720/Reviewer_88Gp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760798942616, "cdate": 1760798942616, "tmdate": 1762927552465, "mdate": 1762927552465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BayesVLM, a post-hoc, training-free method to endow pre-trained VLMs (CLIP, SigLIP) with epistemic+aleatoric uncertainty. The approach fits a Laplace approximation over the final projection layers of the image and text encoders, assumes modality-wise independence to get tractable posteriors, and then analytically propagates uncertainty to cosine similarities via a Gaussian (“ProbCosine”) moment approximation. The method improves calibration (ECE) in zero-shot classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is training-free and works with off-the-shelf CLIP-like models.\n\n2. The proposed method provides overall better calibration performance on the evaluated benchmarks with small computation overhead.\n\n3. Proxy-data robustness: Hessians from CC12M still work decently for CLIP trained on LAION-400M."}, "weaknesses": {"value": "1. Treating image and textual modalities as independent is the core approximation. While the authors justify it via local post-hoc around MAP, it remains a potential mismatch for strongly coupled modalities; discussion is present but could use a stronger empirical illustration.\n\n2. The proposed method puts all epistemic uncertainty in the final projections. This may under-estimate uncertainty on heavy distribution shifts.\n\n3. For closed-source models, the approach needs proxy data; results are promising but slight degradations exist. A sensitivity analysis to proxy domain mismatch (e.g., caption style, image domain) would help."}, "questions": {"value": "1. Please provide some empirical results on the independence of two modalities.\n\n2. For ImageNet setup, please include more ImageNet variants such as ImageNet-A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mXRVaCrLqS", "forum": "XLiUcvHfzS", "replyto": "XLiUcvHfzS", "signatures": ["ICLR.cc/2026/Conference/Submission17720/Reviewer_9dNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17720/Reviewer_9dNF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721470763, "cdate": 1761721470763, "tmdate": 1762927551967, "mdate": 1762927551967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes post-hoc uncertainty estimation in VLMs (CLIP) that does not require additional training. This method performs Laplace approximation in the last layer of encoder to induce probabilistic feature embeddings, which can quantify uncertainty. Then it transform the cosine similarity into a probablistic distribution, and use uncertainty score to support set selection in active learning. In the experiments, they show uncertainty estimates derived from this approximation improve the calibration of these models on several zero-shot classification benchmarks and are effective in active learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The mathmatical process is solid.\n2. This is a post-hoc method, which does not require any retraining, fine-tuning, or modifications to the VLM architecture. It is only a Laplace approximation to quantify uncertainty. It is easy to apply."}, "weaknesses": {"value": "1. The motivation to measure uncertainty of VLM (clip) is not attractive. The significance of the topic need be emphasized. Maybe it is for effectively selecting training data in active learning. If BayesVLM has more application fields, it will be better.\n2. The method relies on two assumptions that may not always hold: (a) the image and text embeddings can be modeled with Gaussian distributions, and (b) the two modalities are independent. These are simplified situations."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZpkiU3WKUV", "forum": "XLiUcvHfzS", "replyto": "XLiUcvHfzS", "signatures": ["ICLR.cc/2026/Conference/Submission17720/Reviewer_VDEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17720/Reviewer_VDEV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000244365, "cdate": 1762000244365, "tmdate": 1762927551504, "mdate": 1762927551504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}