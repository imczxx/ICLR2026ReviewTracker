{"id": "vQCuq1xjC8", "number": 3559, "cdate": 1757474507892, "mdate": 1759898081312, "content": {"title": "Towards Subject-Consistent and Text-Aligned Personalized Image Generation via Precise Attribute Learning", "abstract": "Recent advances in personalized image generation using Diffusion Transformers (DiTs) have shown remarkable progress. However, existing approaches face a trade-off between textual alignment and maintaining reference subjects. This issue primarily stems from the fact that directly injecting subject tokens may disrupt the sampling trajectory of the base model, while the methods through textual inversion struggle to capture detailed attributes of the subject. To address these limitations, we introduce a DiT based subject-driven generation framework Genova with an innovative attribute learning module. This attribute learning module integrates subject image tokens to improve the text-stream modulation, enhancing the representation of the subject's visual attributes distinctly. Contrary to traditional modulation techniques in DiTs, our proposed framework leverages the hierarchical features from the subject image tokens, facilitating more effective attribute learning. This enhancement allows for precise semantic understanding of the subject, thereby optimizing the model's inherent capabilities for textual alignment and enabling more flexible and controllable image generation. Moreover, we develop a synthetic dataset CoupleX featuring subject-paired samples that focus on depicting the activities and interactions within natural scenes, providing a richer context than previous datasets. Extensive experiments demonstrate that our method outperforms current state-of-the-art methods and achieves subject and prompt consistent personalized image generation.", "tldr": "Address the trade-off of textual alignment and subject consistency in personalized image generation", "keywords": ["personalized image generation; hierarchical feature extraction; text modulation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9684e7fe24947bec3a2acb3a99f5b2e5e293d27e.pdf", "supplementary_material": "/attachment/903e0b2cccd4c5f742904dbd36d591f447fa3ebb.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to achieve a balance between text prompts and original image features in\npersonalized image generation. Prior works tend to overemphasize either the text prompt—resulting in\nloss of fine image details—or the original image—leading to stiff, less semantically aligned results. To\naddress this, the authors propose Genova, a framework designed to ensure both text alignment (Text-\nAligned) and subject consistency (Subject-Consistent) in generated images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a key challenge in subject-driven generation: the trade-off between\ntextual alignment and subject consistency.\n2. The proposed attribute learning module is novel in its attempt to integrate hierarchical subject\nfeatures into the modulation process.\n3. The introduction of the CoupleX dataset addresses a meaningful gap in existing data, particularly\nin capturing fine-grained interactions.\n4. The use of dropout as a regularizer is simple yet effective, as shown in the ablation study."}, "weaknesses": {"value": "1. Unclear Contribution of the Core Module:\nThe ablation study (Table 2) reveals a critical issue: adding the proposed attribute learning module\nalone leads to a significant performance drop (CP-PF from 0.600 to 0.467). Performance is\nrecovered and surpasses the baseline only when combined with dropout. This strongly suggests\nthat the observed improvements may be primarily attributable to the regularization effect of\ndropout rather than the intrinsic design of the module itself. The authors should provide further\nevidence to disentangle these contributions, such as a sensitivity analysis across dropout rates or\nexperiments with alternative regularization methods.\n2. Questionable Module Design:\nThe design of the subject-driven self-attention, where K and V are derived from the concatenation\nof attribute and subject tokens while Q comes only from attribute tokens, deviates from standard\npractice. This design risks creating overly strong correlations between the input and output,\npotentially leading to rank collapse in the attention matrix and encouraging the model to learn a\nsimplistic identity mapping rather than a semantically meaningful alignment. The performance\ndrop without dropout supports this concern.\n3. Limited and Subjective Evaluation Benchmark:\nThe reliance on a custom benchmark and metrics (CP-L/PF-L, CP-H/PF-H) that are heavily based\non VLM and human evaluation makes it difficult to objectively compare the method against the\nbroader literature. The inclusion of more established, objective metrics (e.g., CLIP-I, DINO-Vis) is\nnecessary for better reproducibility and fair comparison. The CLIP-T scores are also too close\nacross all methods to be discriminative."}, "questions": {"value": "1. Could the authors provide further justification for the specific Q/K/V design in the attribute learning\nmodule? Were alternative designs (e.g., using subject tokens only for K/V) explored, and if so,\nwhat were the results?\n2. Given that the module alone degrades performance, what specific functionality does it provide\nthat, when regularized by dropout, leads to improvement? Can the authors design an experiment\nto isolate the module's contribution from that of the dropout?\n3. Would the authors consider adding more standard evaluation metrics (e.g., CLIP-I, DINO) to\nfacilitate a more direct comparison with future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ys109aJMXP", "forum": "vQCuq1xjC8", "replyto": "vQCuq1xjC8", "signatures": ["ICLR.cc/2026/Conference/Submission3559/Reviewer_MZfB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3559/Reviewer_MZfB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405944748, "cdate": 1761405944748, "tmdate": 1762916824201, "mdate": 1762916824201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the challenge in the field of personalized image generation, where it is difficult to simultaneously ensure both textual alignment and the fidelity of reference subjects. To overcome this, the authors introduce an innovative attribute learning module. Additionally, they construct the synthetic dataset CoupleX, which features subject-paired samples focused on depicting activities and interactions within natural scenes. The proposed method demonstrates promising results."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper successfully identifies the limitations of the two previously proposed approaches: specialized text learning and token injection. It provides clear reasoning for the constraints observed in each method’s results. \n- The paper is well-written and easy to follow.\n-  A comprehensive introduction to the new subject-paired dataset, CoupleX."}, "weaknesses": {"value": "- What is the significance of this work in the context of powerful base models, such as nano banana?\n\n- From a macro perspective, the proposed framework is simply composed of specialized text learning and token injection. Consequently, the methodological novelty appears limited, and the contribution seems incremental.\n\n- To validate the necessity of the attribute learning module, an ablation study should be conducted. This would involve removing the attribute learning module while keeping the design of the other components intact, and then using simple textual inversion to inject features for comparison"}, "questions": {"value": "In line 157, the 'text input' refers to the phrase 'A butterfly rests on a white lily' or the term 'white lily' itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BtT2XAvqkg", "forum": "vQCuq1xjC8", "replyto": "vQCuq1xjC8", "signatures": ["ICLR.cc/2026/Conference/Submission3559/Reviewer_2kwM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3559/Reviewer_2kwM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546530559, "cdate": 1761546530559, "tmdate": 1762916823155, "mdate": 1762916823155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Genova, a DiT-based subject-driven generation framework featuring a novel attribute learning module. The paper claims that Genova can capture detailed subject attributes. Furthermore, the authors develop CoupleX, a synthetic dataset of subject-paired samples that focuses on activities and interactions within natural scenes. The results show that Genova achieves state-of-the-art (SOTA) performance in subject- and prompt-consistent personalized image generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The goal of capturing attribute-level information presents a strong motivation.\n\nThe hierarchical design is reasonable."}, "weaknesses": {"value": "The main claimed contribution, attribute learning, is not adequately verified in this paper.\n\n*1. Lack of Evidence for Attribute Learning:Why can the proposed Attribute Learning module actually learn attributes? I did not see any compelling evidence to support the claim that the module learns distinct attributes. For instance, the statement in Lines 208-210 (\"shown in Fig. 2, Genova predicts $\\Delta attribute$ of 'white petals,' 'sparse stamens,' and 'green leaves' for the 'white lily' subject from the given image\") cannot be substantiated by an examination of Figure 2.\n\n*2. Irrelevance of CoupleX Dataset:The proposed dataset, CoupleX, seems irrelevant to attribute learning. It appears to be a standard subject-driven generation dataset. Furthermore, the paper is missing a crucial comparison with other existing subject-driven datasets.3. \n\n*3. Evaluation Benchmark:The evaluation benchmark lacks transparency. The paper relies on only one self-curated benchmark gathered from different sources, which contains only 50 subjects. The authors need to clarify: What is the total number of test cases? Why are results from other established benchmarks not reported?"}, "questions": {"value": "The main problem is that the paper does not sufficiently investigate or adequately support its core idea, which is: \"Towards Subject-Consistent and Text-Aligned Personalized Image Generation via **Precise Attribute Learning**\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wy411xU7aq", "forum": "vQCuq1xjC8", "replyto": "vQCuq1xjC8", "signatures": ["ICLR.cc/2026/Conference/Submission3559/Reviewer_Q5N3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3559/Reviewer_Q5N3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002597214, "cdate": 1762002597214, "tmdate": 1762916822740, "mdate": 1762916822740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Genova framework for personalized image generation. Specifically, to avoid the \"copy-paste\" issue, it uses dropout to weaken the direct injection of the reference image. Instead, it leverages image features to guide and enhance the modulation process of textual conditioning. This enables the model to more precisely understand the subject's visual attributes and combine them with the text description. Experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Instead of making incremental improvements on the two existing mainstream methods (text learning vs. token injection), it utilizes image features to enhance text modulation, cleverly bypassing the inherent drawbacks of both traditional approaches.\n\n2. This paper points out the prevalent 'copy-paste' problem in 'token injection' methods and proposes using Dropout on the subject image tokens during training."}, "weaknesses": {"value": "- The custom benchmark from DreamBooth may not contain challenging samples with precise attributes. Considering the large scale of the $45,548$ paired dataset, this raises concerns about simply overfitting to the custom benchmark using such a large dataset. Especially compared with ablation studies on dropout hyperparameters, the results with *0.2* largely outperformed those with *0.5*, indicating that the proposed method lacks effectiveness and robustness.\n\n- Was there an experimental investigation into the dropout hyperparameter settings? The ablation study only shows the values 0.2 and 0.5.\n\n- Only a few qualitative examples on multi-subject driven generation are provided. Regarding multi-concept results, a well-established benchmark already exists in [1].\n\n- There is a citation format inconsistency on line 258.\n\n$[1] \\text{Kumari, Nupur, et al. \"Multi-concept customization of text-to-image diffusion.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.}$"}, "questions": {"value": "- Regarding the motivation for learning precise attributes from hierarchical image tokens, why not directly use fine-grained attribute captions as baseline implementations for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YRMyldVNJl", "forum": "vQCuq1xjC8", "replyto": "vQCuq1xjC8", "signatures": ["ICLR.cc/2026/Conference/Submission3559/Reviewer_quGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3559/Reviewer_quGE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014941566, "cdate": 1762014941566, "tmdate": 1762916820660, "mdate": 1762916820660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}