{"id": "m7gNW26Zih", "number": 10533, "cdate": 1758174853227, "mdate": 1763535334496, "content": {"title": "LLM-Augmented Soft-Label Distillation and Cluster-Guided Alignment for Language-Based Audio Retrieval", "abstract": "Language-based audio retrieval involves fetching audio recordings from a database that most closely align with a provided text query. In this paper, we study language-based audio retrieval with a dual encoder and show that (i) soft-label distillation from an ensemble of retrieval teachers, (ii) LLM-driven caption augmentation (back-translation and caption mix for mixed audio), and (iii) cluster-guided auxiliary classification jointly improve robustness to non-binary audio–caption correspondences. On CLOTHO dataset, our best single model reaches mAP@16 46.6, and a weighted ensemble attains 48.8 on the development test split. While cluster guidance yields mixed gains across backbones, ablations indicate consistent improvements under high correspondence ambiguity.", "tldr": "", "keywords": ["Audio-text retrieval", "contrastive learning", "knowledge distillation", "topic modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2573484457a78c0bbd13c024b71990bee6c53d5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper combines several known engineering tricks to improve audio retrieval performance. The level of innovation and paper writing feels more like a workshop paper rather than a proper ICLR paper candidate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The author acknowledges the use of LLM during paper writing."}, "weaknesses": {"value": "The paper is missing many important details. For example, what do E1, E2, E3, and E4 mean in Table 3? And what does \"system-level\" weighting mean?\nThe paper needs significant improvement on both writing and experimental design."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8539EMLLyU", "forum": "m7gNW26Zih", "replyto": "m7gNW26Zih", "signatures": ["ICLR.cc/2026/Conference/Submission10533/Reviewer_aQdA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10533/Reviewer_aQdA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605072212, "cdate": 1761605072212, "tmdate": 1762921813089, "mdate": 1762921813089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "K1SPoDGKYX", "forum": "m7gNW26Zih", "replyto": "m7gNW26Zih", "signatures": ["ICLR.cc/2026/Conference/Submission10533/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10533/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763535333625, "cdate": 1763535333625, "tmdate": 1763535333625, "mdate": 1763535333625, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a text-to-audio retrieval model with three improvements: 1) soft label distillation from an ensemble of retrieval teachers, 2) LLM-based caption augmentation, including back translation and caption mix, and 3) cluster-guided auxiliary classification."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper shows a model with three improvements and performs well on Clotho dataset."}, "weaknesses": {"value": "This submission might fit a DCASE challenge track. As presented, it does not meet ICLR standards for novelty, comparison to prior work, and clarity of experimental protocol.\n\n1. Missing problem background. The introduction jumps quickly to a recipe without framing the research questions or the nature of audio text correspondence ambiguity beyond brief statements.\n2. No Related Work section. Although citations appear, there is no dedicated related work discussion that situates soft label distillation, clustering for retrieval, or LLM based augmentation relative to prior audio retrieval and cross modal alignment literature.\n3. Lack of external baselines and comparisons. Results are only internal variants and ensembles on Clotho. There is no quantitative comparison to prior published systems on the same split or to widely used retrieval models, which makes it hard to judge novelty or impact.\n4. Ambiguous reporting of evaluation settings. Table 2 mixes “Multiple annotation” and “Single annotation” metrics, but the paper does not define these terms or provide the exact protocol for producing two mAP@10 values. The table header shows two mAP columns and the text around it offers no definitions.\n5. Confusing experiment protocol. In the final paragraph of the results section. It is confusing that the paper is stating: \"we retrained all systems on the entire development split of the CLOTHO dataset and computed the weighted sum of their similarity matrices using the weights from Table 3.\" (What is entire development split? Is it development train split or test split or both?) \"This approach achieved mAP@16 of 0.421 on the evaluation dataset.\" (Before this sentence the mAP@16 reported is multiplied by 100, e.g. \"the ensembles achieved a highest mAP@16 of 48.83\", why here it has a different scale?)"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8rINgo2A9T", "forum": "m7gNW26Zih", "replyto": "m7gNW26Zih", "signatures": ["ICLR.cc/2026/Conference/Submission10533/Reviewer_tZbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10533/Reviewer_tZbx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605546360, "cdate": 1761605546360, "tmdate": 1762921812643, "mdate": 1762921812643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a combination of techniques namely soft-label distillation, LLM-based data augmentation, and cluster-guided alignment for language-based audio retrieval. While the core problem of handling ambiguous audio-text correspondences is significant and the individual methods are reasonable, the paper suffers from critical flaws that severely undermine its contribution. Another issue is that the authors' own ablation study appears to invalidate their core contributions: the proposed LLM augmentation and cluster-guided methods show no consistent improvement over a simpler baseline that uses only distillation. Combined with the complete lack of comparison to existing state-of-the-art methods and a manuscript that reads like an unfinished draft, these issues prevent the paper from making a convincing case for a meaningful contribution."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The focus on handling ambiguous, non-binary audio-text correspondences is a relevant and valuable direction for the field. The techniques employed, such as using LLMs for semantically-aware caption mixing and employing clustering for auxiliary supervision, are conceptually sound and interesting ideas worthy of exploration."}, "weaknesses": {"value": "The paper is structurally incomplete, lacking a proper introduction, related work section, and sufficient detail to be considered a finished submission. This makes a thorough evaluation difficult.  The results in Table 2 indicate that all meaningful gains are attributable to the distillation technique (SID 2). The subsequent addition of LLM augmentation (SID 3) shows a performance drop for the best-performing PaSST model, and the cluster-guided alignment (SID 4/5) fails to recover this loss or provide a clear improvement. This directly challenges the utility of the two techniques highlighted in the title and abstract."}, "questions": {"value": "The manuscript is only 6 pages and lacks a related work section and a substantial introduction. Can you confirm this is the final version intended for review? How do you justify these structural omissions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5rEiWC7eoB", "forum": "m7gNW26Zih", "replyto": "m7gNW26Zih", "signatures": ["ICLR.cc/2026/Conference/Submission10533/Reviewer_K79C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10533/Reviewer_K79C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996664093, "cdate": 1761996664093, "tmdate": 1762921812180, "mdate": 1762921812180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an enhanced dual-encoder framework for text-audio retrieval task with two techniques: Soft-label distillation and Cluster-guided Alignment with auxiliary classification. It leverages an ensemble of teacher models to produce non-binary similarity distributions that capture ambiguous correspondences between audio and captions. Then it applies GPT-4o for back-translation and “LLM-mix” caption generation to expand the caption diversity. Finally it conducts a BERTopic-style clustering on captions to assign pseudo-labels, which are then used as an auxiliary classification objective for both audio and text encoders. The approach is evaluated on the Clotho dataset and the ablation tables show that distillation yields the largest gain. And clustering method gives mixed results depending on backbones (i.e, BEAT, PASST, EAT)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The strength of this paper lies in the motivation. It has clear motivation on improving the text-audio results by enhancing the model's capability in understanding the gap from the predictions to the ground-truths. The paper recognizes that binary audio–caption pairs fail to capture nuanced correspondences (e.g., overlapping acoustic events). The idea of using ensemble soft targets for distillation is reasonable and aligns with prior retrieval improvements in [1].\n\n[1] Estimated Audio-Caption Correspondences Improve Language-Based Audio Retrieval, DCASE technical report, 2024."}, "weaknesses": {"value": "However, the paper has many critical points that hinder its acceptances.\n\nFirst, the paper has limited novelty. The paper mainly combines existing techniques, soft-label distillation, LLM augmentation, and clustering-based auxiliary tasks, which are more or less proposed in many text-audio tasks (audio captioning, text-audio retrieval, and text-to-audio generations) without any new theoretical formulation or model-wise contribution. Each module closely follows prior work [1][2][3] with light adaptations.\n\nSecond, the experimental design is unprofessional. The benchmark comparison is too weak. Only the Clotho dataset is used for evaluation, and it is confusing that the test set of AudioCaps is included in training. The basic benchmark should include both Clotho and AudioCaps test sets for evaluations, the same as many previous text-audio retrieval and audio captioning papers. The text-to-audio retrieval and audio-to-text retrieval performances should split. Nonetheless, the paper does not compare against standard state-of-the-art retrieval or text-audio pretraining frameworks such as CLAP, LAION-CLAP, Comp-A, FLAM, and Audio Flamingo. It has only inner-comparisons among different audio encoders, making it unclear whether the reported numbers represent an advance to other models. \n\nThird, the organization of this paper needs improvement. Figure 1 is difficult to interpret and fails to illustrate the data flow between distillation and clustering modules. The Introduction and Related Work sections are too brief, and the technical sections mix multiple concepts without conceptual clarity. The manuscript reads more like a system-description report than a scientific study, lacking analytical insight and coherent narrative flow.\n\nLast but not least, the theoretical insights of distillation process requires more justification. The proposed soft-label distillation relies entirely on ensemble-generated pseudo-targets and does not incorporate any mechanism to regularize toward ground-truth correspondences. As a result, the convergence behavior of the student model is poorly controlled. It can overfit to the ensemble’s bias without guaranteed correctness. This paper feels more like an empirical “performance trick” in Kaggle (or say DCASE competition ins the audio domain) where you could boost the performance in the last chance. It is rather than a theoretically grounded or generally reliable learning process. The paper does not provide evidence that such soft-target transfer remains effective across datasets or semantic domains, as it is only evaluated on Clotho dataset, where the extreme overfitting might happen.\n\n[1] Estimated Audio-Caption Correspondences Improve Language-Based Audio Retrieval, DCASE technical report, 2024\n\n[2] Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation, ICASSP 2024\n\n[3] BERTopic: Neural topic modeling with a class-based TF-IDF procedure, Arxiv Preprint"}, "questions": {"value": "1. Why was AudioCaps excluded from evaluation despite being part of the pretraining data? \n\n2. How much improvement does each component (distillation, LLM-augmentation, clustering) yield individually on the same backbone? Some combinations of the ablation studies are presented but the individual effectiveness of these three components (or say two components) are not presented.\n\n3. How does the method compare quantitatively with CLAP, Comp-A, and Audio Flamingo under identical Clotho retrieval settings (with and without the same training sets)?\n\n4. What are the computational and data costs introduced by GPT-4o augmentation and large-scale clustering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IZPQjCLRx0", "forum": "m7gNW26Zih", "replyto": "m7gNW26Zih", "signatures": ["ICLR.cc/2026/Conference/Submission10533/Reviewer_ksuK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10533/Reviewer_ksuK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236170393, "cdate": 1762236170393, "tmdate": 1762921811661, "mdate": 1762921811661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}