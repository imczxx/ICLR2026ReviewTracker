{"id": "Yx4xJIepOm", "number": 4863, "cdate": 1757779588105, "mdate": 1762941400957, "content": {"title": "FineBench: Benchmarking and Enhancing Vision-Language Models for Fine-grained Human Activity Understanding", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in general video understanding, yet they often struggle with the fine-grained comprehension crucial for real-world applications requiring nuanced interpretation of human actions and interactions. While some recent human-centric benchmarks evaluate aspects of model behaviour such as fairness/ethics, emotion perception, and broader human-centric metrics, they do not combine long-form videos, very dense QA coverage, and frame-level spatial/temporal grounding at scale. To bridge this gap, we introduce FineBench, a human-centric video question answering (VQA) benchmark specifically designed to assess fine-grained understanding. FineBench comprises 199,420 multiple-choice QA pairs densely annotated across 64 long-form videos (15 minutes each), focusing on detailed person movement, person interaction, and object manipulation, including compositional actions. Our extensive evaluation reveals that while proprietary models like GPT-5 achieve respectable performance, current open-source VLMs significantly underperform, struggling particularly with spatial reasoning in multi-person scenes and distinguishing subtle differences in human movements and interactions. To address these identified weaknesses, we propose FineAgent, a modular framework that enhances VLMs by leveraging a Localizer and a Descriptor. Experiments show that FineAgent consistently improves the performance of various open VLMs on FineBench. FineBench provides a rigorous testbed for future research into fine-grained human-centric video understanding, while FineAgent offers a practical approach to enhance such reasoning in current VLMs.", "tldr": "We built a massive fine-grained video benchmark. Current open-source models stumble; proprietary giants do a little better To help, we designed a plug-and-play booster that makes VLMs pay closer attention to who is doing what and when.", "keywords": ["Dataset and Benchmark", "Video Understanding", "Human Understanding", "Vision-Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a646021b43f2c57effd6d2a470685e4edfc60a41.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces FineBench, a human-centric video question answering (VQA) benchmark specifically designed to assess fine-grained understanding, comprising of 199,420 multiple-choice QA pairs densely annotated across 64 long-form videos (15 minutes each), focusing on detailed person movement, person interaction, and object manipulation, including compositional actions. The paper also proposes FineAgent, a modular framework that enhances VLMs by leveraging a Localizer and a Descriptor."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and motivated, especially I agree that the problem of rigorously benchmarking fine-grained information is crucial and timely.\n\n2. The paper evaluates many models and also proposes a method, FineAgent to improve performance, and conducts many ablations to localise the source of improvement."}, "weaknesses": {"value": "See Questions"}, "questions": {"value": "1. While the key contribution of the paper is a new benchmark, I find the details presented in Section 3.2 regarding the dataset creation process to be severely lacking. For example, the authors mention “The primary strategy involves selecting actions that are semantically similar to the correct answer, based on a predefined similarity mapping”. What is the similarity mapping? How are the semantic similarities computed? Also, the authors mentioned that they perform subject disambiguation, is this done automatically or using manual annotations? If its the latter, do the authors conduct an inter-annotator agreement to ensure things are correctly labeled, etc? I would urge the authors to provide more details here, especially since many people might not be familiar with the vase AVA dataset.\n\n2. The authors claim that existing benchmarks often lack a specific focus on fine-grained human-centric actions. However, I find this claim not to be empirically substantiated in the paper. In my anecdotal experiences, while other benchmarks like VideoMME have not been specifically curated for human interactions, by virtue of many videos featuring humans and their interactions they contain such questions. Can the authors empirically consolidate this point via trying to quantify the proportion of fine-grained human related questions in existing benchmarks? I think since this is one of the main novel contributions, it's important to clearly establish this contribution.\n\n3. What is the rationale behind curating a benchmark with ~200k QAs, especially since we see that performance trends are quite similar on the ~20k benchmark as well. Since the overall question categories are not overly many, each must contain many QAs, and probably do not need so many samples to bring out discriminative power. \n\n4. In Figure-3c), we see that the performance decreases as the number of frames are increased which is very counterintuitive to me. Especially for a finegrained understanding benchmark one would expect that performance improves as more visual information is provided. In general, improved performance with increased frame rate is considered a desirable benchmark property as it denotes the benchmark requires integrating information over different frames and is not biased towards only a singular or few frames, etc.\n\n5. Can you compare more scaffoldings such as VideoAgent as baselines against FineAgent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kxrtLNdNDs", "forum": "Yx4xJIepOm", "replyto": "Yx4xJIepOm", "signatures": ["ICLR.cc/2026/Conference/Submission4863/Reviewer_yHHT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4863/Reviewer_yHHT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891826340, "cdate": 1761891826340, "tmdate": 1762917622808, "mdate": 1762917622808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wbs7ZbuOOc", "forum": "Yx4xJIepOm", "replyto": "Yx4xJIepOm", "signatures": ["ICLR.cc/2026/Conference/Submission4863/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4863/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938476141, "cdate": 1762938476141, "tmdate": 1762938476141, "mdate": 1762938476141, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce FineBench, a dense, human-centric video VQA benchmark aimed at testing fine-grained understanding of human actions and interactions. They show that the current open VLMs struggle on these skills and also propose FineAgent, a framework that improves VLM fine-grained performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel benchmark for fine-grained video understanding. FineBench combines large scale (199,420 QA over 64 long videos) with both dense temporal and spatial grounding.\n\nGood evaluation. The paper benchmarks a wide range of proprietary and open VLMs and surfaces concrete failure modes—accuracy drops in multi-person scenes and lower performance on Person Movement/Interaction vs. Object Manipulation\n\nFineAgent. The modular Localizer+Descriptor add-on is well-motivated and yields consistent gains across models"}, "weaknesses": {"value": "Limited diversity and generalization scope. FineBench, while dense, is built from only 64 long videos. This narrow domain risks overfitting to cinematic, Western-style scenes and may not generalize to other video domains (e.g., egocentric, sports, social media)"}, "questions": {"value": "In figure three, it is hard to see how accuracy drops as the number of people increases.something like c plot would be much better,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "19NjdSPU0p", "forum": "Yx4xJIepOm", "replyto": "Yx4xJIepOm", "signatures": ["ICLR.cc/2026/Conference/Submission4863/Reviewer_CAsR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4863/Reviewer_CAsR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925373783, "cdate": 1761925373783, "tmdate": 1762917622228, "mdate": 1762917622228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FineBench, a new benchmark designed to evaluate fine-grained, human-centric video understanding. The paper conducts a comprehensive evaluation of state-of-the-art VLMs. The paper proposes FineAgent, a modular framework leveraging spatial grounding and contextual captioning, demonstrating its effectiveness in improving the fine-grained video understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Compared with previous benchmarks, the proposed FineBench contains larger number of QA pairs, with spatial grounding and temporal grounding in a human-centric setting.\n2. It conducts thorough evaluation on current open and close -sourced models.\n3. The proposed FineAgent works well several open-sourced models."}, "weaknesses": {"value": "1. The idea of benchmarking fine-grained reasoning is not conceptually new. Several existing works already target similar objectives.\n2. The taxonomy of \"fine-grained reasoning\" mixes low-level visual discrimination with higher-level temporal and spatial reasoning under one umbrella. The evaluation interpretation becomes vague, it is unclear whether model errors reflect poor perception, grounding or reasoning, making the proposed benchmark unreliable.\n3. The paper lacks a comprehensive analysis of how current models failed in each type of QA. (e.g., failure type breakdown, visual attention maps, per-category robustness) And lack of visual examples."}, "questions": {"value": "1. How do you identify which person in the video is the \"person\" in the question referring to? \n2. How do you review the annotation quality of the benchmark? Do you have audio information? In Fig1(b), it is even hard for humans to understand which person is talking at the moment."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kqgQV40bKd", "forum": "Yx4xJIepOm", "replyto": "Yx4xJIepOm", "signatures": ["ICLR.cc/2026/Conference/Submission4863/Reviewer_XCgd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4863/Reviewer_XCgd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975595910, "cdate": 1761975595910, "tmdate": 1762917621936, "mdate": 1762917621936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FineBench, a large-scale benchmark for fine-grained human-centric video question answering (VQA), containing about 200k QA pairs densely annotated across 64 long-form videos. It aims to evaluate models’ ability to reason about subtle human actions, movements, and interactions. The authors also introduce FineAgent, a modular enhancement that combines a Localizer for spatial grounding and a Descriptor for contextual captioning. Experiments demonstrate that FineAgent improves multiple open-source vision-language models (VLMs) on FineBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- FineBench is a well-motivated benchmark that significantly extends existing VQA datasets by emphasizing dense spatial and temporal grounding in human-centric videos.\n- The empirical evaluation is thorough, covering both open and proprietary models with meaningful analyses of weaknesses (e.g., multi-person disambiguation, fine motion reasoning).\n- FineAgent is a practical framework that yields consistent improvements, and its modularity makes it easy to adapt to different models.\n- Figures and tables are clear and well connected to the text. The dataset design and annotation process are transparent and reproducible."}, "weaknesses": {"value": "- The paper lacks discussion and comparison with several recent fine-grained video benchmarks (e.g., MotionBench, VER-Bench, Finer), which weakens the positioning of FineBench’s novelty.\n- The dataset construction, though detailed, relies heavily on templated questions and lacks formal mathematical specification for distractor generation, which could limit reproducibility.\n- Experiments on downstream or transfer tasks are missing, making it unclear whether FineBench improvements generalize to real-world applications.\n- The scalability and computational cost of FineAgent are only briefly mentioned; more quantitative profiling would strengthen the paper.\n- Some aspects of the contribution feel incremental rather than conceptually novel, as the work builds on existing datasets and modular reasoning designs."}, "questions": {"value": "1. Can the authors provide formal notation or pseudocode for distractor generation and spatial referencing?\n2. How does FineBench differ experimentally from MotionBench or VER-Bench?\n3. Have the authors tested FineAgent with different Descriptor models to assess its generality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DQXkJt5Fa8", "forum": "Yx4xJIepOm", "replyto": "Yx4xJIepOm", "signatures": ["ICLR.cc/2026/Conference/Submission4863/Reviewer_CLti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4863/Reviewer_CLti"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762784445729, "cdate": 1762784445729, "tmdate": 1762917621691, "mdate": 1762917621691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}