{"id": "YjGKuOiTbR", "number": 5937, "cdate": 1757947704670, "mdate": 1759897943646, "content": {"title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality", "abstract": "Safeguarding vision-language models (VLMs) is a critical challenge, as existing methods often suffer from over-defense, which harms utility, or rely on shallow alignment, failing to detect complex threats that require deep reasoning. To this end, we introduce **PRISM** (**P**rincipled **R**easoning for **I**ntegrated **S**afety in **M**ultimodality), a system2-like framework that aligns VLMs by embedding a structured, safety-aware reasoning process. Our framework consists of two key components: PRISM-CoT, a dataset that teaches safety-aware chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree Search (MCTS) to further refine this reasoning through Direct Preference Optimization to help obtain a delicate safety boundary. Comprehensive evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90% improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also exhibits strong robustness against adaptive attacks, significantly increasing computational costs for adversaries, and generalizes effectively to out-of-distribution challenges, reducing attack success rates to just 8.70\\% on the challenging multi-image MIS benchmark. Remarkably, this robust defense is achieved while preserving model utility.", "tldr": "This paper introduces PRISM, a framework that teaches VLMs a structured reasoning process to identify harmful intention, which highlights the critical trade-off between safety and utility in VLM alignment.", "keywords": ["Safety Alignment; Vision Language Model; Reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/849c3c08793d80631238d3fe053f612db2bc0463.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes PRISM, a safety alignment framework for vision-language models (VLMs). The goal is to make models (1) refuse unsafe multimodal requests and (2) still answer benign queries helpfully instead of over-refusing. The key idea is to force the model to reason explicitly about safety in a structured 4-step format: (i) analyze the user's textual intent, (ii) describe the image in context, (iii) jointly reason about text and image to detect “problem-unsafe”, “image-unsafe”, or “combination-unsafe” risks, and (iv) either refuse with an explanation of which policy is violated or provide a helpful answer if it is safe. The authors also build PRISM-CoT (a supervised dataset of such structured reasoning traces) and PRISM-DPO (a preference dataset created by sampling multiple candidate reasoning steps and labeling safer/helpful steps as preferred). The final model is trained with supervised fine-tuning on PRISM-CoT and then Direct Preference Optimization using PRISM-DPO. Experiments on jailbreak benchmarks and adaptive attack settings show lower attack success rate while mostly keeping helpfulness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets an important and realistic safety problem for multimodal models: jailbreaks where the harmful intent is only visible when combining the image and the text. This “combination-unsafe” setting is largely underexplored in prior work and is clearly defined here.\n2. The use of a fixed 4-step reasoning template (Problem, Caption, Reasoning, Output) is conceptually interesting. It encourages the model to explicitly articulate why something is unsafe, instead of giving a generic refusal, and it tries to avoid over-defensive behavior on benign inputs.\n3. The training pipeline explicitly balances safety and helpfulness. The data includes both malicious and benign cases, and “good” behavior is either (i) a clear, policy-grounded refusal for unsafe queries or (ii) an actually helpful answer for benign queries, not blanket denial.\n4. The evaluation is broad. The paper reports results on multiple multimodal jailbreak benchmarks, includes an adaptive attacker setting, and also measures helpfulness. Reported numbers suggest PRISM reduces attack success without collapsing utility."}, "weaknesses": {"value": "1. The “MCTS” component in PRISM-DPO appears oversold. The paper claims to use Monte Carlo Tree Search to build preference data. But what is actually done is: for each reasoning step, the model samples a few candidate continuations (e.g., k=3k=3k=3), GPT-4o scores each candidate for safety (on malicious inputs) or helpfulness (on benign inputs), and the higher-scoring candidate is treated as the positive example while the lower-scoring one is treated as negative. Safety rewards are not back-propagated to earlier steps, and preference pairs are formed locally at the same depth using simple score thresholds. This is essentially per-step sampling plus reranking. It is not clear that UCB, visit counts, or other MCTS machinery is doing meaningful global search over full 4-step trajectories. As written, “safety-aware MCTS” looks more like branding than a necessary algorithmic ingredient.\n2. There is no ablation to justify that this “MCTS”-based data construction is actually better than a simpler baseline. The paper does not compare (i) PRISM-CoT SFT only, (ii) SFT with naive DPO built from per-step top-vs-bottom scoring without any tree search framing, and (iii) full PRISM with the claimed MCTS loop. Without this, it is hard to judge how much of the reported gains actually come from the proposed DPO construction versus just standard preference tuning.\n3. The whole pipeline depends on GPT-4o as the judge for both safety (unsafe queries) and helpfulness (benign queries), and the final evaluation also relies on automated judges. This raises questions about potential bias inherented from the model, the concerns on reproducibility, and whether the safety the model learns is simply “whatever GPT-4o calls acceptable” rather than a stably safe policy."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOsQxglctI", "forum": "YjGKuOiTbR", "replyto": "YjGKuOiTbR", "signatures": ["ICLR.cc/2026/Conference/Submission5937/Reviewer_Z9M5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5937/Reviewer_Z9M5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748470133, "cdate": 1761748470133, "tmdate": 1762918363033, "mdate": 1762918363033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PRISM framework to safeguard vision-language models. The framework contains two stages: (1) PRISM-COT, using GPT-4o to generate step-by-step safety-related reasoning traces and reformat parts of benign data from LLAVA-CoT to construct the SFT dataset. (2) leveraging MCTS with the designed safety reward and helpful reward to construct preference data, then perform DPO training. Extensive experiments demonstrate the effectiveness of PRISM in enhancing the safety performance of VLMs and preserving model utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and clear\n2. The performance of PRISM is strong, both in safety evaluation and utility"}, "weaknesses": {"value": "1. **Novelty:** The proposed PRISM is too similar to STAIR[1], where the only difference is modality.\n   - Both PRISM and STAIR are two-stage training frameworks, including the long step-by-step reasoning SFT, and an MCTS-based DPO.\n   - The reward design in the MCTS stage is limited compared to STAIR. STAIR combines safety and helpful rewards together instead of safety reward for safety-related queries, and helpful reward for general verifiable queries.\n   - STAIR can perform self-improvement during the DPO stage without the external signal, which is hard to perform in PRISM-DPO.\n   - Could the authors clarify the differences between PRISM and STAIR beyond the modality aspect?\n\n2. **Can PRISM be extended to online-RL?**\n   - During the DPO stage, the preference data is annotated based on safety reward and helpful reward. Is it able to perform online-RL (PPO, GRPO) to enhance model performance? Since it is a more common method to enhance model reasoning capability.\n\n3. **More experiments**\n   - Can the author provide results on more general VQA benchmarks to prove PRISM preserves the utility (e.g. MMMU, MMStar)\n   - The test-time scaling is applied only to the safety task. However, it is a common strategy that can also be employed for general tasks. It would be better to evaluate test-time scaling on benchmarks such as MM-Vet, MMMU, and MMStar as well.\n\n[1] STAIR: Improving Safety Alignment with Introspective Reasoning"}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9tjc0WhAF6", "forum": "YjGKuOiTbR", "replyto": "YjGKuOiTbR", "signatures": ["ICLR.cc/2026/Conference/Submission5937/Reviewer_425i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5937/Reviewer_425i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774926099, "cdate": 1761774926099, "tmdate": 1762918362670, "mdate": 1762918362670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRISM, a reasoning-based safety alignment framework for Vision–Language Models. It constructs (i) PRISM-CoT, a four-stage safety-aware reasoning dataset (Problem, Caption, Reasoning, Output), and (ii) PRISM-DPO, a step-level MCTS-generated preference dataset for Direct Preference Optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses a timely and important challenge in multimodal safety by proposing a structured reasoning pipeline that explicitly decomposes and interprets image–text interactions responsible for jailbreaking.\n\n- This paper builds a comprehensive multimodal safety dataset combining structured reasoning traces (PRISM-CoT) and MCTS-generated step-level preferences (PRISM-DPO), offering valuable supervision for fine-grained safety alignment."}, "weaknesses": {"value": "- This paper does not include ablations or diagnostic experiments that isolate whether safety gains come from the structured reasoning process itself. Without controlled ablations that remove or vary reasoning steps, it is unclear whether improvements reflect genuine reasoning ability or additional supervision.\n\n- Since GPT-4o is used for data generation, reward definition, and evaluation, the framework effectively learns to reproduce its evaluator’s preferences, forming a closed alignment loop that risks self-confirmation rather than genuine robustness.\n\n- The experiments do not include recent VLMs such as Qwen3-VL or other latest models, and lack analysis across different model scales, limiting evidence of how well PRISM generalizes."}, "questions": {"value": "- How can the authors demonstrate that PRISM’s improvements stem from structured reasoning rather than from additional supervision or data curation, for example through controlled ablations or reasoning-step perturbation studies?\n\n- Given that GPT-4o is used throughout supervision and evaluation, how can the authors ensure that PRISM is learning evaluator-agnostic safety reasoning rather than merely aligning to GPT-4o’s own safety priors?\n\n- Have the authors explored applying PRISM to more recent or larger-scale VLMs (e.g., Qwen3-VL or similar) to verify whether its structured reasoning framework scales consistently with model capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FiXGH8Jl4H", "forum": "YjGKuOiTbR", "replyto": "YjGKuOiTbR", "signatures": ["ICLR.cc/2026/Conference/Submission5937/Reviewer_R53w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5937/Reviewer_R53w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762522944229, "cdate": 1762522944229, "tmdate": 1762918362378, "mdate": 1762918362378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}