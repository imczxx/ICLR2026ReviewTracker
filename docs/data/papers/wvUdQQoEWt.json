{"id": "wvUdQQoEWt", "number": 2794, "cdate": 1757251854376, "mdate": 1759898126777, "content": {"title": "Hierarchical W-Learning", "abstract": "Inspired by a model of the brain called projective simulation, which has attracted interest among physicists in recent years, we develop a simple and generic new method for hierarchical reinforcement learning in this article. The proposed method generalizes the action-value Q function to  W function, enabling the agent to execute actions according to a hierarchical strategy. In the first part of the article, we present a rigorous construction of the hierarchical structure, along with the W-learning algorithm and the hierarchical policy gradient theorem. In the second part, as an example, we illustrate the W-learning procedure in the context of a navigation task. Experimental results show that the introduction of the hierarchical structure can lead to better performance than traditional Q-learning, provided the strategy is well designed and the update parameters are appropriately chosen.", "tldr": "", "keywords": ["Q-learning", "Hierarchical reinforcement learning", "Robot navigation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6513feef88577ad18572cd438ac5759f2057ea96.pdf", "supplementary_material": "/attachment/e87dd0a038ca9a8d99bfa90cf7c0c9ed420e47d8.pdf"}, "replies": [{"content": {"summary": {"value": "The authors claim to „develop a simple and generic new method for hierarchical reinforcement learning in this article“."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "The experimental evaluation is insufficient. Just a very  simple (scalable) grid world problem is considered.\n \nMore importantly, the text does not study a new RL approach in isolation.\nThe experiments mainly compare two settings: RL without prior knowledge about the task and RL with prior knowledge. Any reasonable algorithm that incorporates meaningful prior knowledge should perform better, right?\n\nIt is not about hierarchical vs non-hierarchical, because all „hierarchical“ methods have more information about the task than the standard methods.\n\nIn the experiments, the „W-learning“ method is given extra information about the task.\nSee Figure 3, left branch, which gives hints for a target in the upper right corner (as it is the case in the first experiments) or, explicitly, the definition of F, the optimal policy w/o obstacle, in line 380. The function is provide in the „W-learning“ experiments, see Fog. 6.\n\nThere are many control experiments that are not considered, for example:\n1. Simplest one: Start standard Q-learning with the Q function initialised to the optimal policy without obstacles (i.e. start with Q^F in a table).\n2. Starting from an agnostic policy pi with parameter vector theta, define the policy pi’(s) = \\theta’ F(s) + (1-theta’) pi(s) that incorporate prior information and use some actor-critic method to learn pi’ , i.e., learn (theta, theta’) .\n\nAnother comment regarding the presented experiments/results:\nAs the objective functions are different, the learning rates could be significantly different for Q- and „W“-learning.\nWhile it is appreciated that several learning rates were tested, it would be interesting to see the results for an even higher learning rate for Q-learning (which is perhaps cooled down over time). Would there still be a significant difference?\nWhat about a different initialisations of the Q function?\n\n\n\n## Minor:\n\nThe factorisation in (3) seems to be only for a chain, not for a general hierarchical model.\n\n„and many people believe that building a proper model of it may finally lead to artificial intelligence.“: This is a completely empty statement w/o giving a (working) definition of AI.\n\n„As we can see, the clip structure is a model for the neural network in the brain.“: No, I do not see this. The figure shows some generic acyclic graph. This is totally empty statement. That there is no recurrence in the figure makes it extra sad.\n\nWhat have the path integrals in quantum mechanics to do with the study?\n\nThe authors could check out the literature on policy/state-space factorisation.\n\nWho is \"et al.\" in the reference \"Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction, volume 1. MIT\npress Cambridge, 1998.\"?"}, "questions": {"value": "See questions in \"Weaknesses\" above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kIXOTgsh04", "forum": "wvUdQQoEWt", "replyto": "wvUdQQoEWt", "signatures": ["ICLR.cc/2026/Conference/Submission2794/Reviewer_GJFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2794/Reviewer_GJFn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803993829, "cdate": 1760803993829, "tmdate": 1762916380092, "mdate": 1762916380092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hierarchical reinforcement learning approach called W-learning, The method is based on abstractions called \"Clips\" which are intermediate structures or goals between the perception and the action. Rather than learning to map perceptions to actions, the execution engine goes through a sequence of clips until the final clit selects the action. Q-learning is generalized to W-learning, where W is a function of state, strategy and action. The approach is illustrated in two grid navigation domains where the intermediate \"clips\" correspond to  the task of reaching the final destination and the task of avoiding obstacles. The hierarchical approach is shown to perform better than the non-hierarchical approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Hierarchical reinforcement learning is a well-studied topic and yet not fully understood. The paper addresses an interesting problem and presents an apparently successful approach. The empirical results are reasonable."}, "weaknesses": {"value": "The paper does not define the terms precisely enough to evaluate the soundness of the approach. Words like \"Clip\" and \"strategy\" need to be more precisely defined. It cites most relevant work, but does not explain how this work is different. For example the options or MAXQ framework addresses hierarchical RL in a much more precise notation and correctness proofs. The current framework is different, but it does not explain the embedded assumptions carefully and does not give the proof of the main theorem. \n\nTo illustrate in a problem, it is not clear how one should interpret Figure1. The clips do not sequentially follow each other, but look like a directed acyclic graph. It is said that the clip structure is a \"model of the brain\". But what is its connection to the strategy hierarchy? Why is the brain relevant to model here? As a contrasting example, MAXQ is organized as a graph structure over tasks, where tasks have termination conditions, and are similar to subroutines. The procedural semantics of the MAXQ hierarchy allows a sound derivation of global value function in terms of local value functions. While the paper seems to be attempting a similar decomposition, it is much less clear because the strategies here are not tasks and they do not have termination conditions.\n\nAppendices A and B are missing. \n\nIt is also not clear what notion of optimality is applicable, since there are in general many notions (recursive optimality, hierarchical optimality, global optimality) might be in play.  It appears that V(s) should be a function of the strategy, but the paper does not acknowledge that. The meanings of different W functions should be clearly stated.  \n\nHierarchical policy gradient theorem: Can someone view the hierarchical policy controlled by a set of parameters as simply a policy that takes states and policy parameters and outputs a primitive action. If so, then can't someone just use the policy gradient theorem? \n\nThe grid domains are too weak to illustrate the power of the framework. The domains seem to be setup such that each primitive action falls into one of the other goals. What happens if some actions fall under both goals, i.e, avoiding obstacles and also moving towards the goal. More ambitious domains have been attempted in the past literature on hierarchical RL. Multiple levels of hierarchies would be more interesting."}, "questions": {"value": "Question: Clearly define the framework. What is a strategy? How does the hierarchical policy work given a set of parameters? Can hierarchical policy gradient can be viewed as an instance of simple policy gradient with a new (hierarchical) parameterization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5bA0sVuZNy", "forum": "wvUdQQoEWt", "replyto": "wvUdQQoEWt", "signatures": ["ICLR.cc/2026/Conference/Submission2794/Reviewer_6yDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2794/Reviewer_6yDF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761260533037, "cdate": 1761260533037, "tmdate": 1762916379737, "mdate": 1762916379737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for hierarchically learning the Q-function in reinforcement learning (RL), inspired by projective simulation. A new W-function is introduced, which incorporates both the state sss and a hierarchical strategy $\\omega$. The authors derive corresponding policy gradient theorems and present an empirical experiment on a navigation task demonstrating that the proposed method achieves faster convergence compared with standard Q-learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of incorporating a strategy component, in addition to actions, into the RL learning process is interesting and establishes a meaningful connection to hierarchical RL.\n* The paper provides a detailed and illustrative navigation task that effectively bridges theoretical definitions (e.g.,  $\\omega$ and $g$) with practical scenarios."}, "weaknesses": {"value": "Motivation: The hierarchical modeling of the Q-function is not well motivated. Although the authors briefly mention “modeling intra-option behavior” and “handling dynamic tasks,” these ideas are neither theoretically developed nor empirically verified.\n\nExperimental validation: The experiments are insufficient to convincingly demonstrate the advantages of W-learning over existing methods such as Q-learning.\n\n- Evaluation is limited to a single navigation task, with comparison only against Q-learning.\n- Even within this prototype task, W-learning achieves similar asymptotic performance to Q-learning, differing mainly in faster convergence speed.\n\nClarity of definitions: The relationship between the strategy $\\omega$, action $a$, and W-function could be more clearly defined.\n\n- Based on the paper’s description, ω\\omegaω appears to act as an internal or intermediate action, and the W-function seems analogous to a Q-function extended with this additional variable. However, in the navigation task example, the strategy behaves more like a subgoal, which could be more explicitly articulated.\n\nMinor:\nThe paper should use `\\citep{}` instead of `\\cite{}` for most references."}, "questions": {"value": "How should one design and map the strategy variable $\\omega$ in practice? Since this appears to be highly task-specific, does the initialization of  $\\omega$ significantly affect performance, and how might this approach scale to more complex tasks with many possible actions and strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yl3nNj023S", "forum": "wvUdQQoEWt", "replyto": "wvUdQQoEWt", "signatures": ["ICLR.cc/2026/Conference/Submission2794/Reviewer_8g86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2794/Reviewer_8g86"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007072636, "cdate": 1762007072636, "tmdate": 1762916379552, "mdate": 1762916379552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present a novel hierarchical reinforcement learning algorithm that generalize flat action-value Q to a new hierarchical W function, allowing the agent to execute actions according to an hierarchical strategy. The idea is inspired by the model of the brain called projective simulation, and each node in the hierarchy represent a separate task. The paper shows that this formulation side step the need to learn a termination function like we do in the option framework and is possible to learn all the tasks provided that the strategy could be represented in terms of a hierarchical clip structure. The learning algorithm for this new W function is extended to both the classic off-policy learning with an update rule derived from Q learning and to the actor-critic case by means of generalizing the policy gradient."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the crucial and long-standing goal of developing effective hierarchical reinforcement learning (HRL) agents. This is a highly relevant research direction with the potential to significantly advance the field's ability to solve complex, long-horizon problems.\n\n- While the theoretical development is dense, the simple and well-executed experiments provide an intuitive grounding for the W-learning concept. This section effectively clarifies the method's practical application and benefits.\n\n- The W-learning formulation is novel and elegantly sidesteps significant limitations of prior HRL frameworks. Notably, it removes the need to explicitly learn termination functions and it seems to be able to converge to the optimal policy without the risk of hierarchical suboptimality."}, "weaknesses": {"value": "- The paper is confusing the exact structure of a task/ clip structure is vague and hard to grasp. The paper spends much time on proposing an off-policy and actor-critic learning algorithms but fails to introduce and explain very well the new concepts introduced for a strategy and a clip unit making the paper hard to follow. I would suggest to first explain in detail the new idea and the hierarchical structure needed for this learning algorithm together with his properties and limitations before moving on to practical algorithms for learning that.\n\n- The paper importantly is missing a broad discussion on the differences between the proposed new method and the methods already proposed in the literature like Option framework, Max Q, Feudal, Option Critic. Adding this discussion could help to clarify the differences of the proposed method compared to previous methods and highlight the advantages / disadvantages of the proposed methodology.\n\n- The experiments are limited to a very easy discrete tabular domain, and the comparison is only against flat Q learning and only using the off-policy variant of W learning algorithm. \n\n- One of the main challenges modern HRL is facing is to move from hand-crafted hierarchies to learned hierarchies, this paper yet introduce a new framework that still relies on hand-crafted hierarchies and a broader discussion on how this framework could be extended to learned hierarchies is needed."}, "questions": {"value": "See Weaknesses points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ztA86J2mlq", "forum": "wvUdQQoEWt", "replyto": "wvUdQQoEWt", "signatures": ["ICLR.cc/2026/Conference/Submission2794/Reviewer_o6TZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2794/Reviewer_o6TZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762509630457, "cdate": 1762509630457, "tmdate": 1762916379410, "mdate": 1762916379410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}