{"id": "BXzUi2QZ7z", "number": 636, "cdate": 1756758948983, "mdate": 1759898249307, "content": {"title": "Tracing Concept Circuits to Audit and Steer Vision Transformers", "abstract": "Advanced vision models, e.g., Vision Transformers (ViTs), might base their decisions on spurious cues, even for correct predictions. To ensure their safe deployment in high-stakes applications, it is essential to audit ViT decision-making processes and steer them away from unsafe predictions. Traditional interpretation methods typically attribute predictions to salient pixels or neurons. However, such simplified correlations often overlook the concepts encoded in internal representations, which can be the true causes of failures. To this end, we develop an interpretation toolbox, ViSAE, to trace the concept circuits from ViT representations. These circuits enable users to (i) audit models by identifying spurious shortcuts, and (ii) steer model behaviors by amplifying or suppressing specific concepts along influential paths. Specifically, we construct a neuroscience-motivated probing suite (63K images and 16K concepts) that mirrors the human visual cortex hierarchy. Building upon the data, we train Sparse Autoencoders (SAEs) to read concepts directly from the representations of ViT and trace their causal relationships. Extensive experiments and ablation studies show that our probing suite outperforms existing counterparts by 28.7% in interpretation accuracy. We demonstrate that using ViSAE, we can identify spurious decision paths, localize concepts on pixels, and diagnose the model failure modes. Furthermore, our toolbox enables model steering by editing concepts within representations, which improves worst-group accuracy on the WaterBirds dataset by 48.2%.", "tldr": "", "keywords": ["Interpretable Machine Learning", "Representation Learning", "Sparse Autoencoders"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b94a4d7c79a1672f0d813f021c45a1211abc8e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a intepretability toolbox entitled ViSAE. The toolbox relies on Sparse Autoencoders (SAEs) to create monesemantic concepts that can later be combined with circuit discovery methods to trace decision throughout the network. The contributions of the work are along three axes, data, algorithm, and application. For the data, a new probing dataset is introduced using GPT-5 with the motivation of having more fine-grained concepts in the probing dataset compared to datasets such as Imagenet and MS-COCO. On the algorithm side, vision language models are used to label the concepts in the SAE, and a causal algorithm is used to trace concepts to the input. On the application side, the toolbox is demonstrated for the task of auditing and steering Vision Transfomers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. An extensive benchmarking of different types of SAE for vision data.\n2. Nice figures both giving an overview of the work and showcasing the toolbox.\n3. A clear list of contributions."}, "weaknesses": {"value": "1. The novelty of the contributions is unclear. \n\n(a) Data: A key contribution is the new dataset that provides more fine-grained concepts compared to the more object oriented datasets like ImageNet and MS-COCO. However, the BRODEN dataset [1] that is mentioned already provides more fine-grained concepts specifically designed for this type of analysis. It is unclear why this dataset is not used as a baseline or discussed further. Furthermore, using language models to generate fine-grained concepts is also an established practice, see for example [2]. \n\n(b) Algorithm: The contributions on the algorithm side are connected to the top-down concept reading and the bottom-up causal tracing. The top-down concept reading automatically labels the features of the SAE into labeled concepts, and the bottom-up causal tracing visualizes the concepts in the input space. But both of these algorithms appear to be direct applications of prior works. The top-down concept reading appear to be the CLIP-Dissect procedure [3]. The bottom-up causal tracing also seems to closely follow prior works [4, 5]. It is unclear what the methodological contributions of this work are on the algorithm side.\n\n(c) Application: Both auditing and steering are tasks that are possible to do prior to the introduction of ViSAE [6, 7, 8].\n\n2.The experimental evaluation is limited. Both evaluation methods and baselines are throughout the paper not suitable to demonstrate the potential quality of the introduced toolbox. The probing dataset used as baselines should be replaced with existing fine-grained concept-level probing datasets. The baselines in Table 4 could also be improved. CBM are old, and newer alternatives like Post-hoc CMBs [9] seem more relevant. SpLiCE is a good baseline, but not enough on its own, and comparing to other methods like [6, 7, 8] would strengthen the analysis. Furthermore, the visualization in Figure 5 and 6 are nice, but are only qualitative. Established quantitative measures [10] should be used to evaluated the visualizations.\n\n- [1] Bau et al., Network Dissection: Quantifying Interpretability of Deep Visual Representations, CVPR 2017\n- [2] Yang et al., Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification, CVPR 2023\n- [3] Oikarinen et al., CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks, ICLR 2023.\n- [4] Commy et al., Towards Automated Circuit Discovery for Mechanistic Interpretabiliy, NeurIPS 2023\n- [5] Meng et al., Locating and Editing Factual Associations in GPT, NeurIPS 2022\n- [6] Dreyer et al., Mechanistic understanding and validation of large AI models with SemanticLens, Nature Machine Intelligence 2025\n- [7] Wu et al., Discover and Cure: Concept-aware Mitigation of Spurious Correlation, ICML 2023\n- [8] Joseph et al, Steering CLIP's vision transformer with sparse autoencoders, CVPR Workshop on Mechanistic Interpretability for Vision 2025\n- [9] Yuksekgonul et al., Post-hoc Concept Bottleneck Models, ICLR 2023\n- [10] Hedström et al., Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond, JMLR 2023"}, "questions": {"value": "1. How does the proposed new dataset compare to [1] and [2]?\n2. How does the results look if [1] and [2] are used Table 3 and 5?\n3. What is the difference between the CLIP-Dissect procedure and top-down concept reading?\n4. What is the methodological novelty in the the bottom-up causal tracing?\n5. How does the auditing and steering results look compared to other more recent and relevant baselines?\n6. The visualizations in Figure 6 look good at first glance, but when looking closer they seem inconsistent. In the \"lines\" example, the woman's necklace is highlighted, but the straight black line of her dress is somehow not a \"line\". The \"wooden\" example has parts of a wooden fence highlighted, why not the rest? Similar inconsistencies appear in the other examples. How can we understand these inconsistencies?\n\n- [1] Bau et al., Network Dissection: Quantifying Interpretability of Deep Visual Representations, CVPR 2017\n- [2] Yang et al., Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification, CVPR 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K1v0T14pU1", "forum": "BXzUi2QZ7z", "replyto": "BXzUi2QZ7z", "signatures": ["ICLR.cc/2026/Conference/Submission636/Reviewer_D6zH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission636/Reviewer_D6zH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761334862754, "cdate": 1761334862754, "tmdate": 1762915573109, "mdate": 1762915573109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the interpretability of Vision Transformers (ViTs) and contributes with the following three angles. First, it constructs a dataset from 7 sources and annotates the images with concepts inspired by neuroscience. Second, based on the current literature of Sparse Autoencoders (SAEs), it proposes a reading algorithm to assign an SAE feature a specific concept label based on the (image, concept) probing set, and uses an existing tracing algorithm to build connection graphs between SAE concepts. Third, it conducts experiments to interpret the information flow during ViT decision making and to steer model behavior by editing concepts through SAE interventions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and is easy to read.\n\nThe fine-grained annotation significantly helps ViT interpretability through SAEs.\n\nWith the help of concept annotation, the top-down reading algorithm avoids human labor or imprecise summaries from models for SAE feature label assignment. Further, broader concepts allow for detailed diagnosis of failure modes and bring practical benefits, as reflected in the impressive steering outcomes. Such fine-grained concepts allow for discovering new connection graphs, which brings clear future research benefits."}, "weaknesses": {"value": "The main weakness of the paper I spot is the extent of technical contribution. It seems the improvement mainly comes from the GPT-5 annotation process, which is technically incremental. For example, as shown in both Table 3 and Table 5, the dataset's quality is not that important - with the correct concepts being considered, using MSCOCO alone already achieves comparable results to the carefully curated dataset."}, "questions": {"value": "(1) How are the numbers in Table 1 computed? What is the definition of “Concepts Covered by Images”?\n\n(2) How do you obtain $c_m$ for computing $P_{nm}$? Is it obtained directly through the text description?\n\n(3) In the experiment section for auditing, during the localization of concepts on pixels, how do you pick the layer to perform such attribution? How does this choice affect the heatmap? An ablation on this would be helpful.\n\n(4) How do you launch the experiments in Section 3.2, part (2)? It seems there is a concept set mismatch for the ablation runs because the ground-truth concepts are obtained according to Section 2.2 (from the Ours-16K set) while the SAE concepts are obtained through ablation settings. If this is the case, the numbers reported are not meaningful, and thus the comparison is not valid.\n\n(5) In Section 3.4, the accuracy reported in text (49.6) does not match that in the table (50.3). Why are they different? Also, similar to question (3), which layer do you use to pick the concept for steering? I suppose there are multiple SAE features that have been labeled as exactly the same background concept (because the total number of SAE features is likely exceeding the total number of available concepts). If this is the case, is intervention always effective regardless of the layer index?\n\n(6) For section 3.2/3/4, are your SAE trained on cls token? Does switching to img token make a difference?\n\nMisc: Table 1 seems to be missing SN under data source"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0O7LrKovo9", "forum": "BXzUi2QZ7z", "replyto": "BXzUi2QZ7z", "signatures": ["ICLR.cc/2026/Conference/Submission636/Reviewer_DS6U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission636/Reviewer_DS6U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579173500, "cdate": 1761579173500, "tmdate": 1762915572978, "mdate": 1762915572978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces concept circuits for ViTs: a layer-wise, directed graph where each node is a human-interpretable concept at a specific layer and edges indicate causal influence between concepts as they compose the final prediction. The authors 1) train sparse auto encoders per transformer block, 2) auto label each SAE feature with a text concept using a CLIP-based soft-WPMI alignment over a large, synthetic concept set, 3) perform within-model hard interventions by zeroing a source concept’s SAE activation and decoding, measuring the indirect effect on target concepts at later layers, and 4) create a layer-respecting DAG whose edge weights reflect these measured effects. Empirically, they show faithfulness via targeted ablations and qualitative circuits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles a core gap in ViT interpretability: concept-level, layer-wise circuits.\n- Uses within-model interventions rather than correlational probes.\n- Clean, modular pipeline: per-layer SAEs on the residual stream + automated concept labeling.\n- Produces DAG that attempts to capture compositional flow, providing a computation path that's interpretable.\n- Enables steering by suppressing spurious concepts or amplifying robust ones.\n- Targeted ablations yield consistent logit drops aligned with the inferred circuits.\n- Overall, a good addition to the interpretability arsenal."}, "weaknesses": {"value": "- The 16K concept set is produced by GPT-5, but there’s no systematic human audit.\n- Prompts are given, but generation settings (e.g., temperature/seed/determinism) aren’t specified, so exact reproduction of the concept set isn’t guaranteed. As far as I know, GPT5’s API does not provide a deterministic option at all.\n- Eq 4 clearly implies that this mapping is neither injective nor surjective. This is only briefly acknowledged and not sufficiently analyzed.\n- The do operator is used to define indirect effects via within-model edits, but no explicit SCM/graph is specified.\n- Table 3 shows that “Ours-16k” greatly improves accuracy when the ground truth for the test split is itself GPT-5 concept annotations drawn from their proving suite. This probably privileges their vocabulary vs LAION/Google.\n- The alignment step assumes CLIP’s embedding geometry reflects concept presence. \n\nMinor: The X-ray/MRI analogy is confusing and potentially misleading."}, "questions": {"value": "- What explicit SCM (variables, structural assignments) underlies Eq 5 and 6, and under which assumptions are the reported indirect effects identified? Do you treat the forward computation graph as the causal graph? Please specify.\n- How are interventions kept on-manifold when zeroing SAE features and decoding?\n- How often do multiple SAE features map to the same concept, and how many concepts receive none? What is the impact on edge weights and interpretation accuracy?\n- What sampling settings produced the 16K concept list? Is the list reproducible?\n- How is vocabulary circularity ruled out in 3.2/Table 3?\n- To ensure acyclicity, edges must be $s\\to t$ with $t>s$. Can you state this explicitly?\n- Do steering edits transfer beyond WaterBirds?\n- Did you try not pruning the initial pool?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8OsRRohdIq", "forum": "BXzUi2QZ7z", "replyto": "BXzUi2QZ7z", "signatures": ["ICLR.cc/2026/Conference/Submission636/Reviewer_q9sp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission636/Reviewer_q9sp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846734267, "cdate": 1761846734267, "tmdate": 1762915572863, "mdate": 1762915572863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ViSAE, an interpretation toolbox for auditing and steering ViTs by tracing human-understandable concept circuits within their internal representations. While the presented method addresses an important problem and has many interesting components, the novelty and technical rigor are limited."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addressed the important problem of AI safety by building a framework that integrates multiple components, such as data curation, SAE methods, and use cases.\n- The concept dataset presented in the paper would be a great resource for future research on developing concept-based explanation methods."}, "weaknesses": {"value": "- The technical novelty is limited. There is prior work published in ECCV 2024 (https://arxiv.org/abs/2407.14499) of which core technique largely overlaps with that of this paper. This prior work has not been cited or discussed.\n- The probing image set is limited. I agree that ImageNet is too focused on object-level tasks, and an alternative dataset is required. However, the probing image set of 64K images, while carefully curated and outperforming existing smaller datasets, is still relatively small for training SAEs to capture the full spectrum of visual concepts, especially when compared to the vast datasets (e.g., LAION with billions of images) used for pre-training large vision models like CLIP. Usually, for LLMs, SAEs are trained on huge pre-training datasets.\n- The authors measured monosemanticity by looking into \"whether each basis feature of an SAE consistently activates on images of the same semantics\" I am not sure if this metric is suited for measuring monosemanticity. “Monosemanticity” means that *each feature* in the sparse code represents *only one distinct concept*, even if multiple concepts are present in an image. The current metric might not fully verify if a single feature is truly disentangled from co-occurring concepts within an image, but rather if the *set of images* activating it are similar.\n- The analogy is misleading. I appreciate the authors coming up with an analogy to explain how their work is different from prior work, which I believe is a good practice. However, I think the analogy is misleading in the context of the paper. X-rays and MRIs both provide spatial representations of physical structures, differing in dimensions. In contrast, pixel attribution methods provide spatial saliency maps on the input, whereas concept circuits provide causal graphs of abstract, or even non-localizable *concepts* within a model's latent space. The shift is not merely from \"average\" to \"slice-by-slice\" spatial views, but from input-level spatial attribution to a graph of abstract causal relationships, which fundamentally differs in its nature.\n- The labeling of the two steps in the tracing algorithm as \"Top-down concept reading\" and \"Bottom-up causal tracing\" can be confusing. \"Top-down\" typically implies moving from higher-level to lower-level information, while \"Bottom-up\" means the reverse. In this context, \"concept reading\" involves mapping latent features (mid-level) to human concepts (high-level), which could be seen as an interpretation *of* features. \"Causal tracing\" explicitly builds connections from earlier (lower) layers to later (higher) layers and ultimately the prediction, which is indeed \"bottom-up.\" Clarifying the precise meaning of \"top-down\" in \"concept reading\" or refining the terminology could improve clarity."}, "questions": {"value": "- The process for visualization (e.g., pruning nodes) has not been mentioned. For example, in figure 5, how were the edges and nodes to show determined?\n- GPT-5 was used for automatic concept annotation by GPT-5. How was the accuracy of the process ensured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Vks04m85PP", "forum": "BXzUi2QZ7z", "replyto": "BXzUi2QZ7z", "signatures": ["ICLR.cc/2026/Conference/Submission636/Reviewer_CZUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission636/Reviewer_CZUB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871420443, "cdate": 1761871420443, "tmdate": 1762915572724, "mdate": 1762915572724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}