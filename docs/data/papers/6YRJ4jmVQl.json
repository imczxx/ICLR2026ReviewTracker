{"id": "6YRJ4jmVQl", "number": 2240, "cdate": 1757039731971, "mdate": 1763702218076, "content": {"title": "SIM-CoT: Supervised Implicit Chain-of-Thought", "abstract": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited their adoption.\nWe identify a core latent instability issue when scaling the computational budget of implicit CoT: as the number of reasoning tokens increases, training often becomes unstable and collapses.\nOur analysis shows that this instability arises from latent representations becoming homogeneous and losing semantic diversity, caused by insufficient step-level supervision in current implicit CoT methods.\nTo address this, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring latent states capture distinct and meaningful information.\nThe auxiliary decoder is removed at inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain stability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI by +3.0\\% on LLaMA-3.1 8B.\nIt further surpasses the explicit CoT baseline on GPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the performance gap on larger models like LLaMA-3.1 8B.", "tldr": "", "keywords": ["Chain-of-Thought", "large language model", "math reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e76dc8dd22eb85c18f7abc0ca2d3cd597c2ebcbb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SIM-CoT, a training module that improves implicit Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs). It addresses the performance gap and instability issues found in current implicit CoT methods.\n\nImplicit CoT methods suffer from a \"latent instability\" issue when their computational budget (number of reasoning tokens) is increased. The authors' analysis shows this instability arises because latent representations become homogeneous and lose semantic diversity, which is caused by insufficient step-level supervision. This can cause training to collapse and performance to drop sharply.\n\nDuring training, it uses an auxiliary decoder to align each implicit token with its corresponding explicit reasoning step. This ensures the latent states capture distinct information. The auxiliary decoder is removed during inference, preserving the token efficiency of implicit CoT with no added overhead. The decoder allows for the visualization and diagnosis of each latent reasoning step.\n\nIt provides a systematic analysis of the latent instability issue in implicit CoT approaches, showing it stems from insufficient supervision.\nThe introduction of SIM-CoT, which uses step-level supervision to improve performance and stability with minimal inference overhead.\nExtensive experimental validation showing consistent performance gains across a range of LLMs, including GPT-2 and LLaMA 3 models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary originality lies in the identification and framing of the \"latent instability issue\". While the performance gap between implicit and explicit Chain-of-Thought (CoT) was known, this paper is the first to provide a rigorous, multi-faceted diagnosis of why scaling implicit methods fails. It introduces novel analytical concepts like \"semantic homogenization\" to explain the collapse of the latent space, a significant contribution to understanding the failure modes of these models.\nThe proposed SIM-CoT method is a creative and effective combination of existing architectural components. It leverages an auxiliary decoder—a standard component—in a novel way: as a temporary, training-only tool for applying fine-grained, step-level supervision to otherwise unconstrained latent states. This is a departure from prior work that relied on coarser answer-level or trajectory-level supervision.\nThe work directly addresses and removes a critical bottleneck in implicit CoT research: the inability to effectively scale the computational budget (i.e., the number of latent tokens). By stabilizing the training process, SIM-CoT allows for the use of more latent tokens, which in turn boosts performance where previous methods would fail.\nThe central claims are exceptionally well-supported. The diagnosis in Figure 1 is not based on a single metric but is corroborated through a comprehensive analysis of accuracy, information loss (operator vs. number), geometric properties of the latent space, and qualitative examples of decoded latents.\nThe experimental setup is of high quality. The authors compare against a comprehensive suite of strong baselines, including explicit CoT and state-of-the-art implicit methods like CODI. The evaluation is robust, conducted across multiple model families (GPT-2, LLaMA) and sizes (1B to 8B) on both in-domain and out-of-domain benchmarks to ensure the findings are generalizable.\n\nThe paper follows a clear, logical narrative that is easy to follow. It begins by identifying a problem, provides a detailed diagnosis, proposes a well-motivated solution, and validates it with extensive experiments. The writing is precise and unambiguous.\nThis work marks a significant milestone for implicit reasoning. It is the first training-based approach to show that an implicit CoT method can surpass a strong explicit CoT baseline on a standard benchmark, both in accuracy and token efficiency. This result helps to close a critical performance gap and makes implicit reasoning a more viable and attractive alternative for practical applications.\nSIM-CoT is designed as a \"plug-and-play\" module that is shown to boost the performance of multiple existing implicit methods. This high degree of applicability makes the contribution immediately useful to other researchers. The gains in inference efficiency are substantial and directly address one of the most pressing challenges in deploying large-scale reasoning models"}, "weaknesses": {"value": "1. The method enforces a strict one-to-one mapping between implicit tokens and textual reasoning steps, which may not reflect the variable complexity of real-world reasoning.\nSuggestion: Explore flexible alignment mechanisms, such as allowing a variable number of latent tokens per reasoning step based on its complexity.\n\n2. At inference, the model generates a fixed number of implicit tokens (K), which is inefficient as problems require different reasoning lengths.\nSuggestion: Implement a dynamic stopping mechanism, such as training the model to generate a special \"end-of-thought\" latent token to terminate the reasoning phase adaptively.\n\n3. The paper notes that using larger decoders degrades performance but offers only a speculative explanation.\nSuggestion: Conduct a deeper analysis to explain this counterintuitive result, for example, by measuring gradient flows or using representation analysis to check for \"misalignment\" between the LLM and decoder.\n\n4. The paper highlights the lack of inference overhead but does not discuss the increased computational cost (memory, time) during training due to the auxiliary decoder.\nSuggestion: Quantify the training-time overhead (e.g., increase in parameters, memory usage, and wall-clock time) to provide a complete picture of the method's trade-offs.\n\n5. The evaluation is confined to mathematical reasoning benchmarks. Claims about improving \"complex reasoning\" would be stronger if validated on a broader range of tasks.\nSuggestion: Evaluate the method on other domains, such as commonsense (e.g., StrategyQA) or symbolic reasoning, to demonstrate the generalizability of the approach beyond arithmetic problems."}, "questions": {"value": "This paper introduces SIM-CoT, a novel training module that addresses a key limitation in implicit Chain-of-Thought (CoT) reasoning. The authors identify and provide a rigorous diagnosis of a \"latent instability issue\" that causes performance to collapse when scaling implicit methods. The proposed solution, which uses a training-only auxiliary decoder to apply step-level supervision, is both elegant and effective.\n\nThe paper's strengths are significant. The diagnosis of the problem is original and well-supported by a multi-faceted analysis. The experimental results are strong and comprehensive, demonstrating that SIM-CoT not only boosts the performance of existing state-of-the-art implicit methods across various model scales but can also surpass a strong explicit CoT baseline in both accuracy and token efficiency. The clarity of the presentation, particularly the high-quality figures and diagrams, makes the work easy to understand and appreciate. The contribution is highly significant, offering a practical path toward making efficient reasoning methods more powerful and reliable.\n\nThe paper has a few limitations that should be addressed. The current method relies on a rigid, one-to-one alignment between latent tokens and reasoning steps and uses a fixed number of reasoning steps (K) at inference, which may not be optimal for problems of varying complexity. The authors also do not discuss the training-time overhead introduced by the auxiliary decoder, focusing only on inference efficiency. Finally, the experimental validation is confined to mathematical reasoning tasks, which limits the claims about general \"complex reasoning\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "861pvpTWm8", "forum": "6YRJ4jmVQl", "replyto": "6YRJ4jmVQl", "signatures": ["ICLR.cc/2026/Conference/Submission2240/Reviewer_URA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2240/Reviewer_URA3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760600176902, "cdate": 1760600176902, "tmdate": 1762916158242, "mdate": 1762916158242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIM-CoT that improves implicit reasoning in LLMs. Unlike prior implicit CoT methods such as Coconut and CODI, which only provide coarse supervision at the answer or trajectory level, SIM-CoT introduces fine-grained, step-level supervision. Empirical results show that SIM-CoT is a plug-and-play module that can be seamlessly combined with existing implicit reasoning methods to enhance both accuracy and stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is useful, especially since it functions as a plug-and-play module that can be easily combined with other approaches.\n2. The paper’s presentation is overall clear and well-structured, with a logical and coherent flow of ideas."}, "weaknesses": {"value": "1. At the beginning, the paper claims that one of the reasons for the instability of prior implicit CoT methods is that implicit tokens **collapse into homogeneous latent states**. This part could be better supported experimentally. Although the authors show that their method is indeed more stable than Coconut when the number of latent tokens increases (figure 3), it would be even more convincing if they included additional measurements or visualizations of the heterogeneity of latent tokens (for example, some measure related to the distance of latent tokens) in the main body.\n\n2. While the proposed method introduces more fine-grained step-level supervision compared to prior approaches, this also adds some trouble to training: the model must rely on **preprocessed, structured**, fine-grained training data, which may limit the scalability of the method.\n\n3. The authors mention that the extra decoder model is architecturally identical to the LLM (line 252). This means the two share parameters, or if the decoder is a smaller, separate model?\n\n4. Although the authors report the average number of tokens used during inference, the introduced decoder model also generates a sequence of tokens at each step for supervision. My concern is that the training cost might be higher than Coconut or even SFT-CoT. A more detailed discussion of training overhead would help clarify the efficiency and practicality of the proposed method.\n\n5. Sections 3.1–3.5 are presented clearly but the notations are quite dense. It would be more reader-friendly if some of the notations were also depicted in Figure 2, which would help distinguish the different symbols more easily."}, "questions": {"value": "see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vxdggvOFus", "forum": "6YRJ4jmVQl", "replyto": "6YRJ4jmVQl", "signatures": ["ICLR.cc/2026/Conference/Submission2240/Reviewer_7Zpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2240/Reviewer_7Zpg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632664083, "cdate": 1761632664083, "tmdate": 1762916158055, "mdate": 1762916158055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve existing implicit CoT, such as Coconut and CODI. Basically, the framework is on top of CODI, by distilling the knowledge from COT-SFT (teacher model: decode all explicit tokens) to the student model (implicit cot), i,.e., three loss, teacher loss is to align teacher's answer token to be gt, one is student loss to align the final answer token to gt, one is distillation loss to minimising the discrepancy of hidden states of teacher and student.  In the proposed SIM-COT, the authors introduce the K intermediate reasoning token (process reward) as \"step-level\" supervision to align the intermediate latent states, z_1 to z_K.\n\nIn experiment results, (as the framework is more based on CODI, coconut using a different curriculum learning paradigm),  they evaluate GPT2 and Llama3.2-1B, 3B and 8B in both in-domain and out-of-domain evaluation, showing average improvements between 0.3% to 3.4%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Implicit CoT is a promising direction, but the accuracy still requires to be improved. \n2. It introduce the process supervision to improve  the performance."}, "weaknesses": {"value": "1. For experiment results, SIM-COT is still inferior than COT-SFT, in which the CODI is lower than COT-SFT, even introducing the intermediate supervision.  Even comparing with CODI, the performance boost is not significant, especially in OOD setting, 0.3%, 1.0% and 0.7%, 0.8%. \n2. One of the most distinguished objective is to align the intermediate z to intermediate reasoning steps (tokens), however, this process is not clearly described, and not shown the generalisability to other datasets. For example, \n- only evaluate on math, in which the intermediate reasoning steps are clearly defined and available, how about how datasets without process reasoning process annotation? \n- How is the performance fluctuation, when number of latent tokens in training is different from test (how many implicit tokens set to be, still the pre-defined K)? Table 3 show the performance difference with COCONUT, not CODI. \n- the number of implicit token z_t is fixed, whatever how many the reasoning steps are, so how did you align the z_t with intermediate reasoning steps? any other alternatives, as aligning with intermediate reasoning steps hurts the generalisability (there could be multiple trajectories reach the correct answer)\n- many experiment setups are inspired from CODI, such as In-domain, OOD, Ablation study on different numbers of implicit latents, \n- it is unclear how to visualise the implicit tokens, the top-k decoded token? is it the intermediate result token, or including the operation as well? CODI can only observe some patterns that the intermediate z can decode intermediate result token, what is the advantages after introducing such process supervision?"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KVJXXbZuBo", "forum": "6YRJ4jmVQl", "replyto": "6YRJ4jmVQl", "signatures": ["ICLR.cc/2026/Conference/Submission2240/Reviewer_UZJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2240/Reviewer_UZJG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996484523, "cdate": 1761996484523, "tmdate": 1762916157916, "mdate": 1762916157916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SIM-CoT, a training module that stabilizes implicit CoT reasoning by adding step-level supervision through an auxiliary decoder, preventing latent collapse as reasoning tokens scale. The decoder aligns each implicit token with its explicit reasoning step during training, then is removed at inference, preserving efficiency while boosting accuracy and stability. SIM-CoT improves both in-domain and out-of-domain performance, surpassing explicit CoT and prior implicit methods like Coconut and CODI across GPT-2 and LLaMA models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces step-level supervision that effectively stabilizes implicit CoT training and prevents latent collapse.\n\n2. Achieves higher accuracy and stability than both explicit and prior implicit CoT methods, while keeping inference fully efficient.\n\n3. Provides interpretability by projecting latent tokens onto explicit reasoning steps, enabling per-step visualization and diagnosis."}, "weaknesses": {"value": "Evaluation is mainly on math-style reasoning, leaving generalization to other domains less explored."}, "questions": {"value": "1. How sensitive is SIM-CoT to the choice or quality of explicit reasoning data used for step-level supervision?\n2. Is the auxiliary decoder’s alignment learned jointly with the main model, or does it require separate supervision or tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sBwhHwfSRK", "forum": "6YRJ4jmVQl", "replyto": "6YRJ4jmVQl", "signatures": ["ICLR.cc/2026/Conference/Submission2240/Reviewer_4kCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2240/Reviewer_4kCa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999180014, "cdate": 1761999180014, "tmdate": 1762916156736, "mdate": 1762916156736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}