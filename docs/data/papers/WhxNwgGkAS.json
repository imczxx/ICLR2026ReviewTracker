{"id": "WhxNwgGkAS", "number": 16200, "cdate": 1758261430416, "mdate": 1763732467571, "content": {"title": "Libra: Effective yet Efficient Load Balancing for Large-scale MoE Inference", "abstract": "Distributed inference of large-scale Mixture-of-Experts (MoE) models faces a critical challenge: expert load imbalance. Numerous system-level approaches have been proposed for load balancing, but they either fail to achieve a satisfactory level of balance or introduce new bottlenecks due to the overhead of the load balancing mechanism itself. To this end, we propose Libra, a system that achieves near-optimal load balancing with minimal overhead. Libra adopts sophisticated mechanisms that accurately predict future expert activations and, based on these predictions, systematically perform load balancing. At the same time, it effectively hides the associated overhead by reconstructing the execution flow so that these costs are overlapped with MoE computation. Evaluations with two large-scale state-of-the-art MoE models on 8 H200 GPUs demonstrate that Libra improves throughput by up to 19.2\\%.", "tldr": "", "keywords": ["Mixture-of-Experts", "Model inference", "Expert load imbalance", "Dynamic load balancing"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ef0a4d457e17e639182d1746bd472325a593128.pdf", "supplementary_material": "/attachment/04c542539ed61376290d5712edc914ceeaa6b84b.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenge of expert load imbalance in distributed Mixture-of-Experts (MoE) model inference. The authors propose Libra, a system design that combines accurate prediction of expert activations with sophisticated load balancing algorithms while effectively hiding overhead through careful execution flow reconstruction. The system achieves up to 19.2% throughput improvement on state-of-the-art MoE models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work is well-motivated by demonstrating that recent SoTA MoE models (DeepSeek-V3, Qwen3MoE, GLM-4.5) exhibit intensified load imbalance compared to their predecessors\n\n* The combination of algorithm observation (slow evolution of hidden states) and system design is novel and insightful.\n\n* The experiments cover multiple models, diverse datasets, and demonstrate both static and dynamic workload performance.\n\n* The paper is well-written with effective visualizations that make the technical approach accessible."}, "weaknesses": {"value": "* Despite the overall \"slow evolution of hidden states\", how does the evolution vary acorss different MoE layers? For example, the early MoE layers could result in more significant hidden state dynamics, which might make the proposed method less effective.\n\n* I would love to see how Libra scales to multi-node deployments"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9KjA0rZ8UR", "forum": "WhxNwgGkAS", "replyto": "WhxNwgGkAS", "signatures": ["ICLR.cc/2026/Conference/Submission16200/Reviewer_SZWt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16200/Reviewer_SZWt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709952259, "cdate": 1761709952259, "tmdate": 1762926361243, "mdate": 1762926361243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issue of expert load imbalance during MoE model inference by implementing runtime expert placement based on load prediction.\nIt achieves the effective integration and systematic orchestration of the proposed methods, which are built upon existing methods and offer enhancements.\nExperimental results demonstrate significant end-to-end inference speedup on the evaluated MoE models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This manuscript is well-structured, making it easy to follow.\n2. It effectively integrates the proposed designs in the system implementation, resulting in significant efficiency improvements."}, "weaknesses": {"value": "1. Lack of detailed breakdown analysis regarding the duration of operations and system overhead, including GPU-CPU data transfer overhead as well as the execution times of operations on both the GPU and CPU.\n2. Lack of analysis on various model and system configurations.\n3. The work demonstrates relatively modest innovation, primarily through the integration of established optimization strategies, such as overlapping using local expert computation, load prediction, and expert placement."}, "questions": {"value": "Given that various model configurations (e.g., hidden size, number of experts, top-k activation) and system configurations (e.g., CPU/GPU computational and communication capabilities) influence the duration of specific operations and, consequently, affect the overlap of asynchronous processes, it is essential to provide a detailed breakdown analysis. Such an analysis can elucidate the extent of the optimization space and identify the scenarios in which these optimizations are most beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6ZFunfdFS", "forum": "WhxNwgGkAS", "replyto": "WhxNwgGkAS", "signatures": ["ICLR.cc/2026/Conference/Submission16200/Reviewer_knvi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16200/Reviewer_knvi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984675227, "cdate": 1761984675227, "tmdate": 1762926360618, "mdate": 1762926360618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of load imbalance in mixture of expert large language model (LLM) inference. We propose a system named Libra that achieves efficient load balancing. Libra builds upon the expert replication policy used in Lina, incorporating an improved mechanism for predicting hot experts. Furthermore, we redesign the execution flow to utilize the CPU effectively, minimizing overhead associated with token sharding."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method addresses several drawbacks of Lina. It introduces an improved hot expert predictor and a more efficient expert replication strategy. \n2. Additionally, Libra utilizes the CPU to reduce overhead associated with token sharding operations."}, "weaknesses": {"value": "1. Novelty concern: Utilizing prefetching to identify expert activation in the next layer is not a novel approach. It has been previously used for accelerating Mixture-of-Expert (MoE) inference as described in [1]. In addition, [1] also introduces the use of the CPU to prevent lightweight computations from becoming a bottleneck in GPU processing.\n\n2. Token sharding introduces additional communication overhead, which is not addressed by the authors. This issue could become significant as batch sizes increase.\n\n3. The experiments include only two MoE models, which is limited. Newer MoE models, such as Qwen3, Moonlight-A3B, gpt-oss-120b, and gpt-oss-20b, should be considered.\n\n4. In throughput experiments, the models only execute the prefill stage, whereas the decoding stage is the main time-consuming component. The experiments should focus more on the decoding stage.\n\n5, The experiments are conducted with a setup of 8 batch size and 4k sequence length. Larger batch sizes, as well as shorter and longer sequence lengths, should be tested.\n\n6. The study lacks end-to-end latency experiments that include both prefilling and decoding stages with various inference configurations, such as different batch sizes, input sequence lengths, decoding sequence lengths, and datasets.\n\n7. Lastly, the paper does not provide an analysis of the MoE layer inference time.\n\n[1] Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7XQcEh5Edb", "forum": "WhxNwgGkAS", "replyto": "WhxNwgGkAS", "signatures": ["ICLR.cc/2026/Conference/Submission16200/Reviewer_HMhF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16200/Reviewer_HMhF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985799584, "cdate": 1761985799584, "tmdate": 1762926360126, "mdate": 1762926360126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Libra, a system for efficient and effective load balancing during large-scale Mixture-of-Experts (MoE) inference. It addresses the expert load imbalance problem, where certain experts become bottlenecks due to uneven token distribution across GPUs.\n\nKey Contributions:\n1. Two-Stage Locality-Aware Execution: Splits MoE computation into local and remote phases, enabling overlap of load balancing overhead with computation.\n2. Accurate Expert Prediction: Uses speculative execution based on hidden states to predict expert activations for the next layer, achieving 70–90% accuracy.\n3. Efficient Expert Replication & Token Sharding: Introduces locality-aware expert prefetching and CPU-based token rebalancing, improving throughput by up to 19.2% over strong baselines like Lina.\n4. Strong Experimental Results: Demonstrates consistent performance gains across multiple datasets and MoE models (Qwen3MoE, GLM-4.5), especially under dynamic workloads.\n\nOverall, Libra achieves near-optimal load balance with minimal overhead, making it a practical and scalable solution for MoE inference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem in this paper is clearly defined and strongly motivated. It focuses on the expert load imbalance issue during the inference phase of large-scale MoE models, explicitly pointing out that it introduces the “straggler effect”, severely degrading system throughput.\nThrough experiments (Figure 1), it demonstrates that modern MoE models (e.g., Qwen3MoE, DeepSeek-V3) abandon load-balancing during training in pursuit of expert specialization, exacerbating load imbalance at inference time.\n2. The system is elegantly designed and highly innovative. It proposes Two-Stage Locality-Aware Execution, dividing MoE computation into local (MoElocal) and remote (MoEremote) phases, cleverly hiding the overhead introduced by load balancing. It introduces a hidden-state-based speculative expert-activation prediction mechanism, achieving significantly higher accuracy (70–90% vs. 20–30%) compared to static routing tables used in methods like Lina. It designs two modules—Hierarchical Expert Prefetcher and Adaptive Token Rebalancer—to tackle expert replication and token redistribution, respectively, with clear structure and strong synergy.\n3. Comprehensive experiments and significant improvements. Experiments in this paper are implemented on SGLang, the LLM serving engine with state-of-the-art efficiency, and evaluations are conducted on two large-scale MoE models (Qwen3MoE and GLM-4.5) and multiple datasets. Libra achieves up to 19.2% higher throughput than the strongest baseline (Lina). Under dynamic-load scenarios, Libra demonstrates high stability, whereas methods like Lina exhibit noticeable performance fluctuations (Figure 9). Detailed experimental configurations and reproduction information—including models, datasets, hardware environments, and evaluation metrics—are provided, enhancing reproducibility."}, "weaknesses": {"value": "1. The memory consumption is not examined in this paper. While the expert replication mechanism is introduced in this paper for balanced workload on distributed devices, this would lead to higher memory consumption, since individual experts can also contribute to a certain degree of memory consumption. A detailed breakdown analysis of memory consumption on each rank would be helpful to examine the performance more systematically.\n2. The prediction accuracy in this paper should be better evaluated. In this submission, the predictors are separately trained on different datasets with specialized task domains. While in real LLM serving applications, the task domain may vary across a broad range for different users at different times. Therefore, a more rational approach would be training the predictor on a large-scale dataset with a diverse data distribution and evaluating the prediction accuracy on massive downstream tasks to better evaluate the out-of-distribution (OOD) performance of Libra."}, "questions": {"value": "1. A more detailed and precise explanation of the model configuration should be provided for each model used in this paper. For example, there is no model named Qwen2MoE in HuggingFace.\n2. The definition of the imbalance ratio should be placed in the main text rather than the Appendix for better clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uLcTgfvVf1", "forum": "WhxNwgGkAS", "replyto": "WhxNwgGkAS", "signatures": ["ICLR.cc/2026/Conference/Submission16200/Reviewer_BzVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16200/Reviewer_BzVM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988782940, "cdate": 1761988782940, "tmdate": 1762926359030, "mdate": 1762926359030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}