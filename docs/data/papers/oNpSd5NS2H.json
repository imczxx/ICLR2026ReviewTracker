{"id": "oNpSd5NS2H", "number": 8676, "cdate": 1758094513197, "mdate": 1763673140771, "content": {"title": "Towards Better Generalization in Lifelong Person Re-Identification with Flatness-Aware Learning", "abstract": "Lifelong person re-identification (LReID) requires models to continuously learn from sequentially arriving domains while retaining discriminative power for previously seen identities. A key challenge is to prevent catastrophic forgetting without access to old data, especially under exemplar-free constraints. In this paper, we propose a novel LReID method that unifies selective flatness-aware optimization, dual-model training, and model interpolation. Specifically, we maintain two separate models per task: a {stability model} trained with the distillation loss to retain the prior knowledge, and a {plasticity model} optimized solely for the current domain. To improve the performance of generalization and retention, we selectively apply Sharpness-Aware Minimization (SAM) only to the distillation loss, guiding the stability model toward flat and robust solutions. After task-specific training, these two models are fused through weight-space interpolation, producing a single model that balances stability and adaptability. The resulting model is used to initialize both branches for the next task, enabling continual knowledge integration. Our method is lightweight, modular, and readily compatible with existing LReID frameworks. Extensive experimental results consistently demonstrate that the proposed flat-minima-guided model fusion strategy consistently improves the overall performance of LReID.", "tldr": "", "keywords": ["Person re-identification", "Continual learning", "Flat minima"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7d4ccd327e0c18f9953af672e9d64192bf2a690.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of lifelong person re-identification (LReID), where models sequentially adapt to new domains while avoiding catastrophic forgetting. The proposed approach unifies three main components: (1) selective Sharpness-Aware Minimization (SAM) applied to only the knowledge distillation loss, (2) dual-model training where stability and plasticity branches are optimized independently, and (3) interpolation of these two models’ weights for a fused model that balances retention and adaptability. Experiments and ablation studies on LReID benchmarks demonstrate consistent improvements in generalization and knowledge retention over a range of state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The application of SAM to the distillation branch provides a fresh angle within LReID. The connection to generalization and robustness is clearly supported by the visualizations in Figure 2 and Figure 3.\n2. The proposed method is easily embedded into various existing LReID architectures, as shown by empirical integration with six state-of-the-art baselines."}, "weaknesses": {"value": "1. While selective SAM to the distillation loss is novel in this context, the core ideas, dual-branch training and linear weight interpolation, are not entirely new and can be viewed as straightforward extensions of existing regularization and model fusion paradigms (Exponential Moving Average in DKP, DASK).\n2. The ablation in Table 2 and Table explores various losses for SAM, but does not consider alternative fusion approaches, such as non-linear, confidence-weighted, or meta-learned combinations of the two branches. Given that linear weight interpolation is a central design choice (Section 4.3 and Figure 4), the lack of comparison with more advanced model merging strategies leaves a gap in validating the optimality of their method.\n3. The method’s reliance on dual-model maintenance and per-branch optimization likely increases both memory and computation costs compared to standard single-model baselines, but no discussion or empirical measurement of these costs is provided.\n4. The fixed hyperparameter $\\lambda$ for model fusion is justified with an ablation in the appendix, but the rationale for choosing a universal value across all settings is limited."}, "questions": {"value": "The method requires maintaining two full network. Can you provide measurements or analysis of computational and memory overhead (training time, inference speedup/slowdown, GPU memory footprint) relative to single-model baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PGRhzSQWYn", "forum": "oNpSd5NS2H", "replyto": "oNpSd5NS2H", "signatures": ["ICLR.cc/2026/Conference/Submission8676/Reviewer_3Ukf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8676/Reviewer_3Ukf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761368679484, "cdate": 1761368679484, "tmdate": 1762920491727, "mdate": 1762920491727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the lifelong person reidentification (LReID) task, which has been extensively investigated recently. This paper aims to unify selective flatness-aware optimization, dual-model training, and model interpolation. Promising performance is achieved compared to existing works."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-structured and smoothly written.\n\n2. Promising performance is achieved compared to existing works, verifying the effectiveness of the proposed framework."}, "weaknesses": {"value": "1. Unclear motivation. This paper does not explain the necessity of unifying selective flatness-aware optimization,  dual-model training, and model interpolation.\n\n2. Limited analysis of the relation with the LReID task. This paper introduces an LReID method, while the ReID-relevant loss is not introduced.\n\n3. Limited illustration. This paper does not contain the framework figure containing the main data stream, making it hard for readers to understand some key designs, such as dual-model training and model interpolation.\n\n4. Unfair comparison. The training setting of this paper is different from the previous papers, where an unusual optimizer, ASDM, is adopted. Therefore, it is unclear whether the improvement in this paper is achieved via training setting bias compared to the existing works."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sHsxWwOAZJ", "forum": "oNpSd5NS2H", "replyto": "oNpSd5NS2H", "signatures": ["ICLR.cc/2026/Conference/Submission8676/Reviewer_i8z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8676/Reviewer_i8z3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501698078, "cdate": 1761501698078, "tmdate": 1762920491363, "mdate": 1762920491363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an LReID method that unifies selective flatness-aware optimization, where a stability model trained with distillation loss retains prior knowledge, and a plasticity model optimized solely for the current domain. It further selectively applies Sharpness-Aware Minimization (SAM) only to the distillation loss, guiding the stability model toward flat and robust solutions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The structure is complete.\n2. The experimental results verify the effectiveness of the proposed method to some extent."}, "weaknesses": {"value": "1. The motivation is unclear. I don't understand what the authors mean by \"well-behaved regions of the loss landscape.\" Also, the definition of \"sharp or incompatible solutions\" is confusing. The authors should use the simplest language possible to explain their ideas.\n2. The proposed method is not clear enough. Due to the lack of a diagram to illustrate the method, I am unclear about what the authors did and how the method works.\n3. The lack of visualization experiments makes it difficult to intuitively understand why the proposed method leads to the final performance improvement."}, "questions": {"value": "See the comments below."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GI4SYv0pio", "forum": "oNpSd5NS2H", "replyto": "oNpSd5NS2H", "signatures": ["ICLR.cc/2026/Conference/Submission8676/Reviewer_czw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8676/Reviewer_czw5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894696283, "cdate": 1761894696283, "tmdate": 1762920490947, "mdate": 1762920490947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a novel framework for lifelong person re-identification \n(LReID) task. The framework, which unifies selective flatness-aware optimization, \ndual-model training, and model interpolation, achieves a promising performance and reduces catastrophic forgetting rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper originally generates a framework which can be easily interpolated and used.\n\nThis paper is written smoothly and does not have any long sentences which may lead to understanding difficulty."}, "weaknesses": {"value": "Limited illustration. This paper does not have any figures that illustrate the whole framework, which includes the dual-model training. And this may lead to a misunderstanding about how the framework actually works.\n\nUnmatched result. The results in Table 4 do not align with Table 2, as the experiment for both has the same configuration, while the results are very different. Also, there is a typo in Table 4, Line 3.\n\nLimited formula. This paper provides limited formula derivations. Some formulas, such as the derivation of the selective SAM gradient, have not been given, which may lead to difficulties for readers to replicate.\n\nLack of consistent integration. This paper does not provide enough support for the integration of the main modules, which may make it seem like three modules instead of one framework."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eFI4fx7zY9", "forum": "oNpSd5NS2H", "replyto": "oNpSd5NS2H", "signatures": ["ICLR.cc/2026/Conference/Submission8676/Reviewer_AiGX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8676/Reviewer_AiGX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968424355, "cdate": 1761968424355, "tmdate": 1762920490332, "mdate": 1762920490332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}