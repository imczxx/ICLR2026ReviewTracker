{"id": "A4kEu2A023", "number": 20043, "cdate": 1758301820501, "mdate": 1759897004584, "content": {"title": "A Biconvex Formulation for Transport of Mixture Models", "abstract": "Optimal transport (OT) provides a principled framework for mapping between probability distributions. Despite extensive progress in the field, OT remains computationally demanding, and the resulting transport plans are often difficult to interpret. Here, we propose Optimal Mixture Transport (OMT), an efficient algorithm that leverages mixture modeling and entropic regularization to yield interpretable transport plans. We show that transport between mixtures, in particular mixtures of Gaussians which are universal approximators in $L^2$, can be formulated as a biconvex optimization problem with a unique minimizer. This formulation not only reduces computational cost, but also provides component-level correspondences, offering insights into complex distributions. We demonstrate the practicality and effectiveness of OMT across a diverse collection of synthetic benchmarks and real-world datasets, including large-scale single-cell RNA sequencing measurements.", "tldr": "This paper introduces Optimal Mixture Transport (OMT), a computationally efficient and interpretable method that uses mixture models to find meaningful, component-level mappings between complex probability distributions.", "keywords": ["Optimal transport", "Mixture models", "Biconvex formulation", "Single-cell RNA sequencing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f44efef72d9c44983f35c3dc4634c3a993886ec9.pdf", "supplementary_material": "/attachment/50eb3db3ef5bf5aa27cf8aa67c4b1b870238d5a0.zip"}, "replies": [{"content": {"summary": {"value": "Summary:\nThis paper introduces Optimal Mixture Transport (OMT), an entropic OT framework that transports mixture models (instantiated for GMMs) via a strictly biconvex objective with entropy on both component couplings and mixing weights; the authors prove existence of a unique minimizer and show the GOP procedure reaches it in one iteration. They evaluate on the Korotin W2 benchmark against EOT/ENOT/ExNOT/PROGOT and present single-cell RNA-seq and image translation case studies; for biology, OMT is trained on PCA embeddings rather than raw gene space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strength:\n1. The objective is strictly biconvex for any ε₁,ε₂>0, and the GOP optimization is proven to converge to a unique solution in a single iteration. The method also has theoretical elegance, for GMMs, the optimal mixture transport is itself a GMM, preserving structure and enabling parametric efficiency.\n2. OMT matches or surpasses strong baselines on Dε/MSE while reporting favorable runtime/memory on the Korotin setup.\n3. The method yields component-to-component coupling matrices that expose biologically meaningful correspondences (e.g., OPC→oligodendrocyte maturation trajectories).\n4. Noise ablations show comparatively stable performance across methods."}, "weaknesses": {"value": "Weakness:\n1. This paper lacks the comparison with an amortized optimized W2OT (https://arxiv.org/abs/2210.12153) on the W2OT benchmark. Though these methods are general neural OT, it is still insightful to be included. \n2. The biological evaluation part focuses on evaluation in PCA/latent space, which is generally less informative in biology. Though it is sound in machine learning, you should add evaluation in gene space. Please refer to CellOT (https://www.nature.com/articles/s41592-023-01969-x) and W1OT (https://academic.oup.com/bioinformatics/article/41/Supplement_1/i513/8199349) to add evaluation on the top differentially expressed genes (e.g., DE-gene preservation) and you may also include these two methods as biologist do not really care methodology but care more about biological performance.\n3. Though the whole method looks fine, the writing is not well structured, it takes me a little bit more time to understand the method is not either discrete OT nor common neural OT while the paper introduces a lot about them. The writing may be improved but it is fine if the authors keep current version."}, "questions": {"value": "Please see questions in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J2FQXxOYtw", "forum": "A4kEu2A023", "replyto": "A4kEu2A023", "signatures": ["ICLR.cc/2026/Conference/Submission20043/Reviewer_JQ23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20043/Reviewer_JQ23"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705041162, "cdate": 1761705041162, "tmdate": 1762932939711, "mdate": 1762932939711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Optimal Mixture Transport (OMT), a computational framework for efficient and interpretable optimal transport between probability distributions. The core of OMT is a biconvex optimization problem that leverages mixture models, specifically Gaussian Mixture Models (GMMs), which are universal approximators. The model is trained using a doubly regularized objective: one entropy regularizer acts on the transport couplings between individual mixture components, while another acts on the coupling of the mixture weights themselves. The authors demonstrate the utility of their method on synthetic benchmarks and large-scale real-world applications, including single-cell RNA sequencing data, where it performs competitively with existing methods while requiring substantially less computation and memory, and on image translation tasks, where it achieves competitive performance with generative adversarial networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I think that the core idea behind the paper is quite compelling, namely, studying OT-based mappings on populations. In fields like cellular biology, cells exist and evolve as a coordinated system rather than individually. I believe that a distribution-based approach for learning cellular dynamics is intriguing and holds promise for potential applications to the field of spatial transcriptomics or patient modeling. Moreover, I find it positive that the authors considered multiple biological settings and shared some developmental insight on single-cell datasets. This corroborates the usability of the approach in applied scenarios, going beyond a simple method implementation."}, "weaknesses": {"value": "Unfortunately, in the current state, I am leaning towards a rejection score. Although the model has some positive aspects, I think the contribution and experimental design (especially on the image translation task) fall below the acceptance line for a conference like ICLR. I am happy to discuss more with the authors during the rebuttal phase. \n\n- **General opinion.** If I understand correctly, the main novelty of the paper is to formulate the OMT problem (already existing [1]) in terms of entropy regularization and devise an approach to solve it. The lack of novelty is usually not a sufficient reason for rejection. But I also believe it should be supported by convincing experimental evidence, which I did not find here. If OMT already existed, as I understood, what's the advantage of entropic regularization versus the standard OMT? Is it shown somewhere in the paper? I assume that using the GOP algorithm for optimization is also a central part of the contribution, but in this case, the method already existed.\n\n- Related to one of my questions: I think comparing performance against a continuous Flow Matching or Diffusio Schrödinger Bridge model would reinforce the relevance of the methods. While discrete OT is interesting when analyzing the coupling in the Kantorovivch sense, I feel it may be a bit limited for prediction. \n\n- \n\n- A relevant paper with citation here is [3], where the authors also leverage the closed-form formulation of OT between Gaussians. Can you briefly elaborate on the similarities between [3] and the current paper?\n\n- **Minor.** I recommend defining the entropy $H$ for a broader audience. Of course, not in a formula, it's enough to have one sentence in natural language. Similarly, I would define somewhere $\\phi$ in Eq. 7.\n\n- I think you can enrich the citations for Schrödinger Bridges on page 4 (e.g. [4]). As of now, I see almost no citation thereof.\n\n- I personally would move section 4.2 somewhere before where it is now. I feel it is more coherent with the general description of OT for Gaussian mixtures. Then, I suggest you could make a new section about optimization only. \n\n- **Error bars Fig. 3.** I recommend the addition of error bars to Fig. 3. Since noise addition is inspected, error bars would allow a better understanding of the method's robustness. \n\n- **Fig. 2:** When I look at Fig. 2, the performance of ExNOT stands out compared to the other models. As $>2^7$ is an unlikely feature range, I would probably pick ExNOT in a realistic scenario.\n\n- **Data description and setting.** Although it is a machine learning paper, I feel it would be useful to have a more thorough description of the perturbation prediction task. At the moment, it is not clear what problem is solved (transporting control to perturbed states, I assume). \n\n- **Line 430: typo on *developmetnal*. \n\n- **Image experiments.** For me, the most critical part is the experiments on images, especially on ImageNet. Currently, hundreds of models have been tested on ImageNet and MNIST for translation. Reporting two of them feels a bit selective to me. Moreover, I would recommend presenting Tab. 2 differently, i.e., including error bars everywhere. Based on the results, I don't feel that OMT is coming out very strongly. \n\n- **General remark.** In general, I feel that I miss some solid evidence that the approach is providing something additional than standard OT or image-to-image translation models. The method is not doing much better than others, and the benchmarks are not extremely comprehensive. Maybe you can find some advantage in performing population-based OT worth reporting? \n\n[1] Chen, Yongxin, Tryphon T. Georgiou, and Allen Tannenbaum. \"Optimal transport for Gaussian mixture models.\" IEEE Access 7 (2018): 6269-6278.\n\n[2] Tong, Alexander, et al. \"Improving and generalizing flow-based generative models with minibatch optimal transport.\" arXiv preprint arXiv:2302.00482 (2023).\n\n[3] Bunne, Charlotte, et al. \"The schrödinger bridge between gaussian measures has a closed form.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\n[4] De Bortoli, Valentin, et al. \"Diffusion schrödinger bridge with applications to score-based generative modeling.\" Advances in neural information processing systems 34 (2021): 17695-17709."}, "questions": {"value": "- **Add some Flow Matching references.** Many people are exploring flow-matching-based OT. ** Have you thought of benchmarking against them? Flow Matching works are cited in the paper; have you considered using them for benchmarking, as they are well-established models, for example, in cellular trajectory prediction?\n\n- For the considered problems, the model may be advantageous. But for high-dimensional cases, I feel that using Gaussian mixture models may become too approximate, and the use of another paradigm (like generative OT) could be preferred. Can the authors elaborate on this?\n\n- Why aren't all methods included in Tab. 1 of those listed in Fig. 3? \\\n\n- What is the latent dimensionality for the VAE applied to single-cell data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k62DgVoiOK", "forum": "A4kEu2A023", "replyto": "A4kEu2A023", "signatures": ["ICLR.cc/2026/Conference/Submission20043/Reviewer_A9fm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20043/Reviewer_A9fm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848109869, "cdate": 1761848109869, "tmdate": 1762932939316, "mdate": 1762932939316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Optimal Mixture Transport (OMT), a method to speed up computation of optimal transport maps. The core idea is to approximate source and target distributions with Mixture Models, and compute OT maps between components of these mixtures, rather than expensive sample-to-sample transport. The main theoretical contribution is the entropic formulation of this problem as a strictly biconvex optimization problem that jointly optimizes the mixing weights ($\\Omega$) that define how much mass flows between pairs of components and the entropic transport plans ($P$) between each of those component pairs. The authors prove that this biconvex problem has a unique minimizer and, critically, that it can be solved efficiently in a single alternating iteration. This makes the algorithm efficient, as it essentially reduces to computing a cost matrix where each entry is the closed-form entropic OT cost between two Gaussian components, and then solving a single, small, discrete entropic OT problem on this cost matrix. The authors demonstrate empirically that OMT is faster, less memory-intensive, and achieves competitive or superior performance compared to several sample-based OT solvers"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its  novel theoretical formulation. While GMM-based OT is not new, the authors frame the problem as a doubly-regularized objective (with $\\epsilon_1$ for component plans and $\\epsilon_2$ for mixing weights). Proving that this biconvex problem, which could be optimize as is, can in fact be solved in a single step.  This theoretical contribution translates directly into an efficient and practical algorithm, allowing the solver to effectively compute an \"entropic-OT-on-an-entropic-cost-matrix\" in a single pass. \n\nThe authors obtain compelling empirical results against chosen baselines, showing that OMT provides as significant speed while retaining high-quality transport plans on multiple dataset."}, "weaknesses": {"value": "The paper's primary weakness is its evaluation, which omits comparisons to the most relevant and obvious baselines. The current experiments compare a mixture-based method (OMT) almost exclusively against sample-based methods (EOT, PROGOT, etc.). This makes it impossible to determine if the performance gains come from the paper's novel doubly-regularized biconvex formulation or simply from the GMM approximation itself, which is a common strategy. Key missing comparisons include the standard, well-known GMM-OT solver, which uses the unregularized Bures-Wasserstein distance for its inner cost matrix; without this, the central claim that double regularization is superior is unsubstantiated. Furthermore, comparisons to other clustering-based methods (e.g., k-means centroids), Low-Rank Sinkhorn [1] (which can be thought of a different form of clustering), and modern scalable solvers like Hierarchical Refinement [2] are all necessary to properly contextualize this work.\n\n[1] Low-rank Sinkhorn factorization: https://arxiv.org/abs/2103.04737\n[2] Hierarchical Refinement: Optimal Transport to Infinity and Beyond: https://arxiv.org/abs/2503.03025"}, "questions": {"value": "My main concern is the accuracy and scalability of the GMM approximation itself. The entire algorithm's success hinges on the ability to fit an accurate GMM to the source and target data with a small number of components ($K$). GMMs are known to suffer from the curse of dimensionality. In simple, low-dimensional cases (like 2D synthetic data), a good fit is easy, but in high-dimensional spaces (e.g., scRNA-seq or image embeddings), fitting a GMM is notoriously difficult and may require a large $K$ to capture the data's structure. The OMT algorithm's complexity scales with $O(K_0 K_1)$, which is only fast if $K_0$ and $K_1$ are small. \n\nHow does the performance of OMT (both in accuracy and runtime) degrade as the number of components $K$ must be increased to faithfully represent complex, high-dimensional distributions? How much does the final map's accuracy rely on the quality of the initial GMM fit, and what happens when this fit is poor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P7CFTjVNUG", "forum": "A4kEu2A023", "replyto": "A4kEu2A023", "signatures": ["ICLR.cc/2026/Conference/Submission20043/Reviewer_Dmnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20043/Reviewer_Dmnz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940639856, "cdate": 1761940639856, "tmdate": 1762932938924, "mdate": 1762932938924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The approach offers an alternative method to solving the OT plan through mixture modelling and entropic regularization, which reduces the computational complexity to obtain the transport map. The approach recasts the problem as a biconvex optimization problem with a unique minimizer, and demonstrates speed ups across various synthetic and real-world benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The approach is clearly presented and mathematically rigorous, offering practical value given the importance of efficiently computing OT plans across diverse real-world datasets."}, "weaknesses": {"value": "Although the approach seems to be significantly faster to compute than all of the other examined baselines, it remains unclear whether the computed OT plan is invertible. Additional studies around this with improved benchmarks should be included (e.g., hierarchical refinement, MOP, etc.)."}, "questions": {"value": "- In Fig. 2, the MSEs suggest that OMT isn't particularly invertible on the Korotin et al. benchmarks even for low-dimensional data? Although the MMDs appears low on the simpler target distributions in Fig. 1, this doesn't translate to these more challenging benchmarks? \n- How does the approach compare with hierarchical refinement, another approach to inexpensively computing the OT plan (arXiv preprint arXiv:2503.03025)---both in terms of runtime and invertibility. \n- Including an ML-based baseline like minibatch-OT would also be helpful for contrast. \n- Testing on additional benchmarks would be beneficial, e.g., the MERFISH atlas, and/or ImageNet (to demonstrate scalability of the approach to larger systems, especially given the fast run-time achieved on the smaller systems). \n- The OT cost across baselines would also be helpful to include for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eaUjlKfBgu", "forum": "A4kEu2A023", "replyto": "A4kEu2A023", "signatures": ["ICLR.cc/2026/Conference/Submission20043/Reviewer_8kbG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20043/Reviewer_8kbG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20043/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163458528, "cdate": 1762163458528, "tmdate": 1762932938550, "mdate": 1762932938550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}