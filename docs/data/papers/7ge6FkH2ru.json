{"id": "7ge6FkH2ru", "number": 20426, "cdate": 1758305983270, "mdate": 1759896978178, "content": {"title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs", "abstract": "Recent Multimodal Large Language Models (MLLMs) demonstrate strong high-level visual reasoning on tasks such as visual question answering and image captioning. Yet existing benchmarks largely overlook their ability to capture fine-grained perceptual details. As MLLMs are increasingly deployed in safety and reliability critical settings, perceptual acuity becomes essential. We present HueManity, a scalable automated benchmark for assessing fine-grained visual perception in MLLMs. HueManity comprises 83,850 Ishihara-style images embedding alphanumeric strings, designed to evaluate pattern recognition, a core aspect of visual understanding. Our evaluation of nine state-of-the-art MLLMs uncovers a striking performance deficit: the strongest model achieved only 33.6% accuracy on a simple numeric task and 3% on a harder alphanumeric task, compared to near-ceiling performance from humans (99.38%, 93.25%) and a fine-tuned ResNet-50 (96.5%, 94.5%). These findings expose a critical weakness in MLLMs’ perceptual grounding, one that remains obscured by conventional benchmarks emphasizing high-level semantics.", "tldr": "", "keywords": ["Visual Perception", "Multimodal Large Language Models", "Pattern Recognition", "Robustness"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3f0876c2143d954a2c37508d242da09e1407735.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a synthetic visual text-reading benchmark to study how well MLLMs can perceive alphanumeric patterns in an uncommon visual representation. Specifically, the proposed benchmark represents two-letter texts in an image as a set of circles of one color against a background filled by circles of another color, known as an Ishihara Pattern. The paper shows that several popular MLLMs perform poorly in reading the text from such images, whereas human and ResNet classifiers perform almost perfectly."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. The benchmark is novel and does reflect a striking limitation in MLLMs’ visual perception, which pushes against the misconception that MLLMs’ can outperform humans in all simple visual tasks. The paper also considered several MLLMs, both commercial and open-source, which increases its value as a benchmark for future MLLM development."}, "weaknesses": {"value": "1. The paper misses several related papers that study the ability of MLLMs on perceiving visual details [1, 2, 3, 4], and thus does not properly place its findings in the context of other existing evidence to clarify novelty and relevance.\n\n2. Text recognition datasets (eg, TextVQA) measure the same capability that this paper tries to measure: how well can MLLMs read text in various visual settings. Given that TextVQA contains extensive variations of text and background in real world settings, it can provide a more reliable measure of MLLMs’ overall text reading capability compared to synthetic data which may cause distribution shift. This makes the practical utility of the proposed benchmark a bit unclear: if a model L1 outperforms another model L2 on HueManity, what does it mean for real-world applications? Consider this in comparison to TextVQA that has a clear connection to real-world applications.\n\n3. The paper only considers a single prompt and does not explore the effect of prompt variations on the performance. For example, does including instructions such as “There is a letter on an Ishihara Pattern in the image…” and/or removing the exclusion instructions, help improve the performance? This is important because MLLMs performance is quite sensitive to the prompt.\n\n4. The paper does not explore the causes for the discovered difficulty. The mentioned potential causes in Lines 348-359 are speculations without any evidence. Quantitatively exploring some of these speculative causes can strengthen the paper’s contributions.\n\n5. The results of MLLM fine-tuning seems to contradict the ResNet training performance. The paper does point out that it is very surprising that fine-tuning MLLMs on the task does not result in improvements, but does not explore this surprising observation further. For example, is this because the LLM is also finetuned instead of just its vision encoder? Is this because of bad choices of hyperparameters when fine-tuning? There are many missing details here that make the results unreliable. It is also unclear whether this is just a problem with Gemma, or the same applies to other MLLMs.\n\n6. Providing quantitative results for the “MLLM Failure Patterns” could substantiate the claims in lines-425-439.\n\n7. In Tables 2-5, Wilson intervals seem incorrect since they should not be symmetric and fall outside of [0,1]. Reporting the actual confidence interval bounds instead of +- will clarify this.\n\n[1] MLLMs Know Where to Look: Training-Free Perception of Small Visual Details with Multimodal LLMs. ICLR 2025.\n\n[2] Understanding Depth and Height Perception in Large Visual-Language Models. CVPR 2025.\n\n[3] V*: Guided Visual Search as A Core Mechanism in Multimodal LLMs. CVPR 2024.\n\n[4] Exploring Perceptual Limitation of Multimodal Large Language Models. 2024."}, "questions": {"value": "1. Can the authors explain/clarify why fine-tuning the MLLM does not improve its performance? This seems contradictory to a lot of prior research and the fine-tuning results of ResNest. Also, does this happen to other MLLMs besides Gemma?\n\n2. Can different prompts (eg, explicitly mentioning the Ishihara Pattern) change the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8oaGaPrMHS", "forum": "7ge6FkH2ru", "replyto": "7ge6FkH2ru", "signatures": ["ICLR.cc/2026/Conference/Submission20426/Reviewer_zQip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20426/Reviewer_zQip"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878659254, "cdate": 1761878659254, "tmdate": 1762933869884, "mdate": 1762933869884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes HueManity, a new benchmark designed to evaluate the fine-grained visual perception capabilities of multimodal large language models (MLLMs). The benchmark contains 83,850 samples, each consisting of a \"blind-test\" image (similar to Ishihara plates) and a corresponding question that requires the model to recognize embedded letters or numbers. This task is highly accessible for humans, achieving over 99% accuracy, and is also easily handled by lightweight models such as fine-tuned ResNet-50. In contrast, state-of-the-art MLLMs struggle significantly on this task, revealing a notable gap in their ability to perform precise, low-level visual perception—even for seemingly simple recognition tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper identifies a critical deficiency in modern MLLMs: their surprisingly weak performance in fine-grained visual perception, despite strong performance on higher-level vision-language tasks.\n\n* The proposed benchmark, HueManity, is well-designed and presents a valuable resource for the community. It can be widely used in future work to evaluate and diagnose the fine-grained visual understanding capabilities of MLLMs.\n\n* The authors conduct comprehensive experiments demonstrating that even state-of-the-art MLLMs struggle significantly on this task, highlighting the challenge of achieving robust, low-level perceptual accuracy in current multimodal models."}, "weaknesses": {"value": "* This work focuses on a single aspect of visual understanding—recognizing characters in color-patterned images—which is relatively narrow compared to existing MLLM benchmarks. Modern benchmarks typically evaluate multiple capabilities, including low-level perception, high-level reasoning, OCR, and knowledge integration. While this task presents a challenging variant of OCR, the scope of the benchmark is limited in covering the broader spectrum of multimodal understanding expected from MLLMs.\n\n* The benchmark appears to have high redundancy. Given the simple and repetitive nature of the task—overlaying letters and numbers on textured or colorful backgrounds—it is questionable whether 83,850 samples are necessary to reliably evaluate current MLLMs. A much smaller set (e.g., a few thousand examples) might suffice for stable assessment, especially considering that the variation is primarily in color and noise patterns rather than semantic complexity.\n\n* While framed as an MLLM capability, the core task primarily tests the vision encoder’s ability to extract fine-grained visual features under visual noise. The language component is minimal (simple recognition questions), suggesting that the bottleneck lies in the visual encoder rather than the multimodal reasoning or language generation pipeline. Therefore, the focus may be better positioned as evaluating the fine-grained perception capabilities of vision encoders, rather than MLLMs as a whole."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NwFG9zPC78", "forum": "7ge6FkH2ru", "replyto": "7ge6FkH2ru", "signatures": ["ICLR.cc/2026/Conference/Submission20426/Reviewer_Liuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20426/Reviewer_Liuz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898912666, "cdate": 1761898912666, "tmdate": 1762933868795, "mdate": 1762933868795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HueManity, a new benchmark for assessing fine-grained visual perception in Multimodal Large Language Models (MLLMs). It contains 83,850 Ishihara-style images designed to evaluate the fine-grained perceptual ability of MLLMs. The benchmark embeds alphanumeric characters within colored dot patterns to test whether models can recognize subtle visual patterns. The evaluation reveals that even top-performing models such as GPT-4.1, Claude 3.7 Sonnet, Qwen-VL Max, LLaVA-v1.6, and Pixtral perform poorly compared to human participants and a fine-tuned ResNet-50 baseline. The authors claim that these results expose a critical weakness in the perceptual grounding of MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is well written and clearly structured, making it easy to follow.\n2.\tIt evaluates several state-of-the-art MLLMs, including GPT-4.1, Claude 3.7 Sonnet, Qwen-VL Max, LLaVA-v1.6, and Pixtral, across two tasks: the Number Recognition Task and the Alphanumeric Recognition Task.\n3.\tThe work provides a comparative analysis with existing MLLM benchmarks. However, some key benchmarks (e.g., MMVP [1],  MERLIM [2] and MME [3]) are missing from the evaluation.\n\n[1] Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs (Tong et al., CVPR 2024) \n[2] MERLIM: Multi Modal Evaluation Benchmark for IT LVLMs (Villa et al., CVPRW 2025) \n[3] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models (Fu et al., 2023)."}, "weaknesses": {"value": "1.\tThe paper mainly reports a failure case of existing models but offers no new theoretical insights. Prior work such as Eyes Wide Shut [1] and MERLIM [2] has already shown that the visual backbones of MLLMs fail to capture fine-grained visual details.\n2.\tHueManity measures only color-based figure–ground discrimination under a single visual structure (Ishihara-style dots). While the idea is well motivated, it represents only a narrow and somewhat artificial subset of visual examples for evaluating  fine-grained visual perception. Other state-of-the-art benchmarks (e.g., MMVP [1], MERLIM [2], MME [3] with its OCR tasks) address this challenge from a more realistic perspective.\n3.\tThe LoRA fine-tuning on Gemma-3-4B using only 500 samples and 3 epochs is too limited to support the claim that the issue is unlearnable. The results likely stem from optimization instability (overfitting), data non-representativeness, or implementation issues rather than a fundamental perceptual incapacity.\n4.\tAlthough the paper lists three contributions, the third one overlaps with and is effectively part of the first."}, "questions": {"value": "1.\tYou claim that fine-grained perception is crucial for MLLMs, but is recognizing Ishihara-style digits truly representative of real-world perceptual challenges? What new insights does this benchmark provide beyond state-of-the-art alternatives such as MMVP [1], MERLIM [2], or MME [3] (which already include OCR-style tasks)?\n2.\tHow did you ensure that the alphanumeric strings are balanced across color pairs and not biased by particular hues or contrast levels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mPzjCrQMCz", "forum": "7ge6FkH2ru", "replyto": "7ge6FkH2ru", "signatures": ["ICLR.cc/2026/Conference/Submission20426/Reviewer_GXxi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20426/Reviewer_GXxi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904280888, "cdate": 1761904280888, "tmdate": 1762933867926, "mdate": 1762933867926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uncovers a striking failure mode: mainstream MLLMs struggle to classify Ishihara-style images—even though they excel on general vision benchmarks—whereas humans and a fine-tuned ResNet-50 perform almost flawlessly. To quantify and mitigate this gap, the authors automatically generate a large-scale Ishihara-style dataset (83 850 images) and benchmark leading MLLMs. They hope the corpus will serve as a safety probe and spur progress on “out-of-distribution” visual reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper reveals an intriguing blind spot of current MLLMs and highlights a new axis for robustness research; the observation may catalyze broader studies on rare or specially structured imagery."}, "weaknesses": {"value": "This paper does not investigate whether lightweight fine-tuning (rather than mere in-context learning) can already lift MLLM accuracy to near-human levels. If the deficit can be erased with a few gradient steps, the issue—and the accompanying dataset—may merit only limited attention."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OwV5KnGAi5", "forum": "7ge6FkH2ru", "replyto": "7ge6FkH2ru", "signatures": ["ICLR.cc/2026/Conference/Submission20426/Reviewer_nA9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20426/Reviewer_nA9E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917117575, "cdate": 1761917117575, "tmdate": 1762933867199, "mdate": 1762933867199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}