{"id": "eqvlxO1sKT", "number": 4499, "cdate": 1757690328240, "mdate": 1759898029654, "content": {"title": "Mixture-of-Transformers Learn Faster: A Theoretical Study on Classification Problems", "abstract": "Mixture-of-Experts (MoE) models improve transformer efficiency but lack a unified theoretical explanation—especially when both feed-forward and attention layers are allowed to specialize. To this end, we study the Mixture-of-Transformers (MoT), a tractable theoretical framework in which each transformer block acts as an expert governed by a continuously trained gating network. This design allows us to isolate and study the core learning dynamics of expert specialization and attention alignment. In particular, we develop a three-stage training algorithm with continuous training of the gating network, and show that each transformer expert specializes in a distinct class of tasks and that the gating network accurately routes data samples to the correct expert. Our analysis shows how expert specialization reduces gradient conflicts and makes each subtask strongly convex. We prove that the training drives the expected prediction loss to near zero in $\\mathcal{O}(\\log(\\epsilon^{-1}))$ iteration steps, significantly improving over the $\\mathcal{O}(\\epsilon^{-1})$ rate for a single transformer. We further validate our theoretical findings through extensive real-data experiments, demonstrating the practical effectiveness of MoT. Together, these results offer the first unified theoretical account of transformer-level specialization and learning dynamics, providing practical guidance for designing efficient large-scale models.", "tldr": "", "keywords": ["Mixture-of-Transformers", "Learning Dynamics", "Theoretical Analysis", "Mixture of Classification"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2b4e299c4eda632ffa8aa896c3ff0b2cc47407b.pdf", "supplementary_material": "/attachment/c870e949b2a41b60ed35b242ec87559fbd285f66.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides the first theoretical convergence and generalization analysis of the Mixture-of-Transformers (MoT) architecture, where, unlike the canonical Mixture-of-Experts (MoE), each expert contains a separate dedicated attention block, i.e., instead of being an FFN, each expert is a transformer block (attention head followed by an FFN). Based on a single MoT layer and assumptions on the input data, the paper theoretically shows that MoT converges faster than a two-headed one-layer transformer model, and achieves better performance than an attention-absent MoE model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The first theoretical training dynamic analysis of the MoT architecture, providing insights into the different phases of training of the architecture.\n\n2. The paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. **Lack of novelty:** The theoretical analysis closely follows the setup in [1], e.g., the data model and the network model are very similar. Therefore, the characterization of training dynamics over multiple training phases of MoE (e.g., router exploration phase, expert specialization phase, etc.) does not provide any significant new insights. The only contribution can be the characterization of the role of the attention layer in the MoE transformer block.\n\n2. **Inappropriate baselines:** The paper compares the proposed architecture with attention-absent MoE and the two-headed transformer. However, usually an MoE transformer block includes a shared attention (or multi-head attention) layer followed by FFN experts. Moreover, the theoretical advantage of MoE over dense models has already been established in [1]. Therefore, the paper should demonstrate the theoretical advantage of MoT over shared attention followed by FFN expert based setup, and also should verify this in experiments.\n\n3. **Unfair comparison:** The comparison with the two baselines (two-headed transformer and attention-absent MoE) is unfair. In the vanilla transformer case, there are only two attention heads, whereas the number of expert transformer blocks in the proposed architecture is large ($\\Omega(NlogN)$). It has not been theoretically established whether the advantage of MoT remains when the number of attention heads in the vanilla transformer increases from two. For the case of attention-absent MoE, the number of neurons in each FFN expert is $1$ (i.e., $W^{(i)}=\\mathbb{R}^{d\\times1}$). It has not been theoretically established whether the advantage of MoT remains when the number of neurons in the attention-absent MoE increases from one.\n\n[1] Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. Advances in neural information processing systems, 35: 23049–23062, 2022"}, "questions": {"value": "In stage I, the authors claim that the experts become specialized. However, in this stage, the routers are still exploring. Can you elaborate, why the experts become specialized despite the exploration of the routers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4BxGGEG4a0", "forum": "eqvlxO1sKT", "replyto": "eqvlxO1sKT", "signatures": ["ICLR.cc/2026/Conference/Submission4499/Reviewer_dCo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4499/Reviewer_dCo3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764729075, "cdate": 1761764729075, "tmdate": 1762917403105, "mdate": 1762917403105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a theoretical framework for analyzing \"Mixture-of-Transformers\" (MoT), an architecture where each expert is a full transformer block (both self-attention and FFN) rather than just an FFN layer, as in traditional MoE. The paper's primary contribution is a theoretical analysis of MoT's convergence properties on a synthesized N-mixture classification task. To facilitate this analysis, the authors introduce a specific three-stage training algorithm that involves sequentially freezing and training the FFN and attention components. Under this setup, they prove that MoT achieves an $\\mathcal{O}(\\log(\\epsilon^{-1}))$ convergence rate to an $\\epsilon$-accurate solution. This is exponentially faster than the $\\mathcal{O}(\\epsilon^{-1})$ rate established for standard (non-gated) transformers on similar mixture problems. The authors argue this speed-up stems from expert specialization, which decomposes the complex, non-convex problem into a set of simpler, strongly convex sub-problems. They provide experiments on modified CIFAR and NLP datasets to validate these findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clean theoretical result (in isolation):** The $\\mathcal{O}(\\log(\\epsilon^{-1}))$ vs. $\\mathcal{O}(\\epsilon^{-1})$ contrast is stark and theoretically elegant. Proving that the attention-absent MoE has a fundamental error floor (Lemma 1) is a good theoretical contribution, highlighting why attention specialization is necessary in this problem setup.\n\n- **clarity:** The paper is well-organized and clearly written. It systematically presents the model, the training algorithm, and the theoretical analysis, making the core ideas accessible.\n- **Insightful analysis:** The analysis of the three-stage training process provides valuable insights into the distinct roles of the FFN and attention layers in specialization. Proposition 1 (FFN specialization) and Proposition 2 (attention training) build a clear narrative for how the model learns."}, "weaknesses": {"value": "- **(Maybe) overly simplified theoretical model:** As detailed under \"Soundness,\" the theoretical analysis rests on a foundation of strong assumptions (orthogonal data, single-layer model, merged matrices) that may not hold in practice. This severely limits the direct applicability of the findings to the deep, complex transformers used in the real world. The theory does not account for the interactions between multiple layers of experts.\n\n- **lack of realistic empirical validation:** The experiments are conducted on artificially modified datasets to fit the theory. The absence of results on standard, unmodified benchmarks is a major weakness and makes it impossible to judge the practical utility of the model and the theory. Furthermore, the paper does not compare its staged training algorithm against standard end-to-end training, which is a critical missing baseline."}, "questions": {"value": "- Could you please provide the experimental results (error curves analogous to Figure 2) on the unmodified, standard CIFAR-10 and CIFAR-100 datasets?\n\n- Have you performed experiments training the MoT model end-to-end, rather than with the proposed three-stage algorithm? How do the convergence speed and final performance compare?\n\n- The theoretical analysis relies heavily on assumptions like data orthogonality and a single-layer architecture. Could you elaborate on the potential challenges in extending your analysis to settings with correlated features and deep, multi-layer MoT models? Which of your assumptions do you believe are most critical to the final result, and which might be relaxed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tIiTaGyRgX", "forum": "eqvlxO1sKT", "replyto": "eqvlxO1sKT", "signatures": ["ICLR.cc/2026/Conference/Submission4499/Reviewer_Ru49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4499/Reviewer_Ru49"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836472553, "cdate": 1761836472553, "tmdate": 1762917402824, "mdate": 1762917402824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first unified theoretical framework for Mixture-of-Transformers (MoT), where each transformer block acts as a full expert with its own attention and feed-forward layers, governed by a continuously trained gating network. The authors develop a three-stage training algorithm and prove that MoT achieves near-zero prediction loss in O(log⁡(ϵ^{−1})) steps—substantially faster than the O(ϵ^{−1}) rate for standard transformers. Experiments on image classification benchmarks validate the theoretical findings, showing that MoT outperforms both multi-head transformers and attention-absent MoE models, especially on complex tasks. The work provides practical guidance for designing efficient large-scale models and highlights the critical role of attention specialization in expert architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Provides the first rigorous analysis of full-transformer specialization, bridging a major gap in the MoE literature. The three-stage training procedure is interesting and isolates the roles of FFN and attention specialization.  \n(2) Proves faster convergence rates for MoT compared to standard baselines, supported by detailed proofs.   \n(3) Experiments on CIFAR-10/100 datasets robustly support the theoretical claims, demonstrating practical impact."}, "weaknesses": {"value": "(1) For MoE, we typically have a one-stage training that learn all the model parameters (Attn, MLP, Gating) altogether. This paper proposed 3-stage training, which will complicate the training procedure. It might be better to discuss some ablation analysis, such as how the analysis will change for different stages of training, e.g., 2-stage or one single stage.   \n\n(2) While experiments look solid, they focus on relatively small-scale model (e.g., lightweight Vision Transformer) and benchmarks. Results on larger model and larger datasets would strengthen the claims, such as ImageNet. Also, it might be better to include some popular language benchmarks for transformer-based language models, such as GLUE, MMLU, HellaSwag, PIQA, etc.   \n\n(3) According to some studies on Mixture-of-Experts (MoE) models (see [1-4] from the reference list below), different token routing strategies can play an important role in the model performance. I am wondering how token routing strategies will affect the proposed method and its analysis. It might be better to include some discussions about it.    \n\n**Reference:**     \n[1] Nguyen, et al. \"Statistical Advantages of Perturbing Cosine Router in Sparse Mixture of Experts.\" arXiv preprint arXiv:2405.14131 (2024).   \n[2] Liu, et al. \"Gating dropout: Communication-efficient regularization for sparsely activated transformers.\" International Conference on Machine Learning. PMLR, 2022.   \n[3] Zuo, et al. \"Taming sparsely activated transformer with stochastic experts.\" arXiv preprint arXiv:2110.04260 (2021).  \n[4] Lewis, et al. \"Base layers: Simplifying training of large, sparse models.\" International Conference on Machine Learning. PMLR, 2021."}, "questions": {"value": "See my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0oOCPck1zk", "forum": "eqvlxO1sKT", "replyto": "eqvlxO1sKT", "signatures": ["ICLR.cc/2026/Conference/Submission4499/Reviewer_dwgC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4499/Reviewer_dwgC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874025345, "cdate": 1761874025345, "tmdate": 1762917402603, "mdate": 1762917402603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoT, treating an entire Transformer block as a single expert. The primary goal is to establish a theoretical foundation for the MoT architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. To the best of my knowledge, the theoretical foundation of MoT is new.  \n2. The paper is clear and easy to follow."}, "weaknesses": {"value": "1. While I appreciate the theoretical challenges involved, the proposed model appears somewhat simplified. More importantly, the authors do not sufficiently address the gap between theory and practice. For instance, do the optimal choices of \\( T_1 \\) and \\( T_2 \\) suggested by Propositions 1 and 2 actually translate into practical improvements? Furthermore, Figure 2 does not demonstrate the expected linear convergence rate—performance remains comparable to baselines that only enjoy sublinear theoretical convergence. The paper would benefit from a deeper discussion of why this discrepancy occurs and what strategies could help bridge the theory–practice gap.\n\n2. The experimental evaluation is relatively limited. At a minimum, the authors should include comparisons against standard MoE architectures. Additionally, to strengthen the empirical validation and better support the theoretical claims, experiments with larger-scale models—such as LLMs or MLLMs—and a broader range of tasks (e.g., vision, language, and multimodal benchmarks) would significantly enhance the paper’s impact and credibility.\n\n3. It appears that the MoT architecture currently supports only sequence-level routing. This design choice may lead to suboptimal load balancing across experts and could limit computational efficiency, especially in settings where token-level sparsity is beneficial. The authors should discuss the implications of this limitation and consider whether finer-grained routing mechanisms could be incorporated in future work."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LwuEMs0LPT", "forum": "eqvlxO1sKT", "replyto": "eqvlxO1sKT", "signatures": ["ICLR.cc/2026/Conference/Submission4499/Reviewer_oJET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4499/Reviewer_oJET"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903322185, "cdate": 1761903322185, "tmdate": 1762917402320, "mdate": 1762917402320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}