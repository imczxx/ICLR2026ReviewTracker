{"id": "EQuoft7Yc7", "number": 12126, "cdate": 1758205800735, "mdate": 1763647233714, "content": {"title": "Solving the Traveling Salesman Problem with Positional Encoding", "abstract": "We propose transformer-based neural solvers for the Euclidean Traveling Salesman Problem (TSP) that rely on positional encodings rather than coordinate projections. By adapting ALiBi and RoPE, modern positional encodings originally developed for large language models, to the Euclidean setting, our **Positional Encoding-based Neural Solvers (PENS)** inherit useful invariances and locality biases. To address the increased density of large instances, we introduce a simple yet effective rescaling of city coordinates that further boosts performance. Trained only on TSP-100, PENS achieves **state-of-the-art results on up to 10 000 cities**, a scale that was previously dominated by methods requiring graph sparsification. These findings demonstrate that positional encodings provide effective inductive biases for neural combinatorial optimization.", "tldr": "Positional encodings from language models provide powerful biases for neural TSP solvers, enabling state-of-the-art results up to 10 000 cities.", "keywords": ["Neural Combinatorial Optimization", "Traveling Salesman Problem", "Positional Encoding"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b8e763313fb2db613f521ad57532c60049dba5e.pdf", "supplementary_material": "/attachment/0b1cf1445389b5f2b5b8b9887b383449565138b6.zip"}, "replies": [{"content": {"summary": {"value": "This manuscript proposes Positional Encoding-based Neural Solvers (PENS) for addressing cross-scale instances of the TSP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This manuscript is easy to follow."}, "weaknesses": {"value": "**W1 Limited innovation:** The proposed PENS reads as a combination of several existing ideas rather than a clearly novel contribution. Prior studies [1,2] have already incorporated distance information between nodes into attention computations. In addition, an order embedding strategy has been adopted by the iteration-based NCO solver [3]. Finally, the scaling strategy has also been explored [4].\n\n**W2 Heavy dependence on tuning the scaling factor:**  The scaling factor appears to require extensive preliminary tuning for instances of each scale to obtain good performance, which undermines the method’s practicality in real-world settings.\n\n**W3 Insufficient experiments:**  Experiments are limited to TSP instances only. To demonstrate cross-task generalization, the authors should evaluate PENS on other routing problems, such as CVRP. \n\n\n[1] Distance-aware attention reshaping for enhancing generalization of neural solvers. TNNLS, 2025.\n\n[2] Instance-Conditioned Adaptation for Large-scale Generalization of Neural Routing Solver. arxiv, 2024.\n\n[3] Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer. NeurIPS, 2021.\n\n[4] Improving generalization of neural vehicle routing problem solvers through the lens of model architecture. Neural Networks, 2025."}, "questions": {"value": "PENS selects random origin and destination nodes. Is there a specific selection pattern that yields better performance than random selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3MwiY11qVk", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Reviewer_TNia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Reviewer_TNia"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760755606928, "cdate": 1760755606928, "tmdate": 1762923090807, "mdate": 1762923090807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On the Novelty of PENS"}, "comment": {"value": "We thank the reviewers for their thoughtful feedback on this point. We wish to clarify our novel contribution, which is not the invention of the components (ALiBi, RoPE) but their **novel and highly effective application to solve a key challenge in neural combinatorial optimization**.\n\nOur novelty is threefold:\n- **A new input paradigm**: Our work is the *first to demonstrate* that modern positional encodings (PEs) can serve as the *_sole_ spatial input* for a neural TSP solver. While prior works have added distance-based biases [1], they still rely on projecting raw coordinates and crafting additional features. We show that _replacing_ coordinate projections entirely with PEs is not only viable but superior. This is a non-trivial result.\n- **SotA on large-scale without sparsification**: Our most significant finding is that this simple approach **achieves state-of-the-art results on large-scale instances**, an area previously dominated by methods requiring complex hand-crafted graph sparsification. On TSP-10,000, we reach 5.46% against the previous 7.05% previous SotA result. This directly challenges the prevailing assumption that sparsification is necessary for large-scale generalization.\n- **Bridging NCO and NLP**: Our paper deliberately anchors this NCO solution in the established NLP literature. This connection is significant because it shows that key NCO challenges (like problem-size generalization) have direct parallels in NLP (length extrapolation).\n  - For example, we adopted Scalable-softmax (SSMax) [2] from NLP, which we noted was concurrently developed as ESF [3] in the NCO literature.\n  - This demonstrates that NCO researchers can directly benefit from advances in the NLP domain rather than re-discovering parallel solutions.\n\nWe believe our idea of adapting NLP PEs to spatial positions encountered in NCO is original and provides insightful results about the field in general. Our simple approach challenges the complex SotA methods.\n\n\n[1] Distance-aware attention reshaping for enhancing generalization of neural solvers. TNNLS, 2025.\n\n[2] Scalable-softmax is superior for attention, 2025.\n\n[3] Improving generalization of neural vehicle routing problem solvers through the lens of model architecture. Neural Networks, 2025."}}, "id": "kvtUHKGxKz", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763646408367, "cdate": 1763646408367, "tmdate": 1763646408367, "mdate": 1763646408367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the application and analysis of commonly used positional encoding techniques in NLP, such as ALiBi and RoPE, within the field of neural combinatorial optimization (NCO). The authors assert that by adapting these techniques to the Euclidean Traveling Salesman Problem (TSP), improvements are made in solving the problem. The proposed Positional Encoding-based Neural Solvers (PENS) achieve state-of-the-art results by leveraging these NLP techniques, particularly when scaling to large problem sizes such as TSP instances with up to 10,000 cities, without relying on graph sparsification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is innovative in applying NLP techniques to NCO, especially in combinatorial optimization problems like TSP. It’s a fresh perspective that could have broad applications.\n2. The paper is generally well-written, and the experiments are presented in a structured manner. The figures, including the performance evaluations, are informative and contribute to understanding the results."}, "weaknesses": {"value": "1. The focus on TSP is fine, but the approach should be tested on other combinatorial problems (like CVRP or FJSP) to see how generalizable it is.\n2. The paper mainly adapts NLP techniques without making changes that would specifically suit NCO. This limits the paper's innovation.\n3. The paper shows that increasing the scaling factor improves results, but selecting the right scaling factor still seems trial-and-error. \n4. The methodology relies on an autoregressive approach, similar to NLP tasks, but the TSP's solution space has inherent differences from NLP, such as its cyclical nature. The direct transfer of sequence modeling techniques to this context may not always be suitable, and the authors should address how their approach adapts (or struggles) with combinatorial optimization problems that don't naturally fit the NLP paradigm."}, "questions": {"value": "1. Are ALiBi and RoPE used in both the encoder and decoder?\n2. In Table 1, PENS is much faster than BQ-NCO (similar architecture) on TSP-10,000. Can the authors explain why, particularly with PENS-R?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0LpzFq1qeg", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Reviewer_pGYj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Reviewer_pGYj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761198799931, "cdate": 1761198799931, "tmdate": 1762923090516, "mdate": 1762923090516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "On the Scaling Factor and Large-Scale Generalization"}, "comment": {"value": "A truly scale-invariant solver that requires no adjustment remains the ultimate goal for NCO, and we view this as important future work. However, rescaling the coordinates is more than a heuristic, it is a finding that **mitigates a fundamental bottleneck** in current neural solvers.\n\nThe INViT paper [1] identified two key issues in large-scale generalization:\n- _Interference from irrelevant nodes._\n- _Embedding aliasing._\n\nThe scaling factor is both a solution and an experimental measure:\n- **Targeting aliasing**: By spreading the city coordinates, we effectively lower the density and make it easier for the model to distinguish between close neighbors.\n- **Significance**: This simple transformation **divides by two the optimal gap** on TSP-10,000, offering a strong estimation of how much performance is currently hidden by these density-related mismatches.\n- **Synergy with PENS-A**: This technique uniquely complements PENS-A (ALiBi) because the increased relative distances further amplify ALiBi's soft locality bias, helping the solver focus on the relevant neighborhood. This phenomenon is illustrated in Figure 6 of the appendix, where PENS-A is the only model that remains consistent under multiple scaling factor values.\n\nMoreover, the fact that the **exact same model weights** can successfully solve TSP-10,000 instances simply by rescaling the input coordinates demonstrates that:\n- **The reasoning capacity exists**: The transformer has the computational power and algorithmic logic required to solve these instances without requiring hardcoded sparsification.\n- **The density bottleneck**: We experimentally show that embedding aliasing is of great importance regarding large-scale generalization. This result should motivate focused research toward finding new and better approaches.\n\nWe initially hypothesized that the scaling factor should maintain constant spatial density relative to the training set, following the rule $s \\propto \\sqrt{n_{test}/n_{train}}$. However, our experiments revealed a non-trivial relationship as the empiricaly optimal factors deviate from this theoretical baseline. \nOne possibility that we envisage is to investigate the problem from the point of view of **discrepancy theory**. While the density of the problem generation is uniform, the actual realization of points inevitably contains local irregularities. Deriving a principled scaling formula that accounts for these probabilistic phenomena may be a promising avenue for future work.\n\nWe emphasize that **PENS's competitive performance is not solely reliant on this heuristic**. Even without the scaling factor, our results remain strong, confirming the architectural benefits of using PEs. A detailed comparison can be found in Table 3 of the appendix.\n\n[1] INViT: A generalizable routing problem solver with invariant nested view transformer, ICML, 2024."}}, "id": "dpd6uJKudR", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763646459295, "cdate": 1763646459295, "tmdate": 1763646459295, "mdate": 1763646459295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a transformer-based neural solver for the Euclidean Traveling Salesman Problem (TSP) using positional encodings rather than coordinate projections. By adapting ALiBi and RoPE, two modern positional encoding methods originally designed for large language models, the authors introduce Positional Encoding-based Neural Solvers (PENS). The study demonstrates that these encodings provide inductive biases that improve the neural solver’s ability to generalize, particularly on large-scale TSP instances, achieving state-of-the-art results on problems with up to 10,000 cities. Furthermore, the paper proposes a coordinate rescaling technique to mitigate challenges arising from the increased density of city instances, further boosting performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a novel use of positional encodings (ALiBi and RoPE) in the context of the TSP, leveraging their ability to capture spatial relationships between cities. This innovation helps in scaling neural solvers to large TSP instances, outperforming previous methods that required graph sparsification.\n\n2.The experimental results show that PENS achieves state-of-the-art performance, especially on large instances (up to 10,000 cities). It surpasses previous neural TSP solvers such as INViT and DGL, especially in terms of optimality gaps and computational efficiency.\n\n3.The approach effectively handles large-scale instances of TSP without requiring sparsification, an improvement over prior works that rely on graph sparsification methods. The results on TSP-10,000 demonstrate that the method is capable of handling very large problem sizes efficiently."}, "weaknesses": {"value": "1.Although the model achieves great results on large instances, the method still requires a full forward pass at each decoding step, which could be computationally expensive. Future work could focus on optimizing decoding efficiency and reducing computational overhead.\n\n2.While the paper does a good job comparing positional encoding methods (ALiBi vs. RoPE), it could benefit from a deeper discussion on how other encoding techniques might be integrated with transformer-based solvers for TSP, such as sparse attention or graph-based encodings.\n\n3.The paper uses a heuristic approach to estimate the best scaling factor for different TSP sizes, but it might be useful to provide a more rigorous method for determining the scaling factor, especially for different TSP variants, including those with asymmetric costs."}, "questions": {"value": "1.How well does the method generalize to real-world TSP instances that may involve more complex constraints or asymmetric distances? Could PENS be adapted to handle such cases, and if so, how?\n2.The rescaling factor significantly improves performance on larger instances. However, does this scaling factor have any impact on smaller TSP instances (under 1000 cities), and would it be better to use different scaling factors for different problem sizes?\n3.Could the authors provide more detailed comparisons with non-transformer-based methods, especially those not relying on positional encodings or graph sparsification? How does the performance of PENS compare to other graph neural network-based approaches or reinforcement learning-based solvers for TSP?\n4.While the model shows strong performance, the training time on large instances might be a limiting factor. Would incorporating techniques like gradient checkpointing, multi-GPU setups, or model pruning improve training efficiency without sacrificing accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "exNDwCIkjY", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Reviewer_LAVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Reviewer_LAVY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839771415, "cdate": 1761839771415, "tmdate": 1762923089672, "mdate": 1762923089672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Positional Encoding-based Neural Solvers (PENS), which introduce modern positional encodings (ALiBi, RoPE) into the neural combinatorial optimization (NCO) domain for solving the Euclidean TSP. Instead of projecting 2D coordinates into higher-dimensional spaces as in conventional NCO models, PENS leverages positional encodings as input representations. In addition, a coordinate rescaling scheme is incorporated, enabling the model trained solely on TSP-100 to generalize effectively to instances ranging from 100 up to 10K nodes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes an innovative contribution by introducing an ALiBi-based positional encoding tailored for TSP, where Euclidean distances between nodes are used in place of token index distances.\n\n2. The work explores and evaluates two forms of modern positional encodings (ALiBi and RoPE) in the context of TSP.\n\n3. The overall presentation is clear, with a well-structured and accessible writing style."}, "weaknesses": {"value": "1. **The experimental design is not sufficiently direct**. Since the key contribution is to introduce modern positional encodings (PE) for representing TSP inputs in transformer-based neural solvers, **it would be more convincing to explicitly replace the coordinate projection layers in classical NCO models** (e.g., AM-Kool [ICLR 2019], POMO) and recent strong baselines (e.g., BQ, LEHD) with the proposed positional encodings, while keeping the rest of the model unchanged. Such controlled experiments would clarify whether the improvements, particularly in generalization, stem from the PE themselves.\n\n2. **The experimental results raise some concerns**. In Figure 4, the performance of **CoordNS** appears unusual. According to the paper (p.6, line 299), CoordNS essentially corresponds to a “**standard transformer backbone**.” Yet, when trained on TSP-100, it achieves only a **1.51%** gap relative to Concorde on TSP-1000—surpassing BQ, LEHD, and even [1], where [1] trains directly on TSP-1000 but still reports a **1.95%** gap. Such strong performance using raw coordinates alone seems noteworthy and warrants deeper investigation. It is unclear why the authors did not further analyze or discuss this unexpectedly strong result.\n\n3. The paper **lacks sufficiently strong SOTA baselines**. The included comparisons with BQ, LEHD, and INViT are somewhat outdated. For instance, [1], which is also attention-based, achieves competitive or better performance: its **runtime** on TSP-1K is **far lower than PENS**, with only slightly worse performance, and on TSP-10K it outperforms PENS in **both runtime and solution quality**. While PENS is trained only on TSP-100 and generalized to 100–10K, whereas [1] is trained separately at each problem size, it would still be much more convincing to report results from stronger baselines under the same setting (trained on TSP-100 and tested across scales), including both **runtime** and **optimality gap**.\n\n---\n\n**Based on the above three points, I suggest that the authors conduct controlled experiments by replacing the linear projection layers in existing models with these two PEs, and then observe whether performance improves. This would provide stronger evidence for the effectiveness of PE itself.**\n\n---\n\n4. The experiments are **limited to TSP**, with no evaluation on other combinatorial optimization problems. It is unclear why the proposed approach cannot be applied to CVRP, for example. Is the limitation due to the fact that the two positional encodings used here cannot directly encode node demands as linear projections do? If the method is inherently restricted to TSP, **the broader significance of introducing positional encodings into NCO would be somewhat diminished**. I recommend that the authors at least **discuss how CVRP demands might be incorporated**. Furthermore, since ATSP only requires a distance matrix, additional **experiments on ATSP** (with [2], [3] as baselines) would strengthen the case for the general applicability and significance of PENS.\n\n5. (minors) To improve rigor, the authors should avoid making absolute claims without sufficient literature coverage, even when qualified by phrases such as “to our knowledge.” For example, on p.2 line 99, the statement “the only ones to use distance matrices directly” is too strong. A simple literature search (or even directly using an LLM to deepresearch) may reveal additional NCO works that employ distance matrices [2,3,4]. Softening such claims would strengthen the paper’s credibility.\n\n\n---\n\n[1] Luo, Fu, et al. \"Boosting neural combinatorial optimization for large-scale vehicle routing problems.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n\n[2] Kwon, Yeong-Dae, et al. \"Matrix encoding networks for neural combinatorial optimization.\" Advances in Neural Information Processing Systems 34 (2021): 5138-5149.\n\n[3] Pan, Wenzheng, et al. \"UniCO: On unified combinatorial optimization via problem reduction to matrix-encoded general TSP.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[4] Zhou, Changliang, et al. \"ICAM: Rethinking Instance-Conditioned Adaptation in Neural Vehicle Routing Solver.\" (2025)"}, "questions": {"value": "1. The source of PENS’s performance, particularly its generalization ability, remains somewhat unclear. Could the authors provide additional experiments and discussion to clarify this point, especially in relation to Weaknesses 1 and 2?\n\n2. How does the method perform on other combinatorial optimization problems? Please refer to Weakness 4 for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L43tvPm9Ed", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Reviewer_GQwc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Reviewer_GQwc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843925828, "cdate": 1761843925828, "tmdate": 1762923089087, "mdate": 1762923089087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PENS, a Transformer-based approach to solving the Euclidean TSP. Instead of feeding raw coordinates, it uses positional encodings — namely ALiBi and RoPE, which are commonly used in large language models — to represent spatial relationships between cities. The authors argue that this brings translation, rotation, and scale invariance, and helps models trained on small instances (TSP100) generalize to larger ones (TSP10,000). Results show that PENS performs well and even beats some sparsified Transformer baselines like INViT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is cleanly written and technically sound, with fair comparisons and detailed ablations.\n2. It gives a practical insight: simple positional encodings can help Transformers scale better for geometric problems.\n3. The results are solid and the method is easy to reproduce."}, "weaknesses": {"value": "1. The main issue is limited novelty. The paper basically transfers ALiBi and RoPE (well-known in NLP) to TSP. There’s no new learning idea or inductive bias proposed.\n2. The improvement seems mostly empirical, driven by better coordinate scaling and heuristics rather than a truly new model concept.\n3. It doesn’t help us understand neural combinatorial optimization better. There’s no new training strategy or learning dynamic introduced.\n4. Evaluation is limited to synthetic Euclidean TSP. It’s unclear how it performs on other routing problem types."}, "questions": {"value": "1. Could you provide a clearer theoretical or analytical explanation of why ALiBi or RoPE leads to better translation, rotation, or scale invariance in TSP? \n2. The experiments are only conducted on synthetic Euclidean TSP datasets. Have you tested the model on non-Euclidean graphs or other routing problems (such as VRP)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U9j6D81Emj", "forum": "EQuoft7Yc7", "replyto": "EQuoft7Yc7", "signatures": ["ICLR.cc/2026/Conference/Submission12126/Reviewer_2M8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12126/Reviewer_2M8Y"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900348432, "cdate": 1761900348432, "tmdate": 1762923088677, "mdate": 1762923088677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}