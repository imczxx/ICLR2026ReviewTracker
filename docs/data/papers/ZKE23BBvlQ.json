{"id": "ZKE23BBvlQ", "number": 1153, "cdate": 1756852264564, "mdate": 1759898224825, "content": {"title": "Human Behavior Atlas: Benchmarking Unified Psychological And Social Behavior Understanding", "abstract": "Using intelligent systems to perceive psychological and social behaviors, that is, the underlying affective, cognitive, and pathological states that are manifested through observable behaviors and social interactions, remains a challenge due to their complex, multifaceted, and personalized nature. Existing work tackling these dimensions through specialized datasets and single-task systems often miss opportunities for scalability, cross-task transfer, and broader generalization. To address this gap, we curate Human Behavior Atlas, a unified benchmark of diverse behavioral tasks designed to support the development of unified models for understanding psychological and social behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text, audio, and visual modalities, covering tasks on *affective states, cognitive states, pathologies,* and *social processes*. Our unification efforts can reduce redundancy and cost, enable training to scale efficiently across tasks, and enhance generalization of behavioral features across domains. On Human Behavior Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and  OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models to consistently outperform existing multimodal LLMs across diverse behavioral tasks. Pretraining on Human Behavior Atlas also improves transfer to novel behavioral datasets; with the targeted use of behavioral descriptors yielding meaningful performance gains.", "tldr": "Benchmark of psychological and social behavioral datasets for developing a multimodal unified model.", "keywords": ["Multimodal Learning", "Unified Models", "Benchmarking", "Transfer Learning", "Human Behavior"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3415c7e9ec5b95f2fd2f8362c2d6b9bf562af14d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "By integrating diverse modalities and heterogeneous datasets under a single LLM-based paradigm, it demonstrates the potential of large models to generalize across emotional, cognitive, and behavioral domains. While some architectural or methodological innovations are limited, the work’s scale, data diversity, and behavior-centric vision represent a meaningful contribution that could serve as a foundation for future research in multimodal affective computing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors successfully harmonize heterogeneous datasets and tasks (e.g., emotion recognition, personality, mental health indicators) under a single large language model (LLM)-based paradigm, which demonstrates strong potential for generalization across domains.\n2. Leverages an unprecedented scale of multimodal behavioral data, enabling the model to learn rich representations that reflect both emotional and cognitive dimensions of human behavior.\n3. Experimental results demonstrate solid transfer capabilities across a wide range of downstream behavioral and affective computing tasks"}, "weaknesses": {"value": "1. The authors designed two types of output: a dedicated classifier for categorical predictions, and a decoder that generates open-ended responses from the final hidden states. However, this classifier+decoder structure seems inconsistent with the logic of using large language models. Ideally, we expect an LLM to use a single decoder to produce all outputs, while the classifier approach merely leverages the model’s encoding capability, which deviates from the intended design philosophy.\n2. The main contribution lies in integrating various tasks and datasets into a unified LLM-based training paradigm. However, the authors did not introduce any specific architectural designs to enhance model capability, for example, handling behavioral temporal dynamics, or effectively leveraging facial landmarks, acoustic cues, or pose keypoints. Instead, these modalities are simply fed into the model as additional data for brute-force learning, which shows limited innovation.\n3. Currently, LLMs perform poorly on regression tasks, yet affective computing tasks such as valence-arousal estimation or depression assessment require fine-grained regression rather than simple binary classification. The authors’ choice to convert PHQ-9 into a binary classification problem thus oversimplifies the task and fails to reflect real-world application scenarios.\n4. A common issue in multi-task learning is negative transfer or gradient conflict, but the authors did not address whether their method incorporates any targeted solutions to mitigate these problems.\n5. The superior performance in transfer learning could be attributed to the use of large-scale and diverse multimodal data (visual/acoustic cues) rather than the proposed model architecture or data structure itself, the paper does not clarify this distinction."}, "questions": {"value": "1. Although the authors devote substantial discussion to explaining how behavioral descriptors benefit model training (a point already well established in many multi-task affective models) it is unclear why the model itself is not designed to predict these descriptors directly. Doing so would align more closely with the paper’s title, “Human Behavior Atlas”, which implies an explicit mapping or prediction of behavioral factors.\n2. In the implementation section, the authors carefully adjust minibatch sizes across different tasks. However, it would be more informative if they could quantitatively present the impact of task diversity on training. Moreover, since the datasets vary greatly in size, it remains unclear how the authors address data imbalance during training or does imbalance affect the model performance.\n3. The comparison models were not trained on such a large and diverse dataset, which raises concerns about fairness. A more rigorous and convincing evaluation would involve cross-dataset validation, for example, testing affective models across IEMOCAP, Aff-Wild2, or AffectNet, to demonstrate the generalization ability of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cTh7hRRpXa", "forum": "ZKE23BBvlQ", "replyto": "ZKE23BBvlQ", "signatures": ["ICLR.cc/2026/Conference/Submission1153/Reviewer_tGVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1153/Reviewer_tGVo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401439454, "cdate": 1761401439454, "tmdate": 1762915692459, "mdate": 1762915692459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Human Behavior Atlas (HBA)**, a unified multimodal benchmark for psychological and social behavior understanding. Key contributions include:\n- A standardized **prompt–target interface** across 14 datasets and 10+ behavioral tasks.\n- A **Behavioral Adapter Module (BAM)** for injecting external behavioral descriptors (e.g., pose, acoustic features) into a frozen backbone.\n- Three training variants: **SFT, BAM, and RL (GRPO)**, evaluated comprehensively across tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Unified benchmark** with broad task coverage and standardized metrics.\n- **Clean and modular design** of BAM, enabling non-invasive feature injection.\n- **Consistent empirical gains** of behavior-specialized models over general MLLMs."}, "weaknesses": {"value": "#### **1. Insufficient Ablation Studies on BAM and RL**\n- **BAM Design Choices**: The paper does not justify key design decisions—e.g., why the adapter is inserted at the **penultimate layer**, why **mean and standard deviation pooling** is used for temporal descriptors, or why a **two-layer FFN** is chosen. A systematic sweep over injection points, hidden dimensions, and pooling strategies is missing.\n- **Descriptor Contribution**: The relative importance of **visual vs. acoustic descriptors** is not analyzed. Ablations disabling each stream would clarify their individual and synergistic effects.\n- **RL Hyperparameters**: The use of **β = 0 for KL regularization** is not motivated, and no sensitivity analysis is provided for group size, clipping epsilon, or reward scaling. Training stability and reward hacking risks are unexamined.\n\n#### **2. Evaluation Protocol Gaps**\n- **LLM-as-Judge Reliability**: The open-ended evaluation relies solely on a **single closed-source LLM judge** (GPT-5-nano). No human–judge agreement, inter-judge consistency, or prompt robustness tests are reported.\n- **Prompt Isolation & Contamination**: The paper does not clarify whether the judge and model prompts are isolated, whether sessions are reset, or whether tools are disabled—raising concerns about evaluation integrity.\n\n#### **3. Label Granularity Reduction**\n- Sentiment labels in datasets like **MOSEI (7-point scale)** are collapsed to **binary positive/negative**, discarding nuanced distinctions. This limits comparability with prior work and may mask model failures on fine-grained sentiment.\n\n#### **4. Metric Heterogeneity and Cross-Task Comparability**\n- Discrete tasks use **weighted F1 or weighted accuracy**, while open-ended tasks use **LLM-judged TRUE-rate**. This inconsistency complicates cross-task aggregation and model ranking.\n- No unified scoring mechanism (e.g., normalized score per task family) is proposed.\n\n#### **5. Lack of Computational Efficiency Analysis**\n- The computational cost of BAM (e.g., latency, memory overhead) is not reported, nor is its impact on inference speed or scalability—key for real-world deployment.\n\n#### **6. Limited Discussion on Multimodal Fusion Strategy**\n- The fusion of text, audio, and video is performed via **simple concatenation** in the embedding space. More advanced fusion mechanisms (e.g., cross-attention, gating) are not explored or motivated."}, "questions": {"value": "1. **LLM-as-Judge Reliability**: Please report human–judge agreement (κ or ρ) on a 500-sample subset, add at least one open-source judge, and show sensitivity to prompt variations.\n2. **BAM & RL Ablations**: Systematically vary BAM’s injection layer, hidden size, and descriptor streams. For RL, sweep β, group size, and reward terms—report accuracy vs. compute and training stability.\n3. **Fine-Grained Evaluation**: Provide results on full label sets (e.g., 7-point sentiment) and justify binarization decisions with class-wise F1 and confusion matrices.\n4. **Metric Harmonization**: Propose a unified scoring scheme (e.g., normalized score per task family) and report confidence intervals over multiple runs.\n5. **Prompt and Protocol Transparency**: Document judge–model isolation measures and include a finalized, typo-free version of the judge rubric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KTehuVt3MV", "forum": "ZKE23BBvlQ", "replyto": "ZKE23BBvlQ", "signatures": ["ICLR.cc/2026/Conference/Submission1153/Reviewer_eg78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1153/Reviewer_eg78"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749783726, "cdate": 1761749783726, "tmdate": 1762915692304, "mdate": 1762915692304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an unified benchmark of diverse behavioral tasks designed to support the development of unified models for understanding psychological and social behaviors. The aim is to take the opportunities for scalability, cross-task transfer, and broader generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized and written\n-  The unified benchmark named HUMAN BEHAVIOR ATLAS comes in a timely manner for the field. It covers a large spectrum of situations."}, "weaknesses": {"value": "None"}, "questions": {"value": "What will be the restrictions about the use of this benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5N4zfbtTLQ", "forum": "ZKE23BBvlQ", "replyto": "ZKE23BBvlQ", "signatures": ["ICLR.cc/2026/Conference/Submission1153/Reviewer_3tAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1153/Reviewer_3tAt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891669144, "cdate": 1761891669144, "tmdate": 1762915692155, "mdate": 1762915692155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HUMAN BEHAVIOR ATLAS, a unified multimodal benchmark focused on understanding psychological and social behaviors. The benchmark combines 13 existing datasets into a prompt-response instruction format across text, audio, and video modalities, totaling around 101k samples spanning 10 behavioral task categories. Three model variants are trained and evaluated: OMNISAPIENS-7B SFT, OMNISAPIENS-7B BAM, and OMNISAPIENS-7B RL. Results show that these models outperform general multimodal LLMs (Qwen 2.5, Gemma-3, HumanOmni) on both multi-task and transfer-learning evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark is a useful contribution that could help move multimodal LLM research toward more holistic behavioral understanding. \n2. The experimental setup is thorough, covering 10 in-domain datasets and some \"transfer\" datasets. \n3. The benchmark and code will become publicly available."}, "weaknesses": {"value": "1. No new data is collected - the benchmark repackages existing datasets into a unified format. While valuable, this limits novelty.\n2. The models are fully fine-tuned on the same datasets used for evaluation. In the transfer-learning section, the model is again fine-tuned for a few epochs on the \"held-out\" datasets, and only Qwen 2.5-Omni-7B is used for comparison. This setup mainly measures fine-tuning efficiency, not true zero-shot generalization.\n3. The construction of prompts is unclear. It’s not specified whether they were hand-crafted or automatically generated, nor whether prompt robustness was tested."}, "questions": {"value": "1. In Table 4, can you add results using only the behavioral descriptors (e.g., OpenSMILE, MediaPipe) to show how much these features alone contribute?\n2. Since the models are fine-tuned on the same datasets used for evaluation, have you checked whether fine-tuning affects general-purpose abilities (e.g. text generation)?\n3. Did you perform any true zero-shot evaluations to measure actual generalization?\n4. How were prompts generated - hand-written or LLM-generated?\n5. In Table 5, it would be interesting to add results before fine-tuning on the held-out datasets and also show results for the multimodal LLMs used in Table 4 for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8HnbZftLK", "forum": "ZKE23BBvlQ", "replyto": "ZKE23BBvlQ", "signatures": ["ICLR.cc/2026/Conference/Submission1153/Reviewer_aP6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1153/Reviewer_aP6y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911184656, "cdate": 1761911184656, "tmdate": 1762915692031, "mdate": 1762915692031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HUMAN BEHAVIOR ATLAS, a large-scale multimodal benchmark with numerous tasks for general psychological and social behavior understanding. The core data contribution is curation and standardization of existing public datasets. Three variants of OMNISAPIENS-7B are evaluated to show the effectiveness of multi task training and transfer learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Benchmark, source code, models will be released.\n* Atlas is a large-scale multimodal benchmark with numerous tasks for general psychological and social behavior understanding.\n* Paper is well-written, and easy to follow.\n* Promising performances for multi-task training and transfer learning are shown."}, "weaknesses": {"value": "* My major concern is the limited contributions\n  * The paper does not propose a new model architecture.\n  * The core data contribution is curation and standardization of existing public datasets instead of new data collection.\n* The comparison in Table 4 is unfair because the OMNISAPIENS-7B variants were trained directly on the HUMAN BEHAVIOR ATLAS data, while the general multimodal LLM baselines were evaluated in zero-shot inference mode without fine-tuning on this specific benchmark. The performance gain largely reflects the benefit of SFT or RL on the target tasks, not necessarily the inherent superiority of the model's architecture or the benchmark itself.\n* To demonstrate the superiority of the ATLAS for unified modeling, the authors should compare OMNISAPIENS-7B variants to models trained exclusively on another large, existing social behavior or affective computing dataset (e.g., a comprehensive version of CMU-MOSEI, MELD, or a large synthesized dataset like HumanOmni) and then test all models across the full range of ATLAS tasks.\n* A similar issue in the transfer learning experiment in Section 4.2 and Table 5."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5cDaJq4zIQ", "forum": "ZKE23BBvlQ", "replyto": "ZKE23BBvlQ", "signatures": ["ICLR.cc/2026/Conference/Submission1153/Reviewer_PK4q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1153/Reviewer_PK4q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201418771, "cdate": 1762201418771, "tmdate": 1762915691742, "mdate": 1762915691742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}