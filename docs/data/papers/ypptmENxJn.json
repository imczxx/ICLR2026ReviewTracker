{"id": "ypptmENxJn", "number": 23638, "cdate": 1758346624916, "mdate": 1759896803785, "content": {"title": "GUI‑AIMA: Aligning Intrinsic Multi-Modal Attention with a Context Anchor for GUI Grounding", "abstract": "Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to $\\textit{first select visual patches relevant to the instructions and then determine the precise click location within those patches}$. Based on the observations that general MLLMs have native grounding capability, which is highly correlated with query-to-visual attentions, we propose GUI-AIMA, an attention-only and coordinate-free supervised fine-tuning framework for efficient GUI grounding. This framework aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. \nSpecifically, we convert coordinate-based grounding boxes into soft patch-wise labels considering patch overlap and the center click manner. For attention aggregation, we simplify the merging of attention predictions among all query tokens into a single anchored attention vector with learnable $\\texttt{\\<ANCHOR\\>}$ token. \nMore importantly, GUI-AIMA includes a query-adaptive multi-head weighting mechanism for multi-head attention aggregation by prioritizing text-vision affinity heads with visual-sink query tokens.\nGUI-AIMA-3B, trained on a small training set only with  85k screenshots, achieves the state-of-the-art performance among 3B models ($\\textit{i.e.}$,~$\\textbf{44.9\\%}$ average on ScreenSpot-Pro and $\\textbf{90.8\\%}$ average on ScreenSpot-v2).", "tldr": "We proposed GUI-AIMA, a attention-based GUI visual grounding method supervised on anchored attention with query-adaptive multi-head weighting.", "keywords": ["GUI grounding", "visual grounding", "MLLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8c741a3570736b5d9795d0487c0aac53a44b6a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper mainly focuses on the GUI Grounding problem and the authors explored utilizing intrinsic multimodal attention with a learnable context anchor token to propose a coordinate-free grounding method. To aggregate multi-head attention information, visual-query sink tokens are selected to generate weights for final attention-based grounding decision. Methods are verified on two popular GUI benchmarks to show the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Introducing the visual-sink query to construct attention-based grounding is interesting"}, "weaknesses": {"value": "1. L364-365 states that all model parameters were updated during training. Regarding the selection of Visual-sink Query Tokens in Equation (7), is this selection dynamic and layer-wise, changing with the input during the forward pass during training? Also, is a stop-gradient operation applied to the weight $w$ in Equation (4) or Equation (8)?\n2. In Equation (4), the term $w_{l,h}$ is used to weight the anchor-visual token attention for the $l$-th layer and $h$-th head. However, L318-319 indicates that the specific value of $w_{l,h}$ is obtained by summing the attention scores between the selected visual-sink query token and the visual tokens. Given that the anchor-visual token relationship and the visual-sink query-visual token relationship are not necessarily the same, could the authors please provide a detailed justification for this direct substitution and usage? Is it possible to visualize the final selected visual-sink query token to illustrate which specific query token was chosen for grounding after model training?\n3. In the experiments on ScreenSpot-v2 (Table 2), how were the performance metrics (avg: 90.4%) for GUI-Actor-3B obtained? According to Table 4 (bottom part) of the GUI-Actor paper, GUI-Actor-3B achieves 91.0% average accuracy, further improving to 92.4% with the addition of a verifier module.\n4. Furthermore, the comparison against the GUI-Actor method appears to only consider a weakened version (i.e., the version without the verifier module). Given that the verifier module can be a common and general component for attention-based grounding methods, the authors should investigate if the proposed method could similarly benefit from the lightweight verifier module. Comparisons should ideally be made against the SoTA performance, rather than a potentially sub-optimal baseline.\n5. The experimental results are exclusively based on the Qwen2.5-VL-3B-Instruct model. It is recommended exploring the performance of this method when scaled up to larger model sizes (like ~7B) to demonstrate its generalizability and effectiveness across different scales.\n6. The paper currently only demonstrates the performance of GUI Grounding. As GUI grounding is merely one critical component of a comprehensive GUI Agent, the work lacks exploration into how the proposed method can be further extended to a functional GUI Agent System. Providing a demonstration of how this method contributes to a unified and efficient GUI Agent would be significantly more compelling and practical for the community."}, "questions": {"value": "Please find the question in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eRjT5HwbBS", "forum": "ypptmENxJn", "replyto": "ypptmENxJn", "signatures": ["ICLR.cc/2026/Conference/Submission23638/Reviewer_UHkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23638/Reviewer_UHkz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960756022, "cdate": 1761960756022, "tmdate": 1762942740301, "mdate": 1762942740301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUI-AIMA, an attention-only framework that leverages MLLMs’ intrinsic grounding capability via attention alignment. It does not rely on extra modules( e.g., GUI-Actor’s embedding-based heads) and also avoid cumbersome token aggregation and biased head selection(e.g., TAG) through their pipeline. \n\nThe main pipeline includes 3 major components:\n1. It converts coordinate-based bounding boxes into soft labels by combining IoU (for overlap) and Gaussian distance. It can prioritize center patches (matching human click habits) and down-weights partial-overlap border patches, resolving the annotation gap between coordinates and patches.\n\n2. Design a learnable <ANCHOR> token to the input sequence ([V, Q, <ANCHOR>]) to act as a surrogate for aggregating query-visual attentions. This eliminates the need for manual weighting of all query tokens (a flaw in TAG) and preserves MLLMs’ general capabilities by disentangling grounding from general understanding.\n\n3. Identifies “visual-sink query tokens” to weight attention heads. Heads with robust text-vision affinity are prioritized, avoiding bias from premature <ANCHOR> tokens or irrelevant query tokens. \n\nAnd the final result looks promising. \n\nThe ablation study is sufficient to support the claims of each module."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novelty: \nThis paper proposed an efficient and effective pipeline for GUI grounding. It abandons traditional coordinate-based bounding box generation (which requires additional efforts like scaled corpora or OCR pretraining) and instead frames GUI grounding as patch-level attention prediction. \n\nAlso, the paper clearly motivates the work by highlighting limitations of existing methods. It proposes three novel components: overlap/center-aware patch labels (aligning with human click habits), visual-sink query tokens (for adaptive head weighting), and <ANCHOR>-based aggregation—each addressing a specific pain point in prior GUI grounding research. \n\nThe clarification of motivation and technical novelty looks good to me. Also, the theoretical details of each component are well demonstrated. These details ensure transparency in how the framework maps theoretical ideas to practical implementation. \n\n\n2. Performance: \nThe result looks promising. Trained on only 85k screenshots (far less than SFT baselines requiring millions of samples), this proposed 3B model achieves state-of-the-art results among other 3B models. It even rivals larger 7B models and outperforms embedding-based (GUI-Actor-3B) and RL-based (GUI-G²-3B) baselines. \n\nFrom this result, we could see that a well-designed small model for a specific domain might be able to compete with larger models. Also, the paper implicitly supports the potential of extending this framework to larger models. Its core designs are not limited to 3B backbones. Exploring broader visual grounding tasks, and scaling to larger models (with sufficient computing resources) could further enhance performance by leveraging more powerful pretrained capabilities while retaining GUI-AIMA’s efficient domain adaptation.\n\n3. The paper is well-written and easy to catch."}, "weaknesses": {"value": "1. The problem settings:\n\nThis work presents a strong contribution to addressing the GUI grounding problem through a well-designed coordinate-free pipeline. GUI grounding is indeed a critical component of GUI understanding, and the solutions proposed here hold practical value for advancing the development of GUI agents.\n\nThat said, the scope of GUI grounding itself is relatively narrow within the broader landscape of GUI agent capabilities. A potential concern is that enhancing grounding performance—even via the efficient attention-aligned framework proposed—might inadvertently compromise other core capabilities of the underlying MLLMs (e.g., reasoning), though this trade-off is not explicitly explored in the work.\n\n2. Some details:\na.  The identification of visual-sink query tokens relies on cosine similarity with MLLMs’ intermediate hidden states, instead of attention matrices. It assumes hidden states consistently capture cross-modal affinity—a property that may not generalize to all MLLM backbones (e.g., models with different layer architectures or pretraining objectives).\n\nb. I think the theoretical discussion is almost sufficient for me. Since it is for GUI agent, it would be better if the paper evaluates its performance in end-to-end agent workflows (e.g., combining grounding with action execution, handling dynamic GUI changes like pop-ups or resizing). \n\nc. Some key hyperparameters (e.g., α=0.8 for Gaussian distance in patch labels, top-K=1 for visual-sink tokens) are set empirically without systematic analysis of their impact. The paper does not explore how varying these values (e.g., α=0.6/1.0, K=2/3) affects performance. Will those parameters change if we change to another dataset?"}, "questions": {"value": "As shown in the weakness part, the main questions are listed below:\n1. For the problem setting, a key consideration arises: when the framework focuses exclusively on optimizing GUI grounding, will the MLLM’s capabilities for other purposes (e.g., reasoning or QA) be weakened? Additionally, when designing the entire pipeline, are there any trade-offs between the grounding task and other tasks that the MLLM is expected to perform?\n\n2. Is this work easy to be generalized to other MLLM backbones considering the detailed design of calculating cosine similarity over hidden states. \n\n3. Maybe some more practical discussion of this work and how it will contribute to the GUI agent.\n\n4. As for the hyperparameters, will those parameters change if we change to another dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "egQb1p0GVo", "forum": "ypptmENxJn", "replyto": "ypptmENxJn", "signatures": ["ICLR.cc/2026/Conference/Submission23638/Reviewer_9cY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23638/Reviewer_9cY1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962641072, "cdate": 1761962641072, "tmdate": 1762942740052, "mdate": 1762942740052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GUI-AIMA, a coordinate-free and attention-only framework for graphical user interface (GUI) grounding. Unlike previous methods that generate coordinates directly or add extra grounding modules, GUI-AIMA fine-tunes the intrinsic multimodal attention of multimodal large language models (MLLMs) to achieve efficient grounding without modifying the model architecture. Specifically, the method converts coordinate-based annotations into patch-wise soft labels that reflect overlap and center-click likelihood, and introduces a learnable <ANCHOR> token to aggregate attention across query tokens for simplified supervision. A visual-sink query token weighting mechanism further enhances grounding by emphasizing attention heads with strong cross-modal correlations while maintaining pretrained generalization capacity. Trained with only 85K screenshots, GUI-AIMA-3B achieves competitive performance among 3B-parameter models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a clean, coordinate-free grounding framework that aligns multimodal attention without modifying the model architecture, offering a simple yet effective alternative to complex grounding modules.\n2. Despite its simplicity, GUI-AIMA achieves competitive grounding performance with fewer training samples, and its attention-based formulation provides clearer interpretability of visual-textual alignment in GUI understanding."}, "weaknesses": {"value": "*Method\n\nThe technical differences from GUI-Actor [1] are unclear. The paper positions GUI-AIMA as a coordinate-free alternative to prior grounding methods, but does not clearly explain how its attention mechanism fundamentally differs from that of GUI-Actor. Apart from removing the grounding verifier (referred to as the “extra adaptation stage” in this paper), the design seems conceptually similar since GUI-Actor also introduces an <ACTOR> token as a contextual anchor. It would be better to clarify what intrinsic attention alignment GUI-AIMA achieves that GUI-Actor’s attention schema cannot.\n\n*Experiments\n\n1. Missing key baselines. Although the paper discusses related methods such as GUI-Actor [1] and TAG [2], the experiments omit TAG from direct comparison. It is more convincing if considering it as an additional baseline.\n\n2. Incomplete and possibly misleading performance comparison. \n\n- The reported comparisons against GUI-Actor only use the variant without the grounding verifier. However, the GUI-Actor’s complete method consists of both the <ACTOR> token and verifier components. Even under this simplified baseline, Table 2 results show only comparable performance rather than a clear improvement (90.4 vs. 90.8). To make the performance claim convincing, it would help to either include results for the full GUI-Actor pipeline or evaluate on an additional dataset such as ScreenSpot-v1. \n\n- Besides, since the paper argues the “extra adaptation stage” is inefficient, efficiency metrics (e.g., inference latency) are also necessary.\n\n[1] Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025.\n\n[2] Hai-Ming Xu, Qi Chen, Lei Wang, and Lingqiao Liu. Attention-driven gui grounding: Leveraging pretrained multimodal large language models without fine-tuning, AAAI, 2025."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y62gDHclC0", "forum": "ypptmENxJn", "replyto": "ypptmENxJn", "signatures": ["ICLR.cc/2026/Conference/Submission23638/Reviewer_FRCX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23638/Reviewer_FRCX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003069465, "cdate": 1762003069465, "tmdate": 1762942739725, "mdate": 1762942739725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GUI-AIMA, a coordinate-free, attention-only framework for GUI visual grounding, i.e., mapping a natural-language instruction to a clickable region on a graphical interface.\nUnlike previous coordinate-based or embedding-based models (e.g., GUI-Actor), GUI-AIMA directly leverages the intrinsic multi-head self-attention (MHSA) within a multimodal large language model (MLLM). The method supervises attention weights with patch-wise soft labels converted from bounding boxes using an IoU-weighted Gaussian centered at the click point.\nA learnable <ANCHOR> token is introduced to aggregate query-to-visual attentions, simplifying token-wise aggregation. Moreover, the paper proposes a visual-sink query token weighting scheme that adaptively emphasizes attention heads with strong cross-modal alignment.\n\nWhile GUI-AIMA presents an elegant and efficient way to supervise intrinsic attentions for GUI grounding, its applicability is restricted to single-target tasks, and the conceptual novelty beyond simplifying previous attention aggregation is limited.\nThe idea is interesting and potentially impactful for broader multimodal grounding, but it currently lacks the depth and empirical diversity expected for acceptance at a major venue like ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of aligning the model’s own MHSA to grounding signals is appealing, and the proposed framework adds no extra modules and can be trained in a single stage. The <ANCHOR> token provides a neat mechanism for aggregating attention across query tokens.\n\n- GUI-AIMA-3B outperforms all existing 3B-scale models and rivals much larger (7B–70B) models, showing high efficiency and good generalization with a modest dataset. The paper ablates key components (<ANCHOR> token, visual-sink weighting, and soft patch labeling), and results are consistent and interpretable."}, "weaknesses": {"value": "- The proposed framework is only applicable to single-element grounding tasks and cannot handle dense and overlapping GUI grounding or multi-intent GUI agent tasks,  while prior GUI grounding methods like predicting coordinates as discrete language tokens in MLLM response can naturally perform multi-element grounding. There is no discussion about this limitation and potential ways to extend to complex GUI grounding scenarios. \n\n- The visual-sink token weighting relies on heuristics (top-K cosine similarity), with limited justification. No quantitative analysis shows how these heads correlate with true cross-modal alignment. The paper would also strongly benefit from visualizing how <ANCHOR> attention maps change before/after training to support the claim of “aligning intrinsic attention.”\n\n- The baselines mainly cover prior coordinate- or attention-based models. No comparison to new multi-stage grounding or reinforcement learning approaches that can act on multiple UI components."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cc0dWLVeh4", "forum": "ypptmENxJn", "replyto": "ypptmENxJn", "signatures": ["ICLR.cc/2026/Conference/Submission23638/Reviewer_pAYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23638/Reviewer_pAYT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114674064, "cdate": 1762114674064, "tmdate": 1762942739210, "mdate": 1762942739210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}