{"id": "P8EhH6ypA5", "number": 25582, "cdate": 1758369276310, "mdate": 1759896714472, "content": {"title": "Silver Stepsize for Faster Zeroth-Order Optimization", "abstract": "We study gradient-free minimization of smooth convex functions through Silver stepsizes—a non-monotone, 2-adic schedule that accelerates gradient descent—and show how to compose it with two-point zeroth-order (ZO) estimators on a smoothed objective.\nWe apply Silver’s multi‑step Lyapunov analysis to smoothed objectives and show that it carries over verbatim when gradients are replaced by unbiased two‑point estimators with a tax in the form of a quadratic variance term.\nWe control this term via an orthogonal-on-spikes batching policy that allocates directions proportionally to the Silver steps (with a cap at dimension), achieving budget-optimal variance aggregation. \nEmpirically, we validate our approach through both numerical experiments and MeZO-style forward-pass-only fine-tuning of large language models, incorporating practical considerations such as clipping strategies, and demonstrate its superior performance.", "tldr": "", "keywords": ["Zeroth-Order Optimization", "Silver Stepsize", "Gradient-Free"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d21d0aa5dab5415004d9ad097cad12d34bc893c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new stepsize scheduling approach for solving the smooth convex optimization problem, called Silver stepsize. This method is claimed to be faster than standard approaches. Multiple empirical experiments are taken to validate the result."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The experimental results seem to have significant improvements; that is, the proposed method indeed improves the given baseline. Both synthetic and practical experiments are considered."}, "weaknesses": {"value": "This paper is apparently incomplete. It shouldn't have been submitted.\n\nMinor: It has unlabeled reference on page 2: \".Section??developstheinexact-gradient\". \n\n1. The author didn't list any assumptions and didn't explain notations and the problem setup. \n2. The author didn't include any proof for its lemmas and theorems. No additional supplymentary materials or appendices provided. Only proof sketch. \n3. This paper only considers the smooth convex case, which has been well explored. \n4. The Silver stepsize is not new. The author doesn't justify why it is suitable for gradient-free optimization."}, "questions": {"value": "The current submission is not complete, as no formal proof provided for the main theoretical results. I hope the author include these materials in another submission. For the current one, I suggest rejecting this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EiYwqyJvfO", "forum": "P8EhH6ypA5", "replyto": "P8EhH6ypA5", "signatures": ["ICLR.cc/2026/Conference/Submission25582/Reviewer_27zL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25582/Reviewer_27zL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539742007, "cdate": 1760539742007, "tmdate": 1762943485674, "mdate": 1762943485674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new stepsize scheduling strategy, termed the Silver Stepsize, for accelerating zeroth-order (ZO) optimization. The idea is inspired by a “Silver identity” and involves running blocks of updates called “Silver blocks,” with the goal of improving convergence under limited query budgets. The authors claim that their method achieves better performance than existing ZO optimizers such as MeZO."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* Interesting attempt to improve ZO learning rates: The idea of constructing structured step-size schedules or blocks to reduce variance in ZO methods could be valuable if properly formalized and justified.\n* Potential theoretical contribution: The paper hints at a nontrivial theoretical structure (“Silver identity” and block recursion), which, if made rigorous, could yield new insights for ZO optimization."}, "weaknesses": {"value": "$\\textbf{Logical gaps and unclear derivations}$\n\n* line-163: The derivation of $r_{k}$ is not explained. Simply stating “with explicit” provides no help to the reader or reviewer. It appears that crucial steps are omitted, leaving the correctness unverifiable.\n\n* line-171: The phrase “by the Descent Lemma” is used without specifying which lemma. If it refers to the classical descent lemma for smooth functions, please state it explicitly\n\nOverall, the logic jumps too much, and  key steps are skipped or implied without justification.\n\n---\n\n$\\textbf{Not self-explanatory terminology}$\n* The manuscript assumes readers already understand concepts such as stepsize hedging (Silver), Silver identity, and Silver block, yet none of these are properly defined or cited.\n* For example, Line 145: “we run a Silver block of length $N =2^{k} - 1$\", this is not interpretable without knowing what a “Silver block” is or how it is constructed.\n* The text thus becomes inaccessible to readers who have not read the previous work. The lack of self-contained definitions makes the paper confusing and non-reviewable.\n\n---\n\n$\\textbf{Inconsistent and unclear notation}$\n* Line 131 uses $\\mu v$ but later Line 135 and beyond switch to $\\mu u$ without explanation.\n* The notation $\\text{St}(d, B_{t})$ is undefined. If this represents a Stiefel manifold (orthonormal columns of $V_{t}$, this must be explicitly stated.\n* These inconsistencies make the technical arguments hard to follow and the results unverifiable.\n\n$\\textbf{Experimental evaluation}$\n* $\\textbf{Unfair comparison:}$ \\\nThe proposed method and MeZO differ substantially in computational cost per training step. Reporting results per iteration rather than per query makes the comparison misleading. For ZO methods, query complexity (number of function evaluations) is the standard metric.\n* $\\textbf{Limited baselines and datasets:}$ \\\nThe paper only reports results on small, toy-like problems. Many recent ZO fine-tuning methods show significant improvements over MeZO but are not included for comparison.\n* $\\textbf{Hyperparameter fairness:}$ \\\nFor MeZO, the learning rate is taken directly from the proposed method’s schedule without tuning. A proper grid search for the best learning rate should have been conducted."}, "questions": {"value": "$\\textbf{Algorithmic clarity}$\n\n* Line-4 of Algorithm 1: Sampling mutually orthonormal columns $V_{t}$ can be computationally expensive, comparable to or exceeding a forward pass for deep models. The paper should discuss this cost explicitly.\n* Line-3 of Algorithm 1: How is $L$ estimated or set, and how is $\\alpha_{t}$ chosen? These hyperparameters are not explained.\n* The number of additional hyperparameters introduced compared to MeZO should also be clearly listed.\n\n---\n\n$\\textbf{Theoretical concerns}$\n\n* $\\textbf{Theorem 4.5 and Lemma 4.9}$: Both claim upper bounds, but can the upper bound potentially be arbitrarily small?  If so, please justify.\n* $\\textbf{Lemma 4.9}$: If I understand it correctly, the upper bound appears $d$-dependent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "soG0cvSV3i", "forum": "P8EhH6ypA5", "replyto": "P8EhH6ypA5", "signatures": ["ICLR.cc/2026/Conference/Submission25582/Reviewer_oiV7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25582/Reviewer_oiV7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760937452985, "cdate": 1760937452985, "tmdate": 1762943485428, "mdate": 1762943485428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the use of the Silver stepsize schedule, an explicit fractal non-monotone stepsize sequence, to accelerate zeroth-order (gradient-free) optimization of smooth convex functions. The authors extend the multi-step Lyapunov analysis underlying the Silver schedule to settings using unbiased two-point zeroth-order estimators on smoothed objectives, specifically employing an \"orthogonal-on-spikes\" batching policy to optimally control estimator variance under fixed query budgets. Theoretical results show that the Silver identity remains valid with a quadratic variance cost, and practical experiments—including fine-tuning of large language models (RoBERTa-large) using MeZO-style forward-pass-only schemes—demonstrate improved convergence and stability relative to constant stepsize baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper rigorously adapts the Silver stepsize analysis to the stochastic, zeroth-order regime and transparently derives error bounds using the Silver identity, variance-optimal batching, and martingale concentration.\n\n2. The technical arguments are compact and accessible, with careful attention paid to unbiasedness properties (Lemma 4.2, Lemma A.2) and precise variance analysis (Appendix B). The paper includes direct proofs or proof sketches for all major claims, including second-moment bounds and bias calculations.\n\n3. The paper provides convincing empirical results on synthetic convex problems and on forward-only fine-tuning of LLMs, demonstrating more stable convergence and lower validation loss with the Silver schedule compared to constant stepsizes."}, "weaknesses": {"value": "1. Limited empirical benchmarking breadth and depth.\nWhile the reported results demonstrate robust improvements on both synthetic convex problems and RoBERTa-large fine-tuning, the empirical evaluation remains relatively narrow in scope. The LLM experiments are restricted to 16-shot fine-tuning on only two GLUE tasks (SST-2 and RTE) and a single model architecture. Moreover, Figure 3 reports only the fine-tuning loss curves, which are insufficient to substantiate the claimed performance gains. Since MeZO serves as one of the baselines for this work, the authors should at least present direct comparisons on downstream task metrics—as done in the original MeZO paper—rather than solely showing loss trajectories. Additionally, it would strengthen the paper to include experiments on other model scales or architectures such as GPT-2, LLaMA, or the OPT family used in MeZO.\n\n2. Incompleteness of variance-reduction and acceleration baselines.\nIn the experiments on synthetic convex problems, the authors compare primarily against standard ZO-GD, which is insufficient to convincingly demonstrate the effectiveness of the proposed method. Although I am not deeply familiar with the specific two-point ZO estimation paradigm, it seems that the paper should include comparisons with more advanced accelerated or variance-reduced zeroth-order optimizers—for example, algorithms that incorporate momentum, adaptive smoothing, or gradient-tracking techniques. Such baselines would provide a stronger empirical validation of the claimed acceleration and variance-control benefits.\n\n3. Minor errors. The reference at line 88 (“section.??”) did not compile correctly.\n\n4. Missing references.\nZO optimization has recently attracted increasing attention for its suitability in memory-constrained large language model fine-tuning, due to its gradient-free nature. Although most recent ZO methods adopt multi-point estimators and are not directly comparable to the approach proposed in this paper, the authors should still pay attention to recent developments (e.g., [1, 2, 3]) and refer to their experimental settings to strengthen and contextualize the empirical section.\n\n[1]. Wang X, Qin X, Yang X, et al. Relizo: Sample reusable linear interpolation-based zeroth-order optimization[J]. Advances in Neural Information Processing Systems, 2024, 37: 15070-15096.\n\n[2]. Shu Y, Zhang Q, He K, et al. Refining adaptive zeroth-order optimization at ease[J]. arXiv preprint arXiv:2502.01014, 2025.\n\n[3] Dang S, Guo Y, Zhao Y, et al. FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed[J]. arXiv preprint arXiv:2506.09034, 2025."}, "questions": {"value": "1. The method is evaluated with two-point ZO estimators. Would the Silver schedule and the “orthogonal-on-spikes” batching strategy generalize to multi-point estimators? If not, could the authors elaborate on the challenges?\n\n2. Can the authors provide or cite additional results on standard accuracy metrics (e.g., classification accuracy, F1) for the LLM experiments in Section 5.2? In addition, it would be valuable to include evaluations on a broader set of benchmarks and across different model architectures to better assess the generality of the proposed method. \n\n3. Could the authors provide a more detailed ablation or empirical study on the influence of block length, smoothing radius schedule ($\\mu$), and step size clipping on both convergence and final solution quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IQvz63ykBU", "forum": "P8EhH6ypA5", "replyto": "P8EhH6ypA5", "signatures": ["ICLR.cc/2026/Conference/Submission25582/Reviewer_XZvk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25582/Reviewer_XZvk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898342661, "cdate": 1761898342661, "tmdate": 1762943485228, "mdate": 1762943485228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ZO-Silver, a zeroth-order (ZO) optimization method that combines the Silver stepsize schedule with two-point gradient estimators on a smoothed objective. The authors adapt the multi-step Lyapunov analysis of the Silver schedule to the ZO setting and introduce an \"orthogonal-on-spikes\" batching strategy that allocates query budgets proportionally to step sizes. Theoretically, they show that the method retains the accelerated convergence rate of Silver stepsizes up to a variance term, which is optimally controlled via their batching policy. Empirically, they validate the approach on synthetic problems and in a MeZO-style fine-tuning setting with large language models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "++ The work combines stepsize scheduling and zeroth-order optimization, and offers a practical, memory-efficient method for fine-tuning large models where backpropagation is infeasible. The high-probability analysis via Freedman’s inequality is also a nice addition."}, "weaknesses": {"value": "-- The novelty is incremental since the core idea, applying Silver stepsizes to ZO, is a straightforward combination of two existing techniques. While the batching policy is new, the overall conceptual leap is modest.\n\n-- The experiments are relatively narrow. Synthetic tests are limited to ridge regression and logistic regression, and the LLM fine-tuning is only shown on RoBERTa-large for two GLUE tasks. I suggest the authors conduct experiments on more optimization tasks.\n\n-- The baselines are limited. I suggest the authors compare with more ZO optimizers."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8VUpk2K3hD", "forum": "P8EhH6ypA5", "replyto": "P8EhH6ypA5", "signatures": ["ICLR.cc/2026/Conference/Submission25582/Reviewer_zM9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25582/Reviewer_zM9e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993699964, "cdate": 1761993699964, "tmdate": 1762943484001, "mdate": 1762943484001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}