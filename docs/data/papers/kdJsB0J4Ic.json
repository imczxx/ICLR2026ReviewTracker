{"id": "kdJsB0J4Ic", "number": 16557, "cdate": 1758266017115, "mdate": 1759897232901, "content": {"title": "Structure Learning from Time-Series Data with Lag-Agnostic Structural Prior", "abstract": "Learning instantaneous and time-lagged causal relationships from time-series data is essential for uncovering fine-grained, temporally-aware interactions. Although this problem has been formulated as a continuous optimization task amenable to modern machine learning methods, existing approaches largely neglect the use of coarse-grained, lag-agnostic causal priors, an important form of prior knowledge that is often available in practice. To address this gap, we propose a novel framework for structure learning from time series to integrate lag-agnostic priors, enabling the discovery of lag-specific causal links without requiring precise temporal annotations. We introduce formulations to precisely characterize the lag-agnostic prior, and demonstrate their consequential and process-equivalence to priors, maintaining consistency with the intended semantics of the priors throughout optimization. We further analyze the challenge for optimization due to the increased non-convexity by lag-agnostic prior constraints, and introduce a data-driven initialization to mitigate this issue. Experiments on both synthetic and real-world datasets show that our method effectively incorporates lag-agnostic prior knowledge to enhance the recovery of fine-grained, lag-aware structures.", "tldr": "This paper introduces how to use lag-agnostic prior, commonly available knowledge, to guide the discovery of lag-aware causal interactions from time-series data in the continuous optimization framework.", "keywords": ["Continuous DAG structure learning", "dynamic causal discovery", "structure learning from time series data"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5880c4af4c9ccc6e34238ac5dcc5972b7af8bbd9.pdf", "supplementary_material": "/attachment/e448c9d387f4fb305ebce025354a0fc9c3b2d0bf.zip"}, "replies": [{"content": {"summary": {"value": "This paper considers  the integration of coarse-grained lag-agnostic causal priors. \nThe main claim is that lag-agnostic priors can enable the discovery of lag-specific causal links.\nThe main effort of the paper is put on how to integrate such prior during the practical optimization procedure, and it provides both theoretical and empirical analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- lag-agnostic structural priors is an interesting formulation, and it can be useful in practice\n- The theoretical analysis is clear and rigorous. It reveals the challenges during the optimization and why the proposed  logic-dual Formulation is essential."}, "weaknesses": {"value": "- In Section 4.2 and Figure 1, it would be more comprehensive if a baseline using equation (9) can be added. Such empirical results would further justify the discussion about process-equivalent approach.\n- The concrete research problem is not sufficiently introduced. For example, the concept of \"process equivalence\" lacks a more formal definition. I suggest a minor adjustment to emphasize this part."}, "questions": {"value": "- What is the connection and differences between the Lag-Agnostic Structural Prior and those in the related work? Especially the \"order\" of causal variables, like Partial Orders [1] and Causal Orders [2]. I suggest an additional discussion.\n- Please consider improving the notation about $\\Theta\\_{ij,s}\\:=\\\\{\\\\theta\\\\mid \\|(W\\_s(\\\\theta))\\_{ij}\\| \\\\geq \\\\delta \\\\}$. Readers may expect $\\\\Theta\\_{ij,s}(0)\\:=\\\\{\\\\theta\\\\mid\\|(W\\_s(\\\\theta))\\_{ij}|\\\\geq 0\\\\}$, which does not match the actual definition of $\\Theta_{ij,s}(0)$.\n\n[1] Differentiable Structure Learning with Partial Orders\n\n[2] Causal Order: the Key to Leverage Imperfect Experts in Causal Inference"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jxX98nGvD5", "forum": "kdJsB0J4Ic", "replyto": "kdJsB0J4Ic", "signatures": ["ICLR.cc/2026/Conference/Submission16557/Reviewer_54Cj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16557/Reviewer_54Cj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569964065, "cdate": 1761569964065, "tmdate": 1762926638042, "mdate": 1762926638042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of causal structure learning from time-series data when only lag-agnostic prior knowledge is available. The authors propose a continuous optimization framework that integrates such priors into time-series structure learning. The paper first identifies the process-inequivalence issue in naive maximum-based formulations for lag-agnostic priors, which causes bias toward specific lags during optimization (Section 3.2, Proposition 1). To address this, it introduces two process-equivalent formulations, a binary-masked formulation (Eq. 10) and a logic-dual formulation (Eq. 11), that preserve the semantics of lag-agnostic priors throughout optimization (Section 3.3). The authors further analyze the non-convexity induced by lag-agnostic constraints and propose a data-driven initialization strategy (Section 3.4) to mitigate convergence to poor local optima. Finally, through comprehensive experimental validation on synthetic data, non-linear and non-stationary datasets (using LIN and RHINO backbones), and real-world DREAM4 gene regulatory networks (Sections 4.1–4.4), the paper demonstrates that the proposed framework improves causal recovery and stability compared to both data-only and lag-specific prior methods, particularly when temporal information is noisy or incomplete."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The notion of lag-agnostic structural priors is novel and fills a clear gap between lag-specific causal discovery (e.g., Sun et al., 2023) and coarse-grained prior-based static structure learning (e.g., Zheng et al., 2018). The formal distinction between consequence equivalence and process equivalence is particularly insightful and provides new conceptual clarity (Proposition 1–4, Section 3).\n\nTechnical quality: The theoretical analysis is rigorous. The proofs of process-equivalence (Appendix B) and the illustration of increased non-convexity via Example 1 are convincing. The data-driven initialization strategy is well-motivated and empirically validated.\n\nClarity: The paper is well-structured, with clear notation and well-separated sections. Figures 1–2 and Tables 1–2 (pages 8–9) are clear and they effectively support the claims.\n\nSignificance: The work provides a general, modular mechanism that can be integrated into various differentiable structure learning frameworks (DYNOTEARS, LIN, RHINO). The experiments demonstrate consistent improvement across backbones, suggesting wide applicability."}, "weaknesses": {"value": "Scalability considerations: The computational complexity of the binary-masked and logic-dual penalties is not analyzed. As both require operations across all lags and node pairs, their efficiency and scaling to large graphs (e.g., d > 100) remain unclear.\n\nInterpretability of lag selection: Although the process-equivalent formulations prevent early bias, the final lag assignments are driven primarily by data fitting. The paper could elaborate more on how reliably the method identifies the true lag rather than simply satisfying priors (discussion in Section 3.3).\n\nAblation clarity: While Appendix E reportedly contains ablations, the main text provides limited discussion of which component (formulation type vs. initialization) contributes most to performance gains."}, "questions": {"value": "Complexity and scalability: How does the proposed penalty (especially the product-based logic-dual term) scale with increasing lag length and variable count?\n\nInitialization sensitivity: How sensitive is the method to the choice of unconstrained pre-training in Eq. (14)? Does using different unconstrained learners (e.g., VAR vs. neural backbones) affect performance?\n\nInterpretability of lag selection: Can the authors quantify how often the method identifies the correct lag (when available) rather than merely satisfying the lag-agnostic constraint?\n\nPractical deployment: Are there specific guidelines for tuning λₚ when validation ground truth is unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lem6gWzAho", "forum": "kdJsB0J4Ic", "replyto": "kdJsB0J4Ic", "signatures": ["ICLR.cc/2026/Conference/Submission16557/Reviewer_2t4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16557/Reviewer_2t4w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737475090, "cdate": 1761737475090, "tmdate": 1762926637563, "mdate": 1762926637563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses structure learning for multivariate time series when practitioners have coarse, lag-agnostic causal priors (edge exists but unknown lag). Authors show a straightforward \"max over lags\" penalty is consequentially equivalent to the desired prior but process inequivalent (induces early bias to a single lag). They propose two process-equivalent penalties that act across all lags: 1) Binary-masked loss $p_\\text{bin}$: activates only when all lag-specific edges are below threshold, then pushes them jointly; 2) Logic-dual/product loss $p_\\text{or}$: product of ReLU terms (OR semantics), with a normalization to reduce scale sensitivity. They also argue lag-agnostic constraints increase non-convexity and propose a two-stage, data-driven initialization.\nExperiments on synthetic VAR graphs (ER-k, Gaussian/Exponential noise), nonlinear/non-stationary backbones (LIN, RHINO), and DREAM4 show consistent gains in SHD/F1/AUROC and lower regression loss vs data-only baselines and vs lag-specific priors with wrong lags."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Lag-agnostic prior knowledge is common; formalizing it is useful.\n- Simple, plug-in losses applicable to multiple backbones (DYNOTEARS, LIN, RHINO).\n- Initialization story is well-motivated.\n- Broad evaluation"}, "weaknesses": {"value": "- Propositions establish equivalence of penalties but there's no optimization-theoretic guarantee (e.g., convergence to a correct lag under identifiability conditions).\n- The non-convexity example is illustrative but small; more formal landscape analysis would strengthen claims.\n- The product loss can suffer vanishing gradients when many lags are near but below $\\delta$; the normalization helps but may not fully address scale with larger L.\n- How robust are results to noisy/incorrect presence and incorrect absence priors (false positives/negatives in $C_p$, $C_a$)?\n- Baselines focus on NOTEARS-family and a “random-lag” variant. Important time-series causal methods like DYNOTEARS with group-sparsity across lags isn't compared.\n- No comparison to softmax/log-sum-exp surrogates for max (temperature-controlled) which are a natural alternative to address process-inequivalence."}, "questions": {"value": "-  Any empirical comparison to LSE or Gumbel-softmax over lags?\n- What happens when $C_p$ contains 20-40% spurious pairs or $C_a$ wrongly forbids true edges?\n- For $p_\\text{or}$, how often do you observe near-zero gradients early? Does the normalization fully fix it as L grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4fAmpZIINB", "forum": "kdJsB0J4Ic", "replyto": "kdJsB0J4Ic", "signatures": ["ICLR.cc/2026/Conference/Submission16557/Reviewer_rVC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16557/Reviewer_rVC5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930801732, "cdate": 1761930801732, "tmdate": 1762926636986, "mdate": 1762926636986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lag-agnostic prior constraint for incorporating prior knowledge into causal discovery algorithms for time series data. The authors highlight the drawback of outcome-equivalent constraints like maximum-based prior and theoretically justify the effectiveness of their process-equivalent prior. Empirically, the paper shows the effectiveness of their method in incorporating the provided prior information."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well-written and easy to read.\n2. The theoretical results are well-motivated and clearly stated. The examples are well-constructed to illustrate the point that the authors are trying to convey.\n3. The empirical results are extensive, and include several ablation studies.\n4. The authors tackle an important problem that is often overlooked. Priors coming from domain-experts are almost always lag-agnostic so it's nice to see a paper that tackles this challenge."}, "weaknesses": {"value": "1. Although Proposition 1 is illustrative, it is proven under a very strong assumption, i.e. $\\nabla{|(W_\\tau)_{ij}|} \\mathcal{L} \\geq 0$ for all $\\tau$. This is unrealistic, since many edges in real-world applications are, in fact, not forced to 0 by the data. It is unclear whether the principle being illustrated would still hold in such cases.\n\n2. The main experiments in Section 4 do not include the outcome equivalent baseline \"maximum-based formulation\". Although the authors report some experiments in Appendix E.2, the difference between the two methods seems quite small (especially for the init data setting). This is an important baseline that the authors should consider including in Table 1, Figure 1 and Table 2.\n\n3. A minor weakness is that practical sources and reliability of the availability of such lag-agnostic priors could be discussed more concretely, especially in the motivation."}, "questions": {"value": "1. Do the authors have intuition for why data driven initialization works?\n2. As noted in the limitations section, the model can go wrong due to incorrect priors. How sensitive is the model to incorrect priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fPUkC2Ga6N", "forum": "kdJsB0J4Ic", "replyto": "kdJsB0J4Ic", "signatures": ["ICLR.cc/2026/Conference/Submission16557/Reviewer_tkmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16557/Reviewer_tkmz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963838510, "cdate": 1761963838510, "tmdate": 1762926636570, "mdate": 1762926636570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}