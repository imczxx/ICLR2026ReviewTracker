{"id": "qcFbW8bstj", "number": 9914, "cdate": 1758148659428, "mdate": 1759897686781, "content": {"title": "Training-Free Stylized Abstraction", "abstract": "Stylized abstraction synthesizes visually exaggerated yet semantically faithful representations of subjects, balancing recognizability with perceptual distortion. Unlike image-to-image translation, which prioritizes structural fidelity, stylized abstraction demands selective retention of identity cues while embracing stylistic divergence, especially challenging for out-of-distribution individuals. We propose a training-free framework that generates stylized abstractions from a single image using inference-time scaling in vision-language models (VLLMs) to extract identity-relevant features, and a novel cross-domain rectified flow inversion strategy that reconstructs structure based on style-dependent priors. Our method adapts structural restoration dynamically through style-aware temporal scheduling, enabling high-fidelity reconstructions that honor both subject and style. It supports multi-round abstraction-aware generation without fine-tuning. To evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric suited for abstract styles where pixel-level similarity fails. Experiments across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong generalization to unseen identities and styles in a fully open-source setup.", "tldr": "", "keywords": ["Diffusion Model", "Training Free", "VLLM"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c87d1b9843f9fb8e934551780e742eb44e0f38e1.pdf", "supplementary_material": "/attachment/bf33e2ffd89473f3a2b859efb930ebb1ae9734ab.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a training-free framework for stylized abstraction that leverages inference-time scaling in VLLMs and a novel cross-domain rectified flow inversion to generate identity-faithful yet highly stylized representations from a single image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core Innovations are as follows:\n\n1. Training-Free Identity-Preserving Abstraction: A novel framework that utilizes inference-time feature scaling in Vision-Language Models (VLLMs) to extract and preserve critical identity cues without any fine-tuning.\n\n2. Cross-Domain Rectified Flow Inversion: An innovative inversion strategy that reconstructs semantic structure by leveraging style-dependent priors, effectively bridging the domain gap between realism and abstraction.\n\n3. Dynamic Style-Aware Reconstruction: A flexible temporal scheduling mechanism that adaptively controls structural restoration based on style intensity, enabling high-fidelity results across varying abstraction levels.\n\n4. Specialized Evaluation Metric: The introduction of StyleBench, a GPT-based human-aligned evaluation metric specifically designed to assess abstract stylization where traditional pixel-level metrics fail."}, "weaknesses": {"value": "1. The human evaluation setup for StyleBench is a weakness. The assessment of only 25 generated images is too small a sample size to robustly validate the metric's alignment with human judgment. Furthermore, the methodology lacks crucial details on how annotations from the 15 evaluators were aggregated, particularly in cases of disagreement, which undermines the reliability of the reported results.\n\n2. The framework's capability to preserve fine-grained identity attributes appears limited. As observed in Figure 3, the results for styles like 'knitted', 'anime', and 'chibi' show substantial identity divergence from the reference. This raises concerns about the effectiveness of the VLLM in parsing and retaining subtle but critical identity features (e.g., facial moles).\n\n3. There is a potential factual inaccuracy in the presentation: the image for the 'Barbie doll' style in Figure 7 appears to be incorrect, as it does not visually match the stated style, which could mislead readers."}, "questions": {"value": "1. Regarding the human evaluation, could you specify the exact protocol for processing the annotations from the 15 evaluators? For instance, was a measure of inter-annotator agreement (e.g., Cohen's Kappa) calculated, and how were final scores derived from potentially divergent ratings?\n\n2. What is the granularity of the attributes that the VLLM-based parser can reliably extract? Can it capture very fine details, such as specific facial marks, and if not, how does this limitation impact identity preservation for out-of-distribution subjects?\n\n3. Regarding the cross-domain rectified flow inversion, are there any constraints on the selection of the stylized input? How does the method perform when there is a very large domain gap between the real reference image and the chosen stylized input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ac8Wpl0tx", "forum": "qcFbW8bstj", "replyto": "qcFbW8bstj", "signatures": ["ICLR.cc/2026/Conference/Submission9914/Reviewer_DUXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9914/Reviewer_DUXZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641669926, "cdate": 1761641669926, "tmdate": 1762921371287, "mdate": 1762921371287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defines a new task in the domain of image transfer called stylized abstraction. The authors first proposes a training free pipeline that distills identity with an inference time VLLM loop that produces T5 and CLIP prompts which is fed iteratively into a t2i generation image generation model, and a cross domain rectified flow inversion with temporal scheduling to inject structural fidelity. They also introduce stylebench, a GPT assisted evaluation method for abstraction. The results show qualitative results that adhere to the coined term \"stylized abstraction\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors has a clear problem framing that distinguishes abstraction from classic style transfer, where they preserve identity while allowing large style driven distortions.\n- Their proposed stylebench benchmark explicitly targets abstraction (style adherence + identity + fusion), and also includes the classic KID/CLIPscore to show that these classic metrics are positively correlated with their proposed stylebench benchmark. Including a human preference study in qualitative-heavy work is also appreciated."}, "weaknesses": {"value": "- The main contribution of this work is less technical nor theoretical, but heavily leans into the domain of prompt engineering. The qualitative results do look impressive and I do agree with the authors' formulation of the new task of \"stylized abstraction\", in order for a work of this nature to be given a high rating the contributions has to go beyond ad-hoc tips and needs to show generality; automation, theory, or rigorous evaluation.\n- The main contribution of the proposed workflow seems to be model independent firsthand, but authors do not show any other ablation studies with different models. It is noted that most open source VLM models do use either T5 or CLIP based encoders, but the proposed method hardcodes the number of tokens into their main text (e.g. 77 for CLIP), which limits the generality of the method. \n- There is limited to no analyses explaining why certain parts of their methods work, either on an engineering or theoretical level. There is no ablation study that isolate each component's contribution that go beyond select figures/tables."}, "questions": {"value": "- any additional ablations showing the strength of the proposed method (e.g. the select choice of attribute extraction and prompt compression) isolated from the given backbone models (InternVL/Flux) would be appreciated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gagp2AxwNv", "forum": "qcFbW8bstj", "replyto": "qcFbW8bstj", "signatures": ["ICLR.cc/2026/Conference/Submission9914/Reviewer_fZo6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9914/Reviewer_fZo6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735768755, "cdate": 1761735768755, "tmdate": 1762921370881, "mdate": 1762921370881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free framework for stylized abstraction. This task aims to generate stylized yet identity-preserving representations of individuals in highly abstract visual styles. The method first distills identity-relevant semantic attributes from the input image using VLLM. Then it employs a cross-domain latent inversion process based on rectified flows to reconstruct stylized representations while preserving core identity cues. The authors further propose StyleBench, a GPT-based evaluation protocol that measures abstraction quality more effectively beyond pixel-space similarity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces stylized abstraction as a meaningful application task.\n- Introducing StyleBench addresses an evaluation gap in stylization tasks, where pixel-based metrics are not well-suited.\n- The qualitative results are compelling, demonstrating visually coherent abstractions across diverse styles and subjects."}, "weaknesses": {"value": "- The identity distillation step is largely prompt engineering, while the stylization and reconstruction pipeline is built on RF-Inversion. The contribution feels more like combining existing components than introducing new algorithmic insights.\n\n- The method explanation is difficult to follow. Figure clarity and pipeline grounding are lacking. For example, in Figure 2, the transition between generated outputs and stylized inputs is not clearly explained.\n\n- The computational cost appears significantly higher compared to alternative training-free personalization/editing approaches, but no runtime or resource comparison is provided."}, "questions": {"value": "- The overall method explanation is difficult to follow. In particular, Figure 2 does not adequately convey the whole pipeline. For example, in Fig. 2(a), the output of the generation stage becomes the stylized input in Fig. 2(b), but this transition is not explicitly described. Providing a more step-by-step procedural diagram or pseudocode would significantly improve clarity.\n\n- The evaluation would be stronger with comparisons to recent open-source T2I editing models, such as Flux-Kontxt or Qwen-Image-Edit. Additionally, even if the performance differs, including comparisons to GPT or Gemini outputs would help contextualize the practicality of the method in real-world creative workflows.\n\n- The proposed pipeline relies on iterative prompt refinement and multiple VLLM interactions, which likely increases computational overhead compared to prior training-free approaches. Therefore, it would be essential to include a runtime and resource comparison, especially against training-free baselines, to assess practical feasibility.\n\n- A failure case analysis would also be valuable for understanding the limitations of the proposed method. For instance, if the T2I model fails to preserve identity even when the textual description is accurate, how does the pipeline react? Explicit discussion of such scenarios would clarify the strengths and limitations of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMz4bREqda", "forum": "qcFbW8bstj", "replyto": "qcFbW8bstj", "signatures": ["ICLR.cc/2026/Conference/Submission9914/Reviewer_jPno"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9914/Reviewer_jPno"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906796194, "cdate": 1761906796194, "tmdate": 1762921369522, "mdate": 1762921369522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors define the task of stylized abstraction, that is generating images that exaggerate or simplify appearance while keeping a target subject recognizably the same. This is distinct from classic style transfer, which aims to significantly retain structure. The introduced task is very interesting and the authors also propose a dedicated benchmark/evaluation protocol (StyleBench).\n\nTo solve this task, the authors propose a training-free pipeline that uses multiple V(L)LMs, foundation models like CLIP, and rectified-flow models such as FLUX. First, the authors distill the subject’s identity into natural language by prompting a VLLM multiple times. Next, they regenerate images based on these descriptions and compare them with the original image until a CLIP threshold is reached. Finally, they apply a style-aware prompt transformation to produce stylized prompts.\n\nIn the second stage, the authors invert the stylized image using cross-domain rectified-flow inversion with a style-aware temporal schedule, selectively restoring structure while preserving the intended style.\n\nThe results, both quantitative and qualitative, show that the method performs significantly better than traditional style transfer pipelines on the task of stylized abstraction.\n\nOverall, the method is elegant and the task is interesting; however, I’m unsure whether the heavy usage of pretrained models delivers sufficient methodological novelty for ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The task introduced is interesting. It clearly distinguishes stylized abstraction from classic style transfer and motivates why it matters.\n\n* The presentation of the paper is also nice and clear. The paper is easy to follow, and the visual results strongly support the authors’ claims.\n\n* The method, although it leverages many off-the-shelf models, is intuitive and elegant. The authors also show ablation studies on the effect of cross domain latent reversal, the multi-turn identity distillation and prompt stylization.\n\n* The authors also contribute a benchmark for stylized abstraction, albeit limited."}, "weaknesses": {"value": "1. The evaluation set and human study are pretty narrow to support strong claims. Only 15 people where used on 25 generated images.\n\n2. While the pipeline is elegant, the contribution to ICLR is not very clear, since the authors mainly make usage of large pretrained models.\n\n3. The heavy usage of LVLMs likely has a high inference cost. It would be good if the authors actually mention runtimes."}, "questions": {"value": "* What is the inference cost of the method? How long does it take to obtain one final result ? \n* What are the limitations of your method ?\n* Will the stylebench benchmark be made available for other researchers ?\n\nOverall, my main concern with the paper is that it is mostly a composition of pretrained models and the novelty for ICLR is not apparent, considering also the limited size of the experimental dataset. I would like the authors to comment on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qiTgSMzYpn", "forum": "qcFbW8bstj", "replyto": "qcFbW8bstj", "signatures": ["ICLR.cc/2026/Conference/Submission9914/Reviewer_Kva8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9914/Reviewer_Kva8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128067026, "cdate": 1762128067026, "tmdate": 1762921368607, "mdate": 1762921368607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}