{"id": "FF2Lbu9U6Y", "number": 22639, "cdate": 1758333826568, "mdate": 1759896855441, "content": {"title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models", "abstract": "Large language models have made significant progress in complex but easy-to-\nverify problems, yet they still struggle with discovering the unknown. In this\npaper, we present AlphaResearch, an autonomous research agent designed to dis-\ncover new algorithms on open-ended problems by iteratively running the follow-\ning steps: (1) propose new ideas (2) program to verify (3) optimize the research\nproposals. To synergize the feasibility and innovation of the discovery process,\nwe construct a new reward environment by combining the execution-based verifi-\nable reward and reward from simulated real-world peer review environment. We\nconstruct AlphaResearchComp, a new evaluation benchmark that includes an\neight open-ended algorithmic problems competition, with each problem carefully\ncurated and verified through executable pipelines, objective metrics, and repro-\nducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison\nwith human researchers. Notably, the algorithm discovered by AlphaResearch on\nthe “packing circles” problem achieves the best-of-known performance, surpass-\ning the results of human researchers and strong baselines from recent work (e.g.,\nAlphaEvolve). Additionally, we conduct a comprehensive analysis of the bene-\nfits and remaining challenges of autonomous research agent, providing valuable\ninsights for future research.", "tldr": "We propose AlphaResearch, an autonomous research agent designed to discover out-of-boundary algorithms.", "keywords": ["large language models", "AI for research"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46cb76418dabdb46ac2bc1456b75b36d450c1728.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AlphaResearch, an LLM-based Agent system. Unlike past works that solve problems with pre-defined human answers (e.g., HLE, GAIA), this system attempts to tackle sub-fields that humans have not yet explored. AlphaResearch employs a two-stage process for algorithm discovery, which includes simulated verification with a Reward Model and real-world experimentation. The authors claim to have built an evaluation benchmark named AlphaResearchComp, upon which they conducted a detailed assessment.\n\nI have carefully examined the details of this paper and reviewed related works such as AlphaEvolve, OpenEvolve, and ShinkaEvolve."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is the first to introduce a two-stage verification process. It begins by training a Reward Model on real ICLR peer-review data. This model learns to simulate the preferences of real-world researchers, evaluating whether \"new ideas\" generated by the LLM possess sufficient novelty and feasibility. I appreciate this approach and suggest the authors emphasize its purpose more in the introduction (the current version may leave readers wondering why two verification stages are necessary). I believe this method can reduce the extra resources consumed in the second stage, for instance, by pre-emptively filtering out clearly erroneous examples to better allocate the costs of algorithm discovery."}, "weaknesses": {"value": "The core weakness of this paper is that its contribution is similar to a series of past works, making its improvements incremental.\n\n1.  To my knowledge, AlphaEvolve also employed the iterative process mentioned in line 14: \"(1) propose new ideas; (2) program to verify; (3) optimize the research proposals.\" Therefore, this cannot be considered a core contribution of this paper. As far as I can tell, only the \"reward from simulated real-world peer review environment\" is a novel point in this work, but the authors fail to emphasize it clearly, instead using vague language that conflates their work with AlphaEvolve.\n\n    For example, line 045 states: \"AlphaEvolve (Novikov et al., 2025) introduces an evolutionary coding agent that could tackle open scientific problems with program-based verification. However, the absence of real-world research environment rewards in coding-only agents (Tian et al., 2024) renders the discovery of out-of-boundary knowledge and algorithms challenging for current autonomous research agents.\"\n\n    This sentence is clearly misleading. I do not believe the \"However\" can be used to describe AlphaEvolve, as it did incorporate rewards from a real-world research environment. In fact, it is the work by Tian et al. (2024) that lacked real-world rewards.\n\n2.  The paper builds the AlphaResearchComp benchmark, but [OpenEvolve](https://www.google.com/search?q=https://github.com/codelion/openevolve/tree/main/examples) already includes dozens of different algorithm discovery environments. I believe this new benchmark offers little value, as most of its tasks are already covered in OpenEvolve's public examples.\n\n3.  The paper lacks a sufficient discussion and analysis of AlphaEvolve. I did not see a direct comparison, with the only mention appearing in line 313. I am unaware of the specific experimental settings, including time, cost, and number of iterations, but simply citing results from the original paper is inadequate. The original AlphaEvolve paper used the Gemini-2.0 model, whereas AlphaResearch uses o4-mini. I cannot discern the novelty or performance advantages of AlphaResearch from this. In the \"n=26 Packing Circles\" task, AlphaEvolve achieved 2.635 while AlphaResearch achieved 2.636; this improvement is so marginal that I suspect it could be due to randomness.\n\n4.  The paper is missing a comparison with a range of state-of-the-art baselines, such as OpenEvolve (a codebase that reproduces AlphaEvolve). I would expect the authors to provide adequate comparative experiments to substantiate their core claims.\n\n5.  The use of AlphaResearch-RM-7B is not well-justified. The tasks in AlphaResearchComp are mostly mathematical algorithms, with their human SoTA dating back 10-20 years. The paper provides insufficient evidence to support that a model trained on ICLR data (which mostly covers machine learning and neural networks) can make reasonable judgments on mathematical algorithm tasks. I even suspect that overfitting could be an issue, potentially making the fine-tuned RM less accurate on these math tasks than an untrained LLM."}, "questions": {"value": "The paper is also missing an experiment on cost versus performance. I am curious what the difference would be compared to AlphaResearch if the first-stage RM judgment was omitted. For instance, given the same amount of time or the same budget (API call fees), would AlphaResearch consistently outperform the version without the RM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "csszoAAcxD", "forum": "FF2Lbu9U6Y", "replyto": "FF2Lbu9U6Y", "signatures": ["ICLR.cc/2026/Conference/Submission22639/Reviewer_orst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22639/Reviewer_orst"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761064605546, "cdate": 1761064605546, "tmdate": 1762942315496, "mdate": 1762942315496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AlphaResearch, an autonomous agent designed to discover novel algorithms beyond the frontier of human knowledge using Large Language Models (LLMs). The core methodological innovation is a **dual-reward environment** that aims to balance **innovation** by using a reward model (AlphaResearch-RM-7B) trained on real-world peer reviews, and **feasibility** through a rigorous, program-based execution environment. To evaluate this framework, the authors have constructed AlphaResearchComp, a benchmark suite of 8 open-ended algorithmic problems. The experimental results show that AlphaResearch outperforms human researchers on 2/8 problems, notably achieving a new state-of-the-art (SOTA) result on the **Packing Circles (n=32)** problem, surpassing both human records and the prior SOTA agent, AlphaEvolve.\n\nI affirm the importance of the research direction automated scientific discovery pursued in this paper, and find the proposed dual-reward architecture to be sound and well-motivated. However, the paper suffers from numerous shortcomings in its experimental analysis and comparative evaluation. The demonstrated performance gains are limited, suggesting that the work requires further refinement and revision."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed **dual-reward environment** is a significant and novel contribution. It directly addresses a fundamental challenge in automated scientific discovery: how to balance the pursuit of high-innovation value (proxied by simulated peer review) with the necessity of practical feasibility (ensured by program execution). This decoupled, sequential filtering framework is highly insightful.\n\n2. In NP-hard combinatorial optimization problems like **Packing Circles**, surpassing the SOTA is relatively difficult. The fact that AlphaResearch managed to improve upon a well-established human record and the prior SOTA agent (AlphaEvolve) is a powerful proof-of-concept that clearly demonstrates the potential of the proposed method.\n\n3. The construction of AlphaResearchComp, with its diverse set of problems and varied initialization strategies (e.g., from scratch, improving upon SOTA), is a valuable resource for the emerging field of automated algorithm discovery. It facilitates a more comprehensive assessment of an agent's capabilities across different research scenarios."}, "weaknesses": {"value": "1. A success rate of 2/8 (25%) suggests that the effectiveness of the proposed method may be limited and lacks strong generalizability. It is particularly concerning that the agent failed to make any improvements on the two tasks (**Littlewood polynomials** and **MSTD**) that started from the human-best solution. This strongly implies that AlphaResearch may lack the capability for fine-grained search and optimization within already highly optimized solution spaces.\n\n2. The ablation study reveals that the RM erroneously rejected 43 **viable** ideas, a false negative rate of 28.5% (43/151). For a system aimed at exploring the unknown frontier, prematurely discarding potentially viable paths at such a high rate is a critical risk.\n\n3. This is a core flaw in the study.The RM was trained on ICLR paper abstracts and their review scores, which represent **completed** and **validated** research. It is then applied to evaluate raw, unimplemented, and unrefined **ideas**. Critically, the ideas present in the training data (ICLR abstracts) are, by their nature, ones that have already been proven to be feasible and implementable. This creates a fundamental evaluative bias when the model is used to predict whether a newly generated idea can be successfully implemented by the agent.\n\n4. The paper equates **execution failure** with a **bad idea** and uses this to validate the RM's effectiveness. This is a significant methodological flaw. An execution failure could very well stem from the **limited capabilities of the LLM chosen for code implementation**. \n\n5. The paper's analysis of its experiments and parameters is insufficient. The number of iterations is a key hyperparameter, yet the paper's analysis is superficial. Figure 2 qualitatively shows gains over iterations, but the main results table (Table 4) fails to report the **specific number of iterations** or the **computational cost** required to reach the best solution for each of the 8 tasks.\n\n6. The paper fails to provide a sensitivity analysis for the RM's scoring threshold. How was this threshold determined? How would a more lenient or stringent threshold impact final performance and convergence speed?\n\n7. The paper does not compare its single-trajectory sampling mechanism for idea generation against AlphaEvolve's evolutionary approach. It is therefore impossible to know if the SOTA improvement comes from the dual-reward system or simply from a different search strategy.\n\nI want to reiterate that I agree the research direction of this paper is highly important, and the proposed dual-reward architecture is logical and insightful.\n\nHowever, the current paper is lacking in its empirical evaluation. The demonstrated performance advantage is not overwhelmingly persuasive, and more critically, the experimental analysis is severely insufficient regarding its core components, key hyperparameters, and agent behavior. This makes it difficult to form a complete understanding of the paper's contributions and limitations. Therefore, I recommend **Major Revision** to address the critical issues raised in the experimental evaluation, ablation studies, and methodological rigor."}, "questions": {"value": "1.  Why did the authors choose **04-mini**, a model with potentially limited coding capabilities, for the critical task of code implementation? How can the authors be certain that the 108 **execution failures** filtered by the RM were due to flawed ideas rather than flawed implementations? If a more powerful code-generation LLM (e.g., GPT-5, as cited in the paper) were used as the implementation agent, would the RM's **correct rejection rate** (71.5%) drop significantly?\n\n2.  How do the authors interpret the RM's high false negative rate of 28.5% (rejecting 43 viable ideas)? Is this an acceptable trade-off between efficiency and innovation? Was a qualitative analysis performed on these 43 erroneously rejected ideas? Do they systematically represent a class of high-risk, high-reward, but **non-ICLR-sounding** innovative paths?\n\n3.  Could the authors provide the number of iterations and the approximate computational cost required to achieve the final SOTA results on the two successful tasks? Furthermore, when the agent was stuck in plateaus on the six unsuccessful tasks, what were the characteristics of the ideas it subsequently generated?\n\n4.  AlphaEvolve uses a (quasi-)evolutionary algorithm to maintain population diversity, whereas AlphaResearch's single-trajectory sampling seems more prone to getting stuck in local optima. Have the authors considered combining their dual-reward framework with a more robust population-based evolutionary strategy (e.g., an island model) to more rigorously test the directive benefit of the RM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eTWlaUxoxU", "forum": "FF2Lbu9U6Y", "replyto": "FF2Lbu9U6Y", "signatures": ["ICLR.cc/2026/Conference/Submission22639/Reviewer_SUAh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22639/Reviewer_SUAh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829107753, "cdate": 1761829107753, "tmdate": 1762942315081, "mdate": 1762942315081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for training a reward model to efficiently filter ideas during algorithm discovery. It also introduces a new benchmark for algorithm discovery, AlphaResearchComp. AlphaResearch is an agent that utilizes this reward model. The execution reward is used for performance evaluation, while the reward model is employed for idea filtering. This reward is applied immediately after idea generation, and if the score falls below a threshold, that round is skipped. The results on AlphaResearchComp show that AlphaResearch surpasses human researchers on the packing circles problem, while the other tasks remain challenging, demonstrating the current limitations of autonomous algorithm discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Training a reward model specifically for algorithm discovery is a very good idea. There is a gap between reward and generation, and existing methods have centered on program execution rewards."}, "weaknesses": {"value": "- Although the sampling procedure in AlphaResearch is defined as drawing (i_t, p_t, r_t) from the past trajectory (\\tau_{k-1}) according to a probability distribution (P(\\cdot \\mid \\tau_{k-1})), the paper does not explain the concrete properties or design principles of that distribution (P). As a result, it is unclear by what criteria past steps are selected, for example uniform selection, recency preference, or performance weighting. The main process of the algorithm, namely new idea generation, depends on this sampling, yet its details are not provided, which reduces clarity.\n- If reviews are not publicly available, the RM cannot be trained. In many fields outside machine learning, peer reviews are generally not public, which makes the RM effectively untrainable and limits the method in other domains.\n- The RM incurs inference cost. How does it compare to mechanical filtering methods for ideas, such as detecting keyword overlap with past ideas or computing textual similarity to past ideas?\n- AlphaResearchComp includes only 6 evaluated problems, which is a rather limited number for drawing general conclusions about the agent’s capabilities."}, "questions": {"value": "- Proposing a new benchmark is an excellent contribution. However, it would be beneficial to include comparisons with existing evaluation frameworks such as PaperBench [1] and MLE-bench [2] in the related work section.\n- The RM improves the efficiency of algorithmic idea generation, but is it better than directly training a policy from past ideas [3]?\n- The research agent is only o4-mini [4]. Since the performance of discovered algorithms in algorithm discovery depends heavily on the agent’s capability, running experiments with other agents and demonstrating consistency would make the paper more convincing.\n\n[1] PaperBench: Evaluating AI's Ability to Replicate AI Research\n\n[2] MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering\n\n[3] Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning\n\n[4] OpenAI o3 and o4-mini System Card"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VK30Hw0KCJ", "forum": "FF2Lbu9U6Y", "replyto": "FF2Lbu9U6Y", "signatures": ["ICLR.cc/2026/Conference/Submission22639/Reviewer_tm64"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22639/Reviewer_tm64"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134356282, "cdate": 1762134356282, "tmdate": 1762942314304, "mdate": 1762942314304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AlphaResearch, an autonomous research agent designed to discover novel algorithms that surpass human-created ones. Unlike prior coding-only systems, AlphaResearch combines LLM-based idea generation, program implementation, and optimization within a dual environment: a simulated peer-review system (AlphaResearch-RM-7B) and program-based verification. Through iterative ideation, coding, and evaluation, AlphaResearch generates, tests, and refines research ideas autonomously. In experiments on eight open-ended problems, AlphaResearch outperforms human researchers on two tasks and achieves state-of-the-art results beyond AlphaEvolve. The study demonstrates LLMs’ potential to advance human knowledge while identifying remaining challenges in autonomous algorithm discovery."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an ambitious and novel problem—whether LLM-based agents can autonomously discover algorithms that go beyond human knowledge boundaries. The design of a dual environment that combines peer-review-style reward modeling with executable program verification represents a meaningful conceptual advance over prior coding-only or LLM-as-a-judge approaches. If validated, the proposed framework could represent an important step toward autonomous scientific discovery, a key open question in AI research. The demonstration that AlphaResearch outperforms human experts on certain tasks, even if limited, provides intriguing preliminary evidence of LLM potential beyond human knowledge boundaries."}, "weaknesses": {"value": "1. While combining peer-review simulation with execution-based verification is an interesting integration, the overall conceptual novelty is modest. The work mainly repackages existing ideas from prior frameworks such as AlphaEvolve and LLM-as-a-judge.\n2. The description of the AlphaResearch pipeline lacks sufficient procedural clarity. It is unclear how the system refines research ideas, translates them into executable code, and performs iterative optimization. The explanation of interactions between modules (LLM, RM, executor) is particularly vague.\n3. The experimental setup is under-specified and potentially insufficient to substantiate the paper’s claims. Key issues include (a) limited number of test problems (only 8), (b) unclear evaluation metrics, (c) missing baseline comparisons, and (d) lack of dataset details.\n4. The comparisons with existing approaches (e.g., coding-only or LLM-as-a-judge systems) are superficial. The paper does not adequately isolate which component—peer review or execution-based verification—contributes most to the performance gains.\n5. The paper claims that AlphaResearch “pushes the boundary of human knowledge,” but the results do not convincingly support this. Success on two problems out of eight, without robust statistical validation or expert human evaluation, limits the credibility of such claims."}, "questions": {"value": "LN127: what is the contribution of the paper? Besides the proposed AlphaResearch and AlphaResearchCamp, what are your findings or conclusion from your research? Need to describe it explicitly.\n\nLN161: The method description seems incomplete. Are you trying to formulate the method in the framework of RL? If it is true, the peer-review and execution provides reward signals, and a policy is expected to be optimized using these signals. However, the policy and optimization algorithm are not provided in the section.\n\nLN192: The experimental results and discussion should go to Experiments section.\n\nLN217: What is the unit for Human Best and what is the metric?\n\nLN260: The definition of excel@best is problematic. Considering I_d=1 and a fixed r_human, a r_best=r_human-1 and a r_best=r_human+1 could result in the same excel@best.\n\nLN306: Your approach for evaluation, including datasets, baselines, and metrics, are expected here.\n\nLN310: Are 8 problems sufficient to reach a solid conclusion? Need a justification in the settings section.\n\nLN360: The comparison seems unfair given that AlphaResearch filters the ideas based on the evaluation provided by AlphaResearch-RM-7B. Additionally, figure 3 shows that AlphaResearch is better. However, it is hard to quantify the differences from the figure itself.\n\nLN454: What is advantage of the combination compared to existing studies? Maybe you can justify it here.\n\nLN485: It is hard to make conclusion based on the results and observations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1U4E4zDWf3", "forum": "FF2Lbu9U6Y", "replyto": "FF2Lbu9U6Y", "signatures": ["ICLR.cc/2026/Conference/Submission22639/Reviewer_Vxqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22639/Reviewer_Vxqg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155449910, "cdate": 1762155449910, "tmdate": 1762942313978, "mdate": 1762942313978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}