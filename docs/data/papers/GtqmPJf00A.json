{"id": "GtqmPJf00A", "number": 3001, "cdate": 1757314845461, "mdate": 1759898114422, "content": {"title": "Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking", "abstract": "Unified Vision–Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing–Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. Code and model will be available.", "tldr": "We introduce the interleaved Analyzing–Drafting problem-solving loop for synergizing learning between understanding and generation.", "keywords": ["unified understanding and generation", "multimodal reasoning", "multimodal generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dad819bf5838bf8a3dc3df094325506b17f3e547.pdf", "supplementary_material": "/attachment/802be06db9986f1d013e4aceccfc8caf9e010407.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new thinking paradigm called the Alternating Analysis–Drafting Problem-Solving Loop (AD-Loop), designed to achieve genuine synergy between the understanding and generation capabilities in Unified Vision-Language Models (UVLMs). The AD-Loop enables the model to iteratively refine its comprehension and output by dynamically alternating between textual thoughts (for analysis and reasoning) and visual thoughts (for sketching and spatial layout)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The AD‑Loop paradigm is proposed, emphasizing explicit, dynamic, and reciprocal interaction between understanding and generation. This concept goes beyond mere architectural unification, offering a new approach for achieving deeper levels of general multimodal intelligence."}, "weaknesses": {"value": "Existing MLLM research has begun to explore the alternation or integration of textual and visual information. This paper needs to more clearly define the fundamental differences between AD-Loop and such works — specifically, how AD-Loop achieves genuine synergy rather than merely alternation."}, "questions": {"value": "Please elaborate on the specific composition and weight distribution of the internal reward (Intra-reward) and external reward (Inter-reward) in S2. In particular, how is the internal reward designed to encourage “intelligent and autonomous” alternating decision-making?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5bMqCYtu2", "forum": "GtqmPJf00A", "replyto": "GtqmPJf00A", "signatures": ["ICLR.cc/2026/Conference/Submission3001/Reviewer_tVfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3001/Reviewer_tVfz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882462716, "cdate": 1761882462716, "tmdate": 1762916489629, "mdate": 1762916489629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method to improve the capabilities of unified vision-language models. They propose a new approach that combines analytic processes and drafting thoughts (textual and visual thoughts).  They design a two stage strategy for training consisting of supervised learning and RL.  They show performance improvements for benchmark tests for understanding and generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors address an important issue with what seems to be an innovative approach.\n\nThe comprehensive evaluation for understanding and generation is also a strength.\n\nAblation of the thinking types section was also a strength.\n\nGood discussion addressing interesting questions spanning extensions into other MLLMs,  whether visual thoughts should be derived from understanding vs. the generation encoder, and the visualization of implicit visual thoughts.\n\nVery thorough methods section with detailed descriptions."}, "weaknesses": {"value": "I was unclear about how novel the work is. There are many publications using visual representations (imagination) to augment language reasoning. Please expand on how this is different.\n\nPlease show some examples in Figure 7 where the proposed method did not generate better results. This would also be interesting. \n\nThere were parts where I had a difficult time following the methods and even the evaluation.  The paper is dense, but other reviewers more expert in the field might have an easier time following the paper."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zmkptxJrVJ", "forum": "GtqmPJf00A", "replyto": "GtqmPJf00A", "signatures": ["ICLR.cc/2026/Conference/Submission3001/Reviewer_PeBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3001/Reviewer_PeBS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888281503, "cdate": 1761888281503, "tmdate": 1762916489415, "mdate": 1762916489415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Interleaved Analyzing–Drafting Loop (AD-Loop), a framework for unified vision-language models that alternates between textual analysis and visual drafting during reasoning. Through supervised fine-tuning on interleaved reasoning traces and reinforcement learning using Group-Relative Preference Optimization (GRPO), the model learns to decide when to analyze or draft. Applied to UVLMs (BAGEL-7B), the method improves both visual understanding and image generation, setting new state-of-the-art results on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The interleaved analyzing–drafting mechanism establishes a tighter synergy between vision understanding and generation than prior “unified” models. This new paradigm addresses a clear gap by turning generation and analysis into mutually reinforcing steps rather than independent skills.\n\n2. The two-stage training strategy (SFT of interleaved reasoning followed by RL) is well-motivated. This pipeline enables the model to learn the complex analyze-then-draft procedure in a guided way, then optimize adaptively via reinforcement learning. The use of preference-based rewards and an autonomous switching policy shows a way to balance the dual objectives.\n\n3. AD-Loop yields consistent improvements across diverse benchmarks for both modalities. The approach outperforms baselines on multiple datasets and even when applied to different underlying model architectures, indicating broad applicability."}, "weaknesses": {"value": "1. The framework introduces significant complexity in both training and inference. It requires a specialized two-phase training (including an RL stage), and at runtime the model must perform multiple analyze–draft iterations per query. This likely incurs substantial computational cost and latency compared to standard one-pass models. The paper does not discuss inference speed or resource requirements, which raises practical concerns for real-world deployment.\n\n2. The method relies on a curated interleaved reasoning corpus for supervised training. It is not fully clear how generalizable these textual–visual “thought” sequences are. For example, if the SFT dataset is biased or lacks certain complex reasoning patterns, the RL stage will be unable to discover them."}, "questions": {"value": "1. The qualitative examples focus on successes. Could you provide and analyze a few failure cases? Specifically, are there types of problems where the AD-Loop consistently fails or even degrades performance compared to simpler reasoning strategies?\n2. Could you provide more detail on the heuristics used to \"reorganize\" and \"synthesize\" the AD-Loop traces from existing datasets (Sec 3.2)? How did you ensure the quality and logical coherence of these generated traces?\n3. What is the typical computational overhead (e.g., latency, FLOPs) of invoking the AD-Loop for a complex query compared to a standard generation? How does the number of interleaved steps affect this cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uhcGZfybvC", "forum": "GtqmPJf00A", "replyto": "GtqmPJf00A", "signatures": ["ICLR.cc/2026/Conference/Submission3001/Reviewer_h8QD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3001/Reviewer_h8QD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950861461, "cdate": 1761950861461, "tmdate": 1762916489235, "mdate": 1762916489235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing approaches primarily focus on architectural unification while overlooking the importance of explicit interaction between the two capabilities during task solving.\nThis paper introduces the interleaved Analyzing–Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a two-stage training strategy: (1) supervised learning on interleaved thought data to initialize the alternation, followed by (2) reinforcement learning to promote adaptive and autonomous control.\n- The paper conducts extensive experiments and ablation studies to validate the effectiveness of the proposed method.\n- AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, and further showed the adaptability of the proposed method on other unified VLM."}, "weaknesses": {"value": "- The definition of the inter-group reward in Equation (6) is unclear. What does $m$ represent? Does it indicate whether the trajectory is AD-Loop-enabled or not? Additionally, in Equation (7), the intra- and inter-reward terms on the right-hand side seem to be missing the superscript $m$?\n- In the ablation study (Table 3), it is unclear whether the paper trained three additional variants corresponding to different thinking strategies and evaluated them separately, or whether they used the final model but selectively disabled certain thinking capabilities.\n- In Figure 4, what does unified-R1 refer to? Is it AD-Loop?\n- Although Figure 5 presents examples of latent visual thoughts, it would be beneficial if the paper also included qualitative examples illustrating the complete thinking trajectories along with visualizations of the generated latent visual thoughts."}, "questions": {"value": "- When constructing the dataset, did you control the number of visual drafts generated during the thinking process? What were your design choices and constraints here?\n- In the final training loss (Equation 2), it appears that all loss components are simply summed together without weighting. Could you clarify whether any weighting scheme was considered or tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c6hgUKaW3A", "forum": "GtqmPJf00A", "replyto": "GtqmPJf00A", "signatures": ["ICLR.cc/2026/Conference/Submission3001/Reviewer_QySp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3001/Reviewer_QySp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971262425, "cdate": 1761971262425, "tmdate": 1762916488984, "mdate": 1762916488984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}