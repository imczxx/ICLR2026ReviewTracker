{"id": "E8HGf11jTn", "number": 22306, "cdate": 1758329407201, "mdate": 1759896873388, "content": {"title": "Ransomware Detection on Android: Performance and Energy", "abstract": "The growth of the Android ecosystem has amplified the impact of mobile ransomware variants and exposed the limitations of traditional signature-based solutions. Network traffic analysis presents a promising data source for detection, but it introduces new challenges, as ransomware often disguises malicious communication patterns within standard app behavior. Traditional detection mechanisms, which rely on static signatures or handcrafted rules, struggle to counter modern Android ransomware that employs obfuscation and event-driven triggers. This limitation is particularly significant for devices with limited computational resources, where lightweight yet accurate detection is paramount. This paper proposes a pipeline that uses a network traffic dataset to extract relevant features, compares classic and hybrid classifiers (RF, SVM, XGBoost, and lightweight architectures), quantifies cost and energy efficiency on CPU versus GPU. The methodology employs a stratified training/validation/test split (70/15/15), vectorization, grid search with cross-validation, and a set of technical metrics including Accuracy, Recall, F1-Score, and ROC AUC. Experiments demonstrate that the proposed models outperform baselines reported in the literature, yielding improved metric values even under adversarial scenarios. The pipeline also strikes a balance between computational cost and energy efficiency, underscoring the models' cost-effectiveness for different environments: while GPUs accelerate training in the cloud, lightweight models remain competitive for edge deployment. Together, these findings confirm the feasibility of combining high detection accuracy with practical considerations, creating powerful and deployable models to detect ransomware on Android.", "tldr": "This paper uses network traffic and lightweight/classic classifiers to detect Android ransomware, achieving high accuracy and efficiency, suitable for both cloud GPUs and resource-limited edge devices.", "keywords": ["Ransomware", "Android", "Cybersecurity", "Machine Learning", "RAPIDS"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1ae944c8ed5604a5e50358699ad0c343d957ea0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper builds a supervised pipeline to detect Android ransomware from network traffic. It evaluates several classical ML models under CPU vs. GPU settings, using a 70/15/15 stratified split and reports very high metrics (≈0.998) alongside speedups on GPU and with reduced features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors facilitate reproducibility by using standard dataset splits, shared software libraries, and listing key hyperparameters. This is a positive practice that allows for the validation and extension of the work. The work provides clear, empirical comparisons of CPU and GPU inference times, which is valuable for practitioners. Identifying a compact feature set that maintains high performance while significantly reducing computational latency is a concrete and practical finding."}, "weaknesses": {"value": "* The selected features include Label, Flow ID, Source IP, Destination IP, and Timestamp. When splitting randomly, these can leak ground truth or act as near-perfect proxies for the label (the same device/app/server tuples appear across train/test). \n\n    * Remove Label from features entirely; it must only be the target. Remove identifiers and non-stationary keys: Flow ID, Source/Destination IP, Source Port, Timestamp. Keep only transport-/timing-derived statistics that generalize. Re-do feature selection without any direct identifiers. (Abandon LabelEncoder on label column; encode only categorical predictors.)\n\n* A stratified random 70/15/15 split over rows on this dataset allows the same app family and often the same endpoint tuples to appear in both train and test. The paper’s split description doesn’t enforce family-wise, app-wise, device-wise, or time-wise separation, so models can memorize endpoints/flows.\n\n     * Perform grouped splits that hold out entire families (e.g., train on {SVPeng, Koler…}, test on unseen families) and/or app-level holds. Add time-based splits if timestamps are present (train early, test later).\n\n* _Energy efficiency_ is assessed via wall-clock time; the paper itself notes power was not measured. This cannot support energy conclusions.\n     * Replace the energy claim with measured power/energy: e.g., GPU (nvidia-smi logs), CPU (RAPL / power meters). Report Joules/inference and Joules/train, not just seconds. Add throughput vs. latency plots and hardware utilization.\n\n* There is no new learning objective, representation, architecture, or theoretical insight\n   * Comparison with the state of the art is missing."}, "questions": {"value": "* Were Label, Flow ID, Source/Dest IP, and Timestamp included as features during training?\n\n* Have you conducted experiments with a grouped split (e.g., holding out entire app families)? If so, what are the results?\n\n* How is “energy efficiency” measured?\n\n* What is the novel methodological insight or learning contribution?\n\n* Why only tree-based models? And why only these three models?\n\n* Does the model generalize under changing endpoints or adversarial conditions?\n\n* How were the 11 features selected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t8XW4MrHld", "forum": "E8HGf11jTn", "replyto": "E8HGf11jTn", "signatures": ["ICLR.cc/2026/Conference/Submission22306/Reviewer_QWUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22306/Reviewer_QWUi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761125163055, "cdate": 1761125163055, "tmdate": 1762942162757, "mdate": 1762942162757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tests different ML algorithms on an Android ransomware detection dataset, using HTTP network traffic logs as features, evaluating both their performances and runtime efficiencies."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The addressed topic is interesting and touches a still-open challenge in malware detection"}, "weaknesses": {"value": "- The paper lacks novel contributions. The authors simply tested some well-known ML algorithms on a dataset containing pre-extracted HTTP network traffic logs. None of the four stated contributions is relevant (some, such as the dataset, are not even provided in this paper) and represents an advancement in the state of the art.\n- Writing quality. The manuscript quality falls below the bar for this venue, as it is more similar to an experimental report than a full paper.\n- Missing comparison with the state of the art. The paper only reports 4 references (one of them is the used dataset), almost totally lacking the SoA in its research field. The related work section only mentions the results provided in two competing works, without discussing how the proposed paper relates to them and what the improvements are.\n- Likely experimental bias. The authors overlook the evidence presented in previous work [a] that the evaluation of malware detectors should be performed by applying chronologically consistent splits between the training and test datasets. Otherwise, there is a concrete risk of overestimating the detectors' performance.\n- The paper is tested on a dataset that is not validated nor linked to any publication. The authors should have also considered a more rigorous dataset.\n\n[a] Pendlebury, F., Pierazzi, F., Jordaney, R., Kinder, J., & Cavallaro, L. (2018). TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time. USENIX Security Symposium."}, "questions": {"value": "- In the abstract, you mentioned experiments on adversarial scenarios. Could you please explain the meaning of this reference and the link to the provided experimental results? Initially, I got confused as I interpreted it as related to adversarial robustness evaluations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "apBP0yb8xs", "forum": "E8HGf11jTn", "replyto": "E8HGf11jTn", "signatures": ["ICLR.cc/2026/Conference/Submission22306/Reviewer_WpyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22306/Reviewer_WpyE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579619214, "cdate": 1761579619214, "tmdate": 1762942162030, "mdate": 1762942162030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a machine learning pipeline for detection of Android ransomware using network traffic data. The authors used an existing Android ransomware dataset (available at Kaggle) and compare several standard classifiers, measuring accuracy, F1-score, and training time on CPU vs GPU. The authors show that these classifiers are very accurate on this dataset and claim improvements in both performance and energy efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The authors tackle Android malware detection, which is a relevant topic for the cyber security community and where machine learning solutions can bring important benefits for enhancing malware detection in mobile devices."}, "weaknesses": {"value": "+ There is no novelty in the paper. The authors just reuse existing algorithms on a public ransomware dataset and report the results using standard metrics. There is no novel contribution with respect to other papers in the related work on Android malware detection. \n+ In the experiments the authors just use standard algorithms in standard settings. No exploration of other configurations or selection of hyperparameters is considered. There is no comparison against any competing method. \n+ The energy analysis is superficial. The authors just considered the execution time, which is related but not the same as energy consumption. \n+ The claims of the paper are not supported in the experiments. \n+ The related work just includes 4 references and lacks a proper discussion of the state of the art on Android ransomware detection."}, "questions": {"value": "+ What is the novelty and the contributions of the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WfeXxKvt6u", "forum": "E8HGf11jTn", "replyto": "E8HGf11jTn", "signatures": ["ICLR.cc/2026/Conference/Submission22306/Reviewer_8Ntg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22306/Reviewer_8Ntg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937285232, "cdate": 1761937285232, "tmdate": 1762942161642, "mdate": 1762942161642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a machine learning–based pipeline for Android ransomware detection using network traffic data. The authors evaluate several classifiers (Random Forest, XGBoost, Gradient Boost, Bagging, etc.) and analyze trade-offs between performance, computational cost, and energy efficiency under CPU and GPU configurations. Using the Android Ransomware Detection dataset, they show that ensemble models such as Bagging and XGBoost achieve high accuracy while maintaining good computational efficiency. The work aims to balance accuracy with deployability for edge and cloud environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. About experiments. The study compares multiple classifiers across both CPU and GPU setups, providing a clear empirical view of cost–accuracy trade-offs.\n2. About practicality. The inclusion of energy and time efficiency metrics adds value, especially for real-world deployment scenarios on mobile or edge devices.\n3. About reproducibility. The paper follows a logical and transparent methodological pipeline that can be reproduced with publicly available datasets."}, "weaknesses": {"value": "1. Limited novelty. The technical contribution mainly lies in a systematic comparison of existing classifiers rather than proposing a new model or detection mechanism.\n2. Energy analysis proxy. Energy efficiency is inferred indirectly from training time, without actual power consumption measurements; this weakens claims about energy impact.\n3. Dataset dependency. The experiments rely solely on a single public dataset. Generalizability to unseen or real-world traffic remains uncertain.\n4. Lack of adversarial robustness testing. Although the abstract mentions adversarial scenarios, no concrete adversarial evaluation methodology or results are provided.\n5. Insufficient discussion on deployment feasibility. While edge applicability is mentioned, no prototype or memory/latency analysis is shown to support claims of lightweight deployment."}, "questions": {"value": "1. How would the proposed pipeline handle encrypted or obfuscated network traffic, which is increasingly common in Android malware?\n2. Have the authors considered cross-dataset or real-device validation to assess generalization beyond the Kaggle dataset?\n3. Can the authors clarify how energy efficiency was quantified and whether any profiling tools (e.g., NVIDIA SMI or power meters) were used?\n4. How does the model perform under class imbalance or with new ransomware families not seen in training?\n5. Is there any plan to release the full experimental code and trained models to support community benchmarking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PZrUtgfBkD", "forum": "E8HGf11jTn", "replyto": "E8HGf11jTn", "signatures": ["ICLR.cc/2026/Conference/Submission22306/Reviewer_GFtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22306/Reviewer_GFtv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986021105, "cdate": 1761986021105, "tmdate": 1762942161379, "mdate": 1762942161379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}