{"id": "VdLEaGPYWT", "number": 2309, "cdate": 1757055737969, "mdate": 1759898156380, "content": {"title": "Mastering Sparse CUDA Generation through Pretrained Models and Deep Reinforcement Learning", "abstract": "Code generation is a crucial research area in the field of artificial intelligence, holding the potential to revolutionize software development and streamline programming processes. However, generating the high-performance code, which need to be executed in a shorter time for the low-latency scenario, remains a formidable challenge. Existing methods often struggle to account for the irregularity of input sparse data in sparse programs and the need for domain-specific architectural knowledge, leading to sub-optimal performance. To tackle these issues, we propose the SparseRL framework. SparseRL leverages deep reinforcement learning, treating a pre-trained language model as a stochastic policy. It takes the row and column indices of non-zero elements in the sparse matrix as input and generates CUDA code as output for sparse matrix operations. We also introduce a domain-specific code generation mechanism for the dynamic input, a sinusoidal embedding technique tailored for sparse matrices, and a hierarchical reward function that considers both code correctness and execution efficiency. Experimental results demonstrate SparseRL achieves state-of-the-art performance. In sparse matrix-vector multiplication (SpMV) tasks, it improves the compilation rate by 20% compared to existing methods, and the generated code runs 30% faster on average. For sparse matrix-dense matrix multiplication (SpMM) tasks, SparseRL also shows significant performance gains. These results highlight the effectiveness of SparseRL in generating high-performance CUDA code for sparse matrix operations.", "tldr": "We propose SparseRL, a deep reinforcement learning framework that generates high-performance CUDA code for sparse matrix operations, achieving significant improvements in both correctness and execution efficiency.", "keywords": ["Reinforcement Learning", "CUDA Code Generation", "High-Performance Computing"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0918a511b547424944784c6cb4c4657ee83ac027.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SparseRL, a training framework that leverages pre-trained language models to generate high-performance sparse CUDA kernels for sparse matrix–vector (SpMV) and sparse matrix–dense matrix (SpMM) operations on GPU. This method includes (1) a sinusoidal embedding of sparse matrix structures, (2) domain-specific pretraining on CUDA code, and (3) a hierarchical reward function design, and leverages the PPO algorithm for training. A compilation success gain and performance speedup gain is achieved compared with prior work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using LLMs for GPU kernel optimization is an important topic that is worth more research efforts\n2. Sparse matrix multiplication is a novel subset of problems in kernel optimization."}, "weaknesses": {"value": "1. PPO is an old algorithm. Why isn't GRPO used?\n2. Important baselines, such as directly prompting gpt-5, gpt-o4-mini, claude-4-opus, are missing. The compared ones are GPT-3, Llama3.1, but these baselines are not very meaningful now.\n3. Is PPOCoder the right baseline to compare against? Is PPOCoder / CodeRL trained for GPU kernel optimization? It seems an unfair comparison.\n5. Writing suggestion: Subsection 3.4 is too long with little information. However, the appendix with more experimental results is not even discussed in the main paper."}, "questions": {"value": "1. What's the prompt? The paper says, \"During the SFT and RL stages, the language input prompt is removed and only sparse matrices are provided as input.\" Why is this a good approach? In KernelBench, the Python code that demonstrates the functionality is provided. I think a better prompt should include a baseline GPU kernel and ask the model to optimize an existing implementation.\n2. The performance improvement over cuSPARSE is quite surprising to me. How large are those matrices? What's the GPU utilization?\n3. The ablation study of each individual part is unclear to me. Is pre-training necessary? Is SFT necessary? Is sinusoidal embedding necessary? Why aren't these results in the main paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q9YZUCOvOr", "forum": "VdLEaGPYWT", "replyto": "VdLEaGPYWT", "signatures": ["ICLR.cc/2026/Conference/Submission2309/Reviewer_C533"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2309/Reviewer_C533"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435983960, "cdate": 1761435983960, "tmdate": 1762916188464, "mdate": 1762916188464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the problem of code generation for sparse matrix-vector multiplication (SpMV) and sparse matrix-dense matrix multiplication (SpMM) tasks. Their tool, SparseRL, solved the challenges of sparse programs and the shortage of domain-specific architectures by improving large language models to make them a stochastic policy. This policy generated different solutions before being judged in terms of functional correctness and execution efficiency. For code candidate generation, the policy model was pre-trained and fine-tuned to do this task. Next, at the Reinforcement learning stage, the proposed model will be optimized by the authors’ proposed reward function for execution efficiency, besides the correctness reward. The experiment was done on two tasks and compared with a set of well-known closed-source Large Language Models (LLMs). The authors have collected 1100 matrices with the largest amount of non-zero elements from the Suite Sparse Matrix Collection, with 700 for training and 400 for testing. The evaluation was performed using Correct functionality (pass@k) and compilation rates (CR) scores, with k as the number of generated programs as candidates per matrix. The accuracy results show that SparseRL can be built on various open LLMs, and SparseRL models can outperform other famous LLM-based approaches in terms of the pass@k score and the compilation rate score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written. Every figure and table is well described.\n- The novelty of the rewarding mechanism is the core contribution of this paper. The proposed reward score ensures the functional correctness and execution efficiency of the code.  \n- The idea of integrating LLM with RL makes sense to me since there has been trendy research on applying RL for AI. \n- Experiments were conducted with a number of good baselines."}, "weaknesses": {"value": "- This work required a pre-training stage with CUDA code augmentation. Since the pre-training stage is an expensive process compared to fine-tuning, this work might have a scaling issue when pre-trained on bigger open LLMs than Qwen3 14B.\n- Lack of comparison between the sparse matrix embedding used in SparseRL and other code embedding techniques, such as UniXcoder [1] and GraphCodeBERT [2]. Although these models are not originally built for embedding a matrix, authors can design a simple flattening algorithm from a matrix to a sequence of tokens and compare the effectiveness of other embedding approaches to sparse matrix embedding.\n- The RL training dataset is on a small scale with 1100 entities for both training and testing.\n\nReferences\n1.UniXcoder: Unified Cross-Modal Pre-training for Code Representation. Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin.\n2.GraphCodeBERT: Pre-training Code Representations with Data Flow. Daya Guo et al."}, "questions": {"value": "- In Formula 8, do correctness and efficiency always contribute the same weight to the final score? It is interesting to see the ability of the model to generate more correct code or more efficient code based on the reward function. Authors can provide a coefficient score (from 0 to 1) and modify it to see the variety of quality in the generated code. For example, if R_final=0.9*R_Correctness+0.1*R_efficiency, a good RL model will tend to generate more correct code but be inefficient in terms of running time.\n- Will the approach be transparent and getting consistent accuracy improvement with other LLMs besides Qwen?\n- While this approach is good for matrix operations, will it be applicable to other general types of code optimization? I suggest authors should describe the potential of extending the scale of this application in camera ready version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t5kdckVvmS", "forum": "VdLEaGPYWT", "replyto": "VdLEaGPYWT", "signatures": ["ICLR.cc/2026/Conference/Submission2309/Reviewer_mzXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2309/Reviewer_mzXd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952936095, "cdate": 1761952936095, "tmdate": 1762916188318, "mdate": 1762916188318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SparseRL, a domain‑specialized framework that treats a pretrained code LLM as a stochastic policy and fine‑tunes it with PPO to generate high‑performance CUDA kernels for sparse operators, focusing on SpMV and extending to SpMM. The model ingests the sparse matrix itself via a row/column sinusoidal index embedding, then emits CUDA; a compiler/executor environment supplies rewards for compilation success, functional correctness, and runtime, with a memory‑usage penalty. The RL loop (Figure 1, p.2) and the pretrain→SFT→RL pipeline (Figure 2, p.4) anchor the method. On 400 SuiteSparse test matrices (700 for training) plus DLMC, they report higher compilation and correctness rates and sizable speedups vs CodeRL, PPOCoder, cuSPARSE, and TVM‑S; e.g., for SpMV they claim roughly +20% compilation rate and ~30% faster runtime on average, with pass@1000 up to 49.25 and CR 57.50 (Table 1, p.8). Figures 3–4 visualize GFLOPS/TFLOPS gains on V100/A100. \n\nKey contributions\n\n* A DRL‑based, domain‑specific code generation framework that conditions on dynamic sparse inputs and uses compiler/executor feedback to optimize generated CUDA for per‑matrix performance (Figures 1–2). \n* A row/column sinusoidal embedding tailored to sparse matrices that maps non‑zero coordinates into the LLM’s input space to better capture runtime execution patterns. \n* A hierarchical reward that combines compilation pass, unit‑test correctness, runtime efficiency, and a memory‑overuse penalty, enabling direct optimization for latency. \n* Empirical SOTA on SpMV with strong generalization to SpMM: improved pass@k and CR, and consistent performance speedups vs strong baselines on V100/A100, backed by ablations (embedding choice, reward components) and profiling analyses (occupancy, memory traffic)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\n* Frames sparse GPU kernel generation as an RL problem where a pretrained code LM is the policy and the compiler/executor acts as the environment. The hierarchical reward mixes compilation success, unit‑test correctness, runtime, and a memory‑overuse penalty, directly aligning learning with latency and resource goals. This closes the loop between code synthesis and low‑level performance feedback in a way that is rare for sparse kernels. See the pipeline in Figure 2 (p.4) and reward in Eq. 10 (p.7). \n* Introduces an input representation that conditions the LM on the actual sparse matrix: sinusoidal embeddings of row and column indices for nonzeros. That idea bridges the modality gap between sparse structure and code and enables per‑matrix specialization, rather than “one kernel fits all.” The mechanism and motivation are detailed in §3.2 and reinforced by the embedding ablation in Table 6 (p.20). \n* Combines pretraining on CUDA, supervised fine‑tuning for SpMV, and PPO fine‑tuning with on‑hardware feedback. The pretrain→SFT→RL stack is common in language tasks, but focusing it on sparse CUDA with dynamic inputs is a creative recombination that removes assumptions typical in dense‑kernel work. Figure 2 (p.4) and §3 lay out the stages cleanly. \n* Adds a pragmatic dynamic syntax checker during decoding (Appendix A.3) to terminate hopeless generations early, a small but thoughtful systems detail that improves training efficiency. \n\nQuality\n\n* Sound problem setup and thorough evaluation. Correctness is measured with pass@k and compilation rate; performance is measured as GFLOPS/TFLOPS on V100 and A100 with CUDA 12.1, using 1000 iterations to stabilize timing. Metrics and hardware details appear in §4.1 and Table 1 (p.8). \n* Strong baselines: compares against CodeRL and PPOCoder as learning methods and against cuSPARSE and TVM‑S as engineered libraries. On SpMV, SparseRL improves pass@1000 to 49.25 and CR to 57.50, and shows consistent speedups over both learned and library baselines; results are summarized in Table 1 (p.8) and Figure 3 for SuiteSparse on both GPUs (p.9). \n* Generalization beyond SpMV. The same machinery transfers to SpMM with competitive accuracy and clear TFLOPS gains for different dense‑matrix widths, shown in Table 1 and Figure 4 (p.9). \n* Careful ablations and diagnostics. The paper isolates the impact of the RL reward components, the sparse‑matrix embedding choice, and exploration settings, with clear trends in Figure 5 (p.19), Table 6 (p.20), and Table 7 (p.21). Profiling ties wins to reduced memory traffic and higher SM occupancy vs. baselines (Tables 8–9, p.23). This strengthens the causal story behind the performance improvements. \n* Transparency about limits and cost. The authors document failure cases on extreme matrices, compute overheads of RL fine‑tuning, and scenarios where benefit dominates cost, with concrete numbers (Appendix A.10–A.12). This raises confidence in the claims rather than overpromising. \n\nClarity\n\n* The problem and method are explained with clean, layered visuals. Figure 1 (p.2) gives a task‑level view with the environment loop; Figure 2 (p.4) shows training stages and reward flow. The mathematical setup in §3 is precise without being dense, and equations for PPO and rewards are easy to track (Eqs. 4–5, 9–10). \n* Experimental presentation is readable and decision‑relevant. Table 1 concentrates correctness and compilation outcomes across models, while Figures 3–4 summarize performance across matrix sizes. Appendices include small code snippets to illustrate why different sparsity patterns demand different kernels (Appendix A.6), which makes the motivation for per‑matrix specialization concrete. \n* Limitations and scope are clearly demarcated, including notes on backend portability and when offline search costs are amortized (Appendix A.12–A.13). \n\nSignificance\n\n* For the code‑generation community, the work shifts the target from “compiles and is correct” to “compiles, is correct, and runs fast for my data,” with measured improvements over vendor and compiler baselines on widely used sparse collections. That is a meaningful step toward data‑conditioned, performance‑aware program synthesis. See Figure 3 (SpMV speedups) and the stated averages vs. cuSPARSE/TVM‑S (p.8–9). \n* For practitioners in GNNs and pruned LLM inference, per‑matrix kernel synthesis that beats cuSPARSE on average can translate into tangible latency wins without hand‑engineering every corner case. The extensions to SpMM suggest the approach is not limited to a single operator (p.9). \n* The technique is likely to influence future research on learning‑to‑optimize kernels: the reward design is modular, the input representation is lightweight, and the analysis ties learned choices to classic GPU optimization principles. The profiling evidence in Tables 8–9 (p.23) makes the case that the system is learning the right low‑level instincts, not just gaming the benchmark. \n\nConclusion: A well‑executed, domain‑specific RL framework that conditions on sparse inputs and optimizes for runtime delivers real, measurable wins over both learned and hand‑engineered baselines, with clear analysis and honest limitations. The combination of ideas feels fresh for sparse GPU codegen and practically relevant to a growing slice of ML and HPC workloads."}, "weaknesses": {"value": "1. Positioning vs. prior art is underdeveloped\n   The paper cites AlphaSparse (Du et al., 2022) and classic auto‑tuning work (e.g., SMAT, YASpMV), but there’s no direct, apples‑to‑apples comparison or a clear articulation of what SparseRL fundamentally enables that these matrix‑conditioned search/auto‑tuning systems cannot. Since AlphaSparse also generates SpMV code from the matrix, the novelty claim would be stronger with a head‑to‑head on the same matrices and GPUs, plus a discussion of search cost vs. RL fine‑tuning cost. Actionable fix: add baselines from AlphaSparse/SMAT/YASpMV/SparseTIR or replicate their search pipelines on your SuiteSparse/DLMC splits; report both performance and wall‑clock search/optimization time. \n\n2. Reward design is plausible but under‑specified and potentially unstable\n   The efficiency reward is a scaled time ratio against cuSPARSE (Eq. 9), added after ±0.5 compile/test terms and a memory penalty (Eq. 10). The paper does not specify the actual values of the scaling factor `r_eff` or the memory penalty, nor how reward magnitudes are normalized across matrices with very different runtimes. This matters because reward scale drives PPO stability, exploration pressure, and convergence. Actionable fix: report exact hyperparameters and show sensitivity curves for `r_eff` and the penalty; consider per‑matrix normalization (e.g., log speedup, z‑scored runtime) and reweighting between correctness and efficiency to demonstrate PPO stability across nnz regimes. Include training‑time reward traces and KL/entropy diagnostics. Figures 1–2 set the stage for this analysis but don’t provide it. \n\n3. Matrix representation ignores permutation issues and reordering\n   Sinusoidal row/column embeddings make the model sensitive to absolute indices. SpMV performance, however, is heavily affected by row/column order; classical systems often reorder to improve locality and load balance. The paper neither evaluates robustness to permutations nor includes reordering in the action space. Actionable fix: (a) report robustness under random row/column permutations and standard reorderings; (b) add an optional pre‑processing/reordering step as part of generation, or augment the embedding with permutation‑invariant features (row nnz histograms, bandwidth, blockiness) or learnable Fourier features, and quantify the gains (extend Table 6 beyond Raw/Max‑Min/Sinusoidal). \n\n4. Evaluation budgets are not fully comparable across methods\n   Table 1 reports pass@k with k up to 1000 for some models, but many baselines are “–” at k=1000 due to “environment limits,” and performance plots use Qwen‑14B for SparseRL while CodeRL/PPOCoder are shown on CodeT5‑770M (and performance is measured “under k=5000 on correct programs of partial matrices” for those baselines). This mixes model capacity and sampling budgets with method quality. Actionable fix: standardize sample counts, temperatures, and decoding schemes across all learned methods; provide pass@k curves and wall‑clock search time per matrix; repeat GFLOPS comparisons with SIZE‑MATCHED models (e.g., SparseRL+CodeT5‑770M vs. CodeRL/PPOCoder‑CodeT5‑770M) and again with SIZE‑MATCHED larger backbones. Clarify whether pass@k uses sampling or beam for each model. \n\n5. Closed‑source LLM comparisons conflate domain specialization with methodology\n   Tables 2–3 show better correctness for large closed‑source models but poor runtime, then attribute the gap to “lack of hardware optimization knowledge.” Those models were not adapted with your RL loop or matrix embeddings, so this primarily shows domain specialization matters, not that the RL formulation is superior. Actionable fix: run your RL framework on one closed‑source model’s outputs through the same compiler/executor loop where licensing permits, or at least run the identical SFT+RL recipe on an open counterpart with comparable size to isolate the effect of the RL+reward vs. model scale. \n\nThe work is promising, the claims would be more robust with (i) stricter budget matching and size‑matched baselines, (ii) reward/optimization transparency and stability evidence, (iii) permutation/reordering awareness in the representation or pipeline, (iv) broader hardware/backends, and (v) a concrete cost‑benefit curve for real deployments. Most of these are addressable within the current framework and would materially strengthen the paper."}, "questions": {"value": "1) Reward scaling and stability\n\nQuestion. What exact values do you use for the efficiency scaling factor `r_eff` and the memory penalty `r_penalty` in Eqs. (9)–(10), and how are rewards normalized across matrices with very different runtimes? Provide PPO diagnostics (KL, entropy, value loss) over training.\nWhy it matters. Reward scale drives PPO stability and whether speedups generalize beyond the training matrices.\nWhat would help. Precise hyperparameters, per‑epoch reward traces, and a short sensitivity study varying `r_eff` and `r_penalty`. See §3.4 and Eqs. (9)–(10), p. 6–7. \n\n---\n\n 2) Definition and computation of pass@k\n\nQuestion. In §4.1 you say pass@k uses “k (beam search) synthetically generated program samples,” but §3.3 says RL sampling uses top‑k with temperature. Which decoding is used for pass@k for each method and setting?\nWhy it matters. Beam vs sampling materially changes pass@k and comparability to prior work.\nWhat would help. A one‑line table mapping each model to its decoding strategy for pass@k, plus pass@k curves under a matched decoding policy. See §3.3 (top‑k sampling) and §4.1 (beam), p. 6 and p. 8. \n\n---\n\n 3) Budget and model‑size parity for baselines\n\nQuestion. Table 1 reports pass@1000 for some models but not others due to “environment limits,” and performance plots compare SparseRL with Qwen‑14B against CodeRL/PPOCoder using CodeT5‑770M “under k = 5000 on correct programs of partial matrices.” Can you provide size‑matched and budget‑matched comparisons?\nWhy it matters. Current results confound method quality with model capacity and sampling budget.\nWhat would help. Add a row (or supplementary figure) with SparseRL+CodeT5‑770M and CodeRL/PPOCoder+CodeT5‑770M under identical k and decoding; also report wall‑clock search time per matrix. See Table 1 and Figure 3, p. 8–9. \n\n---\n\n 4) Formal task definition\n\nQuestion. Eq. (2) requires the generated Ŷ to be in a set of “ground‑truth code” Y, yet your system synthesizes new correct programs. Will you revise the formalization or justify this assumption?\nWhy it matters. The mismatch makes the objective look constrained when the method is not.\nWhat would help. A corrected statement aligning with “synthesize any program that compiles, passes tests, and minimizes runtime.” See §3.1, p. 5. \n\n---\n\n 5) Matrix representation: permutation robustness and reordering\n\nQuestion. How robust is performance to row/column permutations and standard reorderings (e.g., RCM, AMD)? Did you test permutation‑invariance or include reordering as a pre/post step?\nWhy it matters. Absolute index sinusoidal embeddings are sensitive to ordering; SpMV performance is often improved via reorderings.\nWhat would help. A permutation/reordering robustness table and, if possible, a variant that augments sinusoidal embeddings with permutation‑invariant features. See §3.2 and the embedding ablation in Table 6, p. 5 and p. 20. \n\n---\n\n 6) Memory penalty details\n\nQuestion. What exactly constitutes “excessive memory” for the penalty in Eq. (10): registers, shared memory, global memory, or allocated buffers? How is the limit chosen and measured?\nWhy it matters. Shared memory can increase speed while lowering occupancy; an opaque penalty could bias the policy.\nWhat would help. Thresholds per resource, measurement method, and a small plot showing speed vs memory usage trade‑offs on a few matrices. See §3.4.3, p. 7 and SM‑occupancy analysis in Table 9, p. 23."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hAFp1YIGie", "forum": "VdLEaGPYWT", "replyto": "VdLEaGPYWT", "signatures": ["ICLR.cc/2026/Conference/Submission2309/Reviewer_o7fk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2309/Reviewer_o7fk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009581164, "cdate": 1762009581164, "tmdate": 1762916187419, "mdate": 1762916187419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SparseRL, a framework to generate high-performance sparse CUDA code for SpMV and SpMM tasks. The authors employ deep reinforcement learning to fine-tune code models with a sinusoidal embedding for sparse matrix row/column indices. Experiments on the SuiteSparse and DLMC datasets demonstrate that SparseRL significantly outperforms strong baselines such as CodeRL, PPOCoder, and cuSPARSE in both correctness and efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SparseRL provides a complete process of the training pipeline from pretraining to SFT and RL optimization. The idea of encoding sparse matrix structures using sinusoidal embeddings is elegant and theoretically well-motivated.\n2. The experiments are very comprehensive and convincing. It is particularly interesting to observe clear improvements in code efficiency, which is highly relevant in CUDA generation scenarios. The ablation studies are thorough too and leave few gaps."}, "weaknesses": {"value": "1. My most concern is the paper’s organization. Many insightful analyses are leaved to the appendix. Reducing some descriptive parts (like section 3.1 and 3.3) and moving more analytical discussion into the main body would make the core contributions more clearly."}, "questions": {"value": "1. Have you compared SparseRL’s generated CUDA code against human expert implementations? Demonstrating superiority over human-optimized kernels would make the results more compelling.\n2. It would be better to try some other RL algorithms (e.g., GRPO, Reinforce++) to test robustness and potential further gains. Additionally, including a comparison with SFT baseline would help demonstrate the contribution of the RL phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3dGsKY95EC", "forum": "VdLEaGPYWT", "replyto": "VdLEaGPYWT", "signatures": ["ICLR.cc/2026/Conference/Submission2309/Reviewer_PAdt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2309/Reviewer_PAdt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762445358837, "cdate": 1762445358837, "tmdate": 1762916186295, "mdate": 1762916186295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}