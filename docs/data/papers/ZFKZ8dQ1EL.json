{"id": "ZFKZ8dQ1EL", "number": 11263, "cdate": 1758194636945, "mdate": 1759897597493, "content": {"title": "FedBARRE: Privacy–Utility Optimized Perturbation Ensemble against Gradient Leakage Attacks in Federated Learning", "abstract": "With the accelerating demand for data privacy and the proliferation of AI applications, federated learning has emerged as a pivotal paradigm for collaborative model training on distributed datasets. However, in horizontal federated settings, adversaries can still exploit gradient-inversion or optimization-based attacks to reconstruct clients’ private data, leading to severe privacy breaches. Although numerous privacy-preserving methods have been proposed, they typically entail substantial utility degradation. To address this trade-off, we introduce FedBARRE, a unified framework that synergizes a Randomized Ensemble Classifier (REC) with optimized data perturbations to markedly enhance privacy protection while incurring minimal performance loss. We establish the convexity of the REC’s adversarial risk, providing a solid theoretical foundation for privacy–utility optimization. Extensive experiments validate that FedBARRE preserves overall federated-learning accuracy while significantly strengthening client-data confidentiality.", "tldr": "", "keywords": ["Federated Learning", "Gradient Leakage Attacks", "Privacy-Utility Trade-off"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a9c598503dbee3e845b8175f367457e709fa364.pdf", "supplementary_material": "/attachment/be4f8d36914eb34af8aaff08779328fb59f3a462.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces FedBARRE, a novel FL framework to address the privacy-utility trade-off. The proposed method maps the privacy-utility trade-off to an adversarial training objective and leverages min-max optimization to optimize it. Extensive experimental results show that the proposed method effectively addresses the trade-off and defends against reconstruction attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper conducts extensive experiments to highlight the advantages of the proposed method.\n- The proposed method is clear and straightforward for implementation.\n- The paper is well-written"}, "weaknesses": {"value": "- Lack of theoretical analysis for the privacy guarantee. \n- Some experiments have vague and suspicious results. For instance, $\\epsilon$ is never defined and indicates how it's computed, but is still used to present the results.\n- The privacy protection mechanism is not sufficient. Specifically, the data is used throughout every step of the optimization process without any protection. Even the noise is optimized with the data and becomes deterministic."}, "questions": {"value": "- For the definition of $\\rho(f)$, why the loss consider argument minimizaition by the noise $\\delta$? Doesn't the noise $\\delta = 0$ minimize this loss? If not, then $\\delta$ is mapping $x$ to some other features?\n- For $G(\\alpha, \\sigma)$, why is it convex w.r.t $\\sigma$? How is this $G$ function elaborated throughout the paper? If $f$ is not convex, how do we ensure the structural property?\n- How did you compute $\\epsilon$ for your mechanism?\n- Can the proposed method defend against Membership inference attacks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JtyvyVof6H", "forum": "ZFKZ8dQ1EL", "replyto": "ZFKZ8dQ1EL", "signatures": ["ICLR.cc/2026/Conference/Submission11263/Reviewer_EY9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11263/Reviewer_EY9r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537195857, "cdate": 1761537195857, "tmdate": 1762922421108, "mdate": 1762922421108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission tackles the issue of preserving privacy in federated learning, where the private data of the clients can be obtained by an adversary via gradient inversion attacks. The submission introduces a FedBARRE, a training algorithm in which clients learn optimized perturbations to their data (bounded within some min/max L2 norm) that minimizes the performance loss due to the perturbation. At each step, each client in the selected batch optimizes their perturbation to minimize their loss, followed by a model update to their local model. After each model update, the client tests the model on their local validation set and eventually sends the gradient of the model (among those encountered throughout this training) that minimizes this validation loss to the central server for model update aggregation. The authors test their method against other privacy-preserving methods from literature to demonstrate that FedBARRE achieves a better utility-privacy tradeoff."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The problem of achieving better privacy–utility tradeoff in FL is well-motivated and an important one. \n\n2) The experiment covers different datasets and also compares their method to various existing methods.\n\n3) The algorithms are easy to understand and intuitive."}, "weaknesses": {"value": "1) There appears to be a fundamental disconnect between what the submission is claiming to do and what the FedBARRE algorithm actually does. First and foremost, the introduction presentes FedBARRER as a method using Randomized Ensemble\nClassifiers and section 4.2 clearly labels the the objective (\"Our final optimization goal...\") as learning M model parameters (for each model in the ensemble) and the ensemble weights $\\alpha$, but this is not what FedBARRE does: there is no computation of any ensemble weights, only a single final global model that results from a standard aggregation of the gradients sent by the calendars. In fact the optimization objective $\\rho(\\alpha)$ never brought up again after being briefly mentioned in the start of Section 5, where it is vaguely stated that the algorithm is \"grounded in the optimization objective introduced in Section 4\" with no justification (in particular it never comes up, implicitly or explicitly, in the actual algorithms). In fact, calling the models {$\\theta_1,...,\\theta_M$} in Algorithm 2 an \"ensemble\" is a stretch by itself, since each model is simply a clone of the previous model that continues with the training, so the \"ensemble\" component of the algorithm simply boils down to the client training a single model and checking its validation loss (using a local validation data that is never formally defined) at M points during the training, and then using epoch among these M checkpoints that achieves minimum error. \n\nFurther, the first key contribution listed in Section  is \"We introduce and analyze three risk measures—standard risk, adversarial risk, and REC adversarial risk—and prove the convexity of the REC adversarial risk, establishing a solid theoretical foundation for ensemble optimization in federated learning.\" However, the terms \"adversarial risk\" or \"REC adversarial risk\" is never brought up again (Definitions 2 and 3 which introduce Privacy–Utility Risk and REC Privacy–Utility Risk do not have an adversarial component) until the conclusion, where it is again claimed that \"We began by proving the convexity of the adversarial risk for the Randomized Ensemble Classifierand deriving a rigorous privacy–utility frontier, thereby providing provable privacy guarantees.\" There also seems to be no such theoretical analysis/proof; the claims made in Section 3.4 (and Appendix B) are relatively straightforward and does not result in a \"rigorous privacy–utility frontier\" as claimed. \n\n2) In addition to the more serious concerns listed above, there are other presentation issues that interrupt the flow of the paper and decrease readability. Some examples include:\n- The datasets for each client $|\\mathcal{D}^{(k)}|$ or their \"local validation sets\" used during training are never properly defined. \n- $\\rho(\\alpha)$ is repetitively defined __three__ times using the same full-line equation (Section 3.3, Section 4.2, Section 5) and is never brought up again. In fact, the only other time $\\rho$ appears is when is it overloaded to denote the learning rate in Algorithm 2. \n- The \"privacy budget\" $\\epsilon$ used in almost all experiments comes out of the blue and is never defined. Only in section 6.4 do we -learn that smaller values of $\\epsilon$ corresponds to larger perturbation radius, which is unclear why this is the cae.\n- Some tables and figures, such as Figure 4 are hard to read / interpret. While the text claims that \" a consistent trend emerges across all datasets\" in Figure 4, the subsequently described trend is not at all easy to see from the figure, since the lines clearly demonstrate non-monotonicities and the scale is hard to read.\n\n3) While the experiments compare FedBARRE to other methods using different \"ensemble sizes\" and \"privacy budgets\", they still use a single attack and do not test for different parameters of the FL settings (number of clients etc) or the parameters of the other methods, which brings into question the robustness of the utility/privacy gain achieved by their method in different settings."}, "questions": {"value": "Can you address the inconsistencies listed in item (1) in the weaknesses above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XnClXNbPS5", "forum": "ZFKZ8dQ1EL", "replyto": "ZFKZ8dQ1EL", "signatures": ["ICLR.cc/2026/Conference/Submission11263/Reviewer_HxEv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11263/Reviewer_HxEv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883830194, "cdate": 1761883830194, "tmdate": 1762922420196, "mdate": 1762922420196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedBARRE to address gradient inversion attacks in Federated Learning (FL). \nThe core of FedBARRE is an ensemble of classification heads where input data is perturbed with randomized noise.\nExperiments demonstrate that FedBARRE achieves better performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is generally well-structured and easy to follow.\n\n* The experimental results demonstrate that the proposed method is effective in achieving a better privacy-utility balance against DLG."}, "weaknesses": {"value": "* The core privacy contribution of Randomized Ensemble Classifier seems to stem from introducing randomness rather than a robust privacy mechanism. This type of random defense has been previously utilized in the literature to obfuscate gradients and deter deterministic reconstruction attacks [1]. The paper lacks a convincing analysis of its resilience against adaptive attacks [2].\n\n* The objective function, specifically the inner minimization, is questionable. $\\delta$ is optimized to minimize the loss, which means finding a perturbation that makes the model perform better on the perturbed data $(x+\\delta)$. This formulation suggests that the model parameters are optimal to $(x+\\delta)$.\n\n* The paper lacks formal privacy or utility proofs typically required for defensive techniques.\n\n* The experimental evaluation is not sufficiently comprehensive. It lacks comparison against several state-of-the-art methods [3,4]. Moreover, the paper omits any analysis of the computational cost and communication overhead.\n\n[1] Precode - a generic model extension to preventdeep gradient leakage\n\n[2] Bayesian Framework for Gradient Leakage\n\n[3] See through gradients: Image batch recovery via gradinversion\n\n[4] Refiner: Data refining against gradient leakage attacks in federated learning"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fmi2JsX5G6", "forum": "ZFKZ8dQ1EL", "replyto": "ZFKZ8dQ1EL", "signatures": ["ICLR.cc/2026/Conference/Submission11263/Reviewer_FhJw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11263/Reviewer_FhJw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964084791, "cdate": 1761964084791, "tmdate": 1762922419826, "mdate": 1762922419826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses gradient leakage in horizontal FL with a semi‑honest server observing per‑client gradients. FedBARRE trains, per mini‑batch, an ensemble of M local classifiers under PGD‑bounded benign perturbations; each client mixes clean/perturbed losses, validates to pick the best submodel, and uploads only that submodel’s gradient, while the server aggregates per mini‑batch (FedSGD‑like). Experiments on MNIST, FMNIST, and CIFAR‑10/100 use 4 IID clients; attacks are simulated using Inverting Gradients (IG), and DLG is executed on the first client’s gradients during rounds 9–11. Utility is test accuracy; privacy uses MSE/PSNR/SSIM; baselines include standard local DP mechanisms (e.g., Wei et al., 2021), PPFA, and Noise‑Add; ablations vary ensemble size M and a privacy budget with privacy–utility curves."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Implementable pipeline: Clear workflow and pseudocode (Figure 1; Algorithms 1–2) make adoption in FedSGD‑style systems straightforward.\n2. Practical measurement: Uses interpretable leakage metrics (MSE/PSNR/SSIM) and presents privacy–utility fronts (e.g., Figure 3).\n3. Tunable knobs with ablations: Documents effects of ensemble size M and privacy budget on privacy/utility (Section 6.3 Table 3; Section 6.4 Figure 4)."}, "weaknesses": {"value": "1. Theory–implementation gap: Section 4.2 optimizes ensemble weights (α) but Algorithm 2 implements deterministic best‑of‑M selection without learning/sampling α.\n2. Guarantee wording inconsistency: The paper says “not a formal privacy guarantee” (Sec 3.4) while the Conclusion claims “provable privacy guarantees.”\n3. Narrow evaluation and no systems costs: Small IID setup (4 clients, 30 rounds), IG (with DLG executed in rounds 9–11) on a single client, no secure‑aggregation/aggregated‑only setting, and no compute/comm/memory/energy reporting."}, "questions": {"value": "1. How will α be aligned with the implementation? Will you implement learning/sampling of α (e.g., with entropy/temperature) or formally justify best‑of‑M as the intended one‑hot solution and revise Section 4.2 accordingly?\n2. Can you broaden and quantify the evaluation? Add non‑IID and larger‑K runs, an aggregated‑only (secure aggregation) scenario, and report cost curves versus M/PGD steps/privacy budget."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sLPlzEmBC2", "forum": "ZFKZ8dQ1EL", "replyto": "ZFKZ8dQ1EL", "signatures": ["ICLR.cc/2026/Conference/Submission11263/Reviewer_LwnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11263/Reviewer_LwnY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966178285, "cdate": 1761966178285, "tmdate": 1762922419459, "mdate": 1762922419459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}