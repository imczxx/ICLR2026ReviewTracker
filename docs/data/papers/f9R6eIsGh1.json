{"id": "f9R6eIsGh1", "number": 8307, "cdate": 1758078081538, "mdate": 1759897792940, "content": {"title": "Primal-Dual Direct Preference Optimization for Constrained LLM Alignment", "abstract": "The widespread application of Large Language Models (LLMs) imposes increasing demands on safety, such as reducing harmful content and fake information, and avoiding certain forbidden tokens due to rules and laws. While there have been several recent works studying safe alignment of LLMs, these works either require the training of reward and cost models and incur high memory and computational costs, or need prior knowledge about the optimal solution. Motivated by this fact, we study the problem of constrained alignment in LLMs, i.e., maximizing the output reward while restricting the cost due to potentially unsafe content to stay below a threshold. For this problem, we propose a novel primal-dual DPO approach, which first trains a model using standard DPO on reward preference data to provide reward information, and then adopts a rearranged Lagrangian DPO objective utilizing the provided reward information to fine-tune LLMs on cost preference data. Our approach significantly reduces memory and computational costs, and does not require extra prior knowledge. Moreover, we establish rigorous theoretical guarantees on the suboptimality and constraint violation of the output policy. We also extend our approach to an online data setting by incorporating exploration bonuses, which enables our approach to explore uncovered prompt-response space, and then provide theoretical results that get rid of the dependence on preference data coverage. Experimental results on the widely-used preference dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.", "tldr": "", "keywords": ["Large language models (LLMs)", "safety or constrained alignment", "rigorous theoretical guarantees for LLM alignment", "primal-dual direct preference optimization (DPO)"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53f4323e12219cdb20941c45fbe583eb52e53512.pdf", "supplementary_material": "/attachment/c27f6edb393b6d5f8bf961f61ebf1e3ffd32f756.zip"}, "replies": [{"content": {"summary": {"value": "The author introduce a approach called Primal-Dual Direct Preference Optimization (PD-DPO). The key contribution: \"rearranged Lagrangian DPO objective\" (Eq. 14). Since preferences are collected separately for rewards ($r$) and costs ($c$), DPO cannot be directly applied to the Lagrangian $L(\\pi; \\lambda) = r - \\lambda c$. The authors rearrange the optimality conditions to express the cost preference likelihood using the reward function. They then substitute the unknown reward $r$ with the implicit reward information captured by a policy $\\pi_{\\hat{r}}^{\\ast}$, which is pre-trained using standard DPO on reward preferences. The proposed algorithm (Algorithm 1) iteratively updates the policy (primal step) using this rearranged objective and updates the Lagrange multiplier $\\lambda$ (dual step) via projected subgradient descent. The dual update relies on estimating the current policy's cost using online binary human feedback.\nFor experiments, they demonstrate PD-DPOâ€™s superior performance compared to SFT and SafeDPO, while improving harmlessness over SFT. However, it is significantly less safe than the computationally intensive Safe RLHF baseline (Beaver-v3.0)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The challenge of applying DPO when preferences are separated by objective (reward vs. cost) is large. The author clearly expresses cost in terms of reward and the optimal Lagrangian policy, and then substituting the reward using a pre-trained standard DPO policy is a clever solution (bypassing the need for explicit reward and cost modeling during constrained optimization).\n2. Extension to the online setting. The reliance on comprehensive offline data coverage is a bottleneck for DPO. The incorporation of exploration bonuses directly into the rearranged DPO objective to guide exploration in a constrained setting is novel.\n3. The adoption of a primal-dual framework for the algorithm is theoretically sound compared to methods that require prior knowledge or the sweeping of the Lagrange multiplier. By integrating the dual update directly into the optimization loop, the algorithm considers the optimal tradeoff between reward maximization and constraint satisfaction."}, "weaknesses": {"value": "1. In Appendix B, lines 700-701, you mention that you fix the Lagrange multiplier $\\lambda$ to 5 due to computational limits, instead of running the dual update. This might limit the empirical validation of the paper. Compared to Beaver-v3.0, the results in Figure 1 only validate the rearranged DPO objective with a fixed penalty. Was any experimental validation done on the primal-dual part for solving the constrained problem?\n2. The cost estimation procedure required for the dual update is expensive in the context of real-world LLM training. require: sampling $N^{CE}$ responses and asking $M^{CE}$ human annotators for binary feedback at every iteration K. It is an synchronous demand for human interaction inside the training loop, which is slow, expensive, and likely high variance. The massive annotation cost that convergence would incur, as suggested by the obtained bound in Theorem 1, seems to outweigh the computational gains of avoiding reward/cost model training.\n3. The results of this paper rely on the accuracy of $\\pi_{\\hat{r}}^{\\ast}$. I assume this policy, trained by the standard DPO, is used as a fixed stand-in for the reward signal on Eq. 16 throughout the constrained optimization. Therefore, any error, bias, or overly optimistic compromise in $\\pi_{\\hat{r}}^{\\ast}$ will be carried over to the PD-DPO training. You did not account for this in your experiments; what happens if $\\pi_{\\hat{r}}^{\\ast}$ is a poor candidate?\n4. Calculating these exploration bonuses in O-PD-DPO also requires constructing and inverting covariance matrices based on the feature representations $\\phi(x,y)$ (also mentioned in Appendix D). This attempt to work in the high-dimensional space of LLMs is computationally infeasible.\n5. PD-DPO also shows a harmfulness gap to Beaver-v3.0 (e.g., Elo score < 1100 vs ~1400); this trade-off is unacceptable in safety-critical applications. You argued that this trade-off is expected for the gains in efficiency,  but reduction in safety suggests the method struggles to enforce constraints strictly (perhaps exacerbated by the use of a fixed $\\lambda$)."}, "questions": {"value": "1. Any experimental results that validate the full Algorithm to show the trajectory of $\\lambda_k$, rewards, and costs across iterations K? \n2. Any clarification on the practicality of the cost estimation step (Algorithm 1, Line 4). Can you quantify the total human annotation effort required for convergence as suggested by your theory? Given this burden, how do you propose making the dual update feasible in a practical LLM pipeline? Have you considered using an offline-trained cost model just for the dual update estimation to avoid human-in-the-loop feedback?\n3. How sensitive is the PD-DPO framework to the quality of the initial standard DPO model? Errors in this model directly impact the optimization objective (Eq. 16). Any ablation study on the performance of the final constrained policy when $\\pi_{\\hat{r}}^{\\ast}$ is trained with varying amounts of reward preference data?\n4. Any additional experiments conducted for O-PD-DPO? \n5. [related to weakness 5]] (and just some suggestions) Have you analyzed the Pareto front achievable by PD-DPO (e.g., by varying the initial $\\lambda_1$ or the optimization parameters, assuming the full algorithm is run)? Is it possible to achieve higher safety with PD-DPO, even at the cost of some helpfulness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EdGYM3ljHi", "forum": "f9R6eIsGh1", "replyto": "f9R6eIsGh1", "signatures": ["ICLR.cc/2026/Conference/Submission8307/Reviewer_QQdF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8307/Reviewer_QQdF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896863426, "cdate": 1761896863426, "tmdate": 1762920235773, "mdate": 1762920235773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a direct preference optimization like procedure for safety constrained alignment, that foregoes the use of trained reward models and constraint costs by using labels to directly construct a supervised Lagrangian objective. They provide near optimality and feasibility guarantees for problems solved by dual ascent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "All existing constrained alignment approaches explicitly train reward and cost models, learning directly from helpfulness and safety  preference binary labels is both novel in the context of constrained alignment and relevant. The paper provides near feasibility and optimality last iterate guarantees under standard assumptions."}, "weaknesses": {"value": "The two stage approach fits a model using DPO and then uses the implicit reward given by this model in the training objective (e.q. 14). It then uses a labeling oracle for the safety cost. This is indeed a novel and reasonable approach. My main concern is that statement that the main advantage of their method is not fitting reward and cost models might be overstated in the sense that (1) the first phase is indeed fitting a reward model and (2) the second phase swaps a cost model for a labeling oracle.\n\nThe big memory gains with respect to using explicit reward an cost models disappear if the reward and costs are evaluated offline for the dataset of prompts and responses - responses are not sampled at each training step but at the end of each training epoch or primal update step. This is indeed the approach used in prior work (Huang et. al and Zhang et. al) that use DPO style losses.\n\nAlso, the formulation of Lagrangian maximization using DPO style losses was proposed at least in Huang et. al (referenced in the submission), and doing dual super-gradient descent on lambda by sampling the model and estimating the slack in Zhang et. al (also referenced in the submission) The only discussion about the distinction between this prior work and the proposed approach is the aforementioned lack of explicit cost and reward models. Although these prior works also have feasibility and optimality guarantees, there is no discussion how do the theoretical results compare to those.\n\nFinally, the experiments present a single run with a single constrained baseline, where the method performs comparatively poorly in terms of safety. Without more experiments it is hard to evaluate how the proposed approach performs in terms of helpfulness/harmlessness trade offs (i.e. its pareto optimality) and, more importantly, wether it empirically succeeds at obtaining near feasible solutions to support the theoretical results."}, "questions": {"value": "Can you provide additional experimental evidence supporting the performance of your method? The only point you provide does very badly in terms of the constraints compared to the baseline. The number of experiments is very limited, only a single run/performance is reported.\n\nCan you point out the relation of your theoretical results to those in Zhang et. al (referenced in the submission) ? \n\nCan you comment on the choice of constraint threshold and perhaps do an ablation on its impact?\n\nCan you include plots of dual variable dynamics and or their values in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xvcSLBkMLi", "forum": "f9R6eIsGh1", "replyto": "f9R6eIsGh1", "signatures": ["ICLR.cc/2026/Conference/Submission8307/Reviewer_kvKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8307/Reviewer_kvKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917282962, "cdate": 1761917282962, "tmdate": 1762920234777, "mdate": 1762920234777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For both offline and online constrained alignment problem of LLMs, this work develops a novel primal-dual DPO approach which does not require trained reward and cost models or prior knowledge of the optimal Lagrange multiplier. Then the performance of this algorithm is demonstrated by theoretical convergence rate of suboptimality and cost violation, and experimental results."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The targeted problem of LLM safety alignment is important. The algorithm design looks novel, by obtaining analytical solution of cost and reward. The presentation is clear."}, "weaknesses": {"value": "The major problem is the costly algorithm and problematic convergence bounds, as shown in the questions 1 and 2 below respectively."}, "questions": {"value": "(1) Algorithm 1 looks costly due to the following reasons. \n\n(1a) Eq. (15) trains $\\pi _ { \\hat{r} } ^ {\\star}$ instead of $\\hat{r}$. It seems that similar to algorithms that train $\\hat{r}$, we also need to train and save two large models. What's the advantage? \n\n(1b) Does line 4 require human annotation in every iteration? Some online DPO-type algorithms use advanced LLM to automatically annotate newly generated samples. \n\n(2) Issues about the convergence bounds: The bounds in Theorem 1 contains $B$, whose last two constants contains algorithm generated variables $\\pi_k$ and cannot be guaranteed small. The final term of $B^{\\rm on}$ for Theorem 2 seems to go to $+\\infty$ as $K\\to+\\infty$. Could you prove that it goes to 9 as $K\\to+\\infty$? \n\n**I'd like to raise my rating if questions 1-2 can be solved well.**\n\n(3) Right above Eq. (8), it seems that safe RLHF has access to only $\\mathcal{D}^c$ but not $\\mathcal{D}^r$? \n\n(4) In the experiment, what evaluation model is used in model-based evaluation? Could you list the hyperparameters for the other algorithms? \n\n(5) Optional: Do you think the primal algorithm [1] will work on constrained alignment problem, which does not require Lagrange multipliers? \n\n[1] Xu, T., Liang, Y., \\& Lan, G. (2021, July). Crpo: A new approach for safe reinforcement learning with convergence guarantee. In International Conference on Machine Learning (pp. 11480-11491). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JUSWhxyODa", "forum": "f9R6eIsGh1", "replyto": "f9R6eIsGh1", "signatures": ["ICLR.cc/2026/Conference/Submission8307/Reviewer_EtwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8307/Reviewer_EtwF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950342683, "cdate": 1761950342683, "tmdate": 1762920234397, "mdate": 1762920234397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a constrained alignment problem in the framework of direct preference optimization (DPO). The authors reduce this problem to a Lagrangian dual problem, where the Lagrangian maximizer is evaluated by minimizing a preference-based optimization objective function. A key idea is to introduce a preference-based presentation of a reward model. Based on this formulation, the authors propose a preference-based primal-dual algorithm: PD-DPO. Furthermore, the authors provide the optimality and constraint violation guarantees, both with and without the preference data coverage assumption. Finally, the authors conduct a Safe RLHF experiment to show effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors study a constrained alignment for LLMs based on preference data. This is an important direction for aligning LLMs with specific human values, since constrained generation is required in LLM applications. \n\n- The authors present a method that allows us to solve the Lagrangian dual problem in the DPO style. A key feature is that the proposed training algorithm: PD-DPO doesn't require direct knowledge of reward and cost models. \n\n- The authors provide iteration complexity guarantees of the proposed algorithm in terms of objective and constraint functions, both with and without the preference data coverage assumption."}, "weaknesses": {"value": "- It would be helpful if the authors could have a table to compare the proposed method with previous preference-based methods in terms of model/algorithm assumptions and computational efficiency. For instance, the authors mention previous preference-based methods (Liu et al. (2024b); Huang et al. (2024); Zhang et al. (2025); Kim et al. (2025)) need to regenerate preference data.   \n\n- The main idea of the proposed method is the policy-based representation of the combined reward and cost function in Equation (12). However, this assumes the Bradley-Terry model for a mixed human preference. It is important to explain in what extent this assumptions is valid in practice. \n\n- The proposed algorithm: PD-DPO utilizes the cost binary feedback from human annotators, which assumes an off-shell cost model. This type of labeling assumption is also used in previous works (Liu et al. (2024b); Huang et al. (2024)). It is useful to clarify their differences. \n\n- The iteration complexity guarantees seem to be limited to theoretical interest, since it assumes optimization steps of PD-DPO are solved exactly. \n\n- The data coverage assumption in Section 4.3 is strong, since it assumes coverage over polices for all iterations. The exploration bonus analysis in Section 5 assumes the Bradley-Terry model, which is not explicitly mentioned in the main paper.\n\n- Another weakness of this paper is the limited experimental evaluation in terms of data sets, and baseline methods."}, "questions": {"value": "Additional to suggestions in Weaknesses, below are some other questions.\n\n- Notation c is abused in Section 4.1. \n\n- Since reward and cost are unknown, how to determine bound constants in (17) and (18)?\n\n- What is rho in Assumption 1? How to choose it based on preference data?\n\n- How large is the data coverage-related constants in Theorems 1 and 2?\n\n- Math writing should be improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7ojSpqQSmJ", "forum": "f9R6eIsGh1", "replyto": "f9R6eIsGh1", "signatures": ["ICLR.cc/2026/Conference/Submission8307/Reviewer_VPsQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8307/Reviewer_VPsQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8307/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210274071, "cdate": 1762210274071, "tmdate": 1762920233940, "mdate": 1762920233940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}