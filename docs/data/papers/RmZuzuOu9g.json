{"id": "RmZuzuOu9g", "number": 22407, "cdate": 1758330659885, "mdate": 1759896867868, "content": {"title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "abstract": "Large-language-model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long-horizon navigation, large-scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks—navigation, information extraction, and execution—so the model concentrates on one skill at a time, and (ii) continuously re-plans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts end-to-end success rates by up to 13.7 percentage points over previous state-of-the-art agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps. Code will be publicly available.", "tldr": "WebDART tackles complex web tasks by splitting navigation, extraction, and analysis, and adding dynamic re-planning, reducing navigation burden and boosting success by +13.7% on WebChoreArena.", "keywords": ["LLM", "Agent", "Web Agent", "Task Decomposition"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d20e8eabcfce626c97939969a499b02f4cdcaf39.pdf", "supplementary_material": "/attachment/aea490857515bd1208af33fc1e2793a22c70f5a2.zip"}, "replies": [{"content": {"summary": {"value": "A novel LLM agent framework (named, WebDART) is proposed to automate long-horizon complex web tasks, with hierarchical planning and adaptation. Sub-task decomposition allows the agents to dedicate their power to the allocated focus scope, which is designed with an in-depth analysis of the target problem and insights into human cognitive behaviors. At the same time, WebDART prompts the agent to dynamically re-plan based on their experiences by leveraging discovered shortcuts. The efficacy of WebDART is verified in two benchmarks: WebChoreArena and WebArena. The proposed method significantly outperforms the baselines across various LLM backbones in the first benchmark, and demonstrates remaining competency in the second testbed indicating a descent balance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In this section, I demonstrate the strengths of this paper.\n\n1. Solid motivation: The design behind the framework stems from grounded observations that complex tasks overload the common agent frameworks. The rationale behind the structural heuristics is also reasonably backed up by mentioning that the quality of sub-tasks differs from each other. The authors also point out the possible limitations, revealing the necessity of the second component (i.e., adaptation). \n2. Empirical supports: While navigating all the possible pages to get the task-relevant information is demanding, it can often be exhaustive. Table 2 demonstrates that such limitations can be significantly overcome with their proposed method. These results strongly support the effectiveness of the design of WebDART.\n3. Case study analysis: The case study analysis allows readers to understand how WebDART performs well in practice. The readability of the study is high, as it is organized as a concise table."}, "weaknesses": {"value": "Here, I present the weaknesses/questions/suggestions of this work.\n\n1. Missing discussions/references: In the related work section, as this work focuses on prompting-based agent frameworks, discussions on possible strengths/limitations compared to fine-tuning methods would make the paper more solid. Currently, the authors compare with AgentSymbiotic, as a representative of finetuning-based methods, but with a lack of depth. I provide several finetuning-based methods, which I hope will be discussed [1,2,3]. To clarify, comparisons with these agents in experiments don’t seem very demanding.\n2. Cost: How much did it cost to run all the experiments, including the baselines? I believe that cost information allows comparing the compute resources used between the baselines, as well as easy estimation of requirements when experimenting with WebDART as a baseline for other research.\n3. Analysis on multi-agent baselines: I think more comparisons with multi-agent frameworks can be included. Mainly, it’d be interesting to see where the main differentiation towards success arises in the pipeline, compared to other multi-agent frameworks. \n4. Marginal improvements on WebArena: While the authors stated that many tasks do not demand complex sub-task planning in WebArena, it is still questionable why the WebDART agents do not gain much in this benchmark, as this phenomenon signals a possibility of biased design in the WebChoreArena benchmark. To be fair, there are stronger baselines in WebArena [4]. I think that the authors should discuss more to clarify this, as (at least) discussing what improvements in WebDART can make it outperform the baselines. Also, the “bypassing” mechanism should be elaborated.\n\n\nReferences:\n\n[1] Qi et al., “WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning” (ICLR 2025).\n\n[2] Lee et al., “Learning to contextualize web pages for enhanced decision making by LLM agents” (ICLR 2025).\n\n[3] Qin et al., “UI-TARS: Pioneering Automated GUI Interaction with Native Agents” (preprint 2025).\n\n[4] https://webarena.dev/."}, "questions": {"value": "Questions and suggestions for the authors are listed in the above weaknesses section for brevity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lvfHVGAzSv", "forum": "RmZuzuOu9g", "replyto": "RmZuzuOu9g", "signatures": ["ICLR.cc/2026/Conference/Submission22407/Reviewer_3GZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22407/Reviewer_3GZA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528203741, "cdate": 1761528203741, "tmdate": 1762942206423, "mdate": 1762942206423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WEBDART, a framework that enables large language model (LLM) agents to handle complex web tasks that require long-horizon reasoning and structured exploration. It dynamically decomposes each task into three subtasks: (1) navigation, (2) information extraction, and (3) execution, allowing the model to focus on one ability at a time. During navigation, the agent adaptively re-plans its strategy when new filters or interface shortcuts appear, reducing redundant actions and improving efficiency. This modular and adaptive design enhances task completion and robustness in complex web environments while maintaining strong performance on simpler tasks. Overall, WEBDART demonstrates that dynamic decomposition and real-time re-planning can significantly improve the reasoning and adaptability of LLM-based web agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-Justified Motivation**: The paper effectively addresses the importance of long-horizon web tasks as a fundamental challenge in current web-agent research.\n\n- **Clear Writing and Organization**: The paper is well-written and easy to follow, with a well-organized structure and clear presentation of the proposed approach.\n\n- **Simple Yet Effective Design**:This paper employs an intuitive three-stage decomposition that mirrors how humans naturally approach complex web tasks, resulting in a method that is both easy to understand and practically effective."}, "weaknesses": {"value": "- **Lack of empirical justification for the conservative decomposition scheme**: The paper adopts the conservative scheme (deferring constraint handling to later stages) as the default strategy. However, this design choice is not supported by any preliminary analysis, empirical comparison, or prior evidence—for example, there is no ablation or user study contrasting conservative versus tightly coupled decompositions. Given that the efficiency of each scheme “hinges on site features” (line 204), a fixed conservative default appears heuristic rather than data-driven, and its general validity across domains remains unclear. A short pilot experiment or reference to earlier literature on adaptive task partitioning would strengthen this methodological decision and clarify why the conservative bias is justified beyond intuition.\n\n- **Heuristic Nature of Information Extraction**: The information extraction pipeline is heuristic, relying on LLM prompts to select relevant pages and extract fields without any quantitative validation or ablation. The paper explains that the model “returns an index set that marks the pages most likely to contain the required information,” yet provides no concrete mechanism or evidence to show how reliable this selection is. Furthermore, the dismissal of the LLM-generated parser baseline is entirely qualitative, lacking any comparative results or failure statistics. Overall, the decision to rely solely on prompt-based extraction appears intuitive rather than experimentally justified, leaving uncertainty about its robustness and reproducibility across diverse web structures.\n\n- **Lack of In-depth Performance Analysis**: While the paper reports overall success rates on the WebChoreArena benchmark [1], it does not provide finer-grained analyses that could strengthen its empirical claims. In the original benchmark, performance is typically broken down by cross-site domains as well as by task types such as Calculate, Long-Term Memory, Massive Memory, and Other. However, WEBDART’s results are aggregated, making it unclear which categories drive the observed improvements. The absence of such detailed breakdowns limits the interpretability of the reported gains and prevents deeper insights into where the proposed method truly excels or struggles.\n\n[1] Miyai, Atsuyuki, et al. \"WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks.\" arXiv preprint arXiv:2506.01952 (2025)."}, "questions": {"value": "- In Section 4.2, the authors claim that the observed improvements “highlight the advantage of shifting constraint handling to the data analysis stage.” However, it is unclear how the empirical results in Table 1 specifically support this interpretation. Could the authors clarify what evidence connects the performance gains to this design choice? \n\n- Table 3 reports the Results on the WebArena benchmark and includes additional baselines such as HybridAgent [1] and WebPilot [2], which show competitive performance. How do these baselines perform on WebChoreArena, and were they excluded due to reproducibility constraints or unavailability of results? \n\n- As an ablation, how does performance change when the routing module is disabled, particularly on the WebArena benchmark? It would be helpful to know how much accuracy drops and what types of routing errors occur (e.g., skipping extraction when it is actually required). Additionally, could the authors provide a brief analysis of the common failure cases in WEBDART?\n\n[1] Song, et al. \"Beyond browsing: Api-based web agents.\" arXiv preprint arXiv:2410.16464 (2024).\n\n[2] Zhang, et al. \"Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration.\" AAAI 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DdAIlm2W7K", "forum": "RmZuzuOu9g", "replyto": "RmZuzuOu9g", "signatures": ["ICLR.cc/2026/Conference/Submission22407/Reviewer_RREB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22407/Reviewer_RREB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722253332, "cdate": 1761722253332, "tmdate": 1762942206210, "mdate": 1762942206210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free framework that improves LLM-based web agents by dividing complex objectives into navigation, information extraction, and execution subtasks while dynamically revising plans as new web elements appear. This modular and adaptive design allows the agent to focus on one skill at a time and adjust strategies in real time, leading to up to 13.7% higher accuracy and 14.7 fewer navigation steps on complex benchmarks, without sacrificing performance on simpler tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1) Clarity and Organization\n- The paper is clearly written and well structured. It effectively identifies the limitations of existing web tasks and presents a complex yet well-justified task definition along with a corresponding solution. The modular framework design and clear categorization of sub-tasks make the methodology easy to interpret. Figures and tables are informative and greatly aid understanding of the workflow.\n\nS2) Technical Soundness and Contribution\n- The overall structure and writing are coherent, and the paper presents a convincing motivation for introducing adaptive re-planning, emphasizing its necessity within dynamic web navigation. The proposed WebDART methodology is applied successfully, demonstrating meaningful improvements over existing baselines.\n\nS3) Competitive Performance\n- The approach achieves strong quantitative results, showing robustness across different web-based benchmarks and supporting the validity of the proposed framework."}, "weaknesses": {"value": "W1) Lack of Novelty\n- The proposed approach primarily integrates existing techniques rather than introducing a fundamentally new concept. While the composition of prior methods is well executed, the paper lacks clear methodological or theoretical innovation that distinguishes it from prior work.\n- Moreover, the claimed design motivation—being inspired by human web search behavior—lacks supporting evidence from prior studies or pilot experiments. Including such references or empirical validation would strengthen this claim.\n\nW2) High Computational and Monetary Cost\n- The framework’s multi-stage navigation process, including dynamic re-planning decisions, likely leads to frequent LLM calls and thus high computational and monetary costs. To substantiate the framework’s practical usability, the paper should include quantitative evidence such as the number of LLM invocations per episode or the overall inference cost, along with a discussion of trade-offs between performance and efficiency.\n- An efficiency analysis comparing total inference cost, time, or action steps with other methods would be valuable, especially since different baselines may not define navigation steps equivalently.\n\nW3) Limited Generalization and Evaluation Scope\n- The framework heuristically decomposes web tasks into three sub-tasks, but it remains unclear whether this decomposition generalizes across diverse web task categories.\n- Experiments are conducted only on two benchmarks—WebArena and WebChoreArena—which, despite differing in complexity, share similar task structures. Consequently, the evaluation does not sufficiently demonstrate robustness to broader and more heterogeneous web task types.\n- The authors should evaluate the framework on at least one additional web-based agent task (e.g., GAIA [2] or SimpleQA [3]), as the current setting appears tailored to the WebArena family.\n- In addition, the prompt design for navigation planning (“navigating through menus and links,” “interacting with buttons and controls”) imposes a strong prior, raising further concerns about generalization capability.\n\nW4) Outdated Baseline Comparison\n- In Tables 1 and 2, most baseline performances are cited from prior studies, resulting in comparisons primarily against older methods.\n- To more convincingly demonstrate the proposed approach’s effectiveness, the paper should include comparisons with more recent web-agent methodologies (for example, WebWalker [1]).\n\nW5) Lack of Analytical Depth\n- The current analysis is shallow. Reporting only accuracy and average steps provides limited insight.\n- Additional ablation studies—such as the number of re-planning events, subtasks per instruction, their distribution, and the effect of fast-path routing—would offer deeper analytical value and better explain the framework’s behavior under different task conditions.\nReferences\n\n[1] Wu et al. WebWalker: Benchmarking LLMs in Web Traversal. Arxiv preprint. 2025\n[2] Mialon et al. GAIA: A Benchmark for General AI Assistants. ICLR 2023\n[3] Jason et al. Measuring short-form factuality in large language models. Arxiv preprint. 2024"}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WPdHfJOklk", "forum": "RmZuzuOu9g", "replyto": "RmZuzuOu9g", "signatures": ["ICLR.cc/2026/Conference/Submission22407/Reviewer_bz3S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22407/Reviewer_bz3S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956197234, "cdate": 1761956197234, "tmdate": 1762942205910, "mdate": 1762942205910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WebDART, a training-free framework for LLM-based web agents that improves performance on long-horizon, multi-step web tasks. The method dynamically decomposes complex objectives into three subtasks: navigation, information extraction, and execution. It also allows continuously re-planning for these subtasks as new webpage elements appear.\nWebDART aims to reduce the overload by letting a single frozen LLM focus on one sub-capability at a time and to improve sample efficiency by adapting plans on the fly. Results on WebArena and WebChoreArena show empirical effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Paper is well-written and easy to understand.\n2. Strong quantitative results: The WebDART framework seems to be quite effective and achieves consistent improvements across three model backbones and different web domains."}, "weaknesses": {"value": "Major:\n\n1. Limited novelty: The three core modules used in WebDART, i.e., navigation, extraction, and execution, have been widely used as standard prompting paradigms and is commonly seen in recent web agent works. The design is also quite heuristic and is only supported by intuition rather than systematic error analysis of previous work.  \n2. Lack of learning or adaptation: All components in WebDART are purely prompt-engineered and rule-based. There is no learning or self-improvement involved in the method to adapt the policy itself  and learn a truly intelligent agent. This limits the framework’s scalability and robustness when deployed beyond the benchmark environments.\n3. Baseline selection: The baseline comparison focuses on earlier methods, e.g., Table 3 misses many recent baselines on the WebArena  leaderboard with much stronger results, so the credibility of many claims on empirical effectiveness needs to be questioned.\n4. Missing related work on multi-agent web navigation systems.\n\nMinor:\n1. Missing citation on line 111."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I81X9nRQkw", "forum": "RmZuzuOu9g", "replyto": "RmZuzuOu9g", "signatures": ["ICLR.cc/2026/Conference/Submission22407/Reviewer_6YZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22407/Reviewer_6YZq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050668425, "cdate": 1762050668425, "tmdate": 1762942205647, "mdate": 1762942205647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}