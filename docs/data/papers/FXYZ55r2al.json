{"id": "FXYZ55r2al", "number": 17594, "cdate": 1758277971692, "mdate": 1759897165945, "content": {"title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "abstract": "Real-world multi-hop question answering (QA) often involves ambiguity that is inseparable from the reasoning process itself. This creates a distinct challenge where multiple reasoning paths emerge from a single question, each requiring independent ambiguity resolution. Because each sub-question can be ambiguous, the model must resolve ambiguity at every step; thus, answering a single question requires handling multiple layers of ambiguity throughout the reasoning chain. We find that current large language models (LLMs) struggle in this setting, typically exploring incorrect reasoning paths and producing incomplete answers. To facilitate research on multi-hop ambiguity, we introduce MIRAGE (Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions), a benchmark designed to analyze and evaluate this challenging intersection of ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142 high-quality examples of ambiguous multi-hop questions, categorized under a taxonomy of syntactic, general, and semantic ambiguity, and curated through a rigorous multi-LLM verification pipeline. Our experiments reveal that even state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity combined with multi-step inference is a distinct and significant challenge. To establish a robust baseline, we propose CLARION (Clarifying Ambiguity with a Reasoning and Instruction), a multi-agent framework that outperforms existing approaches on MIRAGE and points toward more adaptive, ambiguity-aware reasoning systems.", "tldr": "This paper introduces MIRAGE, a 1,142-example benchmark for multi-hop ambiguous QA, and shows that a multi-agent approach (CLARION) outperforms current RAG-based baselines.", "keywords": ["Ambiguity", "Agent", "Dataset", "Multi-hop"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da3eb1e90a5b22985faea11d1459eb77d01521f8.pdf", "supplementary_material": "/attachment/7143f47831418172bf66621698e7284dbea9ceb1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MIRAGE, a benchmark targeting the intersection of multi-hop reasoning and ambiguity, and proposes CLARION, a two-stage agentic framework (planning→acting) that first detects/classifies/clarifies ambiguity and then performs ReAct-style retrieval and synthesis. MIRAGE (1,142 examples) provides type labels (syntactic/general/semantic), clarification pairs, supporting passages, short answers, and integrated long answers, built via multi-LLM consensus and stringent filtering. Experiments across strong LLMs show sizable degradation on multi-hop ambiguity and consistent gains from CLARION, especially due to clarification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper crisply formulates multi-hop ambiguity with a clear taxonomy (syntactic/general/semantic) and aligns each type to actionable LLM behaviors, then validates it with a rigorously constructed benchmark (multi-LLM consensus, evidence-backed short/long answers).\n\n2. The CLARION framework operationalizes “clarify-before-retrieve” via a planning→acting pipeline and demonstrates consistent, meaningful gains over strong baselines, isolating clarification as the key driver through careful ablations."}, "weaknesses": {"value": "1. The benchmark’s “multi-hop + ambiguity” is synthesized from MuSiQue/Wikipedia, yielding tidy linguistic ambiguities that underrepresent real tasks. In practical settings (e.g., diagnosing a GitHub repo issue), ambiguity spans versions, environments, conflicting reports, and evolving evidence across issues/PRs/logs—far beyond entity sense/parse choices. Thus MIRAGE likely misestimates real complexity and offers limited guidance for authentic, high-noise workflows.\n\n2. CLARION’s pipeline (detect→type→clarify→ReAct) mirrors MIRAGE’s annotation/evaluation protocol, inviting prior bias. The paper lacks transfer tests to non-Wikipedia, multi-source, or tool-using scenarios (e.g., code, tables, logs), and no user-study/human-judged deployment evidence. Without cross-domain validation or robustness to noisy/contradictory evidence, claims of broader utility remain unsubstantiated.\n\n3. The paper does not compare against a minimal prompt-driven agent loop that many practitioners would try first. For example, a single-model ReAct-style baseline with a concise system prompt such as: “You are an ambiguity-aware, retrieval-assisted QA model. For any question, enumerate plausible interpretations, retrieve evidence per branch, verify coverage, and produce a consolidated long answer. Be precise and avoid hallucinations.” This simple setup may capture much of the claimed benefit without multi-agent orchestration and needs to be included for comparison.\n\n4. As a benchmark paper, the evaluated model set is narrow. It omits stronger reasoning-centric models/variants (e.g., reasoning-tuned or tool-use–optimized LLMs), which might already handle multi-hop ambiguity well. Without broader model coverage, it is hard to assess whether MIRAGE is genuinely challenging or merely exposes gaps in the specific models tested."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96gjpIHqGZ", "forum": "FXYZ55r2al", "replyto": "FXYZ55r2al", "signatures": ["ICLR.cc/2026/Conference/Submission17594/Reviewer_vvS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17594/Reviewer_vvS5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652363814, "cdate": 1761652363814, "tmdate": 1762927455371, "mdate": 1762927455371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIRAGE, a rigorously constructed benchmark targeting multi-hop question answering under ambiguity, and provides the CLARION agentic baseline for systematic ambiguity resolution in open-domain multi-hop QA.  MIRAGE is derived via a multi-stage pipeline involving four distinct LLMs, resulting in 1,142 annotated examples with diverse ambiguity types, explicit clarification steps, and both short- and long-form answers. Comprehensive experiments show that modern LLMs underperform on MIRAGE, highlighting the intrinsic challenge of ambiguity in multi-hop QA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MIRAGE is the first benchmark to focus explicitly on ambiguity in multi-hop QA\n\n2. The authors propose CLARION, a multi-agent framework, as a robust baseline to tackle the MIRAGE benchmark. This provides a starting point for future research\n\n3. The results report evaluation over a wide range of state-of-the-art LLMs and retrieval-augmented baselines."}, "weaknesses": {"value": "1. As shown in Table 2, the dataset skews heavily toward semantic ambiguity, making the current evaluation results also rely on the ability to deal with the semantic ambiguity. Besides, it seems that the authors do not provide details results for each category.\n\n2. The dataset size is small, and whether we can use the dataset to increase the ability to deal with the ambiguity with such a small size is questionable. Besides, the dataset is constructed by filtering and processing a single existing dataset. This reliance on one source might limit the diversity of domains.\n\n3. The benchmark's construction and quality control pipeline is heavily dependent on LLMs. Though the human evaluation is included, the details of the human evaluation are missing, including who did the evaluation, and 60 samples are not very large.\n\n4. Though the dataset is constructed for multi-hop QA, there is no special ambiguity type for the multi-hop setting. It seems like the dataset captures ambiguity within questions but not ambiguity that arises across reasoning hops, such as dependency conflicts or path fusion errors.  Besides, the paper doesn't clearly distinguish its dataset's challenge from a simpler single-hop ambiguity problem. In detail, the experiments show that models like GPT-4 do poorly on the new MIRAGE dataset (the hard, multi-hop ambiguous questions). But the paper never shows that these same models do well on the easy (single-hop) version of these questions. For example, it doesn't prove that GPT-4 can correctly answer a simple, single-hop question like \"Where is Paris?\" but then fails on the multi-hop version. Because this simple experiment is missing, we don't know if the models are failing because of the multi-hop reasoning (the paper's claim) or just because they were always bad at handling that \"Paris\" ambiguity in the first place."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NnXi3yD3xp", "forum": "FXYZ55r2al", "replyto": "FXYZ55r2al", "signatures": ["ICLR.cc/2026/Conference/Submission17594/Reviewer_jmuS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17594/Reviewer_jmuS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974726101, "cdate": 1761974726101, "tmdate": 1762927451886, "mdate": 1762927451886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MIRAGE, a new benchmark for evaluating large language models (LLMs) on multi-hop reasoning under ambiguity. The dataset contains over 1K carefully curated instances where each question exhibits syntactic, general, or semantic ambiguity, accompanied by clarified sub-questions, supporting passages, short answers, and an integrated long answer. MIRAGE is constructed through a four stage multi-LLM pipeline involving ambiguity detection, clarification, document retrieval, answer generation, and cross model alignment filtering to ensure high quality annotation. The authors further introduce CLARION, a two stage multi-agent framework combining a Planning Agent for ambiguity detection and clarification with an Acting Agent performing ReAct-style reasoning to search and synthesize final answers. Experiments across strong LLMs demonstrate that CLARION outperforms RAG-based and LLM-only baselines on three complementary metrics and LLM-as-a-Judge, showing the benefit of explicit ambiguity modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tProposes a benchmark for evaluating ambiguity-aware multi-hop reasoning.\n2.\tIntroduces a baseline framework integrating planning and reasoning agents.\n3.\tReveals consistent limitations of current LLMs in handling ambiguous multi-hop questions."}, "weaknesses": {"value": "1.\tLimited model scope.\nThe analysis of LLM limitations is based only on mid-sized open models (Qwen3-235B, LLaMA-4 variants, GPT-OSS-20B/120B), without including top-tier models such as GPT-4/5 or Gemini-1.5, making it unclear whether the observed weaknesses generalize to current sota systems.\n2.\tNarrow baseline coverage.\nThe comparison focuses mainly on No-Retrieval, NaiveRAG, and DIVA, omitting several relevant categories of baselines such as (i) ambiguity-focused QA frameworks (e.g., AmbigDocs, AmbigQA), (ii) compositional reasoning agents (e.g., ReAct, Self-Ask, CoT+RAG), and (iii) ambiguity-aware retrieval methods (e.g., AmbiRAG, TACO). Including representatives from these families would offer a more comprehensive evaluation of CLARION’s relative effectiveness.\n3.\tDataset imbalance.\nAs noted in Appendix B, the MIRAGE dataset is heavily skewed toward semantic ambiguity, with far fewer examples of syntactic or general ambiguity. This imbalance limits the benchmark’s representativeness and its ability to test model robustness across ambiguity types.\n4.\tHigh system complexity and limited practicality.\nCLARION introduces a multi-agent, multi-stage pipeline (Planning → Acting) that improves accuracy but also increases computational cost and implementation overhead. Its scalability and real-world applicability remain untested.\n5.\tModerate methodological novelty.\nWhile the task formulation is new, the CLARION framework mainly recombines existing paradigms. Its reasoning loop closely follows ReAct, and its clarification stage resembles DIVA’s multi-interpretation rewriting. The innovation lies more in system integration and benchmarking than in novel algorithmic techniques."}, "questions": {"value": "1.\tDo the authors observe similar limitations in ambiguity-aware multi-hop reasoning even for frontier models such as GPT-4o or GPT-5? If so, could they provide evidence or discussion on whether the observed weaknesses generalize beyond mid-sized open models?\n2.\tCould the authors add or report results for additional baselines, such as ambiguity-focused QA frameworks (e.g., AmbigDocs, AmbigQA) or reasoning agents (e.g., ReAct, Self-Ask, CoT + RAG)? It would be valuable to see how CLARION compares to these stronger or more specialized methods.\n3.\tWhat is the complexity–performance trade-off of CLARION? Specifically, how does the additional planning and acting loop affect token consumption and runtime cost, relative to the performance improvements shown in Table 4?\n4.\tSince the MIRAGE dataset is heavily skewed toward semantic ambiguity, overall accuracy may not fully reflect model robustness. Could the authors report per-type performance (syntactic / general / semantic) to show whether CLARION performs consistently across ambiguity categories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FP0Wpsdt0B", "forum": "FXYZ55r2al", "replyto": "FXYZ55r2al", "signatures": ["ICLR.cc/2026/Conference/Submission17594/Reviewer_tRyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17594/Reviewer_tRyh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990466515, "cdate": 1761990466515, "tmdate": 1762927451089, "mdate": 1762927451089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a QA dataset consisting of ambiguous multi-hop queries, combining the two known factors (ambiguity and multi-hop reasoning) that challenge QA systems. The paper furthermore offers a new model that is specifically tailored to this problem, that improves performance over vanilla LLMs and several retrieval-based alternatives. I am generally in favor of small-scale, targeted datasets, and I think the dataset is potentially useful to the community if some issues I raise below turn out to be not serious. The model perhaps less so since it's very specific to the problem at hand, but I think the claimed contribution is for it to be a strong baseline for the proposed dataset, and it serves that purpose well. I have various conceptual issues that holds me back from assigning a higher rating (details below), but maybe this can be resolved through discussions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work addresses a dataset gap in the literature (Table 5), and the dataset will be useful for some subset of our community.\n- The steps taken to construct the dataset seem generally sensible."}, "weaknesses": {"value": "- The scope of the contribution seems slightly narrow, although this isn't a make-or-break weakness.\n- Missing various details about human annotators: e.g., who they are (their qualification and background), recruitment process, how they were compensated, etc. There is some information in the Appendix but seems insufficient.\n- Human inter-annotator agreement for human evaluation should also be reported (and maybe also LLM evaluator agreement just out of interest?). I also do feel like 20 questions per type is a bit too few although I appreciate the analysis existing. \n- Several conceptual issues:\n\t- \"General ambiguity\" seems like a bad term for the type of ambiguity the authors are shooting for. This makes is sound like a catch-all category, although it actually means something very specific about over-constrained queries that lead to a null set answer. I also have doubts about whether this category should really be treated as an ambiguity at all, because this is now based on inferences about the intent of the question above and beyond what is actually stated, and that intent may or may not even be true (I think this might be my confusion with the \"city center\" question, see below under presentation clarity issues). Maybe the question asker truly intended the narrow scope query and if so, the answer should be \"Unanswerable\" - I think this is actually how people have treated such queries in prior work.\n\t- Although not strictly a weakness but more of an untouched issue is the treatment of many-way ambiguities: see Questions..\n- Some presentation clarity issues throughout the paper. A few examples:\n\t- Interpretation of Figure 2: First, I'm not sure if I understand why this is a syntactic ambiguity. The first bifurcation of the interpretation seems to be reference ambiguity of the phrase \"a Nobel Chemistry laureate\", which is not syntactic ambiguity (this actually makes me question the validity of the ambiguity categories proposed, so maybe this is more than a presentation clarity issue). Second, the sub-question steps don't really make sense to me: the sensical sub-questions route \"Which Nobel Chemistry laureate is being referred to?\" and then the two questions \"Which element discovered by Curie?\" and \"Which element discovered by Seaborg?\", and then finally the actual dates. I also think there's another higher-level ambiguity here where is a version of the question that is not actually ambiguous, which is about the chronologically first instance of an element discovered by a Nobel Chemistry laureate being officially recognized by the international union. Then there is genuinely one answer - the instance that comes first in history. I think this also connects to the issue I will discuss in Questions about multi-way ambiguities. \n\t- The rationale behind L158-180 was difficult to understand. The query literally says \"city center\", why should we expect that committing to city center is premature and consider the broader metropolitan area? Is it because it could not be the case that the Olympics being queried about is only held at the city center? (i.e., the assumption here that the absence of an answer to the query requires generalization of the query to find closest valid answer, but I'm not sure if this is justifiable) I'm not sure if this can really be grouped as an ambiguity type, since the ambiguity itself is of the kind that is covered in other categories (the reference ambiguity of \"the Olympics\"), but what's different is the scope of the query (which is not about ambiguity at all)."}, "questions": {"value": "My main questions concern the n of disambiguated questions I have three questions regarding this. (1) is about the statistics/coverage and what the implication of this is for the proposed dataset, (2) is a more general question about what an ideal QA system should be doing, and (3) is a more nitpicky question about the potential of combinatorial explosion. \n\n(1) A question can be n-way (n>=2) ambiguous, as you also note in your definition of clarifying question in L238. What is the average number of n in your dataset? I could see it potentially being quite large especially for questions like your illustrative example in Figure 3. While only two-way ambiguity is shown in the figure, according to Wikipedia page for Paris (https://en.wikipedia.org/wiki/Paris_(disambiguation) it is a LOT more than 2-way ambiguity. Does the dataset cover all of these cases (how can one even check)? If it only offers a partial coverage, isn't this problematic in that it encourages model development that only rewards a specific subset of all possible ambiguity?\n\n(2) In the case of many-way ambiguities like the Paris one, I'm not sure what a QA system even should do. If I, as a user, were asking questions like the one in Figure 3, I clearly don't want to have 40 different answers even if it were truly 40-way ambiguous. I'm curious whether you have any thoughts on this.\n\n(3) A question could technically display more than 1 ambiguity types: e.g., it could be syntactically and semantically ambiguous, or it could exhibit more than one semantic ambiguity. In this case, there could be a combinatorial explosion of ambiguities. Are there cases like this in the dataset and how should we deal with this? I think the general issue I'm getting at is that there needs to be a way to rank \"worth considering\" ambiguities among many technically possible ambiguities, and there is a clear limitation in only treating ambiguities that multiple LLMs agree on as true ambiguity or ambiguities worth considering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HgTBvDseXj", "forum": "FXYZ55r2al", "replyto": "FXYZ55r2al", "signatures": ["ICLR.cc/2026/Conference/Submission17594/Reviewer_8JCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17594/Reviewer_8JCs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142991514, "cdate": 1762142991514, "tmdate": 1762927450703, "mdate": 1762927450703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}