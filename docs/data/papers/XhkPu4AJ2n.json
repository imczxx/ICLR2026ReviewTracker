{"id": "XhkPu4AJ2n", "number": 13894, "cdate": 1758224528537, "mdate": 1763738589355, "content": {"title": "Beyond Dual Representations: Collaborative Learning for Semi-Supervised LiDAR Semantic Segmentation", "abstract": "Annotating large-scale LiDAR point clouds for 3D semantic segmentation is costly and time-consuming, motivating the use of semi-supervised learning (SemiSL). Standard SemiSL methods typically rely on a single LiDAR representation in a two-stage framework, where consistency between identical models is enforced under input perturbations. However, these approaches treat pseudo-labels from a single network as fully reliable, which reinforces architectural biases and propagates errors during distillation, ultimately limiting student performance. Recent dual-representation methods alleviate this issue but still remain constrained by the limitation of two-stage design. We introduce CoLLiS, a novel framework that leverages Collaborative Learning for LiDAR Semi-supervised segmentation. Unlike prior paradigms, CoLLiS trains multiple representations collaboratively in a single stage by treating them as coequal students. Cross-representation distillation is adaptively balanced by monitoring inter-student disparities to mitigate confirmation bias and improves robustness. Extensive experiments on three public benchmarks show that CoLLiS consistently enhances the performance of all participating models and achieves superior results compared to state-of-the-art LiDAR SemiSL methods. The code will be released upon acceptance.", "tldr": "", "keywords": ["collaborative learning", "semi-supervised training", "LiDAR semantic segmentation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82021b7885b764ac44a263f86eef13cb3f5b1ac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript introduces CoLLiS, a collaborative learning framework for semi-supervised LiDAR semantic segmentation. Unlike traditional two-stage SemiSL methods that rely on a single representation and pseudo-label distillation, or recent dual-representation approaches with cross-view training, CoLLiS treats multiple LiDAR representations (range-view, voxel, polar, etc.) as coequal students trained in a single stage. \n\nThe key ideas of this work are: \n1. consensus-driven augmentation that adjusts perturbation strength based on inter-student agreement.\n2. adaptive pseudo-labeling and distillation balancing absolute and relative reliability among models. \n3. post-training ensemble to further consolidate multi-representation knowledge. \n\nExperiments across nuScenes, SemanticKITTI, and ScribbleKITTI show consistent improvements over state-of-the-art SemiSL baselines, especially under scarce-label settings. Ablations further highlight the contributions of adaptive reliability measures, dynamic augmentation, and multi-representation collaboration"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) The manuscript addresses the inherent confirmation bias in SemiSL for LiDAR segmentation and frames it in the context of representation diversity, which is timely and practically important for autonomous driving. Moving beyond dual representation methods (e.g., IT2), CoLLiS generalizes the paradigm to multiple representations in a unified single-stage pipeline, which is conceptually neat and reduces inefficiencies of prior two-stage designs.\n\n(+) The introduction of consensus-driven augmentation and adaptive reliability-based distillation are methodologically sound extensions that directly target SemiSL challenges. The use of both absolute and relative reliability to weigh pseudo-labels is particularly well-justified.\n\n(+) Results across three benchmarks and multiple label ratios demonstrate consistent gains, with notable improvements under 1% and 10% labels. Ablations are comprehensive, showing the additive benefits of each design choice, and efficiency analysis further supports the single-stage design."}, "weaknesses": {"value": "(-) Limited exploration of representation scalability. Although the framework is presented as scalable to multiple representations, most experiments rely on three specific ones (frustum-range, polar, voxel). The performance impact of adding/removing representations is analyzed, but the exploration of “how far” this scalability goes (e.g., with more than three representations, or novel hybrid encoders) is limited.\n\n(-) While the manuscript introduces several well-motivated components, each (consensus-driven augmentation, online collaborative distillation, post-training ensemble) is individually incremental relative to existing SemiSL and CoL frameworks. The novelty mainly lies in their combination for LiDAR segmentation rather than in fundamentally new algorithmic ideas.\n\n(-) Several components (pseudo-labeling, distillation, ensemble) hinge on confidence measures, which are known to be brittle under distribution shift. While effective in benchmarks, the generalizability to real-world OOD or sensor-noise scenarios is not fully demonstrated, for example, Robo3D.\n\n(-) aluation comprehensiveness: Please consider comparing FRNet [Xu, et al.] and LarseMix++ [Kong, et al.] for a more comprehensive evaluation on the semi-supervised LiDAR segmentation benchmark. Additionally, there are several self-supervised LiDAR segmentation methods that are evaluated on the same benchmark; please consider including those as well, such as SLidR [Sautier, et al.], Seal [Liu, et al.], and SuperFlow [Xu, et al.].\n\nReferences:\n\n- FRNet [Xu, et al.] FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation. TIP, 2025.\n- LarseMix++ [Kong, et al.] Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving. TPAMI, 2025.\n- SLidR [Sautier, et al.] Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. CVPR, 2022.\n- Seal [Liu, et al.] Segment Any Point Cloud Sequences by Distilling Vision Foundation Models. NeurIPS, 2023.\n- SuperFlow [Xu, et al.] 4D Contrastive Superflows are Dense 3D Representation Learners. ECCV, 2024.\n\n---\n\n**Minor Suggestions**\n\n(-) Please expand discussion of potential weaknesses in confirmation bias mitigation: for example, do disagreements among weak students introduce noise despite adaptive weighting?\n\n(-) Include visualizations or failure cases for extremely rare long-tail classes (beyond bicycles), and further discuss why collaboration does not fully overcome extreme imbalance.\n\n(-) Clarify the computational trade-offs more directly: while training efficiency is reported, inference cost under ensemble use could be highlighted.\n\n(-) Improve figure readability (e.g., small font sizes in Fig. 3 and Fig. 4) for clarity."}, "questions": {"value": "The paper is well-motivated, tackling the important issue of confirmation bias in SemiSL for LiDAR segmentation, and presents a clean extension from dual to multi-representation collaborative learning. The empirical results are strong and broadly convincing across multiple benchmarks, particularly in scarce-label settings. However, the technical novelty is a little bit moderate, as the main contributions combine existing ideas (collaborative learning, confidence-based pseudo-labeling, augmentation adaptation) rather than introducing a fundamentally new principle. The improvements at higher label ratios are incremental, which tempers the overall impact.\n\nThat said, the manuscript is solidly executed, well-written, and relevant to the ICLR community, and it could be of interest given the growing focus on SemiSL and multi-representation learning for 3D perception. With minor refinements and broader baseline coverage, this work has potential for acceptance. With additional efforts in addressing the main weaknesses and minor suggestions (as detailed above), the manuscript could have a strong impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xd8Q5u5OLP", "forum": "XhkPu4AJ2n", "replyto": "XhkPu4AJ2n", "signatures": ["ICLR.cc/2026/Conference/Submission13894/Reviewer_qjPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13894/Reviewer_qjPV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761384027687, "cdate": 1761384027687, "tmdate": 1762924403798, "mdate": 1762924403798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of semi-supervised semantic segmentation for LiDAR point cloud data. The authors highlight key limitations in existing two-stage student–teacher frameworks, particularly their reliance on unidirectional knowledge distillation from a single teacher model. This setup can lead to confirmation bias, where erroneous pseudo-labels are repeatedly reinforced during training. Furthermore, extending such two-stage methods to multi-view LiDAR representations introduces significant complexity and computational overhead. To address these issues, the paper introduces a teacher-free collaborative learning framework that distills knowledge directly from student networks using a carefully crafted pseudo-labelling strategy. The approach is further strengthened by a novel data augmentation technique and a post-training ensemble mechanism, both designed to boost model performance. Extensive experiments on nuScenes, SemanticKITTI, and ScribbleKITTI datasets demonstrate that the proposed method consistently outperforms baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-organized and easy to follow.\n- The idea of adopting a single-stage framework, along with the motivation grounded in curriculum learning, is interesting and well-motivated.\n- The proposed method show better training efficiency compared with the baseline methods.\n- The paper have provied extensive experiments to demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "- The authors may consider clarifying what $M_1$, $M_2$, and $M_3$ represent in Figure~1. It is unclear whether these refer to specific LiDAR representations, model architectures, or transformation outputs. A brief explanation in the figure caption or main text would improve clarity.\n- Is it possible that the predictions are biased toward a single modality, potentially limiting the benefits of cross-representation collaboration?\n-Eq.7 relies on filtering pseudo-labels based on the confidence score $c(P_{s1})$. Could the authors provide an analysis of how the coverage of confident predictions changes from early to late training stages? Specifically, how much of the unlabeled data is retained or discarded over time due to the dynamic threshold?\n- In Table. 9, the performance of CoLLiS-Hete appears to be lower than baseline methods such as IT2 and LaserMix under certain settings. I would appreciate it if the authors could elaborate on the underlying reasons for this discrepancy. Specifically, it would be helpful to understand whether architectural incompatibility, representation redundancy, or training instability might have contributed to the observed gap."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0gvHMz4dR4", "forum": "XhkPu4AJ2n", "replyto": "XhkPu4AJ2n", "signatures": ["ICLR.cc/2026/Conference/Submission13894/Reviewer_5ZQn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13894/Reviewer_5ZQn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404589893, "cdate": 1761404589893, "tmdate": 1762924403402, "mdate": 1762924403402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CoLLiS, a semi-supervised LiDAR semantic segmentation framework that integrates multi-representation collaborative learning, consensus-driven augmentation, and adaptive pseudo-label distillation. The method aims to alleviate confirmation bias and improve model generalization. Experiments on three benchmarks — nuScenes, SemanticKITTI, and ScribbleKITTI — demonstrate that CoLLiS outperforms existing semi-supervised approaches under low-annotation settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Proposes a novel single-stage multi-representation collaborative framework that attempts to mitigate confirmation bias in semi-supervised LiDAR segmentation.\n2. Conducts extensive experiments across multiple datasets and label ratios, showing consistent performance improvements.\n3. Introduces a consensus-driven augmentation (CDA) and adaptive pseudo-labeling mechanism that demonstrate a degree of methodological innovation."}, "weaknesses": {"value": "1. Unsubstantiated claim of “confirmation bias mitigation.” The paper repeatedly claims that CoLLiS alleviates confirmation bias（line 25）, yet no quantitative definition or measurable evidence is provided. There is no analysis of pseudo-label precision/recall, confidence distribution, or inter-model consistency to verify that bias indeed decreases. Merely reporting an mIoU improvement does not prove bias reduction. From a learning-theory perspective, confirmation bias refers to self-reinforcing model errors under pseudo-labeling. Without metrics or curves showing how pseudo-label quality evolves, the claim remains unsupported and unverifiable.\n2. Lack of empirical proof of “confirmation bias can be amplified when multiple representations produce inconsistent pseudo-labels.”(line 231) While the method assumes that multi-representation learning provides complementary information, the paper does not empirically verify such synergy. Specifically, it omits: 1）Pairwise pseudo-label agreement or error-overlap matrices between representations; 2）Comparison between “independent training + ensemble” and “online collaborative training”; 3）Analysis of whether one model can compensate when another degrades (i.e., recovery behavior). Therefore, the supposed collaboration effect remains anecdotal rather than demonstrated.\n3. Weak theoretical support and no convergence or bias-correction analysis. The paper provides no theoretical explanation of the optimization objective, no analysis of pseudo-label distribution convergence, and no discussion of the method’s upper bound or failure cases. Without such analysis, the work stays at the empirical stacking level and lacks a clear learning principle.\n4. Limited improvement on long-tailed classes. Figure 6 shows that under the 1% labeled regime, the bicycle class IoU remains extremely low (≈3%), indicating that the approach still fails to handle severely imbalanced or rare classes effectively, despite overall mIoU gains.\n5. Pseudo-label reliability is not quantified. The adaptive pseudo-label weighting depends on model confidence, but the paper does not provide curves of pseudo-label accuracy during training or any calibration analysis. It is unclear whether pseudo-labels actually become more accurate as training progresses.\n6. Reproducibility concerns.The code is not released, and critical hyperparameters (e.g., \\delta_0 and \\lambda_0) lack justification or tuning methodology. Without implementation details or theoretical grounding, it is difficult for readers to reproduce or validate the results."}, "questions": {"value": "Please refer to the weaknesses for my main concerns.\nIn addition, I would like the authors to clarify the following:\n1. Definition and measurement: How do the authors define confirmation bias quantitatively in this context, and what concrete evidence (e.g., pseudo-label precision/recall curves, confidence calibration, or inter-model disagreement) supports that CoLLiS actually mitigates it?\n2. Collaborative effect validation: Can the authors provide ablation studies comparing (a) independently trained models with ensemble inference and (b) online collaborative training, to demonstrate that real cross-model synergy exists?\n3. Reliability of adaptive pseudo-labeling: How does pseudo-label accuracy evolve during training? Have the authors evaluated the correlation between confidence weighting and true pseudo-label correctness?\n4. Long-tail robustness: What mechanisms in CoLLiS specifically target rare or long-tailed categories? Could class-balanced pseudo-label filtering or re-weighting further improve performance on rare classes like bicycle?\n5. Theoretical grounding: Does the optimization objective of CoLLiS guarantee convergence or stability? Can the authors provide at least empirical loss-curve evidence that collaborative training remains stable?\n6. Hyperparameter rationale: Will the code be released? On what theoretical or empirical basis were \\delta_0 and \\lambda_0 chosen, and how sensitive is the performance to these values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sXp4M23Rb8", "forum": "XhkPu4AJ2n", "replyto": "XhkPu4AJ2n", "signatures": ["ICLR.cc/2026/Conference/Submission13894/Reviewer_A5B5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13894/Reviewer_A5B5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068186289, "cdate": 1762068186289, "tmdate": 1762924402993, "mdate": 1762924402993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoLLiS, a collaborative learning framework for semi-supervised LiDAR semantic segmentation. Unlike dual-representation methods such as It Takes Two (IT2), CoLLiS leverages three distinct LiDAR representations (Frustum-Range, Polar, and Voxel) in a joint, single-stage setup, where all models are coequal learners. The key contributions include the CDA strategy and two dynamic distillation weights: AR and RR. These mechanisms aim to allow collaborative learning across diverse representations while mitigating confirmation bias. CoLLiS achieves superior performance over prior methods across three datasets (nuScenes, SemanticKITTI, ScribbleKITTI), particularly in low-label regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. As shown in Table 1, CoLLiS consistently outperforms all SOTA methods—including IT2—across the nuScenes, SemanticKITTI, and ScribbleKITTI benchmarks and under all label ratios. Notably, the performance gain is most pronounced in extremely low-label scenarios.\n2. Despite utilizing three models, CoLLiS achieves faster training speed and lower GPU memory usage than IT2 (which employs only two models), as shown in Table 5. This demonstrates that the single-stage collaborative design of CoLLiS is highly efficient.\n3. The long-tail class analysis in Figure 6 shows that CoLLiS significantly improves the performance of rare classes, which are most vulnerable to confirmation bias. This confirms that CoLLiS effectively addresses the central issue it targets.\n4. The paper validates its mechanisms with extensive experiments.\n\t- The main Ablation Study clearly demonstrates that each component of CoLLiS (AR, RR, CDA, and +Polar) contributes statistically significant improvements to the overall performance.\n\t- Appendix experiments (C.3, Table 12) prove that the adaptive AR/RR scheme is a core contribution, significantly outperforming a fixed-weight approach (by +2.1pp at 1% labels).\n\t- Appendix C.1 (Table 9) also highlights the framework's flexibility, showing performance gains even when collaborating different architectures (CNN+ViT) on a single representation."}, "weaknesses": {"value": "1. The core idea, a multi-representation, multi-model semi-supervised framework, amounts to a form of learned ensemble training. The use of multiple representations and cross-model consistency is not conceptually new and resembles straightforward model ensembling with inductive bias diversity. The paper lacks a clear distinction between CoLLiS’s collaborative learning and post-hoc ensembling of independently trained models. Notably, the empirical analysis does not isolate whether CoLLiS's gains are intrinsic to joint training or could be achieved by independently training each model and ensembling their predictions.\n\n2. While Table 5 reports that CoLLiS is more memory- and time-efficient during training compared to IT2, this comparison omits a critical consideration: inference-time cost. Since CoLLiS relies on three full LiDAR segmentation models at inference, the required compute and memory footprint is substantially higher than most single-model or dual-representation systems.\nThis raises concerns about practical deployability in autonomous driving, where low-latency and resource-constrained inference is a critical requirement. The paper does not address how CoLLiS could be compressed, distilled, or otherwise optimized for real-time use.\n\n3. IT2 is described as a “two-stage” method, but this characterization appears inaccurate. Like CoLLiS, IT2 also performs end-to-end learning with dynamic pseudo-label generation. This misframing weakens the methodological comparison.\n\n4. Conflated Contributions: The paper's core contribution (AR/RR collaboration) and IT2's core contribution (GMM contrastive learning) appear to be orthogonal. The paper does not evaluate CoLLiS with GMM, making it unclear whether the gains are from the CoLLiS framework, or simply from using more models (3 > 2).\n\n5. Missing Recent Baselines: While the baselines are relatively recent, for an ICLR 2026 submission, the paper is missing comparisons to more recent SOTA methods from 2024 and 2025 (e.g., DDSemi (CVPR 2024), HiLoTs (CVPR 2025)). Including these would strengthen its SOTA claims.\n\n6. Questionable Scalability: The paper justifies using three models as a “balanced trade-off” (Table 6) and does not explore N > 3. This implies that for N > 3, the training cost likely outweighs the performance gains, suggesting a practical scalability limit.\n\n7. (Minor) On page 8, Table 4 the citation for IT2 is incorrectly listed as Kong et al. (2023b) (LaserMix). It should be Liu et al. (2025)."}, "questions": {"value": "1. The reproduced IT2 performance in Table 1 is significantly lower than the results reported in the original IT2 paper. Could the authors specify the exact experimental differences (e.g., backbone, resolution) that led to this discrepancy and justify the fairness of the SOTA comparison?\n2. The performance and efficiency gains may stem from using the FRNet backbone rather than FIDNet. Could the authors provide a dual-representation ablation comparing CoLLiS (FRNet, Voxel) vs. CoLLiS (FIDNet, Voxel) to isolate the framework's true advantage? Furthermore, are the efficiency gains (Table 5) also a result of this backbone difference?\n3. The core mechanisms of CoLLiS (AR/RR collaboration) and IT2 (GMM-based contrastive learning) do not appear to be mutually exclusive. Have the authors considered incorporating IT2’s GMM-based contrastive loss into the CoLLiS framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ayy9IxWAGp", "forum": "XhkPu4AJ2n", "replyto": "XhkPu4AJ2n", "signatures": ["ICLR.cc/2026/Conference/Submission13894/Reviewer_J17C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13894/Reviewer_J17C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13894/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155234385, "cdate": 1762155234385, "tmdate": 1762924402674, "mdate": 1762924402674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}