{"id": "VsEkWh3mXu", "number": 15171, "cdate": 1758248554169, "mdate": 1759897323511, "content": {"title": "SteeringSafety: A Systematic Safety Evaluation Framework of Representation Steering in LLMs", "abstract": "We introduce SteeringSafety, a systematic framework for evaluating representation steering methods across nine safety perspectives including bias, harmfulness, hallucination, social behaviors, reasoning, epistemic integrity, and normative judgment, spanning 17 datasets. While prior work often highlights general capabilities of representation steering, we find there are many unexplored, specific, and important safety side-effects, and are the first to explore them in a systematic way. Our framework provides modularized building blocks for state of the art steering methods, enabling us to unify the implementation of a range of widely used steering methods such as DIM, ACE, CAA, PCA, and LAT. Importantly, this framework allows generalizing these existing steering methods with new enhancements, like conditional steering. Our results on Qwen-2.5-7B, Llama-3.1-8B, and Gemma-2-2B uncover that strong steering performance is dependent on the specific combination of steering method, model, and safety perspective, and that severe safety degradation can arise in poor combinations of these three. We find difference-in-means a generally consistent choice for steering models and note situations where slight increases in effectiveness trade off with severe entanglement, highlighting the need for systematic evaluations in LLM safety.", "tldr": "", "keywords": ["steering", "alignment", "interpretability", "safety", "bias", "hallucination"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/809dc5d36935453468e23dfd93a7a617f2199daa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper outlines a systematic framework for the evaluation of representation steering methods across a variety of safety perspectives, to facilitate the performance on the selected safety perspectives and side effects on the other perspectives. The proposed framework, SteeringSafety, comprises modularized building blocks for representation steering methods such that existing methods can be represented as a combination of these blocks. This modularized breakdown enables the construction of novel representation steering methods by swapping out any of the modules. Besides this framework, the key result in this paper is that strong steering performance depends on the selected steering method, model and safety perspective."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The key contribution of this work is the framework, SteeringSafety. This enables the modularization of representation steering methods and the systematic evaluation of these methods on nine safety perspectives. The safety perspectives are drawn from literature, and suitable datasets and metrics are used for evaluation of methods on each perspective. This modular framework could be very useful for future development of representation steering methods and their subsequent testing. Additionally, the standardization of the tests would enable more consistent benchmarking in the future. The other notable aspect of this framework is the evaluation of entanglement on safety perspectives other than the ones the LLMs are aligned to. \n\nI would also like to commend the authors on the general readability of the manuscript. They have done an admirable job of exemplifying the safety perspectives through the infographic in Figure 1, and opted to evaluate the methods on representation steering methods based on datasets and metrics used in existing literature. Furthermore, the breakdown of existing steering methods in terms of the defined modules (see Table 1) serves as a sound justification for the proposed modularization."}, "weaknesses": {"value": "There are several misleading/confusing statements in this manuscript that need to be clarified prior to publication. The primary point of confusion is the restriction of measuring steering effectiveness on three main perspective axes (line 67) – it is not evident why only three perspectives are selected and which three perspectives the authors are referring to.\n\nIn addition to this, the description and mathematical notation used in Section 3.1.3 is quite confusing – the mathematical form of $\\nu^{\\prime}$ is incomplete as there is seemingly an additional projection operation not included in the mathematical expression. \n\nI have listed my remaining questions and suggestions in the subsequent section."}, "questions": {"value": "**Questions:**\n\n1.\tWhy does the metric for effectiveness (Eq. 1) include a normalization by $1 – y_b$? Wouldn’t this result in effectiveness having a different range than entanglement?\n2.\tLine 267 states that “we search from the 25th to the 80th quantile of the layers” but line 268 states that “prior work has shown steering is more effective in the middle layers.” \n\n  a.\tAre these quantiles based on the values or position of the layers? \n\n  b.\tIf it is based on values, what does the latter statement mean?\n3.\tThe description of directional ablation in Section 3.1.3 (line 286) appears to modify the activations in a direction orthogonally to $\\ni$, which contrasts with activation ablation. Is this correct? If yes, how can you reconcile the difference in direction ablations across the two approaches.\n4.\tIs line 343 supposed to read “decreasing harmfulness” since lower values for harmfulness would be desirable?\n\n\n**Suggestions:**\n\nSection 5 should be moved to earlier in the paper as it is important to establish the current landscape of evaluation of steering methods on safety perspectives. This would better establish the utility of the proposed framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uzpZkjfMy2", "forum": "VsEkWh3mXu", "replyto": "VsEkWh3mXu", "signatures": ["ICLR.cc/2026/Conference/Submission15171/Reviewer_T7in"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15171/Reviewer_T7in"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944812046, "cdate": 1761944812046, "tmdate": 1762925481444, "mdate": 1762925481444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STEERINGSAFETY, a comprehensive framework for evaluating representation steering methods in LLMs across several safety perspectives using subsets from 17 datasets. It aims to systematically assess both the effectiveness and unintended side effects of steering interventions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unifies five popular steering methods under the same benchmark, allowing for the evaluation of effectiveness and the side effects of these methods.\n- Empirically, the results provide valuable insights into entanglement, showing that effectiveness and safety trade-offs vary across models and methods, which is a key contribution to alignment research. \n- The paper is also well-situated within related work and connects systematically to ongoing research in the field."}, "weaknesses": {"value": "- While the framework claims modularity and standardization, it doesn’t provide runtime cost  or inference-time overhead details. \n- Effectiveness and entanglement are reported using scaled averages (in the main paper), which can hide nuanced behavior shifts. The results in the appendix seem more informative.\n- For some categories, the number of used prompts seem a bit low. \n- The ethics statement is minimal (“our goal is to improve safety”). The reliance on LLM-as-a-judge should also be discussed given how it can mask some biases in the evaluation."}, "questions": {"value": "- Can the authors put the definitions of the steering methods’ acronyms earlier in the paper? \n- For categories with relatively few prompts (like social behaviors), how do you ensure statistical robustness in the evaluation? \n- Given the reliance on GPT-4/ llama3 for scoring certain behaviors, how do you account for potential biases or inconsistencies in its judgments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k6k8DuGOo6", "forum": "VsEkWh3mXu", "replyto": "VsEkWh3mXu", "signatures": ["ICLR.cc/2026/Conference/Submission15171/Reviewer_cyiT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15171/Reviewer_cyiT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011539583, "cdate": 1762011539583, "tmdate": 1762925480977, "mdate": 1762925480977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce STEERINGSAFETY as a framework for evaluating how representation steering methods affect safety across multiple dimensions in LLMs. The paper points out that previous research has focused on improving performance or alignment with steering methods, but this work shows that such interventions can also introduce unintended safety side effects.\n\nThey systematically evaluate and compare representation steering methods across nine safety dimensions and 17 datasets, revealing unintended safety trade-offs. Their results show that steering outcomes depend on the specific combination of steering method and model, with some interventions improving some target behaviors but harming others."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper offers a broad and systematic evaluation to assess representation steering methods across multiple safety dimensions (nine perspectives, 17 datasets).\n* The authors provide consider diverse steering techniques (e.g., DIM, ACE, CAA, PCA, LAT) for their experiments.\n* Novel safety metrics: The framework introduces Effectiveness and Entanglement metrics to assess both steering performance and side effects, which allow tradeoff analysis. However a comparison to related metrics used for safety assessment is missing. \n* The paper reveals that steering effectiveness depends strongly on the combination of model & steering method. No single best method exists, and they discuss some method-specific insights, e.g. DIM offers strong performance but at the cost of safety trade-offs.\n* The presented insights can help the interpretability community develop steering interventions with finer behavioral control and fewer unintended harms."}, "weaknesses": {"value": "* The framework identifies safety tradeoffs  but does not clearly explain why the studied methods are related to specific harms. The underlying causal mechanisms remain often unclear to the reader.\n* The evaluation relies on LLM-based scorers, which can introduce bias and reduce interpretability of the results. Wouldn't evaluation with the help of human annotators for at least a subset of results provide more insights? \n* Although the benchmark covers nine safety perspectives, it leaves out some very relevant safety dimensions such as adversarial robustness, long-context reasoning stability, etc.\n* The dynamic testing approach uses only small data subsets (20% of each dataset), which might cause some unintended side effects. Also details on selection of the subsets is missing?\n* Some methodological details, e.g. how layer selection impacts with steering strength, are not discussed in detail > can result in issues with reproducibility.\n* The figures summarize results across models and methods but are are dense and difficult to interpret without further breakdowns"}, "questions": {"value": "1. Have you considered including ablation studiesto show how results change with different parameter/steering settings?\n\n2. Could you provide more details on how LLM judges like GPT-4o or LlamaGuard were prompted across datasets to ensure consistent scoring?\n\n3. Could you include finer-grained results or plots to help interpret the tradeoffs more clearly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e4AoUsQ13a", "forum": "VsEkWh3mXu", "replyto": "VsEkWh3mXu", "signatures": ["ICLR.cc/2026/Conference/Submission15171/Reviewer_3DPr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15171/Reviewer_3DPr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160153484, "cdate": 1762160153484, "tmdate": 1762925479767, "mdate": 1762925479767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper offers a framework for safety assessment for models that looks at safety across seven dimensions, operationalizing each with a measure. The paper further assesses safety of several large models using this framework and applying steering methods that intervene directly on neural network activations, requiring no training. The paper offers the assessment toolchain as a contribution. A major finding is that safety properties are \"entangled\" both in this assessment and in prior work, in that improving performance on one operationalization of safety may harm performance on another."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The notion that multiple operationalizations of safety may be necessary to make sense of safety claims is important, as is the notion that improvement in one sense may harm performance in another. This is reminiscent of various \"no-go\" theorems (although there is no formalism offered here, and there could not be because safety claims are epistemically non-falsifiable) that demonstrate e.g. the no-free-lunch theorem or results from Chouldechova (resp. Kleinberg) that it will be impossible to square different operationalizations of bias except under specific theoretical circumstances (e.g., a perfect predictor or already-equalized feature incidence per class stratum in the dataset) or even Kleinberg's clustering impossibility result.\n+ The paper undertakes a substantial level of effort with regard to benchmarking and evaluation."}, "weaknesses": {"value": "- It is never made clear how the seven framework dimensions were selected or why they were operationalized using the specific measures chosen. While each is a topic that has been studied in much prior work, claims that the framework is \"comprehensive\" must be supported by arguments for the completeness/soundness of the evaluation with regard to some exogenously developed notion of safety hazards/risks not tolerated. Indeed, the \"framework\" reads more like a wish list, making it hard to evaluate its quality against any other proffered \"framework\", like the NIST AI RMF or any company's internal framework.\n- The framework is offered as a contribution, but there is no effort in the paper to evaluate the framework other than to apply it. It is not possible to understand if the results obtained represent a purposive sample of safety benchmarking or if applying the framework in some application would reduce some kind of risk or avoid some defined hazard (this is the standard approach to evaluating safety in most disciplines, or there is some accepted proxy metric). The paper could, for example, argue in favor of this breakdown of safety issues to show that it covers the bulk of concerns in this literature (while a literature survey is probably a different paper, arguing for the \"comprehensive\" or \"systematic\" nature of the taxonomy requires analysis that is not given here).\n- I think the paper suffers from a kind of category mistake in which \"safety\" is a property of the model-under-test and not of the model-in-use in some use case. If it is true (as the paper argues) that improving safety performance in some aspects worsens it in others, the next natural question is about whether \"good enough\" performance exists for some use case. But here, the models are considered only for the use case of \"performance on a selected benchmark\" and the paper in its current form does little to link the benchmark contexts to real-world, outside-the-lab use cases or to argue that the chosen benchmarks reflect those use cases well. Instead, a claim of safety is a claim about avoiding certain defined bad outcomes - how does the analysis approach here do when measured against that goal? One thing the framework can help with is mapping between the risks operationalized by the chosen benchmarks and the use cases in which the models-under-test might be applied; in doing so, the framework can help to navigate the tradeoff identified that \"there is no universal best method that maximizes effectiveness while minimizing entanglement across all models\" [384-386] (a claim so broad it cannot possibly be true).\n- Many of the stated contributions in the intro and conclusion are not substantiated by the paper. In addition to the issues of framework comprehensiveness and usefulness mentioned above, there is no effort in the paper to explain why the assessments are \"systematic\" or \"reproducible\" or \"reliable\". These are empirical claims about the use of the framework to evaluate real applications; showing that it is possible to pose questions for which evaluations lead to quantitative results only matters if there is meaning to be made from the resulting quantifications. Here, I think the paper shows its greatest weakness, in that no such meaning is extracted from the many evaluations provided. There may be contribution within these evaluations, but that is not the way the paper presents itself."}, "questions": {"value": "* Why is this framework \"comprehensive\" or \"systematic\", as the paper title and listed contributions claim?\n* How would a user of this approach gain capacity to navigate the tradeoffs identified in the evaluations performed using the techniques in this work? (Either across dimensions in the framework or across models - and to what extent does each of those degrees of freedom matter?)\n* Can the claims about safety evaluation be toned down such that the evaluation work in the paper supports them sufficiently? How?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U2HUY6MSNh", "forum": "VsEkWh3mXu", "replyto": "VsEkWh3mXu", "signatures": ["ICLR.cc/2026/Conference/Submission15171/Reviewer_NFy8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15171/Reviewer_NFy8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15171/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762291788756, "cdate": 1762291788756, "tmdate": 1762925479137, "mdate": 1762925479137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}