{"id": "jdUAlrwppY", "number": 8800, "cdate": 1758098583104, "mdate": 1763109613898, "content": {"title": "Gaussians on a Diet: High-Quality Memory-Bounded 3D Gaussian Splatting Training", "abstract": "3D Gaussian Splatting (3DGS) has revolutionized novel view synthesis with high-quality rendering through continuous aggregations of millions of 3D Gaussian primitives. However, it suffers from a substantial memory footprint, particularly during training due to uncontrolled densification, posing a critical bottleneck for deployment on memory-constrained edge devices. While existing methods prune redundant Gaussians post-training, they fail to address the peak memory spikes caused by the abrupt growth of Gaussians early in the training process. To solve the training memory consumption problem, we propose a systematic memory-bounded training framework that dynamically optimizes Gaussians through iterative growth and pruning. In other words, the proposed framework alternates between incremental pruning of low-impact Gaussians and strategic growing of new primitives with an adaptive Gaussian compensation, maintaining a near-constant low memory usage while progressively refining rendering fidelity. We comprehensively evaluate the proposed training framework on various real-world datasets under strict memory constraints, showing significant improvements over existing state-of-the-art methods. Particularly, our proposed method practically enables memory-efficient 3DGS training on NVIDIA Jetson AGX Xavier, achieving similar visual quality with up to 80\\% lower peak training memory consumption than the original 3DGS.", "tldr": "", "keywords": ["Gaussian Splatting Training"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/34d66cc1e41f8d62219a83ad39f2f991359af5ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the high memory consumption of 3DGS during training. The authors propose a memory-bounded framework that alternates between iterative growing and pruning of Gaussian primitives to maintain a fixed memory budget. The method achieves comparable or superior rendering quality with lower peak memory than standard 3DGS, enabling on-device training on NVIDIA Jetson Xavier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  This paper proposes a novel grow–prune–compensate strategy that dynamically controls the number of Gaussian primitives during training.\n 2.  Experiments on standard datasets demonstrate both quantitative performance gains and significant memory reduction."}, "weaknesses": {"value": "1.\tThe approach builds upon 3DGS with several engineering improvements. Although effective, the contribution seems incremental rather than fundamentally novel.\n2.\tHow does the method’s training time compare to the baselines? Does the grow–prune strategy introduce additional overhead that offsets the memory savings?\n3.\tThe ablation study is not sufficiently detailed, as it only reports LPIPS on two scenes and does not clearly show the contribution of each component. It would be helpful to further analyze the sensitivity to pruning frequency and memory budget, as well as the trade-off between memory constraints and reconstruction accuracy."}, "questions": {"value": "The provided demo link in abstract appears to be inactive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ENvikzhZfo", "forum": "jdUAlrwppY", "replyto": "jdUAlrwppY", "signatures": ["ICLR.cc/2026/Conference/Submission8800/Reviewer_uUER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8800/Reviewer_uUER"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690106463, "cdate": 1760690106463, "tmdate": 1762920570098, "mdate": 1762920570098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Ku4AhCRX0Y", "forum": "jdUAlrwppY", "replyto": "jdUAlrwppY", "signatures": ["ICLR.cc/2026/Conference/Submission8800/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8800/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763109613263, "cdate": 1763109613263, "tmdate": 1763109613263, "mdate": 1763109613263, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes “Gaussians on a Diet”, a memory-bounded training framework for 3D Gaussian Splatting (3DGS) that iteratively grows and prunes Gaussian primitives to maintain a near-constant low memory footprint during training. The core idea is to alternate between (1) dynamic growth using hybrid position–color gradients, (2) pixel-level Gaussian compensation in high-error regions, and (3) memory-aware pruning based on ray contribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important practical issue: peak training memory in Gaussian Splatting.\n\n2. Provides an organized overview of prior compact 3DGS methods and attempts to unify densification and pruning in one framework.\n\n3. Demonstrates consistency across three datasets, suggesting a working implementation."}, "weaknesses": {"value": "1. Most components (iterative pruning, hybrid gradients, pixel-error-based compensation) are straightforward adaptations of existing ideas.\n\n2. Ablation analysis is incomplete and buried in the appendix. The main paper does not present any ablation results in the core sections, which severely limits the reader’s ability to assess the actual contribution of each proposed component (iterative pruning, hybrid gradients, Gaussian compensation). All relevant analyses only appear in the appendix without quantitative discussion or visual evidence in the main text. This omission significantly undermines the transparency and credibility of the claimed improvements."}, "questions": {"value": "1. How long does your iterative growing-pruning framework take compared to Taming 3DGS? Is there additional computational overhead?\n\n2. The memory usage on Jetson (Table 2b) shows “Ours w/loader 2.98 GB.” Does this include model parameters or just data loading? Please clarify the measurement methodology."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PgdxJov4ba", "forum": "jdUAlrwppY", "replyto": "jdUAlrwppY", "signatures": ["ICLR.cc/2026/Conference/Submission8800/Reviewer_k2gG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8800/Reviewer_k2gG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216541038, "cdate": 1761216541038, "tmdate": 1762920569614, "mdate": 1762920569614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-grained Gaussian primitive control method for 3D Gaussian Splatting (3DGS). The approach dynamically adjusts the number of Gaussians at each iteration using a well-designed pruning and compensation algorithm to achieve improved performance. At the same time, it effectively satisfies memory constraints by maintaining a low peak memory footprint. Experimental results demonstrate that the proposed method achieves superior quantitative performance while using significantly fewer Gaussians compared to the baseline."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This method dives deeply in the drawback of 3DGS, proposes a dynamic grow-and-prune strategy to maintain balanced capacity and efficient optimization.\n- The proposed densification method introduces slight positional shifts and pixel-wise compensation to diversify gradients and enhance detail in high-error regions.\n- By combining position and color gradients, the method accurately identifies blur regions and allocates additional primitives to improve rendering quality.\n- The proposed method achieves superior performance under a lower memory budget, while maintaining a smaller model size without requiring post-training pruning.\n- The paper also train 3DGS on embeded platform which is impressive."}, "weaknesses": {"value": "- Although the number of primitives is reduced, frequent pruning and compensation may introduce extra computational overhead per iteration, which is not clearly quantified.\n- The paper focuses heavily on empirical improvements but lacks a deeper theoretical justification for why frequent grow–prune operations stabilize training or improve convergence"}, "questions": {"value": "Since the main objective of this paper is to reduce training cost, I believe the authors should also report the training time. I am not suggesting that the method necessarily increases training time, but including this metric would make the evaluation more complete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0LgBMvDnK", "forum": "jdUAlrwppY", "replyto": "jdUAlrwppY", "signatures": ["ICLR.cc/2026/Conference/Submission8800/Reviewer_4xwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8800/Reviewer_4xwR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772397567, "cdate": 1761772397567, "tmdate": 1762920569091, "mdate": 1762920569091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces several techniques to limit peak memory usage during the training of 3D Gaussian Splatting (3DGS) scenes. Existing methods run the densification algorithm at a fixed iteration frequency (e.g., every 100 in 3DGS), which inevitably slows convergence to a target number of Gaussians. In contrast, this paper proposes running the growing process (i.e., the densification algorithm) at every iteration, attaining target number of gaussians in a short time. To correctly identify under- and over-reconstructed Gaussians, the method additionally incorporates the color gradient into the densification criterion. To spatially diversify cloned Gaussians, it offsets their positions by the sum of positional gradients over N views, whereas existing methods place them at overlapping positions, causing the original and cloned Gaussians to receive coupled gradients. To refine poorly reconstructed and sparsely covered regions, the method also adds Gaussians in areas with high-error pixels at the median depth. Moreover, it prunes insignificant Gaussians identified by low maximum blending-weight scores at every iteration to control the total number of Gaussians. Experimental results show only minor quality degradation compared to vanilla 3DGS, but a significant reduction in the number of Gaussians, leading to lower peak memory usage suitable for edge devices such as the NVIDIA Jetson AGX Xavier."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a novel densification–pruning scheduling strategy that requires only a limited amount of peak memory, making it suitable for edge devices. Although some statements are arguable, the experiments demonstrate comparable reconstruction performance even with a highly constrained number of Gaussians."}, "weaknesses": {"value": "This paper contains some debatable statements and an insufficient ablation study. I appreciate the impressive results, but additional evidence is needed to support the authors’ arguments. Please see Questions section."}, "questions": {"value": "- My primary concern is training efficiency. How long does this method take to fully train (until 30k iterations)? I assume that since the growing, compensation, and pruning processes are executed at every iteration, the latency of a single iteration is significantly prolonged compared to vanilla 3DGS. Moreover, because this method’s fast-growing strategy ensures that the number of Gaussians quickly reaches the target in the early iterations, the rendering latency also start to surge in early iterations, leading to an overall longer total training time. I suggest that the authors include the measured training time in Table 1 to show training efficiency.\n\n- In line 221, the authors state, \"As shown in Fig. 3, with the two main limitations, existing methods cannot correctly optimize Gaussians under a bounded number of Gaussians.\" I do not think this statement is valid because Figure 3 actually compares rendered images at 5k and 10k iterations between Taming 3DGS and the proposed method, and according to the graph in Figure 2, these methods do not have the same number of Gaussians at the iterations. It appears that Taming 3DGS generates about one-third the number of Gaussians compared to the proposed method at 5k iterations. This fact is important since existing methods like Taming 3DGS still use long-periodic densification, their reconstruction quality at early iterations is expected to be poor. To support the authors’ statement, it would be better to highlight the blurry areas of Taming 3DGS using rendered images of fully trained models.\n\n- From the same perspective, the claim in line 474 that \"our actual rendering quality is superior to Taming 3DGS,\" supported by comparisons in Figure 11 at early iterations (2k, 8k), is not acceptable. This is an unfair comparison because Taming 3DGS’s slow-growing configuration clearly prevents the scene from being fully optimized at early iterations.\n\n- In lines 214–215, the authors mention that \"existing methods grow the Gaussians slowly and achieve the user-specified budget after a long-term period ... This growing strategy limits the representation power due to the limited number of Gaussians.\" I disagree with this statement because existing methods provide hyperparameters to control the details of the densification algorithm. For example, users can adjust the number of splits or clones, lower the positional gradient threshold, or shorten the execution frequency to accelerate Gaussian growth. Therefore, even if existing methods exhibit limited representation power, it may not be due solely to their growing strategy.\n\n- For better readability, it would be a good idea to move some core results from the appendix into the main text, i.e., the ablation study section and Table 2.\n\n- Insufficient ablation study leads me further wonderings:\n1. What was the target number of Gaussians for the baseline?\n2. The authors need to provide a more detailed breakdown to justify their modules. \"Iterative Pruning\" and \"Gaussians Compensation\" alone are not sufficient. I suggest to show the following cases:\n    1. Hybrid gradient-based clone and split\n    2. Position adjustment\n    3. Low-quality, high-error pixel compensation\n    4. Pruning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zy7gsyq6H8", "forum": "jdUAlrwppY", "replyto": "jdUAlrwppY", "signatures": ["ICLR.cc/2026/Conference/Submission8800/Reviewer_WB9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8800/Reviewer_WB9M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995682174, "cdate": 1761995682174, "tmdate": 1762920568570, "mdate": 1762920568570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}