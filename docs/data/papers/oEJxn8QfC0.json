{"id": "oEJxn8QfC0", "number": 13883, "cdate": 1758224384136, "mdate": 1759897406689, "content": {"title": "Auto-SPT: Automating Semantic Preserving Transformations for Code", "abstract": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program’s syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong\nimplementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.", "tldr": "", "keywords": ["LLMs", "Semantic Preserving Transformations", "Clone Detection", "Code Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e35bf66c0608f7bb43cf28acf8769a7c01b5fa1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address a gap in the field of code clone detection, where models are typically trained on clean and structured data but struggle in real-world scenarios where code undergoes a variety of Semantic Preserving Transformations (SPTs).\nTo address this gap, the authors introduce Auto-SPT, a novel framework that automatically generates stronger SPTs, which can be used to train more robust clone detectors or to evaluate the robustness of clone detector models.\nThe authors formally define the concept of diversity for SPTs and show that diversity is a key factor in producing strong transformations. They further demonstrate that state-of-the-art clone detectors perform poorly on transformations generated by Auto-SPT, revealing a lack of robustness for challenging clones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work provides a rigorous formalization for constructing worst-case transformations for clone detectors — i.e., programs that are semantically equivalent but not recognized as clones.\nThis formalization helps identify two key properties of strong SPTs: strength (measured as the distance between original and transformed programs) and diversity. Results in Table 2 and Figure 1 clearly demonstrate the importance of these properties.\n2. The proposed framework, powered by LLMs, automates the design, implementation, and composition of SPTs. This is a novel contribution, as previous methods relied on heuristics and required time-consuming, ad hoc implementations.\nAuto-SPT generates 20 new SPTs, as well as alternative implementations of existing ones, showing that both approaches lead to more challenging transformations.\n3. The paper introduces a new method for combining SPTs using beam search instead of heuristic methods, improving both strength and diversity.\n4. Table 2 convincingly shows that even recent clone detectors struggle with clones generated by Auto-SPT. In particular, the New-20 set could serve as a strong benchmark to test the robustness of Code LLMs in distinguishing semantically equivalent code."}, "weaknesses": {"value": "1. Lack of qualitative analysis: examples of programs generated by Auto-SPT are not shown in the main paper nor in the appendix. Similarly, a more quantitative analysis on what makes these SPTs stronger would help to understand more the differences with previous approaches.\n2. Although a reasonable number of clone detection models are evaluated, several modern frontier models (e.g., GPT-5, Claude Sonnet 4.5), or open-source models (e.g., Qwen, DeepSeek) are missing. Would these models struggle as well with programs from Auto-SPT?\n3. The framework could indeed be used to measure model robustness against adversarial clones. However, it is less clear how Auto-SPT can be used for training. Training experiments are limited to CodeBERT in Python: generalizability of the framework could be improved by training different models and expanding to more languages (there are code clone benchmarks for C++ and Java, as well as data sources similar to CodeContest). Also, it is not shown whether training on Auto-SPT programs might harm other coding capabilities of the LLM.\n4. While Figure 1 and Table 2 demonstrate that Auto-SPT-generated clones are challenging across models, the paper lacks a final experiment showing that a model trained on Auto-SPT-transformed programs also performs well on standard clone detection benchmarks."}, "questions": {"value": "Questions related to weakness #1:\n- Why are programs generated from Auto-SPT more challenging for clone detectors? \n- How do these generated clones differ from those produced in previous papers?\n- Which part of the pipeline - design, implementation, or combination - makes Auto-SPT programs so challenging? \n- Do Auto-SPT programs look more natural (i.e., as code clones we could find in human written repositories)?\n\nQuestions related to weakness #3:\n- Why is training limited to Auto-SPT (Orig-4) in Figure 1? If CodeBERT were trained on the New-20 subset, would it perform better on a test set drawn from the same distribution?\n\nMinor comments:  \n- line 250: missing closing “)” \n- Sometimes the paper is not straightforward to read: I think the definition of Orig-4, Orig-20 and New-20 should be made more clear. Similarly, the last part of the introduction (line 070) can be understood only after reading the experimental part of the manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23tYXTxkQm", "forum": "oEJxn8QfC0", "replyto": "oEJxn8QfC0", "signatures": ["ICLR.cc/2026/Conference/Submission13883/Reviewer_MX1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13883/Reviewer_MX1P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793897546, "cdate": 1761793897546, "tmdate": 1762924394980, "mdate": 1762924394980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Auto-SPT, a framework for automatically designing, implementing, and writing various semantically preserving transformations (SPTs) to evaluate and enhance the robustness of code clone detection models. The authors formalize the concepts of transformation strength and diversity and empirically demonstrate that their automated framework can generate more effective transformations than existing heuristic-based methods. Using these generated transformations for data augmentation can improve model robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a formal analysis linking SPT diversity to transformation strength, with empirical validation on real datasets."}, "weaknesses": {"value": "The novelty is limited. Previously work, such as (Zhang et al., 2023), already described semantic-preserving code transformations for \nevaluating machine learning-based code clone detection models. That paper also presented many semantic preserving transformations as well as their combinations. This paper only briefly mentioned (Zhang et al., 2023) but did not compare with it.\nAlso, there are some recent work for improving the robustness of the clone detection model, which can be compared with too. For example:\n\n[1] Tian Z, Chen J, Jin Z. Code difference guided adversarial example generation for deep code models, ASE 2023.\n\n[2] Huang L, Sun W, Yan M. Iterative Generation of Adversarial Example for Deep Code Models, ICSE 2025.\n\nFurthermore, in lines 159-162, the author claims that SPTs, such as converting a for loop to a while loop, are stronger, while variable renaming is a weaker SPT. This claim lacks empirical evidence and additional experiments are needed to support it. In fact, research has shown that variable renaming can induce model errors effectively (see the above two references).\n\nRQ2 and RQ3 involve too few models (only CodeBERT), which is not enough to support the conclusion. Also, many code models (such as CodeBERT and GraphCodeBERT) are quite old. Many recent, SOTA code models are not evaluated.\n\nIn terms of presentation, it would be better to present real code samples that the new SPT generates, particularly for the 20 new SPTs listed in Table 1.\n\nThe paper uses unit tests to verify the preserved semantics of the generated code, it is not clear after drastic transformations (especially the combinatorial transformations), if unit tests can still perform complete semantic verification."}, "questions": {"value": "How is the proposed approach compared with the related work mentioned in the Weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a87fGlzI2C", "forum": "oEJxn8QfC0", "replyto": "oEJxn8QfC0", "signatures": ["ICLR.cc/2026/Conference/Submission13883/Reviewer_UxyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13883/Reviewer_UxyX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906740982, "cdate": 1761906740982, "tmdate": 1762924394222, "mdate": 1762924394222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the performance degradation of clone detection models after deployment when facing test data distribution shifts. The authors leverage LLMs to generate more diverse code transformations and use these transformations to create test cases that are not present in the training data. The paper validates how clone detection models behave when encountering out-of-distribution cases. Experimental results demonstrate that the proposed approach can generate clone code variants that break existing clone detection models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clone detection is an important problem in software engineering, and exploring distribution shift after model deployment is valuable.\n2. The paper achieves automatic discovery of new code transformations using LLMs.\n3. The results show that LLM-based data augmentation can generate realistic cases that exist in practice but are missing in training data."}, "weaknesses": {"value": "Method:\n\n1. Lines 207–209 mention that naively prompting the LLMs does not work due to lack of randomness and hallucinations. The authors propose a new prompt design, but it is not clearly explained why the proposed prompt can address these two issues, nor is there empirical evidence demonstrating that the prompt resolves them.\n2. Verifying whether the transformations can be correctly applied is critical (one of the limitations of prior work mentioned in the introduction). In Section 4.2, the authors use “correctness + applicability” as a joint metric to validate transformations. However, correctness and applicability should be evaluated separately to better understand whether a transformation can produce valid code. Applicability only reflects generality, while correctness reflects whether the output code is valid. For example, if only 9 out of 1000 transformed examples pass correctness + applicability, but most failures are due to applicability, the transformation may still be useful; if failures are due to incorrect code generation, the transformation is of poor quality. The authors should analyze correctness and applicability separately.\n\nEvaluation:\n\n1. Dataset. The authors should more clearly justify the choice of CodeContests, e.g., whether it is widely used in prior clone detection research. Validation on a single dataset is limited. Common datasets in prior work include OJClone [1], GCJ [2], and BCB [3]. The paper should report results on these datasets or clearly explain why they are not applicable.\n2. Baseline. Only one baseline is used, which is insufficient to convincingly demonstrate the effectiveness of the proposed approach. The appendix lists eight related works; more baselines should be included, or the paper should justify why they cannot be compared.\n3. Validity. Although the method verifies the correctness of generated transformations on 1000 code samples, the correctness of transformed code in the evaluation stage is also critical. The paper should include validation of the correctness of the transformed code used in experiments.\n4. Hyperparameters. The choice of hyperparameters (e.g., temperature) needs clearer justification, including their origin and impact on results.\n\n\nLine 348: “across the three models” — should this be four?\n\nReferences\n[1] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Convolutional neural networks over tree structures for programming language processing,” inProceedings of the AAAI conference on artificial intelligence, vol. 30,\nno. 1, 2016.\n[2] “Google Code Jam,” https://code.google.com/codejam/contests.html, 2016, accessed: 2016-10-8.\n[3] J. Svajlenko, J. F. Islam, I. Keivanloo, C. K. Roy, and M. M. Mia, “Towards a big data curated benchmark of inter-project code clones,” in 2014 IEEE International Conference on Software Maintenance and Evolution. IEEE, 2014, pp. 476–480."}, "questions": {"value": "1. How does the designed prompt address the LLM issues of insufficient randomness and hallucinations in transformation generation?\n2. Does combining correctness and applicability into one metric affect the ability to evaluate transformation quality?\n3. Are additional datasets and baselines not applicable to this work? If so, why?\n4. Were all the generated transformed code samples correct during experiments?\n5. Why were the chosen hyperparameters used, and how do they influence the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X1NbR4j1ev", "forum": "oEJxn8QfC0", "replyto": "oEJxn8QfC0", "signatures": ["ICLR.cc/2026/Conference/Submission13883/Reviewer_zt3M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13883/Reviewer_zt3M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012345662, "cdate": 1762012345662, "tmdate": 1762924393815, "mdate": 1762924393815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies semantic-preserving transformations (SPTs) and proposes Auto-SPT, a framework for automatically constructing synthetic data generators for code. The authors use LLMs to design new SPTs and to generate corresponding implementation scripts that automatically perturb Python functions. They then finetune several embedding models on data generated by Auto-SPT and compare performance against baseline methods. Experimental results show that Auto-SPT can effectively degrade clone detector performance, demonstrating its potential as a robustness evaluation tool."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The correctness of the generated transformations is validated through corresponding unit tests, which enhances the reliability of the proposed SPTs.\n\n- The framework is largely automated and demonstrates good scalability for producing diverse code transformations."}, "weaknesses": {"value": "- The automation in Auto-SPT primarily relies on prompting LLMs to design and implement SPTs. While practical, this approach offers limited methodological novelty given the growing application of LLM-based automation.\n\n- The framework currently supports only Python and focuses on function-level transformations, while file- or project-level perturbations would better reflect real-world code evolution.\n\n\n- The paper does not evaluate whether the transformed programs remain natural and consistent with human coding practices, which is important for assessing the realism of generated data."}, "questions": {"value": "- How are the LLM prompt templates designed? Were there iterative refinements or prompt-tuning steps to improve transformation quality?\n\n- The reward function relies on clone detector model scores. How is the reliability of this clone detector ensured or validated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ITdLQmDnAq", "forum": "oEJxn8QfC0", "replyto": "oEJxn8QfC0", "signatures": ["ICLR.cc/2026/Conference/Submission13883/Reviewer_95aw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13883/Reviewer_95aw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120390881, "cdate": 1762120390881, "tmdate": 1762924393445, "mdate": 1762924393445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}