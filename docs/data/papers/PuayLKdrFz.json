{"id": "PuayLKdrFz", "number": 23634, "cdate": 1758346613770, "mdate": 1759896804197, "content": {"title": "LANE: Label-Aware Noise Elimination for Fine-Grained Text Classification", "abstract": "In this paper, we propose Label-Aware Noise Elimination (LANE), a new approach that improves the robustness of deep learning models when trained under increased label noise in fine-grained text classification. LANE leverages the semantic relations between classes and monitors the training dynamics of the model on each training example to dynamically lower the importance of training examples that are perceived to have noisy labels. We test the effectiveness of LANE in fine-grained text classification and benchmark our approach on a wide variety of datasets with various number of classes and various amounts of label noise. LANE considerably outperforms strong baselines on all datasets, obtaining significant improvements ranging from an average improvement of 2.4% in F1 on manually annotated datasets to a considerable average improvement of 4.5% F1 on datasets with higher levels of label noise. We carry out comprehensive analyses of LANE and identify the key components that lead to its success.", "tldr": "", "keywords": ["label noise", "fine-grained classification", "label relationships"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/184dfc49f9ca96d8ed9f80cccb44a5ee1c6f9ca0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LANE, a training framework designed to enhance model robustness under noisy labels. LANE dynamically adjusts the contribution of training examples by reducing the weight of samples likely to be mislabeled. To identify such samples, it leverages training and semantic relationships. Extensive experiments across multiple fine-grained text classification tasks demonstrate consistent gains, showcasing LANE’s effectiveness in handling label noise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The reviewer notes the following strengths:\n- The paper clearly articulates the limitations of prior methods and justifies the need for LANE.\n- The proposed LANE methodology is intuitive and technically sound.\n- The authors provide extensive experiments on multipole fine-grained text classification datasets that demonstrate consistent and meaningful improvements over multiple baselines.\n- The authors also includes a large set of ablations alongside thoughtful exploration of weight distributions that help provide insight into the underlying methodology."}, "weaknesses": {"value": "From the reviewer's perspective, LANE relies heavily on the foundational capabilities of the underlying model. Therefore additional evaluations on language models beyond BERT would add to the real-world applicability of LANE. But in general, the reviewer finds the overall methodology to be sane and showcase valuable improvements."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LkY8KBFjPQ", "forum": "PuayLKdrFz", "replyto": "PuayLKdrFz", "signatures": ["ICLR.cc/2026/Conference/Submission23634/Reviewer_B3Cs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23634/Reviewer_B3Cs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708859885, "cdate": 1761708859885, "tmdate": 1762942740434, "mdate": 1762942740434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LANE provides a method for learning with label noise. The authors suggest that label noise could arise from error-prone labelling processes or just genuinely ambiguous labels and they must be weighed differently. The key idea is to assign high weights to hard but clean examples that have high semantic similarity of class labels and down-weigh truly mislabelled examples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Addresses an important and novel aspect of handling noisy labels - distinguishing between ambiguous or similar labels and erroneous ones\n* The method identifies and retains hard but clean examples with higher weight, taking into consideration the semantic similarity of label names\n* Extensive empirical analysis has been done comparing LANE with other existing methods"}, "weaknesses": {"value": "* Requires additional network training to learn the weights - a computational overhead\n* The label-aware supervise contrastive loss could be explained a bit more intuitively, right now, it requires readers to have a strong prior understanding of contrastive learning\n* The method is motivated to handle ambiguous or semantically close label noise, but the experiments only consider 20% random noise settings. \n* L95 to L99: piling up such a larger number of references without explaining the individual contributions is not helpful at all"}, "questions": {"value": "* Have the authors considered the case of class imbalance? \n* How much compute power is required to learn the weights?\n* Could the authors provide analysis of the learned weights in relation to semantic similarity?\n* How would LANE perform for increased label ambiguity and not just random noise?\n* Edit suggestions for better readability\n1. Algorithm 1 - lines 170-173 could be written more legibly \n2. Font too small for all tables\n3. Line 245 typo ‘mislabeled’"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BJJVeeqQEc", "forum": "PuayLKdrFz", "replyto": "PuayLKdrFz", "signatures": ["ICLR.cc/2026/Conference/Submission23634/Reviewer_5euU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23634/Reviewer_5euU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825708705, "cdate": 1761825708705, "tmdate": 1762942740257, "mdate": 1762942740257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Label-Aware Noise Elimination (LANE), a novel approach to enhance deep learning model robustness against label noise in fine-grained text classification—a task where classes are semantically similar (e.g., distinguishing between subtypes of news articles or product reviews), making label noise particularly detrimental and common. LANE’s core design integrates two complementary signals to identify and downweight noisy training examples: (1) semantic relations between classes and (2) model training dynamics ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Fine-grained text classification is ubiquitous in real-world applications (e.g., e-commerce product categorization) but uniquely vulnerable to label noise—human annotators often confuse semantically similar classes.\n2. LANE’s fusion of dynamic training dynamics (capturing how the model learns an example over time) and static class semantics (capturing inherent class ambiguities) is innovative.\n3.  The reported F1 improvements (2.4–4.5%) are meaningful for fine-grained tasks"}, "weaknesses": {"value": "1. it is not clear how the supervised contrastive loss helps in Eq (8)? Is there any intuitive illustration?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KYZkgYowl9", "forum": "PuayLKdrFz", "replyto": "PuayLKdrFz", "signatures": ["ICLR.cc/2026/Conference/Submission23634/Reviewer_6TRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23634/Reviewer_6TRi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840993251, "cdate": 1761840993251, "tmdate": 1762942740041, "mdate": 1762942740041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Label-Aware Noise Elimination (LANE), a method that introduces a novel metric called label-aware margin. By incorporating inter-class semantic similarities and model training dynamics, LANE quantifies the label noise level of each training sample. It dynamically down-weights suspected noisy samples, thereby mitigating the harmful effects of noisy labels while retaining all training samples (including hard but clean ones). Experiments across ten text classification datasets demonstrate LANE's effectiveness under varying noise levels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear Motivation: The paper effectively identifies two key limitations in existing methods: (1) valuable samples below a low AUM threshold are unnecessarily removed from the training set, and (2) AUM computation treats labels independently, ignoring semantic similarities between them. The authors propose LANE to address these specific shortcomings.\n2. Clear Methodology: The theoretical analysis and step-by-step description of the proposed method are clear and well-presented.\n3. Extensive Experiments: The comparative experiments are comprehensive, covering numerous datasets spanning most text classification tasks. The proposed LANE method achieves near-universal improvements over baselines."}, "weaknesses": {"value": "1. Several works on noisy label learning emerged in 2025. As a submission to a 2026 conference, the lack of comparison with new methods such as [1], [2], and [3] detracts from the overall novelty of the work.\n2. The approach requires training two BERT models for text classification, which is inefficient. Furthermore, while LLM-related experiments are included, the LLMs rely solely on context. The work could be strengthened by incorporating parameter-efficient fine-tuning techniques like LoRA or integrating noisy label learning methods specifically designed for LLMs, such as [3], to enhance persuasiveness.\n3. The Related Work section is disorganized. Noisy label learning encompasses various approaches, but the authors resort to simple listing. This issue is also reflected in the main experiments (Table 2), which lack intuitive grouping. It is recommended to group and present comparisons by methodology type.\n4. This paper lacks the reproducibility statement required by ICLR.\n\nReferences:\n\n[1] Pan et al., Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data, AAAI, 2025.\n\n[2] Xu et al., Revisiting Interpolation for Noisy Label Correction, AAAI 2025\n\n[3] Ye et al., Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement, SIGKDD 2025."}, "questions": {"value": "1. The experiments use BERT as the base model. Is LANE compatible with other architectures and large language models?\n2. Why wasn't PLF compared in the 20% noise setting experiments?\n3. The complexity of this work is not low. Why not include a main framework diagram? This would make the overall workflow and contributions of the work clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3VE2SUu3C1", "forum": "PuayLKdrFz", "replyto": "PuayLKdrFz", "signatures": ["ICLR.cc/2026/Conference/Submission23634/Reviewer_xwV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23634/Reviewer_xwV9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989652759, "cdate": 1761989652759, "tmdate": 1762942739748, "mdate": 1762942739748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}