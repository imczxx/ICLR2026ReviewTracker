{"id": "PYnLd91wZY", "number": 11901, "cdate": 1758204535994, "mdate": 1759897547677, "content": {"title": "Policy Newton Algorithm in Reproducing Kernel Hilbert Space", "abstract": "Reinforcement learning (RL) policies represented in Reproducing Kernel Hilbert Spaces (RKHS) offer powerful representational capabilities. While second-order optimization methods like Newton's method demonstrate faster convergence than first-order approaches, current RKHS-based policy optimization remains constrained to first-order techniques. This limitation stems primarily from the intractability of explicitly computing and inverting the infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in RKHS, the first second-order optimization framework specifically designed for RL policies represented in RKHS. Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function. Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume. We establish theoretical guarantees proving convergence to a local optimum with a local quadratic convergence rate. Empirical evaluations on a toy financial asset allocation problem validate these theoretical properties, while experiments on standard RL benchmarks demonstrate that Policy Newton in RKHS achieves superior convergence speed and higher episodic rewards compared to established first-order RKHS approaches and parametric second-order methods. Our work bridges a critical gap between non-parametric policy representations and second-order optimization methods in reinforcement learning.", "tldr": "We propose the first second-order optimization method for RL policies in RKHS.  Proves quadratic convergence rate and achieves superior performance in experiments.", "keywords": ["Reinforcement learning", "RKHS", "Newton method"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4940eede0eda194b6a352a42cbe2743b20c5c9a2.pdf", "supplementary_material": "/attachment/0145ddd992fcce9a803c8d30ad3187d3f872d361.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a method for utilizing second-order optimization for policies represented in Reproducing Kernel Hilbert Spaces (RKHS). Direct second-order optimization is not feasible due to the infinite-dimensional Hessian operator in RKHS. Hence, the authors introduce a finite-dimensional optimization problem, whose solution is equivalent to the Newton step. The authors compare their method to the vanilla Policy Gradient and second-order Policy Newton method, as well as the Policy Gradient in RKHS, demonstrating faster convergence (in terms of training iterations)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide extensive theory for their method, proving a quadratic convergence rate. The empirical evaluation results reflect the superior convergence rate."}, "weaknesses": {"value": "1. The authors evaluate their method only on three tasks, which are all very low-dimensional, discrete, and relatively simple. I encourage the authors to add more tasks to the evaluation. Is the method also applicable to continuous control tasks?\n\n2. While the proposed method achieves the highest reward out of the methods compared, it still does not seem to solve the LunarLander task consistently (the Gymnasium documentation specifies a reward threshold of 200 for an episode to be considered solved). Furthermore, all of the progress of the RKHS methods in the LunarLander task seems to happen in the first couple of iterations, which are not shown in the plot. For the rest of the training, the performance stagnates. What might be preventing the method from learning to solve the task?\n\n3. The main drawbacks of second-order methods are the increased computation time and the limited scalability to larger models. The paper is lacking an evaluation of the computation time compared to first-order optimization. Furthermore, a comparison of the computation required for different policy sizes would be helpful."}, "questions": {"value": "1. The introduction is relatively vague about the advantages of RKHS policy representations, simply stating that RKHS \"offer a powerful non-parametric alternative, [...], valued for its representational flexibility, potential for improved sample efficiency, and capacity for dynamic adjustment during learning\". Perhaps the introduction could be more explicit about what makes this representation more suitable, and in which kinds of tasks might benefit the most from these representations.\n\n2. The description of plot 1b is too short. What exactly does the plot visualize? How is the PCA reduction done? There is no interpretation of the results. Also, the difference in reward between the optimal policy and suboptimal points is hard to assess, as large parts of the plot seem to have more or less the same color.\n\n3. Lines 435-436 state that the \"policy optimization in RKHS effectively leverages infinite-dimensional feature representations, enabling the optimization process to escape local optima\". How does the feature representation help with escaping local optima?\n\n4. What are the shaded areas in Figure 2?\n\n5. Line 477 states that \"Policy Newton in RKHS achieved significantly faster convergence to superior episodic rewards compared to first-order and parameteric Newton baselines\", but some of the baselines in Figures 2(a) and (b) did not converge yet, so from the plot, it is not clear whether Policy Newton actually converges to superior episodic rewards.\n\nComments:\n\n1. Specifying the training progress in \"training iterations\" in Figures 1 and 2 makes it hard to compare the convergence speed of the methods to other RL algorithms, consider changing the x-axis labels to environment steps.\n\n2. Line 101 cites Maniyar et al. for the policy gradient method. The method, however, goes back to [1], which is not cited here.\n\n[1] Ronald J. Williams \"Simple statistical gradient-following algorithms for connectionist reinforcement learning.\" Machine learning 8.3 (1992)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8zKcf5ApMv", "forum": "PYnLd91wZY", "replyto": "PYnLd91wZY", "signatures": ["ICLR.cc/2026/Conference/Submission11901/Reviewer_ZjiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11901/Reviewer_ZjiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905119506, "cdate": 1761905119506, "tmdate": 1762922912931, "mdate": 1762922912931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies second-order policy optimization (Newton) when the policy is modelled by a function belonging to a RKHS. It introduces a tractable method for computing (matrix-vector products with) the inverse Hessian operator, which is infinite-dimensional due to the RKHS assumption. It proves (quadratic) convergence rates for the proposed algorithm, and provides a few empirical evaluations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is written clearly, and seems to be correct. \nResults are novel and interesting, the proposed method may have a strong impact."}, "weaknesses": {"value": "I do not see any major weaknesses, however I can highlight a couple of minor issues:\n\n- Section 4.3 seems unnecessary, it’s just a re-statement of known results about convergence rate of Newton’s method on strongly convex losses. Perhaps this space could be used instead to extend Section 3, which represents the main contribution and it’s not very easy to grasp. \n\n- Line 94: “The objective of RL is to minimize…” It should be “maximize”. Similarly, the following equation should be “argmax”, not “argmin”"}, "questions": {"value": "- Why regularisation is cubic instead of quadratic in equation (3)?\n\n- How does RKHS Policy Newton do in wall clock time? (Figure 1 and 2)\n\n- How do RKHS policy methods do when state and/or actions are high-dimensional?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NNmg6RMjkS", "forum": "PYnLd91wZY", "replyto": "PYnLd91wZY", "signatures": ["ICLR.cc/2026/Conference/Submission11901/Reviewer_6Lar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11901/Reviewer_6Lar"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015597940, "cdate": 1762015597940, "tmdate": 1762922912533, "mdate": 1762922912533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a policy iteration algorithm for reinforcement learning problems where the policies are formulated directly as elements of a reproducing kernel Hilbert space (RKHS). The method extends second-order optimisation algorithms to the RKHS setting by deriving computationally tractable approximations to the Hessian and the resulting optimal step direction. Theoretical guarantees are provided regarding the approximation error and convergence to an optimal solution, and experiments complement the theoretical results with demonstrations in practical settings, where the algorithm achieves superior performance in contrast to first-order methods and parametric policy iteration approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Paper is well written and follows a clear structure.\n* Rigorous theoretical analysis with resulting guarantees.\n* Experimental evaluations show significant performance improvements."}, "weaknesses": {"value": "* A minimisation problem over $J(\\pi_\\theta)$ is introduced in Sec. 2.1. Yet, $J$ is formulated as the expected cumulative reward, which an agent should be seeking to maximise, instead of minimise. The result of the regularised Newton step in Eq. 5 also seems to be leading in a descent, instead of ascent, direction.\n* Experimental evaluation is limited to a toy experiment and relatively simple classic RL problems (e.g., CartPole).\n* Notation for temperature and trajectories set use the same symbol $\\mathcal{T}$.\n* Non-standard notation for gradient term in first expectation Eq. 4.\n* The kernel for the numerical experiments is not specified. Was it a standard Gaussian or Matern kernel? The specific details should be stated in the paper, or at least in the appendix."}, "questions": {"value": "* Was the objective $J(\\pi)$ supposed to be written as the negative cumulative reward? How do you ensure the Newton step is leading in a direction that maximises the expected cumulative reward?\n* Is there an alternative reference for the outer product kernel in Definition 3.1? Kubrusly and Vieira (2008) only introduce tensor products between general Hilbert spaces, not particularly RKHSs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AYH7sf7l2j", "forum": "PYnLd91wZY", "replyto": "PYnLd91wZY", "signatures": ["ICLR.cc/2026/Conference/Submission11901/Reviewer_Z1vx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11901/Reviewer_Z1vx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243292383, "cdate": 1762243292383, "tmdate": 1762922912124, "mdate": 1762922912124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}