{"id": "chLlLbI7de", "number": 11190, "cdate": 1758192656130, "mdate": 1759897601929, "content": {"title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents", "abstract": "Driven by recent advancements in tool-augmented Large Language Model (LLM) agents, comprehensive benchmark datasets for evaluating these tool-augmented agents are being actively developed. Although these benchmarks incorporate increasingly complex user requests and a diverse array of tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucinations, and adaptivity. The most straightforward method for evaluating these aspects is to compare the trajectory of the agent with a ground-truth trajectory, but this approach is fundamentally limited since annotating all possible ground-truth trajectories is prohibitively expensive. To address these significant gaps, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating evidence store, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory, eliminating the need for a predefined ground-truth trajectory. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs.", "tldr": "", "keywords": ["Tool-augmented Agent", "LLM Agent", "Evaluation", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/71bd3d9d6a5c84710ca9cf9cd4904b850e6722f1.pdf", "supplementary_material": "/attachment/143732a991d62bf49836a151be26773b014ca919.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents TRACE, a framework for multi-dimensional evaluation of tool-augmented LLM agents. The authors argue that existing benchmarks mostly rely on answer matching, which fails to assess important aspects such as efficiency, hallucination, and adaptivity in long, multi-step tasks. TRACE introduces an evidence bank that accumulates information from previous reasoning steps, enabling detailed trajectory-level analysis without requiring exhaustive ground-truth annotations. To validate the framework, the authors construct a meta-evaluation dataset by augmenting existing benchmarks with diverse, flawed trajectories labeled across multiple performance dimensions. Experiments show that TRACE can accurately and cost-effectively evaluate complex agent behaviors, even using small open-source LLMs, and reveal new insights into how agents perform across long, tool-augmented workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel benchmark that focuses on step-level performance, representing meaningful progress in evaluating LLM agents.\n2. It proposes a new evaluation framework capable of automatically assessing agent behaviors in a more fine-grained and systematic manner.\n3. This method gets rid of the drawback of relying on fixed annotations when offline for online interaction data. I hope the author can open source it, which will contribute to the development of this field."}, "weaknesses": {"value": "1. The work lacks sufficient novelty, the step-level storage and verification mechanism is essentially similar to traditional checkpointing, which is not an innovative idea. Moreover, the term “evidence bank” does not effectively conceal this conceptual limitation. \n2. The three proposed evaluation metrics are not clearly differentiated from those in prior work, which limits the contribution mainly to data collection rather than methodological advancement (such as metrcs although authors said that).\n3. The evaluation process largely relies on prompt engineering; although the experiments show improvements over previous methods, the paper fails to provide human evaluation benchmarks for validation.\n4. The writing quality needs improvement (starting from Section 3), the paper reads more like an experimental report than a coherent, well-structured research narrative."}, "questions": {"value": "1. Section 5.1 devotes long passages to describing the experimental procedure and metrics, but many of these details should be moved to the appendix. How do you handle the same instance when there is no gold trajectory, how can we tell when the model has reached an optimal solution?\n2. What is the difference between the information stored in the evidence bank and simply feeding the whole trajectory directly to LLMs?\n3. How do you validate the quality of the newly annotated data?\n4.\tIn Table 1 your method shows a clear improvement over “LLM-as-a-Judge,” but higher scores are not necessarily better — shouldn’t the scores aim to match human evaluation?\n5.\tMore experiments with additional thinking models are needed, for example doubao-thinking-pro or models specialized for tool use.\n6.\tThe experimental analysis lacks concrete, interesting conclusions; it reads vague. Are there cases where other methods fail but your method succeeds? I think the paper needs a systematic summary of such success/failure cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3StOXzw1K5", "forum": "chLlLbI7de", "replyto": "chLlLbI7de", "signatures": ["ICLR.cc/2026/Conference/Submission11190/Reviewer_4JWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11190/Reviewer_4JWY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970770458, "cdate": 1760970770458, "tmdate": 1762922343395, "mdate": 1762922343395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical limitation in evaluating tool-augmented Large Language Model (LLM) agents: the heavy reliance on final-answer accuracy or comparison against a single ground-truth trajectory. The authors argue that merely matching the final answer fails to assess crucial aspects of the problem-solving process, such as efficiency, hallucination, and adaptivity. To solve this, the authors introduce TRACE (Trajectory-based Reasoning Assessment and Comprehensive Evaluation). TRACE is an effective, simple LLM-based evaluation framework that assesses the logical soundness of an agent’s reasoning trajectory without relying on a single, pre-defined ground-truth path. The core mechanism is the evidence bank, a dynamically constructed knowledge base that accumulates factual information from each reasoning step. To validate TRACE, the authors developed a novel meta-evaluation dataset (Meta-GTA and Meta-m&m's) by augmenting existing benchmarks (GTA and m&m’s) with diverse, flawed trajectories (inefficiency, hallucination, adaptivity) labeled with multi-faceted performance scores. Experiments show that TRACE significantly outperforms a naive LLM-as-a-Judge baseline and existing trajectory evaluation methods like PIPA. The final experiments apply TRACE to real-world agents, revealing significant performance differences that were obscured by standard final-answer accuracy metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's core premise is strong and interesting. The field is moving beyond simple task completion, and evaluating the process of reasoning, not just the result, is a critical and necessary next step for building robust agents.\n- The idea of creating a meta-evaluation dataset by synthetically injecting flaws is a clever and pragmatic approach. It creates a clear ground truth for testing the evaluator (TRACE) without facing the impossible-to-scale task of manually annotating all possible flawed trajectories."}, "weaknesses": {"value": "- The efficiency metric is defined by identifying a \"minimal subset of evidence\" ($\\mathcal{E}_{min}$). This task is delegated to an LLM evaluator, which is a complex reasoning task in itself. The paper does not adequately validate the evaluator's ability to correctly identify this minimal path.\n- The validation shows that TRACE is good at detecting synthetically injected flaws (e.g., a \"find-and-replace\" style hallucination, as per the prompt in Fig. 9). This is a much easier task than finding subtle, naturally-occurring agent hallucinations, which may be more about omission or logical leaps rather than direct contradiction. The paper doesn't prove that success on its synthetic benchmark translates to success in finding these \"in-the-wild\" errors."}, "questions": {"value": "- The adaptivity test is very narrow (api failure). How do the authors think TRACE would perform on more nuanced failures, such as a tool returning an empty list or a rate limit error?\n- The paper claims the evidence bank improves both accuracy and speed, do the authors have any ablation experiments to back up this claim? Did the authors consider any other representations? e.g. not tuple, or to the extreme, just the original thinking trace.\n- Given that TRACE itself depends on LLM evaluators, how sensitive are the results to evaluator choice (e.g., GPT-4 vs. smaller open-source LLMs)?\n- How would TRACE handle multi-agent or parallel reasoning settings, where multiple concurrent tool calls are valid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cqkw8miNmJ", "forum": "chLlLbI7de", "replyto": "chLlLbI7de", "signatures": ["ICLR.cc/2026/Conference/Submission11190/Reviewer_hbxg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11190/Reviewer_hbxg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521533636, "cdate": 1761521533636, "tmdate": 1762922342930, "mdate": 1762922342930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TRACE, an LLM-based framework that builds an evidence bank from each tool call (action, input, observation) and then scores a trajectory along three dimensions: efficiency, hallucination, and adaptivity. The authors also construct meta-evaluation datasets. TRACE outperforms a naïve LLM-as-judge and shows better robustness than PIPA’s state-consistency measure. They further apply TRACE to real agents on GTA, revealing differences among models that overall accuracy obscures (e.g., trade-offs between hallucination and adaptivity), and analyze token/turn-length effects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality. Clear reframing from “final answer only” to process-level, multi-dimensional assessment without relying on a single ground-truth trajectory; the evidence-bank abstraction is a neat way to make evaluation modular and scalable.\n\n2. Clarity: The paper reads cleanly; Fig. 2 and the formalization make the pipeline easy to follow (trajectory to evidence bank to per-dimension scoring). \n\n3. Significance: Applying TRACE to several agents on GTA exposes efficiency–hallucination–adaptivity trade-offs that final accuracy masks; the token/turn analysis offers actionable signals for system builders (e.g., shorter trajectories correlate with better accuracy)."}, "weaknesses": {"value": "1. Evaluator dependence & construct validity\nTRACE still relies on an LLM to: (i) select a “minimal” evidence subset, (ii) judge grounding, and (iii) qualify adaptivity. This introduces evaluator bias and potential non-identifiability (different minimal sets may be equally valid).\n\n2. Minimal-evidence identification Current approach asks an LLM to pick E_min; there’s no guarantee of subset minimality or monotonicity.\nNo algorithmic ablation (e.g., backward elimination / counterfactual removal tests) to verify that removing any item from E_min breaks correctness.\n\n3. Scope limitation to ReAct-style traces. TRACE is designed around action–observation loops; programmatic agents (planner–executor, code-gen with tests) may require different evidence schemas. Lack demonstration of adaptation to at least one non-ReAct agent family and discuss evidence schemas for program synthesis or planner-executor traces. \n\n4. Efficiency measured only on successful trajectories\nThis can yield survivorship bias (inefficient-but-correct vs. efficient-but-failed cases).\nActionable: Also report efficiency diagnostics on failed trajectories (e.g., minimal set relative to the intended answer or checklist), or provide a joint metric correlating efficiency with success probability."}, "questions": {"value": "1. Evaluator reliability: What are the inter-evaluator agreements (κ / %-agreement) across metrics when swapping Claude/GPT/Llama/o3-mini as the judge, and how sensitive are results to prompt templates? Please include per-metric κ and A/B prompts. \n\n2. Hallucination policy: How do you treat thoughts that rely on unstated commonsense/background facts? Could TRACE allow an explicit “import fact” step so those facts are added to the evidence bank before being used, for example, using google search tool for latest factual info? \n\n3. Coverage outside ReAct: Can TRACE readily score trajectories from planner-executor or code-synthesis agents? A small demonstration would broaden impact. \n\n4. Efficiency on failures: Do “almost-there” failures look efficient under TRACE? Reporting efficiency distributions on incorrect runs would help diagnose whether inefficiency is a cause or consequence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8KqZH8DGGq", "forum": "chLlLbI7de", "replyto": "chLlLbI7de", "signatures": ["ICLR.cc/2026/Conference/Submission11190/Reviewer_rb2u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11190/Reviewer_rb2u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972838790, "cdate": 1761972838790, "tmdate": 1762922342543, "mdate": 1762922342543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the TRACE framework, which utilizes an \"evidence bank\" to enable LLM evaluators to assess an agent's reasoning trajectory across three dimensions—efficiency, hallucination, and adaptivity—without relying on a single ground-truth trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly demonstrates through experiments that its proposed trajectory metrics are highly correlated with final accuracy, thereby validating the importance of these metrics.\n\n- The meta-evaluation experiment provides a rigorous method for validating the accuracy of the evaluation framework itself, by injecting controlled, labeled flaws into existing benchmarks."}, "weaknesses": {"value": "- The calculation of the efficiency metric relies on the LLM evaluator to accurately identify $\\epsilon_{min}$. Can smaller models really measure this effectiveness? The meta-evaluation results in Table 1 show that the accuracy of smaller models on efficiency evaluation is much lower than on other metrics, which casts doubt on the reliability of this specific metric when used with non-SOTA evaluators.\n\n- The scope of the adaptivity evaluation is too narrow. The metric currently only measures the response to an \"unavailable tool\" error. However, real-world failure modes are much more complex (e.g., the tool returns a syntactically correct but semantically wrong answer, tool timeouts, or incomplete information). The definition of this metric is likely too narrow.\n\n- Hallucination in this work is defined as: TRACE identifies hallucinations by assessing whether an agent's thought at a given step can be logically derived from the evidence collected so far. However, isn't it possible that in some cases, the model's own parameterized knowledge already contains this information, and it doesn't need to be derived from previous thoughts? Would this still be considered a hallucination? This is actually a very important part of knowledge.\n\n- Although the authors used cosine similarity to follow the established GTA benchmark protocol, I think the protocol itself is flawed. It will be interesting to compare this with other available methods, such as using LLM-as-a-Judge, or an embedding model with world knowledge like QWEN3-Embedding.\n\n- To increase the work's impact and generalizability, I recommend adding experiments that use TRACE to evaluate SOTA tool-augmented agent systems."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "swWfsVLtQR", "forum": "chLlLbI7de", "replyto": "chLlLbI7de", "signatures": ["ICLR.cc/2026/Conference/Submission11190/Reviewer_jJq5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11190/Reviewer_jJq5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981873592, "cdate": 1761981873592, "tmdate": 1762922342145, "mdate": 1762922342145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}