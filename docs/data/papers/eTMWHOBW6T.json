{"id": "eTMWHOBW6T", "number": 11386, "cdate": 1758197925901, "mdate": 1763013623393, "content": {"title": "ExploreAugment: Adaptive Exploratory Data Augmentation based on Boundary Awareness", "abstract": "Traditional data augmentation often applies uniform transformations across all samples, prioritizing data volume over addressing specific model limitations. This indiscriminate approach can lead to redundant data expansion and inefficient training. We propose ExploreAugment, a novel model-aware data augmentation framework that dynamically targets and refines decision-critical regions in the latent space. Our method first identifies key samples using task-specific selection strategies. Then, it leverages diffusion-based latent interpolation to generate samples that are boundary-ambiguous yet semantically valid. These tailored samples are seamlessly integrated into training via a closed-loop pipeline that continuously adapts to the evolving model state. Extensive experiments across multiple datasets demonstrate that ExploreAugment consistently enhances task performance while significantly reducing augmentation overhead. Notably, our approach outperforms the best baseline by 7.14\\% on ResNet-50 and 1.75\\% on DeiT, achieving these gains using only about 15\\% of the data volume generated by other augmentation methods. This highlights the significant advantage of our boundary-aware, model-driven augmentation for achieving data-efficient learning.", "tldr": "ExploreAugment is a model-aware data augmentation framework that generates boundary-focused samples via latent interpolation, achieving higher accuracy with only about 15% of the augmented data used by traditional methods.", "keywords": ["Data Augmentation", "Boundary-Aware Learning", "Diffusion Models", "Data Efficiency", "Model-Aware Augmentation", "Classification"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/44737a43d938147e967d6e47a4996467fc4a44db.pdf", "supplementary_material": "/attachment/756a47a1a25a95d09a985b9ae9e36cba40c30298.zip"}, "replies": [{"content": {"summary": {"value": "This paper is focused on improving upon existing data augmentation techniques by augmenting the most informative data points using model-aware selection. The method leverages three stages: first it uses a boundary aware selection strategies to identify critical samples, then using boundary sample generation to generate both boundary-ambiguous and semantically coherent images, and finally using a fusion strategy to incorporate the augmented data points into the training loop."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper is the limited number of augmented samples that are required to achieve similar levels of performance. This will significantly reduce the memory requirements of storing the augmented samples and the training time for image classification. Additionally, the boundary aware strategy is novel, and the paper is well-motivated and each part is well written."}, "weaknesses": {"value": "There a few weaknesses for this paper. \n1) First, there is no comparison to existing model-based / model-aware methods such as SoftAugment [1] or SAFLEX [2]. Such methods leverage the model's predictions to generate new samples, and would be good comparisons to the current approach to strengthen the paper. These should also be added to the related works section.\n2) The boundary aware selection strategy seems rather heuristic. The authors should explain why this composite score makes sense and add ablation studies for each individual metric (entropy and consistency).\n\n[1] Liu, Y., Yan, S., Leal-Taixé, L., Hays, J., & Ramanan, D. (2023). Soft augmentation for image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16241-16250).\n[2] Ding, M., An, B., Xu, Y., Satheesh, A., & Huang, F. (2024). SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation. arXiv preprint arXiv:2410.02512."}, "questions": {"value": "1) The authors showed that with a small number of additional augmentations the performance matches existing augmentation methods. However, does this performance continue to increase as we add more augmentations (up to 60% augmentations instead of 10-20%)?\n2) The method requires training a model to match the classifier latent space with the pretrained diffusion model latent space. What is the training time for this, and does this have to be retrained for different classifier / diffusion model pairs?\n3) Building on Question 2, can the authors provide a efficiency comparison to other methods (wall clock time or FLOPs)? If the time it takes to generate 20% augmentations is more than the time it takes to generate 60% augmentations for other methods, this decreases the value of the method.\n4) How necessary are the filtering mechanisms for the fusion training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C3TFTe7fvv", "forum": "eTMWHOBW6T", "replyto": "eTMWHOBW6T", "signatures": ["ICLR.cc/2026/Conference/Submission11386/Reviewer_bhmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11386/Reviewer_bhmS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412308415, "cdate": 1761412308415, "tmdate": 1762922509496, "mdate": 1762922509496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "jVNz0T5FQQ", "forum": "eTMWHOBW6T", "replyto": "eTMWHOBW6T", "signatures": ["ICLR.cc/2026/Conference/Submission11386/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11386/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763013622377, "cdate": 1763013622377, "tmdate": 1763013622377, "mdate": 1763013622377, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the ExploreAugment framework to address the inefficiency of traditional data augmentation, which is often decoupled from model training, by focusing on improving the coverage of decision-critical regions. It employs a model-aware, adaptive closed-loop system: first, it identifies uncertain \"key samples\" near the decision boundary using model feedback; second, it generates \"boundary-ambiguous\" new samples using diffusion models and LoRA interpolation (DiffMorpher); finally, it filters these samples via a two-stage filter and injects them using a progressive weighting scheme, enabling dynamic coordination between the classifier and augmentation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation.\n2. The writing is overall good.\n3. Good ayalytical experiments."}, "weaknesses": {"value": "1. While the paper prominently highlights \"sample efficiency,\" it largely overlooks the framework's substantial computational overhead. This overhead stems from its iterative, multi-stage process involving inference, LoRA fine-tuning, diffusion sampling, and gradient filtering. \n2. The proposed method has not been evaluated on large-scale datasets, e..g., ImageNet-1k. The empirical validation is confined exclusively to fine-grained classification datasets, a scope that does not fully align with the paper's broader claims of improving generalization.\n3. Without reporting the computational costs compared to vanilla training and other augmentation methods, the practical significance weakens.\n4. The compared baselines are out of date. Many widely used and more recent augmentation methods (e.g., SelectAugment, EntAugment, FreeAugment, TrivialAugment...) are not compared. Thus, the claims made in this paper are not well validated.\nBased on the weakness above and the questions, I vote for rejection at the current stage. I will consider updating my score based on the author's response."}, "questions": {"value": "Could you please quantify the computational overhead of the ExploreAugment framework? While the paper emphasizes \"sample efficiency,\" it omits a discussion of the \"wall-clock time.\" Given the iterative, multi-stage process involving inference, LoRA fine-tuning, diffusion sampling, and gradient filtering, what is the approximate factor increase in total training time on the AFHQ dataset compared to a standard baseline like Cutout?\nThe 'boundary ambiguity' targeted in fine-grained tasks (e.g., distinguishing between species) differs significantly from the 'high inter-class variance' present in general-domain benchmarks like ImageNet. Could you provide further evidence or justification that your method remains effective for non-fine-grained tasks, especially given the absence of validation on such general-domain benchmarks?\nIn the Stage 3 auxiliary selection mechanism, the confidence interval [p_min, p_max] is used to filter generated samples. This appears to be a critical step, yet the paper lacks discussion on how these hyperparameters were set and provides no sensitivity analysis. Could you please specify the values used for p_min and p_max in your experiments, and elaborate on how sensitive the final model performance is to this choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Pho1rTFBi", "forum": "eTMWHOBW6T", "replyto": "eTMWHOBW6T", "signatures": ["ICLR.cc/2026/Conference/Submission11386/Reviewer_4foe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11386/Reviewer_4foe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560945933, "cdate": 1761560945933, "tmdate": 1762922509161, "mdate": 1762922509161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional DA often applies uniform transformations across all samples, leading to redundant data expansion and inefficient training. This paper proposes ExploreAugment that dynamically targets and refines decision-critical regions in the latent space. \nWhile there are some strengths, some critical weaknesses exist. Thus, at this stage, I vote for 2-reject, but I will consider updating my score based on the authors' responses."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is dynamic, adaptive, and model-aware, beyond static methods.\n- The proposed method incorporates a generative model to enhance sample diversity.\n- The paper contains many analytical experiments, which is good."}, "weaknesses": {"value": "- Some recent methods also propose dynamic and adaptive DA mechanisms (e.g., EntAugment, FreeAugment, TeachAugment, AdaAugment, MADAug, etc.). However, none of these related works have been introduced in the related literature or experimental section. - The authors are strongly advised to discuss the difference between the proposed method and these related works.\n- Regarding the main experiment, the compared methods are too limited. In fact, methods such as Cutout and RandomErase are weak baselines in data augmentation. More recent and stronger baselines are strongly suggested to compare with to validate the empirical performance of the proposed method.\n- In line 13 of Algorithm 1, it seems that the augmented samples are incorporated into the original datasets. Thus, during comparison, the augmented data volume of different methods should be the same, i.e., Aug. Ratio in Table 1. The current comparison seems unfair.\n- In Eq. (9), the author uses classifier confidence to select samples, while this may be unreliable.\n- One of the most important evaluation metrics of DA is efficiency. One concern is that using diffusion-based generation during online training may introduce noticeable training costs, which will significantly reduce the effectiveness of DA methods. Without reporting the additional training costs of the proposed method, the effectiveness of the proposed method degrades. \n- In the Abstract, the authors claim that the proposed method is model-aware, while in the Contributions,  they say that the method is model-independent. It requires further clarity."}, "questions": {"value": "- Did the author try to train models only using the augmented data? This is widely used in most DA baselines.\n- Did the author evaluate the performance of different generation methods on model training, not just visualization results, in Section 5.3 in Figure 5?\n- The authors claim that the proposed method is boundary-aware. Does the augmented data utilize the same boundary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RuPM9EMwsP", "forum": "eTMWHOBW6T", "replyto": "eTMWHOBW6T", "signatures": ["ICLR.cc/2026/Conference/Submission11386/Reviewer_2b1M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11386/Reviewer_2b1M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890637210, "cdate": 1761890637210, "tmdate": 1762922508765, "mdate": 1762922508765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ExploreAugment, a model-aware, boundary-targeted augmentation framework. In the first stage, the method selects key training samples that are likely near the classifier’s decision boundary using either BECS or Dataset Cartography. In the second stage, it generates boundary-ambiguous but semantically valid images via diffusion-based latent interpolation (LoRA-adapted Stable Diffusion + SLERP over endpoints). In the third stage, it injects the curated synthetic samples back into training with a ramp-up weighting and a two-stage filter. Across AFHQ, Flowers, Birds-525, and Oxford Pets with ResNet-50 and DeiT backbones, ExploreAugment reports higher top-1 accuracy than baselines such as cutout, random-erase, Text2Img, YONA and DiffMix, while using only ~10%–20% extra samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a closed-loop framework for data augmentation that incorporates key sample identification via BECS or Cartography and sample generation via diffusion models. Generated images, under proper guidance, can potentially serve as hard samples for classification tasks.\n\n2. The paper showed improved classification accuracies using less augmented data (10%-20% augmentation ratio), on various datasets and two representative model architectures."}, "weaknesses": {"value": "1. The motivation of using diffusion models for data augmentation is unclear. While diffusion models have achieved significant success for image generation, the latent spaces for generative and discriminative models can be quite different. And generating similar-looking samples does not fully exploit the generation capacity of diffusion models.\n\n2. The proposed method incurs significant computation overhead, as it needs to LoRA-update the diffusion model twice for each selected pair of samples from different classes. The method is hard to scale to large datasets, such as ImageNet or larger sets. While the method requires less augmented samples to reach competitive performance, the actual compute consumption can still be significant compared to other methods, which the paper does not discuss in details."}, "questions": {"value": "1. What is the compute footprint relative to other methods such as cutout and Text2Img under equal accuracy?\n\n2. The generated samples tend to have a drifted distribution with low contrast, as shown in Figure 1 and Figure 5(a). This can be due to capability limit of the diffusion model, insufficient sampling steps, information loss in latent space mapping, or other reasons. Is there any study for this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zg4AMk5Bxc", "forum": "eTMWHOBW6T", "replyto": "eTMWHOBW6T", "signatures": ["ICLR.cc/2026/Conference/Submission11386/Reviewer_f6ik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11386/Reviewer_f6ik"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895197152, "cdate": 1761895197152, "tmdate": 1762922508449, "mdate": 1762922508449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}