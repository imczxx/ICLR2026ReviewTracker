{"id": "8AtPIGrkVL", "number": 17867, "cdate": 1758281446605, "mdate": 1759897149086, "content": {"title": "Pointwise Generalization in Deep Neural Networks", "abstract": "We address the long-standing question of why deep neural networks generalize by establishing a complete pointwise generalization theory for fully connected networks. For each trained model, we equip the hypothesis with a pointwise Riemannian dimension through the effective ranks of the {\\it learned} feature matrices across layers, and derive hypothesis- and data-dependent generalization bounds. These spectrum-aware bounds break long-standing barriers and are orders of magnitude tighter in theory and experiment, rigorously surpassing bounds based on model size, products of norms, and infinite-width linearizations. Analytically, we identify  structural properties and mathematical principles that explain the tractability of deep nets. Empirically, the pointwise Riemannian dimension exhibits substantial dimensionality reduction, decreases with increased over-parameterization, and captures feature learning and the implicit bias of optimizers across standard datasets and modern architectures. Taken together, these results show that deep networks are mathematically tractable in the practical regime and that their generalization is sharply  explained by pointwise, spectrum-aware complexity.", "tldr": "Complete generalization theory for fully connected deep nets: bounds depend on the effective rank of learned features at trained model, and are empirically orders of magnitude tighter.", "keywords": ["Deep Neural Networks", "Nature of Generalization", "Pointwise Riemannian Dimension", "Feature Learning", "Finite-Scale Geometry", "Avoid NTK and Exponential Norm Barriers"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed3ed4f843b1499793892b2816402ecf4e5ee3e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper gives new generalization bounds to explain the good generalization properties of deep neural networks, a long-standing topic in learning theory. The proposed bounds are called ``pointwise'', meaning that they apply for every hypothesis individually and are also data-dependent. The main technical elements are based on an application of PAC-Bayesian bounds with well-chosen probability distributions. This allows the authors to introduce an effective dimension, which is related to a fractal dimension notion of a data-dependent prior. In the case of fully-connected deep neural networks to a notion of Riemannian dimension, based, in particular, on the feature Gram matrices. Empirical studies support the theoretical findings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Understanding the generalization error of modern deep neural networks is an important topic.\n- The introduced notions of effective dimensions might have interest on their own."}, "weaknesses": {"value": "In my opinion, there are three main weaknesses.\n\n *1. Poor literature review*: The paper pretends to address the long-standing question of generalization and explicitly states \"theory has not kept pace\". Despite this, there is an important lack of literature review on the rich literature review. Most of the classical references are missed and the introduction barely contains citations. For instance, algorithmic stability or information-theoretic bounds are never mentioned. PAC-Bayes bound are mentioned but with almost no reference except (Dziugaite and Roy, 2017). NTK theory is mentioned but with no reference. The same remark can be made or uniform covers, VC dimension, and product of norms. Moreover, similar notions of intrinsic dimensions already appear in machine learning, but with no reference, see [1] for instance.\n In conclusion, the authors claim that theory has not kept pace on generalization without acknowledging the rich literature on this question.\n\n*2. The proofs seem to contain critical mistakes*, such as:\n - Proof of Lemma 4: The proof is based on classical PAC-Bayes bounds but they cannot be applied here. Indeed, Lemma 9 is only true for a prior distribution $\\pi$ that is independent of the data. It can be seen in the proof of thm 2.1 of [Alquier, 2024] (cited for lemma 9): the critical step is to apply Fubini's theorem to switch the prior and the data distribution, it is only possible if \\pi does not depend on the data. \n - Proof of thm 1: First, at line 1030, the notation does not make sense because there is an expectation on z outside the sup, while z is already the integrand of $\\mathbb{P} - \\mathbb{P}_n$ according to your notation. Even if this is fixed, I am not convinced by the symmetrisation argument at line 1040 because the inside of the supremum is not symmetric in z and z'.\n\n*3. Empirical section:* at line 81 and in the contributions section, the empirical validation section is clearly treated as though it was part of the main paper and not of the appendix. In my opinion, an effort should be made to compress the main paper so that a proper empirical section can be included in the main text.\n\nRegarding, the proofs, please correct me if I am wrong. \n\nFinally, here are some more minor issues:\n - Line 69: a effective -> an effective\n - The notation at equation 3 is confusing to me because $z$ because $z$ is the integrand on the left-hand side, so it should not appear in my opinion. Maybe something like $\\mathbb{P}(\\ell(f, \\cdot))$ would be better.\n - Line 133 - 134, is a word missing in the sentence?\n - Line 901 - 902: too much space between so and on\n - Equation 15, I think it should be said that the uniform distribution is meant with respect to $\\pi$\n - I don't think the notion of prior distribution has been properly defined.\n - Proof of Theorem 1: sometimes the $\\otimes$ is missing in the products between distributions, is it a typo?\n\n\n[1] \"Fractal Structure and Generalization Properties of Stochastic Optimization Algorithms\" Cameo et al., 2021."}, "questions": {"value": "- Line 167, it is claimed that the bounds covers a rich class of models. To my understanding, if the point wise dimension grows as $d(f) \\varepsilon^{-2}$, it means that the fractal dimension is at most 2, can you comment further on how this is restrictive?\n- To my knowledge, eq. (14) is a step to prove eq. (13), why not go directly go to eq. (14)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ia1fCGRoHz", "forum": "8AtPIGrkVL", "replyto": "8AtPIGrkVL", "signatures": ["ICLR.cc/2026/Conference/Submission17867/Reviewer_9dbr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17867/Reviewer_9dbr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556833245, "cdate": 1761556833245, "tmdate": 1762927693969, "mdate": 1762927693969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to sharp understand the generalization of neural network. This is done by developing pointwise, spectrum-aware PAC-Bayesian generalization bounds. Their prior is then constructed considering the low-rank features in FFNs. The new bounds reveal that generalization can be captured by finite-scale Riemannian dimension, which also empirically better capture generalization under overparameterization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides deeper understanding by connecting generalization with finite-scale Riemannian dimension, which is then connected to effective dimensions of FFNs. The resulting bounds provide better indicator of generalization compared to existing results and reflects many important generalization behaviours under overparameterization.\n- This paper contributes many advancement in PAC-Bayesian theories, eg, single-hypothesis bounds. The bounds are then refined with consideration on exploiting implicit biases like spectral properties of hidden features. \n- This paper involves multiple techniques of novelty, including loss symmetrization, non-perturbative expansion, etc. It also gives new results in pure mathematics."}, "weaknesses": {"value": "- Weight and data dependence of $\\pi$: The **Key Challenge** paragraph has emphasized that $\\pi$ cannot rely on $W$. However, the hierarchical construction of $\\pi$ involves subspaces subspaces, whose dimension is the effective rank of $G(W)$. It relies not only on weight $W$ but also on data $X$ or $z$. \n- Evidence on tightness is not direct: The bound seems easy to compute and its most complicated part, ie, the Riemannian dimension, has been computed. So why not compute the entire bound.\n- Presentation and typos: \n  - Theorem 1 emphasizes prior's data dependence. However, in latter construction of $\\pi$, lots of efforts have been put to make $\\pi$ independent of $W$ and thus data $z$ (please correct me if my understanding is wrong), making this data dependence useless. Also I found the proof incomplete  for the data dependence of Theorem 1 (See my Question 1 below). I believe it is better to remove the claimed data dependence.\n  - Eq.(7) has incomplete parentheses.\n  - Unspecified matrix norm in Eq.(9)."}, "questions": {"value": "- Questions on proof details: In the proof of Theorem 1, Line 1040 bounds original losses using symmetrized losses. This step relies on the fact that $\\\\ell(f ; z′) − \\\\ell(f ; z)$ has a symmetric distribution. But as far as I am concerned (please correct me if I was wrong), this step not only involves this symmetrically distributed term, but also the $\\\\log \\\\frac{1}{\\\\pi(\\\\dots)}$ term where $\\\\pi$ depends on $z$ only and does not have a symmetric distribution. Could the authors provide more details on how this term is handled in this complicated mixture with $\\\\sup_f$? \n    - I did some calculation and the symmetric distribution seems not sufficient: Let's assume a simplification with $n=1$ so all $\\\\mathbb{P}\\_n$ notations disappear. Then this step can be abstractly seen as whether\n        \n        $$\n            \\\\begin{aligned}\n                \\\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; z) \n                &\\\\overset{?}{=} \\\\mathbb{E}_{\\xi} \\\\mathbb{E}_{z, z'}  \\\\sup_f \\xi (\\\\ell(f; z') - \\\\ell(f; z)) - b(f; z)\\\\\\\\\n                &=\\\\frac{1}{2} \\\\left(\\\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) + b(f; z) \\\\right) + \\frac{1}{2} \\\\left(\\\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z) - \\\\ell(f; z') - b(f; z) \\\\right)\\\\\\\\\n                &=\\\\frac{1}{2} \\\\left(\\\\mathbb{E}_{z, z'}  \\\\sup_f \\ell(f; z') - \\\\ell(f; z) - b(f; z) \\\\right) + \\\\frac{1}{2} \\\\left(\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; z') \\\\right),\n            \\\\end{aligned}\n        $$\n\n        where $b(\\\\cdot, \\\\cdot) \\\\ge 0$ abstracts the pointwise dimension term.\n        It is equivalent to whether\n\n        $$\n            \\\\begin{aligned}\n                \\\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; \\\\red{z}) \n                &\\\\overset{?}{=} \\\\mathbb{E}_{z, z'}  \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; \\\\red{z'})\n            \\\\end{aligned}\n        $$\n\n        There are counter-examples for this statement:\n\n        $$\n            z, z' \\\\in \\\\{-1, +1\\\\}, f \\\\in \\\\{-1, +1\\},\\\\\\\\\n            \\\\ell(f; z) = f z + 42, b(f; z) = \\\\begin{cases}\n                0 & z = f\\\\\\\\\n                42 & z \\\\neq f\n            \\\\end{cases} \n        $$\n\n        Then we have\n\n        $$\n            \\\\begin{aligned}\n                \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; z)\n                =& \\\\sup_f f \\\\times (z' - z) - 42 \\\\cdot \\\\mathbb{I}[z \\neq f]\\\\\\\\\n                =& z(z' - z) \\\\quad\\\\quad (\\\\text{$f$ must equal $z$}),\n            \\\\end{aligned}\n        $$\n\n        whose expectation is $\\\\mathbb{E}[-z^2] = -1$.\n        On the other hand, we have\n\n        $$\n            \\\\begin{aligned}\n                \\\\sup_f \\\\ell(f; z') - \\\\ell(f; z) - b(f; z)\n                =& \\\\sup_f f \\\\times (z' - z) - 42 \\\\cdot \\\\mathbb{I}[z' \\neq f]\\\\\\\\\n                =& z'(z' - z) \\\\quad\\\\quad (\\\\text{$f$ must equal $z'$}),\n            \\\\end{aligned}\n        $$\n\n        whose expectation is $\\\\mathbb{E}[z'^2] = 1$.\n        The two sides do not equal and the statement is generally untrue.\n        Therefore, the symmetrical distribution of the loss difference term alone seems not enough and I kindly ask whether the step relies on more conditions or the symmetrical distribution is applied in some special way?\n    - The conclusion of Theorem 1 also seems a bit unexpected because the data-dependent prior seems comes at little cost. For example, in Theorem 2.4 from Alquier (2024), the data-dependent choice of $\\\\lambda$ costs a penalty of $\\\\log \\\\text{card } \\\\Lambda$. In contrast, Theorem 1 does not contain any similar penalty. It seems all extra terms compared to Lemma 9 comes from either making the hypothesis deterministic ($\\\\epsilon$) or from standard procedures (I skimmed the proof and my finding may be imprecise, so please correct me; but at least they do not depend on $\\\\pi$). As a result, it seems one can arrive at extremely small generalization bounds at least for any deterministic learner: Assume $f = A(z)$ is the output of the deterministic learner. Then we can fix a small $\\\\epsilon \\\\sim \\\\sqrt{1/n}$ and then construct $\\\\pi$ to exactly cover the ball around $f=A(z)$. Since $f$ only depends on $z$, the loss-induced metric only depends on $z$ as well, and the radius $\\epsilon$ of the ball is fixed, such $\\\\pi$ is a well-defined data dependent prior. However, this $\\\\pi$ makes the pointwise dimension term $0$ *without* any penalty. In this case, one would have a $\\\\tilde{O}(1/\\\\sqrt{n})$ rate, regardless of architecture, optimizer or data properties."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v8bbjilsug", "forum": "8AtPIGrkVL", "replyto": "8AtPIGrkVL", "signatures": ["ICLR.cc/2026/Conference/Submission17867/Reviewer_1RYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17867/Reviewer_1RYM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753579542, "cdate": 1761753579542, "tmdate": 1762952150048, "mdate": 1762952150048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the generalization of fully connected networks at the trained parameters. The proposed generalization bound depends on the Riemannian dimension, which is based on the spectral properties of the feature representations. The proposed complexity measure is tighter compared to related approaches, and the Riemannian dimension exhibits appealing properties that can potentially enhance and explain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses an important and timely question about understanding the behavior of neural networks.\n- The analysis and technical content appear rigorous and sound. However, I have not checked the theoretical derivations in detail.\n- The pointwise dimension and the non-perturbative expansion are interesting contributions.\n- The experiments support the theory, showing that the proposed complexity measure is smaller than related metrics."}, "weaknesses": {"value": "- I believe that the clarity and accessibility could be improved. While this is a technical theoretical work and simplifying further is challenging, the text is somewhat hard to follow. Providing clearer intuition and explanations for the theoretical results would help.\n- The paper is dense, as the generalization bound depends on both probabilistic and differential geometric concepts. The exposition of the latter (Sec. 3.2) could benefit from illustrations.\n- Some terms may be well-known in the literature, but I believe, should still be defined in the paper to make it self-contained. See questions below for examples.\n- The relevance of certain sections is not directly clear. See questions below.\n- From the experiments, while the Riemannian dimension appears smaller than related metrics, it does not seem to provide a non-vacuous generalization bound. I may be missing something."}, "questions": {"value": "1. In Eq. 4, the denominator represents the integral of the prior over a ball centered at $f$?\n2. How can a data-dependent prior be considered, and how is it related to Theorem 4?\n3. The discussion after Theorem 2 is somewhat unclear, and similarly, the relevance of Sec. 2.2. As regards, Sec. 4.2 is not immediately obvious if the Riemannian dimension is implicitly regularized during training or if the idea is to consider it as an explicit regularizer.\n4. In the experiments, even if the bounds are smaller than the compared approaches, they still appear rather vacuous. Am I missing something?\n5. The considered architecture does not include bias. Would the analysis change significantly if biases were included?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1sDzGUP968", "forum": "8AtPIGrkVL", "replyto": "8AtPIGrkVL", "signatures": ["ICLR.cc/2026/Conference/Submission17867/Reviewer_63MQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17867/Reviewer_63MQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17867/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903917685, "cdate": 1761903917685, "tmdate": 1762927692717, "mdate": 1762927692717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}