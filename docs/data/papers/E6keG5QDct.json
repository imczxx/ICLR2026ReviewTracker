{"id": "E6keG5QDct", "number": 15536, "cdate": 1758252435679, "mdate": 1759897300970, "content": {"title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Reasoning", "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of *overthinking*, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but it tends to lose reflection ability and harm performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision.  Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 36\\% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for easier ones without losing reflection ability. Code is available at https://anonymous.4open.science/r/REA-RL.", "tldr": "To enhance the efficiency of LRMs, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, and a reflection reward to further prevent LRMs from favoring short yet non-reflective responses.", "keywords": ["online reinforcement learning", "large reasoning model", "efficient reasoning", "overthinking", "reflection"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/861eeaab488631ca6712c55e9f9274cef5613aae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes REA-RL, a reflection-aware online reinforcement learning framework aimed at improving the efficiency of reasoning in Large Reasoning Models (LRMs) without sacrificing accuracy. The work addresses the challenge of overthinking in chain-of-thought (CoT) reasoning, where models often generate excessively long, redundant reasoning steps that increase inference cost with limited benefit.\n\nREA-RL introduces two key components:\n\nReflection Model: A lightweight model trained to identify the earliest point in a reasoning trace where the correct answer appears. Based on this, the model trims subsequent “overthinking” tokens and generates a concise revision. This enables efficient sequential revision that complements parallel sampling in online RL.\n\nReflection Reward: A new reward function based on the density of reflective tokens (e.g., \"wait\", \"but\") that penalizes non-reflective outputs, which often result from using only length-based rewards.\n\nThe proposed framework is implemented within a Grouped Relative Policy Optimization (GRPO) pipeline and combines both the original and revised responses for policy updates. Experiments on several math reasoning benchmarks (e.g., GSM8K, Math500, AMC23) show that REA-RL can reduce token usage by 36% while maintaining or even improving accuracy. The analysis further demonstrates that the system preserves reflection on hard problems and reduces unnecessary reflection on easier ones, achieving a better balance between efficiency and performance than prior approaches."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a reflection-aware revision framework (REA-RL) combining a lightweight reflection model and reflection reward, achieving a 36% inference efficiency gain with no performance loss.  \n- The detection method is principled, with a clear revision boundary strategy and efficient implementation.  \n- The reward shaping component addresses the typical degeneration from using length-only rewards in online RL.  \n- Experiments are thorough and well-structured, with both ablations and scaling comparisons."}, "weaknesses": {"value": "- Limited to 7B distilled models need 32Bmodel for guidance; lacks generalization validation to pre-trained or larger LLMs.  \n- The reflection detection relies on LLM-based heuristics, which may not scale or generalize across domains.  \n- Added complexity in training pipeline (sequential revision) increases compute by ~10%."}, "questions": {"value": "Truncation is somehow brutal? what if answeer first, and explain later.\nwhen calculate  overthink signals ,wait, but , alternativly, is somehow way too heuristic?\nand chose 20th  for punishment, do we have experiment to support this parameter choose?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "35OSQuydbr", "forum": "E6keG5QDct", "replyto": "E6keG5QDct", "signatures": ["ICLR.cc/2026/Conference/Submission15536/Reviewer_1pmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15536/Reviewer_1pmc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760517669515, "cdate": 1760517669515, "tmdate": 1762925815218, "mdate": 1762925815218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REA-RL, an online reinforcement learning framework designed to mitigate overthinking in reasoning language models. REA-RL employs a reflection model to detect and truncate redundant reasoning segments, producing revised trajectories for policy learning. It further incorporates a reflection reward to encourage appropriate self-correction and a refined length reward that promotes concise reasoning only when answers are correct. With this method, REA-RL achieves up to 36% shorter reasoning traces without accuracy degradation across multiple reasoning benchmarks, demonstrating more efficient and balanced reasoning behavior."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an original and well-motivated idea of integrating reflection awareness into reinforcement learning to mitigate overthinking in reasoning models.\n2. The approach is clearly explained, with intuitive motivation, and supporting experiments and ablations.\n3. Experimental results are consistent and persuasive, demonstrating significant reductions in reasoning length while maintaining accuracy."}, "weaknesses": {"value": "1. The revision model design seems questionable. The paper reports that using the revision model alone achieves even better results than revision model + gold answer, which is counter-intuitive. Since matching the correct answer is not a difficult task, this suggests that the revision mechanism may not be well aligned with correctness or may overfit to surface reflection patterns. Clarifying why this happens would strengthen the paper.\n2. The reflection reward based on reflection-token density might lead to reward hacking. Models could insert reflective keywords (“wait”, “check”, “however”) without performing genuine self-correction. A more semantic or context-sensitive reflection metric could better ensure that the reward encourages meaningful reflection rather than stylistic mimicry."}, "questions": {"value": "I’m curious about the training setup: the paper mentions that both the original and revised responses are used together during online updates. Have the authors tried using only the revised responses for training? It would be interesting to see whether excluding the original (possibly overthinking) trajectories leads to more stable learning or better efficiency–accuracy trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7TfBtxhjU9", "forum": "E6keG5QDct", "replyto": "E6keG5QDct", "signatures": ["ICLR.cc/2026/Conference/Submission15536/Reviewer_homH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15536/Reviewer_homH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884237269, "cdate": 1761884237269, "tmdate": 1762925814445, "mdate": 1762925814445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces REA-RL, a reflection-aware online reinforcement learning framework that improves the efficiency–accuracy trade-off of large reasoning models by curbing overthinking without discarding beneficial reflection. The core idea is to augment standard grouped online RL with two complementary mechanisms. First, a lightweight reflection model is trained to detect the first point in a sampled trajectory where the answer is already present; tokens after that point are treated as overthinking and the path is revised by truncating the think segment and prompting the policy to finalize the answer. This yields a sequential-revision signal that, when combined with parallel sampling, creates an additional scaling dimension and can be interpreted as a partial advantage that penalizes overthinking tokens while rewarding the revised completions. Second, a reflection-aware reward measures the density of reflective cues (e.g., wait, but) and penalizes paths whose reflection density falls in the lowest quantiles, thereby preventing the length reward from collapsing the model into non-reflective, error-prone plans. The authors also refine the length reward by zeroing it on incorrect generations. Across five math benchmarks and two generation budgets, the approach maintains or improves accuracy while reducing token cost by up to 36% on average relative to the original R1-7B initialization, and analysis shows it preserves reflection on harder problems while appropriately reducing it on easier ones. A workflow schematic on page 5 clarifies how parallel sampling, reflection-guided truncation, and reward shaping interact during training, and tables on pages 6–8 report the main and component-wise results supporting these claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work is original in how it operationalizes reflection within online RL: rather than only rewarding short outputs, it explicitly detects and trims overthinking in-situ and then optimizes on both original and revised trajectories. This integration of parallel sampling with sequential revision is conceptually clean, computationally practical, and linked to an interpretable partial-advantage view that clarifies why overthinking tokens receive targeted penalties while preserving valid reasoning. \n\nMethod quality is supported by a carefully designed reflection model distilled to 7B for speed, a simple yet effective reflection-density reward, and a refined length reward that avoids incentivizing short but wrong responses. The empirical study is broad for the domain, covering GSM8K, MATH500, Gaokao23, AMC’23, and AIME’24 with two budgets, with ablations isolating the effects of reflection modeling versus reward shaping and training-dynamics plots that explain how the method shortens traces without eroding accuracy. \n\nClarity is aided by step-by-step descriptions, concrete prompts, and a readable workflow diagram on page 5; the case studies on page 16 make the truncation–revision mechanism tangible."}, "weaknesses": {"value": "The reliance on answer-presence detection as the stopping criterion risks truncating useful verification steps when the answer is mentioned early in a speculative way, and while the authors mitigate this with a trained reflection model, there is limited quantitative reporting on its detection precision/recall beyond downstream accuracy and token ratios. \n\nThe study focuses on math word problems with a single distilled 7B base; it remains unclear whether the approach scales to other reasoning domains, like multimodal or non-math reasoning, or to larger pretrained LRMs without distillation artifacts, especially with a reflection model trained for the specific in-distribution task.\n\nThe online pipeline introduces extra complexity and additional training time relative to pure parallel sampling, and although the gains seem to justify this, a more detailed wall-clock and latency analysis at inference would strengthen the case.\n\n Some baselines that aggressively shorten chains show larger efficiency but lower accuracy; while the paper explains the trade-offs, a controlled budget-equalized comparison could more cleanly quantify accuracy at equal compute."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ftTwPc84Vt", "forum": "E6keG5QDct", "replyto": "E6keG5QDct", "signatures": ["ICLR.cc/2026/Conference/Submission15536/Reviewer_mhkM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15536/Reviewer_mhkM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980353029, "cdate": 1761980353029, "tmdate": 1762925813802, "mdate": 1762925813802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **REA-RL (Reflection-Aware Reinforcement Learning)**, a framework for improving reasoning efficiency in Large Reasoning Models (LRMs) without sacrificing accuracy. The authors identify the problem of *overthinking*—excessive reflection that increases inference cost—and introduce two key components: a **reflection model** to detect and remove redundant reasoning in real-time, and a **reflection reward** to preserve necessary reflective behavior. Combined, these techniques reduce inference token usage by **36%** while maintaining or improving performance across several reasoning benchmarks, offering a balanced solution between efficiency and reflection quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets an important and practical issue—over-reflection in large reasoning models.  \n2. The proposed REA-RL framework is conceptually clear.  \n3. The writing is easy to follow.\n4. The experiments demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. The paper evaluates only on the MATH domain, and does not include experiments in other reasoning domains (e.g., code, general QA, agentic tasks), which limits the demonstrated generality of the approach.  \n2. All experiments are conducted solely on R1-Qwen-7B; including additional model families and scales would strengthen the empirical evidence and show broader applicability.  \n3. The method introduces extra computational overhead, requiring double rollouts and an additional reflection-model inference."}, "questions": {"value": "1. This work shows that the reflection model $M_{reflect}$ contributes to the efficiency–efficacy trade-off. Would it be possible to include a baseline that simply truncates the reasoning sequence when the first reflection token appears and then appends </think> to complete the response from that point?  \n2. Compared with methods using only length-based rewards, approaches involving \\( M_{\\text{reflect}} \\) seem to yield slightly lower accuracy. Do the authors have any insights into why this happens?  \n3. In line 319, the paper claims that GRPO with accuracy-only rewards does not further improve accuracy, which seems inconsistent with previous findings such as *DeepScaleR* [1]. Could the authors clarify this discrepancy or provide an explanation?  \n\n[1] *DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL.* https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9uQ6c9ZBdT", "forum": "E6keG5QDct", "replyto": "E6keG5QDct", "signatures": ["ICLR.cc/2026/Conference/Submission15536/Reviewer_xPcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15536/Reviewer_xPcf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141623449, "cdate": 1762141623449, "tmdate": 1762925813140, "mdate": 1762925813140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}