{"id": "iTK8BZ8i3J", "number": 23583, "cdate": 1758345917059, "mdate": 1759896806640, "content": {"title": "Vision Language Models Cannot Reason About Physical Transformation", "abstract": "The ability to comprehend physical transformations is essential for reasoning in dynamic, real-world environments, yet it remains unclear whether vision–language models (VLMs) possess this capacity. To address this gap, we evaluate whether VLMs exhibit conservation—the understanding that physical quantities remain invariant despite changes in appearance. Inspired by Piaget’s framework, we design a systematic benchmark for conservation reasoning across four quantitative domains: number, length, volume, and size. Each task requires models to integrate visual evidence across time to identify invariant properties under transformation. To guard against spurious success, we also curated a set of negative control tasks paired one-to-one with the benchmark tasks, in which the targeted quantities are not conserved. Both benchmarks and controls incorporate four prompt types, three frame extraction methods, and 48 task variations per domain, yielding 384 baseline tasks and 13,824 total questions. Evaluating 34 VLMs, we find that none achieve systematic success: across conserving and non-conserving tasks, models consistently perform only marginally above chance, with those excelling on conservation tasks performing inversely on controls, indicating rigid biases in reasoning about physical processes. Moreover, models show no benefit from higher temporal resolution or prompt design, suggesting a reliance on static visual cues. Together, these findings indicate that current VLMs fail to internalize the structured representations necessary for systematic physical inference.", "tldr": "", "keywords": ["vision language models", "physical transformation", "multi-image understanding", "spurious correlation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3708cee22cefc152c063a4952e4fab777cab3f45.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores whether vision-language models (VLMs) can reason about physical conservation - that the quantitative properties of objects, such as their number, length, volume, and size remain invariant over short time periods. A large benchmark of videos and extracted frame sequences is constructed, and used to study 34 VLMs. They find that no VLMs can robustly reason about conservation, at least not to the level of the average human. This is a surprising result given the established success of VLMs on large-scale general benchmarks and the triviality (for humans) of the conservation tasks being used."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is methodologically sound, and is an incremental contribution to a growing literature exploring the physical reasoning capabilities of VLMs. Particular strengths are:\n* The rigorous control conditions used throughout to test alternative explanations for VLM model performance.\n* The large number of open-source models used.\n* The use of a meaningful human baseline for comparison."}, "weaknesses": {"value": "The paper has a number of weaknesses:\n1. The hybrid evaluation is an interesting solution to the problem of evaluating complex outputs, but using LLM judges incurs significant overhead for the practitioner. Since the paper currently relies only on open-source models (as far as I can tell) and the benchmark uses multiple choice questions, the authors could simply use the log-probability of the choice label, conditional on the text-image input. This could be normalised across the possible outcomes too. This is quite standard in benchmark evaluation (see EleutherAI's lm-evaluation-harness).\n2. Many new models are also capable of processing videos. I would suspect that the next generation of VLM will process video data effectively. Therefore, the longevity of the benchmark would be ensured if video-based evaluation was a central contribution. The authors could conduct a small study with, say, Qwen-2.5-VL-7B (an open source video-language model) to see if there is any difference between performance on video and performance with frame sequences.\n3. Many of the tasks used here might be quite novel to the language model, so it's not clear whether these models *could* learn to reason about conservation if given the right training, or whether there is something deeper about architecture/pre-training that prevents them from doing so. Two further experiments are required to elucidate this. First, examining whether in-context learning can boost model performance. Here, examples of conservation/non-conservation with correct labels are given sequentially prior to the test question. Second, examining whether supervised fine-tuning on a subset of the tasks improves performance. There are three conditions of interest here: training on (a) a random subset of the tasks; (b) conservation problems; (c) non-conservation problems. I suspect, however, that fine-tuning would be just as brittle (see Schulze-Buschoff et al. 2025). This would make a more powerful point about VLMs. It's not just that current models off-the-shelf are incapable, but it's a feature of the architecture/large-scaled pre-training that prevents them from having these common-sense intuitions.\n4. I don't really think the white-image condition is a good control. To me, the bias towards invariance is shown by the delta between the conservation and non-conservation conditions. I would not expect systematic deviations from chance with white images, because where would that systematicity come from, absent some visual input? Perhaps I am missing the logic here.\n5. There is some missing literature, which I include below.\n6. It's a semantic point, but I dispute that the main contribution of this paper is a benchmark. Rather, it's a careful series of experiments to test some hypotheses about MLLM capabilities. I don't think anyone will want to use this as a benchmark again, due to (a) the overhead of the LLM judges, and (b) the narrow scope of what the benchmark seeks to measure. I would recommend reframing it as an empirical investigation of the physical intuitions of MLLMs, rather than a useful dataset for the practitioner.\n7. MLLM and VLM seem to be used interchangeably throughout, starting in the abstract.\n8. In Figure 1, the question for the 'Number' task is the same as the 'Size' task (Is the size of the playdough in the first image the same as in the final image?). Shouldn't it be something to do with coins (given the details in the list in Section 3.1).\n\n### Missing Literature\n\nThere are some further studies on visual reasoning that are not mentioned:\n\n1. Balazadeh, V., Ataei, M., Cheong, H., Khasahmadi, A. H., & Krishnan, R. G. (2025). Physics context builders: A modular framework for physical reasoning in vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7318-7328).\n\n2. Campbell, D., Rane, S., Giallanza, T., De Sabbata, C. N., Ghods, K., Joshi, A., ... & Webb, T. (2024). Understanding the limits of vision language models through the lens of the binding problem. Advances in Neural Information Processing Systems, 37, 113436-113460.\n\n3. Schulze Buschoff, L. M., Akata, E., Bethge, M., & Schulz, E. (2025). Visual cognition in multimodal large language models. Nature Machine Intelligence, 7(1), 96-106.\n\n4. Schulze Buschoff, L. M., Voudouris, K., Akata, E., Bethge, M., Tenenbaum, J. B., & Schulz, E. (2025). Testing the limits of fine-tuning to improve reasoning in vision language models. arXiv preprint arXiv:2502.15678.\n\nWith respect to cognitive psychology, the line of work on the tunnel effect, object files, and object persistence should be discussed and mentioned:\n\n1. Burke, L. 1952: On the tunnel effect. Quarterly Journal of Experimental Psychology, 4, 121 – 138.\n\n2. Flombaum, J. I., & Scholl, B. J. (2006). A temporal same-object advantage in the tunnel effect: facilitated change detection for persisting objects. Journal of Experimental Psychology: Human Perception and Performance, 32(4), 840. \n\n3. Flombaum, J. I., Kundey, S. M., Santos, L. R., & Scholl, B. J. (2004). Dynamic object individuation in rhesus macaques: A study of the tunnel effect. Psychological science, 15(12), 795-800.\n\n4. Mitroff, S. R., Scholl, B. J., & Wynn, K. (2004). Divide and conquer: How object files adapt when a persisting object splits into two. Psychological Science, 15(6), 420-425.\n\n5. Noles, N. S., Scholl, B. J., & Mitroff, S. R. (2005). The persistence of object file representations. Perception & Psychophysics, 67(2), 324-334.\n\n6. Scholl, B. J. (2007). Object persistence in philosophy and psychology. Mind & Language, 22(5), 563-591."}, "questions": {"value": "* This work and others all point to the conclusion that VLMs really aren't that good at 'intuitive physics'. And yet, they seem to perform really well on standard large-scale benchmarks and users love to use them. I'm intrigued whether the authors think that an inability to reason about conservation is actually *problematic* for VLM use and deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w1S9n0SXiD", "forum": "iTK8BZ8i3J", "replyto": "iTK8BZ8i3J", "signatures": ["ICLR.cc/2026/Conference/Submission23583/Reviewer_1A1m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23583/Reviewer_1A1m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480414726, "cdate": 1761480414726, "tmdate": 1762942721501, "mdate": 1762942721501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ConservationBench, a video-based benchmark to test whether VLMs can reason about physical transformations by judging conservation (vs. matched non-conserving counterfactuals) of four quantities—number, length, volume, size. The suite varies temporal sampling (3/8/16 frames), frame-selection strategies (uniform, model-based, human-picked), and prompt types, producing 13824 trials. Across 34 models, the authors report performance only marginally above chance on average; improvements on conserving items often invert on matched non-conserving cases, suggesting brittle heuristics rather than true transformation reasoning. Human accuracy (on a subset) is ~95%. The paper concludes that current VLMs cannot reason about physical transformation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on conservation under transformation with a counterfactual non-conserving item.\n\n- This paper provides the results under different prompt styles, frame counts, and frame-selection methods (uniform / human / SEVILA-style)"}, "weaknesses": {"value": "- The current evaluation setup only provides models maximum 16 frames. It is questionable that is this enough even for human to understand the physical transformation happening in the video. Therefore, the claim like “VLMs cannot reason about physical transformation” are overstated if the inputs to the models does not contain enough information to solve the task.\n\n- The human baseline details are missing. How did you evaluate the human performance exactly? \n\n- The paper does not evaluate state-of-the-art closed-source VLMs such as Openai, Claude, and Gemini models."}, "questions": {"value": "- The question in Figure 1 for number case seems wrong. \n\n- Please fix VLM/MLLM naming consistency (in abstract) and remove duplicated paragraphs (line 180-186). \n\n- It would be helpful to include a full input prompt to the model including both image inputs and text inputs. \n\n- What is the performance of the state-of-the-art closed-source VLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPLDfNi1AQ", "forum": "iTK8BZ8i3J", "replyto": "iTK8BZ8i3J", "signatures": ["ICLR.cc/2026/Conference/Submission23583/Reviewer_z9U6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23583/Reviewer_z9U6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670003449, "cdate": 1761670003449, "tmdate": 1762942721071, "mdate": 1762942721071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark to test if models understand that some physical properties remain invariant under transformations. They show that models can not keep track of conserved properties very well. While testing whether models can keep track of physical properties is a nice idea, I feel the scope of the paper and the insights it delivers are too limited to recommend acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality** I'm not aware of any previous work investigating conservation in vision language models. \n\n**Quality** The benchmark is well constructed.\n\n**Clarity** The paper follows a clear outline, although I feel the writing is a bit bloated and unaccessible at parts.\n\n**Significance** The paper adds to a growing body of work highlighting the shortcomings of vision language models with basic visual processing. I like the split of the benchmark in conserving vs. non-conserving stimuli. Also, the result that models seem to default to either, always saying a property is conserved or always saying it has changed, regardless of the actual transformation, is interesting. However, there have been a large number of papers evaluating specific visual properties in vision language models with specific datasets/benchmarks."}, "weaknesses": {"value": "In general, I feel like this paper does not provide a strong enough novel contribution to recommend acceptance. There is at this point a large growing body of evidence that vision language models fail at very basic visual processing. While this paper adds some novel data to this pile of findings, I find that the aspect of perception that it investigates is just too narrow. Also, the authors do not offer concrete ideas on how these problems could be overcome."}, "questions": {"value": "**Main questions**\n- How is the chance rate that is mentioned throughout at 33% if the models are always asked a binary question (does the property change versus does it not change)? Is this because you map the answer either to one of the two options or to \"Fail\" if it can't be parsed correctly? This is not the most obvious way of computing a baseline for me, if we accept that the models could output anything and LLM judges finally decide if the output maps to one of the two answer options, the chance level is not obvious to calculate. In any case, I think the reasoning for why you set it to 33.3% should be made more transparent in the text.\n- It's interesting and a bit strange to me that the CoT prompting performs the worst. Could you maybe speculate a bit on why that is?\n- For the human evaluation, you write \"The aggregated human accuracy reaches 95.25%\". Just to make sure, this is the accuracy given videos with all frames and in the \"non-strict\" evaluation, right? I think this could be made a bit more clear. \n- Initially I thought the number of frames (3, 8, 16) would be combined with the different methods of selection (Uniform, Human-selected, SeViLA), but here you seem to report them separately in Figure 3. For clarity, B shows uniformly sampled 3, 8, and 16 frames, right? And for C, is the number of frames fixed for all three selection methods and if so, what is it? Again, I may have missed these details in the text but feel they could be outlined more clearly, maybe even in the caption of the Figure.\n\n**Minor comments**\n- I think the abstract is too long and should be cut down.\n- Line 43 \"Yet it remains unclear whether VLMs possess a true understanding of physical principles or the capacity to operate reliably in embodied physical environments.\" there is previous work that shows VLMs do not understand physical principles [1, 2].\n- Line 82 comes out of nowhere \"Physical quantity refers to the measurable magnitude of objects along certain dimensions, while spatial transformation denotes the continuous processes through which objects change in appearance under perception.\" What is this in reference to? It seems a bit misplaced.\n- Line 315 \" 95. 25%\", there's likely a space too much.\n- Figure 2A the colors in the legends do not match the color of the bars in the plot?\n- Figure 2 captions and titles are a not coherent, \"Non-conserving\" in the caption and \"Non-conserve\" in the title. \n\n[1] Schulze Buschoff, Luca M., et al. \"Visual cognition in multimodal large language models.\" Nature Machine Intelligence 7.1 (2025): 96-106.\n\n[2] Balazadeh, Vahid, et al. \"Synthetic vision: Training vision-language models to understand physics.\" arXiv e-prints (2024): arXiv-2412."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UwtNzGovuw", "forum": "iTK8BZ8i3J", "replyto": "iTK8BZ8i3J", "signatures": ["ICLR.cc/2026/Conference/Submission23583/Reviewer_X9Du"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23583/Reviewer_X9Du"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923405350, "cdate": 1761923405350, "tmdate": 1762942720758, "mdate": 1762942720758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}