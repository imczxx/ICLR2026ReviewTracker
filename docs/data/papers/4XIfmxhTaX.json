{"id": "4XIfmxhTaX", "number": 5395, "cdate": 1757906995881, "mdate": 1759897978058, "content": {"title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models", "abstract": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy controlling methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.", "tldr": "", "keywords": ["Reinforcement fine-tuning", "Large language models", "Entropy", "Learning dynamic"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2601a4711fbaec50816830630c7a48b3e22ea58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies entropy dynamics (i.e., the change of policy entropy) in policy gradient methods. It presents a preliminary theoretical analysis of entropy change from the logits perspective. Based on this analysis, the paper proposes entropy-controlling methods that identify tokens within a training batch exerting disproportionate impact on entropy changes. This enables selective mitigation of outlier token influence, achieving fine-grained and flexible control over entropy regularization throughout training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The theoretical analysis in Lemma 1 and Theorem 2 is clear and well-presented. While the derivations are straightforward, the insights they provide are valuable. In particular, the measurement approach in Equation (6) is both insightful and practically useful.\n- The paper provides extensive discussion of existing approaches that fall under this framework, offering a unified view of entropy control methods."}, "weaknesses": {"value": "- The theoretical analysis has notable limitations. Following Ren & Sutherland (2025), the analysis is based on gradients computed from fixed data. This differs from the standard RL setting, where data is sampled from the evolving model distribution, potentially limiting the applicability of the theoretical insights.\n\n- Despite the solid theoretical analysis, the empirical advantage over existing methods remains unclear. Specifically, the paper lacks sufficient comparison with direct entropy regularization methods and the regularization approaches proposed in Cui et al. (2025), Yu et al. (2025), and Wang et al. (2025).\n\n- The empirical improvements over GRPO in Table 1 are marginal, particularly for the Avg@K metric. I am curious about the training dynamics underlying these results. Could the authors provide training curves showing Avg@K performance and entropy changes throughout training? This would help clarify whether the proposed method achieves quite different dynamics."}, "questions": {"value": "I am unclear about the paper's claim that \"the derivation in Cui et al. (2025) relies on idealized assumptions that do not reflect the actual training dynamics of RFT.\" This criticism appears unfounded. In fact, Cui et al.'s analysis seems more aligned with realistic training scenarios by taking into consideration the expectation over model sampling, which better captures the evolving data distribution in RL settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ULGZmufvlV", "forum": "4XIfmxhTaX", "replyto": "4XIfmxhTaX", "signatures": ["ICLR.cc/2026/Conference/Submission5395/Reviewer_STqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5395/Reviewer_STqk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710482931, "cdate": 1761710482931, "tmdate": 1762918036894, "mdate": 1762918036894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how the entropy of large language models changes during reinforcement fine-tuning (RFT). While many prior methods adjust entropy heuristically, this paper builds a theoretical framework to explain these changes in a principled way.\n\nThe authors first analyze how updating a single token’s logit affects model entropy and derive a formula linking entropy change to a new “entropy discriminator” score that depends on the token’s probability and overall entropy. They then extend the analysis to Group Relative Policy Optimization (GRPO) and show that entropy change depends on how a token’s discriminator score compares to its policy-wide average. Using these insights, they design two simple entropy control methods—ClipB and ClipV—which clip or mask updates that would cause large entropy shifts.\n\nExperiments on math reasoning datasets (AIME24, AIME25, and DAPO500) show that these methods prevent entropy collapse, improve exploration, and boost Pass@K accuracy on Qwen models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a clear, simple theoretical insight on the entropy dynamics in the RFT process. The paper derives a compact first-order expression that links a single-logit update to entropy change. This gives an intuitive discriminator that predicts whether an update increases or decreases entropy. The derivation is straightforward and easy to follow (Lemma 1 / Theorem 1). The authors extend the single-token result to a GRPO optimization step. This links per-token effects to the actual optimization algorithm used in practice, improving applicability beyond toy analysis.\n2. The analysis provides a single lens to interpret several previously proposed entropy heuristics (clipping schemes, entropy regularization, probability-weighted updates), which is valuable for the community and may reduce ad-hoc tuning."}, "weaknesses": {"value": "1. Corollary 1 appears to be incorrect. It claims that the expected entropy change within GRPO optimization is zero. If I understand correctly, this is proved by calculating the expectation of the first-order entropy change $\\Delta H$ under the token distribution $k \\sim p$. However, $\\Delta H$ contains the advantage $A$, which depends on the token $k$. The paper seems to ignore this dependence.\n2. The scope of the theoretical analysis is limited. The analysis is first-order (small perturbation) and studies a single-token update effect. While useful intuitively, RFT involves updates based on responses consisting of multiple tokens. Besides, the paper assumes that many practical algorithmic components including clipped importance sampling ratio, KL penalties and entropy bonuses are inactive; this significantly limits the scope of the analysis.\n3. The paper uses Theorem 1 to interpret clipping mechanisms and entropy regularization techniques from previous studies. However, Theorem 1 is overly idealistic—it analyzes updates to a single token's logit. In actual RFT procedures, multiple tokens' logits change within one gradient update. This creates a notable gap between theory and practice. \n4. The paper compares against “vanilla GRPO” and interprets prior methods conceptually, but empirical head-to-head comparisons against recent strong entropy-aware methods (e.g., DAPO) are missing. A direct experimental comparison would clarify where ClipB/ClipV stand relative to the best existing practical methods."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aYUjsN0r9d", "forum": "4XIfmxhTaX", "replyto": "4XIfmxhTaX", "signatures": ["ICLR.cc/2026/Conference/Submission5395/Reviewer_myqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5395/Reviewer_myqB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912631628, "cdate": 1761912631628, "tmdate": 1762918036706, "mdate": 1762918036706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a theoretical framework to explain how the entropy of LLMs evolves during reinforcement fine-tuning. The authors start from single-token logit updates and derive a first-order analytical expression for entropy change, showing that it depends on both the update direction and a derived entropy discriminator $S^*$. They extend this analysis to GRPO, introducing an expected entropy baseline that governs whether updates increase or decrease entropy. Based on this framework, the paper proposes two entropy-control methods, ClipB and ClipV, that clip gradients associated with tokens contributing excessively to entropy fluctuations. Empirical experiments on reasoning benchmarks such as AIME24, AIME25, and DAPO demonstrate that these methods mitigate entropy collapse, preserve exploration, and improve problem-solving diversity in RFT-trained LLMs"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper’s key strength is to provide a very clean expression of the change of entropy during policy updates. \n\n2. This expression motivates clean and simple clipping techniques (ClipB and ClipV) to improve exploration without destabilizing training. \n\nOverall, the work contributes both theoretical depth and practical utility, marking a valuable advance in understanding and stabilizing reinforcement fine-tuning for LLMs"}, "weaknesses": {"value": "1. From an empirical perspective, the experiments are limited to the Qwen model family, leaving uncertainty regarding the generality of the proposed methods across other architectures or training setups. As pointing out in previous paper, Qwen is very different from other models in terms of reasoning.\n\n2. The theoretical analysis presented in Section 3.2 appears broadly applicable to generic policy-gradient algorithms, including not only GRPO but also other methods such as PPO. It is therefore unclear why the proposed entropy-control strategies (ClipB and ClipV) were not evaluated under alternative policy-gradient frameworks. \n\n3. The current analysis is confined to an on-policy setting; an extension or discussion of the off-policy case would significantly strengthen the theoretical completeness of the work. \n\n4. Equation (7) may be confusing at first glance, as it is not immediately evident why the GRPO objective takes that form. In my opinion, it seems more accurate to interpret it as a surrogate loss whose gradient coincides with policy gradient of GRPO loss. \n\nMinor writing suggestion:\n\n1. Few steps in Equation (8) are missed, particularly those involving the gradient of the softmax function, making the result less transparent.\n \n2. Line 161 subscripts and superscripts"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0QZMnSDs6Q", "forum": "4XIfmxhTaX", "replyto": "4XIfmxhTaX", "signatures": ["ICLR.cc/2026/Conference/Submission5395/Reviewer_2wia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5395/Reviewer_2wia"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956155554, "cdate": 1761956155554, "tmdate": 1762918036490, "mdate": 1762918036490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article investigates the factors that drive increases and decreases in a model’s output entropy when logits change. By analyzing variations caused by individual tokens and by the GRPO loss function, the authors propose a key metric for computing entropy change. They explain how existing methods affect entropy, how those effects relate to the new metric, and introduce two algorithms built on this metric. Both algorithms also perform strongly on downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide a theoretical analysis of entropy variation under changes in logits caused by individual tokens and by gradients derived from the GRPO loss, and they propose a metric for assessing entropy change.\n\n2、 Building on the new metric, the authors establish connections with previous reinforcement learning methods and introduce two new algorithms.\n\n3. Both new algorithms achieve performance improvements on downstream tasks."}, "weaknesses": {"value": "All the analysis is done on \"tabular\" cases. However, for the RL in LLMs, the updates of different positions and different tokens will be combined by shared parameters in LLM. Why can the theoretical analysis still apply to the scenarios."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kzCRJHPTco", "forum": "4XIfmxhTaX", "replyto": "4XIfmxhTaX", "signatures": ["ICLR.cc/2026/Conference/Submission5395/Reviewer_BPqb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5395/Reviewer_BPqb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983896447, "cdate": 1761983896447, "tmdate": 1762918036224, "mdate": 1762918036224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}