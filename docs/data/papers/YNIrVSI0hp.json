{"id": "YNIrVSI0hp", "number": 24725, "cdate": 1758359676604, "mdate": 1759896752585, "content": {"title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling", "abstract": "Formulating optimization problems for industrial applications demands significant manual effort and domain expertise. While Large Language Models (LLMs) show promise in automating this process, evaluating their performance remains difficult due to the absence of robust metrics. Existing solver-based approaches often face inconsistency, infeasibility issues, and high computational costs. To address these issues, we propose ORGEval, a graph-theoretic evaluation framework for assessing LLMs’ capabilities in formulating linear and mixed-integer linear programs. ORGEval represents optimization models as graphs, reducing equivalence detection to graph isomorphism testing. We identify and prove a sufficient condition, when the tested graphs are symmetric decomposable (SD), under which the Weisfeiler–Lehman (WL) test is guaranteed to correctly detect isomorphism. Building on this, ORGEval integrates a tailored variant of the WL-test with an SD detection algorithm to evaluate model equivalence. By focusing on structural equivalence rather than instance-level configurations, ORGEval is robust to numerical variations.  Experimental results show that our method can successfully detect model equivalence and produce 100\\% consistent results across random parameter configurations, while significantly outperforming solver-based methods in runtime, especially on difficult problems. Leveraging ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs on optimization modeling. Our results reveal that although optimization modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4 achieve the highest accuracies under direct prompting, outperforming even leading reasoning models.", "tldr": "We introduce a graph theoretical guaranteed evaluation method for modeling equivalence detection.", "keywords": ["AI for OR", "evaluation", "optimization modeling", "autonomous mathematical formulation", "LLM", "benchmark"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/563e6e10fb2f5a6f796aa3f5c2229c3208f39b66.pdf", "supplementary_material": "/attachment/2838a27b89adee82219de387fc6f727aaff49a83.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on evaluating whether LLMs can correctly formulate an optimization model. The authors propose a framework called ORGEval, which represents optimizations models as graphs and tests model equivalence via graph isomorphism rather than by solving instances. The authors prove a sufficient condition called symmetric decomposable (SD) under which a Weisfeiler–Lehman (WL) test is guaranteed to decide isomorphism. ORGEval first verifies SD and then applies a tailored WL test. \n\nThe authors also release a benchmark called Bench4Opt which has 394 problems where all reference instances satisfy the SD condition. Using ORGEval, evaluation is very efficient and 100% consistent across five random configurations, while solver-based comparisons fail frequently and can disagree."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduced a new evaluation paradigm that shifts from solver-based numerical checks to graph isomorphism structure evaluation. This shows better runtime efficiency and consistency compared to solver-based baselines on their benchmark. The Bench4Opt benchmark is the first model data separated benchmark for optimization modeling, and provides a valuable resource for evaluating LLM’s modeling capabilities."}, "weaknesses": {"value": "1. The WL-based decision is guaranteed correct only when the symmetric decomposable (SD) condition holds, and all Bench4Opt problems happen to satisfy SD. This risks overestimating real-world coverage if SD is less common.\n2. Equivalence over model–data separation is approximated by testing five random parameter draws, but this is under-justified.\n3. The evaluation framework relies on strict graph isomorphism, and can unfairly penalize mathematically correct but structurally different formulations (it rewards structural replication but not genuine modeling understanding and alternative solutions).\n4. The paper presents aggregate accuracy scores but lacks an analysis of the types of errors LLMs are making"}, "questions": {"value": "1. How prevalent do you think the SD property is in complex optimization problems\n2. The paper tests model-level equivalence on only five random parameter configurations, but could two non-equivalent models coincidentally appear equivalent under such limited sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6CPYzgX82n", "forum": "YNIrVSI0hp", "replyto": "YNIrVSI0hp", "signatures": ["ICLR.cc/2026/Conference/Submission24725/Reviewer_NtBm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24725/Reviewer_NtBm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860392613, "cdate": 1760860392613, "tmdate": 1762943176678, "mdate": 1762943176678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the ORGEval, which represents optimization models as graphs, reducing equivalence detection to graph isomorphism testing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper innovatively uses graph to determine whether the optimization problem modeling is correct."}, "weaknesses": {"value": "- Is it reliable to judge the correctness of a model by examining its graph structure? Are there complex optimization problems where different modeling methods exist, but the final result is always correct? For example, some problems in combinatorial optimization, or problems with duality.\n- For complex problems, correct modeling and correct solution are not equivalent. For example, some complex problems may have time complexity issues when solving them, and simple modeling methods may prevent the problem from being solved within the limited time and space resources. Optimization problems that use complex modeling techniques are not necessarily isomorphic to the graph given by the ground truth.\n- What are the problem sizes covered by ORGEval? For example, as shown in Appendix B.1, what is the approximate distribution of the number of variables and constraints for each type of problem? For instance, how many nodes are there in the Traveling Salesman Problem?\n- In modeling certain types of problems, the constraints don't seem to increase with the complexity of the data. For example, in the Traveling Salesman Problem, regardless of the number of nodes, the constraints don't appear to become more complex, so the complexity of modeling this problem doesn't increase. In this case, the types of problems covered by the benchmark are more important than the number of problems. How does this article view and consider this issue?\n- How do the chat model (e.g. deepseek-v3) and the thinging model (e.g. deepseek-r1) perform differently on this type of task? Could you provide a detailed analysis?"}, "questions": {"value": "As described in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xAdzDSMMfN", "forum": "YNIrVSI0hp", "replyto": "YNIrVSI0hp", "signatures": ["ICLR.cc/2026/Conference/Submission24725/Reviewer_WWVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24725/Reviewer_WWVn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896927339, "cdate": 1761896927339, "tmdate": 1762943176142, "mdate": 1762943176142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the translating problem descriptions into optimization models using LLMs from the angle of verification of results. \n\nThe main claim is that, instead of relying on ground truth optimal values, the proposed ORGEval represents MIPs as a standard weighted bipartite graphs and reduces equivalence detection to a graph isomorphism problem between the LLM generated model vs. ground truth model. It leverages the Weisfeiler–Lehman (WL) test under a newly proposed sufficient condition called symmetric decomposable (SD), ensuring correctness when this condition holds.\n\nThe paper also presents Bench4Opt, a dataset of optimization problem descriptions, models and instances, and LLM model generation accuracy on this dataset using a suite of different recent LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Interesting and relevant topic for using LLM in translation task for optimization problems.\nIntroduction of a new dataset is always a great contribution to the community\nThere is rigor to the new SD condition"}, "weaknesses": {"value": "I liked reading this paper but fail to see its significance. \n\nAt a high-level, the paper moves the assumption around from knowing the optimality to knowing the model. Still, this requires known a ground truth model --which is arguably is the most daunting part of verification. \n\nThe difficult part in model translation is having the ground truth; either in the form of a model OR the optimal value that it produces. Only after, either of them of is known, one can check the correctness. IF the ground truth optimal value is known; equivalence check comes down to running the solver and a simple value comparison. Alternatively, IF the ground truth model is known; equivalence check comes down to running a form of graph isomorphism (this paper). In that lens; the paper does not change the difficulty of verifying LLM generated models, just changes what we verify. That's the major weakness of the paper. \n\nExperiments are far from ideal: \n\nThe paper claims that running the solver might take time. Alternatively, one might claim that running several smaller instances already serve as a good proxy. One does not need to solve the hardest instance of a problem to verify a model (as done in the MIPLIB hard experiments which I don't see what Table 1 conveys). \n\nOn the contrary, running the solver might be an easier approach (since these solvers are quite established) as opposed to running an ad-hoc mechanism, like the one presented here. Btw, I did not see code/data links in the submission. Given the centrality of SD detection and WL implementation choices open‑sourcing is critical for this part. Same comment for the benchmark dataset as well. \n\nTable 1 compares MIP solver runtime to prove optimality compared to an implementation of a checking algorithm. I don't see what this comparison achieves. This can be made arbitrarily positive in favor of the deterministic/poly algorithm here as to infinity for sizes above solver limits. Again, one does not need to solve the largest instances to check models. Aren't there other graph isomorphism based strong baselines to compare against where you can show the added value of your new SD proposal? \n\nA better comparison to consider; set the time-limit to the runtime of your algorithms and then check if there NO instances that a solver can find the ground truth optimal value. IF that's the case, now your method is providing a verification that was not possible before. Still, the comparison is not apples-to-apples: solving MILPs is fundamentally harder than computing a WL coloring; evaluation cost differs in kind, not degree. \n\nDoes your method require the same number of variables and/or constraints? I don't think so, but it would be good to clarify in the paper. But I believe it requires that same parameter names are used, which is a strong assumption, no? Please discuss in the paper. Also, what happens when there are slack variables or constraint aggregations (common in OR models) that still preserves model correctness? Would your equivalence check work with that? \n\nThere is an assumption in the overall algorithm that \"all ground truth instances are selected to be symmetric\". That greatly simplifies the equivalence checking, no? How can all the ground truth guaranteed to be symmetric? Please explain this further \n\nI am not following what's Table 2 is trying to provide. Why would anyone run models with random parameters? IF all instances in this dataset are somehow SD, isn't it a tautology that your method would return %100. \n\nWhat would have happened if your dataset had non-SD instances? \n\nTable 3, the performance of current LLMs modeling these problems is a nice addition (it's good to know) but not related to the paper's main claims. Btw, in this table, what is structured vs. unstructured optimization tasks. This was never defined, so I am not even sure what this table shows. Also, are this running LLMs one shot against the dataset. There are advanced modeling co-pilots so it would be more interesting to see their results on this dataset rather than vanilla prompting (but again, this is not the main part of the paper so it's ok) \n\nRegarding existing work, a lot of improvements are needed. \n- The similarity/difference between this and the existing work such as EquivaMap is never discussed in the main paper. This is a major problem and must be fixed. \n\n- The claim that Bench4Opt is \"the first model-data separated dataset for optimization modeling\" is NOT correct. Please see Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinchttps://arxiv.org/abs/2503.10642. Text2Zinc must be cited and this claim must be revised. In fact, it is important to point out that this new dataset still hardcodes the parameter names to be found in the data as part of the textual description as well as the variable names and their types. I appreciate the effort but that's still not \"true\" natural textual representation of optimization problems. That does not take away from the hard work and the effort of the team in curating and verifying --which is crucial for the community. Thank you! \n\n- Regarding LLM modeling co-pilots, consider also covering Ner4Opt (Constraint'24, https://link.springer.com/article/10.1007/s10601-024-09376-5), ChatOpt(CP'24 https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.CP.2024.20) and OptiChat (https://arxiv.org/abs/2501.08406). These would help noting that \"asserting optimal values against the ground truth\" is NOT the only verification that exists in the prior work, especially for Feasibility/Satisfaction/Logic problems \"asserting feasibility\" is commonly done which must be mentioned. \n\n- The section on Related Work in the Appendix would benefit from including earlier works, such as for algorithm configuration part: ISAC-Instance specific algorithm configuration (ECAI'10), and for algorithm selection part: Algorithm selection and scheduling (CP'12). \n\n- Typos in references; like \"consingh2012overviewuration\" plz fix. Other typos: “isomporphism,” “sturctured,” “desctiption,” “euqivalent,” “choosen,” etc; plz spell check\n\n- The formal definition of DS requires groups to be disconnected. But isn't the examples in Figure 10-11 has cross-group connections? \n\n- Why would random parameter sampling yield SD instances with probability 1? that sounds too strong, no? Is it because we start with a dataset that's SD already? \n\n- Complexity is claimed O(k(m+n)2) in main text and O(kmn) later. Why?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTdNx5CQZe", "forum": "YNIrVSI0hp", "replyto": "YNIrVSI0hp", "signatures": ["ICLR.cc/2026/Conference/Submission24725/Reviewer_kxC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24725/Reviewer_kxC7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949987042, "cdate": 1761949987042, "tmdate": 1762943175861, "mdate": 1762943175861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes a framework to assess whether LLM-generated optimization models were equivalent (to reference model or another generated model) by representing model instances as bipartite graphs and reducing the problem as a graph isomorphism problem. The core contribution is identifying symmetric decomposable (SD) graphs as a sufficient condition (instead of unfoldable) under which the Weisfeiler–Lehman (WL) test is guaranteed to correctly detect isomorphism. The authors also introduce Bench4Opt, a dataset of 394 optimization problems with model-data separation, and use it to benchmark several state-of-the-art LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Authors made a fair case that current solvers are not efficient or consistent in identifying equivalence in optimization problems, due to difficulty in handling different parameter configurations, computational costs, and inability to handle infeasible instances\n- Experiments demonstrated that their methodology works for this subclass of problems, with 100% consistency across random parameter configurations\n- The theoretical characterization of SD graphs and proof that WL-test correctly determines isomorphism under this condition appears sound"}, "weaknesses": {"value": "- Limited contribution. Essentially, the authors added a check for symmetric decomposable on top of a Weisfeiler–Lehman (WL) test to check whether two optimization models are equivalent. The core pipeline uses the graph representation and WL-test from previous work, and the addition of SD detection being the only novel algorithmic component.\n- Limited testing on other datasets, only worked on its own benchmark.\n- The benchmark dataset is also relatvely small (394), and selection / dataset construction process is not clearly described beyond a list of sub-classes of problems.s\n- Limited explanation on testing performacne on different LLMs on the benchmark. e.g. \n- Accuracy is binary (equivalent v.s. not-equivalent) - Would be interesting to see the degree to which a model is inaccurate and also how (e.g. is it a matter of unable to translate the requirements?)"}, "questions": {"value": "- Is it possible to have non-SD instances that are equivalent? Does the framework just reject them altogehter? \n- How are the \"hand-crafted\" problems derived? There are >100,000 entreis in MIPLIB, how is the Bench4Opt subset selected?\n- Is there a dataset breakdown of the benchmark's composition?\n- For the dataset, the paper mentions \"two level of abstract, structured and unstructured description\" -  How do they differ? and which one is fed into the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pERcog2kQa", "forum": "YNIrVSI0hp", "replyto": "YNIrVSI0hp", "signatures": ["ICLR.cc/2026/Conference/Submission24725/Reviewer_NuHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24725/Reviewer_NuHH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995437850, "cdate": 1761995437850, "tmdate": 1762943175536, "mdate": 1762943175536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduced a new metric to evaluate the accuracy of formulating mixed-integer linear programs (MILPs) by large language models (LLMs). Specifically, instead of checking the optimal value of the formulated MILP, the authors proposed to check the \"structural equivalence\", measured by isomorphism of the underlying graphs of the MILPs. The new metric fixes several limitations of the common practice of checking the optimal values. The author also proposed an algorithm to perform the equivalence check.\n\nOverall, I think this paper addresses an important issue of performance metrics in auto-formulation through an innovative approach.\nHowever, I do have some questions in terms of the clarity of the definitions, the experimental results, and the implementation details."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* I do agree that there are flaws in checking optimal values to determine the correctness of problem formulation. To my knowledge, this paper is the first to address this important issue.\n* The authors attempt to rigorously define the model equivalence and various related concepts.\n  * Some definitions need clarification, though. Please see my comments in \"Weaknesses\".\n* The authors constructed a new benchmark dataset to check formulation correctness based on the proposed model equivalence metric.\n  * It would be great if the authors can make the new benchmark publicly available."}, "weaknesses": {"value": "**Soundness and Clarity of Various Definitions.** The authors made several definitions in terms of model $\\mathcal{M} (`Definition 2`), model isomorphism (`Definition 5`), and graph (`Definition 7`). It is important to make these definitions consistent and coherent. Below are some comments and questions.\n* It is not clear what is the exact definition of *formulation correctness*, namely the equivalence between the ground-truth formulation and the formulation given by a LLM. If I understand correctly, a \"model\" is represented by the underlying graphs $\\mathbf{G}=(\\mathbf{V} \\cup \\mathbf{W}, \\mathbf{E})$ (`Definition 7`), and the \"problem data\" consists of the weights of edges $\\mathbf{A}$ and features of vertices $(\\mathbf{b}, \\mathbf{c}, \\circ, \\tau)$. In which of the following scenarios do we say the LLM \"correctly formulates\" the problem?\n  * The \"model\", represented the graph $\\mathbf{G}$, is correct;\n  * *Both* the \"model\" $\\mathbf{G}$ *and* the \"problem data\" $(\\mathbf{A}, \\mathbf{b}, \\mathbf{c}, \\circ, \\tau)$ are correct; or\n  * The \"model\" $\\mathbf{G}$ is correct, and *any randomly sampled* \"problem data\" is correct (this seems to be consistent with `Definition 5`).\n* I think it is also important to clarify what happens when $A_{ij}=0$ in the problem description (e.g., a constraint like $2 x_1 + 4 x_3 \\leq 0$, where $x_2$ is not involved). What is the underlying graph? Is it a graph with *no edge* between $i$ and $j$, or a graph with a *zero-weight edge* between $i$ and $j$? If both graphs are eligible, are they isomorphic?\n* The algebraic definition of the \"model\" and \"problem data\"/\"modeling parameter\" in `Definition 2` may not be consistent with the graphical definition in `Definition 7`. In `Definition 2`, the problem data seems to be any tuple $\\theta=(\\mathbf{A}, \\mathbf{b}, \\mathbf{c}, \\circ, \\tau)$ even with different dimensions $m,n,p$ (as illustrated in `Figure 8`). So different problem data can result in different graphs (i.e., different numbers of nodes and topologies). Then *all* MILPs can be represented by *one* model $\\mathcal{M}$ with different $\\theta$. This seems unreasonable to me.\n* In `Definition 3` of model-lossless-reduction, what are the restrictions of the mapping $F$? Does the same mapping $F$ exist for *any* parameter $\\theta$? Could you please provide an example?\n\n**Algorithm and Implementation.** From `Definition 5` and `Definition 6`, it seems that we mainly need to check if the LLM formulation is the same as the ground-truth formulation, subject to *permutations of variables and constraints*. \n* If it is just permutations of variables and constraints, are there easier methods than checking graph isomorphism?\n  * For example, [Astorga et al., 2025](https://openreview.net/forum?id=33YrT1j0O0) used *Satisfiability Modulo Theories* (SMT) to check equivalence of expressions, which goes beyond permutation equivalence.\n* In implementation, how do we get the underlying graph of the LLM formulation? Did you consider the errors in translating the formulation to graphs?\n\n**Experimental Results.** I cannot find any explanations on `Table 2` and `Table 3`. \n* It is not clear what are \"feasibility consistency\", \"ORGEval consistency\", \"solver consistency\" in `Table 2`.\n* It is not clear what are \"compile error\", \"structured\", and \"unstructured\" in `Table 3`."}, "questions": {"value": "Please see my comments in \"Weaknesses\".\n\nAdditional question:\n* What are the prompts in `Figure 4` and `Figure 5`? They contain the problem description, but also give away the answers (i.e., parameters, decision variables, objectives, constraints).\n\nMinor typos:\n1. Line 307: \"see figure Figure 3\".\n2. Line 401: \"Specifically Each Bench4Opt problem\".\n3. Line 405: \"algorithm Algorithm 3\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FDcCZcf1B3", "forum": "YNIrVSI0hp", "replyto": "YNIrVSI0hp", "signatures": ["ICLR.cc/2026/Conference/Submission24725/Reviewer_eNh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24725/Reviewer_eNh4"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762287515670, "cdate": 1762287515670, "tmdate": 1762943175335, "mdate": 1762943175335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}