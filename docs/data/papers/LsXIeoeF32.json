{"id": "LsXIeoeF32", "number": 20416, "cdate": 1758305831734, "mdate": 1759896978794, "content": {"title": "Ubiquity of Hebbian Dynamics in Complex Learning Rules", "abstract": "Hebbian learning is widely regarded as a core principle of synaptic plasticity in the brain, yet it appears fundamentally different from Stochastic Gradient Descent (SGD), the optimization method that underlies nearly all modern artificial intelligence. We find that when a neural network is regularized with L2 weight decay, the gradient of the loss aligns with the direction of the Hebbian learning signal as it approaches stationarity. Surprisingly, this Hebbian-like behavior is not unique to SGD: almost any learning rule, including random ones, can exhibit the same signature long before learning has ceased. We also provide a theoretical explanation for anti-Hebbian plasticity in regression tasks, showing how it can arise naturally from gradient or input noise, offering a potential mechanism for anti-Hebbian observations in the brain. These results may help bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles. Although there is evidence that the brain employs dedicated Hebbian mechanisms, much of the Hebbian-like plasticity observed may instead arise from more general optimization processes. From a neuroscience perspective, this suggests that Hebbian plasticity could be less central to learning than traditionally assumed. For machine learning, it highlights a surprising universality across algorithms that could inspire more efficient learning methods.", "tldr": "Hebbian learning emerges from L2 regularized learning rules as they approach stationarity (though before learning has ceased)", "keywords": ["Hebbian Learning", "Machine learning", "Neuroscience", "Learning Dynamics"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d25261e57e61c4563f911698c38ce19597b7c38.pdf", "supplementary_material": "/attachment/1f74be7c42cdc5ccbc3cbc6f7a9dc235418c9899.zip"}, "replies": [{"content": {"summary": {"value": "There is wide empirical evidence for Hebbian learning in the brain. This paper challenges this by showing that strong correlation with Hebbian learning can be achieved by many learning rules that have, in principle, nothing to do with Hebbian learning, as long as they are coupled with weight decay. Additionally, it shows noise on the parameters makes the learning signal more anti-Hebbian. The claims are supported by heuristic theoretical derivations at stationarity as well as experiments on relatively small neural networks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The arguments presented in the paper are simple yet powerful, and raise significant questions regarding our understanding of synaptic plasticity in the brain. The theoretical arguments provide solid intuition that is shown to hold in practice through careful experimentation. The discussion related to neuroscience is well done, and I had the pleasant surprise of finding all the questions the paper raised during my first read carefully addressed."}, "weaknesses": {"value": "What follows should be considered as suggestions rather than weaknesses:\n\n- **Figure 4 could be moved to the appendix.** The results it shows are very intuitive (as mentioned in the main text), and the setup differs from the rest of the paper, requiring considerable time to understand what is happening. As a result, reading this figure alone may bring more confusion than clarity.\n- **The analysis in Section 4 could be made more precise**. The setup considered in this section is simple enough that closed-form solutions of the learning dynamics can be derived (see e.g., Saxe et al. 2013), allowing the evolution of alignment with the Hebbian update to be precisely characterized. This could help better understand the alignment dynamics.\n- **The connection to empirical evidence could be strengthened.** The paper shows that traces of Hebbian learning do not necessarily imply that the underlying learning rule is Hebbian, without directly commenting on whether current empirical evidence suffers from this problem. The following experiment would help: reproduce the methodology of empirical neuroscience papers establishing the existence of Hebbian learning in the brain that the authors cite, and show that the same results can be achieved by a version of SGD. I suspect this would be possible given the current results of the paper, and having this experiment would help make the argument that current evidence is insufficient to conclude that brains learn with Hebbian learning."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "erSZZ563v2", "forum": "LsXIeoeF32", "replyto": "LsXIeoeF32", "signatures": ["ICLR.cc/2026/Conference/Submission20416/Reviewer_td9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20416/Reviewer_td9e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509174917, "cdate": 1761509174917, "tmdate": 1762933856839, "mdate": 1762933856839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show that in the presence of weight decay, the \"learning signal\" caused by SGD will tend to align with Hebbian updates (input time output), whereas in the presence of high weight noise it will instead tend to align with anti-Hebbian updates.\n\nSeveral experiments confirm the intuitive results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The general approach of predicting which conditions will align learning with Hebbian or anti-Hebbian updates seems novel, to my knowledge.  The experiments, if correct, seem to confirm the proposals.\n\nSome of the results are genuinely unexpected, e.g. the neat quadratic boundary shown in Figure 5."}, "weaknesses": {"value": "The main problem is that the paper consistently presents itself as an explanation of *why* Hebbian/anti-Hebbian learning occurs, while being based entirely on homeostatic mechanisms and completely ignoring functional outcomes of Hebbian learning.\n\nFor example, Hebbian learning is a powerful pattern learner, and anti-Hebbian learning enforces decorrelation (a famous example of both together is Foldiak 1991 https://pubmed.ncbi.nlm.nih.gov/2291903/ ).\n\nSince (as the authors acknowledge) Hebbian updates will tend to be aligned with the weight vectors (at least after a certain amount of learning) and thus increase W magnitude, then keeping weights stationary in the face of weight decay basically requires some kind of alignment with Hebbian updates. \n\nTo make this obvious homeostatic necessity a general principle, you first need to assume that something like uniform weight decay occurs in the brain, which is not at all obvious (e.g. I understand daily turnover of synapses tends to erase weak synapses but preserve strong ones). But even if it did, it would not follow that this should explain all, or even most, of Hebbian/anti-Hebbian learning in the brain, which the paper basically hints at repeatedly.\n\nSimilarly, observing anti-Hebbian updates in the presence of noise would not exactly confirm the theory, since this is precisely the expected outcome of several well-known Hebbian learning rules, including BCM and most STDP rules (which involve larer negative than positive windows). The reason for this is precisely to degrade synapses between neurons that fire uncorrelatedly.\n\nThus, while some intriguing results are reported, the paper needs to be significantly \"toned down\" and clarify the reach of its proposals."}, "questions": {"value": "- Eq.4: please add some more parentheses, or spaces - it's not immediately clear what is included in the gradient sign (I think it is just the gradient of l, but please clarify)\n\n- Please expand the maths  between eq 4 and eq 5 (maybe in an appendix). How do you get from 4 to 5 ? Why did the h_a become a second h_b? Why are there traces involved?\n\n- In equation 9, what exactly is theta, and how does it differ from W? Why \"cosine similarity between the learning rule and the Hebbian rule\" is somehow cos Theta? And, again, where do the traces come from?\n\n- Please explain Eq 17 a bit more. What are c0 and c1, and are they really \"constants\"?\n\n- Where's the data to support the next-to-last paragraph in p. 8 (line 420-424)?\n\n- In Figure 7, what does \"Init: 0.5x\", etc. mean?\n\n- I note that the Hebbian update is simply the gradient of y^2, so the initial bump of Hebbian-ness in Figure 7 may be caused by the need to increase response outputs, regardless of overall W magnitude. Perhaps plotting y (output) magnitude would be useful.\n\n- Minor: fix the parentheses in line 131."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d97gYDGiTF", "forum": "LsXIeoeF32", "replyto": "LsXIeoeF32", "signatures": ["ICLR.cc/2026/Conference/Submission20416/Reviewer_7Qh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20416/Reviewer_7Qh1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780397098, "cdate": 1761780397098, "tmdate": 1762933856277, "mdate": 1762933856277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that Hebbian and anti-Hebbian plasticity emerge universally from gradient-based optimization methods when combined with L2 weight decay or noise. The authors derive analytic results showing that learning rules with L2 regularization align with Hebbian updates near stationarity, while stochastic noise induces anti-Hebbian alignment. They support their theory with experiments on small MLPs and Transformers, suggesting that observed Hebbian/anti-Hebbian plasticity in the brain might be a signature of general optimization processes."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and structured.\n- The attempt to bridge biological learning and gradient-based optimization is an important unsolved question.\n- The experiments provide systematic and broad qualitative confirmation of the claimed effects."}, "weaknesses": {"value": "The following weaknesses highlight fundamental conceptual and theoretical issues that undermine the relevance of the paper’s main claims, distinguishing between superficial equilibrium effects and genuine learning dynamics.\n- The paper equates the norm-stabilizing effect of L2 regularization with Hebbian dynamics. L2 weight decay enforces contraction of weight norms, while Hebbian learning refers to directional correlation between pre- and post-synaptic activity (e.g., feature extraction). The alignment the authors observe is a trivial consequence of regularization equilibrium (($\\nabla_\\theta \\ell + \\gamma W = 0$)), not evidence of Hebbian computation.\n- Classical linear Hebbian rules, when combined with weight normalization or decay, are mathematically equivalent to stochastic gradient descent on the PCA objective (Oja, 1982). This connection is well established and forms the foundation of Hebbian learning theory. The authors fail to acknowledge or engage with this known property and instead conflate PCA-type Hebbian learning with the L2 norm stabilization. \n- Hebbian learning, in particular nonlinear forms, implements unsupervised feature learning objectives such as PCA, ICA, or sparse coding (e.g. Oja 1982; Oja 1991; Clopath et al. 2010; Zylberberg et al. 2011). These rules learn statistical structure beyond norm stability. The paper’s framing ignores this and instead treats any gradient-norm correlation as Hebbian, which overlooks decades of theoretical and experimental work.\n- The claim that anti-Hebbian alignment results from noise or that Hebbian signatures imply hidden global optimization lacks quantitative or mechanistic justification. Real synaptic plasticity involves nonlinear, spike- or voltage-dependent mechanisms (e.g., BCM, triplet STDP, Clopath rules) absent from this discussion.\n- The paper ignores key models showing anti-Hebbian plasticity as a structured, biologically grounded mechanism for decorrelation and stability, not a noise artifact (Vogels et al. 2011; King et al. 2013).\n- The manuscript overlooks nonlinear Hebbian learning frameworks that unify Oja’s, BCM, and triplet-STDP rules, as well as recent work linking these to ICA and sparse coding. These studies demonstrate that Hebbian-like rules do much more than stabilize norms, by extracting higher-order structure from data. The omission leads to incorrect generalization about Hebbian dynamics."}, "questions": {"value": "1) Can the authors clarify whether their “Hebbian alignment” has any relationship to the PCA or ICA objectives known to be optimized by Hebbian-like rules? If not, how is it meaningful beyond norm equilibrium?\n2) How does the proposed theory differentiate between trivial weight-norm alignment and genuine correlational structure learning? Would linear networks on whitened data still exhibit the claimed effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rr84PrH3Ao", "forum": "LsXIeoeF32", "replyto": "LsXIeoeF32", "signatures": ["ICLR.cc/2026/Conference/Submission20416/Reviewer_GwC8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20416/Reviewer_GwC8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941452499, "cdate": 1761941452499, "tmdate": 1762933855681, "mdate": 1762933855681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores under what conditions Hebbian and anti-Hebbian learning signal emerges from gradient-based optimization rules. They thereotically and experimentally find that using L2 weight decay, a broad class of learning rules exhibit Hebbian-like alignment of the learning signal near stationarity. With sufficiently strong noise, the alignment flips to anti-Hebbian. However, experiments on more broad class of neural network architecture are missing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A mathematical framework  shows why any learning rule with weight decay should exhibit Hebbian-like alignment at stationarity\n- In linear settings (and empirically in nonlinear ones) the work shows that sufficiently large parameter/gradient noise reverses alignment."}, "weaknesses": {"value": "* The paper claims that Hebbian/anti-Hebbian signals may be auxiliary emergent phenomena of complex learning rules. However, this claim is too strong. The presented theory and experiments more naturally support a stronger statement: Hebbian learning signals can arise from gradient-based rules under L2 weight decay regularization. Or, Hebbian-like updates can be implemented by multiple learning paradigms.\n\n* The current validation is limited to small networks. The paper should include larger-scale models (e.g., ResNet, compact LLMs) and analyze how depth, initialization, activation functions, or others shape Hebbian versus anti-Hebbian orientation. Demonstrating the effect across a broader model family would substantially strengthen the conclusions.\n\n* The paper should investigate the trade-off between the L2 regularization coefficient and task performance, and quantify how “more Hebbian-like” alignment relates to accuracy/generalization."}, "questions": {"value": "* The theoretical analysis focuses on L2 weight decay. Although L1 and Dropout are mentioned, the manuscript should make explicit what learning directions these regularizers induce and how they compare to L2 in driving Hebbian or anti-Hebbian tendencies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p5NFFM8AK9", "forum": "LsXIeoeF32", "replyto": "LsXIeoeF32", "signatures": ["ICLR.cc/2026/Conference/Submission20416/Reviewer_L1Cy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20416/Reviewer_L1Cy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966690071, "cdate": 1761966690071, "tmdate": 1762933855105, "mdate": 1762933855105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the idea that \n phenomenologically Hebbian and anti-Hebbian plasticity can emerge as byproduct of much more general learning rules (gradient based) that include weight decay (a contractive force) and/or noise (an expansive force). \n\n\nThe authors postulate that gradient descent training with weight decay (especially with high weight decay values) results into weight updates that are similar to those happening under purely Hebbian learning.\nThey offer a clear argument that since weight decay is contractive, the learned gradient part of the update must be expansive on average, and an expansive, rank-1-looking update will look Hebbian. That is an interesting explanation for why Hebbian-looking updates can show up during learning with weight decay even when the underlying algorithm is not Hebbian. They find that  stronger weight decay, larger learning rate, and larger batch size\nlead to better alignment between gradient-based and Hebbian weight updates.\nMoreover they find that strong noise in learning results in a learning signal that is anti-Hebbian. They mention that when noise co-exists with weight decay there is a competition between the two forces that contribute Hebbian and anti-Hebbian aligned updates, and they identify a “phase transition” when the interplay between these two forces changes polarity. \n\nIn general the authors try to push the argument that Hebbian and anti-Hebbian plasticity might be a byproduct or components of a more general gradient-like weight optimisation. While I find it an interesting argument, I find their evidence and argumentation not strong enough to support this argument, while their theory mostly holds for stationary states. This argumentation requires stronger biological anchoring and a clearer mapping from the employed weight decay to biological decay processes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors have performed an extensive number of numerical experiments where they explore the alignment of the weight updated with the Hebbian weight updates for different parameter values of weight decay, noise amplitude, network sparsity, network size, learning rate, batch size, and for different learning rules (namely Adam, stochastic gradient descent, direct feedback alignment, and randomNN) and regularisers.\n- The authors explicitly demonstrate that is the gradient part of the weight update that aligns with the Hebbian updates and not the full weight update, and thus there is no issue with weights growing infinitely large (since there is also a weight decay)\n- The authors propose an interesting theory: gradients under realistic constraints project onto Hebbian-looking directions, and thus observing Hebbian plasticity in experiments does not rule out gradient-like learning."}, "weaknesses": {"value": "- I feel the assumption of stationarity  does the heavy lifting in the arguments of the paper, and there is no concrete proof or evidence whether this Hebbian alignment arguments hold also in the out-of equilibrium state.\n- the title of the paper overstates the finding, since in essence this holds only under the assumption of stationarity and weak coupling.\n- No statistical reporting in many of the figures, the results seem like single runs (or the authors omit to mention over how many realisations they average)."}, "questions": {"value": "# Questions\n\n\n- In figure 15 you are trying to show that the alignment persists all over training, however isn’t the Hebbian alignment of the learning signal a bit too weak throughout this experiment? \n- What do the blue and red lines in Figure 3 left indicate? Is it Layer 1 and Layer 2 weight update alignment with Hebbian? Please put a legend key.\n- What experiments do you think one could perform to validate your proposal?  if Hebbian signatures can be a by-product of many learning rules, which empirical measurement/experiment would falsify/validate the argument?\n- You show that any update rule with decay will, on average, look Hebbian. Does that mean that much of the experimental evidence for Hebbian STDP could be reinterpreted as \"we only ever observed the projected, stationary part of a richer gradient estimator\"?\n- You identified a Hebbian - anti-Hebbian transition controlled by (noise, weight decay). Can this be re-read as having Hebbian-like updates when the gradient estimation is clean, and anti-Hebbian when the estimation is noisy, i.e. interpret the boundary as a quality-of-gradient axis?\n- In [1]  the authors mention heterosynaptic pathways as ways to direct gradient information. In the paper you show that heterosynaptic  rules also become Hebbian under decay. Does that mean these anatomical pathways could be there mainly to improve the pre-projection gradient, with Hebbianity just the surface readout?\n- In Figure 16 caption the authors mention that batch normalisation seems to have anti-Hebbian effect, however as I understand the plot the effect is very small (alignment value <-0.1). Can you comment on how you support this statement?\n\n\n\n### Typos and other comments:\n\n- Line 86: Eq. equation\n- In eq. 4 what is $\\ell$?\n- Line 174: “with” is missing\n\n### Papers to be cited\n\n[1] Richards, Blake Aaron, and Konrad Paul Kording. \"The study of plasticity has always been about gradients.\" The Journal of Physiology 601.15 (2023): 3141-3149."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ffX2xDYmFA", "forum": "LsXIeoeF32", "replyto": "LsXIeoeF32", "signatures": ["ICLR.cc/2026/Conference/Submission20416/Reviewer_etsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20416/Reviewer_etsD"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990432483, "cdate": 1761990432483, "tmdate": 1762933854828, "mdate": 1762933854828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}