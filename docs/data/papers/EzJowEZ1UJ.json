{"id": "EzJowEZ1UJ", "number": 6863, "cdate": 1757998849310, "mdate": 1763456304352, "content": {"title": "SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation", "abstract": "Generating hand grasps with language instructions is a widely studied topic that benefits from embodied AI and VR/AR applications. While transferring into **H**and **A**rticulated **O**bject **I**nteraction (**HAOI**), the hand grasps synthesis requires not only object functionality but also long-term manipulation sequence along the object deformation. This paper proposes a novel HAOI sequence generation framework **SynHLMA**, to **Syn**thesize **H**and **L**anguage **M**anipulation for **A**rticulated objects. Given a complete point cloud of an articulated object, we utilize a discrete HAOI representation to model each hand-object interaction frame. Along with the natural language embeddings, the representations are trained by an HAOI Manipulation Language Model to align the grasping process with its language description in a shared representation space. An **articulation-aware loss** is employed to ensure hand grasps follow the dynamic variations of articulated object joints. In this way, our SynHLMA achieves three typical hand manipulation tasks for articulated objects: HAOI generation, HAOI prediction, and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset, and experimental results demonstrate the superior hand grasp sequence generation performance compared with state-of-the-art methods. We also show a robotics grasp application that enables dexterous grasp execution from imitation learning using the manipulation sequence provided by our SynHLMA. Our codes and datasets will be made publicly available.", "tldr": "We construct a large-scale high-quality hand-object interaction dataset with a physics engine and propose a multi-modal VQ-VAE-based generative model for articulated object grasp synthesis.", "keywords": ["Grasp Synthesis", "Articulated Object", "Multi-Modal Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44d1caa76d6656985892f453864993dd456956e8.pdf", "supplementary_material": "/attachment/34cf7914d1c8262de493559725d78ec7a6b50589.zip"}, "replies": [{"content": {"summary": {"value": "This work presents SynHLMA, a novel framework for synthesizing Hand Articulated Object Interaction (HAOI) sequences from natural language instructions. A key contribution is the HAOI-Lang dataset, a new, large-scale benchmark of physically plausible manipulation sequences generated via reinforcement learning and paired with rich language annotations. The proposed method models the problem by first leveraging a multi-stage VQ-VAE to discretize the continuous, long-term HAOI trajectories into hierarchical semantic tokens. These discrete representations are then aligned with language embeddings in a shared semantic space using an HAOI Manipulation Language Model. A novel articulation-aware loss is introduced to ensure the generated hand motions are consistent with the object's joint dynamics. Experimental results demonstrate that SynHLMA achieves state-of-the-art performance on HAOI generation, prediction, and interpolation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "This paper has the following strengths:\n\nA significant contribution is the construction of the HAOI-Lang dataset, a new benchmark for Human Articulated Object Interactions. This dataset was generated using reinforcement learning in a physics simulator and is comprehensive, featuring 256 instances with over 500,000 static grasps and 50,000 manipulation sequences. This is helpful for the community.\n\nThe paper successfully applies a multi-stage VQ-VAE framework to the HAOI task. This approach effectively discretizes the continuous interactions, enabling the generation of corresponding HAOI motion sequences directly from human language instructions.\n\nThe proposed method, SynHLMA, demonstrates state-of-the-art (SOTA) performance on the challenging HAOI generation task, outperforming existing baselines.\n\nThe strategy of tokenizing the interaction data and processing it with a Large Language Model (LLM) is effective. It successfully fuses natural language semantics with the LLM's priors and is shown to be beneficial for generating diverse outputs."}, "weaknesses": {"value": "The paper has several weaknesses that should be addressed:\n\nPresentation Quality: The overall presentation quality is lacking. There are visual issues, such as formatting (e.g., the title of Section 4.3) and poor figure design. For instance, Figure 2 is overly crowded, with elements like the 'Co' label being obscured, despite large amounts of unused white space on the figure's flanks. Furthermore, the writing could be improved; Section 3.3, for example, lacks a clear narrative structure, making the methodology difficult to follow.\n\nLimited Novelty: The paper's core methodology, which relies on a multi-stage VQ-VAE, appears to have limited novelty. This framework has been established in prior work. The current paper seems to be a direct application of this existing framework to the HAOI generation task, with insufficient task-specific innovation.\n\nInsufficient Qualitative Results: The evaluation of the generated results is not comprehensive. Without a video supplement, it is difficult to intuitively assess the quality, fluency, and physical plausibility of the generated motions. Moreover, while the dataset contains 256 instances, the paper and appendix only provide detailed and dense sequence visualizations for a very small subset (only 4 objects in the appendix). This limited sample size is insufficient to fully demonstrate the model's true performance and generalization capabilities."}, "questions": {"value": "Generalization: How well does the model generalize to new scenarios? Specifically, can it handle unseen object instances or entirely new object categories? Can it perform novel manipulation tasks (e.g., actions not present in the training data)? Furthermore, how robust is its generalization to language instructions that specify novel contact locations?\n\nMotion Consistency and Continuity: Do the generated manipulation sequences maintain good temporal consistency and continuity? For example, do the contact points drift unreasonably during the operation? Does the hand pose (gesture) change illogically, or are the generated trajectories unnatural or jittery? It would be beneficial if the authors could provide and analyze some typical failure cases.\n\nPhysical Plausibility and Real-World Transfer: Are the generated motions physically plausible? The paper shows a simulation transfer, but can these generated sequences be successfully and directly applied to a physical, real-world robotic hand? What would be the main challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yoRNZFKrZ3", "forum": "EzJowEZ1UJ", "replyto": "EzJowEZ1UJ", "signatures": ["ICLR.cc/2026/Conference/Submission6863/Reviewer_ENDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6863/Reviewer_ENDg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829020519, "cdate": 1761829020519, "tmdate": 1762919117103, "mdate": 1762919117103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SynHLMA, a new framework for synthesizing hand-articulated object manipulation sequences from natural language instructions. The method discretizes each frame of a hand-object interaction into hierarchical VQ-VAE tokens, encoding global hand pose, local articulation, refinement, and joint state, and trains a manipulation language model (Vicuna-7B with LoRA) to generate, predict, and interpolate Human Articulated Object Interactions (HAOI). A new dataset, HAOI-Lang, is built using physics simulation (RaiSim) and GPT-4 language annotations, comprising 50k manipulation sequences over seven object categories. Experiments show clear gains over HOIGPT and Text2HOI in FID, diversity, and ADE across three tasks (generation, prediction, interpolation), and qualitative transfer to the ShadowHand robot demonstrates physical plausibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Solid methodology integrating VQ-VAE and LoRA-tuned Vicuna. Consistent improvements on benchmarks.\n\n2. Dataset Contribution: HAOI-Lang offers valuable large-scale multimodal data with physics-consistent interactions and generated instructions.\n\n3. Generality: Demonstrated applicability across generation, prediction, and interpolation tasks. Showed extension to robotic dexterity transfer.\n\n4. Clarity: Good pipeline and ablation studies on design choices (token hierarchy, VQ-VAE size, LoRA rank)."}, "weaknesses": {"value": "1. The dataset is simulation-based, and language annotations are GPT-generated, which may limit transfer to real-world hand motions or linguistic diversity.\n\n2. Robotic transfer is only qualitative within a simulator; no human demonstration or physical validation.\n\n3. Comparative Scope: Comparisons are mainly against HOIGPT/Text2HOI; no baselines using diffusion or transformer-based generative models (e.g., AffordanceDiffusion, HOIDiffusion, NL2Contact).\n\n4. Ablation Breadth: Although token-level ablations are detailed, no explicit study isolates the effect of articulation-aware loss or discrete vs. continuous representations.\n\n5. Dataset Documentation: bias considerations of using GPT-4-generated text (e.g., annotation consistency, cultural phrasing) are not discussed."}, "questions": {"value": "1. How sensitive is performance to the choice of Vicuna-7B vs. other LLMs (e.g., Qwen2 or LLaVA)?\n\n2. Can the articulation-aware loss generalize to multi-joint or compound-joint objects beyond single revolute/prismatic types?\n\n3. How does SynHLMA perform when applied to real RGB-D or point-cloud sequences captured from human manipulation instead of simulation?\n\n4. Could discrete HAOI tokens be shared across object categories to enable few-shot generalization?\n\n5. What are the failure modes in complex or bimanual manipulations (e.g., eyeglass folding, cabinet doors with two handles)?\n\n6. Are there plans to release the dataset with verified physical realism or human-validated captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qhRLObLesS", "forum": "EzJowEZ1UJ", "replyto": "EzJowEZ1UJ", "signatures": ["ICLR.cc/2026/Conference/Submission6863/Reviewer_nYnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6863/Reviewer_nYnu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981545259, "cdate": 1761981545259, "tmdate": 1762919116232, "mdate": 1762919116232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SynHLMA, a language-conditioned framework for human–articulated-object interaction. Each frame is discretized into hierarchical tokens via a multi-stage VQ-VAE, and a joint-aware loss enforces articulation consistency. A manipulation language model aligns the tokenized sequences with natural-language instructions, enabling long-horizon generation, prediction, and interpolation of hand–object trajectories. The authors also release HAOI-Lang, a large simulated dataset with GPT-annotated language, and report consistent improvements over strong HOI/motion baselines on FID, diversity, ADE/FDE, and related metrics, complemented by qualitative results. Finally, they demonstrate transfer to dexterous robotics by fitting synthesized MANO trajectories to a ShadowHand for imitation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The dynamic modeling of hand interactions with articulated objects is well motivated and encodes both semantic intent and articulation constraints in a coherent formulation.\n2. The curated dataset is likely to be useful for the community and may enable controlled studies of language-conditioned articulated manipulation.\n3. The experimental evaluation in simulation is reasonably comprehensive, covering generation, prediction, and interpolation with quantitative and qualitative evidence."}, "weaknesses": {"value": "1. Rendering-to-GPT caption pipeline. The paper relies on rendering sequences in Open3D and obtaining descriptions with GPT-4. The realism of Open3D renderings is limited, which may introduce a domain gap for image-to-text captioning and, in turn, for language supervision quality.\n2. Assumption on fixed object base. It is unclear whether the method supports scenarios in which the articulated object’s base moves in the world. The token index \\<j\\> is defined in the object’s canonical space, which may implicitly assume a fixed base (e.g., a laptop fixed on a table), potentially excluding sequences like “pick up the laptop, then close the lid.”\n3. Comparative analysis with SemGrasp. Although SemGrasp targets single-step semantic grasping rather than long-horizon manipulation, a comparison or discussion would help position the contribution relative to semantic grasp baselines."}, "questions": {"value": "The following questions are based on the weaknesses discussed above; please refer to that section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p05PaDvmOz", "forum": "EzJowEZ1UJ", "replyto": "EzJowEZ1UJ", "signatures": ["ICLR.cc/2026/Conference/Submission6863/Reviewer_9XZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6863/Reviewer_9XZF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997119119, "cdate": 1761997119119, "tmdate": 1762919115903, "mdate": 1762919115903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose SynHLMA, a framework for generating hand–articulated-object interaction sequences from a point cloud and natural-language instruction. The method (i) discretizes each manipulation frame using a multi-stage VQ-VAE into tokens representing the global hand pose (⟨g⟩), local articulation (⟨l⟩), refinement (⟨r⟩), and object joint (⟨j⟩); (ii) trains a manipulation language model to autoregressively predict these tokens conditioned on textual input; and (iii) introduces an articulation-aware loss that combines hand–object penetration, pose-consistency, and joint-configuration terms. The model supports three tasks—generation, prediction, and interpolation. The authors also propose HAOI-Lang, a simulated dataset containing approximately 50,000 manipulation sequences paired with GPT-4–generated captions across seven object categories (stapler, laptop, scissors, cabinet, dishwasher, eyeglasses, and box). Experiments demonstrate state-of-the-art performance across six quantitative metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The author proposes a new human–articulated-object interaction framework by LoRA training an LLM.\n\n2. The author also proposes a new hand–articulated-object dataset that includes natural language descriptions."}, "weaknesses": {"value": "Weaknesses:\n\n1. Previous work such as HOIGPT follows a paradigm that is quite similar to the proposed approach. The paper should provide more detailed comparisons with these methods. In particular, it would be helpful to clarify whether those baselines were trained on the same dataset as SynHLMA to ensure fairness and reproducibility.\n\n2. In the ablation studies, please specify the version of LLaMA used. While the choice of LLaMA as a comparison baseline is reasonable, there are other state-of-the-art base models (e.g., Qwen, Gemma) that have shown superior ability in different aspects. Including results for these models would make the evaluation more comprehensive and up-to-date.\n\nMinor Comments:\n\nLine 087 — a space is missing before “By”.\n\nFigures 2 and 3 are cluttered and difficult to read. Simplifying these figures and highlighting the key modules would make the workflow easier to understand.\n\nPlease clarify whether Figure 2 corresponds to the model trained with LLaMA or Vicuna.\n\nLine 418 — the LaTeX equation formatting is broken and should be fixed for readability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NCBY1LEJma", "forum": "EzJowEZ1UJ", "replyto": "EzJowEZ1UJ", "signatures": ["ICLR.cc/2026/Conference/Submission6863/Reviewer_V4CR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6863/Reviewer_V4CR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762734533799, "cdate": 1762734533799, "tmdate": 1762919115532, "mdate": 1762919115532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}