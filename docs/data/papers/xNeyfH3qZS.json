{"id": "xNeyfH3qZS", "number": 15621, "cdate": 1758253269960, "mdate": 1763506635987, "content": {"title": "Compute Where It Counts: Adaptive Compute Allocation for Large Language Models via Learned Granular Sparsity", "abstract": "Sparsity-aware inference can dramatically shrink computation requirements by reducing the number of parameters used in each forward pass. Existing methods tend to be heuristic (zeroing activations below fixed thresholds, retaining top-K activations etc). These methods do not directly optimize individual thresholds using gradient-based methods and experience sharp performance degradation beyond 50% sparsity. This paper describes CWIC (Compute Where it Counts), a method that makes sparsity thresholds learnable and contextual. CWIC encourages conditional computation that allows model to designate different levels of sparsity to different input and layers. We also introduce “granular sparsity\" that decomposes matrix columns into smaller \"stripes\" for more expressive sparsity patterns. CWIC and granular sparsity enable distilling 2-6x compute-efficient sparse models from Llama 3.2-1B and Llama 3.2-3B. Notably, CWIC models are found to allocate little compute to filler words or replicated text and more compute to questions humans deem challenging.", "tldr": "", "keywords": ["contextual sparsity", "learnable sparsity", "granular sparsity", "parameter-efficient-training", "LLM Efficiency", "hardness of samples", "sparse models", "efficient models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a1b7fe47f3682c64d8fdaba3cccf370b3310030.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "CWIC (Compute Where it Counts) introduces a novel method for training sparse LLMs by making sparsity thresholds learnable parameters. The key contributions are: (1) learned contextual thresholds that are optimized using STEs, allowing models to dynamically allocate different amounts of compute to different tokens and weight matrices, and (2) granular sparsity that partitions matrix columns into smaller \"stripes\" for more expressive sparsity patterns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I think the authors attempt to solve a really interesting research question of learning sparsity thresholds. I agree with the importance and limitations of existing works. I also found the idea around GMM quite interesting and intuitive. I found the \n\n1) A really interesting approach to a principled gradient-based threshold learning - to directly optimize sparsity thresholds rather than using heuristics.\n2) I quite liked the insightful analysis of learned sparsity patterns and discussion section. \n3) Strong empirical results on variety of different benchmarks across multiple datasets."}, "weaknesses": {"value": "Major Concerns\n\n1) I am not sure about the claim - *STE  improves performance by removing the variance imparted on the grads when the values of G fluctuate...*: Using STE seems overly aggressive for the use-case? I think there needs to be more justification around this choice. Supporting experiments to compare that this is as a winning choice might also be helpful. Right now, this seems to more like a empirical tuning based selection?\n- If this choice is derived from JumpReLU - their gradient estimator worked for SAE reconstruction doesn't automatically validate these additional modifications for multi-layer distillation. Current explanation is not sufficient for a convincing argument.\n\nIn general, I think section 3.3 is poorly written. Authors should re-word their ideas more clearly here. \n\n2) No analysis of why related kernel choice is better than alternatives?\n\n3) The psuedo-derivative bandwidth (lines 220-224): The interaction between adaptive bandwidth $(\\epsilon_i = \\alpha\\epsilon · std(x_i))$ and input whitening (Section 3.4) is unclear - is std computed on whitened or raw values? How does this affect gradient scales?\n- No comparison of adaptive vs. fixed bandwidth for pseudo-derivatives.\n\n4) Experimental concern: The distillation data includes benchmark training sets (MMLU, ARC, WinoGrande) repeated 5×, which may inflate evaluation scores (refer: appendix C I believe). I think reporting scores on benchmarks not in training data would be a stronger claim.\n\n5) Loss function: Authors need to explain how to interpret this. Right now, it looks asymmetrical: the loss only penalizes using too many parameters, not too few? In other words, it will keep on minimizing, i.e nothing stops the model from becoming 10× sparse if that happens to minimize distillation loss? It seems one would need careful tuning/warm starting to counter this or is there a implicit effect from distillation loss (or am I understanding this wrong)?\n\nI strongly think the presentation wrt to the key ideas (section 3.2-3.6) need to be improved to make the author's work more accessible and make a convincing argument towards a lot of different empirical choices. Additionally, I found the concatenation operator to be a bit confusing (line 180-181), maybe there is a better way to mathematically denote it?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kJgXUHewle", "forum": "xNeyfH3qZS", "replyto": "xNeyfH3qZS", "signatures": ["ICLR.cc/2026/Conference/Submission15621/Reviewer_oPbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15621/Reviewer_oPbm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958776289, "cdate": 1761958776289, "tmdate": 1762925886968, "mdate": 1762925886968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests making sparsity thresholds both learnable and context-aware, with increased granularity. Experiments show these methods outperform alternatives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiment results show the effectiveness of the method compared to alternatives in Pareto front trade-offs.\n2. The authors made commendable efforts to achieve practical benefits through kernel implementations.\n3. Several interesting insights emerged from the discussions."}, "weaknesses": {"value": "Regarding technical details, the complexity of the proposed method is notable. Based on my understanding of Section 3, it appears to involve many hyperparameters.\n\nThis feedback is not intended as a reason to reject, but rather suggestions for improvement:\n- The detailed discussion on \"stripes\" seems unnecessary. From a GEMM kernel perspective, this is simply (part of) a tile (?), and the proposed method essentially introduces sparsity within the tile level (or multiple tiles). This concept makes sense. I listed this in the weakness section, but it should be viewed as a recommendation for improvement. I suggest the authors emphasize this point. Similarly, from a hardware perspective, exploring 2D stripe sparsity, aligning stripes with the Tensor Core size, could be beneficial.\n\n- Regarding Line 203, does the post hoc threshold update affect differentiability? It might be better to parameterize the threshold directly in log space, ensuring it remains positive.\n\n- Section 3.3 seems poorly motivated. Have the authors considered common relaxations of binary variables, such as sigmoid or Gumbel-like methods?"}, "questions": {"value": "- Lines 45-46: \"quantization ... tends to be less expressive than sparsity techniques...\" Could the authors clarify this? I don't understand why quantization is considered less expressive, since I think of sparsity as a form of quantization to zero bits.\n\n- How would batching be implemented?\n\n- What are the conceptual differences between granular sparsity, N:M sparsity, and DeepSeek's Native Sparse Attention? (I understand they are used in different contexts, so I am asking about their conceptual similarities and differences).\n\n- Section 3.8: this refers to Jensen–Shannon divergence.\n\n- In the abstract, the method claims a wall clock speedup of 2.7x on GPU; is this compared to dense models (not TEAL)? Could the authors explain why their GPU implementation might be slower than TEAL? (Reasonable) speculations are okay with me.\n\n- If I understand correctly, TEAL is somewhat training-free. Could the authors elaborate on how much slower their method is compared to TEAL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "avMtJ79fBR", "forum": "xNeyfH3qZS", "replyto": "xNeyfH3qZS", "signatures": ["ICLR.cc/2026/Conference/Submission15621/Reviewer_dRcL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15621/Reviewer_dRcL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106606965, "cdate": 1762106606965, "tmdate": 1762925886635, "mdate": 1762925886635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method to improve LLM efficiency. Computere Where It Counts (CWIC) introduces a granular sparsity pattern in the column vector of the matrix, in contrast to prior sparse activation methods, which mask the entire column. \nThe central claim of the paper is that using a granular sparsity pattern or stripes can improve the performance of the sparse model. The method achieves this by learning the threshold value which is then used by the gating function to mask the column vectors. Experiments are provided to compare the model with equivalent (same number of active parameters) dense models and method yeild wall-clock speedup by implementing Triton kernel."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The use of granular sparsity is interesting and the ablation in 4.2.3 supports the idea of using granular sparsity. \n\n\n* The authors also implemented the Triton kernel for the proposed method, which makes the method easy to deploy for real-world efficiency gains; however, the implementation details or code are not provided.\n\n\n* Experiments in Table 1 support the claim that CWIC can match equivalent dense model performance; however, the comparison provided is limited.\n\n* Analysis on the sparsity pattern (in section 5) is interesting and could be useful for the community."}, "weaknesses": {"value": "* The methodology is not well-written and difficult to follow. \n\n* The method is only compared against TEAL, more  sparse activation baselines should be added. \n\n* The notations are not well-defined and method does not use the variables consistently. For e.g., in section 3.1, GMM functions takes $G$ as an argument but in section section 3.2, it takes $\\theta$. Are G and $\\theta$ same? \n\n* Similarly, H(z) is first defined with only one argument (line 191) but in the next line it takes two arguments (x and t).\n\n*  Minor: The statement about the scaling laws (section 4.2.4) is not convincing. It is possible that the CWIC may not follow the same trend as TEAL. \n\n* It is not clear why and how distillation was used (section 3.8)? How do you initialize the teacher model? Is it a pre-trained model?\n\n* More benchmark needs to be added to compare with other sparse activation baselines (table 1).\n\n* Table 1 only shows the comparision between sparse and equivalent dense model. Comparison with the base model should be added to show the drop in performance is not significant. \n\n* Comparison with structured sparsity/model pruning methods is missing and needs to be added to understand how CWIC is different from structured model pruning (n:m sparsity)."}, "questions": {"value": "1. The equation on page (section 3.1) does not seem to follow proper dimensions. Column vector v has $m$ dimensions but x has $n$ dimensions.\n\n2. Why do we need normalization (section 3.4) separately? LayerNorm and RMSnorm already normalize the hidden features.\n\nSuggestions: \n1. Equations should be numbered and labelled, at least the main ones.\n\n\nThe paper overall needs a bit of rewriting, along with additional experiments/baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WvMJrmP2CQ", "forum": "xNeyfH3qZS", "replyto": "xNeyfH3qZS", "signatures": ["ICLR.cc/2026/Conference/Submission15621/Reviewer_AM9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15621/Reviewer_AM9y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267738827, "cdate": 1762267738827, "tmdate": 1762925886189, "mdate": 1762925886189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CWIC, a sparsity-aware distillation method for large language models that learns activation thresholds in a context-dependent way and applies sparsity at a finer, stripe-level granularity inside linear layers. An explicit active-parameter (AP) loss is used to target a desired compute budget while distilling from dense Llama 3.2 models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated. It addresses a very practical problem: reducing LLM inference cost while maintaining quality, in a way that can be applied directly to real Llama models.\n\n- The core idea is clear: make sparsity thresholds learnable and context dependent, introduce stripe-level sparsity in the linear layers, and control everything with an explicit active-parameter budget.\n\n- The main empirical message is easy to understand: CWIC achieves a better accuracy–compute trade-off than TEAL (SOTA model) over a range of sparsity levels, and the resulting sparse models slightly outperform dense models with comparable active parameters."}, "weaknesses": {"value": "- The method requires substantially more training than TEAL, but the trade-off between extra training time and the observed gains is not analyzed in detail.\n\n- There is no discussion of sensitivity to the many hyperparameters involved in the method (for example the APR schedule, the weight on the AP loss, stripe size, and normalization choices).\n\n- Experiments are limited to Llama 3.2 1B and 3B models trained on roughly one billion tokens, so it is unclear how well the approach scales to larger models.\n\n- Although the method is trained in a teacher–student (distillation) setting, it is not compared to standard distillation baselines such as smaller dense students trained with the same loss and similar compute."}, "questions": {"value": "- How does the amount and composition of training data affect the benchmark scores and the accuracy–compute curves? For example, what happens if you train CWIC with more data, less data, or a different data mixture?\n\n- Have you compared CWIC with non-sparse distillation baselines, such as dense student models distilled from the same teacher under the same objective?\n\n- Can you quantify the training-time versus evaluation-quality trade-off relative to TEAL, which effectively incurs no additional training cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ms9Yr1gVaz", "forum": "xNeyfH3qZS", "replyto": "xNeyfH3qZS", "signatures": ["ICLR.cc/2026/Conference/Submission15621/Reviewer_6kNm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15621/Reviewer_6kNm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15621/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763376285649, "cdate": 1763376285649, "tmdate": 1763376285649, "mdate": 1763376285649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}