{"id": "jPqGHu174f", "number": 16315, "cdate": 1758263078348, "mdate": 1759897248002, "content": {"title": "Teaching Humans Subtle Differences with DIFFusion", "abstract": "Scientific expertise often requires recognizing subtle visual differences that remain challenging to articulate even for domain experts. We present a system that leverages generative models to automatically discover and visualize minimal discriminative features between categories while preserving instance identity. Our method generates counterfactual visualizations with subtle, targeted transformations between classes, performing well even in domains where data is sparse, examples are unpaired, and category boundaries resist verbal description. Experiments across six domains, including black hole simulations, butterfly taxonomy, and medical imaging, demonstrate accurate transitions with limited training data, highlighting both established discriminative features and novel subtle distinctions that measurably improved category differentiation. User studies confirm our generated counterfactuals significantly outperform traditional approaches in teaching people to correctly differentiate between fine-grained classes, showing the potential of generative models to advance human visual learning and scientific research.", "tldr": "", "keywords": ["counterfactual", "machine teaching", "image generation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bac66be12701776556211952a4a113f877dd2981.pdf", "supplementary_material": "/attachment/f98739b84872c55b38935a85c5d3c3cc8c58c320.pdf"}, "replies": [{"content": {"summary": {"value": "The paper presents DIFFusion, a method that leverages diffusion models to automatically discover and visualise minimal discriminative features between visual categories, to teach humans to recognise subtle differences. The approach is particularly tailored for domains where text descriptions are inadequate or unavailable, such as scientific imaging, black hole simulations and fine-grained species classification.\n\nDIFFusion inverts a real image into noise maps, manipulates the CLIP embedding by adding a class-difference vector and re-generates the image to produce identity-preserving counterfactuals that highlight only discriminative features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The introduced methodology combines existing techniques in a novel way while maintaining simplicity.\n\nThe results indicate that the method works well across six binary classification datasets spanning both scientific and regular domains. \n\n This work could have a meaningful impact on scientific education and training, particularly in domains where visual expertise is crucial but difficult to teach through traditional methods. \n\nThe paper includes thorough quantitative metrics, qualitative comparisons across multiple baselines, ablation studies on dataset size and human user studies demonstrating practical teaching effectiveness."}, "weaknesses": {"value": "The conditioning manipulation relies on simple arithmetic, which raises questions about whether this approach can capture more complex, multi-modal differences between classes or whether it may oversimplify the discriminative features.\n\nWhile the paper claims to discover features unknown to domain experts, the validation of these discoveries relies primarily on classifier flip rates rather than expert verification or physical/scientific validity. Additional expert validation would strengthen these claims.\n\nThe method is susceptible to dataset bias, causing unintended feature changes (e.g., background shifts in plant images), which limits precise control despite being useful for revealing biases."}, "questions": {"value": "Could you please elaborate on the core technical novelty of DIFFusion, distinguishing its contributions from existing state-of-the-art image editing and counterfactual generation methods?\n\nHave you explored more sophisticated methods for computing the class-difference vector beyond the simple difference of means?\n\nHow can this methodology be extended to take on multi-class problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TTErdCisRC", "forum": "jPqGHu174f", "replyto": "jPqGHu174f", "signatures": ["ICLR.cc/2026/Conference/Submission16315/Reviewer_THd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16315/Reviewer_THd6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760918697681, "cdate": 1760918697681, "tmdate": 1762926453350, "mdate": 1762926453350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a method to generate counterfactual images. They specialize in classes where text descriptions of discriminating features are not available, which is the case in many scientific fields. To do this, they encode a target image with CLIP, and perturb it by the average of the target class - the average of the original class. They then invert the original image and condition its re-generation on the perturbed CLIP embedding, resulting in an edit in the direction of the target class. \n\nThey evaluate this on two general and four scientific examples, to see that the model incorrectly classifies the perturbed image and it stays close to the original. They also evaluate if these images can be used to teach humans to understand differences. They include additional ablations on the dataset size and biases in the datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1) The problem statement has clear applications in real-world scenarios, specifically for scientific discovery.\n\nS2) The method is elegant.\n\nS3) Presented results look promising."}, "weaknesses": {"value": "My strongest concerns lie in the relationship to existing literature (see W1, W3, W4). If I were convinced that this work is well-situated within current work, and that the results are significant and baselines sufficiently encompass SOTA, I would view the work more favorably.\n\nW1) The work does not seem well-placed within the existing literature, specifically in the areas of Visual Counterfactual Explanations, Image Editing, Diffusion Models with Image Prompts, and Diffusion Inversion. As a particular example, the section Visual Counterfactual Explanations references less than 10 examples of counterfactual work--a field which is much larger (as evidenced in [A], which may provide some starting place).\n\nW2) The choice of text-to-image model is largely overlooked. Given the importance of the choice of model towards results, some discussion would greatly strengthen the reader's ability to understand the method's potentials and limitations (especially because it is only demonstrated on one model).\n\nW3) Given that each dataset only includes a single category pair, 6 examples seems very little towards understanding method performance (see Q1).\n\nW4) The number of SOTA comparison methods seems too few, especially given that they are spread over counterfactuals, text-driven editing, and vision-driven editing. E.g. TIME is the only counterfactual baseline, despite more being present in the related works (e.g. DiG-IN). As it stands, it is not clear if the presented methods fully encompass SOTA; comparing to more methods would improve the readers ability to compare this work to previous.\n\n\n[A] Melistas et al., Benchmarking Counterfactual Image Generation, NeurIPS 2024."}, "questions": {"value": "Q1) Is there justification for the dataset selections? (e.g. is it consistent with previous work)\n\nQ2) The oracle classifiers (ResNet-18, MobileNet-V2, EfficientNet-B0) seem at first glance seem like they may not be sufficiently accurate. It would be helpful if you could provide some justification for the use of these models, e.g. statistics on their accuracy, justification from previous literature, etc. Otherwise, it may be that stronger models available could remove noise from the inaccuracy of weaker models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cWwKcdkvvH", "forum": "jPqGHu174f", "replyto": "jPqGHu174f", "signatures": ["ICLR.cc/2026/Conference/Submission16315/Reviewer_MFDi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16315/Reviewer_MFDi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655081370, "cdate": 1761655081370, "tmdate": 1762926452984, "mdate": 1762926452984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a diffusion based pipeline to generate visual counterfactual explanations (VCEs) with the goal of teaching people to correctly differentiate between fine-grained classes.\n\nTheir pipeline consists of three steps: For a given image, they (i) extract the noise vectors using inversion, (ii) compute a direction in the embedding of a CLIP vision encoder using positive and negative class samples, (iii) generate an image based on the noise vectors and conditioned on the CLIP embedding which was shifted in the computed direction. Optionally, the diffusion model is fine-tuned for domain adaption.\n\nThe pipeline is evaluated on six class pairs from six different datasets and compared to several counterfactual methods which do not utilise a classifier. The reported metrics are the flip-rate and LPIPS distance between original and edited image. Further, the authors conducted a user study to evaluate the applicability of their method to teach humans the fine-grained category differences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents an interesting application of VCEs and provides evidence that their method produces useful visualizations for the presented examples. Further, their method has less computational cost compared to optimization methods for diffusion-based VCEs."}, "weaknesses": {"value": "The introduced method utilises the CLIP vision encoder indirectly as a few-shot classifier (as well as the oracle classifier according to App. A.4). This few-shot classifier could be used in other classifier-guided diffusion approaches to generate VCEs for comparison. Alternatively, one could train another classifier for that purpose on the same training examples. Nevertheless, the authors should consider to add comparisons to such classifier-guided approaches as they are comparable to their method and provide stronger baselines.\n\nAnother weakness is the limited selection of examples, i.e. one class-pair per dataset. While these show interesting use cases, a broader evaluation would be required to better access the performance of the proposed pipeline.\n\nIn the context of the goal to teach humans, an open question seems to be how to distinguish true visual differences from shortcuts or other failure modes of the CLIP embedding. Also, it seems not clear whether the resulting image being out-of-distribution. e.g. by showing an object which is a composition of partial features of both classes, could have problematic side-effects on the task."}, "questions": {"value": "1. How did the authors choose the selection of class pairs?\n\n2. Do the generated images actually actually belong to the target class and how is this process influenced by biases/failure modes of the i) CLIP envoder ii) oracle classifier?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qyG6UXsobj", "forum": "jPqGHu174f", "replyto": "jPqGHu174f", "signatures": ["ICLR.cc/2026/Conference/Submission16315/Reviewer_5Seu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16315/Reviewer_5Seu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995834905, "cdate": 1761995834905, "tmdate": 1762926452651, "mdate": 1762926452651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion model method that reveals subtle, discriminative visual features between fine-grained classes through minimal, identity-preserving counterfactual images. The method works by partially inverting the image and then denoising it with an adjusted textual guidance towards the target class. Its efficacy is tested on different domains, such as black holes, butterflies, and medical images, and perform well compared to previous approaches in teaching human experts to distinguish fine-grained classes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Counterfactual explanations are an important research direction and using them to teach humans about fine-grained details between classes is an very relevant application.\n- The method is simple, while at the same time the chosen qualitative results shows good results.\n- On the evaluated datasets, the proposed method performs on par or better on success rate. The user study is very interesting and shows the method's benefits in practice."}, "weaknesses": {"value": "- The technical novelty is limited.\n  * There have been previous methods that have proposed very similar approaches of renoising and denoising to obtain slight variations of the same image. DiME [A] uses a similar idea for counterfactual generation, and in data augmentation [B] the same ideas have been applied.\n  * The method combines simple primitives and previous methods and does not ablate any design choices, e.g., about the CLIP guidance or inversion process.\n\n- The scientific writing can be improved.\n  * The introduction does not sufficiently support the claims. For instance, the examples given in line 53, for \"In specialized scientific domains, the complete set of visual features distinguishing between categories may be partially or entirely undiscovered\", or specifically in line 83, \"The transformations draw attention to variations in the uniformity of wisps and prominence of the photon ring, which are features that black hole experts themselves had not previously identified\". There is no reference that these claims are true.\n  * The contributions of the paper are not clear. The introduction does not clearly indicate what is novel about this paper.\n\n- Experimental evaluation is not rigorous enough.\n  * The experimental evaluation is quite narrow, only testing for 6 counterfactual class pairs.\n  * There are not enough comparisons to counterfactual diffusion model methods, e.g., DVCE [C], DiME [A], and/or ACE [D].\n  * The proposed method likely benefits from the LoRA fine-tuning compared to the competitors creating unfair comparisons and potentially inflating the improvement compared to baselines.\n  * The dataset bias experiment is weak. There are only few qualitative results, and it is not clear if the bias comes from the diffusion model, the CLIP model, or if it can also be caused by the proposed method.\n  \n- The method seems to require careful hyperparameter choices. Hyperparameters vary per dataset and are manually chosen (to my understanding).\n\n[A] Jeanneret et al., Diffusion Models for Counterfactual Explanations, ACCV 2022  \n[B] Costa et al., Diversified in-domain synthesis with efficient fine-tuning for few-shot classification, arxiv 2023  \n[C] Augustin et al., Diffusion Visual Counterfactual Explanations, NeurIPS 2022  \n[D] Jeanneret et al., Adversarial Counterfactual Visual Explanations, CVPR 2023"}, "questions": {"value": "Please refer to the weaknesses section. Questions of particular interest are:\n- What are the paper's contributions?\n- Do you also use LoRA fine-tuning for the competitor approaches? If not, how would the results change if you do?\n- Can you add comparisons with DVCE, DiME, and/or ACE?\n- Are hyperparameters chosen based on a held-out validation set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ok3Z4RAVOO", "forum": "jPqGHu174f", "replyto": "jPqGHu174f", "signatures": ["ICLR.cc/2026/Conference/Submission16315/Reviewer_Q4m4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16315/Reviewer_Q4m4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996899107, "cdate": 1761996899107, "tmdate": 1762926452107, "mdate": 1762926452107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}