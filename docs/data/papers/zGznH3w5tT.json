{"id": "zGznH3w5tT", "number": 23240, "cdate": 1758341162019, "mdate": 1759896824721, "content": {"title": "Evolving DAGs with LLM: Towards Smart and Hallucination-Mitigated Causal Discovery", "abstract": "Causal discovery seeks to infer causal Directed Acyclic Graphs (DAGs) from observational data. To optimize the fitness between a candidate DAG and the observational data, deep learning and heuristic approaches have been widely adopted. However, both paradigms rely solely on data, influenced by properties such as scale, sparsity, and distribution complexity, which often leads to performance bottleneck in complex causal DAGs. To overcome this bottleneck, a promising alternative is to incorporate the semantic information inherent in variables and further exploit the external world knowledge encoded in large language models (LLMs). In this paper, we propose Evolving DAG with LLM (EvoDAG-LM), a novel LLM-assisted evolutionary algorithm for causal discovery, where EvoDAG-LM integrates LLMs into three key phases of the evolutionary process, i.e., search space construction, evolutionary operation enhancement, and loop elimination. This enables the intelligent heuristic search of DAGs. Incorporating LLMs does not imply unconditional trust in it; unlike prior approaches that let LLMs directly determine causal edges, EvoDAG-LM confines LLMs to auxiliary roles and grounds all final decisions in statistical scoring, thereby effectively mitigating hallucination risks. Additionally, to fully yet cautiously exploit the potential of LLMs, EvoDAG-LM adopts advanced prompting strategies and a scale-aware adaptation mechanism to selectively invoke LLM assistance. Experiments demonstrate the superiority and scalability of EvoDAG-LM compared to diverse state-of-the-art methods.", "tldr": "", "keywords": ["Casual Discovery", "Causality", "Large Language Model", "Evolutionary Algorithm", "Directed Acyclic Graph"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb24f8dd7f63a82c761b10b7191abae4609c419f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces **EvoDAG-LM**, an evolutionary causal discovery algorithm that integrates large language models (LLMs) to guide search-space pruning, mutation, crossover, and loop removal. LLMs provide semantic guidance, while final edge selection relies on statistical scores to reduce hallucination. Experiments show that EvoDAG-LM achieves higher F1 and lower SHD than EA, DL, and LLM-based baselines, especially on medium and large graphs. Overall, it presents a promising LLM-assisted framework combining semantic knowledge with data-driven causal discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a clever hybrid approach, using LLMs not as direct causal predictors but as knowledge-based guides for an evolutionary search. By adding LLM-informed pruning (search-space reduction) and LLM-guided operators (semantic crossover/mutation) as well as loop-breaking, EvoDAG-LM injects external semantic information into causal discovery. This idea of LLM-augmented EA is novel and addresses a known weakness of pure EA or DL methods (lack of domain knowledge)\n2. The design explicitly avoids trusting the LLM’s judgment blindly. All final edge decisions come from standard scoring (e.g. BIC), with the LLM only suggesting candidates. The authors emphasize that this “hallucination-mitigated” strategy guards against spurious LLM outputs In particular, the loop-removal component uses LLM queries only to identify unlikely edges, and if uncertainty remains it falls back to mutual-information scores. This combination is a sensible safeguard.\n3. Experiments span diverse benchmarks and strong baselines, including EA, DL, classical, and LLM-based methods (e.g., ChatPC, LCDHP). Using F1 and SHD metrics, results show EvoDAG-LM clearly outperforms others on medium and large graphs and remains competitive on very large ones, demonstrating its strong overall effectiveness.\n4. The authors recognize LLM limitations (context window, cost) and incorporate a scale-aware mechanism. They decrease the probability of invoking the LLM for very large graphs, and they adopt cheaper LLM variants (e.g. GPT-3.5-Turbo) where feasible. Such considerations reflect a realistic view of deployment. The prompting strategy is also well-designed, which is a clever way to exploit LLM reasoning."}, "weaknesses": {"value": "1. A major concern is the heavy computational demand of EvoDAG-LM, which requires numerous LLM queries per generation (for correlation estimation, Tree-of-Thought evolution, loop validation, etc.). The paper explicitly avoids discussing runtime(substituting Fitness Evaluations (FEs) instead), citing variability in API latency, but this omission sidesteps a crucial issue — the actual computational and costs. There is no comparison with baselines in terms of token consumption, number of LLM calls, or time overhead. Moreover, the authors admit that for very large graphs, they had to disable LLM-driven evolution entirely due to token limitations. This suggests the approach may not scale effectively until LLM efficiency improves. And I would suggest that a detailed efficiency/cost analysis be included in the main body of the paper, rather than being relegated to the appendix.\n2. The effectiveness of EvoDAG-LM hinges on the assumption that each variable has a meaningful name or description that the LLM can interpret. In domains where variable names are arbitrary codes or are not human-readable, the LLM components would likely fail or provide no useful signal. The authors themselves acknowledge this limitation, noting that in the Insurance network, ambiguous labels like “this car” versus “other cars” caused the LLM to make mistakes, which hurt performance. This raises doubts about the method's applicability to real-world data where semantic hints are unclear or absent. The method's significant reliance on the LLM's \"world knowledge\" thus limits its generalizability.\n3. The paper presents a novel approach by integrating LLMs directly into an Evolutionary Algorithm (EA). While this specific integration is new, the underlying concept of using LLMs to supply causal prior knowledge or to refine graph structures builds upon ideas explored in prior work. For example, the recent ALCM framework (Khatibi et al., 2025) and LLM-CD (Du et al., 2025) also employ a similar strategy of combining data-driven discovery with LLM-based refinement. To further highlight the unique contributions of EvoDAG-LM, the paper would be strengthened by conducting a more comprehensive survey of related work and providing a detailed comparison with these approaches. This would help clarify how its specific design—particularly the careful implementation of the LLM as an auxiliary guide within the search—sets it apart from other hybrid methods.\n\n[1] Khatibi, E., Abbasian, M., Yang, Z., Azimi, I., & Rahmani, A. M. (2025). ALCM: Autonomous LLM-Augmented Causal Discovery Framework. arXiv preprint arXiv:2405.01744. https://arxiv.org/abs/2405.01744\n[2] Du, H., Zheng, Y., Jing, B., Zhao, Y., Kou, G., Liu, G., Gu, T., Li, W., & Yang, C. (2025). Causal Discovery through Synergizing Large Language Model and Data-Driven Reasoning. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD '25) (pp. 543–554). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3711896.3736874"}, "questions": {"value": "1. Since the authors state that the proposed method uses the LLM in an auxiliary and guiding role, could you provide a comparison of efficiency and cost between EvoDAG-LM and the baselines? For instance, compared to other LLM-based methods, by how much is the token consumption reduced, and what are the differences in runtime? Can you provide any information on the wall-clock runtime or number of LLM calls (and hence cost) for representative tasks? This is important for assessing feasibility.\n2. The competitors include four state-of-the-art (SOTA) EA-based methods for comparison: MIGA (Yan et al., 2023), Hybrid-SLA (Jose et al., 2019), PC-PSO (Sun et al., 2021), and AESL-GA (Contaldi et al., 2019); four LLM-based methods, including two best LLM-based methods (i.e., GPT4 and GPT4-Turbo (Achiam et al., 2023)) in CausalBench (Zhou et al., 2024) and two latest LLM-based works (i.e., ChatPC (Cohrs et al., 2024) and LCDHP (Wang et al., 2025));'\nHowever, the experimental results table shows: GPT4-Turbo, GPT4, CausalGPT, LCDHP. Are CausalGPT and ChatPC the same method? Or in other words, is the 'CausalGPT' in the experimental table actually 'ChatPC'?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DKyAYShsID", "forum": "zGznH3w5tT", "replyto": "zGznH3w5tT", "signatures": ["ICLR.cc/2026/Conference/Submission23240/Reviewer_gyRr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23240/Reviewer_gyRr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723705466, "cdate": 1761723705466, "tmdate": 1762942570984, "mdate": 1762942570984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is on incorporating the semantic information with LLMs to serve as a prior for the causal discovery task. It proposed a new pipeline: EvoDAG-LM, which integrates causal discovery and evolutionary algorithms. It employs the LLMs to process and exploit the given semantic metadata behind variables. \n\nThe method consists of three stages: (1)  search space reduction; (2) evolutionary operator enhancement; and (3) loop removal. Empirical results are presented on about 11 datasets with four types of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Integration of LLMs into EAs. It integrates evolutionary algorithms to treat causal discovery from a perspective of combinatorial optimization.\n-  Extensive evaluation. Empirical results are presented on about 11 datasets with four types of baselines."}, "weaknesses": {"value": "- The research problem and the technical challenges that motivate the three proposed components are not clear. For example, existing methods that utilize variable descriptions can also help to reduce the search space. What is the specific drawback of the previous baselines that is to be addressed by this paper?\n- Ablation of ToT. The use of Tree-of-Thought (ToT) prompting is highlighted in this paper. It is necessary to report the results about (1)  EvoDAG-LM using CoT and direct answering;  (2) more baselines with ToT in Tables 1 and 2."}, "questions": {"value": "- What is the meaning of equation (2)? What is the difference between $\\text{MI}_{i,j}$ and $\\text{MI}(i,j)$?\n- How would the quality of such metadata influence the proposed pipeline? For example, if some variables' metadata are absent, ambiguous, or even wrong, would the proposed method be robust to these situations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rdz8LySbAN", "forum": "zGznH3w5tT", "replyto": "zGznH3w5tT", "signatures": ["ICLR.cc/2026/Conference/Submission23240/Reviewer_H3Jt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23240/Reviewer_H3Jt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729106030, "cdate": 1761729106030, "tmdate": 1762942570757, "mdate": 1762942570757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "** Summary\n\nThe work proposes a hybrid method, which is based on statistical-based method and LLM-based method, for causal discovery, termed EvoDAG-LM, aiming to mitigate the shortage of methods that purely reply on numerical data. The method consists of three steps: (1) Query LLM to remove edges between weakly correlated variable pairs; (2) Evolutionarily refine the causal graph based on LLM reasoning and statistical information; and (3) Remove cycles from the causal graph. The paper provides empirical results to show that EvoDAG-LM almost outperforms other baseline methods on a series of benchmarks.\n\n** Recommendation \n\nI would like to recommend a rejection to this paper for its limited novelty and presentation. The paper proposes a hybrid causal discovery method, where several similar methods have emerged recently. Comparing to existing methods, EvoDAG-LM uses evolutionary operations, which potentially enhance the pipeline’s accuracy. However, I think the contribution remains limited. Additionally, I believe the paper’s presentation need further improvement to make readers fully understand the technical parts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The method employs LLMs in evolutionary operations to refine the causal graph, potentially enhancing the method’s performance."}, "weaknesses": {"value": "1. The idea of hybrid causal discovery method is not new, and the original contribution remains limited.\n2. The presentation is not clear, undermining the paper’s contribution. For instance, it does not make the setting of combining LLM and statistical methods clear in the beginning of the technical part or write data as input in the algorithm, only emphasising the role of LLMs. This makes me confusing when reading the paper. Many notations and terms of statistics and graph are not defined or used inconsistently, e.g., MI_{I,j} and MI(i,j), Pa, individuals, and using e^g_{i,j,m} as both a matrix and number. Another important point is that, I found the description of some key steps, e.g., the Selection operation, the Crossover operation, and the Mutation operation, is not understandable, while the paper keeps much explanation of prompting strategies like ToT and CoT. This reduces the paper’s readability.\n3. The work uses many popular benchmarks to evaluate the method. However, some of those datasets are well remembered by the LLMs, undermining the quality of the evaluation part. Moreover, for instance, as far as I remember, many LLM-based methods have very high performance on Asia (close to 100%), however, the reported baseline LLM-based methods only have low performances."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L3oMNrSGEC", "forum": "zGznH3w5tT", "replyto": "zGznH3w5tT", "signatures": ["ICLR.cc/2026/Conference/Submission23240/Reviewer_k5FE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23240/Reviewer_k5FE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997577276, "cdate": 1761997577276, "tmdate": 1762942570482, "mdate": 1762942570482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}