{"id": "lO1BLLh8K8", "number": 9865, "cdate": 1758144990751, "mdate": 1759897690856, "content": {"title": "NCGAMI: Domain-Adaptive Drug-Target Interaction Prediction with Graph and Sequence Models", "abstract": "Drug-target interaction (DTI) prediction is of central importance in computational pharmacology, but how robust these predictions are in the face of distribution shift (e.g. between chemical scaffolds, or protein families) remains challenging. We introduce a well-behaved, label-free objective and encoder fusion recipe for DTI, which is referred to as NCGAMI, under an unsupervised domain adaptation (UDA) training paradigm. NCGAMI only uses a graph encoder for the drug molecule graph and a sequence encoder for the proteins, and uses a three-term objective, consisting of (i) the supervised source-domain risk, (ii) an explicit cross-domain representation alignment, and (iii) regularization of the target domain through conditional-entropy minimization and prediction-consistency. The implementation is possible without using target labels. Throughout, we take great care to avoid using target labels during training and to clarify situations where there may be overlap between source and target entities. Using two widely used DTI datasets (Human and DrugBank), and using a protocol (random split) that we applied strictly for feasibility check, NCGAMI achieves good AUC/AUPR and is competitive against representative baselines. On Human our model achieves AUC 0.895 and AUPR 0.852 on Drugbank our model achieves AUC 0.733 and AUPR 0.675. Ablations can be seen to contribute the graph encoder, sequence encoder, and UDA regularization. We also incorporate non-operational geometric background so that theoretical assertion is motivating in your choice of design without overwhelming you with strong theoretical claims.", "tldr": "", "keywords": ["Graph Representation", "Domain Adaptation", "Distrbution Alignment"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10b7d670696cd082e3eeca6c15769d6e6b1d0ddb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes NCGAMI, a UDA (unsupervised domain adaptation) recipe for DTI prediction that fuses a graph encoder for molecules and a sequence encoder for proteins, trained with a label-free three-term objective. They evaluate the proposed model on two well-known DTI datasets, benchmarking across several state-of-the-art models such as MolTrans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written, presenting both theorical definitions/proportions and empirical contributions.\n\n2. The appendix is well organized and effectively supports understanding of the training setup, computational footprint and clarification on metrics, among others.\n\n3. The selected datasets are appropriate for studying domain shift, covering multiple domains.\n\n4. Architectural, training, and dataset details are clearly described, contributing to transparency and reproducibility.\n\n5. The paper incorporates ablation study and additional analysis that help understand better the work contributions."}, "weaknesses": {"value": "1. The observed improvements appear insufficient. From Figure 2 its hard to tell if there is any significant improvement when compared to the evaluated DTI-prediction models.\n\n2. The rationale behind the choice of source and target domains is not clearly explained, despite being central to the study.\n\n3. The paper omits several strong DTI-specific graph-based approaches, such as GeNNiUs (https://doi.org/10.1093/bioinformatics/btad774) and EEG-DTI (https://doi.org/10.1093/bib/bbaa430). Including such methods would better position the proposed framework within current literature."}, "questions": {"value": "1. Is the code and data preprocessing pipeline publicly available? It is not clearly indicated in the paper.\n\n2. How distinct are the source and target domains in the evaluated datasets? Does the proposed model perform better when the target domain is more similar to the source one, and to what extent?\n\n3. Why are the results not reported as mean ± standard deviation, considering that multiple random seeds were used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T7Te5DCpaH", "forum": "lO1BLLh8K8", "replyto": "lO1BLLh8K8", "signatures": ["ICLR.cc/2026/Conference/Submission9865/Reviewer_vjX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9865/Reviewer_vjX4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761216418091, "cdate": 1761216418091, "tmdate": 1762921337472, "mdate": 1762921337472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NCGAMI, an unsupervised domain adaptation (UDA) framework for drug-target interaction (DTI) prediction that combines graph encoders for molecules with sequence encoders for proteins."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and interesting topic: domain adaptation for DTI prediction is a genuine practical challenge with significant implications for drug discovery."}, "weaknesses": {"value": "- The paper's random split allows drugs and proteins to appear in both source and target sets, which can be viewed as warm start but introduces significant data leakage risk. The DTI community has established three rigorous evaluation settings that should be included:\nWarm start: Data is split based on protein-drug pairs, ensuring no common pairs appear in both training and test sets (no pair leakage).\nDrug cold start: Split at the molecule level, guaranteeing that no drug in the test set is present in the training set.\nTarget cold start: Split at the protein level, meaning no protein in the test set is seen during training.\n\n- The paper lacks comparison with newer methods such as: [1] DTIAM : \"A unified framework for predicting drug-target interactions, binding affinities and drug mechanisms\" \n\n- The paper only evaluates on two datasets (Human and DrugBank), which is insufficient:\nMissing standard benchmarks: Should include widely-used DTI datasets: Yamanishi 08 datasets and Hetionet dataset [1]\nMissing mechanism of action evaluation: Should evaluate on mechanism of action (MoA) prediction tasks: Activation datasets and Inhibition datasets [1]"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EfePZXUVhZ", "forum": "lO1BLLh8K8", "replyto": "lO1BLLh8K8", "signatures": ["ICLR.cc/2026/Conference/Submission9865/Reviewer_nWyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9865/Reviewer_nWyd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950871005, "cdate": 1761950871005, "tmdate": 1762921337128, "mdate": 1762921337128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NCGAMI, a domain adaptation framework for drug–target interaction (DTI) prediction. The stated goal is to improve robustness under distribution shift between a labeled source domain and an unlabeled target domain.\nExperiments are run on two benchmark DTI datasets (“Human” and “DrugBank”), using a random partition protocol where each dataset is split into source (labeled), target-unlabeled (used for adaptation), and target-test (labeled, held out until evaluation). The authors demonstrates that the proposed approach achieves better performance than baselines. Ablation analysis was also performed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical UDA recipe for DTI.\nThe paper proposes a simple, fully specified training objective that combines (1) source supervision, (2) batch-level mean/covariance alignment between fused drug–protein embeddings, and (3) unlabeled-target entropy/consistency regularization. This is all implementable without adversarial domain discriminators or complex auxiliary heads. \n\n2. Ablation discussion.\nThe paper qualitatively discusses ablations: removing the consistency term hurts AUPR stability; replacing the long-range protein encoder with a CNN reduces performance on proteins where long-range residue context matters; etc. While numeric tables are not shown in the text provided, the narrative suggests that each term contributes."}, "weaknesses": {"value": "1. Evaluation protocol does not match the stated motivation.\nThe paper motivates the method as a solution for distribution shift between source and target domains, but the experimental setup does not clearly establish such a shift. Both domains appear to be drawn from the same dataset via a random split, and no quantitative characterization of how (or whether) the source and target distributions differ is provided. \n\n2. Potential leakage / unclear domain separation.\nBecause the source and target sets are defined by randomly partitioning a single dataset, it is likely that the same drugs or the same protein targets appear in both. Since unlabeled target data are also used during training, the method may partially benefit from implicit memorization of these shared entities. \n\n3. The model diagram omits the adaptation mechanism.\nFigure 1 illustrates the drug encoder, protein encoder, and cross-attention fusion/classifier, but it does not depict how unlabeled target data are incorporated during training (moment matching / alignment loss, target entropy minimization, consistency regularization). As a result, the figure reads as an ordinary supervised DTI predictor rather than a domain adaptation framework, which makes it harder to understand what is novel about NCGAMI.\n\n4. Lack of full quantitative results tables.\nThe manuscript reports headline AUC/AUPR values in text, but does not provide a complete table comparing all baselines and the proposed method under the same data split. Without such a table, it is not possible to evaluate how large the claimed gains are.\n\n5. No discussion of computational cost vs. baselines.\nThe paper mentions multi-GPU training, but does not report training time, memory footprint, or parameter counts relative to standard DTI models (e.g., MolTrans, DeepDTA, etc.). If the proposed method requires substantially higher compute to obtain modest AUC/AUPR improvements, that trade-off should be made explicit.\n\n6. Unclear statistical significance.\nThe paper states improvements (e.g., “~1% better”) but does not report variance estimates, confidence intervals, or statistical tests. It is therefore unclear whether the observed differences are statistically meaningful or within normal run-to-run noise."}, "questions": {"value": "1. Generalization under realistic shift.\nCan you report results under at least one standard “hard” split, such as cold-drug, cold-target, cold-both, or a cross-dataset transfer (e.g., train on one dataset and adapt to another)? Right now, because source and target are created by random partition of the same dataset, it is difficult to assess true model generalizability.\n\n2. Complete quantitative comparison.\nPlease include a results table with all baseline methods and the proposed method, evaluated under the same protocol. The table should clearly indicate which methods have access to unlabeled target data during training.\n\n3. Statistical significance.\nAre the reported performance gains statistically significant? Please provide significance testing to demonstrate that improvements are not within normal run-to-run variability.\n\n4. Source–target overlap.\nHave you quantified how often the same drug or the same protein target appears in both the source and target splits under your random partitioning procedure? High overlap would suggest the setting is closer to semi-supervised learning than to true domain adaptation.\n\n5. Clarification in the model figure.\nPlease update a model diagram that explicitly shows how unlabeled target data are used during training. As currently drawn, Figure 1 looks like a standard supervised DTI predictor and does not illustrate the domain adaptation components that are central to the paper.\n\n6. Compute and reproducibility.\nYou mention using 8×A100 GPUs for training. Are those resources required for the main NCGAMI model, or were they primarily used to speed up ablations? It would be helpful to report approximate training time, GPU memory usage, and parameter counts for NCGAMI, and to compare these to standard baselines such as MolTrans and DeepDTA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iZ0meZwUoJ", "forum": "lO1BLLh8K8", "replyto": "lO1BLLh8K8", "signatures": ["ICLR.cc/2026/Conference/Submission9865/Reviewer_AYWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9865/Reviewer_AYWv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961726206, "cdate": 1761961726206, "tmdate": 1762921336749, "mdate": 1762921336749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}