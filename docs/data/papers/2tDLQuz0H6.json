{"id": "2tDLQuz0H6", "number": 7758, "cdate": 1758034933365, "mdate": 1759897834286, "content": {"title": "GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization", "abstract": "Repository-level bug localization, the task of identifying where code must be modified to fix a bug, is a critical software engineering challenge. Standard Large Language Models (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. Moreover, the intricate dependencies between code entities mean that bug localization often requires multi-hop reasoning across the repository's structure. Existing approaches typically treat this as an information retrieval (IR) problem, relying on heuristics like keyword matching and text similarity. While some methods incorporate repository graph structures, they often employ simplistic traversal algorithms (e.g., Breadth-First Search). Graph Neural Networks (GNNs) present a promising alternative with their inherent capacity to model complex, repository-wide dependencies, but the absence of a dedicated benchmark has hindered their application. To bridge this gap, we introduce GREPO, the first benchmark designed for repository-scale bug localization using GNNs. It comprises 109 Python repositories and over 10,000 bug-fixing pull requests, offering graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures on a representative subset of 9 repositories in GREPO reveals their competitive performance against established information retrieval baselines. This work demonstrates the strong potential of GNNs for this task and establishes GREPO as a foundational resource for future research. Our code can be found at https://anonymous.4open.science/status/RepoGNN-57C0.", "tldr": "", "keywords": ["Code Agent", "Graph"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97f1979f62500bae41b0548939b925e5da248399.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GREPO, a new benchmark dataset designed to facilitate research on repository-level bug localization using Graph Neural Networks (GNNs). The authors argue that existing methods, such as those based on Information Retrieval (IR) or Large Language Models (LLMs) with limited context, fail to adequately leverage the rich structural information inherent in code repositories. To address this, GREPO provides a large-scale dataset of 109 Python repositories, comprising over 10,000 bug-fixing pull requests, with the codebase pre-processed into graph structures suitable for direct use with GNNs. \nThe work's primary contribution is the benchmark itself, intended as a foundational resource for the community."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The creation of a large, pre-processed, and graph-ready benchmark (GREPO) is a substantial engineering effort. Such a resource is valuable and can lower the barrier to entry for researchers wanting to apply GNNs to this domain.\n2.  The pipeline for constructing the dataset, including the use of a temporal graph to efficiently handle different commit snapshots and the careful collection of high-quality labels from pull requests and issues, is well-designed.\n3. The authors have made the code open-source."}, "weaknesses": {"value": "1. The primary contribution of this work is to be the construction of a new dataset. Given that the main novelty lies in the dataset and its empirical findings, the paper might be a better fit for a conference with a dedicated \"Datasets and Benchmarks\" track or a top-tier software engineering venue (e.g., ICSE, FSE, ASE), where the contribution would be more prominently highlighted. For a venue like ICLR, the innovation seems limited.\n2. The performance of the AgentLess baseline is perplexing and lacks sufficient analysis. It reportedly achieves a near-perfect 92.7% Hit@1 at the file level but a drastically poor 6.26% at the class/function level. This is highly counter-intuitive. The paper fails to discuss or acknowledge this strange result."}, "questions": {"value": "1. The limitation discussed in Section 8 is critical. Could you provide more details about this experiment? What was the exact setup? Why do you believe the GNN's localization signal failed to help SWEBench? \n2. The standard deviations reported in Table 1 are extremely high, in some cases larger than the mean itself (e.g., LocAgent). This severely weakens the claim of generalizability. Furthermore, could authors offer any insights into why the performance is so unstable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gquxzyUWKD", "forum": "2tDLQuz0H6", "replyto": "2tDLQuz0H6", "signatures": ["ICLR.cc/2026/Conference/Submission7758/Reviewer_d85e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7758/Reviewer_d85e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294442520, "cdate": 1761294442520, "tmdate": 1762919803058, "mdate": 1762919803058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GREPO, a large-scale benchmark dataset and evaluation suite for repository-level bug localization using Graph Neural Networks (GNNs). GREPO contains 109 Python repositories and over 10k bug-fixing pull requests, representing each repository as a temporal heterogeneous graph with directory, file, class, and function nodes and contain, call, and inherit edges. The authors build node and query features using LLM embeddings (Qwen3-Embedding-8B) and introduce anchor nodes and similarity features for subgraph extraction. Multiple GNN architectures (GIN, GraphSAGE, GAT, GPS, UniMP) are evaluated and compared against IR-based and LLM/agent baselines using Hit@k metrics. GREPO serves as a dataset and standardized evaluation pipeline intended to support future research into graph-based repository reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1.The research motivation is clear, and the overall idea is meaningful.\n2.The ablation experiments validate the effectiveness of methodological design\n3.GREPO provides a valuable dataset resource for graph-based software repository analysis"}, "weaknesses": {"value": "1.Unclear methodological exposition: The introduction clearly states the research motivation, but the subsequent sections fail to describe the motivation and construction process of the benchmark in a coherent way. As a result, the paper reads as fragmented and lacks a continuous narrative.\n2.Definitions and formulas lack rigor: The concepts of anchor nodes, similarity features, and subgraph extraction are insufficiently described. In Figure 1 (page 4), anchor-related content is not mentioned at all in the “Dataset Construction” section (page 3), making the figure hard to interpret. Moreover, the definitions of anchor and similarity features are only provided later in Section 5 (“The GREPO Graph Formulation”), which disrupts the logical flow. In addition, the second formula on page 6 seems inconsistent with the surrounding text—please clarify this to ensure mathematical correctness.\n3.Missing baselines: The authors criticize existing GNN methods (e.g., GNN-for-CFG, Huo et al., 2020; Ma & Li, 2022) in Section 2.2, but GREPO is not compared with these baselines in the experiments (Section 6.3).This omission weakens the validity of the empirical claims.\n4.Lack of objectivity in the conclusions: In Section 6.3, the authors avoid discussing the fact that their GNN models are significantly outperformed by the Agentless (GPT-4o) baseline on the file-level Hit@1 metric (54.18 vs 92.72).In the Limitations section, they attribute poor results to issues with the agent framework instead of analyzing the root causes.This selective interpretation undermines the credibility of the conclusions.\n5.Lack of interpretive depth: Experimental results are listed but not analyzed to yield deeper insights.The paper would benefit from qualitative case studies or error analyses to explain where and why the models succeed or fail."}, "questions": {"value": "1.Could the authors provide a more detailed definition of anchor nodes and clarify their relationship to similarity features?\n2.Were the GNN baselines mentioned in the introduction actually implemented or tested? If not, why were they omitted from the comparison?\n3.Do the authors have any analysis or explanation for why GAT performs substantially worse than the Agentless baseline on some metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X2NpdPdRWo", "forum": "2tDLQuz0H6", "replyto": "2tDLQuz0H6", "signatures": ["ICLR.cc/2026/Conference/Submission7758/Reviewer_2JMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7758/Reviewer_2JMH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976469718, "cdate": 1761976469718, "tmdate": 1762919802286, "mdate": 1762919802286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores  Graph Neural Networks (GNNs) as an alternative to repository-level bug localization, motivated by their ability to model complex dependencies in codebases. For this, the authors introduce GREPO designed for repository-scale bug localization with GNNs, 109 Python repositories and over 10,000 bug-fixing pull requests. With experiments comparing various GNN architectures against existing baselines, the authors claim the strong potential of GNNs for the repository-level bug localization task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach described in the paper to leverage GNNs for the bug localization task is very detailed and carefully designed.\n2. The ablation experiments are pretty comprehensive and explore a variety of methodological choices.\n3. The results, shown a subset of the GREPO benchmark, look very impressive."}, "weaknesses": {"value": "While the experimental results presented by the authors do look very promising, below are reasons why I am not convinced yet of the author’s claims that GNNs will be able to outperform information retrieval (IR) approaches\n\n1. **Inadequate Baselines**: While the authors claims to compare against IR approaches,  they haven’t considered any embedding-specific or retrieve-and-rerank style methods. I would suggest to compare against the SweRank approach [1], which uses a code embedding model for function retrieval and LLM for reranking. \n2. **Limited Evaluation Benchmarks**: The paper only evaluates on the GREPO benchmark introduced in this paper. The authors should also evaluate the trained GNN models directly on the established Swe-Bench-Lite and LocBench benchmarks, for which there are extensive baseline results presented in [1]. \n3. **Novelty Claims**: The paper’s claim that they are the first to create a benchmark for graph-based bug localization is not actually true. LocAgent also released LocBench, which created a dependency-graph representation of the repo (although without temporal connections across commits) as done in this paper. The authors should acknowledge this accordingly. \n\nIf the above concerns are addressed in the rebuttal, I am happy to increase  my score. \n\n[1] SweRank:  Software Issue Localization with Code Ranking; Reddy et al 2025."}, "questions": {"value": "1. In Table 1, Agentless seems to have considerable high file-localization performance (>90 hit@1) but much lower function-level localization. Any explanation for this?\n2. The paper released  109 repositories in GREPO but only use 9 of them evaluation. Is the scale of training the issue? And what measures were taken to validate the quality / correctness of the remaining 100 repositories \n3. The function-level numbers for GAT seem to have very high (>10) confidence intervals. What could be the reason for such high variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A9VPqf5biz", "forum": "2tDLQuz0H6", "replyto": "2tDLQuz0H6", "signatures": ["ICLR.cc/2026/Conference/Submission7758/Reviewer_Sb8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7758/Reviewer_Sb8k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7758/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145766726, "cdate": 1762145766726, "tmdate": 1762919801834, "mdate": 1762919801834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}