{"id": "Y7IZlDttMM", "number": 1920, "cdate": 1756966021393, "mdate": 1759898178526, "content": {"title": "CoRA: Covariate-Aware Adaptation of Time Series Foundation Models", "abstract": "Time Series Foundation Models (TSFMs) have shown significant impact through their model capacity, scalability, and zero-shot generalization. However, due to the heterogeneity of inter-variate dependencies and the backbone scalability on large-scale multivariate datasets, most TSFMs are typically pre-trained on univariate time series. This limitation renders them oblivious to crucial information from diverse covariates in real-world forecasting tasks. To further enhance the performance of TSFMs, we propose a general covariate-aware adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of foundation models while effectively incorporating exogenous covariates from various modalities, including time series, language, and images, to improve the quality of predictions. Technically, CoRA maintains the equivalence of initialization and parameter consistency during adaptation. With preserved backbones of foundation models as frozen feature extractors, the outcome embeddings from foundation models are empirically demonstrated more informative than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE) to automatically evaluate covariates regarding their causal predictability with respect to the target variate. We incorporate these weighted embeddings with a zero-initialized condition-injection mechanism, avoiding catastrophic forgetting of pre-trained foundation models and gradually integrates exogenous information. Extensive experiments show that CoRA of TSFMs surpasses state-of-the-art covariate-aware deep forecasters with full or few-shot training samples, achieving 31.1% MSE reduction on covariate-aware forecasting. Compared to other adaptation methods, CoRA exhibits strong compatibility with various advanced TSFMs and extends the scope of covariates to other modalities, presenting a practical paradigm for the application of TSFMs.", "tldr": "We introduce CoRA, a general, effective, and interpretable framework to adapt Time Series Foundation Models on covariate-aware forecasting, achieving SOTA performance while requiring fewer training samples.", "keywords": ["Time Series Forecasting", "Adaptation", "Fine-tuning", "Time Series Foundation Models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3935091c0f6851f123dd3e4334d15dfce2045cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoRA, a Covariate-awaRe Adaptation framework for adapting Time Series Foundation Models (TSFMs) to covariate-aware forecasting tasks. CoRA aims to extend the applicability of TSFMs by integrating exogenous covariates from multiple modalities such as time series, text, and images, without modifying the pre-trained backbone. Experiments on a suite of uni-modal and multi-modal benchmarks, including few-shot and multivariate settings, show CoRA consistently outperforming strong supervised and adaptation baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Motivation:** The research motivation of this work is clear. The challenge addressed in this study represents a widely recognized concern in the time series domain, bearing significant practical implications.\n\n2. **General Adaptation Mechanism**: CoRA presents a versatile adaptation framework applicable across various TSFMs and covariate modalities, without requiring modifications to the pre-trained model backbones.\n\n3. **Comprehensive Empirical Evaluation:** The experimental section is thorough, including results on uni-modal, multi-modal, and multivariate forecasting tasks. Ablation studies are performed to assess the contribution of each component."}, "weaknesses": {"value": "1. This paper lacks technical novelty and theoretical contributions, as the methods employed are merely simple applications of existing techniques, such as zero-initialization and adaptive layer-normalization (adaLN).\n\n2. The paper does not provide sufficient validation that the discovered \"causal relationships\" are meaningful or correct. The manuscript lacks a theoretical analysis to interpret the learned relationships and fails to compare its performance against established causal discovery methods. The embeddings could simply be learning spurious correlations.\n\n3. The experimental setup assumes the last dimension is always the endogenous variable. This assumption may be problematic, since the relationship between endogenous and exogenous variables is not necessarily causal and could merely reflect correlations.\n\n4. There is no direct evidence to suggest that the significant performance improvement of CoRA on multi-modal datasets is attributable to its architectural design, rather than the inherent gains from the ViT and Qwen-Embedding components themselves."}, "questions": {"value": "1. This work simply utilizes learnable embeddings to represent weights, a method that captures correlations rather than establishing true causal relationships. The paper lacks a discussion of confounders, instrumental variables, or other causal identification strategies necessary for genuine causal inference. Does a reliable theoretical foundation support this method?\n\n2. The framework is presented as “universally” adaptable, but all adaptation methods in the primary experiments are built on the Sundial TSFM backbone. While some transfer analyses show CoRA applied to other foundation models, direct side-by-side comparisons (with ablations) on all architectures are not exhaustive.  Could the framework's performance depend on the specific architecture of the Sundial model for most tasks ?\n\n3. This paper does not provide a hyperparameter sensitivity analysis or an open-source repository, which means researchers may need to spend a significant amount of extra time adjusting settings to reproduce the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7AZFYNxLB9", "forum": "Y7IZlDttMM", "replyto": "Y7IZlDttMM", "signatures": ["ICLR.cc/2026/Conference/Submission1920/Reviewer_yv98"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1920/Reviewer_yv98"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644950317, "cdate": 1761644950317, "tmdate": 1762915953856, "mdate": 1762915953856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a way to combine frozen foundation models (that could be for different modalities) for time series forecasting, where a key idea is to figure out which features across channels are predictive of each other with the help of an architecture inspired by Granger causality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method seems to be fairly simple/clean so that the resulting adaptation appears easy to implement/train\n- The experimental results of CoRA look to be very good\n- I found the paper for the most part easy to follow"}, "weaknesses": {"value": "- In Section 2.3: there's also an earlier adaptation approach than the ones you cited. See the paper \"Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation Models for Multivariate Healthcare Time Series\" by Liu et al 2024 -- while this paper looks at healthcare time series, the basic idea trivially generalizes to time series from other application domains as well.\n- In Section 3.2: while Granger causality is used to motivate the proposed method, from what I can tell, what is actually implemented is only inspired by Granger causality but is not actually literally doing what Granger causality does. In particular, you're not actually doing any statistical tests, if I understand the setup correctly. If this is the case, then perhaps it would be helpful discussing in a bit more detail to what extent the resulting approach actually resembles and has the same sort of interpretation as Granger causality, or whether we no longer have the same sort of interpretation anymore as there are no statistical tests conducted (showing that the weights correlate well as in Figure 7 is a good first step but leaves me wondering whether there's any sort of deeper connection, and whether we do have any sort of meaningful Granger causality interpretation from the resulting model itself). Perhaps rewording some of the exposition regarding Granger causality would be helpful to make it clear that the proposed CoRA method is not actually reasoning in a causal manner (from my understanding) so we aren't actually making any sort of statements about causality (in particular, the proposed architecture seems to only be inspired by but not the same thing as Granger causality, and Granger causality itself only has a valid causal interpretation under some key assumptions holding --- assumptions which don't appear to be discussed in this paper anyways). Separately, I find that equation (6) could just be interpreted as an attention mechanism, without trying to make any sort of connection to Granger causality. Perhaps commenting on this connection would be helpful. Is this idea already present (in some form) in other approaches for adapting univariate time series to multivariate time series prediction?\n- Minor:\n    - Line ~189-190: the section heading for Section 3.1 has a typo where \"Forzen\" should say \"Frozen\"\n    - Line 191: I'd suggest softening the language so that instead of saying \"exogenous covariates are always multi-dimensional\", perhaps instead say that \"exogenous covariates are very often multi-dimensional\""}, "questions": {"value": "Please see weakness points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ThCTtPtA6z", "forum": "Y7IZlDttMM", "replyto": "Y7IZlDttMM", "signatures": ["ICLR.cc/2026/Conference/Submission1920/Reviewer_seVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1920/Reviewer_seVR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850310011, "cdate": 1761850310011, "tmdate": 1762915953499, "mdate": 1762915953499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Covariate-awaRe Adaptation (CoRA), a framework to incorporate multimodal covariates into TSFMs. Within the CoRA framework, the authors introduce Granger Causality Embedding (GCE) that enables better relate the covariates to the output variate for improved results. The authors claim to surpass the SoTA on Time Series Forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The authors solve a known problem of factoring in exogenous convariates into TSFMs. The problem furthers when there are different modalities to factor in too. \n\n- The use of Granger Embeddings serves as a good technical contribution to evaluate the contribution of each covariate to the target variate.\n\n- The authors have good experiment setup and ablation study to support the claims."}, "weaknesses": {"value": "- Just an avenue for improvement/extension, can be to the test the adaptation on other tasks beside time series forecasting. For example, we can extract the intermediate time series embeddings from Chronos (Ansari et al., 2024)  given their encoder-decoder structure and use that for time-series classification."}, "questions": {"value": "- Given the wide variety of image and text data available how well will CoRA hold for modalities outside of the domain tested in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3qLnJP24QU", "forum": "Y7IZlDttMM", "replyto": "Y7IZlDttMM", "signatures": ["ICLR.cc/2026/Conference/Submission1920/Reviewer_yDBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1920/Reviewer_yDBe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933339279, "cdate": 1761933339279, "tmdate": 1762915952599, "mdate": 1762915952599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoRA, an adaptation approach to incorporate covariates in Time Series Foundation Models (TSFMs). This adaptation is achieved using covariate selection through Granger causality and integrating the selection signals into TSFMs through adaptive layer normalization. CoRA can incorporate multi-modal covariates spanning time series, text and images.\n\nThe authors evaluate on ETT and EPF datasets on short and long term forecasting. For ETT datasets, CoRA obtains 31.1% improvement in MSE over prior supervised methods and 18.7% over existing TSFM adaptation methods. Similarly for EPF datasets reduction in MSE are observed. Authors have also evaluated their work on Time-MMD dataset consisting of text and image covariates.\n\nThe evaluation is comprehensive as authors have covered scenarios and settings spanning limited availability of data, multivariate forecasting, CoRA's generalization to multiple TSFMs, and a thorough ablation study.\n\nThe paper's main contribution is the adaptation framework that integrates with frozen TSFMs to perform covariate-aware forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed adaptation method is well motivated based on prior work. Particularly, the covariate selection mechanism is quite interesting as it can suppress signals from noisy and irrelevant covariates.\n2. The adaptation method can be plugged into any TSFM and it can incorporate covariates with diverse modalities.\n3. The paper is well written and easy to follow and understand.\n4. Authors have thoroughly evaluated the proposed method using ETT, EPF and Time-MMD datasets.\n5. The ablation studies highlight the importance of each component in the approach."}, "weaknesses": {"value": "1. Authors have not address the most significant limitation of the proposed adaptation method. TSFMs have enabled zero-shot forecasting on unseen data during inference. Due to this, no parameter updates are required and the inference overhead is minimal (FM forward pass). CoRA adapts FMs to incorporate covariates but it requires training which takes away the ability of FM to forecast zero-shot. \n2. Insights into difficulty of training additional parameters is missing.\n3. Analysis of computational overhead to incorporate covariates (including the training time) is missing.\n4. Authors should move the training details from appendix to the main sections of the paper. Currently, it is not clear from the main sections of the paper that the training uses TSFM loss. \n5. The architecture and the inference mechanism of the TSFM head is not clear, especially for long-term forecasting where causal TSFMs autoregressively predict patches."}, "questions": {"value": "1. What is the architecture of the TSFM head? Are you finetuning the pretrained TSFM head? If so, how is it used in long-term forecasting? \n2. If the TSFM head is predicting in patches and the input is the scaled E_target, what embeddings do you use to predict the future patches? I am interested in understanding the inference mechanism better.\n3. Can this approach provide 100% accuracy if the oracle label  (the target time series) is used as the covariate?\n4. What is the experiment configuration for few-shot forecasting? Are you training the adapter parameters only with the few-shot data?\n5. Do you need to train a unique adapter for each forecast horizon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NQMzOJXinN", "forum": "Y7IZlDttMM", "replyto": "Y7IZlDttMM", "signatures": ["ICLR.cc/2026/Conference/Submission1920/Reviewer_6z5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1920/Reviewer_6z5j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1920/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973308282, "cdate": 1761973308282, "tmdate": 1762915952446, "mdate": 1762915952446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}