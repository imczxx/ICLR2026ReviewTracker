{"id": "g5ZDfbElUj", "number": 14058, "cdate": 1758227723604, "mdate": 1759897392884, "content": {"title": "PhysiX: A Foundation Model for Physics Simulations", "abstract": "While foundation models have achieved remarkable success in domains like video, image, and language by scaling on massive datasets, this progress has not yet translated to physics simulation. A primary bottleneck is data scarcity: while millions of images, videos, and textual resources are readily available on the internet, the largest physics simulation datasets contain only tens of thousands of samples. This data limitation makes large models prone to overfitting and has confined physics applications to small models, which struggle with complex domains and long-range predictions. Furthermore, the drastic variations in scale and structure across physics datasets—a heterogeneity not typically found in vision or language—further amplify the challenges of scaling up multitask training. We introduce PhysiX, a family of large-scale foundation models for physics simulation. PhysiX is an autoregressive generative model composed of a discrete tokenizer, which converts heterogeneous physical processes to sequences of tokens, and a Transformer that models these sequences via next-token prediction. To mitigate the rounding error in the discretization process, PhysiX additionally incorporates a specialized refinement module. Extensive experiments on The Well benchmark show that PhysiX achieves superior performance over existing foundation models and strong task-specific baselines. Our results demonstrate that PhysiX benefits from synergistic learning through joint training on diverse simulation tasks and can successfully transfer knowledge from natural videos to the physical domain. We further analyze PhysiX’s generalization to unseen domains and conduct careful ablation studies to validate the impact of each design component.", "tldr": "We propose a scalable and performant foundation model for physics simulation", "keywords": ["Physics Simulation", "PDEs", "Foundation Models", "Generative Models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4afafb3cd709174bb907be9eead996160b742e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "PhysiX is a family of autoregressive transformer models for physics simulations on 2D regular grids. A tokenizer converts data to a sequence of tokens, which are quantized. An autoregressive causal transformer performs next-token prediction and tokens are mapped back to the phyiscs space via a decoder. A refinement module corrects discretization errors. Comprehensive experiments on The Well are performed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and follows a clear structure. \n- Strong performance on benchmarks from The Well.\n- Interesting experiments and adequate ablation studies."}, "weaknesses": {"value": "- The paper does not compare to, discuss or mention previous approaches that have used causal autoregressive transformers for 2D physics simulations, such as [1,2,3].  \n\n- Experiments and results are strong, but the novelity of the approach itself is incremental at best. I would appreciate a deeper discussion on the modifications/differences to [1,2,3], ideally also quantitatively; but I also acknowledge that this might be out of the scope of the rebuttal. \n\n- I see the inclusion of the refinement module as a disadvantage rather than an advantage. If I understand correctly, once PhysiX is trained without the refinement module, a new dataset is generated based on the predictions of the trained PhysiX network on which the refinement module is trained. That is a lot of extra work and it's also something that could be done for all baselines. Therefore, for a fair comparison, I would either not include the refinement module for the comparison, or do the same for all the baselines. \n\n- Could you include an architecture benchmark table comparing GFLOPs/VRAM/throughput for PhysiX and all baselines?\n\n- U-Net and C-U-Net from The Well do not represent sota U-Net architectures. I expect that using any modern U-Net for pixel space diffusion gives improved performance, e.g. [4] I am also surprised by the bad performance of Poseidon on the benchmarks, are you sure the network had converged? What were your training hyperparameters?\n \n- The title is overselling it to me; I think limitations such as \"2D\" should be included in the title\n\n\n[1] https://arxiv.org/abs/2501.18972\n\n[2] https://arxiv.org/abs/2406.04501\n\n[3] https://arxiv.org/pdf/2410.03437\n\n[4] https://arxiv.org/abs/2301.11093"}, "questions": {"value": "- What is the advantage of the quantization of tokens for PDEs/physics simulations? I would understand that the quantization could be useful for training a probabilistic model, but PhysiX is not trained on data that would require this; in addition to that, it slows down inference, since inference has to generate each token after another, not allowing to generate multiple tokens in parallel, which is e.g. done in [1]. \n- How did you deal with the normalization of the dataset? The difference in scale, even within the same dataset of The Well can be very large. Did you use any weighting in the loss (not the stratified sampling strategy). Could you expand the training details in the appendix; adding some details like specific loss functions for tokenizer/autoregressive transformer?\n- The unified framework for the input channels makes sense; and works well when all downstream tasks have the same types of fields/channels. As you already mentioned in the paper, this makes zero-shot/finetuning more difficult, since the tokenizer always has to be finetuned. Another disadvantage is that the initial embedding can become very large. For example, the dataset active matter has 10+ fields. How big is the first embedding layer/union of all channels in practice for TheWell? \n- In line 354, \"MPP and DPOT interleave space and time modeling\", can you please clarify this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XXaKaXPUR", "forum": "g5ZDfbElUj", "replyto": "g5ZDfbElUj", "signatures": ["ICLR.cc/2026/Conference/Submission14058/Reviewer_Xbsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14058/Reviewer_Xbsc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828747887, "cdate": 1761828747887, "tmdate": 1762924543181, "mdate": 1762924543181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a foundation model for physics simulations, targeting a model that would be applicable to a relatively broad set of common PDE families. The overall approach is familiar, with transformer and CNN blocks. The proposed approach provides a few targeted improvements: better tokenization across multiple tasks and variables, a refinement layer for denoising, and transfer learning from non-science video models. The results are presented on the Well benchmark and generally compare very favorably to previous methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experiments are fairly complete, considering next-frame predictions, long-timeframe predictions, single and multi-task models, the effect of refinement, scaling across different model sizes, and generalization on unseen tasks. \n\nThe results on the Well are quite strong, improving not only against previous approaches, but continuing to show stronger evidence of generalizability across tasks/PDEs."}, "weaknesses": {"value": "At times, the discussion of Refinement Module makes it seem like a super-resolution task, going from a coarse prediction to a higher-resolution one? And it seems like this is true at least in terms of precision but Figure 2 doesn’t give me the impression that it is predicting a longer token sequence than the autoregressive model did.\n\nCompared with the decomposed attention in MPP, other studies have previously suggested advantages to using a unified autoregressive transformer versus decomposing it. To what extent does this change contribute to the improvement of PhysiX versus other approaches?\n\nRelatedly, despite section 4.3, the paper still feels like it needs an ablation test. I don’t have a good feel at the end of how important, if at all, stuff like the new tokenization and embedding approaches were."}, "questions": {"value": "How much does the additional compute required for the refinement module compare to using smaller patches in the first place? \n\nTo what extent is the approach expected to change, if at all, for 4D (3D spatial + 1D time) data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AnQX6gu2Lh", "forum": "g5ZDfbElUj", "replyto": "g5ZDfbElUj", "signatures": ["ICLR.cc/2026/Conference/Submission14058/Reviewer_mxCq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14058/Reviewer_mxCq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851438732, "cdate": 1761851438732, "tmdate": 1762924542834, "mdate": 1762924542834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a foundation-model approach to simulating physical phenomena across domains. It introduces a universal tokenizer to unify heterogeneous tasks, followed by an autoregressive Transformer and a refinement module after decoding. Experiments indicate consistent gains over baselines in most settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method handles diverse simulation tasks in a unified framework.\n* It outperforms the baseline methods in most cases."}, "weaknesses": {"value": "1. Missing long-horizon visualized results. The supplementary material visualizes at most 24 rollout steps, whereas Table 3 reports predictions up to 56 frames. Please provide additional qualitative results for long-term predictions (e.g., full-trajectory videos or densely sampled frames) to assess temporal faithfulness and smoothness. Quantitative metrics for long-horizon predictions alone are insufficient to evaluate dynamic fidelity.\n2. Clarify “long-horizon” scope and report full-length rollouts. Table 3 shows up to 56 frames. Is 56 frames the maximum sequence length in the dataset? If not, please include full-length rollout results (per-sequence) to substantiate long-term stability and error accumulation claims. An example from \n3. Inference efficiency. Please report inference speed and memory usage relative to baselines, ideally across multiple sequence lengths.\n4. Relation to TIE [1]. Since TIE is a Transformer-based neural simulator for physics simulation, including a discussion of the relations to TIE would benefit this paper.\n\n\n[1]. Shao, et al. Transformer with Implicit Edges for Particle-based Physics Simulation. ECCV 2022."}, "questions": {"value": "The approach appears to be a direct transfer of video-prediction techniques to physics simulation. While data imbalance is a concern in both simulation and video generation, please clarify which simulation-specific adaptations were introduced and how they differ from the video generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KEDu8vQeas", "forum": "g5ZDfbElUj", "replyto": "g5ZDfbElUj", "signatures": ["ICLR.cc/2026/Conference/Submission14058/Reviewer_vMxC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14058/Reviewer_vMxC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925561967, "cdate": 1761925561967, "tmdate": 1762924542429, "mdate": 1762924542429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PhysiX, a family of foundation models designed for physics simulations. The architecture comprises three key components: a U-Net–style tokenizer, an autoregressive transformer, and a ConvNeXt-U-Net refinement module.\nThese components are trained sequentially. First, the tokenizer is trained on all datasets to map spatiotemporal solutions into a universal set of discrete tokens. Next, the autoregressive transformer learns to make next-token predictions. Finally, the refinement module is trained to correct quantization errors from tokenization and reconstruct fine-scale physical details.\nPhysiX models are trained and evaluated on spatiotemporal solutions of 2D physical systems from The Well benchmark, and compared against results reported in the original Well paper. Across most tasks, PhysiX achieves lower VRMSE, demonstrating improved accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses an important research topic. Developing a foundation model that generalizes across physical systems would have great impact on the sciML community.\n2. The technical designs are reasonable, for example, temporal causality is enforced through causal padding."}, "weaknesses": {"value": "1. The primary concern is that the contribution of the work appears limited. Algorithmically, it combines established components rather than introducing fundamentally novel ideas. In terms of pretraining generalizability, the model is trained on only eight 2D physical systems from The Well. Similar efforts, such as MPP and Poseidon, already exist.\n2. Key model architecture hyperparameters are not reported, which makes it hard for reproducibility. Details regarding the training and testing procedures are also missing.\n3. The improvements on unseen simulations in Section 4.5 are minimal, making it difficult to convincingly demonstrate the benefits of pretraining.\n4. It's unclear how the next frame is generated. Is the model making multiple autoregressive next-token predictions per frame, given that each frame maps to multiple tokens? If so, how do the training and inference costs compare to those of MPP?"}, "questions": {"value": "1. What is the motivation for training a separate ConvNeXt-U-Net refinement module for each dataset? Would a single shared refinement module be sufficient? Additionally, have you evaluated the relative contributions of the tokenizer and autoregressive transformer VS the refinement module, given that the ConvNeXt-U-Net in the original paper performed well on several tasks already?\n2. Did the use of causal padding in temporal convolutions affect prediction accuracy? If so, what differences were observed?\n3. What were the exact training and evaluation setups? Specifically, was the model trained purely for next-token prediction, or in an autoregressive manner?\n4. Did the 4B-parameter models exhibit signs of overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kogthGVmFu", "forum": "g5ZDfbElUj", "replyto": "g5ZDfbElUj", "signatures": ["ICLR.cc/2026/Conference/Submission14058/Reviewer_dRQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14058/Reviewer_dRQY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943764952, "cdate": 1761943764952, "tmdate": 1762924542043, "mdate": 1762924542043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}