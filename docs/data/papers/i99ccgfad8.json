{"id": "i99ccgfad8", "number": 12743, "cdate": 1758209949246, "mdate": 1759897490080, "content": {"title": "Disentangling the Factors of Convergence between Brains and Computer Vision Models", "abstract": "Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in both spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.", "tldr": "We disentangle how architecture, training and data shape brain-like representations and hierarchy in vision models (eg DINOv3). Strikingly, the development of these vision models over training mirrors several aspects of the human brain development.", "keywords": ["NeuroAI; Brain–AI alignment; Representational alignment; Hierarchy alignment; Emergence; Vision transformers; Self-supervised learning; fMRI; MEG; Temporal dynamics; Spatial dynamics", "Cortical hierarchy; Development"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6dcb06a55d301ba642addea62e595a450a4ed4cd.pdf", "supplementary_material": "/attachment/b0df3599e6fb1fba8016d5ac2f750590ef89f3e5.pdf"}, "replies": [{"content": {"summary": {"value": "The present paper studies the factors that drive brain-model alignment. In particular, it investigates three dimensions: model size, training time, and training data. The authors train DINOv3 from scratch and compute encoding, spatial, and temporal scores. The three varied dimensions interactively impact each of these alignment metrics: (1) the largest models reaches the highest alignment, (2) the emergence of brain-like representations follows a specific chronology during training, and (3) the developmental trajectory is indexed by both structural and functional properties of the human cortex."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I found this paper to be very insightful and interesting to read. The systematic analysis of differnet model properties on brain alignment is one of the most important analyses to be done in this field. \n\nThe methods and data are very sound (although I cannot speak a some of the very specific MEG/fMRI details). The paper builds on a wealth of prior work, but complements it nicely by taking an important next step. It was easy to read and follow, and is a great thematic fit for ICLR.\n\nIn particular, I found the analyses regarding the temporal and spatial score, as well as the alignment trajectories during training, to be very interesting -- I haven't seen something like this before."}, "weaknesses": {"value": "While the authors vary three different dimensions, they don't explore the whole space (e.g. they did not look at varying the data in all model sizes). This risks of missing potential interaction effects.\n\nThe present study considers only a single model class. This is not a major problem from my side, and it is already acknowledged and discussed by the authors.\n\nMinor: one could use consistent typesetting for R (italic or not)."}, "questions": {"value": "Personally, I found the analyses regarding the training trajectories of the alignment metrics to be the most interesting part. Has there any work been done on this before already? That would help to judge the novelty of the present approach.\n\nAre images from THINGS and the Natural Scenes Dataset in the Human centered 1.7B data? If so, does this matter for the presented results?\n\nFor Figure 3: the temporal score is only based on the MEG data, the spatial score soley on fMRI data, and the encoding score on both, right? Based on this, are these three metrics directly comparable to each other? The fact that that they are in the same figure and the wording in the text seem to suggest so, but I am a bit unsure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cHpRminQhK", "forum": "i99ccgfad8", "replyto": "i99ccgfad8", "signatures": ["ICLR.cc/2026/Conference/Submission12743/Reviewer_omL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12743/Reviewer_omL2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318648127, "cdate": 1761318648127, "tmdate": 1762923562944, "mdate": 1762923562944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors train multiple different DINOv3 models with varying model sizes (s, B, L, g, 7B) and different pretraining datasets (Human centered, Satellite, Cellular). Given checkpoints of these models at different training iterations, they compare the representations with the representations of the human brain recorded with fMRI and MEG on the THINGs and natural scenes datasets. The study finds that (1) larger models converge quicker and encode higher-level ROIs more accurately, (2) longer training and (3) more human-like data (in contrast to medical/satellite data) leads to a higher degree of model–brain similarity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper studies an interesting question and the methodological comparisons between the DINOv3 models and brain measurements is sound (2 different measurements, independent datasets, 3 different metrics).\n\n* The paper is well written and explains the methods and results in an understandable way.\n\n* The alignment is studied over many different training iterations, allowing insights into how it progresses over the training time which is particularly interesting and often missing in other similar studies.\n\n* The authors not only evaluated the representational alignment but also the spatial correlation between the location in the brain and the different layers in the ViT giving insights whether the representations are constructed/progress in a similar way."}, "weaknesses": {"value": "* The study only investigates DINOv3 models and does not investigate any other SSL objective or image/text, supervised objectives. The paper is called: “Disentangling the Factors of Convergence between Brains and Computer Vision Models” but only looks at different training iterations (#samples seen), model size and dataset type (human centered, satellite, cells). Model architecture and training objectives are not studied. In previous studies [1], the objective was a key driver of alignment to human similarity judgements. Therefore, it would be pretty interesting to study how the score differs in relation to publicly available image/text models, supervised models, SSL models that only rely on masking (e.g. MAE [2], CAPI [3], AIMM [4]) and the ones relying on contrastive/multi-view objectives (SimCLR [5], MoCo [6], BYOL [7]). This is partly acknowledged in the limitations section but the title and some of the claims are too general (speaking of Computer Vision Models) given that these factors are not investigated.\n\n* Isn’t the “Impact of image type” analysis confounded by the fact that the datasets that were used to gather the fMRI, MEG data are also more human-like (THINGs and natural scenes)? I would be curious to see how that changes when the alignment is evaluated on medical/cell images and satellite images. This should be discussed in this section.\n\n* Minor typos: In line 239 a space is missing. In line 301, there is one space too much before “?”.\n\n\n[1] Muttenthaler, Lukas, et al. \"Human alignment of neural network representations.\" ICLR 2023.\n\n[2] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[3] Darcet, Timothée, et al. \"Cluster and predict latent patches for improved masked image modeling.\" TMLR 2025.\n\n[4] El-Nouby, Alaaeldin, et al. \"Scalable pre-training of large autoregressive image models.\" ICML 2024.\n\n[5] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. ICML 2020.\n\n[6] He, Kaiming, et al. \"Momentum contrast for unsupervised visual representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[7] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284."}, "questions": {"value": "* The brain “alignment” metrics are shown over the training time, but the DINOv3 framework has several mid-training/posttraining phases (gram anchoring, high resolution training). Did you investigate the effects of this training phases wrt. to the brain alignment scores?\n\n* It was observed that larger models have generally larger correlations to the human brain representations than smaller models. Did you also evaluate the distilled models from the DINOV3 released models? I would be curious whether these would also be able to match the higher prediction scores when having a larger teacher.\n\n* How did you exactly extract the representations at different layers of the model? Were the CLS token, patch tokens, register tokens used and how were they potentially aggregated?\n\n* Did you investigate any more sophisticated probes than linear probes (e.g. attention probes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VLtwy2lsmr", "forum": "i99ccgfad8", "replyto": "i99ccgfad8", "signatures": ["ICLR.cc/2026/Conference/Submission12743/Reviewer_9Mpq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12743/Reviewer_9Mpq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928547786, "cdate": 1761928547786, "tmdate": 1762923562254, "mdate": 1762923562254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates factors contributing to neural network-brain representation alignment by training variants of the self-supervised vision transformer DINOv3 with systematically varied (1) model size, (2) training amount, and (3) image type. Model representations are compared to human brain activity measured via fMRI and MEG using three complementary metrics: encoding score (overall linear similarity), spatial score (hierarchical organization across brain regions), and temporal score. The authors find that larger models trained longer on human-centric images achieve higher brain-similarity scores across all three metrics, and that different brain regions align at different training stages i.e. early sensory regions align first, followed by higher-level prefrontal regions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of fMRI and MEG with three complementary similarity metrics (overall encoding, spatial, temporal) provides a more complete characterization than prior studies relying on single modalities or metrics, which allows evaluation of alignment across both spatial mappings (cortical hierarchy) and temporal processing sequences rather than using only pre-selected layer-region pairs or fixed time windows (unlike most existing studies).\n -The analysis extends beyond the traditional ventral visual stream to include prefrontal regions (BA44, BA45, IFSa, IFSp),  show linear predictability in higher-level associative cortices, though interpretation requires careful consideration (see weaknesses part)\n- The correlational analysis linking alignment emergence speed to cortical properties (developmental expansion, thickness, myelination, intrinsic timescales) provides biological context, though this constitutes post-hoc analysis using existing atlases rather than experimental validation"}, "weaknesses": {"value": "- Limited scope (question of generalizability) -\nAll experiments use DINOv3, It's unclear whether findings generalize to: 1) architecture family with different inductive biases such as CNNs (with presumably much stronger ventral-stream-hierarchical-aligning inductive biases) 2) supervised models (what’s the role of learning objective under the context of diets needed for training?) or (3) other self-supervised methods (e.g., MAE, SimCLR). one could expect conclusions (especially regarding the training dynamics/ relative convergence speed (in terms of half-time) between the three measures) changes accordingly.\n- Ambiguity in ROI-level encoding scores\nSection 2.2 states scores can be \"summarized as the average R score across brain dimensions\" but doesn't specify whether ROI-level comparisons (Figures 5C, 6C) use: A) averaging across all layer-region pairs within each ROI, or B) selecting the best-predicting layer for each voxel then averaging those scores within the ROI. If Method A is used, prefrontal ROI scores could potentially be systematically underestimated because most layers (early to intermediate layers) that perform poorly in behavioral linear readout would be anticipatedly to also poorly predict prefrontal activity (one would expect only the latest layers are relevant). \n- Novelty?\nThe finding that larger models achieve higher brain alignment (Small < Base < Large < Giant; Figure 5) substantially replicates Huh et al. (2024, \"Platonic Representation Hypothesis\"), which tested multiple model families with varying sizes and observed similar scaling trends (with few exceptions where larger ≠ better). The present study's scope is even narrower (single architecture family DINOv3, only 4-5 size variants) to claim general principles about model size effects beyond DINOv3-specific properties. Though the region-specific analysis (Figure 5C showing disproportionate benefits for BA45 vs. V1) adds nuance\n\nMinor prose issues: \nFigure 3 axis- showing the relative score compared to the optimal score rather than raw score? consider clarifying in caption or axis label.\nLine 212 missing line break (fMRI description runs into ROIs description)\nLine 239 missing space after period\nLine 301 extra space before “?”"}, "questions": {"value": "The temporal score emerges very early in training (~0.7% half-time), but this might be trivially expected from architecture alone rather than learned through training. The DINOv3 architecture inherently processes information from early-to-deep layers, which naturally corresponds to the brain's early-to-late temporal dynamics.\nWhat is the temporal score for completely untrained (random) models? If it's already high, then the \"emergence\" during training is misleading.\n\nThe only difference training might make (given that the untrained ver seemed to give negative correlations)  is reducing the randomness in the layer-to-time mapping (i.e. randomly initialized model might just get a random assignment to each of the random-looking layers, thus resulting to low depth - time correlation score), not necessarily creating the “temporal alignment” de novo.\nThey mentioned in DI “deep layers of untrained models best predict early brain responses, which is backwards”, is this consistently observed or just suspicions (i.e. just inferred from ~0 negative correlation score)\nFigure 6C shows that non-human-centric images (satellite, cellular) produce minimal prefrontal alignment. \nDo you observe any alignment difference to prefrontal cortex (specific ROIs or relative alignment strength change) when the neural data were recorded under different behavioral tasks (e.g. those that involve judgement (recognition) (the fmri dataset) vs. passive viewing (the MEG dataset) ?\nIt would be interesting to see quantitative support for statements like \"experience plays a larger role\" in the Discussion. \nSince some findings/ disentangling attempts align with previous study (and people’s intuition) - It would be interesting to dive into how these factors actually interact with each other. e.g.  Does the training amount effect depend on the model size? Is there a minimum model complexity (albeit difficult to define) where adding more training stops helping as much? Can you compensate for a smaller model by training it much longer, and if so, what's the tradeoff? if not, what were the minimal components of models necessary for training to be contributive to brain-alignment (representation level) ?\nWill these findings always model/ training diet-specific? (will quantitative attempts always fail when we change these factors?) Can any general qualitative conclusion be drawn from this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TnZ4YWTTvH", "forum": "i99ccgfad8", "replyto": "i99ccgfad8", "signatures": ["ICLR.cc/2026/Conference/Submission12743/Reviewer_bhwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12743/Reviewer_bhwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209366874, "cdate": 1762209366874, "tmdate": 1762923561801, "mdate": 1762923561801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes how variants of DINOv3, that were trained from scratch while independently varying model size, training duration, and image type, align with human brain activity. Then using  fMRI and MEG, they quantify alignment via three complementary metrics: an overall encoding score (linear predictability), a spatial score (hierarchical correspondence across cortical regions), and a temporal score (layer-to-latency correspondence). They find that all three factors matter, with the strongest brain similarity achieved by larger DINOv3 models trained longer on human-centric images, and that spatial and temporal hierarchies in the model mirror those observed in cortex.\n\nThis alignment follows a developmental trajectory during training: early layers quickly align with early visual areas and fast MEG responses, whereas later layers only align with higher-order (including prefrontal) regions after substantially more training. The speed (half-time) at which regions become linearly decodable from DINOv3 correlates with cortical properties"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the paper is clear and easy to follow. It’s written in a straightforward way and focuses on a well-defined question: how certain factors within DINOv3 (model size, training time, and data domain) affect its ability to linearly predict human brain activity from fMRI and MEG. The methodology is clean, and the factorial setup makes the analysis intuitive.\n\nThis is a very exciting topic and the authors do a good job in  connecting representation learning with neuroscience. Also , they chose sensible axes for exploration. The three proposed metrics (encoding, spatial, and temporal) are complementary and help make the results interpretable. I like that they include both spatial (fMRI) and temporal (MEG) comparisons, which makes the findings richer and more biologically grounded. The “developmental trajectory” result is an interesting angle and ties well with biological plausibility."}, "weaknesses": {"value": "The title should make clear that the analysis is only done on DINOv3, not on vision models in general. \n\nI think the rationale for picking DINOv3 as the model to analyze isn’t well justified. There’s no comparison to other strong baselines that are already known to align well with brain data (e.g., ResNet, CORnet-S, CLIP, SimCLR). Without that, it’s hard to know if the observed effects are special to DINOv3 or just generic to large self-supervised ViTs.\n\nAnother issue is that the paper defines its own metrics and only applies them to DINOv3,  so it risks being a bit circular. It would be stronger if the same metrics were applied across multiple models to show whether DINOv3 is actually distinctive.\n\nA control including the metrics  for untrained or random-weight models would be great. I think  including these would help confirm that the correlations aren’t just due to low-level image statistics or architectural biases (for example, ViTs already encode spatial frequency patterns similar to early visual cortex even before training).\n\nFinally, the overall impact is somewhat limited because the findings: larger models trained longer on naturalistic images better align with brain data, are consistent with earlier work (e.g., Yamins & DiCarlo 2016; Schrimpf et al. 2018; Cichy et al. 2016). The novelty mainly comes from the within-family comparison and the correlations with cortical properties, so it would help if the authors clarified this and avoided overselling the generality."}, "questions": {"value": "Scope clarification:\nWhy was the analysis restricted to DINOv3? Do you expect the same patterns (on size, training, and data domain) to hold for other self-supervised vision transformers or CNN-based models?\n\nChoice of DINOv3:\nWhat motivated selecting DINOv3 as the reference model for brain alignment? Was it chosen because of prior evidence of brain-like representations, or mainly because of its training flexibility?\n\nBaselines:\nHave you considered including other models (e.g., ResNet, CLIP, MAE, or supervised ViTs) in the same evaluation framework to check whether the observed effects are specific to DINOv3 or more general?\n\nMetric validation:\nHow sensitive are your conclusions to the three metrics (encoding, spatial, temporal)? For instance, would a representational similarity analysis (RSA) or decoding-based metric lead to the same developmental trajectory findings?\n\nUntrained controls:\nCould you include results for untrained or partially trained DINOv3 checkpoints? This would help isolate the contribution of training versus architectural priors in explaining the fMRI/MEG alignment.\n\nSpatial hierarchy measure:\nThe spatial score is based on Euclidean distance from V1, which is a pretty coarse proxy for cortical hierarchy. Did you test other spatial gradients (e.g., the principal sensory-to-transmodal gradient or visual hierarchy maps) to verify that the effect holds?\n\nInterpretation of “human-centric data” effect:\nHow do you distinguish whether the advantage of human-centric images comes from their semantic content (ecological relevance) versus low-level statistics or augmentations used during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T9cpWzb0Yo", "forum": "i99ccgfad8", "replyto": "i99ccgfad8", "signatures": ["ICLR.cc/2026/Conference/Submission12743/Reviewer_eWiA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12743/Reviewer_eWiA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12743/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229230814, "cdate": 1762229230814, "tmdate": 1762923561375, "mdate": 1762923561375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}