{"id": "fLalhwqRzz", "number": 8253, "cdate": 1758076344053, "mdate": 1759897796191, "content": {"title": "Benchmarking and Advancing Quantization-Aware Training for Reasoning Models", "abstract": "Reasoning models have excelled at complex tasks such as coding and mathematical competitions, yet their reasoning processes suffer from low inference efficiency. Quantization is a popular way to boost efficiency, but prior work shows that it causes large performance drops in these models. To address this, we comprehensively benchmark the quantization-aware training (QAT) for reasoning models. Our key findings are: (1) knowledge distillation serves as a versatile objective for reasoning models trained with either supervised fine-tuning or reinforcement-learning algorithms; (2) post-training quantization (PTQ) provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) QAT with reinforcement learning is feasible and yields additional gains for the quantized model; and (4) aligning the domain of QAT training data with the PTQ calibration data further improves the performance. Building on these insights, we propose Reasoning-QAT, an optimized QAT workflow tailored to reasoning models. Empirical results show that Reasoning-QAT outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on the DeepSeek-R1-Qwen-Distill-1.5B model, Reasoning-QAT surpasses FlatQuant by 2.92\\% under W4A4KV4 quantization and GPTQ by 4.74\\% under W3G128 quantization, respectively.", "tldr": "We provide a set of key insights on how to improve the quantization-aware training for reasoning models.", "keywords": ["Quantization-aware training", "Reasoning models", "Large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a8dc60516ca721603e6edad12e81ee001328679.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Quantization can produce large drops in reasoning benchmark performance, particularly for small models. This work comprehensively benchmarks quantization-aware training (QAT) for reasoning models, offers some insights into best practices for QAT, and introduces Reasoning-QAT, an optimized QAT workflow tailored to reasoning models which outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors provide the first comprehensive benchmark of quantization-aware training, according to them, and I have no reason to doubt it. This is valuable.\n* The authors provide sufficient experimental evidence to convince me that, at least under certain conditions, Reasoning-QAT improves on the baselines, and they selected strong recent baselines for comparison, good base models, good choice of benchmarks.\n* The ablation Table 5 is very nice.\n* I think the layout of this work is rather nice. I like the way the authors highlighted and numbered their research questions, and the esthetic effect of the colors and figure placement is good."}, "weaknesses": {"value": "* The inconsistency of the experimental settings between figures and tables is vexing; it makes it harder to figure out what is really going on in this paper. For instance, Table 1 compares SFT and KD to an RTN baseline, but Figure 1 uses GPTQ as a point of comparison, but keeps the model architectures the same. There's no reason not to include both RTN and GPTQ in both Table 1 and Figure 1. In Table 4, Qwen3-4B W4A4KV4 is missing QuaRot (only shows FlatQuant), all other models show QuaRot for W4A4KV4. Why does Table 4 compare W4A4KV4 to W3G128 instead of comparing 4-bit to 4-bit or 3-bit to 3-bit? Is it because the authors are trying to control for the difficulty of the quantization task? This would benefit from a clearer explanation.\n* The W4A4KV4 version of Reasoning-QAT is dependent on FlatQuant's specific architecture and transformation matrices. This is a potential confound, as FlatQuant is also a baseline.\n* While the subject is worthy of some study, the authors may be somewhat overstating the centrality of QAT. While the research literature does show that quantization can cause large performance drops in reasoning models, there are a number of mitigating factors, including model size (Liu et al. (2025a) showed that the 32B model maintained 96.77% of baseline performance at 3-bit while the 1.5B model collapsed to 67.15%), PTQ algorithm (shown by the authors as well as others), quantization bit depth, and the nature of the reasoning task, with mathematical reasoning being particularly severely affected. I do not object to the authors studying a niche topic, but I think it is important for the authors to highlight somewhere (for instance, in an appendix Limitations section) that in fact, under many circumstances, PTQ with a reasonable compression scheme such as AWQ will perform just fine, even on reasoning tasks.\n* (nit) Authors should define acronyms in their figures and tables captions, e.g. W4G128 in Figure 1\n* (nit) There are too many figures and tables in the main paper, it's cluttered and it makes it hard to follow the main story. Table 2, for example, would be fine in the appendix as far as I can see, Table 5 tells a similar story but is much more comprehensive.\n* (nit) Figures 2, 3 and 4 are too small and hard to read, again, because the authors are trying to cram everything into the main paper. It isn't all needed.\n* (nit) Why is the workflow diagram on Page 7? Shouldn't that be towards the beginning in a methods paper?"}, "questions": {"value": "* If you used a different W4A4 PTQ method would the QAT approach transfer?\n* Why not show \"QAT without FlatQuant\" for W4A4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W7hyXjQLn9", "forum": "fLalhwqRzz", "replyto": "fLalhwqRzz", "signatures": ["ICLR.cc/2026/Conference/Submission8253/Reviewer_uyks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8253/Reviewer_uyks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761324005332, "cdate": 1761324005332, "tmdate": 1762920194391, "mdate": 1762920194391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this study is to analyze and improve Quantization-Aware Training (QAT)  for reasoning models. Authors first try to benchmark how much quantization hurts reasoning models in two settings W3G128 and W4A4. They systematically ablate various design choices for doing QAT and then propose a reasoning QAT recipe that minimizes quantization loss. The 4 recommendations involves \n* Initialization using a Post-train Quantization (PTQ) checkpoint eg GPTQ\n* doing Knowledge distillation (KD) as opposed to reasoning SFT\n* QAT GRPO on top yields marginal benefit too\n* Matching data domain while doing PTQ calibration in first step helps"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Very comprehensive evaluations\n- Solid experiments followed by concrete recipe\n- Lots of good learning from the well formulated Research Questions like impact of PTQ caliberation data, KD > SFT etc.\n- Results show meaningful reduction in quantization accuracy loss for both the quantization settings\n- Paper is extremely well written and a delight to read."}, "weaknesses": {"value": "- There is rich literature on logit distillation as opposed to doing reasoning SFT by distilling from stronger teacher which is not adequately referenced\n- Gains from RL are unclear and it is within error for reasoning benchmarks\n- avg of 3 is not enough for evals especially for AIME25 \n- no experiments with larger models so unclear how results will translate.\n- Authors posttraining is very math focused and not a general postraining which will target multiple domains and also chat and instruction following. Its unclear if the results will translate over as it is much more tricky to get the balance right in that scenario."}, "questions": {"value": "- I would be curious if authors try on-policy KD where you first sample a response from the quantized student for a given prompt and then minimize KL For that prompt, response compared to the teacher. There is evidence in literature that it works better [1]\n\n- dataset for RL, DAPO is probably better\n\n[1]https://arxiv.org/abs/2306.13649"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7B5pwPACvH", "forum": "fLalhwqRzz", "replyto": "fLalhwqRzz", "signatures": ["ICLR.cc/2026/Conference/Submission8253/Reviewer_sihG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8253/Reviewer_sihG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985481213, "cdate": 1761985481213, "tmdate": 1762920193913, "mdate": 1762920193913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the performance degradation of reasoning-focused Large Language Models (LLMs) under extreme low-bit quantization. The authors find that standard Post-Training Quantization (PTQ) methods are insufficient for these models. They propose Reasoning-QAT, a three-stage workflow combining PTQ-based initialization, knowledge distillation (KD), and a \"cold-start\" reinforcement learning (RL) phase. Experiments show this method significantly recovers performance on several reasoning benchmarks compared to PTQ baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Addresses a Significant Problem: The work tackles the critical and timely challenge of deploying reasoning models efficiently, focusing on extreme low-bit quantization where existing methods fail.  \n\nStrong Empirical Validation: The proposed Reasoning-QAT workflow is shown to be effective and robust, consistently outperforming strong PTQ baselines across some models and benchmarks.  \n\nProvides Actionable Insights: The paper produces practical guidelines, such as the superiority of KD for QAT and the critical need for a \"cold-start\" before applying RL, which are of immediate use to practitioners."}, "weaknesses": {"value": "Limited Novelty: The core weakness is the lack of algorithmic innovation. The proposed workflow is a pipeline of existing techniques (QAT, KD, RL) applied to a new domain. While the execution is effective, it does not introduce a new fundamental concept.  \n\nOverstated Contribution: The paper's claim to be a \"comprehensive benchmark\" is not supported by the limited scope of the experiments, which explore only one RL algorithm and a single pipeline configuration. This overclaiming undermines the paper's credibility.  \n\nLack of Mechanistic Insight: The paper successfully shows what works but provides little explanation for why. For example, it does not offer a deep analysis of why SFT fails catastrophically on RL-trained models or explore the potential dual role of quantization noise as a regularizer in the RL phase.\n\nAmbiguous Framing: The paper's narrative is caught between being a \"benchmark\" paper and a \"novel method\" paper. This dual identity weakens its focus. If the primary goal is to be a benchmark, the scope needs to be broader. If it is to propose a new method, the novelty needs to be more clearly articulated against prior art, and the analysis needs to be deeper."}, "questions": {"value": "Clarification of Contribution: Is the primary contribution intended to be the Reasoning-QAT workflow or the benchmark itself? If it is a benchmark, can you justify the exclusion of other common RL algorithms and QAT methods?\n\nMechanism of SFT Failure: Do you have a hypothesis for the severe performance degradation when applying SFT to the RL-trained model? Could this be a form of catastrophic forgetting of the RL policy?\n\nRole of Quantization Noise: The work treats quantization noise as purely detrimental. Did you consider that this noise might serve as a form of exploration-enhancing regularization during the RL phase, as suggested by other recent work like QERL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5A35ORYPwj", "forum": "fLalhwqRzz", "replyto": "fLalhwqRzz", "signatures": ["ICLR.cc/2026/Conference/Submission8253/Reviewer_amyM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8253/Reviewer_amyM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121021288, "cdate": 1762121021288, "tmdate": 1762920193442, "mdate": 1762920193442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This provides a systematic study of applying QAT to reasoning large language models such as DeepSeek-R1 and Qwen. It benchmarks QAT under different training objectives, initialization methods, reinforcement learning integration, and data choices. Based on their findings, they propose Reasoning-QAT, a three-stage workflow that combines PTQ initialization, knowledge distillation, and cold-start reinforcement learning. Experiments show that Reasoning-QAT consistently outperforms state-of-the-art PTQ baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper studies an increasingly important problem of applying QAT to LLMs and provides an empirical benchmark.\n2) The paper is generally well-written with clear organization."}, "weaknesses": {"value": "1) The paper’s originality is limited. The proposed “Reasoning-QAT” pipeline merely stacks existing techniques (PTQ warm-start, KD, and RL fine-tuning) without introducing any new algorithmic innovation, theoretical formulation, or quantization method.\n2) The authors report accuracy improvements but offer little explanation for why the observed effects occur. For example, there is no detailed investigation into the interaction between quantization noise, KD signal, and RL reward dynamics.\n3) It will be better if authors can investigate more architectures besides DeepSeek/Qwen-family models.\n4) The authors benchmark various QAT methods; however, the proposed Reasoning-QAT is primarily compared against PTQ baselines. Since QAT inherently benefits from retraining, such a comparison is not entirely fair and weakens the convincingness of the claimed improvements."}, "questions": {"value": "1) The proposed Reasoning-QAT mainly combines PTQ initialization, KD, and RL fine-tuning. Can the authors clarify what specific interactions or dependencies exist between these stages beyond simple sequential training?\n2) Can authors compare Reasoning-QAT with SOTA QAT methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IilJodNZQg", "forum": "fLalhwqRzz", "replyto": "fLalhwqRzz", "signatures": ["ICLR.cc/2026/Conference/Submission8253/Reviewer_EgAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8253/Reviewer_EgAz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762289516513, "cdate": 1762289516513, "tmdate": 1762920192880, "mdate": 1762920192880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}