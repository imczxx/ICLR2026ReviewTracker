{"id": "fiW30OP7iY", "number": 22340, "cdate": 1758329759645, "mdate": 1759896871587, "content": {"title": "ESMfluc: Predicting Flexible Regions in a Protein Using Language Models", "abstract": "Proteins are dynamic molecular machines whose functionality emerges not merely from their static structures but critically from their intrinsic conformational flexibility. Understanding how a protein sequence encodes this flexibility is essential for deciphering the connection between sequence, dynamics, and biological function. While recent advances in deep learning and protein language models have significantly improved structural prediction, predicting sequence-encoded dynamics remains challenging. In this work, we introduce ESMfluc, a biLSTM model trained on molecular dynamics simulation data, utilizing embeddings from the Evolutionary Scale Modeling (ESM) architecture to predict local flexibility directly from protein sequences. Using fluctuation data derived from extensive molecular dynamics simulations, ESMfluc accurately identifies flexible residues without computationally expensive simulations while providing interpretability via attention maps. The model notably highlights distal flexible regions relevant for allosteric regulation and drug targeting. Our approach demonstrates substantial improvements over traditional flexibility proxies, offering researchers a computationally efficient method to reveal critical functional sites beyond active or binding regions.", "tldr": "We predict flexible regions in proteins through fine tuning a large language model on molecular dynamics data.", "keywords": ["Protein language modeling", "BiLSTM", "Attention", "Molecular Dynamics", "Protein flexibility", "Evolutionary Scale Modeling"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0cbcb36d835f4485ec3632375964715b7e722cac.pdf", "supplementary_material": "/attachment/40d4b31cf6ad221577978a6b6991cb1c3053643e.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose to use a biLSTM model with an attention layer to predict flexible regions in proteins.\nThey model the problem as a per-amino-acid binary classification problem in two classes: rigid and flexible.\nThe model inputs are embeddings from a pretrained ESM2 model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors make efficient use of existing models and technology."}, "weaknesses": {"value": "The title talks about language models, but there are none in the paper.\n\nIn the end, the contribution of this work amounts to training a biLSTM+Attention model on a binary sequence-element classification task.\nIn my opinion, this is not enough to warrant reading by the ICLR audience."}, "questions": {"value": "Why did you group all `N_{eq} > 1` into one class?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EJpCyleyw4", "forum": "fiW30OP7iY", "replyto": "fiW30OP7iY", "signatures": ["ICLR.cc/2026/Conference/Submission22340/Reviewer_KXNV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22340/Reviewer_KXNV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063923554, "cdate": 1761063923554, "tmdate": 1762942176140, "mdate": 1762942176140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate a sequence-based model for predicting local protein flexibility, using frozen ESM-2 embeddings followed by a lightweight BiLSTM and attention classifier.\nThe authors employ the ATLAS molecular dynamics dataset, deriving binary flexibility labels, and claim superior performance compared to structure-based predictors.\nHowever, the entire framework is essentially a direct application offering no methodological novelty.\nFurthermore, the experimental setup raises serious concerns about the scientific validity of the reported results."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper tries an interesting direction: linking protein sequence representations with dynamic flexibility signals derived from molecular dynamics simulations."}, "weaknesses": {"value": "1. Predicting molecular dynamics–derived flexibility directly from sequence embeddings seems to be scientifically weak. While amino acid composition and local motifs indeed encode limited flexibility trends, MD-derived quantities such as RMSF or Neq reflect complex structure- and environment-dependent fluctuations that cannot be reliably inferred from sequence alone.\n2. Methodologically, the paper presents no genuine innovation. The designed framework merely stacks a BiLSTM and a single attention layer on top of frozen ESM-2 features, without introducing any new architecture, loss formulation, or theoretical insight.\n3. The experimental setup is also flawed.\n    * the binarization of flexibility labels to 0/1 is arbitrary and discards most quantitative signal\n    * random data splitting also raises potential issues of family-level data leakage\n4. The writing quality is weak. References are frequently misused, with \\citep and \\citet incorrectly used throughout, resulting in broken sentence structures. In addition, this paper contains typos such as line 165 and line 332 suggesting inadequate proofreading."}, "questions": {"value": "All my concerns about this paper is listed in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "avAtnYtXJk", "forum": "fiW30OP7iY", "replyto": "fiW30OP7iY", "signatures": ["ICLR.cc/2026/Conference/Submission22340/Reviewer_A8jB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22340/Reviewer_A8jB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930600517, "cdate": 1761930600517, "tmdate": 1762942175896, "mdate": 1762942175896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ESMFluc is a protein sequence model that is trained to directly predict the dynamics of a protein. More specifically, it predicts the residue-level flexibility metrics derived from MD simulations. This enables the model to identify flexible regions without any structural inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors show that ESMFluc outperforms the NetSurfP disorder predictor from static structures, showing the utility of sequence models over structure models for disorder prediction. The authors also provide classical machine learning baselines that validate the effectiveness of ESM features."}, "weaknesses": {"value": "This paper is somewhat narrow in scope. It train and evaluate on the ATLAS dataset of all-atom MD trajectories but does not provide downstream applications for the flexibility predictor."}, "questions": {"value": "Have you evaluated your flexibility predictor on downstream tasks, such as intrinsically disordered protein prediction [1][2]?\n\n[1] Direct prediction of intrinsically disordered protein conformational properties from sequence. Jeffrey M. Lotthammer, Garrett M. Ginell, Daniel Griffith, Ryan J. Emenecker & Alex S. Holehouse.\n[2] Critical assessment of protein intrinsic disorder prediction. Marco Necci, Damiano Piovesan, CAID Predictors, DisProt Curators & Silvio C. E. Tosatto.\n\nWhy are you using a biLSTM as opposed to attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eSHWwOWbMP", "forum": "fiW30OP7iY", "replyto": "fiW30OP7iY", "signatures": ["ICLR.cc/2026/Conference/Submission22340/Reviewer_BBuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22340/Reviewer_BBuE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940683391, "cdate": 1761940683391, "tmdate": 1762942175663, "mdate": 1762942175663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ESMFluc, a model built upon ESM-2 to directly predict residue-level flexibility from protein amino acid sequences. The flexibility labels are preprocessed by binarizing the $N_{eq}$ values from the ATLAS dataset. Based on embeddings extracted from ESM-2, ESMFluc adds a biLSTM and an attention module to predict the binary flexibility class. Several prediction module designs (FC, LSTM, BiLSTM, and their combinations with attention modules) are evaluated. Comparisons with classical machine learning models, including logistic regression and random forests, demonstrate superior prediction accuracy. Compared with the structure-based approach NetSurfP, ESMFluc shows a clear advantage in both AUROC and Spearman metrics. Analysis of the attention weights reveals that a residue tends to attend to other residues with similar secondary structure classes and flexibility labels, even when they are distant in the sequence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of attention homophily is interesting—it shows how residues with similar flexibility contribute to each other’s prediction results.\n2. The paper clearly presents the methods, including detailed descriptions of the dataset and experimental setup.\n3. The results reveal, to some extent, that protein sequences contain intrinsic information about structural flexibility. At least, a mapping between sequence and flexibility can be effectively learned using ESMFluc."}, "weaknesses": {"value": "1. The paper focuses on binary classification during training but does not provide a strong motivation for binarizing the $N_{eq}$ labels from the original dataset. As such, the model demonstrates the capability to distinguish rigid versus flexible residues, but it remains unclear whether it can effectively capture different degrees of flexibility.\n2. The evaluation lacks comprehensive baselines. Many pretrained protein models could be fine-tuned for the task, such as ESM-3. The conclusion that sequence-only modeling (as in ESM-2) is sufficient for flexibility prediction would be more convincing if additional backbone models—including those trained for structure prediction—were also evaluated.\n3. Some details of the evaluation are missing. One important question is how NetSurfP was applied to the curated dataset. Was it evaluated in a zero-shot manner, or was it trained/fine-tuned using the $N_{eq}$ labels?\n4. The paper’s current presentation lacks emphasis on its main contributions. For example, the methods section devotes extensive discussion to dataset preprocessing but much less attention to the design of the model architecture."}, "questions": {"value": "1. How was NetSurfP used in the evaluation?\n2. Since $N_{eq}$ is inherently a continuous variable, why was it necessary to convert it into a classification problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "grPvt0aG6t", "forum": "fiW30OP7iY", "replyto": "fiW30OP7iY", "signatures": ["ICLR.cc/2026/Conference/Submission22340/Reviewer_PYQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22340/Reviewer_PYQX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974176104, "cdate": 1761974176104, "tmdate": 1762942175450, "mdate": 1762942175450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}