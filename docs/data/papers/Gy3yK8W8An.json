{"id": "Gy3yK8W8An", "number": 2429, "cdate": 1757083580412, "mdate": 1759898148486, "content": {"title": "Code2MCP: Transforming Code Repositories into MCP Services", "abstract": "The Model Context Protocol (MCP) aims to create a standard for how Large Language Models use tools. However, most current research focuses on selecting tools from an existing pool. A more fundamental, yet largely overlooked, problem is how to populate this pool by converting the vast number of existing software projects into MCP-compatible services. To bridge this gap, we introduce Code2MCP, an agent-based framework that automatically transforms a GitHub repository into a functional MCP service with minimal human intervention. Code2MCP employs a multi-agent workflow for code analysis, environment setup, tool function design, and service generation, enhanced by a self-correcting loop to ensure reliability. We demonstrate that Code2MCP successfully transforms open-source computing libraries in scientific fields such as bioinformatics, mathematics, and fluid dynamics that are not available in existing MCP servers. By providing a novel automated pathway to unlock GitHub, the world's largest code repository, for the MCP ecosystem, Code2MCP serves as a catalyst to significantly accelerate the protocol's adoption and practical application. The code is public at \\url{https://anonymous.4open.science/r/Code2MCP-5B47.", "tldr": "A framework that automatically transforms any GitHub repository into a standardized tool (an MCP service).", "keywords": ["Agent", "Model Context Protocol", "MCP", "Large Language Model", "LLM", "Multi-Agent", "Github"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6c31278aca77aa1b7eff548d1d233f893e0bc92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Code2MCP, a multi-agent workflow that clones a repository, reconstructs its environment, analyses core functionality, generates MCP-compliant adapters, and iteratively repairs failures through a run–review–fix loop before packaging a pull request."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Quantitative evidence is restricted to the six manually selected repositories in Table 2, and “manual time” is only estimated by experts rather than measured, leaving real productivity gains unverified.\n- There are no ablations on agent roles, no comparisons to alternative pipelines (e.g., OpenAgents, RepoMaster, RAG-MCP), and no reporting of failure cases or acceptance by upstream maintainers.\n- Evaluation omits real benchmark outcomes (e.g., tool success on downstream agent benchmarks or broader GitHub samples); with only qualitative walkthroughs, it is unclear how much capability is actually delivered to MCP consumers."}, "weaknesses": {"value": "- Quantitative evidence is restricted to the six manually selected repositories in Table 2, and “manual time” is only estimated by experts rather than measured, leaving real productivity gains unverified.\n- There are no ablations on agent roles, no comparisons to alternative pipelines (e.g., OpenAgents, RepoMaster, RAG-MCP).\n- Evaluation omits real benchmark outcomes (e.g., tool success on downstream agent benchmarks or broader GitHub samples); with only qualitative walkthroughs, it is unclear how much capability is actually delivered to MCP consumers."}, "questions": {"value": "1. Can you share quantitative results on a broader repository set, including measured human baselines, success/failure rates, and PR acceptance statistics.\n2. Can you add ablations of the agent roles/run–review–fix loop and comparisons with OpenAgents, RepoMaster, or RAG-MCP?\n3. Are there standardized benchmarks or downstream MCP tasks showing end-user gains beyond the current walkthroughs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PyRMLLYMTN", "forum": "Gy3yK8W8An", "replyto": "Gy3yK8W8An", "signatures": ["ICLR.cc/2026/Conference/Submission2429/Reviewer_nM4C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2429/Reviewer_nM4C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851377832, "cdate": 1761851377832, "tmdate": 1762916234751, "mdate": 1762916234751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Code2MCP, a framework that automatically converts GitHub repositories into MCP services, addressing a critical \"supply-side\" bottleneck in the LLM tool ecosystem. \nIt implements a multi-agent system, using 7 specialized agents working collaboratively. It applies a iterative debugging mechanism that ensures end-to-end reliability by automatically detecting, diagnosing, and fixing errors throughout the conversion process. It proves to convert highly complex and diverse scientific libraries into MCP services with little manual effor (4-8x speedup vs. manual conversion (12-50 minutes vs. 1-5 hours)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall I think this paper is trying to solve a very unique and novel problem: how to automatically convert a github repo into a MCP server. The key originality lies in identifying and formalizing the \"supply-side bottleneck\" in tool-augmented LLMs. While prior work focused on tool selection (consumption), this paper tackles tool creation (supply). This reframing is intellectually valuable even if somewhat overstated. While the motivation of this question might need further discussion, the problem it self is intersting and definitely worth studying, considering that in the future more and more workflow will be introduced into LLM ecosystems. The paper propose a relative sound technical solution to build up the MCP server, which proves to be effective on several scientific systems."}, "weaknesses": {"value": "1. Only 6 repositories tested. No systematic analysis of failure modes, success rates across repository types, or boundary conditions where the approach breaks down. Not all Github repos can be converted into MCP servers. Some are just scripts or learning projects, which makes the argument of 268 million GitHub repos not as convincing. Success on 6 carefully selected repos doesn't demonstrate scalability to the broader GitHub ecosystem. What percentage of scientific repos can be successfully converted?\n2. No comparison with simpler alternatives (template-based wrapping, manual conversion by different skill levels, GPT-4 with basic scaffolding). The \"manual time\" baseline uses developer estimates rather than measured time-on-task, weakening the efficiency claims. I would suggest at least let some graduate students try to do the conversion and measure the time.\n3. No evaluation of generated tool quality beyond \"tests pass\". From my understanding, the test suite is also generated by LLMs. I'm concerned about the functionality correctness of the generated artifacts. \n4. It's worth briefly discussing the security concerns of generated MCP servers."}, "questions": {"value": "1. You demonstrate success on 6 carefully selected repositories. What is the success rate when applied to a broader, randomly sampled set of repositories?\n2. What are the primary failure modes, and what percentage of attempts fall into each category?\n3. Beyond \"tests pass,\" how do you assess the quality and security of generated MCP services?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WUnlxZN9Li", "forum": "Gy3yK8W8An", "replyto": "Gy3yK8W8An", "signatures": ["ICLR.cc/2026/Conference/Submission2429/Reviewer_ooW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2429/Reviewer_ooW6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893761796, "cdate": 1761893761796, "tmdate": 1762916234423, "mdate": 1762916234423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to address the supply-side problem in the MCP ecosystem—how to automatically create standardized, agent-ready tools from existing open-source code. The authors propose Code2MCP, a multi-agent framework that converts GitHub repositories into functional MCP services with minimal human intervention. The system includes specialized agents for code analysis, environment setup, interface generation, and validation, coordinated through a Run–Review–Fix self-correction loop to improve reliability. Experiments on several scientific libraries show that Code2MCP reduces the time and effort required for manual conversion and provides a systematic approach to expand the MCP tool pool for large language model agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This is a timely and important problem in the MCP and LLM-agent ecosystem—the shortage of standardized and accessible tools. By focusing on the often-overlooked “supply-side” challenge, the work contributes to a crucial and relevant area of current research.\n2. The proposed multi-agent framework, coordinated through a Run–Review–Fix self-correction loop, represents an innovative and well-structured approach to automating the creation of MCP services. This design improves reliability compared to traditional one-pass automation pipelines and provides a clear, modular structure for handling complex code conversion tasks.\n3. The experimental evaluation covers multiple scientific domains and demonstrates consistent gains in both efficiency and functional accuracy. These results support the feasibility and scalability of the framework in diverse application settings. The paper is clearly written, with a coherent presentation of motivation, methodology, and results. The accompanying figures effectively illustrate both the conceptual structure of the framework and concrete examples of its outputs."}, "weaknesses": {"value": "1. The experiments are a bit limited in scope.  There’s no quantitative evidence on overall success rates or how robust the system is when dealing with projects of different sizes, structures, or dependency setups.\n2. The paper does not include a systematic analysis of failure cases. It is unclear what types of projects or environments most often lead to unsuccessful conversions, or how the self-correction mechanism responds when persistent errors occur. \n3. The comparison to other automation systems is quite limited. The paper talks about how Code2MCP differs conceptually from previous tool-generation frameworks, but it doesn’t provide concrete results to show how its performance actually stacks up against existing methods."}, "questions": {"value": "See weakness please"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pq0tgmbrTD", "forum": "Gy3yK8W8An", "replyto": "Gy3yK8W8An", "signatures": ["ICLR.cc/2026/Conference/Submission2429/Reviewer_JFzj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2429/Reviewer_JFzj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901216067, "cdate": 1761901216067, "tmdate": 1762916234231, "mdate": 1762916234231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces code2mcp, an LLM-based pipeline that translates an existing code repository into Model Context Protocol (MCP) services. The goal is to reduce manual effort in MCP service creation by leveraging prompt-driven code understanding and conversion. Preliminary experiments suggest that the approach can automate portions of MCP service generation and yield useful results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Automating the transformation of code repositories into MCP services is a timely and practically relevant problem as LLM-enabled tooling and MCP adoption accelerate.\n\n2. The paper articulates a clear need to lower the manual engineering burden for MCP service development, positioning the work as a bridge between legacy codebases and MCP-compatible tools.\n\n3. The experiments provide initial evidence that the proposed workflow can produce functional MCP services, indicating potential for real-world impact if matured."}, "weaknesses": {"value": "1. The method appears to rely primarily on prompt engineering for LLM-driven translation, with limited description of algorithmic design, safeguards, or adaptation (e.g., fine-tuning, tool-augmented parsing, static analysis). Clarify the technical contributions beyond prompting (e.g., repository analysis modules, interface inference, schema mapping). Add ablations contrasting prompt variants, tool use, and (if applicable) fine-tuned models.\n\n2. Table 2 reports results from only three participants, which undermines statistical reliability and generalizability. Increase the sample size, report inter-rater agreement, and provide confidence intervals or significance tests. Include participant expertise profiles (e.g., MCP familiarity, software engineering background).\n\n3. The paper does not analyze which categories of repositories are suitable candidates for conversion (e.g., libraries vs. services, I/O patterns, dependency complexity). Provide a taxonomy of repository types, preconditions for successful conversion, and decision guidelines on when not to convert.\n\n4. The paper suggests broad convertibility without reporting conversion success rates, error types, bug counts, or remediation workflows. Report per-repository success/failure, categorize common failure modes (e.g., interface mis-inference, dependency resolution), quantify bugs found, and document the debugging loop"}, "questions": {"value": "1. What is the concrete technical novelty of code2mcp beyond prompt engineering, and how do you verify MCP-spec compliance and enforce least-privilege security by design?\n\n2. Can you strengthen the evaluation with solid baselines/ablations, a larger human study with inter-rater agreement, clear success criteria, and cost/latency and reproducibility analyses across repository types?\n\n3. What are the limits and failure modes (including which repos should not be converted), with per-repo success/bug statistics and an automated repair loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pXXqJ99KG0", "forum": "Gy3yK8W8An", "replyto": "Gy3yK8W8An", "signatures": ["ICLR.cc/2026/Conference/Submission2429/Reviewer_aB4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2429/Reviewer_aB4H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968935699, "cdate": 1761968935699, "tmdate": 1762916234039, "mdate": 1762916234039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}