{"id": "UJB2uOS9MR", "number": 23642, "cdate": 1758346698035, "mdate": 1759896803623, "content": {"title": "GQA-$\\mu$P: The Maximal Parameterization Update for Grouped Query Attention and Fully Sharded Data Parallel", "abstract": "Hyperparameter transfer across model architectures dramatically reduces the amount of compute necessary for tuning large language models (LLMs). The maximal update parameterization (μP) ensures transfer through principled mathematical analysis but can be challenging to derive for new model architectures. Building on the spectral feature-learning view of Yang et al. (2023a), we make\ntwo advances. First, we promote spectral norm conditions on the weights from a heuristic to the definition of feature learning, and as a consequence arrive at the Complete-P depth and weight-decay scalings without recourse to lazy-learning. Second, we consider a modified spectral norm that preserves the valid scaling law of network weights when weight matrices are not full rank. This enables (to our knowledge, the first) derivation of μP scalings for grouped-query attention (GQA). We demonstrate the efficacy of our theoretical derivations by showing learning rate transfer across the GQA repetition hyperparameter as well as experiments regarding transfer over weight decay.", "tldr": "We introduce a novel extension of the spectral muP theory and use it to re-derive Complete-P, as well as to derive GQA scalings.", "keywords": ["muP", "gqa", "llm", "pre-training"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baae6e9f5aa9c54f81b3becc1709fbc07cb25b56.pdf", "supplementary_material": "/attachment/ba4f811f6dc1c09ad72dd01de61f105eabfe057d.pdf"}, "replies": [{"content": {"summary": {"value": "The authors consider hyperparameter transfer across model architecture sizes, in particular, how to flexibly adapt the maximal update parameterization for modern architectures which naturally exhibit low rank weight matrices. In order to adapt the framework to such settings, they consider spectral norms conditions as the definition of feature learning, which makes a subtle weakness of the definition via activations apparent. In addition, they define a spectral norm which preserves scaling for sparse matrices by measuring expected transformation rather than maximal transformation as possible with full rank matrices. Together with empirical experiments they show that the hyperparameter transfer works for grouped query attention across the number of repetitions and weight decay."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting contribution to making $\\mu$P parameterisation more precise for specific modern architecture types.\n- Clear presentation of the intuition and results.\n- New derivation of previous work on depth and weight decay scaling using a different method is insightful.\n- The idea of having the norm of scaling in expectation seems like it might be useful elsewhere."}, "weaknesses": {"value": "- It is unclear how significant the difference between the vanilla muP scaling and the new method are in practice. The main advantage (as also anticipated from Fig.1) seems to occur for very small $r$. I have difficulty in understanding intuitively whether this is expected or why the problem with the vanilla scaling does not seem to hold for large r anymore. I might miss a point here, so it would be great to clarify this.\n- The code is not available."}, "questions": {"value": "Typos:\n- L.431 : . .\n- Table 1 math mode in some of the lines"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rJ1RMhC1Ly", "forum": "UJB2uOS9MR", "replyto": "UJB2uOS9MR", "signatures": ["ICLR.cc/2026/Conference/Submission23642/Reviewer_fnei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23642/Reviewer_fnei"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761341021295, "cdate": 1761341021295, "tmdate": 1762942741686, "mdate": 1762942741686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “GQA-μP: The Maximal Parameterization Update for Grouped Query Attention” extends the theoretical foundation of the maximal update parametrization (μP) by proposing a generalized mathematical framework that introduces the expected operator norm to handle low-rank weight structures, such as those found in Grouped Query Attention (GQA). The authors begin with the standard μP scaling condition on weight matrices $W_\\ell \\in \\mathbb{R}^{n_\\ell \\times m_\\ell}$ given by $||W_\\ell^0|| = \\Theta(\\sqrt{n_\\ell/m_\\ell})$ and $||\\Delta W_\\ell^t|| = \\Theta(\\sqrt{n_\\ell/m_\\ell})$ They argue that while this condition ensures feature learning for full-rank operators, it fails for rank-deficient matrices like those in GQA, since the spectral norm grossly overestimates the true deformation of random inputs and the event has measure zero, in a simple sense. To resolve this, they define the expected operator norm as $||A||_{E,\\Omega,p} := \\mathbb{E}{x \\sim \\Omega} \\left[ \\frac{||Ax||_p}{||x||_p} \\right]$. For full-rank matrices, $||A||_E = \\Theta(||A||)$ asymptotically. \n\nUnder this proposed norm, with some basic conditions on the input, the paper re-derives feature learning, enforcing the stronger spectral condition directly on weights rather than activations. They extend this framework to include weight decay under AdamW updates $\\Delta W_t = -\\lambda \\eta W_t - \\eta \\hat{r}_t$, showing that correct scaling requires $\\lambda^0 = \\Theta(1)$ for input layers and $\\lambda^l = \\Theta(n)$ for hidden/output layers. The analysis of residual architectures recovers the Complete-P depth scaling, proving that residual strength $\\beta = \\Theta(L^{-1})$ keeps $||\\bar{G}_t^\\ell|| = \\Theta(1)$ across layers. \n\nAs a core contribution, for Grouped Query Attention, where keys/values are repeated $r$ times over $p$ distinct heads ($H/p = r$), the authors derive the corresponding parametrization when the concatenated matrices are used (these concatenated matrices follow the $\\sqrt{r}$ scaling for the norm of the corresponding matrix in each group $i, i\\in [r]$).  \n\nEmpirical experiments confirm that this GQA-μP scaling restores learning rate transfer across repetitions and maintains consistent training dynamics, unlike standard μP implementations. The paper thus unifies spectral and expected-norm perspectives, re-derives Complete-P scaling laws, and delivers the first μP-compatible parameterization for GQA, achieving transferable learning rate and weight decay across varying model configurations"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The extension of $\\mu$P to shared key value algorithms, such as GQA is certainly a useful addition to the literature, given the exploding size of the models. \n\n2. The Voroni interpolation in Figure 5 clearly demonstrates the strength of this work."}, "weaknesses": {"value": "1. A core reason that I will thoroughly stress – the Related Work section should have been put right after the Intro on the second page. For this work, which heavily compares and references the background work, I believe the current strategy to put the Related Work at the end is perhaps not the best for the reader, irrespective of the fact that the reader is familiar with the literature. \n\n2. The mismatch between the paper's title on Openreview - “GQA-$\\mu$ P: The Maximal Parameterization Update for Grouped Query Attention and Fully Sharded Data Parallel” and the one in the actual PDF - “GQA-µP: THE MAXIMAL PARAMETERIZATION UP\u0002DATE FOR GROUPED QUERY ATTENTI” is a but troubling. There is no discussion of FSDP across the whole paper. \n\n3. The authors have attached the supplementary section separately, but then also deferred all the experimental details to the supp PDF. What exactly is the reason here? \n\n4. Are the authors sure about Figure 1 on page 6? The denominator seems to be showing norm of the concatenation of A to adjust the rank lower. Should it not have been the norm of the vector “x”? Also, why was each data point averaged, when we are instead looking at max value attained for spectral norm? Additionally, averaging over merely 30 points is too low for such high dimensional cases as considered in the ML setup. \n\n5. I have some questions about the proof of lemma 2 – how is it that the proof simply expands the expectation of the multiplication of the matrix A with the vector x, ie, ||A x|, without considering the numerator of norm of the vector “x” ? Also, the last statement on page 13 about the norm of the vector “x” scaling as $\\Theta{\\sqrt{n}}$ does not make sense. There should some expectation value in this equation. \n\n6. This statement at the end of page 2 - “For neural network training all distributions have full support.” seems a bit misleading to me. A majority of the data distributions can lie on low-dimensional manifolds, or simply there can be extremely low variance subspaces around the null vector, making it problematic. Further discussions around this point will be helpful. \n\n7. Equation on line 111 is incorrect. Many notational inconsistencies such as equation of $G^l(x)$ on line 181 and equation 5 for the concatenation operation."}, "questions": {"value": "1. What exactly is the definition of “coordinate checks” that the authors mention throughout the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tPJtCiLU5a", "forum": "UJB2uOS9MR", "replyto": "UJB2uOS9MR", "signatures": ["ICLR.cc/2026/Conference/Submission23642/Reviewer_9sqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23642/Reviewer_9sqf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909475015, "cdate": 1761909475015, "tmdate": 1762942741394, "mdate": 1762942741394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Maximal Update Parameterization (μP) framework to Grouped Query Attention (GQA), enabling zero-shot learning rate transfer across complex Transformer architectures. Building on the spectral feature-learning view of Yang et al. (2023a), the authors redefine feature learning via spectral norms and introduce the expected operator norm to handle low-rank GQA matrices where standard spectral norms fail. This unified spectral μP framework yields the first derivation of μP scalings for GQA, aligning with Complete-P depth and weight-decay principles without relying on lazy-learning theory. Empirical results confirm effective transfer of learning rate and weight decay across GQA repetition factors, though with increased noise and computational cost when the number of key–value head repetitions varies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides the first known derivation of μP scalings for Grouped Query Attention (GQA). The derived scalings enable learning rate transfer for GQA models, which the vanilla Adam-μP implementation failed to achieve. \n- By relying on the spectral condition on weights as the core definition of feature learning, the authors re-derive the Complete-P depth and weight-decay scalings.  This method unifies the Complete-P extensions with the broader principles of the μP literature.\n- The empirical analysis shows that the proposed implementation significantly improves the transferability of both optimal learning rate and optimal weight decay across model scales, qualitatively and quantitatively, compared to the standard μP baseline."}, "weaknesses": {"value": "- Although the paper successfully extends μP, the necessity of introducing a specialized norm—the expected operator norm—highlights the intrinsic difficulty of applying μP to novel architectures. This suggests that deriving μP for future complex architectures may continue to be challenging. \n- While the GQA-μP implementation yields better weight decay transfer than vanilla μP, the authors note that the related quantity transfers slightly better. This suggests that the derived weight decay scaling may not be optimally tuned for zero-shot transfer across all training regimes.\n- Experiments appear limited to models ≤ 177 M. It is unclear if findings generalize to billions-scale models."}, "questions": {"value": "- Given the observed dichotomy between achieving feature learning transfer and managing increasing noise as the number of KV heads decreases, what is the practical threshold or model size/GQA setting where the accumulated noise negates the benefit of zero-shot learning rate transfer?\n- The paper states that the spectral condition is a stronger definition of feature learning than the activation condition. Can the authors provide a more detailed theoretical explanation as to why maintaining the activation norm size is insufficient for guaranteeing transfer stability in this specific rank-degenerate context (GQA)?\n- Have you tested the proposed GQA-μP scaling on >1 B-parameter models or on real pre-training datasets to verify zero-shot transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "on4lUcsyUg", "forum": "UJB2uOS9MR", "replyto": "UJB2uOS9MR", "signatures": ["ICLR.cc/2026/Conference/Submission23642/Reviewer_U6y7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23642/Reviewer_U6y7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963118118, "cdate": 1761963118118, "tmdate": 1762942741111, "mdate": 1762942741111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits the spectral norm conditions on the weight evolution in Tensor Programs, and observes conditions where the empirical and analytical checks on the existence of feature learning passes, despite failure to transfer optimal learning rates across width, $n$.\nThe authors propose an `expectation operator norm` as a more suitable norm for analysing grouped-query attention (GQA) layers that share the KV weights across the Q groups and can thus break the full rank of the GQA weight matrix. \nThey thus propose a new view for feature learning that encompasses rank degenerate architectural components such as the GQA layer.\nThe authors also re-establish existing literature findings in the role of the learning rate and weight decay interaction in width scaling being of constant scale, i.e., $\\lambda \\eta = \\Theta(1)$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear motivation and scope.\n\n* Solves a practical gap ($\\mu$P for GQA layers) using theory-driven contribution ($||A||_E$).\n\n* Easy to follow notations and math for the most part.\n\n* Reconciliation or rediscovery of previous literature findings (CompleteP depth-scaling; $\\tau$ from AdamW's EMA perspective)"}, "weaknesses": {"value": "* Despite a practice-driven motivation and problem scope, a lack of empirical evaluations to show the explicit gain in practice or improvement in zero-shot hyperparameter transfer.\n\n* Unclear how the rank degeneracy of certain architectural components (like GQA) affects the learning rate landscape overall, and also convergence rates: L1 checks pass, but LR shifts, but unclear if GQA's low rank enforces a general flatter LR landscape or not.\n\n* Some of the inline math has assumptions and jumps that can do with more referencing or explanations."}, "questions": {"value": "Below is an enumeration of various questions and suggestions.\n\nPlease note, the score increase in contingent on the points below, with more weightage on points: 4, 5, 7.\n\n1\\. L41-47: Could the authors perhaps refer here to a plot or a table in the paper for the reader?\n\n2\\. L111: Shouldn't the second term on the RHS be: $W_{t-1}$ instead of $W$?\n\n3\\. L118-134: Hard to follow and the link with the previous and after of this section.\n\n4\\. L149-152: Could the authors please break the steps here a bit more?\n\n5\\. Is Figure 4 on GQA's only? Given the usual expectation that in SP we typically see a scaling relation with width, which allows $\\mu$P to *scale* appropriately with $n$ and align the $LR_0$ at $n$. Could the authors please explain how to interpret this here?\n\n6\\. Minor typos:\n* L127: *resdiual* $\\rightarrow$ residual\n* L132: *applying* $\\rightarrow$ apply\n\n7\\. Figure 5: \n* How does one interpret the sizes of the voronoi cells here?\n* Do all the plots share the same loss scale and therefore the colors?\n* Can we observe the LR landscape wrt width, assuming optimal Weight Decay? (Or how does this play out with a tuned, fixed $\\tau$?)\n* Does the GQA-scaling appear to have larger regions of loss compared to vanilla-$\\mu$P? (top purple region)\n* Can there be a learning curve comparison too (at least in the Appendix)?\n\n8\\. L643-647: Something that has been explored in [1]?\n\n\n---\n\nReferences:\n\n[1] Scaling Exponents Across Parameterizations and Optimizers, Everett et al., 2024, arXiv:2407.05872 [cs.LG]."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BNnszyeIbP", "forum": "UJB2uOS9MR", "replyto": "UJB2uOS9MR", "signatures": ["ICLR.cc/2026/Conference/Submission23642/Reviewer_F9pN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23642/Reviewer_F9pN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005834504, "cdate": 1762005834504, "tmdate": 1762942740924, "mdate": 1762942740924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}