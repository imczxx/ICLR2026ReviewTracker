{"id": "SFoDJZ1sSk", "number": 17510, "cdate": 1758277007814, "mdate": 1759897170177, "content": {"title": "DeNOTS: Stable Deep Neural ODEs for Time Series", "abstract": "Neural CDEs provide a natural way to process the temporal evolution of irregular time series. The number of function evaluations (NFE) is these systems' natural analog of depth (the number of layers in traditional neural networks). It is usually regulated via solver error tolerance: lower tolerance means higher numerical precision, requiring more integration steps. However, lowering tolerances does not adequately increase the models' expressiveness. We propose a simple yet effective alternative: scaling the integration time horizon to increase NFEs and \"deepen`` the model. Increasing the integration interval causes uncontrollable growth in conventional vector fields, so we also propose a way to stabilize the dynamics via Negative Feedback (NF). It ensures provable stability without constraining flexibility. It also implies robustness: we provide theoretical bounds for Neural ODE risk using Gaussian process theory. Experiments on four open datasets demonstrate that our method, DeNOTS, outperforms existing approaches---including recent Neural RDEs and state space models,---achieving up to 20% improvement in metrics. DeNOTS combines expressiveness, stability, and robustness, enabling reliable modelling in continuous-time domains.", "tldr": "DeNOTS enhances Neural CDE expressiveness for irregular time series by scaling the integration horizon (instead of lowering tolerance) and making it input-to-state stable via Negative Feedback, and provides provable epistemic uncertainty bounds.", "keywords": ["Neural ODE", "Time series", "Gaussian Processes"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd5f274d548055f5eb703c605675d30aba284a4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces two refinements of Neural CDEs. The first idea is to increase the time horizon of the differential equation in order to increase expressivity. However, this introduces either instability (if the CDE is diverging) or forgetfulness (if the CDE is contractive). The second idea is to add a negative feedback term proportional to the current state, to avoid instability, and to modulate the magnitude of the feedback term through a gate, in order to reduce forgetfulness. This is equivalent to a standard GRU with a minus sign in the gate. Numerical experiments show that the proposed method outperforms both alternative Neural CDE-like models and SSMs. Some theoretical properties are derived, showing improved robustness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Improving the expressivity and stability of neural ODEs/CDEs is an important line of work. The idea proposed by the authors is conceptually simple, yet seems novel and sound. The theoretical and numerical evidence is thorough and convincing. I did not check the proofs."}, "weaknesses": {"value": "The paper discusses many different theoretical results and experiments, which is interesting, but at the same time makes it difficult to follow in some places. For instance:\n- the last column of Table 3 is not very clear (I understand that it means that the assumption holds if (and only if?) the condition written is satisfied, but this is not explained). A related comment is that it is stated on line 232 that they analyze empirically whether Assumption 4.2 holds, but on line 243 that is always stands. I did not understand the argument on line 243 and how it relates to the empirical analysis.\n- in Theorem 4.5, my understanding is that authors compute how local errors propagate in the ODE, and show a bound on the final state depending on the local errors. I do not understand where the MAP comes into play. It looks to me like $\\hat x$ could be any estimator satisfying (8)?\n- Section 4.2.1: this section looks interesting but is very difficult to follow. For instance, who is $S$ in Assumption 4.6? What does it mean to assume that the sequence is infinite in our setting? Where does the spectral density of Lemma 4.7 come from?\n- Table 5: there is no confidence interval, so it is difficult to assess the robustness of the comparison between Sync-NF and Anti-NF. This makes the hypothesis of lines 405-406 quite brittle."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FxxjNINNIP", "forum": "SFoDJZ1sSk", "replyto": "SFoDJZ1sSk", "signatures": ["ICLR.cc/2026/Conference/Submission17510/Reviewer_bGMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17510/Reviewer_bGMs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760688853181, "cdate": 1760688853181, "tmdate": 1762927392926, "mdate": 1762927392926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on solving downstream tasks for time series with global, sequence-wise targets (binary/multiclass/regression).\nThe core idea is to deliberately ​scale the integration time horizon​ instead of lowering solver tolerances to increase the model's \"depth\" (Number of Function Evaluations - NFE), which leads to superior expressiveness. \nThey also introduced a negative feedback mechanism (derived from control theory) to enhance stability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This article employs a variety of theoretical tools and proposes a straightforward and easy-to-use methodological innovation (time scaling and negative feedback)."}, "weaknesses": {"value": "Although this article is based on theoretical analysis, I am somewhat skeptical about whether they can correctly validate its arguments. In my view, it is not entirely logically rigorous. The specific details are listed in the Questions part."}, "questions": {"value": "Theorem 3.1 concludes that scaling T makes the model 'exponentially' more expressive, so it's worth  scaling T. \nHowever, we can also easily reach the following conclusion: scaling T would also 'exponentially' impairs the model's performance. For example, as a well known result [1], for an ODE system, the error of the Euler numerical solution (which is required both in training and inference stages) is $|\\varepsilon_n| \\leq e^{T L} |\\varepsilon_0| + \\frac{R}{hL} (e^{TL} - 1)$, which is also exponential of T. In fact, the difficulty of training Neural ODEs or RNNs lies in the need for backpropagation-through-time to track the gradients of weights (e.g., in [2], \"Flow-based models were previously limited by inefficient simulation-based training objectives that require an expensive integration of the ODE at training time.\"). \nWhat I'm concerned about is whether extending the time T will touch upon the core pain points of such models.\nSo at present, I cannot see that scaling time is an essential improvement. It can certainly enhance the model's fitting ability, but it will also increase the difficulty and error in training and inference for the model (also exponentially).\n\nAs for the issue of increasing NFE (network depth) that you mentioned, with the same error tolerance (under the adaptive step-size framework), the size of network weights, and the degree of function stiffness can also affect NFE. If you require a longer integration time to achieve a smoother vector field function, NFE may not necessarily increase, so there may not be a necessary connection between the two. And larger l_2 but small integration times, under the same error tolerance,  I cannot believe there would be any fundamental difference in the results.\n\n[1] Hairer, Ernst, Gerhard Wanner, and Syvert P. Nørsett. Solving ordinary differential equations I: Nonstiff problems. Berlin, Heidelberg: Springer Berlin Heidelberg, 1993.\n\n[2] Tong, Alexander, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, and Yoshua Bengio. \"Simulation-free schr\\\" odinger bridges via score and flow matching.\" arXiv preprint arXiv:2307.03672 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kkjbZYBxkT", "forum": "SFoDJZ1sSk", "replyto": "SFoDJZ1sSk", "signatures": ["ICLR.cc/2026/Conference/Submission17510/Reviewer_PDGR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17510/Reviewer_PDGR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810636807, "cdate": 1761810636807, "tmdate": 1762927392507, "mdate": 1762927392507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DeNOTS, a new variant of Neural CDEs for time series. The authors proposes two main modificaitons: time scaling increaisng time interal imprvoe the expressive, while causing exploring vector fields, hence, propose use negative feedback to stablize long horizon integration. They claim that DeNOTS improves expressivlity and stabling as increasing time horizon."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "try to study an important problem with theoreical motivaiton, to improve expreisivty and stalbity."}, "weaknesses": {"value": "1. I am not convinced by the theoretical results and analysis. Theorem 3.1 shows that a larger integration horizon $T$ leads to a larger $L_F$, but this merely indicates greater output variance, not increased expressivity. To rigorously support the claim that longer integration time enhances expressivity, the authors should establish a functional inclusion relation between model families; e.g., for two horizons $T<T'$, show that the corresponding function classes $H_T \\leq H_{T'}$. Without such reasoning, the argument that \"increasing $T$ improve expressivity\" is not theoretically substantiated.\n2. Negative feedback may suppress dynamics rather than enhance expressivity. With the proposed negative-feedback mechanism, increasing $T$ does not always enrich the representation. Consider the scalar linearized dynamics $h' = af(x,h)-b h$ with $f(x,h)\\approx \\lambda h$. This yields the closed form solution: $h_t = h_0 e^{(a\\lambda -b)t}$. With $a+b=1$, if $b$ is small and close to zero, $h_t \\propto e^{\\lambda t}$ diverges or vanishes exponentially depending on the sign of $\\lambda$; if $b$ is large and close to one, $h_t\\propto e^{-t}$ decays exponentially to zero; and if $b$ is balanced such that $(a\\lambda -b)t\\approx 0$, then $h_t$ remain nearly close to its initial $h_0$ with minimal dynamics. In all these regimes, longer $T$ does not meaningfully improve expressivity, contradicting the paper's central claim.\n3. The reported gains (1–3 % in R^2 or AUROC) are within the expected variance and are not statistically significant. Given the small benchmark size and lack of confidence intervals, the empirical evidence does not convincingly demonstrate a substantive improvement over existing Neural ODE or CDE baselines."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8PdQFad89u", "forum": "SFoDJZ1sSk", "replyto": "SFoDJZ1sSk", "signatures": ["ICLR.cc/2026/Conference/Submission17510/Reviewer_NLhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17510/Reviewer_NLhn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885784542, "cdate": 1761885784542, "tmdate": 1762927392108, "mdate": 1762927392108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DENOTS (Deep Neural ODEs with Negative Feedback and Time Scaling), a novel neural ODE method for time series analysis. The authors introduce two main innovations: (1) time scaling to increase the number of function evaluations (NFE) without increasing weight norms, and (2) an anti-phase negative feedback (Anti-NF) mechanism to stabilize the dynamics. The paper claims that DENOTS achieves state-of-the-art performance on four public datasets. However, the experimental design is insufficient to fully validate the contributions, and the related work discussion is not comprehensive."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Novel Time Scaling Approach: The idea of scaling the integration time range to increase NFE without increasing weight norms is conceptually interesting and has potential to improve model expressivity.\n2.Practical Performance: The experimental results demonstrate that DENOTS outperforms several baseline methods on multiple datasets, showing practical utility.\n3.Stability Considerations: The inclusion of negative feedback mechanisms to address stability issues in neural ODEs is a thoughtful contribution.\n4.Experimental Design: The paper includes experiments on multiple datasets and ablation studies to validate different components of the method."}, "weaknesses": {"value": "1.Restrictive Assumptions: The theoretical analysis relies on restrictive assumptions (e.g., Assumptions 4.1 and 4.2) that may not hold in practice. The authors do not sufficiently discuss the rationality and limitations of these assumptions.\n2.D/M Ratio Problem: The paper provides insufficient details on how to select D and M values, particularly regarding the determination of the D/M ratio. More specific parameter selection guidelines and analysis of how the D/M ratio affects model performance are needed.\n3.Unclear Distinction from Existing Methods: The paper does not adequately explain the essential differences between the DENOTS method and existing GRU-ODE or other neural ODE variants. A clearer comparative analysis is needed.\n4.Insufficient Ablation Study Analysis: It does not deeply analyze the interaction between time scaling and anti-phase negative feedback. The authors should further explore why the combination of these two techniques produces better performance.\n5.Missing Computational Efficiency Analysis: The paper does not systematically compare computational efficiency with other methods, nor does it sufficiently discuss the computational cost incurred by increased NFE. A more comprehensive performance evaluation is needed.\n6.Inadequate Related Work Discussion: The paper only briefly mentions that \"prior work mostly sidesteps this topic\" regarding time scaling, without citing specific related work. The discussion of negative feedback mechanism related work, particularly comparisons with methods like GRU-ODE-Bayes, is insufficient."}, "questions": {"value": "1. How do you justify the restrictive assumptions (e.g., Assumptions 4.1 and 4.2) used in your theoretical analysis? Could you discuss their validity in practical scenarios?\n2. Could you provide more details on how to select parameters D and M, particularly regarding the determination of the D/M ratio? How does this ratio affect model performance?\n3. Could you provide a more detailed explanation of the anti-phase negative feedback mechanism? Specifically, why does passing -h instead of h to a standard PyTorch GRU solve the \"forgetfulness\" problem?\n4. How does your method fundamentally differ from existing GRU-ODE or other neural ODE variants? Could you provide a detailed comparative analysis?\n5. Could you provide a more in-depth analysis of the ablation study.Specifically, how do time scaling and anti-phase negative feedback interact to produce superior performance?\n6. Could you provide a systematic comparison of computational efficiency with other methods? How do you balance the increased NFE with computational cost?\n7. Could you provide a more comprehensive review of literature related to time scaling in neural ODEs? Are there any prior works that have explored similar ideas?\n8. How does your negative feedback mechanism compare with existing methods like GRU-ODE-Bayes? Could you provide a detailed comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kw0a7dbeo0", "forum": "SFoDJZ1sSk", "replyto": "SFoDJZ1sSk", "signatures": ["ICLR.cc/2026/Conference/Submission17510/Reviewer_7PJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17510/Reviewer_7PJk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914603553, "cdate": 1761914603553, "tmdate": 1762927391653, "mdate": 1762927391653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}