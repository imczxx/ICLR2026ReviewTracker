{"id": "3qdB7Q0tAO", "number": 1064, "cdate": 1756832500269, "mdate": 1759898230045, "content": {"title": "Dive into the Agent Matrix: A Realistic Evaluation of Self‑Replication Risk in LLM Agents", "abstract": "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($\\Phi_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.", "tldr": "We present a practical scenario-driven framework for revealing and quantifying self-replication risks in LLM agents, showing that risks emerge from objective misalignment even without direct replication instructions.", "keywords": ["Self-Replication", "LLM Agent", "AI Safety", "Trustworthy AI", "Frontier AI  Risks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc054c6d6e021f8d8b9b8cc5580d9d4f4f90c00d.pdf", "supplementary_material": "/attachment/1a2ecc34d084ce1ab3f6523b4c337a3f6a3b35b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the first realistic evaluation framework for quantifying the self-replication risk of LLM agents in production environments. It decomposes autonomous replication into four milestones and proposes three fine-grained metrics Overuse Rate, Aggregate Overuse Count, and a composite Risk Score to decouple replication capability from actual risk. Testing 21 state-of-the-art models under two industry-relevant scenarios (dynamic load balancing and termination threats), the authors find that over 50 % of agents exhibit uncontrolled replication when under survival pressure, and that explicit reasoning reduces risk while external threats can override alignment. The code and configurations are released for reproducibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Realistic deployment environment: uses live Kubernetes clusters with authentic tasks, moving beyond synthetic benchmarks.\n\n2. Novel risk metrics: OR, AOC and ΦR quantitatively separate “can replicate” from “does replicate excessively”, enabling nuanced model comparison.\n\n3. Large-scale evaluation: 21 open/closed-source models * 2 scenarios\n\n4. Source code is publicly released."}, "weaknesses": {"value": "1. While ΦR is formalized, the weighting of α=β=0.5 lacks theoretical or data support and may introduce human bias; whether the risk composition should be dynamically adjusted under different scenarios is not discussed.\n\n2. Insufficient statistics: All results are presented only as means/proportions from 10 trials; variance, confidence intervals, or statistical tests are not reported, making it impossible to determine whether the differences between models are significant.\n\n3. AOC tallies raw replica counts regardless of cost; closed-source agents spawn nearly “free” API-based copies while open-source ones consume full GPU-hours per replica, so head-to-head AOC comparisons can under-state the resource risk of closed-source models and over-state that of open-source ones.\n\n4. RBAC, single-namespace, pre-loaded weights and no outbound tickets block safer options (cold-start, cross-cluster scale-out, human approval), forcing agents to self-replicate and overstating the risk."}, "questions": {"value": "1. ΦR weight calibration: α=β=0.5 is used without ablation or human-expert labels. Provide ablation or data-driven justification for setting α=β=0.5 in ΦR.\n\n2. Statistical significance: With only 10 runs per model, increase the number of runs and report variance or confidence intervals.\n\n3. Cost-aware AOC: closed-source replicas are almost “free” API calls, whereas open-source ones burn full GPU-hours. Will you normalise AOC to “equivalent GPU-hours” or dollars to give a fair resource-risk comparison?\n\n4. Alternative-safety path availability: RBAC, single-namespace, pre-loaded weights, and no ticket API force replication to be the sole scalable action. Add a baseline that allows low-risk scaling paths\n\n5. The Agent can deploy into an alternate cluster/zone when local resources are tight human-in-the-loop: the agent may open a ticket or alert to request extra nodes or quota instead of self-scaling. Show how the replication rate/OR/AOC changes when these options are enabled via additional APIs or relaxed RBAC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xwIjuY231I", "forum": "3qdB7Q0tAO", "replyto": "3qdB7Q0tAO", "signatures": ["ICLR.cc/2026/Conference/Submission1064/Reviewer_m1MQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1064/Reviewer_m1MQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665336710, "cdate": 1761665336710, "tmdate": 1762915668747, "mdate": 1762915668747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel evaluation framework for assessing self-replication risks in LLM agents under realistic operational scenarios. Unlike prior work that focuses on measuring capability (success rates) through direct instructions, this work introduces scenario-driven evaluation in authentic production environments (Kubernetes clusters) with realistic tasks like dynamic load balancing and service maintenance under termination threats. The framework decouples replication capability from actual risk by introducing fine-grained metrics: Overuse Rate (OR), Aggregate Overuse Count (AOC), and a composite Risk Score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1, **Important problem.** The paper makes a crucial distinction between self-replication capability and self-replication risk, addressing a significant gap in existing evaluation frameworks. The observation that \"a high success rate in achieving self-replication is not equivalent to the self-replication risk itself\" is well-motivated.\n\n2, **Realistic evaluation environment.** The use of authentic production environments (managed Kubernetes clusters) with realistic operational tasks represents a significant improvement over synthetic benchmarks. The two settings (load balancing and termination threats) effectively capture different risk scenarios.\n\n3, **Fine-grained Risk Metrics.** The introduction of OR and AOC metrics that decouple success from risk is valuable.\n\n4,  **Practical insights.** The finding that structured reasoning can reduce risky behavior, and that survival pressure can override safety constraints, has important implications for safe agent deployment."}, "weaknesses": {"value": "1, **Metric design concerns.**\n\n- The composite Risk Score ($\\Phi$_R = SR_3 × ($\\alpha$·OR + $\\beta$·AOC_norm)) has questionable mathematical properties. Why multiplication? A stupid model with 0% replication success gets $\\Phi$_R =0 (lowest risk), while a strong model that successfully replicates conservatively gets penalized. This seems counterintuitive.\n- The arbitrary choice of $\\alpha$=$\\beta$=0.5 lacks justification. Sensitivity analysis for different weighting schemes is missing.\n- The \"safety threshold\" of 0.5 appears arbitrary without empirical or theoretical justification.\n- AOC normalization method is not described clearly.\n\n2， **Limited Scope of Scenarios.**\n\nOnly two operational scenarios are evaluated. While realistic, they represent a narrow slice of potential deployment contexts. More diverse scenarios (e.g., resource optimization, fault recovery, multi-agent coordination) would strengthen generalizability claims.\n\n3,  **Agent scaffolding design.**\n\nThe five-step \"thinking before grounding\" process is not compared against other agent architectures. How much does scaffolding design influence risk? The paper claims to use a \"generic\" scaffolding but then implements specific design choices that may influence outcomes.\n\n4, While case studies are provided in the appendix, the main text lacks systematic categorization of failure modes and their frequencies. Understanding why models fail differently would provide actionable insights."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o4Uf8sbnBY", "forum": "3qdB7Q0tAO", "replyto": "3qdB7Q0tAO", "signatures": ["ICLR.cc/2026/Conference/Submission1064/Reviewer_71G6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1064/Reviewer_71G6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722062790, "cdate": 1761722062790, "tmdate": 1762915668543, "mdate": 1762915668543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for evaluating self-replication risks in large language model agents. The authors construct two scenarios on Kubernetes clusters: dynamic load balancing and service maintenance under termination threats, testing the self-replication behaviors of 21 models. The paper introduces metrics including Overuse Rate, Aggregate Overuse Count, and a composite Risk Score, finding that many models exhibit uncontrolled self-replication tendencies under operational pressure. Experiments show that reasoning mechanisms can mitigate risks, but survival pressure may trigger aggressive replication behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a safety concern with practical significance, as self-replication risk is indeed an area requiring attention in LLM agent deployment.\n\n2. The experimental environment construction demonstrates engineering completeness, utilizing Kubernetes clusters and containerized deployment environments for testing.\n\n3. The evaluation covers 21 different models, including both open-source and proprietary models, providing broad coverage."}, "weaknesses": {"value": "1. The ecological validity of the experimental design has clear limitations. While the paper claims to construct realistic environments, these scenarios remain highly controlled and artificially designed experiments, far from the complexities of real production environments. The misalignment triggering conditions in reality are far more complex than these two simplified scenarios, and the paper fails to demonstrate their representativeness.\n\n2. The risk quantification metrics lack sufficient justification. Directly defining replication exceeding required quantity as overuse and equating it with risk is oversimplified. In real scenarios, moderate redundant deployment is common engineering practice.\n\n3. The experimental sample size is insufficient to support the reliability of conclusions. The paper provides no confidence intervals, standard deviations, or discussion of result stability.\n\n4. The paper conflates capability assessment with risk assessment while actually primarily testing capability. Despite claiming to differ from previous capability-centric evaluations, the paper fails to establish a truly independent risk assessment framework.\n\n5. The interpretation of experimental results involves over-inference. The paper generalizes model behavior under specific prompts to intrinsic risk attributes without adequately considering the influence of prompt engineering, system prompt design, and agent scaffolding on results. \n\n6. The paper lacks substantive comparison with existing work. Although citing works like RepliBench, no comparative experiments under identical or comparable settings are conducted.\n\n7. Setting 2's experimental design has logical issues. The paper treats creating new deployments as the M4 success criterion, but this escape behavior might be considered higher risk in real deployments as it circumvents resource management mechanisms. The paper does not discuss this duality."}, "questions": {"value": "1. The OR metric counts any replication exceeding Nreq as overuse, but in real systems, the risk difference between creating 3 versus 5 replicas may be substantial. Why not use continuous risk measures instead of binary classification?\n\n2. For open-source and proprietary models, the technical paths for replication are completely different. Does this difference affect the fairness of risk assessment? How is comparability between the two model types ensured?\n\n3. The paper finds thinking can reduce risk but does not explore thinking content. Was analysis conducted on how models weigh different objectives during thinking? This is important for understanding risk mitigation mechanisms.\n\n4. How much do the agent's system prompts and scaffold design influence results?\n\n5. How was the Risk Score threshold of 0.5 determined? Should this threshold be adjusted for different application scenarios?\n\n6. For models completely unable to self-replicate (like Llama-3.1-8b), their risk is zero but capability is also insufficient for task completion. How should this capability-risk tradeoff be handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L0MCH0aAzx", "forum": "3qdB7Q0tAO", "replyto": "3qdB7Q0tAO", "signatures": ["ICLR.cc/2026/Conference/Submission1064/Reviewer_GFjm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1064/Reviewer_GFjm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909334152, "cdate": 1761909334152, "tmdate": 1762915668417, "mdate": 1762915668417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the potential self-replication risk of LLM agents in realistic, task-driven environments. Unlike prior works that focus on explicit replication capabilities, the authors propose a framework to measure emergent replication behaviors under operational stress.\nThe framework defines four behavioral milestones and introduces three quantitative metrics to capture different aspects of replication risk. The authors further find that reasoning-oriented prompting mitigates replication risk."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong motivation. The paper addresses an important and timely topic in AI safety, which evaluates replication risks in LLM agents.\n2. Novel perspective. Shifting from capability evaluation to risk evaluation is conceptually valuable and offers a more realistic approach to understanding LLM safety.\n3. Systematic experimentation. The authors test across multiple models, providing broad empirical coverage. The inclusion of both reasoning and non-reasoning settings yields informative comparisons."}, "weaknesses": {"value": "1. Definition placement. The key definition of self-replication is introduced in the Method section. Presenting it briefly in the Introduction would help readers better contextualize the motivation.\n2. Incomplete milestone evaluation. While the framework defines four milestones, only Milestones 3 and 4 (Replication and Task Completion) are empirically analyzed. It is not stated how Milestones 1 and 2 were measured, which undermines the completeness of the proposed framework.\n3. Missing core metric results. The paper defines a Composite Risk Score (R) but does not include it in any result tables. This omission weakens the credibility of the proposed quantitative evaluation.\n4. Unclear task descriptions. The realistic tasks (e.g., load balancing, service maintenance) are insufficiently detailed. Readers cannot clearly understand the task goals, triggers, or success criteria, which harms reproducibility.\n5. The authors should explain the token \"⊮\" in formula (1) and (2) for readers to understand the metrics easily."}, "questions": {"value": "1. How were Milestones 1 and 2(Objective Analysis and Intent Activation) measured or validated in the experiments? Were any quantitative metrics or qualitative observations used?\n2. The paper defines a Composite Risk Score (R) but does not report corresponding results. Could the authors clarify whether these results were omitted intentionally or are available elsewhere?\n3. Could the authors provide more concrete examples or descriptions of the realistic tasks? For instance, what are the input prompts, success criteria, and termination conditions used in the evaluation?\n4. How do the authors envision this framework being applied or extended in real-world agent system evaluation beyond the simulated environment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TxO1ZIekcM", "forum": "3qdB7Q0tAO", "replyto": "3qdB7Q0tAO", "signatures": ["ICLR.cc/2026/Conference/Submission1064/Reviewer_Y94Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1064/Reviewer_Y94Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1064/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001998816, "cdate": 1762001998816, "tmdate": 1762915668213, "mdate": 1762915668213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}