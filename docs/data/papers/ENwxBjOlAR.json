{"id": "ENwxBjOlAR", "number": 6589, "cdate": 1757989933828, "mdate": 1763111532252, "content": {"title": "Vision-Language Preference Optimization for Weakly Supervised Temporal Action Localization", "abstract": "Weakly supervised temporal action localization (WS-TAL) aims to localize actions in untrimmed videos using only video-level labels. Due to the absence of frame-level annotations, classification predictions during the initial training phase predominantly rely on the prior knowledge embedded in pre-trained video foundation models.\nHowever, the foundation model's inherent erroneous biases persist uncorrected during training, resulting in compounding error propagation throughout the learning process.\nTo address this issue, we develop a dual-branch framework called Vision-Language Preference Optimization (VLPO) that enhances WS-TAL tasks through systematic integration with vision-language model (VLM). \nOur framework introduces two key components: \n(1) The Vision-Language Fine-Tuning (VLFT) branch, \nwhich effectively establishes a multimodal feature alignment mechanism through video-level supervision, conducts online adaptive fine-tuning on the vision-language features. This significantly enhances the semantic sensitivity of temporal localization under weakly-supervised conditions;\n(2) The Preference Driven Optimization (PDO) branch, through the predictive preferences provided by VLM, optimizes the traditional WSTAL framework and actionness learning at the snippet-level from both class-aware and class-agnostic perspectives, significantly enhancing the accuracy of action localization.\nExtensive experiments on WS-TAL benchmarks demonstrate that VLPO significantly outperforms state-of-the-art methods, showcasing its effectiveness in WS-TAL.\nThe source code will be released upon acceptance.", "tldr": "", "keywords": ["Temporal Action Localization", "Weakly Supervised Learning", "Vision-Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fbe9599493f139ffcd6fb23455c55b8f32862e8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "1. Insufficient theoretical novelty: Dual-branch (VLFT/PDO) and modules (CM-AFA/DSP) are incremental, lacking groundbreaking frameworks\n2. Limited generalization: Only VideoCLIP-XL and 2 datasets tested; no complex scenario/other VLM validation \n3. Incomplete experiments: Omitted recent SOTA comparisons; superficial ablation without hyperparameter rationale\n4. Weak qualitative analysis: Few examples; evasive \"LongJump\" failure attribution\n5. Minor strength: Outperforms some SOTA on THUMOS14/ActivityNet 1.3 with low inference overhead"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper proposes the VLFT branch to enable effective fine-tuning of VLM under weak supervision and introduces the PDO branch, which optimizes the WS-TAL task by refining the prediction preferences of VLM from both class-aware and class-agnostic perspectives."}, "weaknesses": {"value": "1. The core design of the VLPO framework lacks groundbreaking theoretical innovation. First, the dual-branch (VLFT + PDO) paradigm for weakly supervised temporal action localization (WS-TAL) is not novel—existing works (e.g., PVLR [19], Li et al. [13]) have already explored integrating vision-language (VL) signals with WS-TAL via multi-branch structures.\n2. Key modules like Cross-Modal Anchored Feature Alignment (CM-AFA) and Dynamic Selection Pooling (DSP) only implement incremental adjustments to existing cross-modal alignment (e.g., CLIP’s contrastive learning) and temporal pooling (e.g., Top-k pooling in MIL) methods, without proposing a new theoretical framework or mathematical mechanism to justify their superiority. \n3. The framework’s generalization ability is not adequately verified, leading to doubts about its practicality. First, the authors only use VideoCLIP-XL as the vision-language model (VLM) without testing other mainstream VLMs (e.g., BLIP-2, FLAVA, LLaVA-1.5) or lightweight VLMs (e.g., CLIP-ViT-B/32). This makes it impossible to confirm whether VLPO’s performance gains come from the framework itself or the specific VLM’s pre-trained features. \n4. The experimental design is incomplete, and the analysis lacks rigor, failing to fully support the framework’s effectiveness. Specifically,  the comparison with state-of-the-art (SOTA) methods is incomplete. On ActivityNet 1.3, the authors only compare with 11 methods (mostly pre-2024) and omit recent SOTA works (e.g., 2024’s AFPS [17] variants, 2025’s non-VLM-based WS-TAL methods), making the \"SOTA outperformance\" claim unconvincing. \n5. The ablation study is superficial. For example, in Table 3, the authors only verify the \"presence/absence\" of modules but do not analyze the impact of key hyperparameters (e.g., the anchoring strategy in CM-AFA, the threshold selection in DSP) on performance. The hyperparameter robustness test (Appendix A.3) only reports fluctuations but does not explain why λ₁=300, αₕ=0.9 are optimal—no rationalization or cross-dataset validation of hyperparameter selection is provided"}, "questions": {"value": "See the above comments"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IXXG2uMsjn", "forum": "ENwxBjOlAR", "replyto": "ENwxBjOlAR", "signatures": ["ICLR.cc/2026/Conference/Submission6589/Reviewer_CuKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6589/Reviewer_CuKb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703908723, "cdate": 1761703908723, "tmdate": 1762918918646, "mdate": 1762918918646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "2AmEbMqJZb", "forum": "ENwxBjOlAR", "replyto": "ENwxBjOlAR", "signatures": ["ICLR.cc/2026/Conference/Submission6589/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6589/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763111530891, "cdate": 1763111530891, "tmdate": 1763111530891, "mdate": 1763111530891, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Vision-Language Preference Optimization (VLPO), a dual-branch framework for weakly supervised temporal action localization (WS-TAL) that leverages vision-language models (VLMs). VLPO addresses the bias and error propagation in pre-trained video models through two components: (1) Vision-Language Fine-Tuning (VLFT), which aligns multimodal features and enhances semantic sensitivity via adaptive fine-tuning, and (2) Preference Driven Optimization (PDO), which refines snippet-level actionness learning using VLM-guided preferences. Experiments on WS-TAL benchmarks show that VLPO achieves superior performance over existing state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an innovative dual-branch weakly supervised framework (VLPO) that leverages vision-language preference optimization to correct model bias and enhance cross-modal feature interaction.\n\n2. The proposed approach achieves significant performance improvements in temporal action localization by mitigating error accumulation and improving snippet-level localization accuracy."}, "weaknesses": {"value": "1. The overall framework is complex, and the multi-module design increases computational cost during both training and inference.\n\n2. The method relies heavily on the quality and generalization ability of the pre-trained vision-language model, which may reduce robustness in domain-shifted or low-quality datasets.\n\n3. The Related Work section could be further enriched to provide a more comprehensive comparison and discussion of existing WS-TAL and vision-language integration methods."}, "questions": {"value": "Please refer to the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nl2lfsAYrM", "forum": "ENwxBjOlAR", "replyto": "ENwxBjOlAR", "signatures": ["ICLR.cc/2026/Conference/Submission6589/Reviewer_r1Fk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6589/Reviewer_r1Fk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924809263, "cdate": 1761924809263, "tmdate": 1762918918293, "mdate": 1762918918293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a set of adaptive modules that leverage complementary information from vision language models (VLMs) to improve weakly supervised temporal action localization (WS-TAL). Instead of directly applying a VLM to WS-TAL, the authors introduce CM-AFA and DSP to better align visual and textual features with the WS-TAL setting, and PPG and APR to further optimize the localization objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method achieves strong performance and clearly outperforms existing approaches.\n2. The ablation studies are comprehensive and support the effectiveness of the proposed modules."}, "weaknesses": {"value": "1. The motivation for the proposed modules is not sufficiently developed. While the design seems reasonable for addressing the generalization gap of VLMs on WS-TAL, the paper does not deeply analyze why these modules are necessary or how each specifically mitigates the mismatch between VLM features and the WS-TAL objective.\n2. In the ablation studies in the appendix, it would be more informative to report results at multiple IoU thresholds (e.g., 0.3/0.5/0.7) rather than only the average score, so readers can better judge the localization quality.\n3. The captions for the qualitative results are too brief. Please provide more detailed observations, especially regarding how the proposed modules affect different action classes or challenging temporal patterns.\n4. It would strengthen the paper to include failure cases and a short discussion of the method’s limitations."}, "questions": {"value": "1. The paper mentions that VLMs are not good at direct deploying on WS-TAL, but the motivation for each adaptive module is not fully analyzed. Could you clarify what specific mismatch each module is designed to address?  \n2. Why are CM-AFA and DSP the right choices for aligning VLM visual/textual features to WS-TAL? Did you consider alternative designs?  \n3. In the appendix ablation, it only reports average performance. Can you provide results across multiple IoU thresholds?  \n4. The qualitative results have very short captions. Could you add more detailed descriptions?  \n5. Can you show a few failure cases and analyze their causes? What are the current limitations of the approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3IqucpKa67", "forum": "ENwxBjOlAR", "replyto": "ENwxBjOlAR", "signatures": ["ICLR.cc/2026/Conference/Submission6589/Reviewer_D6Lc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6589/Reviewer_D6Lc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940608155, "cdate": 1761940608155, "tmdate": 1762918917855, "mdate": 1762918917855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}