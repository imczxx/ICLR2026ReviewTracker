{"id": "7jdmXx869Q", "number": 11203, "cdate": 1758193231574, "mdate": 1759897601258, "content": {"title": "MindPilot: Closed-loop Visual Stimulation Optimization for Brain Modulation with EEG-guided Diffusion", "abstract": "Whereas most brain–computer interface research has focused on decoding neural signals into behavior or intent, the reverse challenge—using controlled stimuli to steer brain activity—remains far less understood, particularly in the visual domain.\nHowever, designing images that consistently elicit desired neural responses is difficult: subjective states lack clear quantitative measures, and EEG feedback is both noisy and non-differentiable. \nWe introduce MindPilot, the first closed-loop framework that uses EEG signals as optimization feedback to guide naturalistic image generation. Unlike prior work limited to invasive settings or low-level flicker stimuli, MindPilot leverages non-invasive EEG with natural images, treating the brain as a black-box function and employing a pseudo-model guidance mechanism to iteratively refine images without requiring explicit rewards or gradients. We validate MindPilot in both simulation and human experiments, demonstrating (i) efficient retrieval of semantic targets, (ii) closed-loop optimization of EEG spectral features, and (iii) human-subject validations in mental matching and emotion regulation tasks. Our results establish the feasibility of EEG-guided image synthesis and open new avenues for non-invasive closed-loop brain modulation, bidirectional brain–computer interfaces, and neural signal–guided generative modeling. Our code is available at \\url{https://anonymous.4open.science/r/MindPilot-0924}.", "tldr": "", "keywords": ["Neuroscience", "Brain Modulation", "EEG", "Closed-loop", "Brain Coding", "BCI", "Generative Model", "Black-box Guidance", "Encoding Model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ce64dd18eb042ffe8a42589f780875d108f5fe4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MindPilot, an innovative closed-loop EEG-guided visual stimulus optimization framework.The core concept of this framework is to treat the human brain as an indivisible black-box system, iteratively guiding diffusion models to generate images through EEG feedback signals.This framework introduces a pseudo-model to provide surrogate gradients, enabling gradient-free optimization for various neural objectives such as semantic or spectral EEG features.The paper validates MindPilot through agent-based model simulations and human experiments, demonstrating its potential in EEG-guided image retrieval, generation, and real-time human brain control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novelty: Innovation: First to propose a closed-loop image generation framework based on EEG signal diffusion models.\n\nTechnical soundness: Technical Rationality: The pseudo-model guidance mechanism is clearly designed and effectively replaces explicit gradient computation.\n\nExperimental breadth: Demonstrates stable convergence characteristics in both simulated and human experiments.\n\nPotential impact: Openes up new avenues for non-invasive neural modulation and human-machine collaborative adaptation studies."}, "weaknesses": {"value": "EEG Individual Variability: Insufficient discussion of inter-subject EEG variability; a brief explanation could be added to emphasize the model's adaptability and generalization capability."}, "questions": {"value": "1.  Could the authors elaborate on whether MindPilot could be generalized to other modalities (e.g. fMRI)?\n\n2. How do pseudo-model guidance and traditional gradient-free reinforcement learning methods differ in their convergence properties?\n\n3. Additionally, there are minor inconsistencies in the formatting of references within the text. For example, several NeurIPS/ICLR-style citations only provide the conference name without volume, issue, or page information (e.g., Bashivan et al., 2019; Black et al., 2024; Luo et al., 2024a). Similarly, the citation for “Transactions on Machine Learning Research, 2024” (Oquab et al., 2024) omits both volume and page numbers. We recommend that the authors carefully review these entries and align them with the journal article format, ensuring that volume, issue, and page numbers are included where available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k9yHOtcmXX", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Reviewer_LDfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Reviewer_LDfZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550199306, "cdate": 1761550199306, "tmdate": 1762922353452, "mdate": 1762922353452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank Reviewers pRhZ, h3JQ, uwSG, and LDfZ for their time and constructive feedback. We are encouraged that the reviewers recognize our work as addressing a \"novel and significant problem\" with \"considerable practical value,\" and affirm that the proposed method is \"technically sound and experimentally thorough.\"\n\nAs highlighted by the reviewers, MindPilot stands as the first closed-loop framework to successfully apply EEG-guided controllable visual stimulus optimization to brain modulation, validated extensively through human-in-the-loop experiments.\n\nTo provide a comprehensive response, we have summarized the reviewers' questions into four core themes: (1) proxy-brain alignment, (2) closed-loop efficacy, (3) optimization model baselines, and (4) human experiments. We address these below with additional analyses and clarifications to demonstrate the robustness of our approach.\n\n\n**Q1: \"The proxy model has a low correlation with the real EEG. (R-pRhZ)\"**\n\nResponse to Q1 (Proxy Model Correlation): We thank the reviewer for examining the proxy model's performance. We clarify that the correlation of $r \\approx 0.17$ in Table 1 represents an average across a broad time window ([60, 500] ms). This metric inherently dilutes the performance by averaging peak response intervals with less informative, noise-dominated segments.\n\nCritically, the model performs significantly better at key neural latencies. As detailed in Appendix Figure A.8, the time-resolved correlation reaches a peak of $r \\approx 0.6$ around 100 ms, which aligns with the N100 component of visual processing.\n\n1.This peak performance is highly competitive for single-trial non-invasive EEG prediction (comparable to Gifford et al., 2022 [1] in Figure 4).\n\n2.High correlation at these specific latencies confirms that our proxy model effectively captures the transient but critical neural dynamics required for effective stimulus optimization.\n\n\n**Q2: \"Reasons for choosing the pseudo-model when optimizing image. (R-pRhZ, R-LDTZ)\"**\n\nResponse to Q2 (Choice of Pseudo Model): We chose the pseudo-model approach over black-box optimization (e.g., RL) for two critical reasons: differentiability and sample efficiency.\n\n1. Gradient-based Guidance: Unlike RL methods (e.g., DDPO [1], D3PO [3]) that treat the diffusion model as a black box, our pseudo model operates on the clip embedding, thus enabling it to directly provide a stable optimization direction for SDXL-lightning, ensuring the quality of the generated images. This makes it intrinsically compatible with Guided Diffusion, allowing for precise image editing without extensive trial-and-error.\n\n2. Extreme Query Efficiency: To meet the real-time constraints of Human-in-the-Loop (HITL) systems, high query efficiency is non-negotiable. As shown in our new benchmark (see below), our method achieves superior performance with significantly fewer queries compared to RL baselines.\n\nWe benchmarked MindPilot against leading RL-based fine-tuning methods (DDPO [1], DPOK [2], D3PO [3]) across 10 targets. We evaluated performance and efficiency using varying data scales (50, 100, 150, 200 samples). Results (Table 1): MindPilot demonstrates state-of-the-art optimization speed and competitive generation quality even with limited data. This \"high query efficiency\" is what makes our closed-loop framework feasible for real-world brain modulation.\n\n| Method | EEG Score (5 Samples) | Time (5 Samples) | EEG Score (10 Samples) | Time (10 Samples) |\n| :--- | :--- | :--- | :--- | :--- |\n| DDPO | 0.5125 ± 0.0139 | 107.8163 ± 19.4221 | 0.5095 ± 0.0097 | 220.2666 ± 17.2916 |\n| DPOK | 0.5093 ± 0.0121 | 116.6111 ± 21.7522 | 0.5138 ± 0.0124 | 221.8881 ± 12.1397 |\n| D3PO | 0.5138 ± 0.0162 | 117.7019 ± 9.934 | 0.5113 ± 0.0154 | 285.3849 ± 6.5634 |\n| **MindPilot (Ours)** | **0.5271 ± 0.0085** | **7.7625 ± 1.0955** | **0.5266 ± 0.01** | **10.0072 ± 1.7502** |\n\n**Q3: \"Is EEG-driven optimization necessary? How about CLIP-only?  (R-pRhZ, R-uwSG)\"**\n\nResponse to Q3 (Necessity of EEG & CLIP-only Baseline):Yes, EEG-driven optimization is fundamental. We must clarify a critical distinction in the Brain Modulation task:\n\nUnlike traditional experiments where a user consciously selects a target (e.g., \"I choose this image\"), our framework operates under the assumption that the target image is unknown, and the target brain feature is the known goal (e.g., maximizing alpha power, or matching a target neural semantic representation).\n\nThis setup is the essence of \"brain modulation.\" Therefore, an optimization process without EEG is not a valid baseline, as it would have no guiding signal. The entire challenge is to use the brain's feedback (EEG) to discover the visual stimulus that steers the brain toward that target state."}}, "id": "ZwfbK3PIrb", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729957611, "cdate": 1763729957611, "tmdate": 1763729957611, "mdate": 1763729957611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response (1/3)"}, "comment": {"value": "We sincerely thank Reviewers pRhZ, h3JQ, uwSG, and LDfZ for their time and constructive feedback. We are encouraged that the reviewers recognize our work as addressing a \"novel and significant problem\" with \"considerable practical value,\" and affirm that the proposed method is \"technically sound and experimentally thorough.\"\n\nAs highlighted by the reviewers, MindPilot stands as the first closed-loop framework to successfully apply EEG-guided controllable visual stimulus optimization to brain modulation, validated extensively through human-in-the-loop experiments.\n\nTo provide a comprehensive response, we have summarized the reviewers' questions into four core themes: (1) proxy-brain alignment, (2) closed-loop efficacy, (3) optimization model baselines, and (4) human experiments. We address these below with additional analyses and clarifications to demonstrate the robustness of our approach.\n\n\n**Q1: \"The proxy model has a low correlation with the real EEG. (R-pRhZ)\"**\n\nResponse to Q1 (Proxy Model Correlation): We thank the reviewer for examining the proxy model's performance. We clarify that the correlation of $r \\approx 0.17$ in Table 1 represents an average across a broad time window ([60, 500] ms). This metric inherently dilutes the performance by averaging peak response intervals with less informative, noise-dominated segments.\n\nCritically, the model performs significantly better at key neural latencies. As detailed in Appendix Figure A.8, the time-resolved correlation reaches a peak of $r \\approx 0.6$ around 100 ms, which aligns with the N100 component of visual processing.\n\n1.This peak performance is highly competitive for single-trial non-invasive EEG prediction (comparable to Gifford et al., 2022 [1] in Figure 4).\n\n2.High correlation at these specific latencies confirms that our proxy model effectively captures the transient but critical neural dynamics required for effective stimulus optimization.\n\n\n**Q2: \"Reasons for choosing the pseudo-model when optimizing image. (R-pRhZ, R-LDTZ)\"**\n\nResponse to Q2 (Choice of Pseudo Model): We chose the pseudo-model approach over black-box optimization (e.g., RL) for two critical reasons: differentiability and sample efficiency.\n\n1. Gradient-based Guidance: Unlike RL methods (e.g., DDPO [1], D3PO [3]) that treat the diffusion model as a black box, our pseudo model operates on the clip embedding, thus enabling it to directly provide a stable optimization direction for SDXL-lightning, ensuring the quality of the generated images. This makes it intrinsically compatible with Guided Diffusion, allowing for precise image editing without extensive trial-and-error.\n\n2. Extreme Query Efficiency: To meet the real-time constraints of Human-in-the-Loop (HITL) systems, high query efficiency is non-negotiable. As shown in our new benchmark (see below), our method achieves superior performance with significantly fewer queries compared to RL baselines.\n\nWe benchmarked MindPilot against leading RL-based fine-tuning methods (DDPO [1], DPOK [2], D3PO [3]) across 10 targets. We evaluated performance and efficiency using varying data scales (50, 100, 150, 200 samples). Results (Table 1): MindPilot demonstrates state-of-the-art optimization speed and competitive generation quality even with limited data. This \"high query efficiency\" is what makes our closed-loop framework feasible for real-world brain modulation.\n\n| Method | EEG Score (5 Samples) | Time (5 Samples) | EEG Score (10 Samples) | Time (10 Samples) |\n| :--- | :--- | :--- | :--- | :--- |\n| DDPO | 0.5125 ± 0.0139 | 107.8163 ± 19.4221 | 0.5095 ± 0.0097 | 220.2666 ± 17.2916 |\n| DPOK | 0.5093 ± 0.0121 | 116.6111 ± 21.7522 | 0.5138 ± 0.0124 | 221.8881 ± 12.1397 |\n| D3PO | 0.5138 ± 0.0162 | 117.7019 ± 9.934 | 0.5113 ± 0.0154 | 285.3849 ± 6.5634 |\n| **MindPilot (Ours)** | **0.5271 ± 0.0085** | **7.7625 ± 1.0955** | **0.5266 ± 0.01** | **10.0072 ± 1.7502** |\n\n**Q3: \"Is EEG-driven optimization necessary? How about CLIP-only?  (R-pRhZ, R-uwSG)\"**\n\nResponse to Q3 (Necessity of EEG & CLIP-only Baseline): Yes, EEG-driven optimization is fundamental. We must clarify a critical distinction in the Brain Modulation task:\n\nUnlike traditional experiments where a user consciously selects a target (e.g., \"I choose this image\"), our framework operates under the assumption that the target image is unknown, and the target brain feature is the known goal (e.g., maximizing alpha power, or matching a target neural semantic representation).\n\nThis setup is the essence of \"brain modulation.\" Therefore, an optimization process without EEG is not a valid baseline, as it would have no guiding signal. The entire challenge is to use the brain's feedback (EEG) to discover the visual stimulus that steers the brain toward that target state."}}, "id": "ZwfbK3PIrb", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729957611, "cdate": 1763729957611, "tmdate": 1763736643700, "mdate": 1763736643700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "he paper presents MindPilot, an approach to control image generation from EEG-BCI. The area is under active research, and the present paper is a continuation of previous work."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n- The paper tackles a challenging and interesting problem.\n- The paper is well-written and aims for rigorous experimentation."}, "weaknesses": {"value": "Weaknesses:\n\n- The paper claims to be the first to tackle the problem, but the principles of the approach have been extensively explored in the last years: \n\nMany core-related works are missing, which hinders the novelty statement of the papers. Examples (not exhaustive list) of a few recent references of well-known papers and some even using similar CLIP approaches, generative modeling, and diffusion processes:\n\nhttps://proceedings.neurips.cc/paper_files/paper/2024/hash/84bad835faaf48f24d990072bb5b80ee-Abstract-Conference.html\nhttps://ieeexplore.ieee.org/abstract/document/10798967\nhttps://arxiv.org/abs/2306.16934\nhttps://arxiv.org/abs/2506.11151\nhttps://dl.acm.org/doi/abs/10.1145/3379337.3415821\nhttps://openaccess.thecvf.com/content/CVPR2022/html/Davis_Brain-Supervised_Image_Editing_CVPR_2022_paper.html\nhttps://arxiv.org/abs/2308.02510\nhttps://www.nature.com/articles/s42003-025-07731-7\nhttps://dl.acm.org/doi/full/10.1145/3716553.3750786\nhttps://proceedings.neurips.cc/paper_files/paper/2024/hash/4540d267eeec4e5dbd9dae9448f0b739-Abstract-Conference.html\nhttps://arxiv.org/abs/2508.20705\n\n- The reward and spreading operations are somewhat ad-hoc. I understand that the reward model cannot directly find the best matches, but this makes the robustness and replication of the approach problematic as the connection across the models is not straightforward. Figure 3d also shows that the alignment task is not easy and the results are not clearly demonstrating that simple CLIP with the underlying ad-hoc process is suitable.\n\n- There are good attempts to evalute the approach. \n\n- It is known that EEG does not generally contain fine-grained semantic codes; it means that the pattern of responses across time and sensors partially aligns with the structure of semantic space for their geometry. Thus, I feel that some of the wordings in the paper are not in line with this general understanding.\n\n- The closed loop experiment has results that do not support the message of the paper (Fig 6 C&D). In fact, when tested  “in-vivo” the model does not seem to perform.  This is also problematic as the paper claims novelty in closed-loop, but does not demonstrate that convincingly.\n\nMinors:\n\n- I don’t understand why it is highlighted that a brain is non-differentiable. I think we do not know the exact mechanisms of learning in the biological/cellular system, especially when making such simplified mathematical statements for analyzing EEG.  There is emerging evidence (https://academic.oup.com/pnasnexus/article/3/7/pgae261/7702306), but I am not sure how meaningful this is for the submitted paper.\n\n- Closed-loop is only a small portion of the paper, while most of the paper relies on a single existing dataset."}, "questions": {"value": "1. I would like to stress novelty compared to many previous works that are \"not closed loop\" -- as I think the present paper also has very limited closed-loop contribution\n\n2. I think the results in the most critical parts of the paper are weak and the authors are trying to oversell them."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Human BCI experiment reported. This is ok, but potential data release needs to be separately checked."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hcG1yt8Wed", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Reviewer_uwSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Reviewer_uwSG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748227182, "cdate": 1761748227182, "tmdate": 1762922353082, "mdate": 1762922353082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on a novel problem: given neural activity recordings evoked by a visual stimulus (e.g., EEG), how can we find or generate the most likely visual stimulus? I believe this problem has potential value for related research in neuroscience. In this work, the authors first propose an iterative optimization method that progressively updates the probability of each image in an image database by computing the correlation between EEG representations and the images, thereby identifying the most likely visual stimulus. Subsequently, the paper introduces an EEG-guided image generation model capable of attempting to generate the possible visual stimulus. The authors conducted corresponding evaluations, and I believe the evaluation is also sufficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a research problem that I believe has considerable practical value.\n\n2. According to the authors’ evaluation, the method proposed in this paper is effective.\n\n3. To facilitate a better evaluation, the authors conducted a visual rating experiment with human participants and demonstrated that the results obtained by their method are highly correlated with the ground truth provided by the participants.\n\n4. The evaluation in the paper is thorough.\n\n5. The paper is well-constructed, with figures and formatting presented nicely."}, "weaknesses": {"value": "This paper has some limitations, but I think the authors have provided a thorough discussion of them in the “Limitations” section on page 9."}, "questions": {"value": "1. Normally, better visual encoders and representation learning strategies are expected to learn more accurate and robust visual representations. However, according to the results in Table 1, representations obtained from ResNet trained on image classification tasks perform the best. What could be the reason for this?\n\n2. Continuing from the above question, I have noticed a paper [1] emphasizing that visual representations obtained through contrastive learning are more consistent with human brain activity. Although that study focused on fMRI data, why does a different conclusion appear when it comes to EEG signals?\n\n3. How should we understand ATM-S as the performance upper bound for image generation in Table 2?\n\n4. In Figure 4C, what do the images in each row represent?\n\n[1] Better models of human high-level visual cortex emerge from natural language supervision with a large and diverse dataset. Nature Machine Intelligence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8BRpt7En4I", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Reviewer_h3JQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Reviewer_h3JQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010958258, "cdate": 1762010958258, "tmdate": 1762922352619, "mdate": 1762922352619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MindPilot: a closed-loop visual stimulation optimization framework based on EEG feedback. It shows a participant a batch of images, records their EEG, and calculates a score indicating \"how similar\" their brain state is to a target state."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Novel and significant problem: Closed-loop visual modulation using non-invasive EEG, advancing towards \"neural feedback-guided generative modeling.\"\n\nPractical implementation: Gradient-free black-box guidance + simple score propagation + roulette wheel sampling, resulting in low engineering cost.\n\nMulti-level validation: Proxy simulation (Tables 1, 3), dual targets of semantics and PSD (Figs. 3–5), small-scale real-time human closed-loop experiments (Fig. 6), and the provision of anonymous code."}, "weaknesses": {"value": "1. Motivation and advantages of the \"pseudo-model\" need highlighting: The current discussion is insufficient regarding why Gaussian Process was chosen as the pseudo-model over other black-box optimizers (e.g., Bayesian Optimization). It is recommended to add a brief discussion or comparative experiment in the main text or appendix to explain the rationale for selecting GP and its advantages relative to other methods.\n\n\n2. The hyperparameters α and β in Eqs. (3) and (4) are set as fixed values, but their specific chosen values or selection criteria are not reported, nor is there any sensitivity/ablation analysis. It is recommended to supplement the specific values, search ranges, and their impact on performance in the main text or appendix. Furthermore, the ambiguity caused by using the same notation as the crossover/mutation ratios in §4.3 should be clarified (or the symbols should be changed).\n\n\n3. The core premise of the paper is that the proxy model can substitute for the real human brain in closed-loop optimization. However, the data in Table 1 severely weaken this premise: the maximum Pearson correlation between any proxy model and real EEG is only ~0.17. This value is too low. Successful optimization demonstrated on the proxy model does not necessarily generalize to the real human brain. The authors are advised to provide evidence demonstrating a systematic link between the optimization trends observed in the proxy model and those in the real human brain.\n\n\n4. Figure captions could be more detailed. For instance, how exactly is the ordinate \"EEG semantic feature similarity\" in Fig.3.D calculated? What do the bold and underlining in Table 1 signify? The authors are recommended to briefly explain this.\n\n\n5. The current experiments cannot distinguish whether the success of the closed-loop optimization stems from the unique neural information in the EEG or from visual information related to CLIP features that the proxy model learned from the images. To conclusively prove that EEG drives the optimization, the following control experiment is essential: A CLIP-only baseline: Remove the EEG feedback and directly optimize the image similarity to the target within the CLIP space. This baseline serves to quantify MindPilot's performance gain over pure visual semantic optimization."}, "questions": {"value": "Human experiment details need more transparency. The paper mentions excluding 4 participants due to poor data quality but does not specify the exact exclusion criteria (e.g., artifact proportion exceeding a certain threshold). It is recommended to clearly state the data quality threshold criteria to enhance the experiment's reproducibility and rigor."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "A new dataset for EEG"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z0JZD8XrHI", "forum": "7jdmXx869Q", "replyto": "7jdmXx869Q", "signatures": ["ICLR.cc/2026/Conference/Submission11203/Reviewer_pRhZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11203/Reviewer_pRhZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762840399683, "cdate": 1762840399683, "tmdate": 1762922352013, "mdate": 1762922352013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}