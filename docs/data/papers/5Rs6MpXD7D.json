{"id": "5Rs6MpXD7D", "number": 12028, "cdate": 1758205266785, "mdate": 1759897538454, "content": {"title": "Scaling Weisfeiler–Leman Expressiveness Analysis to Massive Graphs with GPUs", "abstract": "The Weisfeiler–Leman (WL) test is a cornerstone for analyzing the expressiveness of Graph Neural Networks, but computing its stable coloring at scale has remained a bottleneck. Classical refinement algorithms are inherently sequential and, despite optimal asymptotic complexity, do not exploit modern massively parallel hardware. Moreover, the problem is P-complete, suggesting limited parallelizability in the worst case. We show that these theoretical barriers do not preclude practical scalability. We obtain a linear-algebraic view of stable colorings by reformulating WL refinement as repeated matrix–vector multiplications. Building on this, we introduce two key contributions: (i) a randomized refinement algorithm with tight probabilistic guarantees, and (ii) a batching method that enables the analysis of stable colorings on subgraphs while preserving global correctness. This approach maps directly to GPU-efficient primitives. In numerical experiments, our CUDA implementation delivers up to ~24x speedups over classical CPU-based partition refinement and, for the first time, successfully computes stable colorings on web-scale graphs with over 30 billion edges, where CPU baselines time out or fail.", "tldr": "A novel parallel algorithm computing the 1-WL test (describing GNN expressiveness) that enjoys ~24x speed-ups over established algorithms", "keywords": ["Weisfeiler-Leman Test (1-WL)  Computation", "Randomized Parallel Algorithm", "Linear Algebra Characterization"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e55eefeede655078cdcba356ff0c9d812d8136e7.pdf", "supplementary_material": "/attachment/d34e6781d6db1971a7b4ce6082259b078a1d1902.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a probabilistic WL variant that can be parallelized and efficiently scales to very large graphs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "**(S1) Theoretical contribution (Originality, Significance):** Parallelizing the inherently sequential WL algorithm is a significant and interesting contribution. \n\n**(S2) Empirical results (Quality):** Efficiently scaling WL to billions of nodes is impressive. In particular, the speed-up over classical WL seems significant. The experiments demonstrate that (1) the proposed batched algorithm is faster than previous algorithms; and (2) that it scales to very large graphs."}, "weaknesses": {"value": "**(W0) Minor weakness:** This work could be clearer about the applications of parallelizing WL. While WL is used to evaluate GNN expressivity, it is mainly used as a mathematical tool for proofs rather than as a practical algorithm. While WL is used practically in graph kernel methods, I am unaware of any recent works using these methods.\n\n**(W1) Clarity:** While the underlying concept of the WL algorithm is intuitively simple, the paper is difficult to follow. In particular:\n\n **(W1.1)** The connection to connection to backward equivalence (BE) in dynamical systems is unclear. Defining these concepts and showing an example would help (the example in line 57 does not explan the connection to BE).\n\n> BE defines a partition of nodes such that each of the nodes in a block has equal sum of edge weights  going to nodes of each block. Formally, BE requires that (...)\n\nThis explains what BE defines, but does not **define** BE (see also W2). Are these all properties of a BE?\n\n**(W1.2)** The P-completeness of stable coloring does not follow from the explanation. As this is a key motivation behind the paper it is important that this gets communicated clearly. \n\n**(W2) Formality:**  Definitions and Theorems are incomplete or not formal enough. Consider Theorem 1:\n\n> We call $x \\in \\mathbb{R}^n$ symmetric on $H$ if and only if $x_i = x_j$ for all $B \\in H$ and $i, j \\in B$. Then, $H$ is a BE if and only if for all $B \\in H$ it holds that $Ax$ is symmetric on $H$ for any $x \\in \\mathbb{R}^n$ that is symmetric on $H$.\n\nWhat is $H$? Only from previous context one can conclude that $H$ is a partition of a graph (which is not part of this definition). What is $A$? It is probably the adjacency matrix of the graph for which $H$ is the partition of. However, the reader should not need to guess what a definition or theorem means. Finally, in the second sentence, the authors quantify over all $B \\in H$ but then never use $B$.\n\nThis (W2) is my key issue. The papers formalization are incomplete or difficult to follow, making it difficult to evaluate the correctness.\n\n**(W3) Experimental results:** While the explanation of experiments is difficult to follow, I have multiple problems with the implication of the results. \n- In Table 2, the fastest model  is in 10 / 12 cases the single-GPU model raising questions about the practical usefulness of batching WL. Even on the large graphs of Table 3 (with > 1 billion nodes), larger batch sizes are faster than smaller ones. Thus, batching only seems to be useful if the graphs are too large to fit into memory. How much VRAM each graph dataset requires.\n- The naive PyTorch 1-WL implementation is significantly slower than I would intuitively expect. Do the authors have an explanation for this? I quickly checked the code but could not find the code for the WL baseline.\n- The massive graph experiments (Table 3) lack CPU baselines that completed, making it impossible to quantify speedups at this scale."}, "questions": {"value": "- What is AggregateAdjMatrix in Algorith 2?\n- Please see weaknesses\n\n#### Other Pointers\n- Table 1 needs more than half a page and contains little information. \n- The graph definition should state that it is weighted graph.\n- Refinement is not formally introduced (furthermore, the concept of BE refinement seems different than that of WL refinement). \n- This paper could be significantly improved by using figures to illustrate concepts such as definition or algorithms.\n- Please make references to the appendix clickable (e.g. using \\label and \\ref)\n- At least one figure without a caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tdcommq7Ir", "forum": "5Rs6MpXD7D", "replyto": "5Rs6MpXD7D", "signatures": ["ICLR.cc/2026/Conference/Submission12028/Reviewer_vpTt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12028/Reviewer_vpTt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971846919, "cdate": 1760971846919, "tmdate": 1762923010226, "mdate": 1762923010226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a reformulation of the WL test as a repeated-matrix vector operation. The approach can be efficiently mapped to a parallelized implementation on GPUs/ Tje stable coloring of 1-L is formulated as an iterative set of multiplications, some node coloring vector with the adjacency matrix.  The key contributions of the work are an MC algorithm that uses randomized vectors to find partitions with probabilistic guarantees and a batched algorithm to enable processing large graphs while preserving global correctness. The algorithm provides an order-of-magnitude speedup over even multicore CPU baselines. The algorithm substitutes time complexity for parallelizability.  The authors evaluate reduction power and runtime efficiency on 12 medium to large-scale graphs and scalability studies on 3 massive web-scale graphs and show strong results over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed methodology is sound, and a trade-off for worse time complexity for practical improvements is well justified \n- While the reformulation only provides probabilistic bounds, the trade-off is negligible  \n- Evaluation on large graphs with tens of billions of edges is the largest evaluation done on this scale."}, "weaknesses": {"value": "- The evaluation is a bit confusing at times. Table 1 may be alternatively presented as a percent change between the baseline and batched methods. \n- RBBE enables concurrency but does not provide improvements in wall time after parallelization. The authors mention this is due to the merge overhead, but it would be good to provide performance numbers beyond a single dataset (sk2005) if possible. This can also inform strategies for parallelization. \n-  The scope of the work is limited to 1-WL only and generalized k-WL is not discussed"}, "questions": {"value": "- The CPU baseline fails due to OOM, but wouldn't the batched implementation also run on CPU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ay890idt1R", "forum": "5Rs6MpXD7D", "replyto": "5Rs6MpXD7D", "signatures": ["ICLR.cc/2026/Conference/Submission12028/Reviewer_ZLaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12028/Reviewer_ZLaf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960870745, "cdate": 1761960870745, "tmdate": 1763012523676, "mdate": 1763012523676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mult-gpu algorithm and cuda implementation for getting 1-WL coloring or the coarsest equitable partition. The paper claims designing two algorithms, the first is based on iterative refinement with a random starting vector via power iteration, which is suitable to find approximate coarsest equitable partition when converge for small graphs/subgraphs that can fit into single-gpu memory. The second algorithm is basically a way to split the giant graph into blocks of local graphs for iterative refinement, and then merge these local partitions back to the global partition of the giant graph. The author implemented them with Nvidia cuda libraries and tested its effectiveness in large-scale graphs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The writing is clear, while I think the background knowledge is too much for the main paper, better to put into appendix. \n2. Each designed algorithm has error analysis and theoretical support for finding coarsest equitable partition. \n3. The speed improvement claimed by the paper is great. \n4. The ability to handle billons of edges is very useful, given all existing baselines are not working."}, "weaknesses": {"value": "1. The central idea of using power iteration to find equitable partition / 1-WL color refinement is not new at all. For example, Power-Iterated Color Refinement (AAAI’14): establishes the power-iteration lineage for refinement.  The connection between 1-WL and matrix vector production is well known in the commuity. \n2. Also, using random evaluations to separate equivalence classes is a standard trick (Schwartz–Zippel-style arguments underpin many Monte-Carlo tests) — again, the probabilistic splitter idea isn’t brand new, even if the exact instantiation varies.\n3. It seems to me that the main contribution is: 1) implementation on GPUs. 2) partition the massive graph into blocks while keeps boundary nodes as singletons for correctness. Nevertheless, I still feel that there is not much novelty inside. I think it would be interesting if the author can: \n- study how to batching graphs comparing to the current random batch to make the merged partition much accurate. For now we can see from Table 3 that increasing the number of batches hurts the correctness a lot."}, "questions": {"value": "1. Where the is the metric for the gap to oracle equitable partition? The current reduction power does not show that. Given that the algorithm relies on randomness with certain error analysis, it is important to show that empirically the real-world error is matching the theorem.\n\n2. Can you talk more on how you merge the boundary nodes?\n\n3. Is there any better batching split method and partition merging method other than random? \n\n4. It seems that the code is working on 8GPUs at most? How about mult-node version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DA7ICdfxqD", "forum": "5Rs6MpXD7D", "replyto": "5Rs6MpXD7D", "signatures": ["ICLR.cc/2026/Conference/Submission12028/Reviewer_3iTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12028/Reviewer_3iTk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762388092520, "cdate": 1762388092520, "tmdate": 1762923009322, "mdate": 1762923009322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new computing scheme for 1-WL coloring by viewing refinement as repeated matrix–vector multiplications. Building on this, the authors introduce a randomised refinement algorithm with probabilistic guarantees and a batching scheme that operates on subgraphs while preserving global correctness. Experiments across social/web graphs show near-coarsest partitions under batching (≤5% gap) and up to 24× wall-clock speedups vs classical CPU baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 GPU-based RBBE implementation consistently outperforms classical CPU implementations.\n\n2. The guarantee of error ≤ n/r makes the method practical for real-world scenarios.\n\n3. The method can be scaled to multiple GPUs."}, "weaknesses": {"value": "1. Today there are many frameworks that run GNNs efficiently such as Pytorch Geometric and Deep Graph Library. It is also known that the Graph Neural Network of GIN [1] has an expressive power as 1-WL. Therefore, I believe a comparison between an efficient GIN and the proposed method is required, where GIN will be used to compute the coarsest refinement.\n\n2. Tables and results are very difficult to read.\n\n3. The actual implementation is not based on real random number selection. How does it affect the empirical error and theoretical guarantees?\n\n\n[1] Xu, Keyulu, et al. \"How Powerful are Graph Neural Networks?.\" International Conference on Learning Representations, 2019"}, "questions": {"value": "Please see my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8hC98q19D0", "forum": "5Rs6MpXD7D", "replyto": "5Rs6MpXD7D", "signatures": ["ICLR.cc/2026/Conference/Submission12028/Reviewer_Y6CY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12028/Reviewer_Y6CY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762666565452, "cdate": 1762666565452, "tmdate": 1762923008917, "mdate": 1762923008917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}