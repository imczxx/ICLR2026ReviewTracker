{"id": "wz4ZajBvB2", "number": 7617, "cdate": 1758029437688, "mdate": 1763603687822, "content": {"title": "From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance in a wide range of graph-related learning tasks. However, explaining their predictions remains a challenging problem, especially due to the mismatch between the graphs used during training and those encountered during explanation. Most existing methods optimize soft edge masks on weighted graphs to highlight important substructures, but these graphs differ from the unweighted graphs on which GNNs are trained. This distributional shift leads to unreliable gradients and degraded explanation quality, especially when generating small, sparse subgraphs. To address this issue, we propose a novel iterative explanation framework which improves explanation robustness by aligning the model’s training data distribution with the weighted graph distribution appeared during explanation. Our method alternates between two phases: explanation subgraph identification and model adaptation. It begins with a relatively large explanation subgraph where soft mask optimization is reliable. Based on this subgraph, we assign importance-aware edge weights to explanatory and non-explanatory edges, and retrain the GNN on these weighted graphs. This process is repeated with progressively smaller subgraphs, forming an iterative refinement procedure. We evaluate our method on multiple benchmark datasets using different GNN backbones and explanation methods. Experimental results show that our method consistently improves explanation quality and can be flexibly integrated with different architectures.", "tldr": "", "keywords": ["Interpretability", "Graph explanation", "XAI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a51144d53cf51fb2247fb398994adc2cd29ed2d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes STORE, an iterative framework to address the distributional discrepancy between GNN training and post-hoc explanation. After training a base GNN and an explainer, STORE retrains the GNN using structure-regularized graphs generated by the explainer, aligning the two distributions over several iterations. Experiments on synthetic and molecular graph datasets show improved explanation fidelity and stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and well articulated, highlighting an important gap in existing GNN explainers.\n- The idea of iteratively aligning GNN and explainer training is interesting and may inspire follow-up work.\n- The experiments are reasonably comprehensive, including both synthetic and real-world datasets, and Appendix I provides a runtime analysis suggesting modest overhead on small datasets."}, "weaknesses": {"value": "- The proposed framework is unnecessarily heavy and complicated. To address the distributional discrepancy, STORE introduces a third iterative step that requires retraining both the GNN and the explainer, which significantly increases methodological and computational complexity.\n\n- The paper does not convincingly justify why such retraining is essential. The same goal could likely be achieved with simpler mechanisms, for example a reinforcement learning–based approach that adds or modifies edges sequentially to align distributions without full retraining.\n\n- While Section 5 provides a theoretical justification that retraining preserves explanation faithfulness, it does not analyze convergence behavior or guarantee that the iterative process stabilizes in practice.\n\n- The efficiency study is limited to small datasets; there is no large-scale or FLOP-level analysis to demonstrate scalability.\n\n- Although explanation fidelity improves, there is no clear evidence that retraining does not distort or degrade the base GNN’s performance on its original task."}, "questions": {"value": "- How sensitive is STORE to the number of retraining iterations? Does performance plateau quickly?\n- Can the authors clarify runtime or memory scaling on larger datasets?\n- Does retraining the GNN affect its original classification accuracy?\n- Could simpler alternatives, such as reinforcement learning for subgraph generation, achieve similar alignment without retraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N7jNPgqPsE", "forum": "wz4ZajBvB2", "replyto": "wz4ZajBvB2", "signatures": ["ICLR.cc/2026/Conference/Submission7617/Reviewer_xuGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7617/Reviewer_xuGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696271799, "cdate": 1761696271799, "tmdate": 1762919697851, "mdate": 1762919697851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed STORE aims to provide more robust and reliable explanations by addressing the problem of explanatory graphs falling into out-of-distribution regions that are distant from the distribution of graphs in the training data."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an interesting research topic, handling out-of-distribution (OOD) issues in explanatory graphs, which has emerged as one of the most actively researched areas in the Graph XAI field."}, "weaknesses": {"value": "1. Pre-trained GNNs should be treated as fixed targets for explanation. However, since the proposed method re-trains the target model to address the OOD explanatory graphs problem, the resulting explanations cannot be considered explanations of the original pre-trained GNN.\n2. The approach requires retraining a pre-trained GNN, which represents a highly restrictive setting with limited feasibility in real-world applications where models need to remain fixed.\n3. When the explanation fails to capture the ground-truth properly following the proposed STORE, the OOD problem may be addressed, but the performance after retraining will deteriorate significantly, and consequently, the explanation will also be learned in a more negative direction.\n4. The iterative explanation framework does not account for computational cost and time requirements, while the gains are incremental."}, "questions": {"value": "1. When the target GNN is retrained, the explanation target changes. If so, the provided explanation is not for the original GNN that we intended to explain. Then how can explanations for the original GNN be provided?\n2. The original trained GNN and the retrained GNN are not theoretically and practically identical models. What is the rationale for providing explanations for the retrained GNN instead of the original trained GNN?\n3. What are the performance changes of the GNN before and after retraining? Since it is trained on a dataset that includes explanatory graphs, what kind of performance changes occur?\n4. In most papers, the weighted graph in Figure 1(b) is located in the OOD region, but unlike the example shown, the prediction label is not incorrect. This appears to be an extreme case if the given GNN is well-trained. \n\nPlease address the questions with consideration of the mentioned weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5wy9V5oS81", "forum": "wz4ZajBvB2", "replyto": "wz4ZajBvB2", "signatures": ["ICLR.cc/2026/Conference/Submission7617/Reviewer_aHyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7617/Reviewer_aHyN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719044445, "cdate": 1761719044445, "tmdate": 1762919697513, "mdate": 1762919697513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel iterative framework to improve the robustness and quality of explanations in GNNs. The work identifies a key issue in existing GNN explanation methods, i.e., a distributional mismatch between the unweighted graphs used for model training and the weighted graphs employed during explanation. To mitigate this, the proposed method alternates between two phases: (1) identifying explanation subgraphs using soft edge masks, and (2) retraining the GNN on importance-weighted graphs to align training and explanation distributions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes a novel iterative stochastic re-weighting explanation framework to enhance the explanation of Graph Neural Networks. Comprehensive experiments have been conducted on GCN and GIN models and demonstrate consistent improvements in explanation performance and reliability. In addition, the paper provides theoretical foundations to support the proposed framework."}, "weaknesses": {"value": "W1. In Section 5, the theorem provides a justification for the iterative framework, but the paper lacks sufficient analysis of the effectiveness and theoretical soundness of the stochastic re-weighting strategy.\n\nW2. The proposed framework emphasizes graph weight tuning more than previous methods, which significantly expands the weight search space and makes the approach appear more engineering-oriented rather than theoretically innovative.\n\nW3. The baseline performance results reported in the paper differ considerably from those in the original papers. No convincing explanations are provided for these discrepancies."}, "questions": {"value": "Q1. Is it possible to provide a more detailed theoretical or empirical justification for the stochastic re-weighting approach? Specifically, how does it contribute to improving explanation robustness, and under what conditions does it remain effective?\n\nQ2. The proposed framework introduces additional weight tuning during the explanation process, which potentially increases computational complexity and search space. How will this impact scalability, and whether any optimization or regularization strategies were considered to mitigate this issue?\n\nQ3. The baseline results reported in the paper differ notably from those presented in the original studies. What are the reasons for these discrepancies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "71iXsqZLUZ", "forum": "wz4ZajBvB2", "replyto": "wz4ZajBvB2", "signatures": ["ICLR.cc/2026/Conference/Submission7617/Reviewer_kq85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7617/Reviewer_kq85"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912901229, "cdate": 1761912901229, "tmdate": 1762919697066, "mdate": 1762919697066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a critical distributional shift in GNN explainability: GNNs are trained on unweighted graphs but are required to produce reliable predictions and gradients on the weighted graphs generated by soft-mask explanation methods. This mismatch leads to unreliable explanations, especially when seeking sparse subgraphs. To address this, the authors propose an iterative framework that identifies an explanatory subgraph and (2) retrains the GNN model on stochastically weighted graphs where explanatory edges are assigned high-importance. The proposed model improves the quality and faithfulness of the resulting explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper addresses an important flaw in GNN explanation methods. The distributional shift between binary training graphs and weighted explanation graphs is a critical vulnerability.\n\nS2. The paper provides a clear empirical diagnosis of the problem\n\nS3. The experimental validation improves the performance (AUC-ROC) of five different baseline explainers (e.g., GNNExplainer, PGExplainer) across five benchmark datasets."}, "weaknesses": {"value": "W1. The iterative framework, by design, introduces a significant computational cost. The method requires $L$ rounds of both GNN retraining (on augmented weighted graphs) and explainer retraining. This is a substantial increase in computation compared to a standard, one-shot post-hoc explanation method. \n\nW2. The STORE framework introduces several new hyperparameters that require careful tuning. \n\nW3. Dependence on Initial Explainer: The iterative process is seeded by an initial explanation ($\\Psi_{\\psi}^{(0)}$) generated from the original, non-robust GNN. The quality of the entire iterative refinement process may be dependent on the quality of this initial, potentially unreliable, explanation. The paper does not explore how STORE would perform if this initial seed explanation was fundamentally flawed or missed the correct explanatory subgraph."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ry5jt0zPX8", "forum": "wz4ZajBvB2", "replyto": "wz4ZajBvB2", "signatures": ["ICLR.cc/2026/Conference/Submission7617/Reviewer_toYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7617/Reviewer_toYR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963688855, "cdate": 1761963688855, "tmdate": 1762919696709, "mdate": 1762919696709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}