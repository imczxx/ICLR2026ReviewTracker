{"id": "3SyoD7yc2l", "number": 2879, "cdate": 1757296155351, "mdate": 1759898121403, "content": {"title": "UnitNorm: Rethinking Normalization for Transformers in Time Series", "abstract": "Normalization techniques are crucial for enhancing Transformer models' performance and stability in time series analysis tasks, yet we originally identify that traditional methods like batch and layer normalization often lead to issues such as token shift, attention shift, and sparse attention.\n  We propose UnitNorm, a novel normalization approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing these challenges.\n  Grounded in existing normalization frameworks, UnitNorm's effectiveness is demonstrated across diverse time series analysis tasks, including forecasting, classification, and anomaly detection, via a rigorous evaluation on 6 state-of-the-art models and 10 datasets.\n  UnitNorm demonstrates superior performance, particularly where robust attention and contextual understanding are vital, achieving up to a 1.46 MSE decrease in forecasting and a 4.89\\% accuracy increase in classification.\n  This work not only calls for a re-evaluation of normalization strategies in time series Transformers but also sets a new direction for enhancing model performance and stability.", "tldr": "We propose UnitNorm, a novel normalization design that scales input vectors by their norms to address token shift, attention shift, and sparse attention issues in Transformer-based time series analysis.", "keywords": ["time series", "Transformer", "normalization"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/47e148ad2e61e1040cf22493fa7a3d68f8c62145.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyzes the drawbacks of widely used normalization approaches in Transformers, such as LayerNorm and BatchNorm, and proposes a new normalization method named UnitNorm for time series analysis (TSA) tasks. The paper provides theoretical analysis to explain how conventional normalization methods may cause issues such as token shift, attention shift, and sparse attention. Experimental results show that UnitNorm achieves superior performance on forecasting, classification, and anomaly detection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides thorough and well-motivated theoretical analysis regarding the limitations of conventional normalization methods, identifying three potential issues—token shift, attention shift, and sparse attention—that could degrade Transformer performance on time series tasks.\n- The effectiveness of the proposed UnitNorm is validated across diverse time series applications, including forecasting, classification, and anomaly detection. The experimental results demonstrate that the UnitNorm with proper parameter settings achieves superior performance compared to widely used normalization techniques."}, "weaknesses": {"value": "- Some claims lack sufficient justification:\n  - In *line 123*, the statement *“altering token vector orientations and obscuring long-range dependencies”* is vague. It would be helpful to provide clearer theoretical or empirical evidence explaining why standard normalization methods cause this issue.\n  - In *line 267*, the paper claims that sparse attention is problematic in TSA tasks, but this claim is not sufficiently explained or analyzed. Additional clarification or supporting experiments would strengthen this argument.\n\n- The motivation for focusing exclusively on time series analysis (TSA) is not entirely clear. The identified issues—token shift, attention shift, and sparse attention—may also arise in other Transformer-based applications (e.g., NLP or vision). It would be beneficial to discuss whether UnitNorm could generalize beyond TSA tasks.\n- The proposed solution may lack of technical contribution. The proposed UnitNorm formulation appears to be a relatively minor modification of RMSNorm, differing mainly by a scaling factor. The paper would benefit from a clearer discussion of the fundamental difference and why such a seemingly small change leads to consistent improvements.\n- The experiments indicate that UnitNorm’s performance is sensitive to its hyperparameter $k$, which could limit its practical applicability. Providing a heuristic guideline for selecting $k$ would make the method more practical.\n- It is recommended to place the important experiments in the main text instead of the appendix."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RH58e8J4aM", "forum": "3SyoD7yc2l", "replyto": "3SyoD7yc2l", "signatures": ["ICLR.cc/2026/Conference/Submission2879/Reviewer_8fXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2879/Reviewer_8fXg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809298017, "cdate": 1761809298017, "tmdate": 1762916428355, "mdate": 1762916428355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies three issues with center-and-scale normalizations (LayerNorm[1], BatchNorm[2]) in time series Transformers: (1) token shift-centering can flip dot-product signs, (2) attention shift-altering relative importance of tokens, and (3) sparse attention-reducing entropy of attention distributions. UnitNorm is proposed as a scale-only alternative. Theoretical analysis includes a sign-flip probability bound (Theorem 2.1) and an entropy lower bound (Theorem 3.2). Experiments across forecasting (4 datasets), classification (4 datasets), and anomaly detection (1 dataset) with 6 Transformer architectures show UnitNorm often outperforms LayerNorm/BatchNorm in average rank, with notable gains on periodic datasets (ETTh2: -1.46 MSE). However, comparisons miss key time-series normalization baselines (RevIN[3]), and improvements are inconsistent across datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A minimal, easily implementable normalization layer, drop-in for many backbones. (The code is provided with reproducible information.)\n2. Links token scale to attention entropy/sparsity with interpretable bounds. This helps reason about when cantering may hurt.\n3. Linking normalization to attention entropy via Theorems 3.2-3.3 is insightful and not addressed in prior work. The entropy lower bound provides actionable guidance.\n4. Covers multiple architectures and tasks, including forecasting, classification, and anomaly detection, with signs of benefit at longer horizons (6 architectures × 10 datasets × 3 tasks).\n5. Synthetic and probing analyses that visualize how normalization affects token geometry and attention distributions."}, "weaknesses": {"value": "1. Missing time-series baselines for non-stationarity. No comparison with RevIN (input-level reversible instance normalization) and related methods (e.g., DAIN[4]) that explicitly target distribution/regime shifts-central to the paper’s motivation.\n2. Incremental novelty vs. RMSNorm[3]. Algorithmically close to RMSNorm (scale-only) with a fixed modulus; the core novelty is the explicit norm target and entropy framing. More distinctive empirical behavior is needed to clear a top-tier bar.\n3. k-parameter and practical novelty. The proposed approach is presented as a tunable factor modulating attention entropy. However, the paper’s results indicate that fixed values (0.5 – 0.7) are empirically optimal, while learnable values yield no consistent gain. As a result, behaves as a hyper-parameter rather than a learnable control variable. This weakens the claim of architectural innovation, since without adaptive learning or systematic tuning analysis, UnitNorm functions as RMSNorm with an extra scalar rescaling. Demonstrating that a learnable consistently improves training stability or OOD robustness would strengthen the contribution.\n4. Inconsistent effect sizes. Improvements are not universal across datasets/tasks; claims of generality would benefit from per-dataset significance tests and analyses correlating gains with data properties (seasonality, trend, shift severity).\n5. Ablations are not fully developed. How sensitive is performance to? Is learning beneficial/stable? Interactions with Pre-LN vs post-LN, residual scaling, and long-sequence depth are not fully explored."}, "questions": {"value": "1. Authors can consider adding RevIN (and, if possible, DAIN) across the same backbones/splits. Since these are designed for non-stationarity, they are the decisive baselines for your motivation.\n2. Can you include regime-split evaluations (e.g., train on one market regime/weather season, test on another), rolling-origin time-shifted OOD, and cross-dataset transfer to verify robustness claims?\n3. Can you show curves of performance vs. and attention entropy vs. Does learning consistently beat fixed? Any stability issues?\n4. Have you tried UnitNorm at input embeddings (like RevIN), within MHA/FFN, or only at the first layer with re-projection at the end? Any insight on optimal placement(s) and interactions with residual scaling?\n5. Can you report wall-clock/runtime and memory deltas, or anything that implies the efficiency or complexity to compare with baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hmuPZiU9iC", "forum": "3SyoD7yc2l", "replyto": "3SyoD7yc2l", "signatures": ["ICLR.cc/2026/Conference/Submission2879/Reviewer_MaWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2879/Reviewer_MaWK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896155130, "cdate": 1761896155130, "tmdate": 1762916428198, "mdate": 1762916428198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose UnitNorm, a normalization method that scales input vectors by their norms without centering and introduces a scaling factor k. The authors argue that conventional normalization methods (LayerNorm, RMSNorm, etc.) may cause token shift, attention shift, and sparse attention problems in Transformers—especially for time-series forecasting—due to sign flips in normalized representations.\nThe authors provide theoretical analysis (probability of sign flips, gradient invariance, entropy lower bounds), and demonstrate empirical results across forecasting, classification, and anomaly detection tasks.\nWhile the method is conceptually sound and well-motivated, its empirical validation is limited, especially given the emergence of large-scale time-series foundation models (Chronos2, Moirai, Sundial). As a result, the paper’s contribution does not yet meet the bar for such strong generalization claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The approach is well motivated, clearly identifies normalization–attention interactions as a key issue, framing token/attention shift as a quantifiable phenomenon.\n2. Solid theoretical analysis — Includes formal theorems (sign-flip probability, gradient invariance, entropy bound) with proofs in the appendix, giving strong mathematical grounding.\n3./Implementation simplicity — UnitNorm can directly replace existing normalization layers, making it easy to adopt."}, "weaknesses": {"value": "1. Insufficient empirical validation on large-scale datasets and models. The paper mainly uses small to medium-sized benchmarks (ETTh1/2, ECL, Exchange, Solar). However, recent works such as Chronos2, Moirai, and Sundial have trained on large-scale datasets (LOSTA e.g.), together with the large-scale, multi-domain benchmarks (GIFT-EVAL, FEV) that test model robustness and scalability. Without evaluation on these, the claim that UnitNorm “generalizes across time-series domains” is not sufficiently supported.\n2. Theoretical assumptions may not hold in real data.  In real world, time series are often non-Gaussian, autocorrelated, and heteroskedastic. The paper does not provide empirical evidence that sign flips actually occur frequently in practice or that they correlate with performance drops."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dskO8ivClK", "forum": "3SyoD7yc2l", "replyto": "3SyoD7yc2l", "signatures": ["ICLR.cc/2026/Conference/Submission2879/Reviewer_kJDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2879/Reviewer_kJDv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922110741, "cdate": 1761922110741, "tmdate": 1762916427966, "mdate": 1762916427966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits normalization in time-series Transformers and identifies theoretical issues with centering-based normalization methods, such as token-direction distortion and attention misalignment. It introduces UnitNorm, norm-based normalization method keeping vector directions and provides a higher entropy of attention values, enabling models capturing periodicity in time series. The paper combines theoretical analysis, synthetic dataset experiments, and results on multiple time-series tasks to argue that UnitNorm stabilizes attention geometry and yields gains over existing normalization layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Normalization in time-series Transformers is an under-examined but increasingly important research direction.\n- The discussion on vector orientation, sign-preservation, and attention fidelity provides useful intuition to the field.\n- Evaluation spans multiple time-series tasks: forecasting, classification, and anomaly detection."}, "weaknesses": {"value": "- PatchTST, FEDFormer, and CrossFormer performances in this paper are significantly weaker than official numbers, raising validity concerns for all reported gains.\n- PatchTST relies critically on RevIN (Kim et el., ICLR 2022), yet it is unclear whether RevIN is applied fully or consistently. This is a major concern because the presence or absence of RevIN fundamentally alters the statistical properties of time-series input (scale distribution, domain-shift behavior, and vector directions), thereby substantially impacting PatchTST performance and training stability.\n- The use of bold and underline in Table S9 appears inconsistent, making it unclear how to interpret the reported results. Please revise for clarity and consistency.\n- In Table S7, UnitNorm is evaluated across multiple values of k. While it is reasonable that UnitNorm requires selection of k, presenting multiple k values during comparison complicates the fairness and interpretability of the results. This presentation also suggests that UnitNorm may be sensitive to the choice of k. Notably, even with multiple k configurations, LayerNorm or RMSNorm still outperform UnitNorm in several cases.\n- If the authors intend to write classification and anomaly-detection results into the main scripts, then the corresponding results for those tasks should also be included in the main text for completeness and consistency."}, "questions": {"value": "- In Figure 5, the y-axis is described as “average rank over models,” yet the values are shown in the range of 0 to 1. It is unclear how a rank metric can yield values in this range. Could the authors clarify how the rank is computed, and whether this reflects a normalized rank, or some others?\n- The paper does not specify which models were included to compute the “average rank over models” in Figure 5. Please explicitly list the models used for this aggregation, as the lack of this information makes the figure difficult to interpret and reproduce."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p7pT081ne2", "forum": "3SyoD7yc2l", "replyto": "3SyoD7yc2l", "signatures": ["ICLR.cc/2026/Conference/Submission2879/Reviewer_wABE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2879/Reviewer_wABE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994715143, "cdate": 1761994715143, "tmdate": 1762916427517, "mdate": 1762916427517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}