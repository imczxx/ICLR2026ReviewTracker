{"id": "fRNUftg096", "number": 14201, "cdate": 1758230211254, "mdate": 1759897384265, "content": {"title": "Uncertainty quantification in clinical settings: A retinal fundus screening study and benchmarking", "abstract": "We offer the most extensive benchmark for uncertainty quantification (UQ) in retinal AI screening, methodically assessing six well-known UQ techniques in three main diseases: glaucoma (114K+ images), age-related macular degeneration (28K+ images), and diabetic retinopathy (100K+ images). Our benchmark comprises two Vision Transformer topologies, standardized train/test/calibration splits, and evaluation on both public datasets and actual clinical data from a local hospital.\nResults indicate that although UQ is helpful, gains are method- and disease-dependent, and we are still a long way from the ideal clinical target-risk operation. Coverage drastically decreases as risk limits increase, and no single approach is consistently dependable in all contexts.\nWhile neither method consistently outperforms the others, Deep Ensembles and Test-Time Augmentation (TTA) are the two practical UQ approaches that most frequently enhance selective prediction and/or calibration. Conformal Prediction (CP) serves as a must-have safety rail, ensuring alignment between nominal and observed coverage. However, no method can reliably achieve the 2% target risk required for autonomous screening without sacrificing coverage. These findings highlight the need for more robust UQ methods, both for in-distribution scenarios and under domain shifts (out-of-distribution), as well as improved mechanisms for capturing disagreements and implementing policy-aware thresholding in human-in-the-loop workflows. To facilitate progress in this field, we release our benchmark, which includes standardized splits, trained model checkpoints, code, and an online demo for interactive exploration, thereby providing a reference for future UQ research in ophthalmic AI screening.", "tldr": "", "keywords": ["Responsible AI", "Trustworthy AI", "Uncertainty quantification", "Computer-aided diagnosis", "Benchmarking", "clinical validation", "Retinal fundus imaging"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b53f1578a60188c948356ab0c9ffb49a21b7f9ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The article studies classification performance and uncertainty quantification in a retinal fundus screening. The study is based on two vision transformers that are pretrained on ImageNet or on a large dataset of unlabeled fundus images, and evaluates these models both on publicly available datasets (~10-100K images) as well as a separate clinical dataset collected at a local hospital (500 images). Different uncertainty quantification strategies are evaluated and compared to ground-truth test data. Key findings are i) there is a significant drop in classification performance from the public dataset to the private one, raising doubts about the applicability of current machine learning models in clinical practice, ii) uncertainty quantification approaches generally improve reliability, albeit only to a small degree and not consistent with uncertainty patterns reflected in clinicians, iii) split-conformal prediction is essential for callibration, as model confidences and uncertainty estimates tend to be severly overconfident."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a practical article that provides a summary of the status quo in uncertainty quantification and deep learning in a clinically relevant use-case and with actual data from real-world-deployment (lab to clinic). The article is well-written and the research is sound and carried out well. I very much enjoyed reading.\n\nThe fact that lab-to-clinic experiments are made makes the contributions significant and credible. The article has the potential to guide future research in uncertainty quantification and retinal fundus screening and has therefore significant scientific value. I recommend acceptance."}, "weaknesses": {"value": "There are very few weak points that I could identify (see also questions). The article's contribution is not a new methodology but a scientifically sound evaluation of the status quo, which is largely missing in todays AI landscape, so the lack of new methodology should not be counted as weakness.\n\nA weakness appears to be the focus on vision transformer architectures, which tend to be really large machine learning models that require substantial amounts of pretraining. It is unclear whether smaller model architectures would have performed better in this setting, as datasets are relatively small. However, the approaches are representative for the status quo in deep learning.\n\nThe real-world dataset for testing real-world-deployment appears to have a comparable small size (500 instances) from a machine learning perspective."}, "questions": {"value": "Will all models and datasets be publicly released?\n\nCan the authors further motivate the focus on vision transformers for their study?\n\nThe discrepency between model/UQ performance on publicly sourced data and newly collected data is striking to me. How much would a little bit of fine tuning on test-examples help with model performance? Can the authors pinpoint the reasons for this significant drop (e.g. preprocessing of images, different measurement device, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSMZS7Y6J4", "forum": "fRNUftg096", "replyto": "fRNUftg096", "signatures": ["ICLR.cc/2026/Conference/Submission14201/Reviewer_gA5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14201/Reviewer_gA5j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775473670, "cdate": 1761775473670, "tmdate": 1762924653007, "mdate": 1762924653007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a study of a real benchmark that provides reality check on the promises of medical AI.\nThe authors used real-world clinical dataset to measure the crucial \"lab-to-clinic\" gap. Their findings are interesting; we are still a long way from safely automating these screening tasks. This is a good message for the community. Plus, they deserve credit for open-sourcing everything: their code, models, and data splits which sets a high bar for reproducibility and makes their work useful.\nThat said, my main concern is that they put all their eggs in one basket by only using Vision Transformer models. I'm left wondering if these same conclusions hold true for the classic CNNs that many people still use. It feels like a missed opportunity to make their findings more universal. They also completely sidestepped the practical cost of these methods. A \"Deep Ensemble\" sounds great, but it means training and running five models instead of one, which is a massive resource drain. For a hospital on a budget, that’s a potential deal-breaker. A bit of discussion on this trade-off would have made the paper more grounded."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "On top of the points in the summary, these also are strengths in the paper\n•\tComprehensive benchmark covering three diseases with rigorous evaluation metrics.\n•\tStrong commitment to reproducibility (releasing code, models, splits, demo)."}, "weaknesses": {"value": "Many issues with the paper in its current form:\n•\tLimited Novelty: The level of mathematical grounding of the ideas in the paper is somewhat below ICLR standards. The ICLR standards assume theoretical mathematical analysis of “why” these methods fails, in addition to the empirical evidence.\n•\tA main concern is that they put all their eggs in one basket by only using Vision Transformer models. I'm left wondering if these same conclusions hold true for the classic CNNs that many people still use.\n•\tSmall clinical dataset (536 images).\n•       Only two ViT architectures tested, both with frozen features.\n•\tAMD models perform poorly (low AUPRC).\n•\tBenchmarking study with no methodological novelty.\n•\tNo justification for hyperparameters (T=50, N=5, K=20).\n•\tIdentifies domain shift degradation but offers little insight into why or how to fix it"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dFPJI4Urau", "forum": "fRNUftg096", "replyto": "fRNUftg096", "signatures": ["ICLR.cc/2026/Conference/Submission14201/Reviewer_y3sp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14201/Reviewer_y3sp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927287661, "cdate": 1761927287661, "tmdate": 1762924652718, "mdate": 1762924652718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a large-scale benchmark for uncertainty quantification in retinal AI screening across three diseases glaucoma, diabetic retinopathy, and age-related macular degeneration. They evaluate six post-hoc UQ techniques on two ViT backbones (ViT/DINOv2 and RETFound-Green). The authors assemble >114k glaucoma, >100k DR, and >28k AMD fundus images; define standardized train/test/calibration splits; and report discrimination, calibration, selective prediction, and conformal prediction (CP) results, including an external clinical test on a 536-image glaucoma set with 3-rater labels."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clear, end-to-end benchmark framing. The work covers discrimination (AUROC/AUPRC), calibration (ECE/NLL/Brier), selective prediction (AURC, Risk@90% coverage, Coverage@5% risk), and CP validity, with sound descriptions of each metric. \nRisk–coverage analyses are thoughtfully interpreted; ensembles show the most consistent gains in glaucoma/AMD, while signals are weaker for DR. \nExternal clinical test & disagreement analysis. The hospital glaucoma set (n=536) reveals a realistic domain shift; the analysis of physician disagreement vs. UQ scores (with a significance test) is valuable."}, "weaknesses": {"value": "Missing baselines and UQ methods post-hoc calibration baselines like temperature scaling and isotonic regression/Dirichlet calibration are not compared. Similarly, Laplace approximation, SWAG, and evidential deep learning"}, "questions": {"value": "Why was the calibration set drawn from the test sets rather than from held-out training/validation data?\nit would be also useful if the authors provides reliability diagrams (before/after best calibrator) with ECE bins fixed across methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iFpn0E64hg", "forum": "fRNUftg096", "replyto": "fRNUftg096", "signatures": ["ICLR.cc/2026/Conference/Submission14201/Reviewer_Ug1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14201/Reviewer_Ug1G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929871246, "cdate": 1761929871246, "tmdate": 1762924652254, "mdate": 1762924652254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors provide a benchmark for understanding the predicted uncertainty in a highly relevant retinal screening task including the popular diabetic retinopathy detection. The authors just compare the 6 widely used methods for uncertaintiy quantification like the Monte-carlo dropout, test-time augmentations and use the already proposed uncertainty extraction mechanisms for these methods to calculate unertainty and orgainize it as a proper benchmark to accelerate the development of reliable models in this field. Mostly they use the existing labelled datasets for popular tasks like diabetic retinopathy but also include data from some hospital for the macular and galucoma task and evaluate them under setups like domain shift by testing on local clinical dataset. Based on this, the authors draw important conclusions for the effectiveness of these methods. The setups are the usual selective prediction and calibration analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall the analysis can be quite useful for enhancing the research in this direction, since the authors have interesting insights like Glaucoma benefits the most from this uncertainty based selective prediction or deep ensembles emerge as a reliable approach and can decently decompose uncertainty into aleatoric and epistemic components. Also findings like the uncertainty estimates become unreliable under domain shift are intersting. Section 3.5 discussing disagreement analysis among the physicians is also an important point of discussion for this setup."}, "weaknesses": {"value": "The authors have considered very simple methods and have not considered more recent methods for selective classification like the SelectiveNet or Self-adaptive training and have also missed some other works like Deep Gamblers since it is still not exactly clear which of these approaches will work the best for these tasks and whether they are a better alternative to these considered simple method. Also there was another recent paper [1] which proposed extensions to large language models to make them more relialble which also considered the Diabetic Retinopathy dataset and showed advantages for the selective classification setup. Secondly the authors have also missed out on popular calibration methods like focal loss [2]. Like the current benchmarks seem to consider somewhat standard/older methods and is not entirely convincing how much effectively (or not) the current best models can solve this problem. \n\n[1] Plex: Towards Reliability using Pretrained Large Model Extensions\n[2] Calibrating Deep Neural Networks using Focal Loss"}, "questions": {"value": "Please see the weaknesses section. A major question is why only such limited baselines were considered which are not very recent and so it cannot directly indicate the state of model development for these tasks? Also are there any other possible tasks that can be included to make this study more comprehensive or maybe like does these tasks indicate something more about other relevant problems similar in nature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxoLOgGglm", "forum": "fRNUftg096", "replyto": "fRNUftg096", "signatures": ["ICLR.cc/2026/Conference/Submission14201/Reviewer_oGkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14201/Reviewer_oGkH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959562097, "cdate": 1761959562097, "tmdate": 1762924651495, "mdate": 1762924651495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}