{"id": "L4IoWcyMFm", "number": 7315, "cdate": 1758015382626, "mdate": 1759897860367, "content": {"title": "HypBrain: Hyperbolic Space Guided Cross-Subject Vision-Brain Representation Learning Framework", "abstract": "Understanding the intricate mappings between visual stimuli and their corresponding neural responses is a fundamental challenge in cognitive neuroscience and artificial intelligence. Current vision-brain representation learning approaches predominantly align paired images and functional magnetic resonance imaging (fMRI) responses within a shared Euclidean embedding space. However, Euclidean geometry struggles with the exponential complexity of visual/neural hierarchies, resulting in semantically undiscriminating embeddings. To overcome this, we propose HypBrain, a novel framework that leverages hyperbolic geometry to learn a shared, cross-subject vision-brain representation. Our framework maps both visual information and multi-subject fMRI responses into a shared Lorentz model, a geometry uniquely suited for embedding hierarchical data. We introduce a new mapping logic where abstract visual concepts are embedded near the hyperbolic origin, while more specific fMRI responses are situated in the exponentially expanding periphery, naturally capturing the ‚Äúentailment‚Äù relationship between visual and neural data. Notably, we train a hyperbolic encoder on multi-subject fMRI data to integrate both common and unique characteristics of individual brain responses. Experimental results demonstrate that HypBrain not only exhibits robust capabilities in accurately quantifying semantic alignment but also achieves significant advancements in capturing cross-modal semantic relationships solely by optimizing the geometric properties of the embedding space. Our method confirms the superiority of hyperbolic geometry in aligning cross-modal semantic representations and modeling hierarchical associations, thereby offering an innovative perspective in the field of vision-brain representation learning.", "tldr": "", "keywords": ["fMRI", "Hyperbolic Space", "Lorentz", "cross-modal"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3536310a4c56796b48e45836a43529fd47abeb22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Hyperbolic embedding space for fMRI decoding, i.e. retrieval and multi-class label prediction(detection).\nThe main claim is that this representation is beneficial for this task as it can capture well the commonalities and differences in subject brain responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's application of hyperbolic embedding to fMRI decoding is a novel and interesting contribution.\n- The proposed approach demonstrates improved performance on two tasks compared to the baselines."}, "weaknesses": {"value": "The main justification given for the approach is its empirical performance on the two tasks. However, the reported performance is significantly worse than current methods, such as UMBRAE (Xia et al., 2024) and Wills Aligner (Bao et al., 2025), which makes it uncertain if the proposed approach can outperform current leading methods.\n\n\nPapers:\n- Bao, G., Zhang, Q., Gong, Z., Zhou, J., Fan, W., Yi, K., Naseem, U., Hu, L., & Miao, D. (2025). Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding. Proceedings of the AAAI Conference on Artificial Intelligence, 39(13), 14194-14202.\n\n- Xia, W., de Charette, R., √ñztireli, A. C., & Xue, J. (2024). UMBRAE: Unified Multimodal Brain Decoding. European Conference on Computer Vision (ECCV)."}, "questions": {"value": "- Do you think your approach can give performance gap when applied on state-of-the-art approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6o6t2qhulL", "forum": "L4IoWcyMFm", "replyto": "L4IoWcyMFm", "signatures": ["ICLR.cc/2026/Conference/Submission7315/Reviewer_3ddF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7315/Reviewer_3ddF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761044565151, "cdate": 1761044565151, "tmdate": 1762919428412, "mdate": 1762919428412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on fMRI visual decoding research, mainly addressing fMRI-to-visual-concept classification and fMRI-to-image retrieval tasks. The authors introduce hyperbolic space, including Lorentz neural networks and representation learning methods in Lorentz space. The model achieves good results on the classification task, but its performance on the retrieval task is unsatisfactory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The manuscript is reasonably well-structured, and the formatting is clear."}, "weaknesses": {"value": "1. In my view, this paper simply applies established Lorentz-space-based model designs and representation learning methods to the tasks of fMRI classification and fMRI‚Äìimage retrieval. Although the authors emphasize the importance of hyperbolic space in the introduction, I still feel that the paper‚Äôs contribution and insight to the field are rather limited.\n\n2. At present, fMRI visual decoding studies based on the NSD dataset almost inevitably involve the fMRI-to-image reconstruction task. However, this manuscript does not include any evaluation on that task, which I consider to be a limitation.\n\n3. The baseline models compared in the manuscript (ICLR 2024, MM 2024) cannot be considered advanced, and in the retrieval task, the model‚Äôs performance is even far below that of the baselines. This raises concerns about the necessity and effectiveness of introducing hyperbolic space.\n\n4. Some of the formulas in the paper may contain errors or are not clearly explained; see Question 1‚Äì3 below for reference."}, "questions": {"value": "1. Why is ùêπ_{ùëñùëõ}^ùê∏ in line 167 of dimension ùë£+1? Why is there one extra voxel?\n\n2. Are the superscripts ùê∏ and ùêø on ùêπ meant to denote Euclidean space and Lorentz space, respectively? I can guess it, but I think the author should explicitly clarify their meanings.\n\n3. The expressions for ùêπ_{ùëñùëõ}^L and ùêπ_{ùëöùëñùëë}^L in line 170 are strange ‚Äî why do they simultaneously belong to both Lorentz and Euclidean spaces?\n\n4. When the model receives an fMRI scan from a subject, how does it determine which Lorentz Tokenizer should be used for that fMRI?\n\n5. Based on my understanding, does the baseline model EucBrain-CLIP use the pretrained CLIP semantic space? If so, what is the difference between EucBrain-CLIP and MindEye (Backbone + Projector)?\n\n6. Could the authors consider introducing some more recent baselines for comparison? In the classification task, the strongest baseline compared ‚Äî CLIP-MUSED ‚Äî is from ICLR 2024, and in the retrieval task, the strongest baseline ‚Äî Lite-Mind ‚Äî is from MM 2024. I believe that in the past one or two years, there should have been additional methods worth including for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NCCmQLQouJ", "forum": "L4IoWcyMFm", "replyto": "L4IoWcyMFm", "signatures": ["ICLR.cc/2026/Conference/Submission7315/Reviewer_FWnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7315/Reviewer_FWnS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972317940, "cdate": 1761972317940, "tmdate": 1762919428045, "mdate": 1762919428045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article focuses on understanding the intricate mappings between visual stimuli and their corresponding neural responses. Specifically, it tackles vision-brain representation learning to align paired images and fMRI responses, within hyperbolic geometry so as to capture the hierarchical associations. The article introduces a novel vision-brain representation learning framework with carefully designed network and loss functions. The method delivers promising performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation of the approach is clear. The proposed vision-brain representation learning framework is well designed, and novel to me. The core techniques are well justified in ablative experiments. The entire framework shows promising performance on public benchmarks."}, "weaknesses": {"value": "- The rational behind introducing a dedicated hyperbolic tokenizer is unclear to me. Can we achieve tokenization more directly by applying a Lorentz Linear transformation, as used in the image branch? Moreover, there lacks an ablation study to verify the superority of the proposed tokenizer against alternative designs.\n\n- Table 2 reveals substantial performance differences across various VLMs (CLIP, BLIP, DeepSeek). The question is what factors account for these disparities, why do certain VLMs excel while others underperform on this task? Additionally, it would be valuable to explore how performance scales with larger VLMs, or even closed-source models like GPT4o.\n\n- How is the value of coefficient \\lambda determined? How sensitive is the overall performance to the value?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Df8QUhPPxz", "forum": "L4IoWcyMFm", "replyto": "L4IoWcyMFm", "signatures": ["ICLR.cc/2026/Conference/Submission7315/Reviewer_niWa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7315/Reviewer_niWa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994631831, "cdate": 1761994631831, "tmdate": 1762919427632, "mdate": 1762919427632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HypBrain, a framework for cross-subject vision-brain representation learning that maps fMRI signals and image embeddings into a shared Lorentz manifold (hyperbolic space). The method uses a novel hyperbolic fMRI encoder and a combined hyperbolic contrastive and entailment loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The results are good. HypBrain consistently outperforms its direct Euclidean counterpart (EucBrain) in multi-label prediction and retrieval tasks. This strongly supports the central hypothesis that hyperbolic space is advantageous for semantic alignment in this domain.\n\nThe ability to achieve near full-data performance on a new subject (S7) with only 50% of the subject's data is a significant practical advantage for fMRI research, which is limited by data scarcity.\n\nFigure 3 visually confirms the desired geometric separation: abstract Image embeddings cluster near the hyperbolic origin, while specific fMRI responses are pushed to the periphery.\n\nCompetitive Efficiency: HypBrain is highly competitive with powerful SOTA generative models (like MindEye), achieving high performance with a fraction of the parameter count (around 40M vs. 1000M+). This is well-argued as a focus on high-level semantic alignment over low-level pixel reproduction."}, "weaknesses": {"value": "I have problems with section 2.1.1 and the equations.\nIs Fin defined in line 170 as the projection of Fin in line 168 to hyperbolic space? Shouldn‚Äôt the dimension be the same one + 1 time dimension?\nWhat is cin in equation 1? \nEquation 1 follows Equation 16. However, why is a transform of Fin calculated first? I also don‚Äôt get where the projection to the Lorentz model via the exp map is performed. Then, in equation 2, to have the representation, there is a concat with a rescale of cmin and cin. But does the paper assume that Fmid is lying in a space with cmid curvature? Having the affine transform can break this assumption. Can the authors provide a clear mathematical derivation for these equations to show what space they are in and make sure that the final representation is a valid Lorentz representation in a space with cmid?\n\nI also have a problem with equation 5. Comparing it with equation 21, why is cmid used? Can the authors provide elaborations on this equation? In line 210, what is Mmid? It is not defined. \nLorentzian MLP and the process to get Umid are unclear. \n\nThe paper motivates the use of hyperbolic space to capture hierarchical and is-a relationships. However, it does not include any analysis of the hierarchical structure within the learned space. Figure 3 indicates that the image representations are positioned closer to the origin, but no further examination of the space‚Äôs hierarchical properties is provided.\n\n\nWould you please provide detailed, explicit information on the differences in the pipelines for EUCBrain and HypBrain?\n\nI would highly recommend citing more hyperbolic-related work, such as Hyperbolic Deep Learning in Computer Vision: A Survey. \n\n\nI have a suggestion for the title: HYPBRAIN: A Hyperbolic Space‚ÄìGuided Framework for Cross-Subject Vision‚ÄìBrain Representation Learning\n\n\nIt would be more beneficial to provide citations inside Table 1. \nThe sentence in line 337 does not have a meaning. \nLine 54, why is the citation black?\nFigure 1 : cat and cat and dog have the same distance from the origin?"}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EFQfooYhrb", "forum": "L4IoWcyMFm", "replyto": "L4IoWcyMFm", "signatures": ["ICLR.cc/2026/Conference/Submission7315/Reviewer_HG9t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7315/Reviewer_HG9t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179346093, "cdate": 1762179346093, "tmdate": 1762919427272, "mdate": 1762919427272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}