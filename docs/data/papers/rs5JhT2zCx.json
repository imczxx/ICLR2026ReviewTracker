{"id": "rs5JhT2zCx", "number": 15302, "cdate": 1758250067800, "mdate": 1763719860276, "content": {"title": "ROAM: A Relation-aware Optimal Transport-based Adaptive Mixture-of-Expert-Group Framework for Multimodal Knowledge Graph Completion", "abstract": "Multimodal Knowledge Graph Completion (MMKGC) aims to predict missing facts by reasoning over heterogeneous information sources, including structural, textual, and visual modalities. A central challenge in this task lies in effectively integrating modality-specific information while preserving their distinct semantics and mitigating cross-modal interference. Recent efforts have explored employing Mixture of-Experts (MoE) architectures to address this issue. However, many of these approaches rely on predefined expert routing or task-agnostic distance measures, thereby limiting their adaptability and performance. This paper proposes ROAM, a Relation-aware Optimal Transport-based Adaptive Mixture-of-Expert-Group framework, which dynamically routes modality-specific embeddings across expert groups conditioned on relation semantics. Specifically, ROAM first establishes modality-specialized expert groups to disentangle representation learning across modalities. Then, an Optimal Transport-based gating mechanism is introduced with a learnable, relation-conditioned cost function. Expert groups are further represented dynamically via their constituent parameters, enabling context-sensitive routing and capturing relation-aware specialization. Extensive experiments on multiple MMKGC benchmarks demonstrate that ROAM achieves state-of-the-art performance, achieving up to 9.76% relative gains.", "tldr": "", "keywords": ["Multimodal Knowledge Graph", "Mixture-of-Expert-Groups", "Optimal Transport", "Conditional Aware Optimal Transport"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cc9cc58a6f91e96bda9f606ce49fb68e45328cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents ROAM, a new framework for MMKGC. ROAM integrates Optimal Transport and Mixture-of-Experts architectures via relation-aware adaptive expert routing. The model consists of two main modules: MoSEG, which builds modality-specific expert groups to reduce cross-modal interference, ReGOR, a relation-guided OT routing mechanism with a learnable cost function for dynamic group selection. Extensive experiments on four MMKGC benchmarks demonstrate consistent state-of-the-art results, with up to 9.76% relative improvement, supported by ablation and efficiency analyses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Originality:**\n\n1.The first work to unify OT and MoE under a relation-aware routing paradigm for MMKGC.\n2.Proposes dynamic expert-group representation using learnable parameters, which is superior to the fixed prototype-based model.\n\n**Quality:**\n\n1.Theoretical formulation is solid and mathematically detailed.\n2.Comprehensive experiments across multiple datasets and baselines with fair comparisons. Ablation, sensitivity, and efficiency analyses convincingly support the claims.\n\n**Clarity:**\n1. Well-structured presentation with clear modular separation between MoSEG and ReGOR.\n\n**Significance:** \n\n1. The framework achieves notable performance gains."}, "weaknesses": {"value": "1.Although complexity is analyzed, ROAM incurs higher time and memory costs compared to some baselines (e.g., MMKRL), with limited discussion on practical deployment. And the scalability discussion is limited, it remains unclear how ROAM performs on large-scale knowledge graphs.\n2.The paper lacks interpretability analysis, e.g., visualization of expert activation under different relation types."}, "questions": {"value": "1.How robust is ROAM in scenarios with weak or noisy relational semantics?\n2.Could the efficiency of ReGOR be further improved, e.g., via sparse routing or approximate OT methods?\n3.Part of the font in Figure 1 is somewhat unclear and can be appropriately resized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QfGP3yyH3G", "forum": "rs5JhT2zCx", "replyto": "rs5JhT2zCx", "signatures": ["ICLR.cc/2026/Conference/Submission15302/Reviewer_w6dR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15302/Reviewer_w6dR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442774558, "cdate": 1761442774558, "tmdate": 1762925598321, "mdate": 1762925598321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of multimodal information fusion in Multimodal Knowledge Graph Completion (MMKGC) by proposing ROAM, a Relation-aware Optimal-transport-based Adaptive Mixture-of-expert-group framework. The key contributions include: (1) enhancing multimodal semantic fusion through relation-aware modeling, (2) optimizing mixture-of-experts gating via Optimal Transport theory, and (3) improving modality-specific routing by dynamically representing experts based on their parameters. Experimental results demonstrate consistent improvements over existing methods across multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This work represents the first integration of Optimal Transport with Mixture-of-Experts for MMKGC, offering a novel perspective on multimodal fusion.\n\n2.The proposed method achieves measurable performance gains across several established benchmarks, validating its practical effectiveness."}, "weaknesses": {"value": "1.The distinction between ROAM and Sinkhorn-MoE — particularly regarding the integration of OT and MoE — remains inadequately clarified, beyond differences in task specificity.\n\n2.Ablation studies on the ReGOR component show only marginal differences between variants, raising questions about the necessity of its individual elements."}, "questions": {"value": "1.Could the authors elaborate on the fundamental differences between ROAM and Sinkhorn-MoE in terms of how OT and MoE are combined, beyond task-level distinctions?\n\n2.Given the limited performance gaps observed in the ReGOR ablations, what further evidence supports the necessity of each sub-component? Are there qualitative or quantitative analyses that better demonstrate their contributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RJaLpXOHUs", "forum": "rs5JhT2zCx", "replyto": "rs5JhT2zCx", "signatures": ["ICLR.cc/2026/Conference/Submission15302/Reviewer_dMkC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15302/Reviewer_dMkC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478230644, "cdate": 1761478230644, "tmdate": 1762925597690, "mdate": 1762925597690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ROAM, a novel framework for Multimodal Knowledge Graph Completion that integrates Mixture-of-Experts with Optimal Transport in a relation-aware manner. ROAM organizes modality-specific experts into groups and employs a relation-guided OT routing mechanism with a learnable cost function, enabling dynamic and context-sensitive expert selection. Extensive experiments on four benchmarks demonstrate that ROAM achieves state-of-the-art performance, with significant improvements over existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper includes rigorous ablation studies, hyperparameter analysis, and statistical significance tests, providing strong empirical support for the proposed model’s effectiveness.\n\nS2. The paper is well-written and easy to follow. The paper is well-structured."}, "weaknesses": {"value": "W1. The motivations for certain key technical designs remain unclear. For instance, why does the author employ expert groups? What does each group represent, and what specific information do they capture? Additionally, the relation condition is already utilized within each expert. Why is it necessary to reuse this condition during routing to incorporate relational information again?\n\nW2. The key technical contribution of the paper remains unclear. Given that both Mixture-of-Experts and Optimal Transport are already well-established techniques in multimodal knowledge graph learning, the authors should more explicitly highlight their novel methodological elements. Furthermore, the core motivation behind this work requires further clarification. Simply stating that \"no prior work has unified OT and MoE for the MMKGC task\" does not sufficiently justify the contribution; a deeper rationale explaining why such a unification is beneficial or necessary should be provided."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bsT79G6Hp2", "forum": "rs5JhT2zCx", "replyto": "rs5JhT2zCx", "signatures": ["ICLR.cc/2026/Conference/Submission15302/Reviewer_36XF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15302/Reviewer_36XF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494730341, "cdate": 1761494730341, "tmdate": 1762925597048, "mdate": 1762925597048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ROAM, a novel framework for Multi-Modal Knowledge Graph Completion that integrates Modality-Specific Expert Groups with a Relation-Guided Optimal Transport Routing mechanism. ROAM constructs expert group representations directly from their constituent parameters and conditions the transport cost on relational semantics. The results across multiple datasets demonstrate state-of-the-art performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed ReGOR module introduces a relation-aware, learnable cost function, which is a significant advancement over prior OT-MoE methods that use fixed or prototype-based costs.\n\nROAM consistently outperforms existing methods across four benchmarks, with notable gains in Hits@1 and MRR.\n\nDespite its complexity, ROAM maintains reasonable training time and GPU memory usage."}, "weaknesses": {"value": "Table 1 is quite confusing. Does it imply that the proposed ROAM has the most powerful designed features? But I don't think more designed features equate to more strengths and advantages. While efficiency and theoretical complexity are analyzed, the scalability limits for large KGs are still a potential issue for the proposed method.\n\nThe proposed ROAM seems like a combination of existing methods. For example, MoSEG is an extension of MoMoK with group of experts, while ReGOR simply fuses the relation embedding vector into the entity embedding for computation."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pVsLv9y9jo", "forum": "rs5JhT2zCx", "replyto": "rs5JhT2zCx", "signatures": ["ICLR.cc/2026/Conference/Submission15302/Reviewer_rWS6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15302/Reviewer_rWS6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15302/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640839083, "cdate": 1761640839083, "tmdate": 1762925596725, "mdate": 1762925596725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}