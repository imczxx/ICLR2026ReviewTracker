{"id": "FwtgOQvMol", "number": 13285, "cdate": 1758216022322, "mdate": 1759897448807, "content": {"title": "Qualitative and Quantitative Quality Assessment of Low-Light Enhanced Images: A Dataset and Benchmark Metric", "abstract": "Low-light image enhancement (LLIE) improves visibility and restores details in challenging lighting conditions. It is crucial to fairly evaluate LLIE methods to foster the development of more effective models. However, quality assessment of low-light enhanced images proves to be as challenging as the enhancement itself. From a quantitative perspective, full-reference image quality assessment (FR-IQA) metrics (e.g., PSNR and SSIM) are commonly employed to assess the perceptual quality of enhanced images. However, they are not suitable when a pristine reference image is unavailable, which is often the case in real-world applications. From a qualitative perspective, the absence of a standardized and reproducible evaluation pipeline makes it extremely difficult to ensure fair comparisons across different studies. To confront these challenges, we present the Low-light Image Distortions and Quality (LIDQ) dataset, featuring both overall quality scores and distortion distribution annotations collected through formal subjective testing. Leveraging LIDQ, we propose a no-reference Low-light Enhanced Image Quantitative and Qualitative Quality Assessment (LIQ$^3$A) method that not only estimates perceptual quality without requiring a reference, but also provides qualitative assessments of enhancement-induced distortions. Experiments show that LIQ$^3$A aligns closely with human perception while accurately identifying distortion patterns. We anticipate that the proposed dataset and metric will facilitate future advances in low-light image enhancement by providing reliable evaluation feedback.", "tldr": "", "keywords": ["Image quality assessment; low-light image enhancement; qualitative quality assessment; vision-language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9cf93418d06d352a3e61eeddeec169752ba3930.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper mainly introduces a dataset (LIDQ) for quality assessment of low-light image enhancement and a no-reference evaluation metric (LIQ3A). The authors collected 253 low-light images, generated 5,566 enhanced images using 21 existing enhancement algorithms, and obtained both quantitative mean opinion scores (MOS) and qualitative distortion annotations through subjective experiments. Based on this dataset, they trained a multitask learning framework, LIQ3A, built on the SigLIP-2 vision-language model, which can simultaneously predict overall image quality scores and distortion distributions. Experimental results show that LIQ3A achieves better consistency with human perception compared to other BIQA models and can effectively identify major distortion types. However, overall, the workload of this study is relatively limited, mainly focusing on applying existing models and integrating data rather than developing fundamentally new methodologies. The methodological innovation is weak and the contributions are not very significant. Moreover, many similar datasets and no-reference evaluation methods (e.g., LIEQ, LEISD, MLIQ) already exist in the field of low-light image quality assessment, so the incremental novelty of this paper is quite limited."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.The paper is well-organized and clearly written, with good formatting and visual presentation that make it easy to follow. The figures, tables, and examples are clearly labeled and well integrated into the main text."}, "weaknesses": {"value": "1.The paper does not provide public access to the benchmark, dataset, or model implementation, which severely limits its reproducibility and usability for the research community. Without publicly available resources, the claimed benchmark value of LIDQ and the practical utility of LIQ3A cannot be independently verified or fairly compared.\n\n2.The contribution is not sufficiently novel. There already exist multiple well-established datasets and benchmarks for low-light image enhancement and quality assessment, such as MLIQ (Wang et al., 2024a), which provide larger sample sizes, richer distortion annotations, and stronger validation protocols. Compared to these prior works, the proposed LIDQ dataset is relatively small in scale and lacks distinctive innovations.\n\n3.The dataset and method coverage in this paper are not comprehensive. Many recent low-light enhancement algorithms and blind IQA models are not included in the benchmark, which limits the generality of the conclusions. Expanding the dataset to cover more methods and more diverse real-world scenes, and including comparisons with stronger baselines, would make the study more convincing and complete.\n\n4.The experiments are limited to the authors’ own dataset (LIDQ) and a few cross-dataset tests. There is no large-scale validation on real-world images or user studies demonstrating the practical benefit of LIQ3A in real enhancement pipelines. The improvement margins over prior BIQA methods are small and sometimes inconsistent across datasets.\n\n5.LIQ3A mainly adapts an existing vision–language backbone (SigLIP-2) with minor architectural modifications and loss combinations. There is no fundamentally new model design or theoretical innovation; the main novelty lies only in problem framing. This makes the technical contribution rather incremental.\n\n6.While the paper claims to integrate qualitative distortion estimation into BIQA, the actual benefit of this joint learning is not clearly demonstrated. There is no ablation showing how qualitative learning improves quantitative prediction or vice versa.\n\n7.The paper mainly reports correlation scores (SRCC, PLCC) but lacks deeper analysis—such as qualitative failure cases, human perception alignment studies, or task-level relevance (e.g., whether better LIQ3A scores correlate with downstream detection/segmentation performance).\n\n8.The conclusion briefly mentions data volume as a limitation but does not analyze other important aspects (e.g., subjectivity bias in MOS collection, bias toward specific enhancement types, or computational efficiency of LIQ3A)."}, "questions": {"value": "1.Will the authors release the LIDQ dataset, benchmark results, and LIQ3A code? Public access would greatly improve reproducibility, fairness, and the long-term impact of this work.\n\n2.How does LIDQ differ from existing DBs such as MLIQ (Wang et al., 2024a)? Please clarify what specific gaps these prior works do not address and why a new dataset is needed.\n\n3.Expanding the benchmark to cover more recent methods and models would make the evaluation more comprehensive and convincing.\n\n4.Have the authors validated LIQ3A on larger or more diverse real-world datasets (like SID)? Broader experiments would better demonstrate its generalizability and practical value.\n\nSID: Chen, Chen, et al. \"Learning to see in the dark.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n5.Can the authors show ablation results to verify whether qualitative distortion estimation improves quantitative prediction? This would clarify the benefit of the multitask design.\n\n6.Beyond correlation metrics, can the authors provide more qualitative analysis—such as failure cases or perceptual alignment, to improve interpretability and insight?\n\n7.Besides dataset size, what other limitations exist (e.g., annotation bias, computational cost)? A clearer discussion would strengthen transparency and credibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bJOquvxREV", "forum": "FwtgOQvMol", "replyto": "FwtgOQvMol", "signatures": ["ICLR.cc/2026/Conference/Submission13285/Reviewer_42jA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13285/Reviewer_42jA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761440610678, "cdate": 1761440610678, "tmdate": 1762923958373, "mdate": 1762923958373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigate the vital problem of quality assessment for low-light enhanced images. A benchmark dataset and a quality-assessment method is proposed for this problem. Extensive experiments shows that proposed LIQ^3A aligns closely with human preception quality without the need of reference images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ This manuscript attempts to solve a long-standing problem of evaluating low-light enhanced images in a preceptual-aligned way.\n+ The manuscript is well-written and well-organized.\n+ This work presents a neat dataset for training IQA in low-light scenarios, which can be a neat contribution to the community."}, "weaknesses": {"value": "+ LLIE method involoved are trained/evaluated on LOL dataset. It tends to perform well on these datasets. However, in practice, these LLIE methods often fail in real-world scenarios. It is vital to evaluate the LLIE performance on real-world scenarios, say ExDark dataset. \n+ In my experience, the degradation type annotation of the proposed method is not comprehensive. The output sometime contains severe artifact (Fig.4 g, h). It would be better to annotate this type of degradation.\n+ Fig. 5 only exhibit 4 cases. It would be better to see more visual cases to confirm the performance gain.\n+ How to define primary distortation and secondary distoration? What if an image contains multiple, mixed degradation?"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oHbRsS91IQ", "forum": "FwtgOQvMol", "replyto": "FwtgOQvMol", "signatures": ["ICLR.cc/2026/Conference/Submission13285/Reviewer_BESs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13285/Reviewer_BESs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909003469, "cdate": 1761909003469, "tmdate": 1762923957921, "mdate": 1762923957921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the challenges in evaluating low-light image enhancement (LLIE) methods. The paper presents two primary contributions. First, it introduces the LIDQ dataset, a new benchmark for LLIE evaluation. Second, leveraging this dataset, the authors propose LIQ³A, a no-reference image quality assessment (NR-IQA) model. LIQ³A is designed as a multitask learning framework, built upon a pretrained vision-language model (SigLIP-2), to concurrently predict a quantitative quality score and estimate a qualitative distortion distribution. The authors report that their experiments demonstrate that the proposed model aligns well with human perception and can effectively identify distortion patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical foundation of the proposed evaluation metric (LIQ³A) is sound. The multitask approach, which simultaneously assesses quantitative quality and qualitative distortion patterns is well-motivated.\n2. The method achieves good performance on the results reported by the authors."}, "weaknesses": {"value": "1. The model has three losses: a ranking loss, a scoring precision loss, and a loss for estimating degradation type. It is necessary to conduct an ablation study on whether the weights of these three losses affect the output results. Why is there not a single ablation study in the paper?\n2. The dataset was initially created through human scoring and identification of degradation types, with each image being limited to only two degradation types. This method feels like it could have considerable bias. This is especially true for images with multiple degradations. It is not only difficult for the naked eye to judge completely, but degradations are also often composite. Therefore, it is not ideal to only allow annotators to specify one primary and one secondary degradation.\n3. Table 3 shows the superior performance of the authors' proposed method on their own LIDQ dataset. This is to be expected, as the model was trained on the training set of this dataset, and the test set comes from the same distribution. This does not prove the model's generalization ability.\n4. The evaluation on datasets like LoL is not comprehensive and potentially not accurate, because it is too small and comes from the same distribution as the training set. The comparison needs to cover more out-of-domain, real-world datasets, such as ExDark, DarkFace, or other evaluation data.\n5. The cross-dataset test in Table 4 is key to examining the model's generalization, but the authors' proposed method does not perform stably here, could even say it is not ideal. On the LIEQ dataset, its performance (SRCC 0.8165) is significantly lower than that of VisualQuality-R1 (SRCC 0.8487). Although it leads on the LEISD and Hybrid-LLIE datasets, the advantage is marginal."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U6AgeGpW0w", "forum": "FwtgOQvMol", "replyto": "FwtgOQvMol", "signatures": ["ICLR.cc/2026/Conference/Submission13285/Reviewer_Ac2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13285/Reviewer_Ac2z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977292232, "cdate": 1761977292232, "tmdate": 1762923957606, "mdate": 1762923957606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LIDQ, a new dataset for evaluating low-light image enhancement, containing both MOS quality scores and artifact annotations collected through controlled subjective studies.\nBased on LIDQ, the authors propose LIQ3A, a no-reference metric built on SigLIP-2 to jointly predict quality scores and artifact distributions via multitask learning. LIQ3A achieves strong correlations (SRCC/PLCC) and shows effectiveness in cross-dataset testing and perception-guided refinement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset provides more fine-grained annotations, including large-scale MOS-based quantitative labels.\n2. A unified BIQA framework jointly estimates overall quality and artifact characteristics.\n3. Extensive experiments are conducted, offering comprehensive validation."}, "weaknesses": {"value": "1. Although LIDQ supplements artifact annotations, it is essentially an extension of existing LLIE IQA datasets.\n2. LIQ3A is mainly composed of existing components—SigLIP-2, prompt templates, and a multitask framework—without introducing substantially new architectures or theoretical innovations.\n3. The number of subjects involved in the subjective annotation process remains relatively limited.\n4. In addition, the quantitative performance gain over strong existing baselines (e.g., LIQE) is modest."}, "questions": {"value": "1. Has the method been compared against lighter or purely vision-based backbones? If similar performance can be achieved without a VLM, does this imply that the multimodal design contributes only marginally?\n2. Would the artifact distribution prediction remain reliable with a larger dataset or more annotators? Has the impact of class imbalance on qualitative prediction been evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oSPxBCb3q9", "forum": "FwtgOQvMol", "replyto": "FwtgOQvMol", "signatures": ["ICLR.cc/2026/Conference/Submission13285/Reviewer_2dhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13285/Reviewer_2dhb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13285/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982082756, "cdate": 1761982082756, "tmdate": 1762923957281, "mdate": 1762923957281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}