{"id": "FooLtc0uvn", "number": 12173, "cdate": 1758206151959, "mdate": 1759897527469, "content": {"title": "KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning", "abstract": "Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing  their knowledge manipulation performance—the ability to effectively **recall, reason, and transfer relevant knowledge**—still remains challenging. \nExisting methods mainly leverage supervised fine-tuning (SFT) to enable LLMs to recall task-relevant knowledge by continuing the training process on labeled datasets. However, we observe that LLMs fine-tuned via SFT still occasionally exhibit the *known\\&incorrect* phenomenon, where LLMs explicitly possess the relevant knowledge of a given question but cannot effectively manipulate it to answer correctly. To address this challenge, we propose KALE—a novel post-training framework that leverages knowledge graphs (KGs) to generate high-quality relevant rationales and enhance the knowledge manipulation ability via **K**nowledge-**A**ware **LE**arning. Specifically, KALE **first** proposes a **K**nowledge-**I**nduced (KI) data synthesis method to generate high-quality data rationales, i.e., a textual reasoning process from each question to correct answer through external KGs. **Then** KALE proposes a **K**nowledge-**A**ware (KA) fine-tuning paradigm to enhance the knowledge manipulation ability of LLMs. Extensive experiments on **eight** popular benchmarks across **six** different LLM backbones demonstrate the effectiveness of KALE, leading to an accuracy improvement of up to 11.72\\% and an average of 4.18\\%.", "tldr": "We propose KALE, a novel post-training framework that leverages knowledge graphs to generate high-quality relevant rationales and enhance the knowledge manipulation ability of large language models.", "keywords": ["Large Language Models", "Knowledge Manipulation", "Post-training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f67b6c5508a3f19eb28323007fa2e19d8a0e983.pdf", "supplementary_material": "/attachment/260a7e74dfe12367d2bcdea2ab23ee5be4125969.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles the “known & incorrect” failure in LLMs—models hold the right facts but fail to use them—and proposes KALE, a post-training framework that strengthens knowledge manipulation (recall, reasoning, transfer).  KALE has two parts: Knowledge-Induced data synthesis that extracts multi-hop reasoning paths from external knowledge graphs and uses them to generate high-quality textual rationales, and Knowledge-Aware fine-tuning that aligns the model’s token distributions *with* and *without* these rationales by minimizing the KL divergence, encouraging the model to internalize rationale information so it can retrieve and apply relevant knowledge even when no rationale is provided at test time. Across eight benchmarks and six backbones, KALE consistently outperforms strong baselines, underscoring more reliable knowledge use."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper formalizes the “known & incorrect” gap and empirically shows this failure mode remains common after SFT, which shows a clear problem framing and strong motivation.\n2. The paper uses external KGs to extract multi-hop reasoning paths → generate textual rationales (KI), then minimize KL divergence between outputs with/without rationales for knowledge-aware fine-tuning (KA), so the model can retrieve relevant knowledge even when no rationale is provided at inference. The pipeline is coherent and goal-aligned.\n3. The main experiment results and ablation studies consolidate the effectiveness and scalability of the proposed frameworks."}, "weaknesses": {"value": "1. All experiments fine-tune on each benchmark’s training set separately. This setup resembles “task-specific adaptation” rather than evaluating cross-task generalization.\n2. While the authors emphasize no extra inference-time cost, training includes:\n(1) path extraction from large KGs (still requires full preprocessing, though faster than BFS),\n(2) GPT-4o calls for rationale generation (API cost and reproducibility issues), and\n(3) KL-based consistency training (dual forward passes).\nCombined, these are likely much heavier than inference-time methods."}, "questions": {"value": "1. How does performance degrade as KG coverage/quality drops (e.g., ablate edges, introduce noise)? Any robustness to wrong or conflicting triples, and do you weight paths by confidence?\n2. When no full path connects question to answer, what fraction of training pairs fall back to partial paths, and how does that affect accuracy?\n3. If you inject KI-style rationales at inference (without KA training), how much do base and SFT models improve, and can base + rationales ever surpass KALE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sd4ifB51fZ", "forum": "FooLtc0uvn", "replyto": "FooLtc0uvn", "signatures": ["ICLR.cc/2026/Conference/Submission12173/Reviewer_DHgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12173/Reviewer_DHgt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760717084560, "cdate": 1760717084560, "tmdate": 1762923124379, "mdate": 1762923124379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Large Language Models (LLMs) often struggle with \"knowledge manipulation,\" failing to answer questions correctly even when they possess the necessary information, a phenomenon known as \"known & incorrect.\" This paper proposes KALE, a post-training framework that uses Knowledge Graphs (KGs) to generate data rationales, creating structured reasoning paths for Q&A pairs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's problem definition is clear and significant. The \"known&incorrect\" phenomenon is a key pain point in LLM research. The authors clearly articulate this problem with illustrative cases, providing a strong motivation.\n\n2. The paper proposes a novel data generation framework (KI) that combines KG paths with LLM generation. This offers a systematic method for creating high-quality reasoning data with a clear logical basis.\n\n3. The authors conducted extensive experiments on 8 benchmarks and 6 different LLM backbones."}, "weaknesses": {"value": "1. The attribution of efficacy for the KI synthesis stage, a core contribution in this paper, is severely confounded. The process is critically dependent on a powerful, SOTA proprietary model (GPT-4o) to \"translate\" KG paths into \"high-quality\" rationales. This makes it difficult to discern if the performance gains stem from the KALE framework's superiority or simply from distilling a stronger \"teacher\" model. The authors' own results in Appendix Q (Table 18) amplify this concern: using a weaker rationale generator (Llama3 70B), KALE's performance on key benchmarks (AbsR, Common, MMLU, BBH) falls below the $KALE_{w/o~KI}$ ablation baseline (Table 2). This strongly suggests KALE's success relies heavily on the external teacher's capability, not its framework's generalizability.\n\n2. The novelty of the second core innovation—Knowledge-Aware fine-tuning—is limited. The method's use of KL divergence to align model (no rationale) and teacher (with rationale) distributions is a mature technique in knowledge and self-distillation. For instance, recent work[1] has employed nearly identical KL-divergence SFT for similar motivations. The paper lacks a sufficient comparison and differentiation from this prior art.\n\n3. The experimental design completely omits a mainstream and powerful alternative: Outcome-Based Reinforcement Learning. This RL approach, which hypothesizes that rewarding final outcomes is sufficient for implicit reasoning, circumvents KALE's central challenge: the \"lack of high-quality textual reasoning data.\" A discussion and empirical comparison with RL methods is strongly suggested.\n\n[1] Efficient Knowledge Injection in LLMs via Self-Distillation."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8E4DLOYWHh", "forum": "FooLtc0uvn", "replyto": "FooLtc0uvn", "signatures": ["ICLR.cc/2026/Conference/Submission12173/Reviewer_aHZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12173/Reviewer_aHZc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674438864, "cdate": 1761674438864, "tmdate": 1762923123970, "mdate": 1762923123970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KALE, a two-stage framework to improve knowledge manipulation in large language models.\n- First, a Knowledge-Induced (KI) data generation step uses external knowledge graphs (e.g., Wikidata) to extract multi-hop reasoning paths and generate rationales via GPT-4o.\n- Second, a Knowledge-Aware (KA) learning paradigm minimizes the KL divergence between distributions of models trained with and without rationales, encouraging internalization of explicit reasoning.\n- Experiments on multiple knowledge-intensive benchmarks (MMLU, RACE, ARC, BBH, AbsR) show consistent accuracy improvements."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method elegantly integrates external structured knowledge with rationale-based learning, addressing the “known-but-incorrect” problem in LLMs.\n\n2. The proposed model consistently improves across model scales (7B–32B), showing stable generality."}, "weaknesses": {"value": "1. No evaluation on open-ended generation or general abilities. The paper focuses solely on accuracy in knowledge tasks and does not verify whether KALE harms general fluency or creativity after fine-tuning.\n\n2. No comparison with modern reasoning or “thinking-style” models.\nBaselines (ToG, StructGPT, GraphRAG) are early models and do not include current SOTA models like DeepSeek-R1, Qwen2.5-Think, or Llama3 thinking model. Hence, the claimed “SOTA” results may be overstated."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mRGS47lqXb", "forum": "FooLtc0uvn", "replyto": "FooLtc0uvn", "signatures": ["ICLR.cc/2026/Conference/Submission12173/Reviewer_EeXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12173/Reviewer_EeXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965283236, "cdate": 1761965283236, "tmdate": 1762923123576, "mdate": 1762923123576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes knowledge-aware learning (KALE), which consists of two key components: knowledge-induced data synthesis (KI) to generate high-quality relational data, and knowledge-aware fine-tuning (KA) to enable large language models (LLMs) to better manipulate task-relevant knowledge. For KI, the method finds multiple reasoning paths that connect a question entity to an answer entity using the A* algorithm, where a heuristic function is derived from anchor entities. These rationales are expected to provide high-quality textual reasoning data that bridges the question and answer. For KA, the approach minimizes the divergence between the generative distributions with and without the KI-based rationales. Experimental results on various benchmarks demonstrate that KALE outperforms several baseline models, validating its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed knowledge-aware learning (KALE) framework, which integrates knowledge-induced data synthesis (KI) and knowledge-aware fine-tuning (KA), is novel and interesting. The rationale extraction process is well designed to enhance efficiency through the use of anchor entities and a three-step BFS strategy.\n2.\tThe experimental results demonstrate that the proposed KALE framework achieves notable performance improvements, and the ablation studies clearly illustrate the individual effects of KI and KA.\n3.\tThe presentation is clear, well-structured, and generally easy to follow, with good overall organization."}, "weaknesses": {"value": "1.\tThe extracted rationales are regarded as a form of Chain-of-Thought (CoT), but it remains unclear why KA is formulated based on the KL divergence between the generative distributions with and without rationales. What is the motivation for using KL divergence, compared to more conventional fine-tuning approaches that jointly generate both rationales and answers under an autoregressive loss?\n2.\tFrom a data augmentation perspective, it is unclear why the generated dataset is relatively large compared to those of other baseline methods. How does the size of the automatically augmented dataset compare quantitatively to other approaches?\n3.\tThe proposed rationale extraction relies on named entity recognition (NER) and a graph-based search using the A* algorithm. However, GPT-generated rationales could also be used for training on question–answer pairs. Why does the model exhibit improved performance on test questions for which such rationales are not available? It also remains unclear how the model performs when rationales for question entities are absent in the training data."}, "questions": {"value": "1.\tIt remains less convincing that the proposed rationale extraction process is truly necessary. Could simpler or alternative search-based methods address this problem as effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1ulOQmmSjD", "forum": "FooLtc0uvn", "replyto": "FooLtc0uvn", "signatures": ["ICLR.cc/2026/Conference/Submission12173/Reviewer_QCEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12173/Reviewer_QCEj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126226492, "cdate": 1762126226492, "tmdate": 1762923123184, "mdate": 1762923123184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}