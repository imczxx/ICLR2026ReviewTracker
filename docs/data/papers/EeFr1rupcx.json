{"id": "EeFr1rupcx", "number": 8243, "cdate": 1758075909605, "mdate": 1759897796637, "content": {"title": "Bridging Modalities for Forgery Detection via Learnable Representations with Query-Guided Contrastive Learning", "abstract": "Image manipulation localization (IML) aims to identify tampered regions in edited images, which may range from object-level composites to subtle traces. Recent studies have began to explore the integration of multi-source cues, such as RGB, high frequency and noises, in pursuit of more precise localization. Despite this progress, the potential of cross-modal interactions and hierarchical perceptions deserves deeper investigation and exploitation. \nInspired by how humans detect forgeries through dynamic zooming to capture holistic-local and semantic-detail cues, we propose BriQ (Bridge-Modality Query), a query-based framework that learns forged-aware representations to perceive multi-scale information. Meanwhile, we incorporate a structured attention to effectively model cross-modal interactions. \nTo further enhance discriminative capability, we introduce query-to-regions contrastive learning (Q2R), which encourages representations to capture the essential contrast between tampered and authentic regions and aggregate forgery-related features, thereby significantly improving IML task performance. \nExtensive experiments conducted on multiple benchmark datasets validate BriQ's state-of-the-art effectiveness and robustness, while comprehensive ablation studies confirm the contributions of each component.", "tldr": "", "keywords": ["image manipulation localization", "forged representation", "bidirectional cross-attention", "contrastive learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddc04de74c7095b0ebb7c7f46c30606e9fac9b5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a structured framework for image manipulation localization (IML) using learnable forged representations extracted from multi-scale, multi-modal feature maps. The approach achieves strong results on challenging benchmarks and offers new insights into the structure of manipulated regions. Future work will extend the framework to diffusion-generated images and integrate large language models for natural language explanations and improved interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-organized and easy to follow\n\nThe experiments are promising and comprehensive"}, "weaknesses": {"value": "1, my primary concern is the technical novelty of this paper. The multi-level, feature pyramid, and coarse-to-fine modeling are all well-known for vision problems. Frequency information is also widely studied in many works like [A,B].  Incorporating the learnable vectors to aid the association modeling are also explored in prompt learning like coop  [C] and VLMs like Q-Former in Blip [D]. For Q2R objective, the label constrution is similar to the patch manipulation modeling in ASAP [E]. Given the above, what's the new technical contributions of this paper?\n\n[A] Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-modal Manipulation, IJCV 2025\n[B] Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Domain Learning, AAAI 2024\n[C]  Learning to Prompt for Vision-Language Models, IJCV 2022.\n[D] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models, ICML2023\n[E] ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding, CVPR 2025.\n\n\n2, How the multi-scale feature achieve the \"dynamic zooming\" human-like ability?\n\n3,  In query-to-region CL, why is the related Q2R better than R2R? I'm also confused by the claim \"Q2R utilizes intermediate queries as proxies.This approach maintains effective region discrimination while simplifying the learning process\"  Q2R introduces extra paramters, why this simplifies the learning process?"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tmgKzo03TI", "forum": "EeFr1rupcx", "replyto": "EeFr1rupcx", "signatures": ["ICLR.cc/2026/Conference/Submission8243/Reviewer_fZUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8243/Reviewer_fZUs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971324081, "cdate": 1760971324081, "tmdate": 1762920188791, "mdate": 1762920188791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BriQ, a query-based framework for Image Manipulation Localization (IML) that integrates multi-modal cues (RGB for semantic content and high-frequency/noise for subtle traces) using learnable forged-aware representations. Inspired by human perceptual processes, BriQ employs hierarchical bidirectional attention for cross-modal interactions and introduces Query-to-Regions (Q2R) contrastive learning to enhance discrimination between tampered and authentic regions, even in homogeneous forgeries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Adapts query-based Transformer architectures (inspired by DETR and BLIP2) to IML, introducing novel learnable forged-aware queries for hierarchical multi-scale feature aggregation and explicit cross-modal interactions, addressing gaps in prior works like MGQFormer that neglect inter-modal dependencies.\nQuality: Provides rigorous experimental validation on standard benchmarks (e.g., CASIAv1, Coverage, NIST16, Columbia), showing average improvements of +6.53% in F1 and +4.71% in Permute-F1 over the second-best method (Mesorch); includes detailed ablations on hierarchical strategy, attention mechanisms, contrastive designs, and query quantity, plus robustness tests under perturbations like Gaussian Noise and JPEG Compression.\nClarity: Well-structured with informative sections, figures (e.g., t-SNE visualizations of feature distributions, qualitative mask comparisons), and a comprehensive related work survey contextualizing the approach within CNN, Transformer, and hybrid IML methods.\nSignificance: Advances forgery detection for real-world applications in journalism and justice by improving localization of subtle manipulations; the Q2R contrastive objective offers a promising shift from region-to-region contrasts, potentially better handling copy-move forgeries and boundary ambiguities compared to methods like SAFIRE or MMRL-Net."}, "weaknesses": {"value": "Insufficient Justification for Q2R as a \"Relaxed\" Version: The paper claims that Q2R is a \"relaxed version\" of R2R and cites a cosine metric inequality. This theoretical argument appears brief and somewhat vague. While the empirical results are undeniable, the intuitive explanation could be clearer. A more likely scenario is that the queries are learning to become category-specific prototypes, contrasting features with these stable prototypes is a simpler and more direct learning task than contrasting noisy patch features with each other. A deeper intuitive explanation would improve the paper.\nLack of Analysis on Query Specialization: The ablation study on the number of queries finds that 16 is optimal. This is a good ablation, but it misses the opportunity for deeper analysis. What do these 16 queries learn? Are they specialized for different types of tampering (e.g., query 1 for splicing, query 2 for copy-move) or different visual patterns? Qualitative analysis of query activation maps for different tampering types would make the claim of \"forgery-aware representations\" more concrete and insightful.\nGeneralization Limits: Evaluations focus on traditional datasets; no tests on emerging generative forgeries (e.g., from diffusion models) such as the GLIDE test set for this diffusion model's forgery dataset, though mentioned as future work—adding such experiments could better demonstrate robustness to modern threats.\nInsufficient Baseline Comparisons: Recent accepted expert models for tampering localization are not included in the comparisons, such as the SparseViT expert model from the 2025 AAAI paper."}, "questions": {"value": "On the justification for Q2R as a \"relaxed\" version of R2R: Could the authors provide a deeper intuitive explanation beyond the brief cosine inequality? For instance, elaborate on how queries act as category-specific prototypes, making contrastive learning simpler and more stable compared to direct noisy patch contrasts in methods like SAFIRE or MMRL-Net?\nRegarding query specialization: Beyond the ablation showing 16 queries as optimal, what do these queries specifically learn? Are they specialized for different tampering types (e.g., splicing vs. copy-move) or visual patterns? Could qualitative analyses, such as query activation maps across various forgery examples, be added to make the \"forgery-aware representations\" claim more concrete?\nHow novel is the hierarchical bidirectional attention compared to borrowed elements from DETR or BLIP2? Is there a principled theoretical basis (e.g., information flow analysis) for its superiority over unidirectional or naive fusions, or was this design primarily empirical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDSUu8oLqx", "forum": "EeFr1rupcx", "replyto": "EeFr1rupcx", "signatures": ["ICLR.cc/2026/Conference/Submission8243/Reviewer_DKM8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8243/Reviewer_DKM8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476874511, "cdate": 1761476874511, "tmdate": 1762920188282, "mdate": 1762920188282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BriQ, a query-based framework for image manipulation localization (IML) that enhances forgery detection through structured cross-modal interactions and hierarchical feature modeling. The method addresses limitations in existing approaches by introducing learnable tampering-aware representations that integrate multi-scale features from RGB and high-frequency domains, guided by a bidirectional attention mechanism. A key innovation is the Query-to-Regions (Q2R) contrastive learning strategy, which explicitly models relationships between forged-aware queries and regional features to capture subtle tampering cues even in visually similar regions. The framework achieves state-of-the-art performance on benchmark datasets by combining hierarchical feature propagation with a novel contrastive objective that strengthens differentiation between authentic and manipulated content without relying on complex decoders. Extensive experiments validate its effectiveness in both accuracy and robustness, particularly for imperceptible forgeries."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths Assessment:\nOriginality:\nThe paper introduces a novel framework, BriQ, which creatively combines structured cross-modal interactions with hierarchical feature modeling to address limitations in existing image manipulation localization (IML) methods. A key innovation is the Query-to-Regions (Q2R) contrastive learning strategy, which explicitly models relationships between forged-aware queries and regional features to capture subtle tampering cues. This approach diverges from traditional methods by integrating multi-scale features from RGB and high-frequency domains via a bidirectional attention mechanism, enabling more robust detection of imperceptible forgeries. The hierarchical feature propagation and novel contrastive objective further distinguish the work, offering a fresh perspective on modeling tampering patterns without relying on complex decoders.\n\nQuality:\nThe method is rigorously validated on benchmark datasets, achieving state-of-the-art performance in both accuracy and robustness. The experimental design is comprehensive, with ablation studies dissecting the contributions of individual components (e.g., hierarchical feature propagation, Q2R contrastive learning). The results are compelling, particularly for detecting visually similar forgeries, and the technical details (e.g., implementation specifics, hyperparameters) are well-documented. The use of high-frequency domain features and structured attention mechanisms demonstrates a deep understanding of the problem, while the framework’s efficiency (e.g., avoiding complex decoders) suggests practical applicability.\n\nClarity:\nThe paper is exceptionally well-written, with a clear problem formulation and structured presentation of the methodology. The Q2R contrastive learning strategy and bidirectional attention mechanism are explained with intuitive diagrams and pseudocode, making the technical contributions accessible. The experiments are logically organized, with detailed comparisons to prior work and visualizations of detection results. The limitations are acknowledged (e.g., potential generalizability to unseen forgery types), and the language is precise, avoiding overly technical jargon that could obscure the ideas."}, "weaknesses": {"value": "1. Limited Novelty in Core Components: While the Query-to-Regions (Q2R) contrastive learning strategy is positioned as a novel contribution, the use of cross-modal attention mechanisms and hierarchical feature modeling has been extensively explored in prior work on multimodal representation learning. The paper does not sufficiently contextualize how its design differs from existing approaches in domains like visual-question answering or cross-modal retrieval, potentially undermining the claim of originality. For example, the bidirectional attention mechanism shares conceptual similarities with previous work, yet the analysis of these overlaps is omitted.\n\n2. Insufficient Evaluation on Real-World Forgery Types: The experiments focus on synthetic benchmarks, but real-world forgeries often involve complex manipulations (e.g., GAN-generated content, adversarial attacks) that differ significantly from controlled datasets. The paper does not report performance on such cases or provide ablation studies on the robustness to domain shifts (e.g., varying lighting, resolution). This limits the generalizability of the claims about \"imperceptible forgeries.\"\n\n3. Ambiguous Theoretical Justification for Contrastive Objective: The Q2R contrastive loss is introduced as a heuristic to strengthen tampering-aware representations, but the paper lacks a theoretical analysis of why this formulation is optimal for IML. For instance, there is no discussion on how the loss aligns with principles from information theory (e.g., mutual information maximization) or how it interacts with the hierarchical feature propagation. This makes it difficult to assess whether the improvement stems from the loss design itself or other factors (e.g., increased model capacity).\n\n4. Overemphasis on RGB and High-Frequency Features: The framework relies heavily on RGB and high-frequency domain features, but the paper does not explore alternative modalities (e.g., semantic segmentation maps, motion vectors for video) that could further enhance tampering detection. Additionally, the choice of high-frequency features as a standalone cue is not justified theoretically, leaving open the question of whether this design is a bottleneck for detecting subtle forgeries in textured regions.\n\n5. Inconsistent Benchmarking: While the method achieves SOTA on the primary datasets, the comparisons to prior work are based on reported results rather than direct implementation. For example, the paper does not re-evaluate baseline methods under identical training conditions, making it unclear whether the performance gains are due to the proposed framework or hyperparameter tuning."}, "questions": {"value": "1. Clarification of Novelty vs. Prior Work\nThe paper emphasizes the Query-to-Regions (Q2R) contrastive strategy as a novel contribution, but cross-modal attention mechanisms and hierarchical feature fusion are well-established in vision-language models (e.g., CLIP [Radford et al., 2021]) and object detection (e.g., Feature Pyramid Networks [Lin et al., 2017]). How does the proposed bidirectional attention mechanism differ fundamentally from these existing approaches? For instance, are there specific architectural or training modifications that address limitations in prior work?\n2. Generalization to Real-World Forgery Types\nThe experiments focus on synthetic benchmarks (e.g., Deepfakes). How would the method perform on real-world forgeries involving GAN-generated content or adversarial attacks? \n3. Theoretical Rationale for Q2R Contrastive Loss:\nThe Q2R loss is described as a heuristic for enhancing tampering-aware representations, but the paper lacks theoretical grounding. What principles (e.g., mutual information maximization, information bottleneck theory) guided its design? Is there a formal analysis of how this loss improves forgery detection compared to alternatives like triplet loss or contrastive learning with negative samples?\n4. Role of High-Frequency Features:\nThe framework relies heavily on RGB and high-frequency domain features. What is the theoretical justification for this choice? For example, are high-frequency features inherently more discriminative for subtle forgeries, or is this an empirical observation without deeper analysis?\n5. Ablation on Domain Shifts:\nThe experiments do not evaluate robustness to domain shifts (e.g., varying lighting, resolution). How does the method perform when trained on synthetic data but tested on real-world images with different distributions? Are there specific components (e.g., hierarchical feature propagation) that mitigate this issue?\n6. Comparison to Non-Contrastive Methods:\nThe paper focuses on contrastive learning, but non-contrastive approaches (e.g., self-supervised pretraining) have shown success in IML. Why was contrastive learning chosen over alternatives? Are there scenarios where non-contrastive methods might outperform BriQ?\n7. Interpretability of Tampering-Aware Representations:\nThe learnable tampering-aware representations are central to the framework. How interpretable are these features? For example, can they be visualized or mapped to specific forgery artifacts (e.g., seam lines, lighting inconsistencies)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "phgmPkmogB", "forum": "EeFr1rupcx", "replyto": "EeFr1rupcx", "signatures": ["ICLR.cc/2026/Conference/Submission8243/Reviewer_YidY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8243/Reviewer_YidY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625966000, "cdate": 1761625966000, "tmdate": 1762920187865, "mdate": 1762920187865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BriQ, a novel query-based framework for image manipulation localization (IML).\nIt introduces:\n1. Learnable tampering-aware queries that propagate across multi-scale RGB and high-frequency features through bidirectional cross-modal attention.\n2. A Query-to-Region (Q2R) contrastive loss, improving discriminative ability between tampered and authentic regions.\n3. A lightweight voting-based mask prediction instead of a heavy decoder.\nThe approach achieves state-of-the-art (SOTA) performance and robustness across multiple IML benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is written with clear motivation, identifying two important limitations in IML, insufficient cross-modal interaction and weak region-level discrimination in homogeneous manipulations.\n2. Methodologically, the paper contributes an elegant query propagation design with explicit gradient analysis and a lightweight, decoder-free prediction mechanism.\n3. Empirically, BriQ achieves strong and consistent improvements across multiple IML benchmarks and demonstrates enhanced robustness under noise and compression perturbations."}, "weaknesses": {"value": "While the paper is well motivated and structured, it lacks computational cost analysis and analysis of what the queries actually learn.\n\nAlso, most citations are scattered throughout the body text, which significantly disrupts the reading flow and harms overall readability. The authors are encouraged to rephrase sentences so that citations are integrated more naturally into the text, rather than inserted mid-sentence. Grouping related works or moving non-essential references to the end of paragraphs would further improve clarity and narrative coherence."}, "questions": {"value": "1. Lack of computational cost and efficiency analysis. A comparison to existing methods in terms of latency or GPU memory would make the contribution more concrete.\n\n2. While “query-guided” mechanisms suggest interpretability, there’s no quantitative or qualitative analysis of what the queries actually learn beyond t-SNE plots. Visualizations of attention maps or query-response localization could clarify interpretability claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gRNGFgjj5B", "forum": "EeFr1rupcx", "replyto": "EeFr1rupcx", "signatures": ["ICLR.cc/2026/Conference/Submission8243/Reviewer_dtW3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8243/Reviewer_dtW3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808749256, "cdate": 1761808749256, "tmdate": 1762920186656, "mdate": 1762920186656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}