{"id": "ePycZoAvYQ", "number": 15898, "cdate": 1758256782242, "mdate": 1759897274580, "content": {"title": "Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL", "abstract": "Offline goal-conditioned reinforcement learning remains challenging for long-horizon tasks. While hierarchical approaches attempt to address this by decomposing tasks into high-level subgoals, most existing methods rely on two-level architectures with separate networks, leading to fundamental limitations: they generate only a single intermediate subgoal, leading the low-level policy to act without awareness of the final goal when misled by erroneous subgoals, and prevent end-to-end optimization due to separate training objectives. We discover a novel solution to these challenges through chain-of-thought reasoning from large language models. Building on this insight, we introduce the Chain-of-Goals Hierarchical Policy (CoGHP), a new framework that reformulates hierarchical control as autoregressive sequence generation within a single unified architecture. CoGHP generates a sequence of latent subgoals and the primitive action in a single forward pass, where each subgoal acts as a \"reasoning token\" encoding intermediate decision-making. To implement this chain-of-thought approach in hierarchical RL, we pioneer the use of the MLP-Mixer architecture. This design enables efficient cross-token communication through simple feedforward operations and captures consistent structural relationships essential for hierarchical reasoning. Experimental results on challenging navigation and manipulation benchmarks demonstrate that CoGHP consistently outperforms strong baselines, demonstrating its effectiveness for long-horizon offline control tasks.", "tldr": "", "keywords": ["Offline Reinforcement Learning", "Hierarchical Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac7ba3908d937c347e1faaedb2262f919eff7040.pdf", "supplementary_material": "/attachment/6031caa15f257c4f098367418e28f88006d6c451.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Chain-of-Goals Hierarchical Policy (CoGHP), a framework for offline goal-conditioned RL. Instead of using separate high- and low-level networks, CoGHP generates multiple latent subgoals and the final action within a single forward pass. In particular, it adopts an MLP-Mixer backbone and demonstrates its advantage over transformer backbone. Experiments on navigation and manipulation benchmarks show that CoGHP consistently outperforms strong baselines like HIQL, especially on long-horizon tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Reformulating hierarchical RL as autoregressive sequence generation within a single unified network is a neat and interesting idea that simplifies architecture design and enables end-to-end optimization.\n2. The proposed CoGHP consistently outperforms strong baselines across both navigation and manipulation tasks. \n3. The finding that an MLP-Mixer backbone outperforms the commonly used Transformer architecture is interesting."}, "weaknesses": {"value": "1. The connection with chain-of-thought reasoning from LLMs is weak and a stretch. It is not convincing to consider the subgoal token as reasoning token. Framing offline RL as a sequence modeling problem has a well established literature. It is more appropriate to discuss the connection with this line of work such as DecisionTransformer and Trajectory Transformer.\n2. The authors argue that generating the subgoal sequence in a reverse order is better. However, there is no corresponding ablation study to support that claim. Also, ideally, the subgoal sequence should be optimized jointly. In this sense, alternative formulations such as bidirectional generation or iterative refinement (e.g., diffusion-based planning) could be more principled.\n3. The authors claim that the framework can handle other forms of subgoal representation. However, this generality seems non-trivial. Incorporating abstract or semantic subgoals would likely require additional modalities or dataset features. The authors should further elaborate on the additional modifications or assumptions needed to make such accommodations."}, "questions": {"value": "1. The empirical results clearly show that the MLP-Mixer backbone outperforms the Transformer variant. Could the authors further elaborate on why this is the case? Which aspects of the MLP-Mixer architecture contribute to this advantage? In particular, could the authors clarify what is meant by “stepwise procedural consistency\", and why is this property important for hierarchical RL in the proposed framework? \n2. Does the proposed framework reduce to HIQL when the planning horizon is one? An ablation on the number of predicted subgoals (planning horizon) would help clarify how much of the performance gain arises from multi-step subgoal prediction versus the unified architecture."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Xeqjoktew", "forum": "ePycZoAvYQ", "replyto": "ePycZoAvYQ", "signatures": ["ICLR.cc/2026/Conference/Submission15898/Reviewer_7rmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15898/Reviewer_7rmc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798210969, "cdate": 1761798210969, "tmdate": 1762926112773, "mdate": 1762926112773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Chain-of-Goals Hierarchical Policy (CoGHP), a method for offline goal-conditioned reinforcement learning that the authors mention draws inspiration from chain-of-thought reasoning in large language models to generate sequences of latent subgoals and actions using a unified MLP-Mixer architecture. The authors claim that the approach addresses limitations in traditional two-level hierarchical RL by enabling end-to-end training and multiple subgoal generation in reverse order (farthest to nearest). The main contributions include adapting MLP-Mixer for RL sequence modeling with a causal mixer, using advantage-weighted regression with a shared value function, and demonstrating performance gains on OGBench navigation (pointmaze, antmaze) and manipulation (cube, scene) tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "CoGHP demonstrates solid empirical performance on long-horizon tasks vs HIQL, highlighting benefits of multi-subgoal generation for navigation. The unified MLP-Mixer architecture enables end-to-end training, reducing the fragmentation in separate network methods like HIQL, and ablations confirm the causal mixer's role in complex settings."}, "weaknesses": {"value": "1. It seems like the chain-of-thought inspiration is superficial rather than intuitive or practical: latent subgoals are opaque embeddings, not explicit reasoning steps, making the LLM analogy more rhetorical than substantive. The LLM analogy is conceptually motivating but technically superficial. The real contributions are (a) unified autoregressive generation and (b) end-to-end training. Also, there is no ablation to test forward subgoal generation, leaving the reverse-order claim unverified. \n\n2. The baselines are limited to OGBench standards, which might question robustness to other domains. \n\n3. The limitations like distribution shift vulnerability are acknowledged but untested, with no OOD experiments despite offline RL's emphasis on generalization. \n\n4. Computational overhead (training/inference time) is unreported, hindering practical assessment against baselines."}, "questions": {"value": "1. Why generate subgoals from farthest to nearest? Can you provide an ablation comparing this to nearest-to-farthest ordering on antmaze-giant and cube-triple to justify the design.\n\n2. How does CoGHP handle OOD goals? Can you provide evaluations on success rates on goals outside the dataset distribution, as offline RL often faces unseen targets.\n\n3. Please provide comparisons of runtime and parameters to HIQL: Report training time, inference latency, and FLOPs for fair efficiency claims.​\n\n4. Does CoGHP generalize to diverse robotic manipulation tasks beyond the curated OGBench setup? Like MetaWorld Benchmark (Robotics)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Yh0kAtevt", "forum": "ePycZoAvYQ", "replyto": "ePycZoAvYQ", "signatures": ["ICLR.cc/2026/Conference/Submission15898/Reviewer_DvEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15898/Reviewer_DvEG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826123272, "cdate": 1761826123272, "tmdate": 1762926112316, "mdate": 1762926112316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Chain-of-Goals Hierarchical Policy, a unified policy that generates a short sequence of latent subgoals followed by the final action in a single forward pass. The key idea is to treat subgoals as reasoning tokens, produced autoregressively so that later predictions condition on earlier ones while remaining aware of the original goal. The architecture uses an MLP-Mixer backbone with a learnable causal token mixer, trained end to end with a shared goal-conditioned value function and advantage-weighted objectives. Experiments on OGBench navigation and manipulation tasks report sizable gains over strong offline goal-conditioned RL baselines, with ablations suggesting benefits from the Mixer backbone and the causal mixer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper targets a pain point in offline goal-conditioned RL for long horizons and argues for a cohesive alternative to two-level hierarchies. Formulating hierarchical control as autoregressive subgoal generation inside one network is conceptually neat and practically appealing, since it preserves access to the final goal and allows gradients to flow through all stages.\n\n* Empirically, the method shows strong results across diverse domains. Notably, it improves success on difficult giant mazes and complex manipulation sequences where multiple intermediate decisions matter. These gains are consistent with the claim that multi-step intermediate guidance helps long-horizon tasks.\n\n* The ablations are helpful. Replacing the Mixer with a Transformer hurts on the hardest tasks, and removing the causal mixer degrades performance further, which supports the specific architectural choices rather than attributing wins to generic capacity."}, "weaknesses": {"value": "* The novelty is partly architectural refactoring and framing. The chain-of-thought analogy is evocative, but the subgoals are supervised by fixed k-step future states from trajectories. This is closer to structured imitation with value-based weighting than to learned reasoning.\nThe methodology introduces potential train–test mismatch. Training uses teacher forcing with ground-truth subgoals, while inference relies on the model’s own subgoal predictions. The paper acknowledges teacher forcing but does not quantify error accumulation or compare against scheduled sampling or consistency regularizers designed for autoregressive rollouts.\n\n* Some comparisons and analyses feel underdeveloped. The claim that Transformers are less suitable is asserted and supported by a single ablation, but hyperparameter parity and capacity normalization are not deeply probed. It is also unclear how sensitive results are to the choice of k for subgoal extraction, to the advantage temperature, and to the exact weighting between subgoal and action losses. The experiments report strong headline numbers, yet each environment evaluates only five predefined state–goal pairs and success rates are averaged across eight seeds, which makes the statistical picture somewhat narrow. More per-task breakdowns or success-vs-horizon plots would improve confidence.\n\n* On the representation side, most subgoals are encoded future states. The paper briefly notes that other subgoal types would fit, but the only concrete visual evidence comes from decoding to coordinates for antmaze. Demonstrations of learned abstract subgoals or skills, or at least richer visualizations across tasks, would better support the generality claim."}, "questions": {"value": "* How robust is performance to the choice of H and the spacing k used to sample supervision subgoals from trajectories? A sensitivity sweep that varies H and k together would help establish whether improvements persist beyond the selected settings.\n\n* Can the authors quantify exposure bias from teacher forcing? For example, report success when the policy is rolled out with its own predicted subgoals but trained without teacher forcing, or include scheduled sampling. A plot of success vs rollout depth of predicted subgoals would clarify compounding error.\n\n* What is the effect of the causal mixer relative to simpler positional encodings or strictly triangular masking without learnable mixing? An ablation that replaces the causal mixer with fixed lower-triangular averaging could isolate the benefit of learnability.\n\n* How fair and capacity-matched are the Transformer baselines? Please provide layer counts, parameter totals, token dimensions, and training curves for Mixer vs Transformer across tasks to rule out under-tuning.\n   \n* Do results hold when evaluating on larger sets of randomly sampled state–goal pairs and under distribution shift in goals? Reporting confidence intervals across many goals, as well as success vs geodesic distance to goal, would strengthen the empirical case.\n   \n* Can the method operate with non-state subgoals, such as latent skills or semantic waypoints, and still retain advantages? A small-scale experiment with learned discrete subgoals would substantiate the generality claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BvTbqm2Z58", "forum": "ePycZoAvYQ", "replyto": "ePycZoAvYQ", "signatures": ["ICLR.cc/2026/Conference/Submission15898/Reviewer_chFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15898/Reviewer_chFA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917939639, "cdate": 1761917939639, "tmdate": 1762926111453, "mdate": 1762926111453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}