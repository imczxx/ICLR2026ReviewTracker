{"id": "323ximYcsk", "number": 190, "cdate": 1756730653140, "mdate": 1763631000651, "content": {"title": "DA$^2$: Depth Anything in Any Direction", "abstract": "Panorama has a full FoV (360$^\\circ\\times$180$^\\circ$), offering a more complete visual description than perspective images.\nThanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision.\nHowever, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization.\nFurthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (\\textit{e.g.}, cubemaps),\nwhich leads to suboptimal efficiency.\nTo address these challenges, we propose $\\textbf{DA}$$^{\\textbf{2}}$: $\\textbf{D}$epth $\\textbf{A}$nything in $\\textbf{A}$ny $\\textbf{D}$irection, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator.\nSpecifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create $\\sim$543K panoramic RGB-depth pairs, bringing the total to $\\sim$607K.\nTo further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance.\nA comprehensive benchmark on multiple datasets clearly demonstrates DA$^{2}$'s SoTA performance, with an average 38\\% improvement on AbsRel over the strongest zero-shot baseline.\nSurprisingly, DA$^{2}$ even outperforms prior in-domain methods, highlighting its superior zero-shot generalization.\nMoreover, as an end-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based approaches.\nBoth the code and the curated panoramic data will be released.", "tldr": "DA$^{2}$ is a zero-shot, end-to-end panoramic depth estimator trained on large-scale curated data with a sphere-aware ViT to reduce distortions. It runs efficiently and surpasses prior methods by a large margin both qualitatively and quantitatively.", "keywords": ["Panoramas", "Depth (Distance) Estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/246ce57c5bfb0348ea0c88fcb697247dbf2cc104.pdf", "supplementary_material": "/attachment/639f21f4c20795d444b73480576edb8bf37c9011.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DA$^2$, an end-to-end, panoramic depth estimation model that achieves strong zero-shot generalization.\n\nPrevious methods face challenges in zero-shot panoramic depth estimation because:\n1. [Data scarcity] Existing panoramic depth datasets are rare\n2. [Spherical distortions] Common equirectangular projection of panoramas clearly distorts near poles.\n\nCorrespondingly, the authors addess this problems with two main contributions:\n1. [Panoramic data curation engine] Considering the rich data in perspective depth estimation, this paper proposes a panoramic data curation engine. Firstly, the perspective RGB / depth images are transformed into spherical space via Perspective-to-Equirectangular (P2E) projection. Then, for the transformed RGB is panoramic out-painted to obtain the \"full\" panorama. This engine curates additional 540K data samples.\n2. [SphereViT] A ViT network that explicitly incorporates spherical coordinates into latent feature via spherical embeddings, designed to mitigates the effect of spherical distortions.\n\nTo validate DA$^2$, the authors conduct a comprehensive comparison, including both panoramic depth estimation methods and perspective methods. The results clearly show the SOTA performance of DA$^2$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reasonable analysis of existing methods, and straightforward solutions targeting the existing limitations. The existing issues (Data scarcity, Spherical distortions) is critical in panoramic depth estimation. And the solutions (Panoramic data curation engine, SphereViT) effectively address the limitations, making this paper valuable in panoramic depth estimation.\n\n2. In the experiments, the author built a comprehensive benchmark among various datasets, and compared DA$^2$ with both zero-shot / in-domain, panoramic / perspective approaches. The quantitative and qualitative results clearly show DA$^2$'s SOTA performance. The ablation studies clearly show the performance gained via scaling-up with perspective data, also the spherical embeddings in SphereViT.\n\n3. The authors claimed that the code and the curated data will be open-sourced. This will be a valuable contribution to the research community.\n\n4. The writing is clear and also the figures. The paper is easy to follow."}, "weaknesses": {"value": "1. Performance concerns on real-world panoramas: The reviewer noticed that the curated panoramic data are basically come from synthetic perspective data. The domain gap between synthetic data and real-world data may decrease the model's performance on real-world panoramas.\n\n2. While the data curation engine is effective, the depth label is largely missing, which is better obtained. The author should add more discussions on this matter.\n\n3. More qualitative results: There are only 2 cases in this paper's qualitative comparison.\n\n4. The reviewer also noticed left-right seams in the reconstructed 3D point clouds. The author should add more discussions about this problem, which can be critical in real-world applications.\n\n5. Minors:\n\n    (1). The arrow on the right figure of figure 2 is in opposite direction.\n\n    (2). The citation of MVS-Synth in line 371, use \\citep instead."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VOHJ7T8OG9", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Reviewer_RcRx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Reviewer_RcRx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630561276, "cdate": 1761630561276, "tmdate": 1762915467131, "mdate": 1762915467131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DA2, a method for zero-shot, monocular panoramic depth estimation. The work tackles two fundamental challenges in this domain: the scarcity of large-scale panoramic training data and the geometric distortions inherent in the equirectangular projection.\n\nTo address data scarcity, the paper proposes a panoramic data curation engine. This engine leverages abundant perspective RGB-D datasets by first projecting a perspective image into a partial panorama and then using a image generative model (i.e. FLUX-I2P) to out-paint the partial panorama into a full one. This process increased their training set size by nearly tenfold, from ~63K to ~607K pairs.\n\nTo handle spherical distortions, the paper proposes SphereViT, a ViT-based architecture. Instead of using standard 2D positional embeddings, SphereViT creates a fixed Spherical Embedding derived from the spherical coordinates (azimuth and polar angles) of each image patch. It then uses cross-attention, with the image features as queries and the spherical embedding as keys and values, to explicitly inject distortion-awareness into the feature representations.\n\nThrough extensive experiments on standard benchmarks (Stanford2D3D, Matterport3D, PanoSUNCG), DA2 is shown to achieve state-of-the-art performance, significantly outperforming prior zero-shot methods and many specialized in-domain models with high inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its two-fold innovation.\n\n1) The data curation engine creatively uses a generative model (i.e. FLUX-I2P) to synthesize panoramic training data from abundant perspective data, the authors have effectively broken through the data barrier in the academic community that restricts a open source panoramic depth estimation comparable to commercial models. The resulting \"scaling law\" (Fig. 2) is a powerful demonstration of the data-centric approach.\n\n2) The SphereViT architecture introduces a neat and elegant solution to the problem of spherical distortion. Its use of cross-attention with a fixed spherical embedding is a more principled and efficient approach than the fusion strategies common in prior work.\n\nThe resulting model, DA2, is not only accurate but also efficient. As a fully end-to-end method, it is significantly faster than fusion-based alternatives, making it practical for real-world applications, as mentioned in the appendix. The planned release of the code and the large-scale curated panoramic dataset is a big contribution in itself and will be a substantial asset to the research community.\n\nThe paper is easy to follow. The experimental validation is thorough, featuring a comprehensive benchmark against a large number of competing methods."}, "weaknesses": {"value": "The paper is novel and effective, the following points are intended as constructive suggestions for further improvement.\n\n1) About Out-painter\n\nThe quality of the curated data, and thus the performance of DA2, is intrinsically linked to the performance of the FLUX-I2P out-painter. Any biases or artifacts of the generative model could be implicitly learned by DA2. A brief discussion on the limitations of the out-painter, and how different out-painters affect depth estimation would add valuable context.\n\n2) About SphereViT\n\nThe cross-attention mechanism in SphereViT is a key architectural novelty. The ablation study shows that including the spherical embedding is crucial. However, it does not compare the proposed cross-attention design against other plausible alternatives for incorporating this embedding. For instance, the standard ViT approach (adding the embedding to image features before self-attention) or simply concatenating the spherical coordinates as extra channels would provide stronger evidence that the specific cross-attention is optimal.\n\n3) About Seam Artifact \n\nThe paper acknowledges visible seams at the panorama's left-right boundary as a limitation. Given that SphereViT is designed with explicit knowledge of spherical coordinates, it is somewhat surprising that this issue persists. A brief discussion on why the current architecture does not fully resolve this and potential solutions would strengthen the paper."}, "questions": {"value": "Thank you for this good work. Besides the points raised in the discussion of weakness, I have a few further questions to better understand the nuances of your contributions.\n\n1) Current image generative methods are capable of out-painting depth information. If the perspective depth maps are also out-painted, would this lead to performance improvements?\n\n2) As mentioned in the paper, the depth estimation model and data will be open-sourced. Will the FLUX-I2P data and model also be open-sourced? The paper does not provide sufficient details about the data used to train FLUX-I2P.\n\n3) Please discuss the relevance to the method proposed in \"S2Net: Accurate Panorama Depth Estimation on Spherical Surface\". If relevant, please cite this work.\n\n4) In Table 3, what would the results be if only Pano. out-painting is used without SphereViT and Normal loss?\n\n5) In Section A.1, the authors mention that only translation is needed to align the 3D points from panoramas taken at different locations. Please explain why rotation is not needed?\n\n6) Please proofread the manuscript again to identify and correct spelling errors—for example, \"date\" in Line 380."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tprrmi0MgY", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Reviewer_xeLR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Reviewer_xeLR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746840361, "cdate": 1761746840361, "tmdate": 1762915466922, "mdate": 1762915466922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to create a universal, end-to-end depth estimation model that generalizes across domains and any directions for panoramic application.\nMain contribution:\n1. provide a synthetic panoramic data engine/pipeline mainly powered by utilizing FLUX-I2P out painting. And evaluated interest scaling law on that curated data.\n2. Design a SphereViT which incorporates spherical embedding, can yield accurate geometrical estimation by distortion-aware image features.\n3. Evaluation on zero-shot/in-domain benchmark shows strong quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Good work which represents a step toward universal depth perception across all directions.\n\n2. It combines synthetic panoramic data and sphere-aware architecture could inspire like Geometry-consistent generative modeling.\n\n3. Solid results on comprehensive panoramic benchmarks show good zero-shot performance, outperform in-domain models proved good generalizability."}, "weaknesses": {"value": "1. The large-scale training set is created by FLUX I2P. This work tries to find a good way to utilize the strong FLUX base model capacity. However, the generation pipeline might have distribution gaps like lighting environment & geometry consistency may still have some gaps from real panoramic scenes.\n2. Efficiency: this paper’s E2E pipeline is faster than fusion based method but remains at same level with other E2E methods like UniK3D. Both are 0.3s. The E2E pipeline seems not novel.\n3. The data and ViT arch design are mainly focused on depth estimation quality improvement. Other methods like PanDa, DepthAnyCamera or UniK3D could also cover the tasks."}, "questions": {"value": "1. What if we collected more real panoramic data (not limited in some domains) and use them to evaluate the scaling law? It should also have improvement for depth estimation task.\n2. Can we also use video-generation model to generate some temporal panoramic data for temporal consistency? Single frame is easier to handle.\n3. For the ablation study, spherical embedding didn’t show too much improvement (less than 1 point), is there any other better design for the specific utilization of spherical information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LbecnggYhn", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Reviewer_c7zZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Reviewer_c7zZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902878656, "cdate": 1761902878656, "tmdate": 1762915466752, "mdate": 1762915466752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to build a strong zero-shot depth estimation model for panoramic images. The main contributions are:\n\n- A data curation pipeline that maps perspective images into spherical coordinates followed by image out-painting, substantially expanding the training data scale.\n- A customized transformer with angle-based positional embeddings and a cross-attention design.\n- A comprehensive evaluation of zero-shot and in-domain depth estimation across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies limitations in current panoramic depth estimation methods and significantly expands the data scale, which is crucial for the field’s progress.\n2. Benefiting from the enlarged dataset, the model outperforms existing methods on standard benchmarks.\n3. It provides a comprehensive benchmark, including perspective-based models, which clarifies the current landscape.\n4. The paper is clearly written and contains rich technical details."}, "weaknesses": {"value": "1. Limited ablations on the dataset curation process.\n   - Because ground-truth depth is only available in the central regions of the curated data (due to the limited FOV of perspective images; Fig. 3), it is unclear how missing annotations at the borders affect final performance.\n   - The model uses image out-painting to compensate for limited FOV, but the visual quality of the out-painted regions is not clearly evaluated.\n\n2. Limited novelty in some architectural components.  \n   The paper emphasizes angle-based positional embeddings (L269–299), cross-attention (L300–311), and a normal loss (Table 3, listed as a separate ablation item), but:\n   - Positional embeddings: Improvements appear modest. In Table 3, removing $E_{\\text{sphere}}$ yields only a small degradation (AbsRel 6.62 → 6.84) compared to gains from out-painting and the normal loss. It is also unclear whether the baseline here uses standard \\(uv\\) positional embeddings or no PE. The utility of the proposed PE seems smaller than claimed.\n   - Cross-attention: The idea is not new and is under-ablated. Prior work (HUSH [1]) uses cross-attention to attend image features and SH features. Here, the method uses \\(uv\\) PE instead of SH and fixes image+feature as queries with PE as keys/values. Ablations should compare:  \n     (a) Cross-attention vs. self-attention (with image+PE as inputs, matching L303), and  \n     (b) Swapping queries/keys (PE vs. image features), following HUSH.  \n   - Normal loss: It contributes notably (Table 3), but normal losses have been used previously (e.g., [1])."}, "questions": {"value": "1. Center vs. border performance: Can you provide ablations comparing performance in central regions versus border regions on panoramic images, to clarify benefits and limitations of the proposed dataset (related to Weakness 1-1)?\n2. Out-painting quality: Can you include visual quality comparisons (e.g., SSIM, LPIPS, PSNR) between the out-painted images and corresponding ground-truth images (Weakness 1-2)?\n3. Attention ablations: Please provide ablations for self-attention as well as query/key-value choices in cross-attention (Weakness 2).\n4. Ablation dataset: Which evaluation dataset is used for the ablation results? The reported scores do not seem to align with any dataset in Table 2.\n\n---\n\nReference  \n[1] HUSH: Holistic Panoramic 3D Scene Understanding using Spherical Harmonics, CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LPga340ChT", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Reviewer_GN9K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Reviewer_GN9K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987994818, "cdate": 1761987994818, "tmdate": 1762915466467, "mdate": 1762915466467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DA2 trains an end-to-end panoramic depth (scale-invariant distance) model by first,  curating a large dataset containing panoramas from perspective RGB-D via P2E projection plus generative panoramic out-painting (FLUX-I2P), and develop a sphereViT backbone that cross-attends from image features to a fixed spherical angle embedding. On benchmarks, DA2 reports the best zero-shot results, beating strong zero-shot baselines like UniK3D.  Authors highlight better or comparable speed to end-to-end competitors while much faster than fusion pipelines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong empirical gains. The proposed method demonstrates strong performance compared to both zero-shot fusion, zero-shot end-to-end and in-domain methods.\n2. The proposed method scales well. The paper shows clear scaling-law-like improvements as more perspective data are \"panoramaized\"\n3.  The proposed architecture is effective. The SphereVIT's fixed spherical embedding plus cross attention is a simple but effective way to inject spherical awareness, which shows strong results."}, "weaknesses": {"value": "1. It seems there is a supervision mismatch introduced by the generative out-painting. The curation engine out paints RGB to a full panorama but supervised only the P2E covered part of depth (no out-painted depth due to acc concern). Hence, many pixels in the training panoramas lack ground-truth depth, yet their RGB content is model-generated. This opens risks of learned correlation with out-painter priors and biased geometry in unsupervised regions (especailly near poles and seams).  The paper shows gains with out-painting but it doesn't quantify how much of the test-time improvement stems from distribution mathcin got FLUX-I2P artifacts compared with real geometric learning. \n\n2. Injecting coordinates/angles and using attention to a fixed positional bank is not conceptually new. [1] mitigates ERP distortion via spherical tangent tokens and transformer design. [2] operates directly on spherical meshes, and several recent spherical ViTs or positional-encoding approaches aim at similar goals. \n\n3. Metric depth is not addressed when comparing to Unik3D. DA2 predicts scale-invariant distance, not metric depth. With Unik3D showing metric 3d across cameras (including panoramic), it is important to discuss scale recovery or to provide an optional scale head, since it is important for downstream applications like AR or robotics.\n\n\n[1] Shen, Zhijie, et al. \"PanoFormer: panorama transformer for indoor 360∘ depth estimation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[2] Yan, Qingsong, et al. \"Spheredepth: Panorama depth estimation from spherical domain.\" 2022 International Conference on 3D Vision (3DV). IEEE, 2022."}, "questions": {"value": "1. What fraction of pixels per training panorama actually have GT depth supervision, on average and by latitude?\n\n2. Can we consider add a simple scale-head (or post-hoc scale regressor) and evaluating metric errors where ground truth exists. Compare to UniK3D on the same splits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Du7tY5zbOs", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Reviewer_t7tq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Reviewer_t7tq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000715441, "cdate": 1762000715441, "tmdate": 1762915466281, "mdate": 1762915466281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Author's general response to all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their time and insightful, professional comments on our submission. We also sincerely thank our AC and SAC for their time in coordinating the discussion process. Thank you reviewers, AC, and SAC for your effort in pushing forward ICLR’26!\n\nFirstly, we are highly encouraged and grateful to see the strengths of our submission mentioned by the reviewers, such as:\n1. The paper “clearly identifies current limitations” (GN9K), targets on “critical issues” (RcRx), and “represents a step toward universal depth perception across all directions” (c7zZ).\n2. The data curation engine “significantly expands the data scale, which is crucial for the field’s progress” (GN9K, xeLR, RcRx), and “the model scales well” with “clear scaling-laws” (t7tq, xeLR) \n3. The SphereViT “introduces a neat and elegant solution to the problem of spherical distortion” (xeLR), is “sphere-aware” (c7zZ), and “effectively injects spherical awareness” (t7tq, RcRx).\n4. The paper “provides a comprehensive benchmark” (GN9K, c7zZ, xeLR, RcRx), and shows “strong empirical gains” (t7tq, RcRx).\n5. “The paper is clearly written” (GN9K, RcRx), “easy to follow” (xeLR), and “contains rich technical details” (GN9K).\n6. “The release of the code and the large-scale curated panoramic dataset is a big contribution” (xeLR, RcRx), and “will be a substantial asset to the research community” (xeLR).\nAnd the good (t7tq, c7zZ, xeLR, RcRx) soundness,  good (t7tq, xeLR, RcRx) and excellent (GN9K, c7zZ) presentation, good (t7tq, GN9K, xeLR, RcRx) contribution.\n\nWe also sincerely appreciate the weaknesses pointed out and the questions raised by the reviewers. These are valuable suggestions from different aspects to improve the overall quality of this paper. We noticed that the main concerns raised from the reviewers are about:\n1. The missing depth label in our data curation engine.\n2. The architectural design of SphereViT.\nAs each reviewer has his/her specific perspectives, we separately replied to each reviewer in a strictly point-by-point manner, aiming to settle each concern from each reviewer by providing additional discussions, doing and reporting new experiments, etc. We also updated the PDF submission correspondingly, according to reviewers’ comments and our responses.\n\nThanks and best regards,\n\nAuthors of submission #190"}}, "id": "r6hE0nC6hf", "forum": "323ximYcsk", "replyto": "323ximYcsk", "signatures": ["ICLR.cc/2026/Conference/Submission190/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission190/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission190/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763632368124, "cdate": 1763632368124, "tmdate": 1763647528204, "mdate": 1763647528204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}