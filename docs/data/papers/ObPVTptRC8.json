{"id": "ObPVTptRC8", "number": 4720, "cdate": 1757753056680, "mdate": 1759898017889, "content": {"title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.", "tldr": "", "keywords": ["Vision-and-Language Navigation", "MLLMs Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a137e251341d519a13e0a650161efe0332f4479.pdf", "supplementary_material": "/attachment/23bbafee7469c8d8dbd50e2c67d33697550c196a.pdf"}, "replies": [{"content": {"summary": {"value": "The authors present VLN-MME, a benchmark framework for multimodal-LLMs on vision and language navigation (VLN) tasks. The framework provides an interface for tasks, agents and models, such that a new methods can be evaluated in a standardized setting. The tasks include REVERIE, ObjectNav and R2R, three commonly used VLN tasks. A welcomed addition are the prerendered images for each viewpoint in the environments, such that costly and task-specific environment rendering is abstracted away. The framework also provides a fixed subset of representative routes for each task such that the full suite can be run in a reasonable amount of time. The provides baseline agents and granular evaluation metrics will make it straight forward to benchmark new MLLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- simulation/rendering free evaluation\n- baseline agents\n- fine-grained metrics"}, "weaknesses": {"value": "- limited set of tasks. Could include outdoor VLN [1,2] for an additional distinct setting. \n\n\n[1] Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments, Chen et al., 2018\n[2] VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View, Schumann et al., 2024"}, "questions": {"value": "- Why GPT-4o is used to pre-generate textual descriptions of the scene instead of making this part of the evaluation/benchmark?\n- Does prerendering limit this benchmark to discrete navigation instead of taking continuous actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iIOt36VJYE", "forum": "ObPVTptRC8", "replyto": "ObPVTptRC8", "signatures": ["ICLR.cc/2026/Conference/Submission4720/Reviewer_MRS8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4720/Reviewer_MRS8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565299531, "cdate": 1761565299531, "tmdate": 1762917533646, "mdate": 1762917533646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a modular, simulator-free framework for evaluating MLLMs as zero-shot agents in vision-and-language navigation. It standardizes agents, models, and tasks; supports representative R2R/REVERIE/ObjectNav datasets; and leverages pre-rendered, annotated panoramas for fine-grained diagnosis. Experiments show that while zero-shot MLLMs set useful baselines, they trail finetuned specialists, and Chain-of-Thought often fails to improve performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Studying MLLMs as embodied agents for language-guided visual navigation is an important research direction.\n\n* The proposed framework provides a practical and effective way to evaluate MLLM-based agents.\n\n* Its simulator-free design significantly reduces the computational cost of simulation."}, "weaknesses": {"value": "* Overall, the paper’s contribution is quite limited. Evaluation of MLLM-based navigation agents already appears in EmbodiedBench [1] and EmbodiedEval [2], which cover diverse scenes, instructions, and difficulty levels, and already support modular MLLM evaluation in EmbodiedBench. The paper does not clearly differentiate itself from these closely related efforts, so the true contribution is unclear and quite limited to my understanding.\n\n* The paper uses three datasets, R2R, REVERIE, and ObjectNav, but does not explain their differences or the rationale for selecting them. A brief, focused introduction and justification would help. \n\n* While the simulator-free setup reduces cost, it constrains agents to predefined actions that may not reflect real-world behavior. This raises concerns about realism and the size of the sim-to-real evaluation gap.\n\n* In addition, I think this paper does not provide enough insight for agent design as claimed. Beyond noting that CoT/Reflection often underperform, the paper does not analyze why or propose concrete ways to improve them. More diagnostic evidence and actionable guidance are needed from the large table.\n\n[1] EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, 2025.\n\n[2] EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents, 2025."}, "questions": {"value": "Please refer to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "luER4NEdK1", "forum": "ObPVTptRC8", "replyto": "ObPVTptRC8", "signatures": ["ICLR.cc/2026/Conference/Submission4720/Reviewer_MoHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4720/Reviewer_MoHw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726321004, "cdate": 1761726321004, "tmdate": 1762917533297, "mdate": 1762917533297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VLN-MME, a modular and simulator-free benchmark and evaluation framework to assess MLLM as zero-shot navigational agents in VLN tasks. The framework emphasizes diagnostic analysis beyond aggregate metrics, supports diverse agent/model architectures, and provides a lightweight evaluation through pre-rendered panoramic data. Experimental analysis compares several open-source MLLMs and agent prompting strategies, revealing unexpected performance degradations when incorporating COT reasoning and self-reflection. Detailed error analyses highlight fundamental gaps in spatial reasoning and perception-action grounding for these models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a well-conceived and extensible evaluation suite for VLN tasks, which is a practical tool for the community. The modularity is clearly described, and the simulator-free design reduces computational burden, lowering the barrier to entry for benchmarking and reproducibility.\n2. The paper is well structured and easy to follow. The benchmark design, experiments, and analysis are presented in a clear and logical way."}, "weaknesses": {"value": "1. The method section focuses on modular and simulator-free design. While I acknowledge the design efforts, I feel they are more like engineering work rather than scientifically driven research. VLN tasks have been deeply studied. I appreciate that the authors probably wrapped the evaluations into easy-to-use APIs, but I feel the this work is not substantially different in identifying the key capabilities of VLMs compared to other existing VLN benchmarks.\n\n2. The authors use four pre-rendered, non-overlapping perspective images at each location to achieve a simulator-free setup. However, this design essentially functions as a simulator or a “space-for-time” trick. Moreover, it likely results in a fixed step length for the agent, making the setting less aligned with real-world conditions.\n\n3. There is a noticeable absence of rigorous algorithmic or theoretical detail. Critical aspects such as prompt composition (order, length limits, input formatting), failure-handling policy specifications, and sampling protocols for negative/ambiguous visual cues in navigation are not formalized mathematically. \n\n4. The paper does not evaluate proprietary models such as GPT-5, and most of the open-source models tested are under 10B parameters. I wonder whether simply scaling up the model size or using more powerful models could solve this benchmark."}, "questions": {"value": "Please see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mEpLe7mfMb", "forum": "ObPVTptRC8", "replyto": "ObPVTptRC8", "signatures": ["ICLR.cc/2026/Conference/Submission4720/Reviewer_HHTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4720/Reviewer_HHTS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970934985, "cdate": 1761970934985, "tmdate": 1762917532883, "mdate": 1762917532883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the capabilities of Multimodal Large Language Models (MLLMs) as zero-shot agents in Vision-and-Language Navigation (VLN). The authors introduce VLN-MME, an evaluation framework designed to be modular, extensible, and simulator-free to enable fast and accessible benchmarking. This framework is used to systematically evaluate various open-source MLLMs and agent architectures. The most significant and counter-intuitive finding is that applying advanced prompting strategies like Chain-of-Thought (CoT) and self-reflection consistently degrades performance. The authors attribute this to the models' poor context awareness and inability to ground reasoning in the sequential, embodied nature of the task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **The paper is well-written.** The proposed method is well-illustrated and easy to follow.\n\n2. **The simulator-free approach, which pre-renders observations and metadata, is evaluation-friendly.** It lowers the computational barrier to entry for VLN research.\n\n3. **Insightful findings:** The central finding—that CoT and reflection-based reasoning harm performance—is counter-intuitive and impactful. It challenges the prevailing assumption that such techniques are universally beneficial and forces a deeper consideration of how MLLMs reason in embodied contexts. \n\n4. **Thorough diagnostic analysis:** The paper focus on *why* models fail, rather than just reporting leaderboard metrics. The breakdown of errors into looping, poor region recognition, and the \"perception-action gap\" (Section 4.3) provides concrete, actionable insights into the core weaknesses of current MLLMs."}, "weaknesses": {"value": "1. **Limited scale of models tested:** The experiments are conducted on MLLMs in the 7B-8B parameter range. While this is representative of current open-source models, the conclusions about the failure of CoT and fundamental reasoning limitations might not generalize to significantly larger, more powerful models (e.g., GPT-4o, Gemini 2.5 Pro). \n\n2. **Simulator-free design is unfavorable for video-based models:** The simulator-free design, while efficient, inherently limits the scope of models that can be evaluated. By pre-rendering static images, it removes the temporal continuity present in a real environment or simulator. This makes the benchmark unsuitable for video-based multimodal models (e.g., Qwen 2.5 VL w/ video inputs), which rely on motion and temporal cues between frames to understand the environment and agent dynamics.\n\n3. **Unnatural panoramic representation:** The panoramic view is constructed by stitching four 90° images. This representation is not a continuous 360° view and contains artificial seams. It is likely that the MLLMs tested have not been trained on such artificial panoramic compositions, which are different from the natural images they are typically exposed to. This domain gap could impair their ability to understand spatial relationships across the image boundaries, potentially contributing to the observed navigational failures.\n\n4. **Lack of evaluation on thinking models:** The models evaluated (e.g., Qwen2.5-VL, InternVL3) may lack the strong endogenous thinking abilities required to effectively leverage CoT. The analysis would be more conclusive if it included thinking VL models, such as MiMo-VL-7B or newer models in the Qwen family (e.g., Qwen3-VL), to see if they can better execute the reasoning strategy."}, "questions": {"value": "1. **Quantifying efficiency gains:** The paper highlights the efficiency of the simulator-free method. Could the authors provide specific metrics on these gains? For instance, what is the reduction in VRAM consumption per agent, and what is the speed-up in evaluation time compared to running the same agent in a traditional simulator like Habitat?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QUHoriZd4S", "forum": "ObPVTptRC8", "replyto": "ObPVTptRC8", "signatures": ["ICLR.cc/2026/Conference/Submission4720/Reviewer_B8ug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4720/Reviewer_B8ug"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989546019, "cdate": 1761989546019, "tmdate": 1762917532524, "mdate": 1762917532524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}