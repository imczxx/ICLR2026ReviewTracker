{"id": "NrSx6J6IZk", "number": 9799, "cdate": 1758140976447, "mdate": 1759897694979, "content": {"title": "Benchmark of Benchmarks: Unpacking Influence and Code Repository Quality in LLM Safety Benchmarks", "abstract": "The rapid growth of research in LLM safety makes it hard to track all advances. \nBenchmarks are therefore crucial for capturing key trends and enabling systematic comparisons. \nYet, it remains unclear why certain benchmarks gain prominence, and no systematic assessment has been conducted on their academic influence or code quality.\nThis paper fills this gap by presenting the first multi-dimensional evaluation of the influence (based on five metrics) and code quality (based on both automated and human assessment) on LLM safety benchmarks, analyzing 31 benchmarks and 382 non-benchmarks across prompt injection, jailbreak, and hallucination.\nOur findings challenge common assumptions: counterintuitively, we find that benchmark papers show no significant advantage in academic influence (e.g., citation count and density) over non-benchmark papers. \nWe uncover a key misalignment: while author prominence correlates with paper influence, neither author prominence nor paper influence shows a significant correlation with code quality.\nOur results also indicate substantial room for improvement in code and supplementary materials: only 39\\% of repositories are ready‑to‑use, 16\\% include flawless installation guides, and a mere 6\\% address ethical considerations.\nGiven that the work of prominent researchers tends to attract greater attention, they need to lead the effort in setting higher standards.", "tldr": "", "keywords": ["Meta-Study", "Benchmark", "Large Language Model", "Safety"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d84e395871f34a53fc32a26fd882ba383cda351.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper „Benchmark of Benchmarks: Unpacking Influence and Code Repository Quality in LLM Safety Benchmarks” presents the results of an analysis of 31 benchmarks on safety issues (jailbreaks, hallucinations, prompt injection). The analysis is uses 382 papers on safety issues that are not benchmarks as control group to measure difference between benchmark papers and other papers. Moreover, the authors study how several factors influence the success of benchmark papers measured in citations. The study finds that author prominence is related to success of the benchmarks, but that code quality is not. Moreover, having executable code is important, while having perfect documentation has no strong influence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper „Benchmark of Benchmarks: Unpacking Influence and Code Repository Quality in LLM Safety Benchmarks” presents the results of an analysis of 31 benchmarks on safety issues (jailbreaks, hallucinations, prompt injection). The analysis is uses 382 papers on safety issues that are not benchmarks as control group to measure difference between benchmark papers and other papers. Moreover, the authors study how several factors influence the success of benchmark papers measured in citations. The study finds that author prominence is related to success of the benchmarks, but that code quality is not. Moreover, having executable code is important, while having perfect documentation has no strong influence."}, "weaknesses": {"value": "While most of the paper seems sound, there is a general weakness in the setup of the statistics (comment 1) and, crucially, an important confounding factor that is not considered at all that could also explain one core conclusion, i.e., the relationship between benchmark success and author prominence (comment 1). For me, this second point is the most crucial aspect and the main reason for my judgment. The comments from 3 onwards are relatively minor. \n1) I wonder why the analysis is not based on a linear model. The coefficients of a linear model effectively measure linear correlations, same as Pearson’s correlation coefficient. However, a linear model has the additional advantage that correlation between factors can be taken into account, typically leading to better results when multiple factors are analyzed in parallel. \n2) While I intuitively also believe that author prominence is related to success, I do not believe that the study design is sufficient to conclude this. Notably, nothing in the study design controls *benchmark quality* (e.g., size of benchmark, novelty of benchmark, quality of benchmark data, or similar). Since this is not controlled for, there is a simple, possible alternative explanation for the observed effect that is not ruled out: prominent authors create author higher-quality benchmarks. The lack of control for this confounder is the key issue I have with this study that greatly limits the value of this conclusion. \n3) The execution time until the example scripts were run is a strange and biased metric. If a benchmark is more comprehensive, this might have longer examples, which would not be a bad thing. Still, this would show up as higher execution time. A cleaner measurement would be to measure the time between cloning the repository and starting the successful run, since this would only quantify the researcher effort to get to this point, excluding the confounding effect of example runtime. \n4) The ethics section is not an ethics section at all. Instead, the section reports limitations of the study. This needs to be changed and format requirements must be adhered to. \n5) Figure 2 is very hard to read and I strong suggest to avoid this strange pie-bar like visualization method. A grouped (stacked?) bar chart would likely be a lot easier to read. This is actually what is used in Figure 3, which is easier to read. However, Figure 3 suffers from bad legend placement, which partially hides the information. \n6) There seems to be a mismatch between table 2 and the textual description of the contents (Page 4, L181). The text mentions the geolocation, which I cannot find in the table. The table instead mentions area, which I assume is the research area – though I may misinterpret this. Anyways, this should be harmonized. \n7) All references are broken (missing brackets), likely because the latex stile was changed without ever checking this."}, "questions": {"value": "1) What is the impact of not using a linear model for the analysis, especially given that there seem to be strong correlations between the independent variables?\n2) Why are the results valid, even though there is no control for actual benchmark quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iQ40zKvjqf", "forum": "NrSx6J6IZk", "replyto": "NrSx6J6IZk", "signatures": ["ICLR.cc/2026/Conference/Submission9799/Reviewer_6K5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9799/Reviewer_6K5m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553976436, "cdate": 1761553976436, "tmdate": 1762921284568, "mdate": 1762921284568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the academic influence and code quality of 31 LLM safety benchmarks compared to 382 non-benchmark papers. The authors claim that benchmark papers show no clear advantage in citations, and neither author prominence nor paper influence correlates with code quality. Many repositories have usability and ethical shortcomings, with only 39% of repositories ready-to-use and 6% addressing ethical considerations. The authors suggest that prominent researchers should take the lead in improving standards."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents several interesting and valuable findings. \n\nNotably, some of the ethical and reproducibility-related metrics—such as only 39% of repositories being ready-to-use, 16% including flawless installation guides, and a mere 6% addressing ethical considerations—highlight the need for researchers to pay more attention to open-sourcing and maintaining their code alongside their research contributions. \n\nAdditionally, the observation that author's h-index does not show a strong correlation with code quality is intriguing and provides an important perspective on the relationship between academic influence and research artifacts."}, "weaknesses": {"value": "Much of the experimental design seems to rely on somewhat imprecise metrics and a fair amount of manual inspection, which makes the paper’s motivation a bit hard to follow.\n\nAdditionally, the conclusions, experimental design, and motivation appear somewhat subjective, reflecting the authors’ own perspective rather than broader community evidence. It might be more appropriate to frame this part as an initial motivation supported by larger-scale community surveys rather than just the authors’ judgment. For example, the statement in the introduction, “counterintuitively, we find that benchmark papers show no significant advantage in academic influence over non-benchmark papers”, carries some subjective interpretation; collecting feedback from a larger set of participants could make this claim more robust and less reliant on individual judgment."}, "questions": {"value": "1.\tRelevance of conclusions in RQ1: The authors conclude that benchmark papers do not show a statistically significant difference in citation metrics compared to non-benchmark papers, based on GitHub Citation Count and Citation Density. However, benchmark and non-benchmark papers inherently serve different roles in research, so it is not clear that a “higher or lower” comparison is meaningful. Additionally, measuring influence through GitHub stars may be misleading: benchmarks often serve as tools that are widely used but not necessarily “starred,” whereas other papers may receive stars more readily. Therefore, using GitHub data alone to assess influence may introduce bias.\n\n2.\tExperimental design and evaluation metrics: Although the authors acknowledge limitations in the “Imperfect Metrics” section of the ethics statement, the choice of metrics seems questionable. For instance, in RQ2, the primary criterion for evaluating benchmark quality is code quality and reproducibility. This seems unusual, because benchmarks are meant to provide measurement standards in a field, and the underlying evaluation ideas may be more important than the engineering quality of the code. Many researchers are not professional software engineers, and their code is often written for research purposes rather than for production-grade usage. While such criteria may make sense for toolboxes, it is unclear why code quality should be the main standard for benchmarks. This seems closer to evaluating whether research code undergoes proper code review rather than assessing the scientific quality of the benchmark itself.\n\n3.\tSubjectivity of conclusions: While the paper’s conclusions may have value, the path to reaching them appears questionable. Determining how to evaluate research work, what metrics best reflect quality, and the relative importance of code within a research contribution are highly subjective issues. More justification or broader evidence may be needed to support the chosen evaluation design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bAlBXEVB0j", "forum": "NrSx6J6IZk", "replyto": "NrSx6J6IZk", "signatures": ["ICLR.cc/2026/Conference/Submission9799/Reviewer_g9kb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9799/Reviewer_g9kb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811356630, "cdate": 1761811356630, "tmdate": 1762921284101, "mdate": 1762921284101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an study to evaluate the quality of benchmarks of large language models (LLMs) and safety.  The authors investigated three research questions (RQs): RQ1:the influence of current benchmark RQ2: the quality associated code repositories of the benchmark and factors to assess the quality; and RQ3: the relationship between influence of benchmark papers and the code quality."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Strengths\n- Timely topic focusing on the safety of LLM benchmark\n- Broader impact towards the research community and well as industry practioners who deploy or develop LLM"}, "weaknesses": {"value": "Weakness.\n- Any insights of security researchers would be insightful"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bX6K1kMDRq", "forum": "NrSx6J6IZk", "replyto": "NrSx6J6IZk", "signatures": ["ICLR.cc/2026/Conference/Submission9799/Reviewer_jdUQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9799/Reviewer_jdUQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929416156, "cdate": 1761929416156, "tmdate": 1762921283531, "mdate": 1762921283531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an analysis of LLM safety benchmarks, analyzing 31 benchmarks and 382 non-benchmarks. The analyses what causes benchmark papers to get cited. Is it code quality or author prominence? It also analyses wheter the benchmark papers are cited more than non-benchmark papers. \n\nData is colleced in a structured and transparent way.  The analysis is well-motivated and rigorous relying on statistical analyses.\n\nConclusions are prominent resarchers are cited more, code quality is not important, benchmark papers are not more cited than non-benchmark papers (at least in this subfield), and becnhmark with functional code that can be used without modification are more cited than those offering code that requires modifications"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and structured well. \n* The methodology is rigorous, and I trust the conclusions.  \n* The conclusions are interesting and, I would guess, probably valid in general and not only for LLM safety benchmarks, although none that have ever looked at resarch code will be surprised to learn that resarch code often is not of high quality (the incentives are not there). Everyone that publishes a benchmark should take note that making it easy to run increasesits scientific impact. \n* The ethical statement is very nice and an example to follow with its detailed discussion of limitations of the method."}, "weaknesses": {"value": "I do not find many weaknesses in this study. On the contrary, I find it very rigorous and trustworthy.\n\nMy main concern is whether the topic is too narrow, as it is a benchmark of LLM safety benchmarks. It is a bit on the side of representation learning, so I am not sure the community would value this study despite its many strong qualities. \n\nMy strong belief is that this type of meta studies that informs us, the AI community, what constitutes good resarch and what influences impact are important. However, I am well aware that many do not."}, "questions": {"value": "Did you consider to follow the PRISMA methodology [1] for structured literature reviews when conducting your structured search for benchmarks? \n\nI would have liked to see a flow diagram, such as the one proposed in the PRISMA methodology, for understanding how many papers were retrieved and excluded at different steps.\n\n[1] https://www.prisma-statement.org/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "60NbDLG4tf", "forum": "NrSx6J6IZk", "replyto": "NrSx6J6IZk", "signatures": ["ICLR.cc/2026/Conference/Submission9799/Reviewer_xYHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9799/Reviewer_xYHc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9799/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932263551, "cdate": 1761932263551, "tmdate": 1762921282888, "mdate": 1762921282888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}