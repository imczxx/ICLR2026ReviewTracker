{"id": "hFS7XWGqXi", "number": 20623, "cdate": 1758308280349, "mdate": 1759896967351, "content": {"title": "Interpretable Hierarchical Concept Reasoning through Graph Learning", "abstract": "Concept-Based models (CBMs) are a class of deep learning models that provide interpretability by explaining predictions through high-level concepts. These models first predict concepts and then use them to perform a downstream task. However, current CBMs offer interpretability only for the final task prediction, while the concept predictions themselves are typically made via black-box neural networks. To address this limitation, we propose Hierarchical Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for both concept and task predictions. H-CMR models relationships between concepts using a learned directed acyclic graph, where edges represent logic rules that define concepts in terms of other concepts. During inference, H-CMR employs a neural attention mechanism to select a subset of these rules, which are then applied hierarchically to predict all concepts and the final task. Experimental results demonstrate that H-CMR matches state-of-the-art performance while enabling strong human interaction through concept and model interventions. The former can significantly improve accuracy at inference time, while the latter can enhance data efficiency during training when background knowledge is available.", "tldr": "We introduce Hierarchical Concept Memory Reasoner (H-CMR), a novel CBM that combines rule learning, rule selection and graph learning to provide interpretability for both concepts and tasks while maintaining state-of-the-art accuracy.", "keywords": ["Concept-based models", "explainable AI", "neurosymbolic"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a8d8e12082ad0f7825d49492625cfbf3bcd22e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Concept-based models offer interpretability by predicting human-understandable concepts, but the prediction of the concepts themselves typically relies on black-box neural networks. This paper proposes a hierarchical concept prediction framework that learns an acyclic graph of concepts, aiming to improve interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of hierarchically predicting concepts is sound and well-motivated."}, "weaknesses": {"value": "**Major Weaknesses**\n\n1. **Lack of Discussion on Related Work:**\nThere is a line of prior work that uses logic rules as model concepts [1, 2, 3]. The proposed approach seems similar in that a neural network selects rules based on their embeddings. However, the paper does not provide a clear discussion of how it differs from these methods.\n\n2. **Concepts Are Not a True Bottleneck:**\nA major concern is that in the proposed framework, concepts no longer act as a true information bottleneck. As shown in Figure 1, there appear to be direct black-box paths from input to output that may bypass the concept layer entirely. This could undermine the interpretability claim, as the model might “cheat” by encoding all information through embeddings rather than concept structure [2, 3].\n\n3. **Methodology Unclear and Not Well-written:**\nFirst, it is not explained how the concept pool is constructed. It is unclear whether the model relies on human-annotated concepts (as in datasets like CUB) or learns the concepts automatically. If the former, this raises concerns about scalability and generalizability; if the latter, it is unclear how the learned concepts maintain interpretability.\nSecond, the motivation behind parameterizing the encoder with a Bernoulli distribution and enforcing a delta distribution for the concept embeddings is not well justified. I suspect that this choice might have been made for training stability or sparsity, but it is not clear whether it plays a crucial role in performance. If this probabilistic formulation is important, it should be supported by ablation studies; if not, the reasoning for adopting it should be clarified.\nThird, the description of the decoder mechanism - particularly how the model aggregates a pool of candidate rules based on parent concepts and selects one for child concepts - is confusing. From the current explanation, I suspect that the decoder may sequentially sample rules conditioned on parent availability, but this process is not clearly described. This lack of clarity makes it difficult to understand how the hierarchical structure is actually learned or enforced.\nFinally, many notations and equations in Sections 2.2.2, 2.2.3, and 3 are not properly defined, which adds further confusion and makes the paper challenging to follow.\n\n4. **Ambiguity in Likelihood Maximization (Eq. 9):**\nEquation (9) suggests maximizing the likelihood of predicted concepts $\\hat{c}_i$, implying access to their ground-truth labels. However, it is unclear whether such supervision is available or how the model learns the “correct” concepts if not. \n\n5. **Non-Deterministic Interpretations:**\nIt appears that the decoder selects rules probabilistically, which raises the concern that the same input-output pair could yield different interpretations across runs.\n\n6. **Questionable Use of \"Theorem:**\nThe statements labeled as Theorem 5.1 and Theorem 5.2 do not appear to present substantial theoretical results. It is not evident what value these “theorems” contribute.\n\n7. **Limited Scope and Evaluation:**\nThe experiments are restricted to simple vision datasets. Moreover, the paper later reveals that concept annotations are required, which significantly limits practicality. There is also no qualitative demonstration of interpretability, despite this being the paper’s main goal.\n\n**Minor Comments**\n\n8. In several equations, $\\hat{x}$ appears where $x$ would be expected. Please clarify the distinction between the two.\n\n**References**\n\n[1] Deep Neural Networks Constrained by Decision Rules (AAAI 2019)\n\n[2] Self-Explaining Deep Models with Logic Rule Reasoning (NeurIPS 2022)\n\n[3] Toward Faithful and Human-Aligned Self-Explanation of Deep Models (npj Artificial Intelligence 2025)"}, "questions": {"value": "1. Clarify whether and how the model architecture enforces the concept bottleneck. Consider adding experimental evidence (e.g., ablation or information flow analysis) showing that the model genuinely relies on concepts rather than shortcutting through embeddings. (W2)\n\n2. Provide a clearer, methodological explanation, including the presence of ground-truth labels for concepts and whether the interpretations are deterministic. (W3, W4, W5)\n\n3. Consider rephrasing these sections as Propositions or Lemmas, or alternatively, move them to an appendix if they are primarily for clarification. Explicitly state what each result contributes to the method or guarantees about model behavior. (W6)\n\n4. Evaluate on larger-scale datasets (e.g., ImageNet) to demonstrate scalability. Include at least one qualitative visualization or example of model interpretation. Discuss how the method could be extended to domains lacking concept annotations (e.g., via weak supervision or learned concepts). (W7)\n\n5. Include a more detailed comparison with prior works that integrate logic reasoning into neural networks. Highlight what conceptual or methodological novelty your hierarchical approach introduces beyond embedding-based rule selection. (W1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KSmAyOopiS", "forum": "hFS7XWGqXi", "replyto": "hFS7XWGqXi", "signatures": ["ICLR.cc/2026/Conference/Submission20623/Reviewer_3RNr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20623/Reviewer_3RNr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760926473, "cdate": 1761760926473, "tmdate": 1762934024915, "mdate": 1762934024915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hierarchical Concept Memory Reasoner (H-CMR), a new Concept-Based Model (CBM) that provides interpretability not only for the final task but also for intermediate concept predictions, which learns a DAG of concepts and represents their relationships via neural-selected symbolic logic rules stored in a shared memory. Specifically, H-CMR contains three different modules: an encoder which predicts source concepts and a latent embedding, a decoder which hierarchically infers other concepts through symbolic rule execution, and a memory which stores learnable logic rules defining parent–child relations. Experiments show that H-CMR achieves SOTA concept accuracy and maintains universal classification capability comparable to black-box networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is interesting. Compared to previous CBM methods, which can only explain task-level predictions, H-CMR explicitly models how concepts depend on one another via interpretable logic rules. \n2. The paper is well-written and easy to follow. \n3. The experiment results on three different datasets show the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. Although the idea is interesting. However, I feel suspicious about whether the proposed methods can be extended to real-world scenarios. Currently, the experiments are done on some small toy datasets, which limit the contribution of the proposed methods. In order to provide a more comprehensive view of the proposed methods, it is better to show the results of the model in more complex scenarios. For example, there are lots of different vehicle categories in ImageNet. Is it possible to use the H-CRM to extract concepts that humans can understand and explain the logic between different concepts and tell the logic between different concepts? If humans cannot understand the extracted concept, the interpretability is not valid, even if humans can understand the logic between concepts. For example, a model can tell me $C_k=C_i \\vee C_j$, without any understanding of $C_k, C_i, C_j$, we can not even tell whether the logic is true or false. Also, without such understanding, model intervention is not valid because we cannot even tell how to change the logic behind the concepts. \n2. The ablation studies are not sufficient. Currently, the H-CMR claims to provide interpretable inference for both concept and task predictions, the paper does not include any empirical validation that the extracted explanations (logic rules or hierarchical DAGs) are semantically or causally correct."}, "questions": {"value": "I will raise my rating if the author can provide additional evidence of the interpretability and demonstrate that it is correct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ijfwm20aVv", "forum": "hFS7XWGqXi", "replyto": "hFS7XWGqXi", "signatures": ["ICLR.cc/2026/Conference/Submission20623/Reviewer_U5fB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20623/Reviewer_U5fB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094056434, "cdate": 1762094056434, "tmdate": 1762934024210, "mdate": 1762934024210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed H-CMR (Hierarchical Concept Memory Reasoner), a neuro-symbolic framework that performs interpretable concept reasoning through a directed acyclic graph of learned logical rules. \nIt first predicts semantically grounded concepts from perceptual inputs and then composes these concepts hierarchically via differentiable rule modules to infer higher-level concepts and final task decisions. \nEach rule defines an explicit logical relation among parent concepts, enabling transparent reasoning across multiple abstraction levels. \nThe model jointly optimizes concept prediction, rule learning, and task objectives to balance interpretability with performance. \nExperimentally, H-CMR is validated across synthetic and visual benchmarks such as MNIST-Addition, CIFAR-10, and CUB, demonstrating improved intervenability and interpretability over concept-based baselines. \nThese results highlight H-CMR’s ability to achieve structured, human-understandable reasoning without sacrificing predictive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1. Strong integration of concept-based and neuro-symbolic reasoning**\n\nThe proposed method provides a well-structured integration of concept bottleneck modeling with neuro-symbolic reasoning through its unified architecture of concept learning and rule-based inference. \nIt jointly learns concept embeddings and logical rules within a directed acyclic graph, allowing the model to represent both the semantics and dependencies among concepts in an interpretable manner.\nThe neural encoder captures perceptual information and grounds the concepts, while the symbolic reasoning layer operates over these learned concepts using explicit logical compositions. This design effectively connects data-driven neural representations with transparent symbolic reasoning, enabling interpretable predictions at both the concept and task levels.\n\n- **S2. Clear formulation of hierarchical reasoning and strong theoretical grounding**\n\nH-CMR clearly defines how hierarchical reasoning emerges through the recursive composition of learned logical rules. \nEach concept is inferred from its parent concepts via explicit rule evaluation, and these dependencies propagate across multiple levels of the directed acyclic graph (DAG), forming interpretable hierarchical structures. \nAlso, the authors provided solid theoretical grounding for this formulation. \nIt formalizes the model’s expressivity, acyclicity, and computational complexity, with clear statements of the underlying assumptions and guarantees."}, "weaknesses": {"value": "- **W1. Computational complexity and scalability considerations**\n\nWhile the proposed method offers a principled formulation for interpretable concept reasoning, its computational complexity raises concerns about practical scalability. \nEmpirical results in Table 4 show that runtime and memory usage increase significantly with the number of concepts, suggesting that efficiency could become a bottleneck for large-scale or densely connected concept graphs. \nIt would be valuable to explore how the practicability of the proposed method could be improved--particularly by testing it on real-world, higher-dimensional datasets or by incorporating optimization strategies such as hierarchical pruning, rule caching, or sparse evaluation. Such extensions could strengthen the framework’s applicability beyond medium-scale benchmarks while preserving its interpretability advantages.\n\n- **W2. Need for stronger qualitative concept visualization and complementary quantitative validation**\n\nWhile the proposed method provides clear rule-based reasoning and interpretable dependency graphs, it would benefit from richer qualitative visualization of the learned concepts--a practice that has become standard in concept-based interpretability research. Many concept bottleneck and concept-based models support their claims with visual examples showing how individual concepts are localized, clustered, or activated across samples. Such qualitative results help readers intuitively grasp what each learned concept represents and how concept combinations contribute to hierarchical reasoning.\nIncorporating visual analyses of representative concepts or rule activations--such as heatmaps, or example patches--would make the interpretability of H-CMR more tangible and relatable. \nIn parallel, pairing these visuals with quantitative measures (e.g., localization accuracy, sparsity, or activation consistency across samples) could provide stronger empirical support for the model’s interpretability claims.\n\nThis also opens several natural questions for further exploration:\n- How can concept localization be structured hierarchically, reflecting parent–child relationships in the rule graph?\n- Can hierarchical visualizations reveal whether higher-level concepts emerge from spatial or semantic composition of lower-level ones?\n- How can visual concept maps be used to validate the internal reasoning paths of H-CMR?\n\nAddressing these questions through both visualization and quantitative validation would strengthen the link between H-CMR’s symbolic reasoning and the perceptual evidence that grounds its concepts, making the framework more convincing and complete."}, "questions": {"value": "Most of my main concerns or questions have been outlined in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o9ZJ4nDrCy", "forum": "hFS7XWGqXi", "replyto": "hFS7XWGqXi", "signatures": ["ICLR.cc/2026/Conference/Submission20623/Reviewer_99og"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20623/Reviewer_99og"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147427963, "cdate": 1762147427963, "tmdate": 1762934023501, "mdate": 1762934023501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hierarchical Concept Memory Reasoner (H-CMR), a concept-based model that combines symbolic reasoning with neural attention for interpretable concept and task prediction. The key view is to learn a directed acyclic graph (DAG) over concepts and tasks, where each node is predicted using neurally selected, symbolic logic rules. This framework enables step-by-step symbolic inference and supports both concept-level and model-level interventions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. More interpretability: The model offers clear, logic-based explanations for both concept and task predictions, going beyond typical CBMs that only explain task-level outputs.\n\n2. Human-in-the-loop friendly: The architecture is explicitly designed to allow interventions during both inference and training, making it suitable for scenarios where expert knowledge is available."}, "weaknesses": {"value": "1. Latent embeddings still required: Despite the symbolic reasoning layer, a latent embedding is still used in rule selection, which introduces some opacity.\n\n2. Limited task diversity: Most experiments focus on standard classification datasets; it remains unclear how well the method generalizes to more complex domains.\n\n3. Quadratic complexity: The approach has worst-case quadratic time complexity in the number of concepts, which may limit scalability."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7oyUIIzpU3", "forum": "hFS7XWGqXi", "replyto": "hFS7XWGqXi", "signatures": ["ICLR.cc/2026/Conference/Submission20623/Reviewer_zruZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20623/Reviewer_zruZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325620730, "cdate": 1762325620730, "tmdate": 1762934023029, "mdate": 1762934023029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}