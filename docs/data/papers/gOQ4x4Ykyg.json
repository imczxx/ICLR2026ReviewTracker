{"id": "gOQ4x4Ykyg", "number": 11705, "cdate": 1758203236612, "mdate": 1759897559662, "content": {"title": "MACEval: A Multi-Agent Continual Evaluation Network for Large Models", "abstract": "Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.", "tldr": "", "keywords": ["large generative models", "dynamic evaluation", "continual evaluation", "multi-agent systems"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d6fe359909c4e66c599eed33ce899248addcbfe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **MACEval**, an automated multi-agent evaluation framework for large language models (LLMs).  The core idea is to move beyond static benchmarks by letting LLMs themselves propose new problems, generate candidate solutions, and act as judges in a closed-loop evaluation pipeline.  The authors claim that such a *multi-agent, saturation-resistant* benchmark allows continuous discovery of challenging tasks without human intervention.   MACEval reportedly adjusts task difficulty dynamically and provides automatic scoring based on agent interactions.   The paper positions this work as a step toward scalable, self-improving LLM evaluation.\n\nThe paper has two central weaknesses.  First, it **does not report** the new and challenging tasks that the pipeline has dynamically generated, so nobody knows really whether those tasks are good or bad.  For a paper submitted to datasets and benchmarks, this problem really hurts.  \n\nSecond, the tasks, which are not reported, are most likely **low-quality, contrived, pointless, low in construct validity or even wrong**.  The proposed math tasks are so hard that gemini-2.5-pro, according to the authors, cannot solve a system of linear equations with 3 variables.  (Who believes that?). The pipeline makes coding tasks arbitrarily and pointlessly harder by adding a lot more `for` and `while` loops.  (Who cares about such tasks?)\n\nI myself believe that multi-agent eval is the way to go and have experimented similar ideas months ago.  I have found that the core difficulty of doing this is precisely that **LM-generated difficult tasks become pointless and meaningless very quickly**.  This paper does not contain any ideas that I did not try when I ran my experiments, and instead contains many suggestive signs that those synthetically generated tasks are of low-quality, so my best guess is that those tasks are as low-quality as the ones I produced.  \n\nFor now, my overall recommendation is **rejection**.  While the conceptual direction—automated, multi-agent benchmarking—is correct and timely, the current implementation lacks transparency, reproducibility, and conceptual rigor.  Strong revision with released tasks, code, and clearer construct validity analysis could make this a valuable contribution in future iterations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Motivation and vision.**  \n   The authors correctly identify a central problem in modern benchmarking: *saturation*.  As top models quickly approach near-perfect scores on traditional datasets, automatic generation of new and harder tasks is a promising direction.  The proposed “multi-agent” perspective—where one LLM proposes tasks, another solves them, and a third judges—captures the spirit of adaptive evaluation.  Conceptually, this is aligned with current interest in *adversarial*, *continual*, and *self-play* evaluation methods.\n\n2. **Relevance.**  \n   The work addresses a pressing bottleneck: human-authored benchmarks cannot keep pace with the speed of model scaling.  The vision of having autonomous agents design and grade tasks is highly appealing, especially for reasoning, coding, and planning domains.\n\n3. **Potential originality.**  \n   While similar efforts (e.g., *self-challenging agents* from Meta) exist, MACEval’s multi-agent framing offers a distinctive lens if implemented correctly.   Even a partial success in generating meaningful new tasks could have impact."}, "weaknesses": {"value": "This paper is likely problematic, but due to lack to transparency (point 1), it is not easy to pin down the exact problem.  I can explain my reasoning based on some circumstantial evidence (points 2-4).  My best guess is that this paper makes the same mistake as in *Illusion of Thinking*, i.e., **misunderstanding contrived complication for difficulty with realistic value**.\n\n1. **Lack of transparency in the generated tasks.**  \n   The most severe weakness is the *absence of released or described tasks*.  Since the paper claims to generate novel and challenging problems that even frontier models fail to solve, the readers must be able to inspect examples.  Without this, the claimed contribution is unverifiable.  For a “datasets and benchmarks” track, this omission is fatal—the contribution of the paper hinges heavily on the quality of generated tasks.\n\n   > **Actionable fix:** Release at least 20 representative generated tasks (including both the prompt and reference solution) across domains.  \n   > Provide human evaluation of task validity and difficulty.  \n\n\nBecause I cannot see the tasks themselves, I really cannot tell what is going on behind the scenes, but there are many telltale signs suggesting that the tasks are really bad.  \n\n2. **Highly implausible evaluation results suggesting implementation errors.**  \n   The paper claims that even GPT-4o and Gemini-2.5-Pro can only solve linear systems of equations with two or three variables (line 1102-1108).  I cannot imagine anyone believing in this.  OpenAI and Anthropic are talking about getting IMO gold medals yet their models cannot solve linear equations.  Seriously?\n\n   I asked GPT-4o to solve something like the following \n   $$\n   \\\\begin{cases}\n   x + y + z + w + t = 15 \\\\\\\\\n   2x - y + 3z + w - t = 10 \\\\\\\\\n   x + 2y - z + 4w + t = 12 \\\\\\\\\n   3x - y + 2z - w + 2t = 14 \\\\\\\\\n   x - y + z + w + 3t = 16\n   \\\\end{cases}\n   $$\n   and it does not seem to have a problem. \n\n   Such extreme underperformance suggests that either (a) task formulation or parsing is flawed, or (b) the evaluation code is incorrect.  Because no code or dataset is available, these results cannot be reproduced or inspected or trusted.\n\n   > **Actionable fix:** Release several failed examples and their model outputs, or share the generator code so that reviewers can verify whether the equations are malformed.  Please kindly provide 5 examples of three-variable linear equations that both GPT-4o and Gemini-2.5-pro fail to solve.\n\n3. **Confusion between complexity and complication.**  \n   The authors equate *increasing task difficulty* with *adding more elements or loops* (e.g., deeper trees, larger graphs, more for-loops in code).  This confuses “complication” with “conceptual complexity.”  True **complexity** involves deeper abstraction or reasoning (e.g., algorithm synthesis, multi-step planning), not longer or more tedious or more **complicated** arithmetic.  \n   This same critique applies to prior work such as Shojaee et al. (2025) *“The Illusion of Thinking,”* later refuted by Opus & Lawsen (2025) for exactly this mistake.  \n   In that heavily criticised paper, Shojaee et al asked LMs to solve larger and larger Tower of Hanoi tasks until the LMs gave up writing down the detailed full solutions.  The point is that by just increasing the size, the Tower of Hanoi did not become more complex, deeper, harder, or more intellectually demanding; it just became more complicaited, tedious, and requiring patience to solve properly.   As shown in the refutation, the LMs can easily write down the correct recursive formulas.  “The generated solutions correctly implement the recursive algorithm, demonstrating intact reasoning capabilities when freed from exhaustive enumeration requirements.”\n\n   Back to MACEval, the same error is repeated.  The tasks involved in paper are built in a similarly bad taste.  Instead of proposing genuinely more complex and more challenging tasks, the authors' operationalisation of controllable difficulty level is just making the task more complicated.  To wit, \n\n- Instead of checking if the LM is able to propose and implement algorithms for binary tree traversal, MACEval just makes the tree more complicated. \n- Instead of checking if the LM is able to propose and implement algorithms for shortest path search, MACEval just makes the graph more complicated. \n- Instead of checking if the LM is able to generate useful and meaningful code for longer horizon realistic tasks, MACEval just adds arbitrary and pointless requirements like incorporating yet another for loop or while loop or if conditional statement.  A code generation task does not become meaningfully more complex if one introduces a dozen more for loops and another dozen more if conditional statements.  That is just jumping throw some arbitrary and useless hoops.   \n\n   In short, the “harder” tasks in MACEval are simply more verbose, not more cognitively demanding.\n\n   > **Actionable fix:** Incorporate explicit *conceptual challenge dimensions* (e.g., compositional reasoning, ambiguity, multi-objective constraints) rather than numeric scaling of task size.  Otherwise, the tasks are not genuinely more difficult and do not represent meaningful future directions where we want to scale our future LMs. \n\n4. **Low construct validity of the proposed tasks.**  \n   Drawing on classical measurement theory (Cronbach & Meehl, 1955), a benchmark must have clear construct validity—it should measure the intended capability.  In MACEval, task difficulty in code generation, for example, is operationalized as the number of `for`, `while`, or `if` statements in code, which does not reflect coding competence.  Experienced programmers know that debugging logical flow, managing dependencies, and ensuring semantic correctness—not loop count—define challenge.  Thus, the authors are misaligned with what the community cares about and want to evaluate.  There are so many meaningful and economically valuable aspects of coding that needs to be evaluated. One really should not pay attention to adding more `while` loops to coding task requirements.\n\n   > **Actionable fix:** Re-define difficulty metrics based on semantic or reasoning depth (e.g., algorithmic novelty, multi-file context comprehension) rather than syntactic count.\n\n5. **Presentation and clarity issues.**  \n   The command of English is overall good, but the math appears to be merely decorative and does not serve much narrative purposes.  After being defined it is not really used elsewhere and does not help with understanding.   This is just unnecessary cognitive load for a paper which suffers from clarity of expression. \n\n   > **Actionable fix:** Simplify the math or actually use it.\n\n\n**In summary**,\nmy question is this: \n   > Why hasn't anyone published a benchmark containing lengthier and lengthier long-division and integer multiplication tasks?  (I am sure models all struggle with a task like calculating 182734018743091374 times 182347190387409138275, or any two huge numbers that an LM randomly comes up with.)  \n\nIt is not because it is difficult to come up with such an idea, but because the idea is stupid.  The authors really need to explain why adding more `for` loops to a coding task requirement that they are doing is fundamentally different from my **dynamically-generated**, **extremely challenging**, **saturation-resistant**, **human-label-free**, **multi-agent**, long-division-and-multiplication benchmark generation pipeline."}, "questions": {"value": "1. Could the authors release a subset of generated tasks (e.g., 20 examples across domains) to allow reviewers to inspect quality?  \n2. How are “difficulty levels” operationalized and verified, beside just \"harder than before\"?\n3. Can the authors clarify the apparent inability of GPT-4o and Gemini-2.5-Pro to solve basic linear systems? This is really hard to believe.\n4. How does MACEval ensure that generated tasks maintain *construct validity* rather than devolving into contrived puzzles?  \n5. Have human evaluators assessed whether tasks are meaningful or solvable? If not, do the authors plan to include such validation in a future version?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J7pqqtJimO", "forum": "gOQ4x4Ykyg", "replyto": "gOQ4x4Ykyg", "signatures": ["ICLR.cc/2026/Conference/Submission11705/Reviewer_SaU7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11705/Reviewer_SaU7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760946946662, "cdate": 1760946946662, "tmdate": 1762922751985, "mdate": 1762922751985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method of using LLMs to automatically generate evals and continuously change their difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Automated eval methods are promising and an important avenue of work. This paper proposes a workflow to achieve automated eval results for LLMs."}, "weaknesses": {"value": "Missing (very) related work:\nhttps://arxiv.org/pdf/2312.14856\nhttps://arxiv.org/abs/2310.17567\nhttps://arxiv.org/pdf/2502.06453\nhttps://alignment.anthropic.com/2025/petri/\n\n- It would be good to reflect on the quantity vs quality argument.\n- This is presented as an automated eval method but humans have to manually come up with the tasks (only 9 introduced in the paper). E.g. construct an eval where you can iteratively add more noise to an image. While this is fine (the above linked papers do similar things) this paper goes to far in suggesting the full pipeline is automated.\n- the 9 tasks are very simple, akin more to standard augmentation methods and not features which test frontier AI intelligence.\n- Very little work was done to investigate how accurate the autograders were. Does a high score even correlate with intended behaviours/abilities? They have an ablation study measuring agreement between the autograders and humans but this was only on hallucination detection. The autograders have been tasked with doing considerably more complex tasks than this.\n- over all this seems to be a very token hungry evaluation which yields hard to interpret results (what does it mean to be good at this eval?)."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aGleBl1R5o", "forum": "gOQ4x4Ykyg", "replyto": "gOQ4x4Ykyg", "signatures": ["ICLR.cc/2026/Conference/Submission11705/Reviewer_BfFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11705/Reviewer_BfFa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957507986, "cdate": 1761957507986, "tmdate": 1762922751661, "mdate": 1762922751661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I actually read the paper carefully several times. I think the paper has several major flaws: \n\n1. the authors need to significantly improve their presentation. As an AI researcher has done both benchmarking and agent paper myself, especially end-to-end agent self-evaluation frameworks, I almost could not understand the paper. The figures are not informative, the results are not enough, and the overall contribution is not clear in the paper.\n\n2. The contribution of the paper is not clear. The authors propose a self-evaluation framework with agents, but could not clearly define the research gap, and the differences to exisiting works. Is the authors main contribution on the agent design side, or on the evaluation side, or both? How topology is helping the framework, what are the key diffeneces of the paper's benchmark, to, say, existing famous benchmark evaluation results, like in coding and math? \n\nOverall, I feel the paper reads more like a tech report, not a research paper, especially at the ICLR level."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Good motivation"}, "weaknesses": {"value": "See summary"}, "questions": {"value": "Would suggest improve the contribution and the framework figures, like figure 1 and 2, too complicated and not clear at high level, not able to have take aways, solve all issues in summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ns8kJpdGiU", "forum": "gOQ4x4Ykyg", "replyto": "gOQ4x4Ykyg", "signatures": ["ICLR.cc/2026/Conference/Submission11705/Reviewer_8Ctc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11705/Reviewer_8Ctc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971532430, "cdate": 1761971532430, "tmdate": 1762922751203, "mdate": 1762922751203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}