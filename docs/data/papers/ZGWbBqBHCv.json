{"id": "ZGWbBqBHCv", "number": 20888, "cdate": 1758311478321, "mdate": 1759896953565, "content": {"title": "DNAMotifTokenizer: Towards Biologically Informed Tokenization of Genomic Sequences", "abstract": "DNA language models have advanced genomics, but their downstream performance varies widely due to differences in tokenization, pretraining data, and architecture. We argue that a major bottleneck lies in tokenizing sparse and unevenly distributed DNA sequence motifs, which are critical for accurate and interpretable models. To investigate, we systematically benchmark k-mer and Byte-Pair Encoding (BPE) tokenizers under controlled pretraining, evaluating across multiple downstream tasks from five datasets. We find that tokenizer choice induces task-specific trade-offs, and that vocabulary size and training data strongly influence the biological knowledge captured. Notably, BPE tokenizers achieve strong performance when trained on smaller but biologically significant data. Building on these insights, we introduce DNAMotifTokenizer, which directly incorporates domain knowledge of DNA sequence motifs into the tokenization process.  DNAMotifTokenizer consistently outperforms BPE across diverse benchmarks, demonstrating that knowledge-infused tokenization is crucial for learning powerful, interpretable, and generalizable genomic representations.", "tldr": "A novel DNA tokenizer designed to incorporate prior knowledge of DNA sequence motifs for better genomic representation", "keywords": ["DNA language model", "Tokenization", "Genomics", "Sequence motifs"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/814ddb7e153eee1d2dd1672b6a43a33e708b5d25.pdf", "supplementary_material": "/attachment/c1a4ac391dca5d71160614cbe2e0c065acc19ee9.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors proposed DNAMotifTokenizer, a PWM-driven tokenizer that trims low-information flanks, handles reverse complements, and segments sequences via greedy trie matching. The tokenizer integrates seamlessly with BERT-style masked language modeling using motif-aware masking and an end-to-end, cache-friendly pretraining pipeline compatible with DNABERT. The goal is to replace purely k-mer tokenization with a more interpretable, biologically grounded alternative that preserves coordinate-level traceability. The given results suggest consistent gains on most of the existing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The author proposed a novel and interesting method to integrate the motif information to the tokenization, instead of eliminating such info as previous method did. It could bring more biologically useful information and help the language model investigate deeper in existing data.\n\n2. The authors provide the implementation of the core idea, which aligns well with the paper.\n\n3. The given results show that the proposed DNAMotifTokenizer can bring general improvement to most tasks.\n\n4. Apart from normal content, the authors also took a deeper look into the existing BPE tokenizer for a further discussion and comparison with the proposed one, which improves the rationality."}, "weaknesses": {"value": "1. Method: Heavy reliance on heuristics (length cap, flank-trimming thresholds, greedy matching) and PWM quality. Such components may bias learning and miss unknown motifs.\n\n2. Experiment: Although the overall performance of the proposed method is very good, it shows obvious bad results on DART-EVAL. For example, in Table 4, the accuracy of the proposed DNAMotifTokenizer is much lower than the SOTA method. The authors are suggested to give the explanations of why this happens."}, "questions": {"value": "How is the proposed DNAMotifTokenizer on more cross-species tasks? And what about more modern model architectures (e.g., llama, mamba), instead of BERTs only."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BB6VjYEcY1", "forum": "ZGWbBqBHCv", "replyto": "ZGWbBqBHCv", "signatures": ["ICLR.cc/2026/Conference/Submission20888/Reviewer_zmZc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20888/Reviewer_zmZc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808775116, "cdate": 1761808775116, "tmdate": 1762999989565, "mdate": 1762999989565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DNAMotifTokenizer, a motif-aware tokenization scheme for genomic sequences that integrates curated TF-motif priors and cCRE annotations to build a vocabulary of motif tokens. The work evaluates across several public genomics benchmarks (e.g., Genomic Benchmarks, GUE, NT-benchmarks, DART-Eval)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Brings explicit biological priors (TF motifs, cRE) into the tokenization step, offering a more interpretable alternative to opaque subword units.\n2. Provides multiple ablations on vocabulary size, segmentation strategies, and qualitative motif coverage"}, "weaknesses": {"value": "1. The paper’s own results indicate k-mer consistency better than BPE, and DNAMotifTokenizer’s average on NT-benchmarks remains notably below k-mer. This undercuts the central narrative that knowledge-injected tokenization improves fundamental understanding. \n\n2. Converting PWMs via a fixed 0.5 threshold and trimming wildcard ends discards degenerate bases and positional uncertainty, which are biologically meaningful. This can fragment genuine motif families and reduce robustness to natural variation.\n\n3. When multiple motifs overlap, the default random choice introduces non-determinism. The paper lacks multi-seed repeats and dispersion metrics to judge stability."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5wsnH08GvG", "forum": "ZGWbBqBHCv", "replyto": "ZGWbBqBHCv", "signatures": ["ICLR.cc/2026/Conference/Submission20888/Reviewer_UhJ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20888/Reviewer_UhJ2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935321283, "cdate": 1761935321283, "tmdate": 1762999989485, "mdate": 1762999989485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper benchmarks DNA tokenization strategies under a controlled pretraining setup and proposes DNAMotifTokenizer, which injects motif knowledge into the vocabulary and uses a greedy, locally flexible matching procedure. The results suggest that larger BPE vocabularies are not necessarily better, and that training the tokenizer on biologically informative subsets (e.g., motifs or cCRE regions) can perform comparably to training on the entire genome."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed tokenizer is conceptually simple and biologically informed, with clear pseudocode that enhances reproducibility.\n2. The experimental design of this study is rigorous as it meticulously isolates the impact of tokenization by matching computational FLOPs, model architecture, and fine-tuning pipelines across all comparisons (GUE, SCREEN, DART-Eval, Genomic Benchmarks, NT Benchmarks)."}, "weaknesses": {"value": "1. The technical presentation of this work requires further efforts.  The paper presents both a benchmark and a new method in a 9-page paper. The direct result is that the benchmark is not comprehensive and the analysis of the method is not enough. \n2. The experimental results show small absolute gains, and the variance is not reported, e.g. some improvements are ≤ 0.0005 in absolute terms.\n3. The 0-2 bp offset and random tie-breaking are reasonable, but their stability and computational complexity are not fully characterized. More discussion are needed.\n4. The figure captions are insufficiently detailed, lacking explanations for the individual subpanels (a, b, c, etc.), and the absence of descriptive legends hinders the interpretation of key elements."}, "questions": {"value": "1. What's the algorithm's sensitivity to parameters like the 0-2 bp offset and the tie-breaking mechanism? The performance gains appear modest relative to the added complexity compared to standard BPE. Please discuss the specific scenarios where this complexity is justified.\n·What is the primary intended contribution of this paper — a new method or a benchmark? The current structure does not fully align with either goal: as a benchmark paper, the experimental section of the main text is not enough; as a method paper, the narrative structure is not very reasonable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o9yWAc0fBa", "forum": "ZGWbBqBHCv", "replyto": "ZGWbBqBHCv", "signatures": ["ICLR.cc/2026/Conference/Submission20888/Reviewer_xLqN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20888/Reviewer_xLqN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995652881, "cdate": 1761995652881, "tmdate": 1762999989496, "mdate": 1762999989496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a DNA sequence tokenizer called DNAMotifTokenizer, which is based on transcription factor binding sites (TF motifs). By directly incorporating biological motifs into the vocabulary, it aims to enhance the interpretability and task performance of DNA language models. The authors conducted extensive experiments under multiple benchmarks (GUE, DART-Eval, NT) to systematically evaluate the impact of different tokenization strategies on model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core contribution is the hard-coding of biological prior knowledge (transcription factor motifs, TF motifs) into the vocabulary as \"tokens,\" demonstrating performance gains across multiple benchmarks through experiments.\n- Focuses on the core issue of DNA language models—the impact of tokenization strategies—and conducts systematic and reproducible comparative experiments.\n- The introduction of biological priors (motifs, cCREs) enhances interpretability, showing consistent gains across multiple tasks.\n- The experimental design is rigorous, and the training budget and parameter scale are well-controlled, making the results reliable."}, "weaknesses": {"value": "- The tokenization relies entirely on external databases (JASPAR, ENCODE), making the approach essentially \"manual knowledge injection,\" which cannot adapt to unknown regions or new species.\n\n- Limited Innovation: Using motifs as a vocabulary is an engineering improvement that is insufficient for a theoretical breakthrough. There is inadequate biological interpretative analysis, as the paper does not quantify the impact of motif tokens on the model's internal representations.\n\n- About Generalizability: The use of a traditional BERT architecture and short sequence inputs restricts the model's generalizability.\n\nImprovement Suggestions:\n- The core contribution is the hard-coding of biological prior knowledge (transcription factor motifs, TF motifs) into the vocabulary as \"tokens,\" demonstrating performance gains across multiple benchmarks through experiments.\n- Focuses on the core issue of DNA language models—the impact of tokenization strategies—and conducts systematic and reproducible comparative experiments.\n- The introduction of biological priors (motifs, cCREs) enhances interpretability, showing consistent gains across multiple tasks.\n- The experimental design is rigorous, and the training budget and parameter scale are well-controlled, making the results reliable.\n- Introduce a learnable motif discovery module to enable the model to have adaptive tokenization capabilities.\n- Test generalizability on unknown regions or artificially mutated data.\n- Provide interpretability metrics such as motif recovery rates and functional region enrichment.\n- Explore the scalability of the method in long sequence modeling or generation tasks."}, "questions": {"value": "please refer to the weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzbHYwUrXJ", "forum": "ZGWbBqBHCv", "replyto": "ZGWbBqBHCv", "signatures": ["ICLR.cc/2026/Conference/Submission20888/Reviewer_DHL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20888/Reviewer_DHL6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076270146, "cdate": 1762076270146, "tmdate": 1762999990276, "mdate": 1762999990276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}