{"id": "kRpTTF7drV", "number": 18636, "cdate": 1758289651841, "mdate": 1759897090503, "content": {"title": "SysIdBench: A Benchmark for System Identification Methods", "abstract": "Modeling the behaviour of dynamic systems is a difficult problem because (i) there is a plenitude of existing system identification methods and (ii) the broadly varying characteristics of different dynamic systems are not all addressed by a single best method.\nWhile benchmarking system identification methods has been recognized as an important asset for developers who want to select the most suitable method for their problem, these benchmarks currently lack the capabilities developers require for systematic benchmarking.\nAnalysing related work and our own, we have worked out five requirements on benchmarking system identification methods that have shaped the design of SysIdBench, our novel benchmark, which comprises data sets with specifically tailored data types, data splits, and evaluation metrics. \nIn particular, SysIdBench comprises a principle-based summarizing evaluation metrics using predictions of energy as the key measurement target, it allows for judging generalization capabilities of system identification methods, and it investigates the fulfillment of physical principles. \nThe code for our benchmark, including the links to the datasets, is available on [GitHub](anonymous.github.repository).", "tldr": "", "keywords": ["system identification", "benchmark", "dynamic sytems"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e99006180ad53fd455e988e35b5966f9532134d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new benchmark for system identification. The authors rigorously identify requirements for benchmark evaluation. A small set of data sets is provided and empirically evaluated on a small set of methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- the idea of unifying a system identification benchmark in the era of machine learning method is sound\n- the systematic identification of requirements for metric selection is rigorous"}, "weaknesses": {"value": "- The benchmark provides only a small set of four data sets which already exist\n- There is no comparison to existing classical system identification methods\n- The paper draws no conclusion about the use of metrics, the type of preprocessing, or the fitting method across different settings"}, "questions": {"value": "- line 079: \"archtiectures\" misspelled\n- in related work, can you relate the existing benchmarks to your work?\n- line 89: \"paper\" should be plural. There is a number of more spelling mistakes. I urge the authors to rigorously double check the manuscript.\n- line 95/96: \"We have synthesized developer requirements for system identification in different application domains by surveying evaluation techniques over an extensive range of system identification papers.\" What is the surveying criterion?\n- section 3: can you translate the stated (soft) requirements into precise mathematical formulations?\n- line 141: \"we found that existing performance comparisons of models for dynamic systems are not suitable for high-dimensional models that outperform classic methods\". How did you find that with the analysis of the requirements in section 3?\n- line 159: \"While the ID dataset has the same data distribution as the training dataset, the OOD datasets stems from a different distribution.\". How do you define a different distribution here? Is it on input sequence, output sequence, and what is defines the sequence to be different? Some KL-divergence > X?\n- What about non-parametric identification approaches? How are they accounted for in your methodology?\n- is the F16 dataset taken from the nonlinear system identification benchmark or are these new datasets? What about the other data sets?\n- section 6.1:\n  - how do you select the orer of the system?\n  - you fit a linear model to nonlinear or potentially switching dynamics systems. How can you ensure that (a) your model is meaningful, (b) the step response that you obtain is somewhat correct?\n  - Can you provide empirical results and possibel failure cases of your truncation length algorithm? \n- section 6.2: What is the theoretical justification for defining OOD samples based on input sequence energy? One could theortically compe up with examples where a widely different input sequence energy leads to similar output sequences.\n- line 323: \"valuereferstomoreaccuratepredictions\"\n- line 397: \"Recurrent neural network (RNN) models are infinitely-dimensional\". Can you justify this? Or what dimensionality are you defining here? RNNs still have a finite dimensional state and transition matrices.\n- Can you define more clearly what ReLiNet is?\n- Table 1: \n  - why are there 4 values for some dataset on ID and OOD NRMSE values?\n  - Why are values for ReLiNet only provided for the Hyst dataset?\n  - the column name for method and dataset are switched\n  - Can you provide performance of classical system identification method, i.e., non machine learning based method on these datasets? E.g. I am aware that there is a range of papers dealing with the f16 dataset from the nonlinear system identification benchmark.\n- is the benchmark publicly available, how are new methods evaluated or ranked, will there be a leaderboard, ...?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eJQ7gFvRuh", "forum": "kRpTTF7drV", "replyto": "kRpTTF7drV", "signatures": ["ICLR.cc/2026/Conference/Submission18636/Reviewer_TGvv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18636/Reviewer_TGvv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475263803, "cdate": 1761475263803, "tmdate": 1762928346728, "mdate": 1762928346728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SYSIDBENCH: A BENCHMARK FOR SYSTEM IDENTIFICATION METHODS\n\nThe document presents SYSIDBENCH, a novel benchmark for systematically evaluating system identification methods based on developer requirements and various performance metrics.\n\nSYSIDBENCH is a novel benchmark designed to systematically evaluate and compare various system identification methods based on specific developer requirements. ​\n\nSYSIDBENCH addresses the challenges of selecting suitable system identification methods due to the variety of existing algorithms and system characteristics. ​\nIt incorporates tailored datasets, data splits, and evaluation metrics to enhance benchmarking capabilities.\nThe benchmark focuses on five key requirements: accuracy, frequency recovery, heterogeneity, out-of-distribution generalization, and robustness.\nThe code and datasets for SYSIDBENCH are publicly available for developers."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "SYSIDBENCH provides a nice summary of several data sets and tries to adopt a systematic approach. It  is built upon five essential requirements derived from the literature on system identification. ​\n\nRequirement 1: Accuracy in time domain predictions is crucial for evaluating model performance. ​\nRequirement 2: Frequency recovery ensures that models can reproduce oscillatory behaviors accurately. ​\nRequirement 3: Heterogeneity addresses the need for metrics that handle outputs with different physical units. ​\nRequirement 4: Out-of-distribution (OOD) generalization evaluates model performance on unseen input signals. ​\nRequirement 5: Robustness assesses how well models perform under input disturbances. ​\n\nSYSIDBENCH employs a structured methodology to evaluate system identification algorithms against the defined requirements. ​\n\nIt utilizes four publicly available datasets: SHIP, MSD, HYST, and F16, each representing different dynamic systems. ​\nThe methodology includes dynamic-aware preprocessing to optimize training and validation datasets. ​\nSix evaluation metrics are used to assess the performance of five different identification algorithms. ​\nThe benchmark outputs evaluation metrics alongside the identified model parameters."}, "weaknesses": {"value": "The English is poor and needs attention to bring it up to publication standards. It obscures understanding of the article.\n\nThe objectives of this article are quite unclear. The authors claim: \"no benchmark currently exists that systematically evaluates identified models based on the requirements developer have on the resulting model.\" However, sytsem identification can be applied is so many different ways, and the algorithms employed are quite different for the different applications. Typically for design you need highly accuracte model, but for online use (diagnostics) less accuracy is required. Some uses are:\n\n 1. Control System Design\n\nModel-based control: Accurate models are needed for designing controllers like PID, LQR, MPC, etc.\nAdaptive control: Online system identification helps update models in real-time for systems with changing dynamics.\nRobust control: Identification helps quantify uncertainty and design controllers that can handle model variations.\n\n\n2. Fault Detection and Diagnostics\n\nResidual generation: Compare measured outputs with model predictions to detect anomalies.\nMode identification: As you mentioned, hybrid systems require identifying discrete modes (e.g., gear shifts, valve states).\nHealth monitoring: Track system degradation over time using identified parameters.\n\n\n3. Prediction and Forecasting\n\nTime series modeling: ARX, ARMAX, state-space models for forecasting future behavior.\nEnergy systems: Predict demand or generation in smart grids.\nEconomics and finance: Identify models for forecasting market trends or consumer behavior.\n\n\n4. Simulation and Digital Twins\n\nVirtual prototyping: Use identified models to simulate system behavior before physical implementation.\nDigital twins: Real-time models that mirror physical systems for monitoring and optimization.\n\n\n5. Machine Learning Integration\n\nFeature extraction: Identified parameters can serve as features for classification or regression tasks.\nHybrid modeling: Combine physics-based models with data-driven approaches (e.g., grey-box models).\nModel learning: Use system identification as a form of supervised learning where the system dynamics are the target.\n\n\n6. System Understanding and Exploration\n\nScientific discovery: Identify governing equations or dynamics from experimental data.\nModel validation: Compare different hypotheses about system behavior.\nParameter estimation: Infer physical parameters (e.g., mass, damping) from observed data.\n\n7. Process Optimization\n\nIndustrial processes: Identify models to optimize throughput, energy use, or quality.\nBatch processes: Model transitions between phases for better scheduling and control.\n\n\nGiven this background, what is the purpose of SYSIDBENCH? Section 3 (DEVELOPER REQUIREMENTS) addresses this but it should be obvious that you cannot have a SINGLE benchmark for every purpose. The 5 requirements are poorly developed, and the level of technical specification of these requirements is weak.\n\nWhat is the underlying model class? This is never addressed. Is it:\n\n-- a simple dynamical system\n-- a hybrid dynamical system\n-- a distributed dynamical system\n\nI suspect you need different benchmarks for each. Further for the 6 applications listed above I argue that you need a purpose-specific benchmark.\n\nThe Benchmark Results for Selected Methods also show how poorly conceived this paper is. Consdider diagnostics: we want to diagnose faults within some time frame, so how is this captured? For ealth monitoring: we want to track system degradation over time using identified parameters, and accurately estimate remaining useful life. So how is this captured?"}, "questions": {"value": "1. How is it possible to cover all possible uses of systems identification with one benchmark?\n\n2. Please address how your benchmark enables accurate diagnostics and prognostics development? Don't you need fault data specifically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jqx0vP9LFg", "forum": "kRpTTF7drV", "replyto": "kRpTTF7drV", "signatures": ["ICLR.cc/2026/Conference/Submission18636/Reviewer_9Crf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18636/Reviewer_9Crf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643522308, "cdate": 1761643522308, "tmdate": 1762928346379, "mdate": 1762928346379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new benchmark for system identification methods called SysIdBench. Having a good benchmark in any field is crucial, as it enables the systematic study of existing methods and facilitates the transfer of this knowledge to real-world problems. The benchmark was created based on synthesized developer requirements across various applications of system identification methods, including accuracy, frequency recovery, heterogeneity, out-of-distribution generalization, and robustness. It includes the results of four different system identification methods evaluated on four datasets, using five performance metrics across both in-distribution and out-of-distribution test sets. The main contribution lies in defining five evaluation metrics related to the five developer requirements. However, the paper lacks sufficient detail to justify the design choices made in the experimental setup—for example, the selection of datasets, the choice of benchmarked methods, and the approach used for splitting the data into training, validation, and test sets. These aspects require more careful consideration, and different strategies should be analyzed to assess sensitivity. Furthermore, the performance analysis is not adequately conducted. A deeper investigation is needed to demonstrate the importance of the proposed benchmark, including whether the datasets are correlated based on the newly defined metrics. If the dataset distributions in the meta-space are too similar, the benchmark may yield biased results. The main issue with benchmarking today is that many contributions introduce new benchmarks without a clear understanding of their strengths and weaknesses. These are crucial steps that are missing from the paper. Therefore, I recommend that the paper be rejected."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a new benchmark for system identification methods, comprising four methods evaluated on four datasets using five performance metrics. These metrics correspond to key development requirements for system identification across different application domains, making the contribution highly relevant for advancing the field.\n\nThe originality of the paper lies in defining five evaluation metrics aligned with the five development requirements for system identification—accuracy, frequency recovery, heterogeneity, out-of-distribution generalization, and robustness. This represents a significant improvement over existing benchmarks, which typically focus on a single metric, most often accuracy."}, "weaknesses": {"value": "The paper lacks sufficient arguments regarding the criteria used to select the four datasets included in the benchmark. Could you please also elaborate on how these datasets differ from those used in other benchmarks for system identification?\nThe paper also does not provide justification for the choice of the method used for determining the truncation length. It reads as if one method was arbitrarily chosen—what would happen if a different method were used, and how would that influence the benchmark results?\nRegarding the out-of-distribution datasets, it is stated that 10% of the signals with higher energy are included in the out-of-distribution set. How would the results change if this percentage were increased or decreased? Additionally, it is mentioned that the remaining signal sequences are randomly split into training, validation, and in-distribution test subsets. How many random splits were performed? What would happen if the split were stratified based on energy distribution? How would that influence the benchmark outcomes?\nCould you also provide justification for the selection criteria of the four methods included in the experiments?\nIt appears that the paper simply runs four system identification methods on four datasets and reports five evaluation metrics. However, the paper lacks any analysis of the benchmark itself—for instance, whether the included datasets are correlated across performance metrics, how they differ in their distributions within the meta-space, or what the signal energy distributions look like across datasets and between training, validation, and test splits. These analyses are crucial to determine whether the proposed benchmark truly contributes to benchmarking practices in the field."}, "questions": {"value": "Please elaborate on the details behind the experimental design choices, such as the selection of datasets, methods, and the different training, validation, and test splits.\nPlease further analyze the benchmark results to identify and discuss the strengths and weaknesses of the proposed benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xV1ehzTpb0", "forum": "kRpTTF7drV", "replyto": "kRpTTF7drV", "signatures": ["ICLR.cc/2026/Conference/Submission18636/Reviewer_vZQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18636/Reviewer_vZQs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832210825, "cdate": 1761832210825, "tmdate": 1762928345928, "mdate": 1762928345928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SYSIDBENCH, a benchmark framework for evaluating system identification methods - algorithms that learn dynamic system models from input–output data. Unlike existing benchmarks that are often domain-specific, rely on synthetic data, or use limited metrics, SYSIDBENCH aims to provide a comprehensive, principle-based, and domain-independent evaluation.\n\nThe authors define five developer-oriented requirements for evaluating system identification methods:\n1. Accuracy in the time domain\n2. Frequency Recovery: Evaluate predictions in the frequency domain\n3. Heterogeneity: Ensure fair evaluation across outputs with different physical units\n4. OOD Generalization: Evaluate model generalization to unseen input distributions\n5. Robustness: Assess model stability and performance under perturbed or noisy inputs"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel benchmark design integrating energy-related principles\n2. Five requirements are motivated by the literature \n3. Energy-based OOD dataset extraction is interpretable\n4. The paper structure is clear"}, "weaknesses": {"value": "1. Evaluation focuses primarily on neural network models, lacking classical baselines like arx\n2. No statistical analysis (variance or significance tests) in the reported results\n3. The results could be described with more informative figures\n4. Not clear if the OOD and the robustness requirements are conceptually distinct: both feed some unusual inputs into the model\n5. Not clear how the energy error is related to the heterogeneous prediction in Sec 7.3"}, "questions": {"value": "1. Could you provide some meaningful plots showing the generalization (Requirement 4)?\n2. Could you provide meaningful examples showing robustness (Requirement 5)?\n3. Could you provide the missing baselines and statistical uncertainty in the baseline performance metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yCrqkh3Bgc", "forum": "kRpTTF7drV", "replyto": "kRpTTF7drV", "signatures": ["ICLR.cc/2026/Conference/Submission18636/Reviewer_thFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18636/Reviewer_thFi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950092139, "cdate": 1761950092139, "tmdate": 1762928345524, "mdate": 1762928345524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}