{"id": "wSGle6ag5I", "number": 2255, "cdate": 1757042104970, "mdate": 1763741529672, "content": {"title": "Improving Diffusion Models for Class-imbalanced Training Data via Capacity Manipulation", "abstract": "While diffusion models have achieved remarkable performance in image generation, they often struggle with the imbalanced datasets frequently encountered in real-world applications, resulting in significant performance degradation on minority classes. In this paper, we identify model capacity allocation as a key and previously underexplored factor contributing to this issue, providing a perspective that is orthogonal to existing research. Our empirical experiments and theoretical analysis reveal that majority classes monopolize an unnecessarily large portion of the model's capacity, thereby restricting the representation of minority classes. To address this, we propose Capacity Manipulation (CM), which explicitly reserves model capacity for minority classes. Our approach leverages a low-rank decomposition of model parameters and introduces a capacity manipulation loss to allocate appropriate capacity for capturing minority knowledge, thus enhancing minority class representation. Extensive experiments demonstrate that CM consistently and significantly improves the robustness of diffusion models on imbalanced datasets, and when combined with existing methods, further boosts overall performance.", "tldr": "", "keywords": ["Imbalance", "Diffusion Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f0367b159bb284504a4d190c2c66a321e28be80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates the challenge of generative modeling for imbalanced datasets. The hypothesis is that the poor generation quality for minority classes is primarily caused by an imbalance in \"model capacity,\" where the model's learning resources are disproportionately occupied by the head (majority) classes. To address this, the paper introduces a novel technique named Capacity Manipulation (CM), which explicitly reallocates and reserves model capacity for the tail (minority) classes. The proposed method employs a low-rank decomposition of the model's parameters, enabling fine-grained control over capacity allocation. A bespoke capacity manipulation loss function is introduced to ensure sufficient capacity is dedicated to learning the features of minority classes, leading to a significant enhancement in their generative representation. The claims are substantiated by comprehensive experimental results, and the overall methodology is presented with clear and coherent logic."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I find this approach remarkably novel in how it attributes the class imbalance problem to \"uneven model capacity allocation.\" It represents a significant departure from traditional paradigms like data resampling or loss re-weighting, introducing a fresh perspective by intervening directly at the model parameter level. By the way, I'm also curious if the author's method could be applied to long-tail recognition tasks (e.g., with ResNeXt-50 on CIFAR). No detailed explanation is needed if the implementation is complex—I'm simply wondering about its potential.\n2. The design of  loss function is exceptionally clear in its objective. By creating a \"push-pull\" dynamic between 'consistency' and 'diversity', it effectively channels distinct knowledge into separate parameter subspace.\n3. The paper doesn't just rest on solid experimental results; it also provides theoretical analysis (Theorems 2.1 and 3.1) to substantiate its core thesis: that majority classes indeed dominate parameter updates and that low-rank decomposition can effectively mitigate this dominance.\n4. The experimental validation is remarkably comprehensive. It covers a wide range of datasets (from simple to complex, low-res to high-res), various imbalance ratios, and multiple evaluation metrics, all benchmarked against strong baseline methods."}, "weaknesses": {"value": "1. I'm also curious if the author's method could be applied to long-tail recognition tasks (e.g., with ResNeXt-50 on CIFAR). No detailed explanation is needed if the implementation is complex—I'm simply wondering about its potential.\n2. My main question is about the capacity 'reservation.' The structure of the parameter decomposition seems to be fixed. This makes me wonder: is this 'hard partitioning' approach truly optimal? Could there be a way for the model to dynamically and adaptively decide how much capacity to allocate to each component during training, rather than relying on a predefined split?\n3. Are there any toy experiments that can visually illustrate this? For instance, using the two-class example you mentioned, could you show how the majority class ends up occupying most of the model's parameter capacity?\n4. I think this assumption has some limitations, especially with varying balance ratios like in ImageNet-LT. For example, in an extreme case with one head class and 999 tail classes, is a single, the setting of rank still appropriate? Or does the rank itself need to be adjusted based on class frequency?\n5. To empirically validate the hypothesis, is it possible to visually demonstrate that the class-specific parameters specialize in learning features unique to minority classes, while the class-agnostic parameters focus on capturing generic features dominated by the majority (head) classes? We propose achieving this through visualization techniques.\n6. My last question is about the diversity within the tail itself. Can you visualize them?"}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AkjtQger4z", "forum": "wSGle6ag5I", "replyto": "wSGle6ag5I", "signatures": ["ICLR.cc/2026/Conference/Submission2255/Reviewer_JwQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2255/Reviewer_JwQN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636248066, "cdate": 1761636248066, "tmdate": 1762916164082, "mdate": 1762916164082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the poor minority-class performance of diffusion models trained on long-tailed data, arguing that a key culprit is capacity misallocation—majority classes dominate parameter updates and monopolize representational space. \nTo tackle this, it proposes Capacity Manipulation (CM): each weight matrix is decomposed into a general/majority component and a reserved low-rank minority component, and training employs a capacity-manipulation loss that enforces consistency for majority classes while promoting diversity for minority classes. \nAt inference, parameters are merged, introducing no additional latency. \nAcross imbalanced CIFAR-10/100, CelebA-HQ, ImageNet-LT, iNaturalist, and ArtBench-10 (including Stable Diffusion fine-tuning), CM improves FID/KID and delivers especially strong gains on Medium/Few splits over strong baselines (e.g., CBDM, OC), while remaining orthogonal and complementary to them."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a clear and original lens—capacity allocation—and introduces a simple, effective mechanism that reserves low-rank capacity for minority classes, moving beyond reweighting or oversampling.\nMethod quality is strong: the parameter split plus a consistency/diversity loss is minimally invasive, theoretically motivated by gradient/representation analyses, and incurs no inference overhead due to weight merging.\nEmpirically, results are broad and convincing across multiple datasets/backbones (including SD fine-tuning), with especially large gains on Medium/Few splits and stable ablations over ranks and loss weights.\nThe approach is practical and orthogonal to existing long-tail remedies (e.g., CBDM, OC), making it easy to adopt and combine for further improvements."}, "weaknesses": {"value": "1. The paper should more clearly distinguish CM from class-balanced objectives, reweighting/oversampling, class-specific adapters/LoRA, and Mixture-of-Experts. Add a side-by-side comparison and reproduce at least one adapter/MoE-style baseline under matched compute.\n\n2. The analysis explains majority gradient dominance and motivates reserving rank, but does not specify conditions ensuring no loss of global likelihood or bounds on interference.\n\n3. Most results are class-conditional image benchmarks.\nFor instance, text-to-image (multi-attribute, compositional) and multi-label long-tails are underexplored."}, "questions": {"value": "1. How is the minority/majority split determined, and how sensitive are results to this choice under dataset drift or rebalancing?\n\n2. When merging weights at inference, how do the authors prevent cross-talk between the general and minority subspaces?\n\n3. Does reserving capacity degrade majority-class fidelity or diversity in any regimes?\n\n4. What are the exact training overheads introduced by the extra low-rank factors and CM loss? Do gains persist under tight compute budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FTiqr7l1BR", "forum": "wSGle6ag5I", "replyto": "wSGle6ag5I", "signatures": ["ICLR.cc/2026/Conference/Submission2255/Reviewer_pAbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2255/Reviewer_pAbs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966836027, "cdate": 1761966836027, "tmdate": 1762916163626, "mdate": 1762916163626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Capacity Manipulation (CM), a method to improve diffusion models trained on class-imbalanced data. It identifies that majority classes dominate model capacity, limiting minority representation. CM explicitly reserves capacity for minority classes through low-rank parameter decomposition and a capacity manipulation loss that balances consistency and diversity. Experiments on multiple benchmarks show that CM consistently enhances minority-class generation quality and overall robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, well-structured, and easy to follow.\n2. The proposed Capacity Manipulation (CM) method is conceptually simple yet effective, relying on low-rank decomposition and a targeted regularization loss to reserve model capacity for minority expertise.\n3. Theoretical analyses provide solid intuition about how imbalance affects parameter updates and how CM mitigates this effect.\n4. Extensive experiments across small- and large-scale datasets convincingly demonstrate that CM improves minority-class quality without degrading majority-class performance."}, "weaknesses": {"value": "1. The calculation of loss change in figure 1(b) is not explained.\n2. Although the authors evaluate CM across multiple datasets, there is limited discussion of failure cases or sensitivity to extreme imbalance ratios beyond 100:1.\n3. Some comparisons (e.g., with Overlap Optimization) are only mentioned in passing, a direct experimental comparison would strengthen claims of superiority."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a7RgfPGjct", "forum": "wSGle6ag5I", "replyto": "wSGle6ag5I", "signatures": ["ICLR.cc/2026/Conference/Submission2255/Reviewer_7n4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2255/Reviewer_7n4d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019787851, "cdate": 1762019787851, "tmdate": 1762916163379, "mdate": 1762916163379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of class imbalance in diffusion models, which leads to poor generation performance on minority classes. The authors identify model capacity allocation as a key overlooked factor, where majority classes dominate model parameters, leaving insufficient capacity for minorities. To mitigate this, they propose Capacity Manipulation (CM), a method that reserves model capacity for minority classes via low-rank decomposition of parameters and a novel capacity manipulation loss. The method is orthogonal to existing approaches and does not increase inference cost. Extensive experiments on multiple datasets demonstrate consistent improvements in minority-class generation without sacrificing majority-class performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The method is well-motivated, supported by both empirical observations (e.g., pruning sensitivity) and theoretical analysis (Theorems 2.1 & 3.1). The experimental setup is rigorous, covering multiple datasets, architectures, and metrics.\n\n(2) The paper offers an orthogonal viewpoint on class imbalance in diffusion models by focusing on model capacity allocation, diverging from prior works that primarily emphasize loss reweighting or knowledge transfer (e.g., CBDM and OC). The integration of low-rank decomposition with a tailored loss function represents a creative combination of ideas from parameter-efficient fine-tuning and imbalanced learning for targeted capacity reservation."}, "weaknesses": {"value": "(1) The term \"capacity\" is not clearly defined. Is it the number of parameters, the magnitude (e.g., L1-norm) of the weights or something else? The pruning experiment suggests a link to weight magnitude, but this connection is not explicitly made or theoretically grounded. Therefore, \" capacity\" remains a somewhat vague concept.\n\n(2) The capacity manipulation loss is designed to force minority-specific knowledge into the low-rank adapter. A potential risk is that this adapter becomes too specialized, failing to leverage the shared, general features learned by the main model. This could limit its ability to generate diverse minority samples that still rely on common underlying features (e.g., a \"rare breed of dog\" should still benefit from general \"dog\" features). The paper does not discuss or analyze this potential limitation."}, "questions": {"value": "(1) The method proposed in the paper primarily focuses on the context of known classes. A natural follow-up question is how Capacity Manipulation would perform in scenarios involving more compositional and fine-grained concepts. For example, in a dataset imbalanced towards \"photos of cats\" vs. \"paintings of dogs,\" how would the model reserve capacity for the minority concept of \"painting\" style, which is orthogonal to the object \"dog\"? Does this framework extend to reserving capacity for concepts rather than just classes? \n\n(2) The method's architecture—using a LoRA-like adapter—makes a strong, implicit assumption: that \"minority expertise\" is inherently low-rank. What is the theoretical or empirical justification for this? One could easily argue the opposite: minority classes might be more complex and have a higher intrinsic dimensionality (e.g., \"impressionist painting\" vs. \"female face\") but are simply under-sampled. If the minority knowledge is, in fact, high-rank, then the fixed low-rank of the adapter would become the primary performance bottleneck, ironically limiting the minority class's capacity more than a standard full-rank model. How does CM cope with this potential issue? \n\n(3) The current formulation appears to use a single $\\theta^e$ to capture the expertise for all minority classes collectively. On datasets with highly heterogeneous minority classes (e.g., the \"Few\" split in Imb. CIFAR-100 or ImageNet-LT, which can contain wildly different concepts), is it plausible that a single low-rank subspace can effectively represent this diverse and multimodal knowledge? Does this not create a new \"capacity collapse\" problem within the minority adapter itself? Have the authors considered a more flexible architecture, such as a Mixture-of-Experts (MoE) model for $\\theta^e$, where different \"experts\" (adapters) are dynamically allocated to different minority clusters?\n\n(4) The paper should discuss and cite relevant literature on reweighting or balancing techniques for generative models [1-5].\n\nReference:\n\n[1] Xie et al. Doremi: Optimizing data mixtures speeds up language model pretraining. NeurIPS, 2023.\n\n[2] Fan et al. DoGE: Domain Reweighting with Generalization Estimation. ICML, 2024.\n\n[3] Kim et al. Training unbiased diffusion models from biased datase. ICLR, 2024.\n\n[4] Li et al. Pruning then Reweighting: Towards Data-Efficient Training of Diffusion Models. ICASSP, 2025.\n\n[5] Liu et al. RegMix: Data Mixture as Regression for Language Model Pre-training. ICLR, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y03pIN2wNq", "forum": "wSGle6ag5I", "replyto": "wSGle6ag5I", "signatures": ["ICLR.cc/2026/Conference/Submission2255/Reviewer_NJJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2255/Reviewer_NJJB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762470955807, "cdate": 1762470955807, "tmdate": 1762916163200, "mdate": 1762916163200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their time, constructive feedback, and encouraging assessment of our work. We are gratified that the reviewers reached a consensus on the novelty and effectiveness of our approach. Specifically, we appreciate that the reviewers recognized:\n\n- **Novel & Orthogonal Perspective:** Reviewers praised our **identification of \"model capacity allocation\"** as a key factor in class imbalance, noting it as a \"**fresh perspective**\" (JwQN) and an \"**original lens**\" (pAbs) that is \"**orthogonal**\" to existing loss-reweighting approaches (NJJB).\n- **Strong Theoretical Foundation:** Reviewers highlighted that our method is \"**well-motivated**\" (NJJB) and supported by \"**solid intuition**\" (7n4d) and theoretical analysis (Theorems 2.1 & 3.1) that substantiates the **core thesis** of majority gradient dominance (JwQN).\n- **Effectiveness & Efficiency:** Reviewers commended the method for being \"**conceptually simple yet effective**\" (7n4d), yielding \"**comprehensive**\" experimental validation (JwQN) with \"**strong gains**\" on tail classes (pAbs), all while introducing \"**no additional inference overhead**\" (NJJB, pAbs).\n\nIn response to the valuable suggestions, we have extensively revised the paper (changes highlighted in **teal**). A summary of the key updates follows:\n\n**1. New Baselines and Comparative Experiments**\n\n- **Comparison with MoE-style Baseline:** To distinguish our method from parameter-efficient fine-tuning or routing techniques, we implemented a \"Group-Expert LoRA\" baseline. Results confirm that our explicit capacity reservation significantly outperforms simple expert assignment (pAbs).\n- **Comparison with Overlap Optimization:** We added a direct comparison with the concurrent work *Overlap Optimization* (Yan et al., 2024), demonstrating that CM achieves superior FID and Recall (7n4d).\n\n**2. Extension to Broader Tasks**\n\n- **Discriminative Tasks:** We extended CM to long-tailed recognition (CIFAR-100 with ResNet-18). Experiments show that CM consistently improves strong baselines (LDAM, Logit Adjustment), validating the generalizability of the \"capacity reservation\" principle beyond generative models (JwQN).\n- **Toy Experiment:** We added a 2D toy experiment visualizing gradient directions, empirically confirming Theorem 2.1 that majority classes dominate optimization trajectories (JwQN).\n\n**3. Clarifications and Visualizations**\n\n- **Definition of Capacity:** We added a formal definition of \"Model Capacity\" in Sec. E.1, operationalizing it via both structural rank and optimization/gradient dominance (NJJB).\n- **Visual Ablation:** We added visualizations (Fig. G.4) comparing samples from the general branch ($\\theta^g$) vs. the full model, empirically proving that generic features are stored in $\\theta^g$ while minority-specific traits are preserved in $\\theta^e$ (JwQN, NJJB).\n- **Metric Definition:** We explicitly defined the \"Relative Loss Change\" metric used in Fig. 1(b) (7n4d).\n\n**4. Discussion and Literature**\n\n- **Conceptual Comparison:** We added Table E.1 to clearly distinguish CM from reweighting, adapters, and MoE approaches (pAbs).\n- **Limitations:** We expanded the discussion on limitations, specifically regarding absolute sample scarcity (few-shot to zero-shot transition) (7n4d).\n- **Literature:** We incorporated suggested citations regarding reweighting and data mixture techniques (NJJB).\n\nWe believe these revisions significantly strengthen the paper and address the reviewers' questions. We have responded to each reviewer individually below.\n\nBest regards,\n\nAuthors of Submission 2255"}}, "id": "zWo7lys9EI", "forum": "wSGle6ag5I", "replyto": "wSGle6ag5I", "signatures": ["ICLR.cc/2026/Conference/Submission2255/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2255/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission2255/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763741989294, "cdate": 1763741989294, "tmdate": 1763741989294, "mdate": 1763741989294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}