{"id": "UubLs3lUfk", "number": 10426, "cdate": 1758170993119, "mdate": 1759897651190, "content": {"title": "Mitigating Conversational Inertia in Multi-Turn Agents through Context Bias Calibration", "abstract": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multi-turn agent scenarios where excessive mimicry of previous interactions undermines agentic exploration. We identify the root cause as conversational inertia—a phenomenon where models exhibit strong diagonal attention to previous assistant responses, creating imitation bias that constrains exploration. This phenomenon manifests prominently at moderate context lengths (e.g., 4K tokens) and worsens with longer conversations, explaining why agent performance degrades well before reaching the model's context limits. Through attention analysis, we find that models increasingly focus on previous responses while attention to task instructions shows marginal change, disrupting the exploration-exploitation balance for agents. We propose Context Bias Calibration as a unified framework to mitigate inertia. Our approach operates through two complementary mechanisms: clip context periodically clears interaction history, and Context Preference Learning that calibrates model preferences to favor responses generated with shorter contexts over those from longer contexts, using their own outputs without environment rewards. Experimental results across eight diverse environments demonstrate that Context Bias Calibration Framework reduces conversational inertia and achieves performance improvements.", "tldr": "We identify conversational inertia as a limitation in multi-turn agents and introduce Context Bias Calibration framework to mitigate imitation bias and enhance agentic performance.", "keywords": ["LLM Agent; few-shot learning; Long-context; Multi-turn; reward-free"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/beeb00ff851fe7c70711d0bf4cf2ba6c3752fbbd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the performance degradation of multi-turn AI agents in long conversations , attributing the issue to a newly identified phenomenon called conversational inertia. This inertia stems from a strong diagonal attention pattern where the model excessively imitates its own previous responses, thereby suppressing exploration and accumulating errors. To solve this, the paper proposes a Context Bias Calibration framework featuring two complementary methods: a training-free clip context approach that periodically clears conversation history to break the inertial chain , and Context Preference Learning, which uses DPO to teach the model to prefer actions generated from shorter, less-inert contexts without needing environmental rewards. Experiments demonstrate that this framework effectively improves task success rates across eight diverse environments by fundamentally reducing the problematic diagonal attention and mitigating inertia."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This study investigates the performance degradation of multi-turn AI agents as conversations lengthen, attributing the root cause to a newly identified phenomenon called conversational inertia. \n\n2. The training-free clip context method is simple, practical, and computationally efficient, notably preserving KV cache compatibility, which is a major advantage over sliding window approaches."}, "weaknesses": {"value": "1. How can we be certain that reducing \"diagonal attention\" is the cause of the performance improvement, rather than merely a symptom of a better underlying strategy?\n\n2. The paper treats \"conversational inertia\" as a negative bias. However, in certain tasks (e.g., following a multi-step, fixed procedure), imitating the format or behavioral pattern of the previous turn might be beneficial. Is there a risk that the proposed methods might incorrectly \"calibrate away\" useful behaviors in tasks where consistency is key? \n\n3. The paper compares Clip Context with Full Context and Window Context. A stronger baseline might be context summarization techniques, where older conversation turns are compressed into a summary by an LLM."}, "questions": {"value": "1. The paper defaults to H=12, L=1. How sensitive is the performance to these values? Is there a principled way to determine them, or is it purely empirical tuning? \n\n2. Could there be scenarios where a shorter context lacks critical historical information, leading to a decision that appears exploratory but is actually naive? How does the method balance avoiding inertia against leveraging long-term history for well-informed decision-making?\n\n3. Context Preference Learning fine-tunes the model to prefer \"short-context thinking.\" Does this fine-tuning affect the model's capabilities on other, non-agentic tasks (e.g., general Q&A, text generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xqAEfHCr9d", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_1cYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_1cYG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602897668, "cdate": 1760602897668, "tmdate": 1762921733895, "mdate": 1762921733895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a phenomenon in LLM called conversational inertia, in which the LLM will over-imitate the previous action and therefore limit its next-round exploration. By looking into the attention value, the author notices that this issue is caused by diagonal attention, that is, the LLM will pay more attention to the token that has the same relative position in the last conversation round. To tackle this issue, the author proposes context clipping and context preference learning to help the LLM focus more on the recent context. Their experiments covered a wide range of tasks, showing that context clipping and context preference tuning can solve conversational inertia effectively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The author provides a meaningful explanation for why the agent performs badly under a long context history, which is also a straightforward explanation for some of our daily tasks, e.g., you may find that ChatGPT gets stuck in some wrong directions in a multi-turn conversation and cannot always provide the expected answer to you, and you may start a new conversation and found that ChatGPT suddeny becomes smarter.\n- The experiments show that the proposed method significantly outperforms the original long context LLMs."}, "weaknesses": {"value": "- The proposed method is somehow naive and lacks novelty because people often deal with long context issues by summarizing the previous conversation rounds, but do not directly prune and discard them. But in this paper, the author does not involve such experiments. Also, the agent may be stuck in a loop of repeating early actions due to the lack of early-round memory. For some long-term tasks like deep research, such clipping will fatally affect the model's performance.\n- On the other hand, similar designs of context management already exist in multi-agent systems, where they use an orchestrator to manage multiple agents, and each agent has their own context for subtask solving `[1, 2, 3]`. If the author thinks deeply about these multi-agent works, they may find that the context management in multi-agent works is quite similar to their proposed method (and even better):\n    -  An orchestrator is designed to manage multiple subagents and solve a user task in multiple rounds\n    - Each subagent has its own context.\n    - Subagents will not keep memory from the previous term, which is identical to context clipping.\n- The DPO loss design in this work has a very strong assumption, which is that the action generated from long-term history is worse than that generated from short-term history. I believe such a strong assumption only works on tasks that can be decomposed into multiple short-term subtasks, and the required steps for each subtask are less than or equal to the clipped threshold. I hope the author provides some ablation study in this part.\n\nRefs:\n\n[1] Chen, Weize, et al. \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[2] Song, Linxin, et al. \"Adaptive in-conversation team building for language model agents.\" arXiv preprint arXiv:2405.19425 (2024).\n\n[3] Zhuge, Mingchen, et al. \"Language agents as optimizable graphs.\" arXiv preprint arXiv:2402.16823 (2024)."}, "questions": {"value": "- Somehow, I think the author has a very good discovery on why agents fail in multiple rounds, but their approach/solution becomes a regret of this paper. I would suggest the author focus more on how to solve the overwhelming instruction following issue based on what they've discovered (e.g., the diagonal attention) and try to keep a consistent agent memory when modifying the context.\n- Table 1 does not look complete. Why are there no \"Long\" baselines and only \"Window6\" and \"Clip12\" for Llama-3-8B-instruct, gpt-4o-mini, and Qwen3-8B-RFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6D1YkTw59", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683991643, "cdate": 1761683991643, "tmdate": 1762921733460, "mdate": 1762921733460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies conversational inertia as a key failure mode in multi-turn LLM agents: with growing histories, models exhibit strong diagonal attention to prior assistant tokens, imitating their own earlier outputs and degrading performance even at moderate context lengths. The authors propose a Context Bias Calibration framework with two components. (1) Clip Context periodically clears interaction history, which both weakens the diagonal alignment and enables effective KV-cache reuse within a cycle. (2) Context Preference Learning performs DPO fine-tuning on long-short context action pairs, using the short context as input for both options to teach a preference for lower-inertia actions without environment rewards. Experimental results demonstrate that Context Bias Calibration framework reduces conversational inertia and achieves performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper operationalizes “conversational inertia” via role-wise attention ratios and a diagonal-attention metric that reveal self-alignment to past assistant tokens, linking a visible attention pattern to a concrete failure mode and to targeted interventions.\n\n2. The paper tests multiple base models across diverse environments, and pairs headline results with careful analyses, including role-wise/diagonal attention metrics, long-horizon scaling, “bad initialization” probes, and compute profiling.\n\n3. Clip Context is training-free and Context Preference Learning is lightweight, so the method is easy to bolt onto existing agents while also delivering latency gains via better KV-cache reuse."}, "weaknesses": {"value": "1. The proposed clipping scheme may underperform in environments that require uninterrupted short-term memory across turns. In the extreme, if the task is “sum of the last six observed numbers,” a Window-6 agent always retains the necessary evidence, whereas Clip-12 with L=1 periodically resets and cannot compute the target after resets because recent evidence is missing. \n\n2. Based on the previous point, the paper needs to provide principled rules or diagnostics for selecting the clipping horizon based on task or environment. Otherwise, practitioners lack a reliable procedure to configure Clip for different environments.\n\n3. Compared with the strong empirical and mechanistic support for Clip, the Context Preference Learning component offers thinner theoretical grounding and more limited ablations/comparisons (on only one model and one setting of context windows). The incremental gains are less thoroughly isolated from confounds, leaving its standalone contribution and generalization less clear."}, "questions": {"value": "1. How should H and L be chosen? Are the strong results contingent on environments that are effectively restartable (i.e., the agent can clear all previous memory and start at any state)? I'm wondering what will happen if L increases.\n\n2. Why is “same average input to agents” a fair basis of comparison? It does not align with compute complexity, context limits, or information volume, and inertia implies that “more rounds” can even be harmful. For example. could you justify this choice and additionally compare Clip-12 not only to Window-6 but also to bounds that bracket its effective context (e.g., Window-1 and Window-11)?\n\n3. When is clipping appropriate across broader multi-turn tasks? Here are two examples. (a) For conversational agents with non-terminal feedback/rewards, periodic memory clearing may harm user experience (preference retention, discourse coherence). Is clipping still advisable? (b) In single-state tasks with ongoing feedback (e.g., coding/reasoning where the goal is fixed but observations change), does clipping effectively “reset and retry”? More generally, could you provide a taxonomy for when to use Clip vs Window vs full memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sf8S94UfId", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_dcR6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_dcR6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025288808, "cdate": 1762025288808, "tmdate": 1762921733111, "mdate": 1762921733111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why multi-turn dialogue agents degrade in performance at moderate context lengths (~ 4K tokens) despite modern LLMs' capacity for much longer sequences. The paper identifies \"conversational inertia\" where models exhibit strong diagonal attention patterns to previous assistant responses, essentially mimicking past actions rather than adapting to new environmental feedback. They proposed Context Bias Calibration framework addresses this through two mechanisms. 1st, training-free clip context method periodically truncates conversation history every H rounds to L recent rounds, breaking error propagation loops while enabling KV cache optimization for computational efficiency. 2nd, Context Preference Learning uses DPO to fine-tune few #params training models to favor actions generated from shorter contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Mechanistic evidence for conversation inertia is interesting and motivating where there is a clear degradation at moderate context lengths.\n\n- Comprehensive experiments: Ablation experiments on the hyperparams used, different environments and a few models.\n\n- DPO method doesnt require ground truth making the method more generalisable."}, "weaknesses": {"value": "- While the conversation inertia is well motivated, I am struggling to see enough compelling evidence empirically. For example, the paper assumes diagonal attention causes poor performance, but one could observe similar pattern when there in an increase in the performance in the initial iteration. \n\n- The solution probably works for short-term reasoning problems, where the agent \"forgets\" the history periodically. For longer tasks, the agent must actually \"learn\" from the experiences.\n\n- I found some of the comparisions a bit unfair -Window-6 (always sees recent 6 rounds) vs Clip-12 (sees 1→12 rounds cycling).\n\n- The paper claims that they introduced the concept of conversation inertia but I found many relevant papers that have not been cited:\n\n[1] Hankache, Robert, et al. \"Evaluating the Sensitivity of LLMs to Prior Context.\" arXiv preprint arXiv:2506.00069 (2025).\n\n[2] Gupta, Akash, et al. \"Llm task interference: An initial study on the impact of task-switch in conversational history.\" EMNLP (2024)\n\n[3] Castillo-Bolado, David, et al. \"Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models.\" Neurips D&B (2024)."}, "questions": {"value": "- A potential baseline: what if you ask LLM to summarise the past conversation?\n\n- How would this method and hypothesis hold up in multi-step reasoning tasks?\n\n- I can imagine these methods to have some variance, can you please report those.\n\n- How does window-6 compare against clip-6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PqLefBmZd5", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_Pp5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_Pp5L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106051372, "cdate": 1762106051372, "tmdate": 1762921732299, "mdate": 1762921732299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}