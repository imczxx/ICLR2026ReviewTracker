{"id": "UubLs3lUfk", "number": 10426, "cdate": 1758170993119, "mdate": 1763652807904, "content": {"title": "Mitigating Conversational Inertia in Multi-Turn Agents through Context Bias Calibration", "abstract": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multi-turn agent scenarios where excessive mimicry of previous interactions undermines agentic exploration. We identify the root cause as conversational inertia—a phenomenon where models exhibit strong diagonal attention to previous assistant responses, creating imitation bias that constrains exploration. This phenomenon manifests prominently at moderate context lengths (e.g., 4K tokens) and worsens with longer conversations, explaining why agent performance degrades well before reaching the model's context limits. Through attention analysis, we find that models increasingly focus on previous responses while attention to task instructions shows marginal change, disrupting the exploration-exploitation balance for agents. We propose Context Bias Calibration as a unified framework to mitigate inertia. Our approach operates through two complementary mechanisms: clip context periodically clears interaction history, and Context Preference Learning that calibrates model preferences to favor responses generated with shorter contexts over those from longer contexts, using their own outputs without environment rewards. Experimental results across eight diverse environments demonstrate that Context Bias Calibration Framework reduces conversational inertia and achieves performance improvements.", "tldr": "We identify conversational inertia as a limitation in multi-turn agents and introduce Context Bias Calibration framework to mitigate imitation bias and enhance agentic performance.", "keywords": ["LLM Agent; few-shot learning; Long-context; Multi-turn; reward-free"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f29f59cbdc4f90dbb6b5e235a5a7de7bec56f89e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the performance degradation of multi-turn AI agents in long conversations , attributing the issue to a newly identified phenomenon called conversational inertia. This inertia stems from a strong diagonal attention pattern where the model excessively imitates its own previous responses, thereby suppressing exploration and accumulating errors. To solve this, the paper proposes a Context Bias Calibration framework featuring two complementary methods: a training-free clip context approach that periodically clears conversation history to break the inertial chain , and Context Preference Learning, which uses DPO to teach the model to prefer actions generated from shorter, less-inert contexts without needing environmental rewards. Experiments demonstrate that this framework effectively improves task success rates across eight diverse environments by fundamentally reducing the problematic diagonal attention and mitigating inertia."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This study investigates the performance degradation of multi-turn AI agents as conversations lengthen, attributing the root cause to a newly identified phenomenon called conversational inertia. \n\n2. The training-free clip context method is simple, practical, and computationally efficient, notably preserving KV cache compatibility, which is a major advantage over sliding window approaches."}, "weaknesses": {"value": "1. How can we be certain that reducing \"diagonal attention\" is the cause of the performance improvement, rather than merely a symptom of a better underlying strategy?\n\n2. The paper treats \"conversational inertia\" as a negative bias. However, in certain tasks (e.g., following a multi-step, fixed procedure), imitating the format or behavioral pattern of the previous turn might be beneficial. Is there a risk that the proposed methods might incorrectly \"calibrate away\" useful behaviors in tasks where consistency is key? \n\n3. The paper compares Clip Context with Full Context and Window Context. A stronger baseline might be context summarization techniques, where older conversation turns are compressed into a summary by an LLM."}, "questions": {"value": "1. The paper defaults to H=12, L=1. How sensitive is the performance to these values? Is there a principled way to determine them, or is it purely empirical tuning? \n\n2. Could there be scenarios where a shorter context lacks critical historical information, leading to a decision that appears exploratory but is actually naive? How does the method balance avoiding inertia against leveraging long-term history for well-informed decision-making?\n\n3. Context Preference Learning fine-tunes the model to prefer \"short-context thinking.\" Does this fine-tuning affect the model's capabilities on other, non-agentic tasks (e.g., general Q&A, text generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xqAEfHCr9d", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_1cYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_1cYG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602897668, "cdate": 1760602897668, "tmdate": 1762921733895, "mdate": 1762921733895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a phenomenon in LLM called conversational inertia, in which the LLM will over-imitate the previous action and therefore limit its next-round exploration. By looking into the attention value, the author notices that this issue is caused by diagonal attention, that is, the LLM will pay more attention to the token that has the same relative position in the last conversation round. To tackle this issue, the author proposes context clipping and context preference learning to help the LLM focus more on the recent context. Their experiments covered a wide range of tasks, showing that context clipping and context preference tuning can solve conversational inertia effectively."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The author provides a meaningful explanation for why the agent performs badly under a long context history, which is also a straightforward explanation for some of our daily tasks, e.g., you may find that ChatGPT gets stuck in some wrong directions in a multi-turn conversation and cannot always provide the expected answer to you, and you may start a new conversation and found that ChatGPT suddeny becomes smarter.\n- The experiments show that the proposed method significantly outperforms the original long context LLMs."}, "weaknesses": {"value": "- The proposed method is somehow naive and lacks novelty because people often deal with long context issues by summarizing the previous conversation rounds, but do not directly prune and discard them. But in this paper, the author does not involve such experiments. Also, the agent may be stuck in a loop of repeating early actions due to the lack of early-round memory. For some long-term tasks like deep research, such clipping will fatally affect the model's performance.\n- On the other hand, similar designs of context management already exist in multi-agent systems, where they use an orchestrator to manage multiple agents, and each agent has their own context for subtask solving `[1, 2, 3]`. If the author thinks deeply about these multi-agent works, they may find that the context management in multi-agent works is quite similar to their proposed method (and even better):\n    -  An orchestrator is designed to manage multiple subagents and solve a user task in multiple rounds\n    - Each subagent has its own context.\n    - Subagents will not keep memory from the previous term, which is identical to context clipping.\n- The DPO loss design in this work has a very strong assumption, which is that the action generated from long-term history is worse than that generated from short-term history. I believe such a strong assumption only works on tasks that can be decomposed into multiple short-term subtasks, and the required steps for each subtask are less than or equal to the clipped threshold. I hope the author provides some ablation study in this part.\n\nRefs:\n\n[1] Chen, Weize, et al. \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[2] Song, Linxin, et al. \"Adaptive in-conversation team building for language model agents.\" arXiv preprint arXiv:2405.19425 (2024).\n\n[3] Zhuge, Mingchen, et al. \"Language agents as optimizable graphs.\" arXiv preprint arXiv:2402.16823 (2024)."}, "questions": {"value": "- Somehow, I think the author has a very good discovery on why agents fail in multiple rounds, but their approach/solution becomes a regret of this paper. I would suggest the author focus more on how to solve the overwhelming instruction following issue based on what they've discovered (e.g., the diagonal attention) and try to keep a consistent agent memory when modifying the context.\n- Table 1 does not look complete. Why are there no \"Long\" baselines and only \"Window6\" and \"Clip12\" for Llama-3-8B-instruct, gpt-4o-mini, and Qwen3-8B-RFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6D1YkTw59", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683991643, "cdate": 1761683991643, "tmdate": 1762921733460, "mdate": 1762921733460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 1/3)"}, "comment": {"value": "We sincerely thank all reviewers for their thorough reviews and constructive feedback. We are encouraged that the reviewers recognize our innovations in context management through the Context Bias Calibration Framework. We have carefully addressed all concerns raised and conducted additional experiments to strengthen our findings. In the following, we provide detailed responses to the key questions.\n\n## General Response 1: Does clipping historical information lead to lower performance compared to baseline methods like summarization?\n\nWe appreciate this question about whether clipping historical context might degrade performance. To address this concern, we provide three key clarifications:\n\n1. **Our method retains moderate historical information rather than clipping all of it.** The purpose of clipping is not primarily to forget information, but to reduce inertia. In our paper, we did not clip all history completely. Different environments may benefit from retaining different amounts of history. For the 8 evaluated task environments involving state transitions (navigation), we keep 1 context turn (Clip 12to1). For research scenarios discussed in General Response 2, we retain 6 context turns (Clip 12to6).  \"Clip 12to1\" means that we drop the 11 oldest history turns and keep 1 latest history turn when context rounds accumulate to 12. The corresponding hyperparameter is H=12, L=1.\n\n2. **LLM-based summarization is comparable to our clipping method.** We conducted comprehensive comparisons with summarization methods, which are widely used in existing frameworks. We adopted the ReSum[1] prompt for summarization with a setting of Sum 12to1. \"Sum 12to1\" means based on Clip 12to1, when clipping, we add a summary of the current history. The summary buffer is updated when clipped again. We use Qwen3-8B as both actor and summarizer. More summary details can be found in Appendix I in the revision. Results averaged across 8 environments are shown in Rebuttal Table 1 (R-Table 1): \n\nR-Table 1: Average performance across 8 environments\n\n| Method | 8 env avg score |\n|---|---|\n| Long context | 54.5 |\n| Window 6 | 64.9 |\n| Sum 12to1 | 68.8 |\n| Clip 12to1 | 68.9 |\n\nThe results show that Clip 12to1 achieves comparable performance to summarization while being simpler to implement. Clip does not need a summarization prompt, which can be sensitive to environments.\nFor comparisons between Clip and summarization in multi-step long-term reasoning environments, please see General Response 2.\n\n3. **Clipping extends the principle that longer context is not always better.** While extending context can provide more information, it also introduces inertia that prevents agents from exploring new strategies. Our experiments (Table 1) demonstrate that Window significantly outperforms Long context by dropping history. Our Clip method drops larger chunks of history at once, which more effectively reduces this inertia compared to simple Window-based approaches, thus better leveraging the benefits of reducing inertia through history dropping.\n\n[1] Wu, X., Li, K., Zhao, Y., Zhang, L., Ou, L., Yin, H., Zhang, Z., Yu, X., Zhang, D., Jiang, Y., Xie, P., Huang, F., Cheng, M., Wang, S., Cheng, H., & Zhou, J. (2025). ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization. arXiv:2509.13313."}}, "id": "g0NB6MXb2p", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763649762556, "cdate": 1763649762556, "tmdate": 1763651397584, "mdate": 1763651397584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a phenomenon in LLM called conversational inertia, in which the LLM will over-imitate the previous action and therefore limit its next-round exploration. By looking into the attention value, the author notices that this issue is caused by diagonal attention, that is, the LLM will pay more attention to the token that has the same relative position in the last conversation round. To tackle this issue, the author proposes context clipping and context preference learning to help the LLM focus more on the recent context. Their experiments covered a wide range of tasks, showing that context clipping and context preference tuning can solve conversational inertia effectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The author provides a meaningful explanation for why the agent performs badly under a long context history, which is also a straightforward explanation for some of our daily tasks, e.g., you may find that ChatGPT gets stuck in some wrong directions in a multi-turn conversation and cannot always provide the expected answer to you, and you may start a new conversation and found that ChatGPT suddeny becomes smarter.\n- The experiments show that the proposed method significantly outperforms the original long context LLMs."}, "weaknesses": {"value": "- The proposed method is somehow naive and lacks novelty because people often deal with long context issues by summarizing the previous conversation rounds, but do not directly prune and discard them. But in this paper, the author does not involve such experiments. Also, the agent may be stuck in a loop of repeating early actions due to the lack of early-round memory. For some long-term tasks like deep research, such clipping will fatally affect the model's performance.\n- On the other hand, similar designs of context management already exist in multi-agent systems, where they use an orchestrator to manage multiple agents, and each agent has their own context for subtask solving `[1, 2, 3]`. If the author thinks deeply about these multi-agent works, they may find that the context management in multi-agent works is quite similar to their proposed method (and even better):\n    -  An orchestrator is designed to manage multiple subagents and solve a user task in multiple rounds\n    - Each subagent has its own context.\n    - Subagents will not keep memory from the previous term, which is identical to context clipping.\n- The DPO loss design in this work has a very strong assumption, which is that the action generated from long-term history is worse than that generated from short-term history. I believe such a strong assumption only works on tasks that can be decomposed into multiple short-term subtasks, and the required steps for each subtask are less than or equal to the clipped threshold. I hope the author provides some ablation study in this part.\n\nRefs:\n\n[1] Chen, Weize, et al. \"Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[2] Song, Linxin, et al. \"Adaptive in-conversation team building for language model agents.\" arXiv preprint arXiv:2405.19425 (2024).\n\n[3] Zhuge, Mingchen, et al. \"Language agents as optimizable graphs.\" arXiv preprint arXiv:2402.16823 (2024)."}, "questions": {"value": "- Somehow, I think the author has a very good discovery on why agents fail in multiple rounds, but their approach/solution becomes a regret of this paper. I would suggest the author focus more on how to solve the overwhelming instruction following issue based on what they've discovered (e.g., the diagonal attention) and try to keep a consistent agent memory when modifying the context.\n- Table 1 does not look complete. Why are there no \"Long\" baselines and only \"Window6\" and \"Clip12\" for Llama-3-8B-instruct, gpt-4o-mini, and Qwen3-8B-RFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6D1YkTw59", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_3MMN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683991643, "cdate": 1761683991643, "tmdate": 1763680788943, "mdate": 1763680788943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies conversational inertia as a key failure mode in multi-turn LLM agents: with growing histories, models exhibit strong diagonal attention to prior assistant tokens, imitating their own earlier outputs and degrading performance even at moderate context lengths. The authors propose a Context Bias Calibration framework with two components. (1) Clip Context periodically clears interaction history, which both weakens the diagonal alignment and enables effective KV-cache reuse within a cycle. (2) Context Preference Learning performs DPO fine-tuning on long-short context action pairs, using the short context as input for both options to teach a preference for lower-inertia actions without environment rewards. Experimental results demonstrate that Context Bias Calibration framework reduces conversational inertia and achieves performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper operationalizes “conversational inertia” via role-wise attention ratios and a diagonal-attention metric that reveal self-alignment to past assistant tokens, linking a visible attention pattern to a concrete failure mode and to targeted interventions.\n\n2. The paper tests multiple base models across diverse environments, and pairs headline results with careful analyses, including role-wise/diagonal attention metrics, long-horizon scaling, “bad initialization” probes, and compute profiling.\n\n3. Clip Context is training-free and Context Preference Learning is lightweight, so the method is easy to bolt onto existing agents while also delivering latency gains via better KV-cache reuse."}, "weaknesses": {"value": "1. The proposed clipping scheme may underperform in environments that require uninterrupted short-term memory across turns. In the extreme, if the task is “sum of the last six observed numbers,” a Window-6 agent always retains the necessary evidence, whereas Clip-12 with L=1 periodically resets and cannot compute the target after resets because recent evidence is missing. \n\n2. Based on the previous point, the paper needs to provide principled rules or diagnostics for selecting the clipping horizon based on task or environment. Otherwise, practitioners lack a reliable procedure to configure Clip for different environments.\n\n3. Compared with the strong empirical and mechanistic support for Clip, the Context Preference Learning component offers thinner theoretical grounding and more limited ablations/comparisons (on only one model and one setting of context windows). The incremental gains are less thoroughly isolated from confounds, leaving its standalone contribution and generalization less clear."}, "questions": {"value": "1. How should H and L be chosen? Are the strong results contingent on environments that are effectively restartable (i.e., the agent can clear all previous memory and start at any state)? I'm wondering what will happen if L increases.\n\n2. Why is “same average input to agents” a fair basis of comparison? It does not align with compute complexity, context limits, or information volume, and inertia implies that “more rounds” can even be harmful. For example. could you justify this choice and additionally compare Clip-12 not only to Window-6 but also to bounds that bracket its effective context (e.g., Window-1 and Window-11)?\n\n3. When is clipping appropriate across broader multi-turn tasks? Here are two examples. (a) For conversational agents with non-terminal feedback/rewards, periodic memory clearing may harm user experience (preference retention, discourse coherence). Is clipping still advisable? (b) In single-state tasks with ongoing feedback (e.g., coding/reasoning where the goal is fixed but observations change), does clipping effectively “reset and retry”? More generally, could you provide a taxonomy for when to use Clip vs Window vs full memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sf8S94UfId", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_dcR6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_dcR6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025288808, "cdate": 1762025288808, "tmdate": 1762921733111, "mdate": 1762921733111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 2/3)"}, "comment": {"value": "## General Response 2: Clip Context vs Summary Method on Multi-Step Long-Term Reasoning Scenarios (e.g., deep research)\n\nWe conducted experiments on multi-step reasoning scenarios to compare Clip and summary methods based on BrowseComp [2]. Proactive answer means the agent answers before the maximum step is reached, and forced answer means the agent answers only when the maximum step is reached. More evaluation details are provided in Appendix J of the paper. Results are shown in R-Table 2:\n\nR-Table 2: Deep research scenario\n\n| Method | Score (+- SEM) | Proactive answer rate | Proactive answer accuracy | Forced answer accuracy |\n|---|---|---|---|---|\n| Window 6 | 25.0 +- 1.7 | 27.3% | 55.7% | 13.4% |\n| Window 9 | 23.4 +- 1.5 | 26.6% | 58.8% | 10.6% |\n| Window 12 | 24.2 +- 1.8 | 25.4% | 58.5% | 12.5% |\n| Clip 12to0 | 25.0 +- 1.8 | 29.3% | 56.0% | 12.2% |\n| Sum 12to0 (ReSum) | 27.7 +- 2.0 | 63.7% | 37.4% | 10.6% |\n| Clip 12to6 | 29.3 +- 1.5 | 30.9% | 61.0% | 15.1% |\n| Sum 12to6 | 28.1 +- 1.6 | 30.8% | 63.3% | 13.2% |\n\nThese results further demonstrate the effectiveness of the Clip method:\n\n- **Sum 12to0 (ReSum) performs worse than Clip 12to6:** Model-generated summaries provide less accurate information than directly using the 6 most recent history turns.\n- **Window 6 performs worse than Clip 12to6:** Due to insufficient decision-making information, this degrades exploitation capability.\n- **Window 12 performs worse than Clip 12to6:** Due to excessive inertia, this degrades exploration capability.\n- **Window 9 performs worse than Clip 12to6:** Even though they have the same average input length, by perfectly balancing exploration and exploitation, Clip 12to6 achieves the best results.\n- **Sum 12to6 is comparable to Clip 12to6:** When historical examples are already available, adding an extra summarizer does not bring improvements.\n\nExisting summarization approaches show unstable performance, having Context Collapse and Brevity Bias problems [3]. Through our empirical study, we also found issues with summary-based methods such as over-confidence (e.g., model-generated summaries may lead the actor model to have higher proactive answer rates but lower answer accuracy). More detailed empirical analysis can be found in Appendix K in the revision. Therefore, our method does not adopt summarization. In contrast, our Clip method is simple as an applicable approach, and we recommend it as a strong baseline.\n\n\n[2] BrowseComp: A Multi-Step Web Browsing Benchmark. OpenAI. https://openai.com/index/browsecomp/\n\n[3] Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., Thakker, U., Zou, J., & Olukotun, K. (2025). Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models. arXiv:2510.04618."}}, "id": "7XPX7nRepL", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763649817537, "cdate": 1763649817537, "tmdate": 1763651059326, "mdate": 1763651059326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why multi-turn dialogue agents degrade in performance at moderate context lengths (~ 4K tokens) despite modern LLMs' capacity for much longer sequences. The paper identifies \"conversational inertia\" where models exhibit strong diagonal attention patterns to previous assistant responses, essentially mimicking past actions rather than adapting to new environmental feedback. They proposed Context Bias Calibration framework addresses this through two mechanisms. 1st, training-free clip context method periodically truncates conversation history every H rounds to L recent rounds, breaking error propagation loops while enabling KV cache optimization for computational efficiency. 2nd, Context Preference Learning uses DPO to fine-tune few #params training models to favor actions generated from shorter contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Mechanistic evidence for conversation inertia is interesting and motivating where there is a clear degradation at moderate context lengths.\n\n- Comprehensive experiments: Ablation experiments on the hyperparams used, different environments and a few models.\n\n- DPO method doesnt require ground truth making the method more generalisable."}, "weaknesses": {"value": "- While the conversation inertia is well motivated, I am struggling to see enough compelling evidence empirically. For example, the paper assumes diagonal attention causes poor performance, but one could observe similar pattern when there in an increase in the performance in the initial iteration. \n\n- The solution probably works for short-term reasoning problems, where the agent \"forgets\" the history periodically. For longer tasks, the agent must actually \"learn\" from the experiences.\n\n- I found some of the comparisions a bit unfair -Window-6 (always sees recent 6 rounds) vs Clip-12 (sees 1→12 rounds cycling).\n\n- The paper claims that they introduced the concept of conversation inertia but I found many relevant papers that have not been cited:\n\n[1] Hankache, Robert, et al. \"Evaluating the Sensitivity of LLMs to Prior Context.\" arXiv preprint arXiv:2506.00069 (2025).\n\n[2] Gupta, Akash, et al. \"Llm task interference: An initial study on the impact of task-switch in conversational history.\" EMNLP (2024)\n\n[3] Castillo-Bolado, David, et al. \"Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models.\" Neurips D&B (2024)."}, "questions": {"value": "- A potential baseline: what if you ask LLM to summarise the past conversation?\n\n- How would this method and hypothesis hold up in multi-step reasoning tasks?\n\n- I can imagine these methods to have some variance, can you please report those.\n\n- How does window-6 compare against clip-6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PqLefBmZd5", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Reviewer_Pp5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Reviewer_Pp5L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106051372, "cdate": 1762106051372, "tmdate": 1762921732299, "mdate": 1762921732299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 3/3)"}, "comment": {"value": "## General Response 3: A Fair Comparison Between Clip and Window\n\nWe note that Window is a special case of Clip (e.g., Window 6 equals to Clip 7to6). To provide a comprehensive comparison, we conducted extensive ablation studies on Clip parameters (R-Table 4). \n\nR-Table 4: Clip parameter ablation\n\n|  | H=2 | H=3 | H=6 | H=7 | H=12 | H=13 |\n|---|---|---|---|---|---|---|\n| L=1 | 62.2 (W=1) | 66.4 | 68.7 | - | 68.9 | - |\n| L=6 | - | - | - | 64.9 (W=6) | 62.1 | - |\n| L=12 | - | - | - | - | - | 61.3 (W=12) |\n\nBelow we analyze Clip parameters and compare them with Window from three perspectives:\n\n1. **Effect of H parameter.** The upper limit H provides more contextual information for decision-making, making the agent more informed when proactively answering. R-Table 5 shows the effect when L=1:\n\nR-Table 5: Effect of H parameter (L=1)\n\n| L=1, H=? | L=1, H=2 | L=1, H=3 | L=1, H=6 | L=1, H=12 |\n|---|---|---|---|---|\n| 8 env avg Score | 62.2 | 66.4 | 68.7 | 68.9 |\n\n2. **Effect of L parameter.** The lower limit L controls the ability to break inertia: lower L values strengthen the ability to propose new methods and paths, making it more suitable for exploratory tasks; moderate L values retain more information, making it more suitable for summarization and search tasks. R-Table 6 shows the effect when we mainly adjust L:\n\nR-Table 6: Effect of L parameter\n\n| hyperparameter | L=1, H=12 | L=3, H=12 | L=6, H=12 | L=11, H=12 |\n|---|---|---|---|---|\n| 8 env avg Score | 68.9 | 67.1 | 62.1 | 60.7 |\n\n\n3. **Clip vs Window.** When Clip and Window method have the same average input to model, Clip (L=1) outperforms Window at various context lengths (R-Table 7):\n\nR-Table 7: Clip vs Window at different context lengths\n\n| Hyper-parameter| Clip (L=1,H=?) | Window (W=?) |\n|---|---|---|\n| L+H=7,W=3 | 68.7 | 64.0 |\n| L+H=11,W=5 | 68.8 | 64.2 |\n| L+H=13,W=6 | 68.9 | 64.9 |\n| L+H=15,W=7 | 68.9 | 63.8 |\n\nFrom the above experiments, we can analyze the advantages of Clip over Window:\n\n1. **Superior performance compared to Window context.** We fairly compared Clip with Window from multiple perspectives: R-Table 7 shows that with optimal parameters and various context lengths, Clip consistently outperforms Window methods. R-Table 5 demonstrates that with the same H (memory limit), Clip progressively outperforms Window methods as inertia is reduced. Our Clip method periodically alternates between short contexts (for exploration by reducing inertia) and long contexts (for exploitation by utilizing interaction history), achieving a dynamic balance across conversation turns. We adopted the approach of balancing average input tokens ($L=1, H=2*W$) as a comparison described in the paper.\n\n2. **Lower hyperparameter sensitivity.** Referring to R-Table 5 and 7, compared to Window, Clip is less sensitive to hyperparameters and does not introduce difficult parameter tuning. This is also because the Clip method achieves dynamic balance across conversation turns. This design makes Clip adaptable to scenarios with different requirements."}}, "id": "t4i2MhIv6p", "forum": "UubLs3lUfk", "replyto": "UubLs3lUfk", "signatures": ["ICLR.cc/2026/Conference/Submission10426/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10426/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10426/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763649966329, "cdate": 1763649966329, "tmdate": 1763649966329, "mdate": 1763649966329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}