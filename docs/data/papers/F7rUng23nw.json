{"id": "F7rUng23nw", "number": 7805, "cdate": 1758036895462, "mdate": 1763657218348, "content": {"title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing", "abstract": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.", "tldr": "We propose KVComm, a communication framework that enables efficient inter-LLM collaboration by selectively sharing key-value pairs, achieving near upper-bound performance with significantly reduced communication cost.", "keywords": ["Large Language Models", "Multi-Agent Systems", "Inter-LLM Communication", "Multi-agent Debate"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f1b3117f7dfcacf88878855e61d7a35f8ef83c8.pdf", "supplementary_material": "/attachment/ad7387bbb5fb018dbb30197b19d6cfa84a168443.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes KVComm for inter-LLM communication via selective KV-cache sharing. The specific techinique is guided by attention-based importance and Gaussian layer prior. Experiments show moderate gains over prior activation- or embedding-based protocols, yet KVComm still sometimes fall short of the full transmission baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Selective KV-cache sharing with attention-based layer choice yields measurable gains over prior hidden-state or embedding protocols.  \n- Single-sample calibration generalizes across datasets, offering a lightweight deployment recipe."}, "weaknesses": {"value": "1. I may misunderstand: the method appears restricted to two-agent pairs that share a parameter space (Table 5: same model or parent-offspring). If KVComm neither extends to >2 agents nor to heterogeneous backbones, its scope seems a bit narrow.\n\n2. The authors should clarify whether KVComm generalizes to fully-heterogeneous, multi-LLM scenarios (differing hidden dimensions & vocabularies); otherwise the contribution risks being incremental and may overlap prior work [1].\n\n3. Line 78 claims “using all tokens’ hidden states does not guarantee effective communication,” yet the same input-concatenation is later labeled “Skyline” and outperforms KVComm on Tipsheets/MuSiQuest (Table 1); this reads somehow contradictory.\n\n4. Excluding Skyline from “best result” bolding in Table 1 warrants justification; its inclusion would contextualize absolute gains.\n\n5. Fixing k % of layers manually is not adaptive; an automatic, data-driven or model-driven criterion to decide the subset size would strengthen practicality.\n\n6. suayptalha/DeepSeek-R1-Distill-Llama-3B seems to be an unofficial version; the official releases are DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Llama-70B. Using an unofficial checkpoint complicates reproducibility a bit; why not adopt the official distillations?\n\n7. The authors may want to discuss the discrepancy and their unique contribution compared with [2], which seems to share similar motivation yet stronger adaptivity compared to this work.\n\n---\n\n[1] Dense Communication between Language Models\n\n[2] KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems"}, "questions": {"value": "Please refer to Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vXrxZ0W6Zm", "forum": "F7rUng23nw", "replyto": "F7rUng23nw", "signatures": ["ICLR.cc/2026/Conference/Submission7805/Reviewer_XrXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7805/Reviewer_XrXt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100998148, "cdate": 1761100998148, "tmdate": 1762919848546, "mdate": 1762919848546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for communication between LLMs in multi-agent systems. Instead of using natural language or hidden states to exchange information, it enables models to communicate efficiently by selectively sharing KV pairs from their attention layers. The authors introduce a layer-wise KV selection strategy, identifying the most informative layers for transmission. Experiments across eight model pairs and multiple reasoning datasets show that KVComm achieves performance comparable to an upper-bound “Skyline” setup while reducing computation and communication costs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an important but underexplored question: how to design effective and efficient communication protocols between LLMs in multi-agent systems. Both the *effectiveness* and *efficiency* of inter-agent communication are crucial for scaling such systems, making this direction timely and meaningful.\n* The authors conduct several preliminary experiments to justify their design choices, such as analyzing token importance and the limitations of hidden-state communication. These analyses help motivate the proposed selective KV sharing mechanism.\n* The paper compares against a wide range of baselines and evaluates on diverse tasks and model pairs."}, "weaknesses": {"value": "* The proposed setup essentially reduces the “multi-agent” interaction to one model encoding the context and another generating the response. And since the paper uses KV-cache as medium, this feels closer to a *single-model sparse attention* or *cache reuse* scenario rather than true multi-agent communication. Moreover, if we really consider this problem under the \"multi-agent\" context, then transmitting activations or KV caches between agents incurs substantially higher communication cost (floating-point tensors) than natural language (token IDs). If minimizing \"transmission cost\" is a goal, as stated in Line 127, the paper needs stronger justification for why direct context forwarding or lightweight textual communication is not preferable.\n* Some explanations are vague or underspecified. For example, the term \"*strong* attention distribution\" seems not formal; Section 2.2.1’s experimental procedure (“remove or retain hidden states of specific tokens”) is poorly explained; and Section 2.2.2’s description of “prepending hidden states” is ambiguous, does it concatenate layer j's output from speaker model to the first position of layer k of receiver, and go through the remaining forward pass? Figures 3 also do not clearly define axes or mapping between sender and receiver layers.\n* The NLD baseline performs unexpectedly poorly across datasets. It is unclear whether this is due to an inappropriate setup or prompt mismatch. If the NLD agents still follow a debate-style dialogue rather than an information-transfer style, the comparison may not be meaningful, since the objective differs fundamentally.\n* A key challenge of activation- or KV-based communication is its poor transferability across models with different architectures or base weights. This greatly limits the generality of the proposed method. The current experiments only involve models fine-tuned from the same base, so the claim of broad applicability is not yet substantiated.\n* Several figures suffer from layout or readability problems, e.g., overlap in Figure 1 (“Extract attention”), overly tight spacing in Figures 2–3, and Figures 4–6."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fd0yFH4JQh", "forum": "F7rUng23nw", "replyto": "F7rUng23nw", "signatures": ["ICLR.cc/2026/Conference/Submission7805/Reviewer_i79Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7805/Reviewer_i79Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718363732, "cdate": 1761718363732, "tmdate": 1762919848044, "mdate": 1762919848044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **KVComm**, a novel communication protocol for multi-agent systems based on Large Language Models (LLMs). Instead of relying on natural language or hidden states, KVComm selectively shares key-value (KV) pairs across models using a strategy guided by attention importance scores and a Gaussian prior. The method significantly reduces communication and computation overhead while maintaining or surpassing task performance on diverse benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: Using KV pairs rather than natural language or hidden states is a compelling, less explored direction that avoids known bottlenecks like information loss and concentration bias.\n\nS2: The use of attention importance with a Gaussian prior to guide layer selection is well-motivated and supported by empirical validation.\n\nS3: The authors benchmark across multiple datasets and LLM pairs, including ablations and comparisons with strong baselines like Skyline and AC.\n\nS4: The approach offers substantial computation and communication savings (2.5x–6x), making it relevant for scalable deployment scenarios.\n\nS5: The paper provides detailed setup information, dataset samples, and makes its code and synthetic datasets available."}, "weaknesses": {"value": "To be honest, I carefully read this paper but I have not found some serious weakness. Below are just some suggestions that may lead to a more solid work.\n\n\n\nSuggest 1: The method is primarily validated on same or fine-tuned model pairs. Scalability to truly heterogeneous models remains unexplored.\n\nSuggest 2: The selection strategy is static post-calibration. Dynamic or context-aware layer selection could further enhance flexibility.\n\nSuggest 3: Several datasets are synthetically created or downsampled, which might not fully reflect real-world task complexity or communication needs."}, "questions": {"value": "1. **How transferable is the KVComm strategy across model families?** The paper evaluates KVComm mostly on pairs of identical or fine-tuned models. How would this framework perform if the sender and receiver are *structurally different models*, e.g., GPT-style vs. Mistral-style? Would attention alignment or KV dimensions cause integration issues?\n\n2. **Can the selection of layers be made context-adaptive instead of fixed?**\n   The Gaussian prior is fixed during calibration. Could performance improve if KV layer selection is dynamically adjusted based on the current query or context (e.g., using entropy or other uncertainty measures)?\n\n3. **Why use a Gaussian prior for layer selection?**\n   While intuitive and empirically justified, the Gaussian prior assumes a unimodal distribution over informative layers. Have other priors or non-parametric approaches (e.g., entropy-weighted or data-driven learned selection) been considered?\n\n4. **Does the integration of KV pairs affect positional encoding or cause mismatch issues?**\n   Since KV pairs include positional biases (especially in rotary or absolute encodings), how does the concatenation affect the receiver’s decoding stability and accuracy?\n\n5. **What is the impact on latency and memory footprint during runtime?**\n   The paper discusses FLOPs and communication savings, but does the selective KV sharing induce memory overhead (e.g., extra caching or tensor reshaping), particularly for large batch sizes or streaming inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6aesngmAnL", "forum": "F7rUng23nw", "replyto": "F7rUng23nw", "signatures": ["ICLR.cc/2026/Conference/Submission7805/Reviewer_1XmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7805/Reviewer_1XmS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977924263, "cdate": 1761977924263, "tmdate": 1762919847450, "mdate": 1762919847450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author studies the interesting problem of inefficient communication in multi-agent LLM systems. The authors argue that existing methods, such as using natural language communication, suffer from high inference costs and information loss , while methods using hidden states of last token can be ineffective. To address this, the authors propose KVComm, a framework for communication via the selective sharing of KV pairs from a sender model to a receiver model. The framework is based on identifying the most informative layer's KV pairs to transmit. This strategy assumes that intermediate layers contain the most transferable semantic knowledge, and that layers with higher attention paid to the context are more valuable. Based on evaluation on 6 benchmarks, the paper highlights the efficacy of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written in general and the research problem has been well-articulated to the reader. It addresses an important and timely topic, communication among large language model (LLM) agents, which is relevant to the community given the growing interest in multi-agent systems.\n\n2. The proposed framework of using KV pairs as the medium for communication is intuitive and also looks technically sound. \n\n3. The paper shows dominant performance over existing communication protocols like AC, NLD, and CIPHER across a diverse set of tasks and model pairs demonstrating its efficacy. Further, the authors perform several ablations to support the claims."}, "weaknesses": {"value": "1. The only concern is whether the proposed method generalizes to scenarios where the sender and receiver are LLMs with different architectures. In the experiments, the authors state that they \"limit the choices of the two LLMs to (1) two instances of the same LLM, and (2) two models that are fine-tuned versions of the same base LLM\". If its a limitation then the authors should clearly mention it.\n\n2. The KV cache sharing framework has some conceptual similarities with a prior work [1]. The authors are encouraged to clarify the distinctions of their approach relative to that work to better highlight the novelty.\n\n\n[1] DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving"}, "questions": {"value": "Please clarify weakness 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w84riBeXjJ", "forum": "F7rUng23nw", "replyto": "F7rUng23nw", "signatures": ["ICLR.cc/2026/Conference/Submission7805/Reviewer_1arg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7805/Reviewer_1arg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981697250, "cdate": 1761981697250, "tmdate": 1762919846572, "mdate": 1762919846572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary"}, "comment": {"value": "We sincerely thank all reviewers for their comments and especially suggestions for improving our paper. Besides addressing each comment in per-review comment(s), we have updated the paper and clearly marked our changes in blue. In the updated PDF we have also indicated which review comment is being addressed by each change, and we hope that, combined with our comments in the system, this way of marking the changes will make it much easier for reviewers to check how we have addressed their feedback. This is especially the case for new figures that we could not inline in the OpenReview system.\n\nMultiple reviewers questioned the ability of our approach to work in a multi-agent setting (as opposed to having just one sending and one receiving model as we already demonstrated). We have taken this opportunity to demonstrate the ease of adapting our approach to having two senders communicate their selected KV pairs to one receiver. The resulting 2-to-1 performance typically exceeds that of the 1-to-1 approach we have previously shown.\n\nAlso, multiple reviewers wanted us to clarify whether our approach currently works across models that are heterogeneous (i.e., come from different baselines). We acknowledge that our current prototype does not have this functionality, and we leave adapting it to work in the heterogeneous setting as future work.\n\nOur responses and clarifications are numerous, and besides the 2-to-1 case, include evaluation on an additional model, introduction of expanded datasets, quantification of the memory consumption, explanation of prior choices, exploration of the possibility of dynamic layer selection, clarification/evaluation of the way our approach carefully integrates KV pairs while adjusting the positional encoding, and revised Natural Language Debate (NLD) performance that is still inferior to our approach.\n\nFinally, we explained how our work differs from the references that the reviewers have thoughtfully brought up. For example, our work decisively goes beyond DroidSpeak [1] and the contemporaneous work similarly titled KVCOMM [2] (published very recently at NeurIPS 2025) by making it possible to efficiently and selectively choose layers of KV pairs that will be transmitted (as opposed to transmitting the entire KV or a contiguous chunk of it).\n\n[1] DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving\n\n[2] KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems"}}, "id": "anovIRKuxv", "forum": "F7rUng23nw", "replyto": "F7rUng23nw", "signatures": ["ICLR.cc/2026/Conference/Submission7805/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7805/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission7805/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725155428, "cdate": 1763725155428, "tmdate": 1763725155428, "mdate": 1763725155428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}