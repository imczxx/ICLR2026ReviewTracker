{"id": "EnA3YTB0dp", "number": 12672, "cdate": 1758209410257, "mdate": 1759897494584, "content": {"title": "Source Knowledge Anchored Regularization for Unsupervised Domain Adaptation", "abstract": "Deep neural networks trained on labeled source-domain samples often experience significant performance drops when used on target domains with different data distributions. Some unsupervised domain adaptation methods (UDA) address this by explicitly aligning the source and target feature distributions; however, enforcing full alignment without target labels can misalign class semantics. We propose Source Knowledge Anchored Regularization (SKAR) for UDA. This unified end-to-end framework transfers discriminative source knowledge via a composite loss on the network outputs, without explicitly enforcing distributional alignment. Our loss comprises of: (1) an adaptation-loss minimizing the entropy on target predictions to boost model confidence by leveraging source domain knowledge; (2) a regularization-loss for penalizing the model when its predictions falls under a few classes, thereby preventing class collapse; (3) a self-supervised-loss enforcing agreement between two strong augmentations of each target sample; and (4) a fidelity loss for anchors learning the source labels while mitigating overfitting. A curriculum learning schedule is applied to gradually shift the optimization focus from source fidelity to target-oriented objectives. Our main contribution is to couple the adaptation and regularization terms; we demonstrate theoretically (via gradient analysis) and empirically (via ablation and hyperparameter studies, and t-SNE visualizations) that these terms interact synergistically. On the Office-Home, Office-31, and VisDA benchmarks, SKAR achieves state-of-the-art performance, while requiring no auxiliary networks.", "tldr": "The paper proposes two loss terms for unsupervised domain adaptation problem in computer vision", "keywords": ["Domain Generalization", "Domain Adaptation", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7347028b29f08eb98b66c56bc1bdbbc0ca684337.pdf", "supplementary_material": "/attachment/f649e48b83c122a12e4f9f88fb83ca018679cae6.zip"}, "replies": [{"content": {"summary": {"value": "To address UDA, this paper propose an objective including label-smooth based source error and FixMatch with regularized entropy to balance the class prediction for target adaptation, where a curriculum learning schedule is applied to gradually shift the optimization focus from source fidelity to target-oriented objectives."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the idea is simple and easy to follow\n- detailed experiments including gradient analysis and hyper-parameter selection"}, "weaknesses": {"value": "__Major Concerns:__\n- limited novelty:\n  - I strongly suggest the author check [1], which propose exactly the same regularized entropy loss; actually, this kind of regularization is widely used DA literature such as [4,5] and has been thoroughly investigated in [6].\n  - the proposed algorithm is basically a combination of several existing techniques, SHOT [1] + FixMatch [2] + Label-Smooth [3]\n\n- insufficient comparison:\n  - I doubt MJE (2023) is still the most competitive baseline; for instance, [] achieve much better performance without accessing source data.\n  - even compared to MJE, the improvement is marginal.\n  - it would better to include to the results of DomainNet benchmark.\n\n__Minor Concerns:__\n- instead of placing tables inside text,  it would better to combine several tables to save space and report the details in appendix.\n- the title claims source knowledge anchored regularization; however, the proposed regularization is purely based on target prediction.\n\n***\n[1] Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation, ICML 2020\n\n[2] Fixmatch: simplifying semi-supervised learning with consistency and confidence, NeurIPS 2020\n\n[3] When does label smoothing help? NeurIPS 2019\n\n[4] Maximum classifier discrepancy for unsupervised domain adaptation, CVPR 2018\n\n[5] Unsupervised domain adaptation via structurally regularized deep clustering, CVPR 2020\n\n[6] Discriminative clustering by regularized information maximization, NeurIPS 2010\n\n[7]  Understanding and improving source-free domain adaptation from a theoretical perspective, CVPR 2024\n\n[8] Revisiting Source-Free Domain Adaptation: Insights into Representativeness, Generalization, and Variety, CVPR 2025"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMAZxJGZ1q", "forum": "EnA3YTB0dp", "replyto": "EnA3YTB0dp", "signatures": ["ICLR.cc/2026/Conference/Submission12672/Reviewer_hyyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12672/Reviewer_hyyh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484699694, "cdate": 1761484699694, "tmdate": 1762923511598, "mdate": 1762923511598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new domain adaptation algorithm to address the poor structure alignment and model collapse in traditional UDA methods. Paraphrasing from the abstract: they propose a loss which does 1. entropy regularization on target sample, 2. regularization to avoid class-mode collapse, 3. SSL loss to ensure target consistency along with a curriculum strategy to ensure robust learning. Experiments on standard UDA datasets show the effectiveness of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses several problems known in UDA such as poor structure alignment and mode collapse.\n\n2. The illustration of the method using gradient analysis (sec 3.2) as well as theoretical analysis (3.4) help to further explain the working of proposed approach."}, "weaknesses": {"value": "- Several aspects presented as novelty in the paper are already present in the literature in various forms. For instance,\n1. **Entropy regularization on target samples**: MME [Saito, Kuniaki, et al. \"Semi-supervised domain adaptation via minimax entropy.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019.]\n2. **Model collapse on target samples**: \n    1. Na, Jaemin, et al. \"Contrastive vicinal space for unsupervised domain adaptation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n    2. Sun, Tao, et al. \"Safe self-refinement for transformer-based domain adaptation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n3. **Self-supervised Loss on Target Augmented Samples**: AdaMatch [Berthelot, David, et al. \"Adamatch: A unified approach to semi-supervised learning and domain adaptation.\" arXiv preprint arXiv:2106.04732 (2021).]\n\nWhile some of these are cited and some aren't, this paper offers nothing specifically which might interest researchers or practitioners in the field. \n\n\n- The datasets used for experimentation in this paper are not relevanrt anymore in 2026 [when this paper will potentially be presented]. Office-31, VisDA are already quite saturated, and the gains offered by current method (+0.6% on VisDA, +0.2% on Office-31) is very minimal and not statistically significant. More newer and challenging datasets need to be presented for drawing any meaningful insights into this work, such as [1] and [2]\n\n[1] Peng, Xingchao, et al. \"Moment matching for multi-source domain adaptation.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019. \n\n[2] Kalluri, Tarun, Wangdong Xu, and Manmohan Chandraker. \"Geonet: Benchmarking unsupervised adaptation across geographies.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ifWH5OAyJ3", "forum": "EnA3YTB0dp", "replyto": "EnA3YTB0dp", "signatures": ["ICLR.cc/2026/Conference/Submission12672/Reviewer_3Je1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12672/Reviewer_3Je1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716560345, "cdate": 1761716560345, "tmdate": 1762923511224, "mdate": 1762923511224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework “Source Knowledge Anchored Regularization (SKAR)” for Unsupervised Domain Adaptation (UDA) that transfers discriminative source knowledge without explicit distribution alignment. The core contribution is a combined objective function with four loss terms: an Adaptation Loss (entropy - based) to boost target prediction confidence; a Regularization Loss to prevent degenerate solutions (class collapse) by encouraging class diversity; a Self-Supervised Loss to enforce invariance against augmentations; and a Fidelity Loss (standard supervised CE loss on source with label smoothing) using label smoothing to anchor the model to the source domain. The authors demonstrate the model’s performance on standard UDA datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Synergistic coupling of Adaptation loss (target prediction entropy minimization) and Regularization (prediction diversity in a batch) is a novel discussion. \n2. Achieves  domain alignment without explicit alignment objective like DANN or MMD."}, "weaknesses": {"value": "Lack of novelty in the approach. The primary objective function is a combination of standard objective functions used for DA. The regularization term is a popular loss for ensuring diversity. The self-supervised loss is popular for consistency regularization. The other two loss terms are even more routine. \n\nMinor: \n1. typo in Equation 3. \n2. Is it forward KL or reverse KL in Eq. 4. I.e., are both Pt and Pt^Aug adjusted or one of them is fixed for the sake of the objective."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nJPCtFj9T2", "forum": "EnA3YTB0dp", "replyto": "EnA3YTB0dp", "signatures": ["ICLR.cc/2026/Conference/Submission12672/Reviewer_ToRd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12672/Reviewer_ToRd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154977428, "cdate": 1762154977428, "tmdate": 1762923509151, "mdate": 1762923509151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}