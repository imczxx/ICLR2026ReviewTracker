{"id": "WwDNiisZQm", "number": 966, "cdate": 1756825713528, "mdate": 1759898233551, "content": {"title": "Content-Aware Mamba for Learned Image Compression", "abstract": "Recent Learned image compression (LIC) leverages Mamba-style state-space models (SSMs) for global receptive fields with linear complexity. However, the standard Mamba adopts content-agnostic, predefined raster (or multi-directional) scans under strict causality. This rigidity hinders its ability to effectively eliminate redundancy between tokens that are content-correlated but spatially distant.\nWe introduce Content-Aware Mamba (CAM), an SSM that dynamically adapts its processing to the image content.\nSpecifically, CAM overcomes prior limitations with two novel mechanisms. First, it replaces the rigid scan with a content-adaptive token permutation strategy to prioritize interactions between content-similar tokens regardless of their location. Second, it overcomes the sequential dependency by injecting sample-specific global priors into the state-space model, which effectively mitigates the strict causality without multi-directional scans.\nThese innovations enable CAM to better capture global redundancy while preserving computational efficiency. Our Content-Aware Mamba-based LIC model (CMIC) achieves state-of-the-art rate-distortion performance, surpassing VTM-21.0 by -15.91\\%, -21.34\\%, and -17.58\\% BD-rate on Kodak, Tecnick, and CLIC benchmarks, respectively.", "tldr": "We propose Content-Aware Mamba for learned image compression, outperforming previous Mamba-based LIC models.", "keywords": ["Learned Image Compression"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46a4f5d886b624cbca7ccb12bcaa2c3d4e1544ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Content-Aware Mamba (CAM) for learned image compression. This paper proposes two contributions: (1) content-adaptive token permutation via a codebook/K-means clustering that reorders tokens to put feature-similar patches adjacent before a 1-D selective scan; and (2) global-prior prompting, which injects sample-specific “prompts” derived from cluster assignments into the Mamba output projection to relax strict causality. On Kodak, Tecnick, and CLIC, CMIC claims strong BD-rate gains over VTM-21.0 and recent LIC models, with moderate compute and memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation & design: Shows how fixed raster scans and causality in SSMs are misaligned with image structure; proposes permutation + prompting to address both.\n2. Empirical results: Solid BD-rate improvements on three standard datasets; Comparable complexity numbers (params/FLOPs/latency/memory)."}, "weaknesses": {"value": "1. Possible shape mismatch: The prompting equation\n    \\[\n    O_i = (C + P) h_i + D x_i\n    \\]\n    leaves the shape of $P$ ambiguous. Earlier $P \\in \\mathbb{R}^{N \\times d_s}$ (per-token prompt vectors), whereas $C \\in \\mathbb{R}^{d \\times d_s}$. Adding them is dimensionally inconsistent unless $P$ is lifted to $\\mathbb{R}^{d \\times d_s}$ per token or used to gate $C$ via a mapping. Please clarify this precisely (e.g., broadcast).\n\n2. Report encoding latency (not only decoding). Only decoder latency on A100 is provided.\n\n3. The paper does not justify this permutation design choice. It would be strengthened by a comparison to simpler, end-to-end differentiable alternatives for learning a permutation, such as using attention scores for sorting, Gumbel-Softmax sampling, or an optimal transport-based approach.\n\n4. The ablation study in Appendix A.3.2 shows that replacing convolutions with CAM blocks in the entropy model provides negligible gains and increases latency. While the authors conclude that the entropy model benefits more from local context, this finding is interesting and slightly undermines the generality of the CAM block. A brief mention of this limitation in the main paper's ablation study would provide a more balanced perspective.\n\n5. The core idea of clustering tokens for content-adaptive processing is not entirely new. The authors cite Zhang et al. (2024), which also uses a clustering scheme. While the authors provide a good discussion in Appendix A.2, differentiating their fine-grained, non-Euclidean clustering from the coarse, grid-anchored approach of Zhang et al., this critical comparison is relegated to the appendix. For a top-tier conference, this discussion of related work and key differentiators should be integrated into the main paper."}, "questions": {"value": "Please see the weakness part. Overall, I find the paper satisfying. The paper would be further improved if the authors addressed the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Vhkg4q9cc", "forum": "WwDNiisZQm", "replyto": "WwDNiisZQm", "signatures": ["ICLR.cc/2026/Conference/Submission966/Reviewer_ysic"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission966/Reviewer_ysic"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574538119, "cdate": 1761574538119, "tmdate": 1762915649392, "mdate": 1762915649392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the issues of standard Mamba-style state-space models (SSMs) in learned image compression (LIC), namely content-agnostic fixed scanning (failing to effectively associate tokens that are spatially distant but content-similar) and strict causality (solved by multi-directional scanning yet with quadrupled complexity). It proposes Content-Aware Mamba (CAM), which relies on two core innovations: content-adaptive token permutation (reordering tokens by feature-space proximity via a shared learnable codebook) and global-prior prompting (injecting sample-specific global priors to relax causality)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The overall writing of the paper is clear, and the figures are well-presented. \n2. The proposed method achieves the optimal experimental performance."}, "weaknesses": {"value": "1. It is necessary to clarify the specific differences between the proposed method and Mambairv2 [1].\nBoth content-aware selective scanning and Learnable Prompt have been proposed in Mambairv2. \nThough I understand that Mambairv2 and the proposed CMIC are applied to two different tasks, the designs in this paper are mostly borrowed from Mambairv2 in a direct manner. There is insufficient technical contribution or further insights beyond Mambairv2 and specifically benefits image compression.\n2. The comparison with the latest approaches, such as MLICv2 [2], is missing.\n\n---\n[1] Guo, Hang, et al. \"Mambairv2: Attentive state space restoration.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Jiang, Wei, et al. \"MLICv2: Enhanced Multi-Reference Entropy Modeling for Learned Image Compression.\" arXiv preprint arXiv:2504.19119 (2025)."}, "questions": {"value": "See the Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6vWrRKNbC3", "forum": "WwDNiisZQm", "replyto": "WwDNiisZQm", "signatures": ["ICLR.cc/2026/Conference/Submission966/Reviewer_QkwG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission966/Reviewer_QkwG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749688445, "cdate": 1761749688445, "tmdate": 1762915649264, "mdate": 1762915649264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a clustering based content based scanning method and a global prompt based non causal modeling approach to address the issues of content independent scanning order and causal characteristics in the Mamba structured learned image codec. Ultimately, while preserving the original linear complexity of Mamba, this approach is effective, ,  reducing BD-rate by 15.91%, 21.34%, and 17.58% on the Kodak, Tecnick, and CLIC datasets over VTM-21.0."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Good RD performance while maintaining low complexity\n- Relieve the high memory usage problem of prior Mamba based learned image codec"}, "weaknesses": {"value": "- The methods proposed for both clustering partitioning and global prior modulation are common in prior \"content-adaptive\" studies, such as Rounting Transformer [1], even in the field of learned image compression, there are similar jobs available [2].\n- The main contribution of this paper is to improve the Mamba module in the Mamba-based learned image codec. However, it seems that there are many other differences compared to the previous works, e.g., MambaVC and MambaIC, including the entropy models, etc. The paper lacks detailed performance analysis.\n- Regarding the difference in receptive fields, according to Appendix Fig.16, it seems that this global receptive field and the so-called content aware features are also present in MambaIC, What is the actural difference?\n- The paper lacks a performance comparison with the latest methods listed, e.g., [3].\n\nRef:\n\n[1] Roy A, Saffar M, Vaswani A, et al. Efficient content-based sparse attention with routing transformers[J]. Transactions of the Association for Computational Linguistics, 2021, 9: 53-68.\n\n[2] Zhang Y, Duan Z, Lu M, et al. Another way to the top: Exploit contextual clustering in learned image coding[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(8): 9377-9386.\n\n[3] Li Y, Zhang H, Li L, et al. Learned image compression with hierarchical progressive context modeling[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025: 18834-18843."}, "questions": {"value": "- I can understand that the method proposed in this paper can improve the memory consumption of Mamba based image codecs, but further analysis is needed to determine the source of the model's performance improvement, including comparisons with existing Mamba-based methods such as MambaIC, and the benefits of visualizing/analyzing the proposed non causal approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r8oKkyjC5V", "forum": "WwDNiisZQm", "replyto": "WwDNiisZQm", "signatures": ["ICLR.cc/2026/Conference/Submission966/Reviewer_GLne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission966/Reviewer_GLne"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795889598, "cdate": 1761795889598, "tmdate": 1762915649030, "mdate": 1762915649030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a mamba-based image compression method that enhances mamba’s adaptability to the input content. In mamba, the predefined raster (or multi-directional) scans are content-independent, which limits the effective elimination of redundancy between tokens that are content-correlated but spatially distant. To address this, the authors introduce a content-adaptive token permutation strategy to enhance interactions between similar tokens. Additionally, they adopt strategies from mambairv2, such as injecting sample-specific global priors into the state-space model. Experiments demonstrate the method's effectiveness, surpassing VTM-21.0 by 15.91%, 21.34%, and 17.58% in BD-rate on the Kodak, Tecnick, and CLIC datasets, respectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe writing quality is good, and the explanation is clear.\n2.\tThe motivation for improving mamba’s fixed scanning order is reasonable, and enhancing adaptability is a valid direction.\n3.\tThe paper provides a comprehensive evaluation of complexity, including model size, FLOPs, latency, and memory usage.\n4.\tThe ERF visualizations clearly demonstrate the model's global perception ability and content adaptiveness."}, "weaknesses": {"value": "5. The paper compares only with MambaIC from CVPR 2025, ignoring LALIC and DCAE. Considering that LALIC and DCAE released code several months ago, a comparison with these methods is essential.\n6. The transform module in CMIC appears to be much deeper than in other methods. In Stage 2 and Stage 3, there are four blocks of attention and CAM in total—is this correct?\n7. The proposed Learnable Prompt Dictionary is essentially the Attentive State Space Module from mambairv2. The authors do not mention this in the paper, but since they cite mambairv2, they must be aware of it. I believe this is problematic and undermines the paper’s contribution and originality.\n8. In the supplementary materials, the authors compare their method with Zhang et al.’s approach. Are there quantitative results that align with other modules?\n9. Based on Figure 1, it seems that the clustering only scans within the current class. Does this harm the global nature of the method?\n10. In the ablation study, using just the 2D mamba outperforms MambaIc—what is the reason for this?\n11. According to Tables 2 and 3, removing CTP and GPP results in worse performance than just using 2D mamba. However, using pure mamba + CTP + GPP (CAM) does not match CMIC. What is the reason for this inconsistency?"}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jCiDe8meOR", "forum": "WwDNiisZQm", "replyto": "WwDNiisZQm", "signatures": ["ICLR.cc/2026/Conference/Submission966/Reviewer_72TK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission966/Reviewer_72TK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797717209, "cdate": 1761797717209, "tmdate": 1762915648884, "mdate": 1762915648884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}