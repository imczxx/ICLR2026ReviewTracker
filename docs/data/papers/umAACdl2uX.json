{"id": "umAACdl2uX", "number": 11803, "cdate": 1758203922609, "mdate": 1759897553785, "content": {"title": "Factor Graph Optimization for Belief Propagation Decoding", "abstract": "Belief Propagation (BP) is a highly efficient message-passing algorithm for inference on graphical models, famously applied to the decoding of sparse codes. The performance of BP, however, is critically dependent on the structure of the underlying factor graph. Designing a graph structure that is optimal for BP decoding remains a significant challenge, especially when constrained by short block lengths or novel channel models.\nIn this work, we introduce, for the first time, a gradient-based and data-driven framework to directly optimize the factor graph for the Belief Propagation algorithm. We learn locally optimal graph structures by running simulations under channel noise. This is enabled by a novel, complete graph tensor representation of the Belief Propagation algorithm, which makes the decoding process end-to-end differentiable. This representation allows us to optimize the graph structure over finite fields via backpropagation, coupled with an efficient line-search method. \nWhen applied to the design of sparse codes, the resulting BP-optimized factor graphs demonstrate decoding performance that outperforms existing popular codes and show the power of data-driven approaches for code design.", "tldr": "We provide a gradient-based approach for structure learning of the Bayesian Graph underlying Belief Propagation", "keywords": ["Binary Programming", "Belief Propagation", "Structure Learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35dac74806127c7405f53c83005814fd417bcbc0.pdf", "supplementary_material": "/attachment/6216018dfd0284c9dab0f1c816b282d5d27d3716.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework for optimizing the design of error-definite codes using gradient methods. Conventional design methods for LDPC codes, etc., have fixed the code structure and optimized performance based on analytical and combinatorial techniques. However, in recent years, there has been a demand for flexible code designs that can adapt to new communication channels such as short block lengths, IoT, and 5G.　\nWhile many machine learning approaches have focused on designing \"neural decoders,\" this research focuses instead on optimizing the \"code itself (factor graph).\"\nThe core idea of this research is, assuming a Belief Propagation (BP) decoder, to machine-learn the optimal code structure for the BP decoder."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: This research is the first attempt to optimize a code structure (factor graph) using gradient descent while keeping the belief propagation decoder fixed.\nQuality: As long as the authors examined, the superiority of the proposed method is shown objectively. \nClarity: What was done is clear. \nSignificance: Unlike conventional neural decoder-type ML-ECC, this method is compatible with existing BP implementations and has low implementation costs. The resulting code is sparse, practical, and performs well even in high SNR environments."}, "weaknesses": {"value": "The relaxation method used for optimization has no theoretical basis. While significant performance improvements are observed for short and medium block lengths, this comparison is based solely on BP decoding. Unless the block length is sufficiently long, BP is strongly affected by cycles in the graph and is not necessarily a good decoding method. Performance comparisons with the current best codes for short and medium block lengths are desirable."}, "questions": {"value": "How good is the performance obtained compared to the current best codes for short and medium block lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yWWIUq0alO", "forum": "umAACdl2uX", "replyto": "umAACdl2uX", "signatures": ["ICLR.cc/2026/Conference/Submission11803/Reviewer_pZQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11803/Reviewer_pZQ6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760744390223, "cdate": 1760744390223, "tmdate": 1762922825680, "mdate": 1762922825680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests learning the Tanner graph of an error correcting code by optimizing the performance of a BP decoder on that code. In order to achieve this, the BP decoding is formulated in a way that is differentiable. Results show significant improvements in bit error rate (BER) as a result of the optimization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow. I agree with the authors that it is better to use \"data driven\" approaches to optimize the code for BP rather than using a gigantic transformer to learn how to decode. The results show a clear advantage when the search is well initialized."}, "weaknesses": {"value": "The work could be improved by showing the importance of a good initialization for the code optimization. For example, figure 2 shows that using the optimization method starting from a highly designed 5G code will lead to a better code. But what happens if we initialize with a bad code? Since the optimization landscape is highly nonconvex, I would presume that a good initialization is crucial.\n\nThe work could also be improved by comparing to other numerical methods for optimizing codes. Although the authors call their approach \"data driven\", I see it more as a competitor to methods such as density evolution and EXIT diagrams which have existed for over 20 years. In such methods, the expected BER of a code with certain properties is estimated and code optimization is performed by numerically optimizing these performance as a function of the parameters. I think both approaches have their strengths and weaknesses but the paper could be improved by discussing these explicitly."}, "questions": {"value": "See the section on weakenesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wvkflH459q", "forum": "umAACdl2uX", "replyto": "umAACdl2uX", "signatures": ["ICLR.cc/2026/Conference/Submission11803/Reviewer_hwrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11803/Reviewer_hwrJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806124350, "cdate": 1761806124350, "tmdate": 1762922825230, "mdate": 1762922825230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a gradient-based, data-driven method for designing sparse-graph codes tailored to belief propagation (BP) decoding. The main contribution is to learn the factor-graph structure through a differentiable representation that enables backpropagation. Specifically, the authors start from a complete bipartite graph where the edges are learnable.\nThe approach is interesting and, as the authors show, leads to codes that outperform some existing ones. However, I recommend rejecting the paper due to the limited scope of its contribution, the weak experimental comparisons, and the additional concerns detailed in the \"Weaknesses\" section."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The approach is interesting and, as the authors show, leads to codes that outperform some existing ones."}, "weaknesses": {"value": "This paper was previously submitted to ICLR 2025 and rejected for well-founded reasons (see OpenReview). I would like to disclose that I served as one of the reviewers of that earlier submission. Upon inspection, this version appears nearly identical to the previous submission, with minimal or no substantive changes. Therefore, the same weaknesses identified in the earlier reviews remain unaddressed.\nIn short, the main issues are:\n\n1. The writing of the paper requires significant improvement.\n2. More importantly, the proposed codes do not outperform the state of the art. Yes, they do perform some good codes, but not the best ones.\n3. While the work introduces some interesting ideas, it does not demonstrate sufficient impact or advancement to merit acceptance at a top-tier venue like ICLR. The contribution is more suitable for a specialized (not major) venue such as ISIT or ITW.\n\nOverall, in my opinion does contain some interesting and potentially valuable ideas that deserve to be published. However, I also strongly believe that it does not merit publication in a top-tier conference/Journal. The decision made in the previous submission was well justified and should be upheld."}, "questions": {"value": "I do not have additional questions beyond those raised in the previous submission. The paper is not technically flawed; however, its significance and potential impact are insufficient to merit publication in a venue of this caliber. In the opinion of this reviewer, the contribution is minor (besides the fact that the proposed approach does not surpass the state of the art)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pp2tjLJK39", "forum": "umAACdl2uX", "replyto": "umAACdl2uX", "signatures": ["ICLR.cc/2026/Conference/Submission11803/Reviewer_26ev"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11803/Reviewer_26ev"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998960212, "cdate": 1761998960212, "tmdate": 1762922824316, "mdate": 1762922824316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to directly learnthe factorgraph for BP by backpropagating through a tensorized BP and updating the binary parity-check matrix with a discrete-aware (line-search) step, showing better BP decoding on several codes/channels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Keeps standard BP as the final decoder (more deployable with respect to other methods)\n\nDifferentiable formulation of BP over a learnable adjacency is novel to my knowledge\n\nA clever and novel binary-aware update to make STE actually work is designed and tested\n\nConsistent empirical gains across multiple code families"}, "weaknesses": {"value": "1. The paper sounds a bit like it is the first work to learn the factor graph. It would be good to have a comparison to PEG, differentiable LDPC search, neural BP with learnable edges, etc.\n\n2. You start from a dense bipartite graph and run tensor BP; the training numbers are heavy for relatively small codes. Is it possible to use this on realistic blocklengths or 5G-like structured graphs? It would be good to quantify the cost, show a lighter variant (e.g. start from structured sparse, optimize only a subset), or show it works on a realistically constrained graph\n\n3. You claim “factor graph optimization,” but you don’t show girth or cycle-spectrum improvements. It would be good to add before-after cycle histograms, degree constraints kept... Otherwise it seems difficult to distinguish the method from small edge changes driven by the loss (or be convincing that the latter is enough).\n\n4. optimize for 5 iteration, test at 15 iterations could look a bit tuned to the experiment\n\n5. Ablations on the key trick (binary-aware line search) are missing."}, "questions": {"value": "Please address the weaknesses, I will consider increasing my score upon convincingly addressing them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IvuNFs9Juc", "forum": "umAACdl2uX", "replyto": "umAACdl2uX", "signatures": ["ICLR.cc/2026/Conference/Submission11803/Reviewer_iT7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11803/Reviewer_iT7u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999892273, "cdate": 1761999892273, "tmdate": 1762922823911, "mdate": 1762922823911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}