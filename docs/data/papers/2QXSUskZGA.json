{"id": "2QXSUskZGA", "number": 22513, "cdate": 1758332087837, "mdate": 1763095060857, "content": {"title": "ONLINE RANKING WITH UNFAIR FEEDBACK AND HUMAN VERIFICATION: HIERARCHICAL ELIMINATION AND REGRET BOUNDS", "abstract": "Online platforms rely heavily on user feedback for ranking systems, such as restaurant ratings and e-commerce listings. However, these systems face challenges from unfair feedback, including merchant-induced and malicious feedback. Thus, platforms have adopted human verification to increase the reliability of the rankings. It can certainly identify genuine feedback, but introduces high latency into real-time updates, leading to non-static queuing dynamics and creating challenges for online learning. We model this as a continuous-time online learning problem, establish the lower bound on regret, and propose two algorithms: Hierarchical Elimination (HE) and Deficit Hierarchical Elimination (DHE), dealing with the case of single and multiple verifiers, respectively. We further prove upper regret bounds for both algorithms and demonstrate their effectiveness through numerical experiments.", "tldr": "", "keywords": ["Online learning", "Ranking", "Queueing System"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/606f85452eef8c3771df618a3b67a6bc75b03267.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies online ranking when user feedback can be unfair and only a limited portion of the feedback can be verified by humans with delay. The authors model the process as a continuous-time online learning problem with verification queues and propose two algorithms: Hierarchical Elimination (HE) for a single verifier and Deficit Hierarchical Elimination (DHE) for multiple verifiers with heterogeneous speeds. Theoretically, the authors provided regret analysis for both algorithms. They have also conducted numerical experiments on small synthetic data to show the efficacy of their methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper formalizes ranking with unfair feedback and verification latency as a continuous-time bandit-queue hybrid. I think the idea here is conceptually novel and has not been studied by prior works. \n- The authors provided a nice description of their algorithmic ideas, which consists of both ranking and scheduling components. (However, I think it'd be better to include the main algorithm in the main body of the paper to improve clarity.) The theoretical results also appear sound and complete, with both upper and lower bound analysis, though I have not checked the proofs in detail."}, "weaknesses": {"value": "1. I have some major concerns related to the practicality of the proposed framework/methods:\n- In terms of the numerical experiments, I think they are conducted under very small synthetic setups $(K \\leq 3, T \\leq 2000) $ without real or large-scale data. The paper’s claims about practical relevance to real-world online platforms are therefore not empirically supported.\n- The numerical comparisons are restricted to variants of the authors’ own methods and there is no comparison against other learning-to-rank methods.\n- Neither HE nor DHE’s computational or queue-simulation cost is analyzed, and it is unclear whether the proposed method would be practically feasible to implement. \n2. The model assumes that unfair feedback is corrected by human verifiers operating in queues with exponential service times, which in my opinion is not very realistic for modern platforms. In practice, large-scale online platforms can use automated anomaly detection or even LLM-based verification, which can handle biased or malicious feedback at far lower computational and operational cost. So I feel that the proposed model is rather limited in real-world applicability."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vwCCCVzELM", "forum": "2QXSUskZGA", "replyto": "2QXSUskZGA", "signatures": ["ICLR.cc/2026/Conference/Submission22513/Reviewer_sLp3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22513/Reviewer_sLp3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535321288, "cdate": 1761535321288, "tmdate": 1762942250227, "mdate": 1762942250227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates online learning to rank (OLTR) under manipulated or unreliable feedback. Motivated by real-world systems that employ human verification mechanisms, the authors model the problem as one with delayed feedback and verification-induced queuing effects. They formulate it within a continuous-time online learning framework and introduce two algorithms that effectively combine verified and unverified feedback. Theoretical analysis establishes regret guarantees and asymptotic optimality, while empirical results demonstrate the practical advantages of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper tackles a timely and practically important problem arising in real-world online platforms, where ranking systems must contend with manipulated or unfair feedback and delays caused by human verification. The problem formulation captures these challenges well and is highly relevant to practical applications.\n2) It introduces a novel and insightful perspective by incorporating queuing dynamics into the OLTR framework, leading to a delayed feedback model with verification-induced latency. This integration is conceptually interesting.\n3) The proposed HE and DHE algorithms are theoretically sound and designed to utilize both verified and unverified feedback while accommodating verifier heterogeneity. Their design is well-motivated and grounded in theoretical analysis."}, "weaknesses": {"value": "1) When unverified feedback is excluded, the problem reduces to a new variant of delayed feedback learning with queuing dynamics. In this case, a more thorough comparison with existing work on delayed or batched bandits would help better situate the paper’s contributions and clarify its novelty.\n2) The proposed algorithms rely on a known upper bound for the probability of unfair feedback to incorporate unverified observations. This assumption may be unrealistic in practice, as such bounds are rarely available or easy to estimate. It would strengthen the work to explore adaptive approaches that can estimate or learn this parameter online.\n3) The experimental evaluation is limited in scope, relying solely on synthetic simulations without validation on real-world data. In addition, the lack of comparisons with simple and intuitive baselines, such as ranking based only on verified feedback combined with standard scheduling heuristics (e.g., FCFS), makes it difficult to fully assess the empirical benefits of the proposed methods. The current setup, involving only three items and two verifiers, also appears overly simplified and may not capture the complexity of practical scenarios."}, "questions": {"value": "1) The only experiment with a larger setting (50 items and 10 verifiers) appears in Fig. 3b of the Appendix. It would be helpful to include more experiments with larger-scale settings, especially considering recommender system applications where the number of items is typically much higher. \n2) The paper does not discuss or empirically evaluate the algorithm’s time complexity. Since the proposed methods repeatedly solve LP tasks, it would be helpful to include results that assess the computational efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMHW1IkJik", "forum": "2QXSUskZGA", "replyto": "2QXSUskZGA", "signatures": ["ICLR.cc/2026/Conference/Submission22513/Reviewer_Ve2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22513/Reviewer_Ve2N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994024624, "cdate": 1761994024624, "tmdate": 1762942249036, "mdate": 1762942249036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates incorporating human verification mechanisms into online ranking algorithms to address the impact of unfair feedback. The authors tackle this problem by proposing two algorithms: Hierarchical Elimination (HE) for a single verifier and Deficit Hierarchical Elimination (DHE) for multiple heterogeneous verifiers. Both algorithms achieve logarithmic regret bounds while handling verification scheduling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The integration of human verification feedback into online ranking frameworks is novel and interesting.\n- The algorithms consider both the use of unverified data and the presence of multiple verifiers, offering flexible solutions for different setups."}, "weaknesses": {"value": "**Assumptions**\n- Assuming that human verification can always identify the manipulation behavior is too strong. I think it is not easy to accurately judge whether a feedback is manipulated in practice.\n\n**Theoretical analysis**\n- The proof of Theorem 1 in appendix A.1 seems not complete. The authors only present the regret bound of system $H^2$, but do not show that the original system that operates under HE algorithm enjoys less regret than that of the $H^2$ system.\n- The proof of Lemma 5 is unclear. The authors appear to use the Hoeffding's inequality to bound the failure probability. However, since the value $q_j(t)$ can be arbitrarily determined by an adversary, it may depend on the system’s history, making the feedback variables depend on the history feedbacks. As the Hoeffding's inequality requires the random variables to be independent, it is not applicable in this case.\n\n**Writing**\n- I'm confused about Algorithm 1. When $UCB_i < LCB_j$, the Algorithm 1 in Appendix C removes $I_i$ from both $\\mathcal{A}^q$ and\n$\\mathcal{A}^{q+1}$. However, the description in section 4.1 says that the algorithm should send $I_i$ from $\\mathcal{A}^q$ to $\\mathcal{A}^{q+1}$. Which one is correct?\n- In theorem 3, what does \"satisfying 1\" mean?\n- In line 175, the paper introduces a tuple to denote the system state, but does not provide formal definition of the entries in the tuple.\n- The paper should also provide the pseudocode of the DHE algorithm.\n\nMinors:\n- The titles of Appendices A.2 and A.3 should be swapped.\n- It seems that the references in line 511 and line 514 are the same paper."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sDUCzIKOR2", "forum": "2QXSUskZGA", "replyto": "2QXSUskZGA", "signatures": ["ICLR.cc/2026/Conference/Submission22513/Reviewer_kHHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22513/Reviewer_kHHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001385328, "cdate": 1762001385328, "tmdate": 1762942248232, "mdate": 1762942248232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "I have updated the revised version, and hopefully it resolves most of the concerns you raised before in the comments."}}, "id": "jq8bfin5Ub", "forum": "2QXSUskZGA", "replyto": "2QXSUskZGA", "signatures": ["ICLR.cc/2026/Conference/Submission22513/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22513/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22513/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763095225962, "cdate": 1763095225962, "tmdate": 1763095225962, "mdate": 1763095225962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}