{"id": "7gA6ZYii0e", "number": 3965, "cdate": 1757575694129, "mdate": 1759898060642, "content": {"title": "RAISE the Bar: Ensemble-based Online Reinforcement Learning for Dynamic Workflow Scheduling", "abstract": "Dynamic workflow scheduling (DWS) in cloud computing poses major challenges due to unpredictable workflow arrivals, heterogeneous resources, and evolving system states. While reinforcement learning (RL) has shown promise for learning adaptive scheduling policies, existing single-policy approaches often struggle in online settings with non-stationary workloads.\nWe propose **RAISE** (*Robust Actor-Critic Integration for Scheduling Ensembles*), an ensemble-based online RL approach designed to improve adaptability and stability in dynamic environments. RAISE maintains a set of pre-trained actors and critics that are continually updated during deployment to support robust scheduling. It integrates three key components: (1) *Value-Ranked Action Aggregation*, which combines majority voting with critic-guided tie-breaking for stable action selection; (2) *Dual Critic Ensembles with Decoupled Updates*, which balance fast adaptation and stable value estimates; and (3) *Decision-Aligned Policy Updates*, which enhance sample efficiency by updating only the actors responsible for chosen actions.\nExperiments on large-scale DWS benchmarks show that RAISE consistently outperforms state-of-the-art baselines in both performance and robustness, demonstrating the effectiveness of ensemble-based online RL for real-time scheduling under non-stationary conditions.", "tldr": "We propose RAISE, an online RL method with ensemble actors and dual critics that delivers stable and adaptive scheduling under non-stationary workloads.", "keywords": ["Workflow Scheduling; Reinforcement Learning; Online Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b646c3448825c34eb020333d8239768af9c5f7f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the Dynamic Workflow Scheduling (DWS) problem in cloud computing, which is characterized by unpredictable workflow arrivals and non-stationary workloads. The authors point out that existing reinforcement learning (RL) methods, especially single-policy approaches, struggle with instability and poor adaptability in these real-time, non-stationary environments. To solve this, the paper proposes a method called RAISE (Robust Actor-Critic Integration for Scheduling Ensembles), an ensemble-based online RL method built on a PPO-style Actor-Critic framework. RAISE contributes three novel components that work in synergy:\n\n1. **Value-Ranked Action Aggregation (VRAA):** A robust decision-making mechanism. It first uses a majority vote from an Actor ensemble to find candidate actions. It then uses the relative rank from a Critic ensemble for tie-breaking, rather than using scale-sensitive raw Q-values.\n2. **Dual Critic Ensembles with Decoupled Updates:** To balance stability and plasticity, RAISE maintains two sets of Critic ensembles. \"Adaptive\" Critics are updated frequently with recent data to adapt quickly to new dynamics, while \"Conservative\" Critics are updated slowly via Polyak averaging to preserve stable, long-term value estimates from prior knowledge.\n3. **Decision-Aligned Policy Updates:** A specialized training strategy designed to improve sample efficiency and maintain ensemble diversity. First, experience samples ($s_t$, $a_t$, ...) are only assigned to the experience replay buffers of the Actors that actually proposed the executed action $a_t$. Second, it employs a conservative gradient control mechanism that discards (sets to zero) large, potentially destabilizing gradients during online updates. The authors conduct extensive experiments on a DWS benchmark using real-world workflow patterns (e.g., Montage, CyberShake) and non-stationary arrival patterns. The results show that RAISE consistently outperforms state-of-the-art (SOTA) baseline methods, including heuristic-based methods and other DRL schedulers (like GOODRL). Ablation studies validate the effectiveness of the three proposed components."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Importance and Problem Relevance:** The paper tackles the important and practical problem of Dynamic Workflow Scheduling (DWS). Its focus on non-stationary online environments addresses a key weakness of much prior work.\n- **Methodology:** The paper's main strength lies in the design of the RAISE algorithm itself. The three primary contributions (VRAA, Dual Critics, Decision-Aligned Updates) are original, well-motivated, and empirically shown to be effective.\n- **Experimental Validation:** The ablation studies are comprehensive.\n- **Clarity:** The paper is clearly written. Complex concepts are explained intuitively, and the architecture is clearly presented."}, "weaknesses": {"value": "1. **Rationale for Gradient Control:** The gradient control mechanism in Eq. (9) is unconventional and quite aggressive. Instead of standard clipping (rescaling), it discards any gradient whose norm exceeds a threshold based on the previous batch's statistics. While the ablation study (Table 3) shows this method works (RAISE w/o Grad performs poorly), the paper offers limited intuition as to why this \"all-or-nothing\" approach is superior to standard gradient clipping. How sensitive is the model to the hyperparameter $\\tau_0$? In practice, how frequently are gradients dropped? A deeper analysis of this specific design choice would make the paper more rigorous.\n2. **Scalability and Overhead Analysis:** The paper claims the computational overhead is \"minimal\" and \"almost negligible,\" citing 0.03-0.09s inference time. This is for an ensemble size of n=5 Actors and m=5 for each Critic type (15 networks total). While this is acceptable, a more thorough scalability analysis would be welcome. How do inference and online update times scale as the number of ensemble members (n, m) increases? Is there a performance vs. compute trade-off, or do the returns diminish quickly? This is a key practical concern for ensemble methods.\n3. **Anomalous Baseline Results:** The ERL-DWS baseline performs disastrously poorly (e.g., >800% gap in Table 1). The authors state they \"followed the official implementation,\" but this result is extreme. It's possible this baseline was simply not designed for this kind of non-stationary online adaptation. A brief sentence or two speculating on the reason for this drastic failure (e.g., \"Its Transformer-based policy may be prone to catastrophic forgetting during online updates\") would add context and assuage reviewer concerns about a potentially flawed baseline implementation.\n4. **Marginal Improvement vs. Complexity:** The main comparison for RAISE is GOODRL (a 2025 paper). However, RAISE's performance improvement is very marginal. In the 12 scenarios in Table 1, the \"Gap\" (advantage of RAISE over GOODRL) is often below 4%, and even as low as 0.51% in the (6x4, 5.4, 20k) scenario and 1.38% in the (4x10, 5.4, 20k) scenario. Given the immense complexity RAISE introduces (from GOODRL's 1 Actor/1 Critic to 5 Actors/10 Critics, 15 networks total), it is questionable whether this slight performance gain is practically significant and statistically significant.\n\n**Other Suggestions:**\n\n- The RAISE architecture is relatively complex. Compared to GOODRL (1 Actor, 1 Critic), RAISE needs to maintain 15 separate neural networks (5 Actors, 5 Adaptive Critics, 5 Conservative Critics). However, as shown in Table 1, this huge increase in complexity (potentially >7-8x computational and memory overhead) often yields performance gains of less than 4%, sometimes even less than 1%. It would be appropriate to discuss the trade-off between the increased computational cost from the network structure's complexity.\n- The \"non-stationary\" environment used is just a gently ramping up/down Poisson arrival rate. This is one of the simplest forms of non-stationarity. Real-world distribution shifts are often more drastic, abrupt, and varied. Whether the method can handle more complex non-stationarity (e.g., sudden changes in workflow type, abrupt machine failures) is unverified. These might be points to address in the final version of the paper."}, "questions": {"value": "1. **Regarding Gradient Control (Eq. 9):** This strategy of dropping gradients (setting to 0) is novel. (a) Can you provide more intuition for this choice over standard gradient clipping (i.e., rescaling the norm)? Was standard clipping tried and found to be ineffective? (b) Can you report the frequency with which gradients were dropped during the experiments? This would help us understand if the mechanism is a rare safety net or a common operation. (c) How sensitive is the performance to the maximum gradient norm (\\tau_0) hyperparameter?\n2. **Regarding the Use of Dual Critics:** In the policy update (Eq. 8), the advantage is calculated using *only* the adaptive critic ensemble {Q_{\\phi_{j}}}. However, in action selection (Eq. 5), the algorithm randomly switches between the adaptive and conservative ensembles for ranking. (a) What is the rationale for *only* using the adaptive critics to compute the advantage? (b) Did you experiment with using the conservative critics {\\hat{Q}*{\\psi*{k}}} or a combination of both (e.g., average or minimum) for the advantage calculation?\n3. **Regarding Scalability:** The ensemble size was fixed at n=5 Actors and m=5 for each Critic type. (a) How do inference and online update times scale as the number of ensemble members increases (e.g., n=10, 20)? (b) What is the impact of changing the ensemble size on performance? Is n=5 the optimal balance for the performance/compute trade-off?\n4. **Regarding Gradient Dropping (Eq. 9):** (a) Why choose this ad-hoc gradient \"dropping\" mechanism instead of the standard and mature gradient \"clipping\"? (b) Please provide a comparative experiment against standard gradient clipping (e.g., the PPO default clip). (c) Please provide a hyperparameter sensitivity analysis for \\tau_0 and report the frequency of gradient dropping during training.\n5. **Regarding Complexity vs. Performance Trade-off:** As shown in Table 1, RAISE's performance improvement over GOODRL is very marginal (often <4%), yet its network complexity increases several-fold (15 networks vs. 2 networks). (a) How do the authors justify this significant additional overhead? (b) Is this slight improvement statistically significant?\n6. **Regarding Inconsistent Use of Critics:** In action selection (Eq. 5), the algorithm randomly uses either conservative or adaptive critics for ranking. However, in the policy update (Eq. 8), the Advantage function is calculated *only* by the adaptive critics. This design choice seems inconsistent. Please explain why the (e.g., more stable) conservative critics are not also utilized in the advantage function calculation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTexdkMlwN", "forum": "7gA6ZYii0e", "replyto": "7gA6ZYii0e", "signatures": ["ICLR.cc/2026/Conference/Submission3965/Reviewer_wFb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3965/Reviewer_wFb3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300562999, "cdate": 1761300562999, "tmdate": 1762917116603, "mdate": 1762917116603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work studies the problem of workflow scheduling in a computing center with non-stationary workflow arrivals. It proposes an ensemble-based actor–critic method that balances adaptability and stability. The approach relies heavily on prior solutions, with some extensions—for example, using a value-ranked aggregation mechanism instead of probabilistic methods or majority voting. The study relies on assumptions that are not realistic or valid. Moreover, the performance gains are not significant."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem is important and timely, and it has been studied extensively in the past few years. The proposed solution, if the assumptions are correct, has potential."}, "weaknesses": {"value": "1. The work heavily relies on previous research, with only adjustments to how the ensemble decision is made.\n\n2. There are multiple issues with the system model:\n\n- The execution time of running a task on a machine is assumed to be deterministic, i.e. $et^q_{ij} = tw_{ij} / ms_q$, where $tw_{ij}$ is the workload and $ms_q$ is the processing speed of machine $M_q$. This is a very simplistic assumption, as a machine’s processing speed is time-varying due to other tasks running on the machine (e.g., the operating system, virtualization programs, etc.).\n\n- The authors assume that machines use a FIFO-based waiting queue for assigned tasks, but the queuing delay at this queue is completely ignored. In particular, $ft^q_{ij}$, $ft_i$ and $F_i$ are modeled only as functions of $et^{q}_{ij}$. \n\n- The results in Table 1 show that the decision-making time of RAISE is not negligible (40–90 ms). Does $F_i$ take this latency into account? Note that this latency is observed for each task in $O_{W_i}$: RAISE must run each time a task is ready to execute, so the latency accumulates.\n\n4. The overall gain from using RAISE is not significant, especially given that the calculation of $F_i$ does not include RAISE’s decision-making latency."}, "questions": {"value": "1. Is there any justification for assuming that task execution time on a machine is deterministic? Could you provide real-world measurements to back up this assumption?\n\n2. Why were the queuing time and RAISE’s decision-making latency excluded from the calculation of F_i? How do the results change if these two factors are included in the study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3XqXvBPFEK", "forum": "7gA6ZYii0e", "replyto": "7gA6ZYii0e", "signatures": ["ICLR.cc/2026/Conference/Submission3965/Reviewer_KLDs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3965/Reviewer_KLDs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818870369, "cdate": 1761818870369, "tmdate": 1762917115788, "mdate": 1762917115788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an ensemble-based reinforcement learning framework that adaptively tunes policies to accommodate non-stationary and evolving environments for dynamic workflow scheduling. It resolves tie situations among different actors using value rankings derived from their corresponding critics, balances adaptation speed and training stability through dual critic networks, and enhances sample efficiency and preserves policy diversity by selectively updating actors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a simple value ranking mechanism to effectively solve the tie problems while ensuring the inference speed.\n\nInstead of updating all actors, the paper updates only those actors responsible for the selected actions, effectively accelerating training and reducing computational overhead while preserving strong policy adaptability. This is a simple yet effective solution for ensemble-based RL methods."}, "weaknesses": {"value": "In the Introduction, you mentioned that most RL-based DWS approaches rely on offline training. This statement is not very accurate and may mislead readers. In fact, classic RL is an online training paradigm by consistently interacting with environments to collect samples and learning policy. I think what you meant is that most current RL methods focus on training the policy from scratch, while neglecting subsequent online tuning of the policy to adapt to changing environments. So please state your point more accurately.\n\nIn Section 4.2, you mentioned that adaptive critics may offer accurate estimations but are less stable, and conservative critics provide stability but tend to underestimate action values. However, there seem to be no experimental results supporting this assumption.\n\nIn Section 5.4, the experimental results can only demonstrate the adaptability introduced by the decision-aligned sample assignment and gradient control mechanism, but do not show their ability to ensure training stability and enhance sample efficiency claimed in the Abstract."}, "questions": {"value": "There are several recent advances in online tuning within reinforcement learning, often referred to as continual RL. It is recommended that the authors conduct a literature review and select several representative algorithms for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Yb12767XV", "forum": "7gA6ZYii0e", "replyto": "7gA6ZYii0e", "signatures": ["ICLR.cc/2026/Conference/Submission3965/Reviewer_7eFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3965/Reviewer_7eFH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948873618, "cdate": 1761948873618, "tmdate": 1762917115176, "mdate": 1762917115176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAISE (Robust Actor-Critic Integration for Scheduling Ensembles), which is an online, actor-critic reinforcement learning paradigm for dynamic workflow scheduling.  Taking significant influence from the recent GOODRL (Yang et al., 2025) approach to dynamic workflow scheduling, RAISE extends this approach to ensembles of actors and critics, introducing several improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Demonstrates state-of-the-art performance in minimizing mean flowtime for dynamic workload scheduling, improving upon state-of-the-art performance by up to 5-6% and demonstrating improved robustness of performance across varying workloads and machine configurations.\n\n- Utilizes realistic, highly dynamic workload scheduling scenarios for testing making experimental results highly relevant to real-world scheduling.\n\n- Introduces multiple mechanisms to stabilize system performance, including (i) selective actor updates, to provide direct, relevant gradients to actors, encouraging ensemble diversity and, consequently, system stability, and (ii) dual critic ensembles with decoupled updates, creating critic ensembles with differing update rates, allowing for more balanced value estimation signals and improving overall system stability."}, "weaknesses": {"value": "- The motivation for development of the proposed approach, which significantly overlaps with the offline-online RL approach proposed in Yang et al. (2025), is the unsupported claim that the reliance of the approach in Yang et al. (2025) on a single policy/actor may render it incompetent across a long time period.  \n- Tie breaking mechanism in value-ranked action aggregation randomly utilizes either the set of Adaptive Critics or the set of Conservative Critics to generate action rankings, rather than utilizing both sets of critics to ensure highest accuracy in all tie-breaking action rankings.  Authors state that this prevents the final action selection from consistently favoring one critic set over the other but do not explain why this is unfavorable, do not explore potential alternative approaches that do utilize both sets of critics, and neglect to include any such alternative approach in their ablation studies (however, see question below).\n- Computational overhead of proposed approach is multiple times that of current state-of-the-art approaches, including GOODRL (about 2-3x higher) and GPHH (about 10x higher).  Authors discount this increased computational overhead as practically negligible due to latency in other aspects of one of the real-world environments in which this system would be employed (i.e., cloud computing) but do not address other aspects of computational overhead (e.g., energy consumption) or net impact in other real-world contexts."}, "questions": {"value": "1.\tWhat is the basis for your claim that the single policy/actor approach in Yang et al. (2025) may render it incompetent across a long time period?\n2.\tWith respect to the random selection of the set of critics used in a tie-break valuation, did authors ever consider or investigate any correlation between the critic set utilized and the actor(s) whose action was selected?  That is, did the authors confirm that a given set of critics selected sufficiently evenly across actors across samples to allow for both the adaptive and conservative ensembles to affect the gradients of all actors (i.e., avoiding the potentially unintended creation of adaptive actors and conservative actors)?\n3.\tRegarding the ablation study of action aggregation methods, did the ensemble with value strategy utilize the dual sets of critic ensembles (adaptive and conservative) and average Q-values across them, or did it utilize only a single type of critic ensemble?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ct7x8fer7T", "forum": "7gA6ZYii0e", "replyto": "7gA6ZYii0e", "signatures": ["ICLR.cc/2026/Conference/Submission3965/Reviewer_rcSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3965/Reviewer_rcSX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952695432, "cdate": 1761952695432, "tmdate": 1762917114867, "mdate": 1762917114867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}