{"id": "tryFvnBKjy", "number": 597, "cdate": 1756751966264, "mdate": 1763713661166, "content": {"title": "LHM++: An Efficient Large Human Reconstruction Model for Pose-free Images to 3D", "abstract": "Reconstructing animatable 3D humans from casually captured images of articulated subjects without camera or pose information is highly practical but remains challenging due to view misalignment, occlusions, and the absence of structural priors. In this work, we present LHM++, an efficient large-scale human reconstruction model that generates high-quality, animatable 3D avatars within seconds from one or multiple pose-free images.  At its core is an Encoder–Decoder Point–Image Transformer architecture that progressively encodes and decodes 3D geometric point features to improve efficiency, while fusing hierarchical 3D point features with image features through multimodal attention.  The fused features are decoded into 3D Gaussian splats to recover detailed geometry and appearance. To further enhance visual fidelity, we introduce a lightweight 3D-aware neural animation renderer that refines the rendering quality of reconstructed avatars. Extensive experiments show that our method produces high-fidelity, animatable 3D humans without requiring camera or pose annotations. Our code and models will be released to the public.", "tldr": "", "keywords": ["3D Human Avatar", "3D Gaussian Splatting"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ec9aed6572e1fdadfc20f12aa88fdd5f77635c4.pdf", "supplementary_material": "/attachment/3c7c21663c5aa4a148bb7835b23eb589d0a729c6.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents an advanced version of LHM that accepts multiple images and produces high-quality, animatable 3D avatars with a large feed-forward model. The authors propose an Encoder–Decoder Point–Image Transformer that fuses 3D points with image features to handle multiple images efficiently. The fused tokens are decoded into 3D Gaussian splats and rendered with a lightweight 3D-aware neural renderer for real-time animation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well-written and easy to understand.\n- Extensive ablation studies and visualizations are provided in the main paper and supplementary materials.\n- The proposed Encoder–Decoder Point–Image Transformer reduces inference time compared to LHM, especially as the number of images increases, while also improving performance."}, "weaknesses": {"value": "- The main concern is that the network is designed for sparse views. Why is this design choice necessary? Although the authors extend LHM to multi-image input, the paper shows limited gains beyond 16 images (e.g., Table 2/5 at 64 views). This raises the question of whether the multi-image extension offers meaningful benefits at higher view counts. In particular, the paper notes that “the gains become marginal with an increasing number of views” (Line 431), which is counter-intuitive if more views should provide more information. Please clarify.\n\n- The overall framework feels close to LHM: both use multimodal transformers to fuse 3D geometry with image features inside the network. The technical contribution beyond LHM on model design is not entirely clear from the framework description.\n\n- In Model Design, the paper highlights LHM’s quadratic attention complexity O(N_points + N)^2, but in Point–Image Attention the complexity remains quadratic after token merging, while N reduced to N/r. This suggests limited improvement as N grows large. A more detailed complexity analysis would strengthen the efficiency claim.\n\n- There is no visualization of avatar on canonical pose. It would be nice if the canonical pose visualization is included.\n\n- Missing recent references (recommend adding and, if possible, comparing in Table 1):\n    \n    [1] Kocabas, Muhammed, et al. \"Hugs: Human gaussian splats.\" CVPR 2024. → Monocular video based reconstruction\n    \n    [2] Shin, Jisu, et al. \"Canonicalfusion: Generating drivable 3d human avatars from multiple images.\" ECCV 2024. → Monocular video based reconstruction\n    \n    [3] Liao, Tingting, et al. \"High-fidelity clothed avatar reconstruction from a single image.\" CVPR 2023. → Single image based reconstruction\n\n    [4] Moreau, Arthur, et al. \"Human gaussian splatting: Real-time rendering of animatable avatars.\" CVPR 2024. → Multi-view video based reconstruction\n    \n    [5] Wang, Rong, et al. \"FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images.\" CVPR 2025 → Few image based reconstruction\n\nI will reconsider the score when all the concerns are handled well."}, "questions": {"value": "- What is the main difference between LHM and this paper’s framework beyond the point–image attention details?\n\n- Why doesn’t performance improve after 16 images? Is this due to training distribution, or difficulty handling deformations?\n\n- What are the common failure cases of the framework (e.g., loose garments, extreme poses, or heavy occlusions)? A small failure case visualization would be informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BFsh7wvqUa", "forum": "tryFvnBKjy", "replyto": "tryFvnBKjy", "signatures": ["ICLR.cc/2026/Conference/Submission597/Reviewer_nJbr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission597/Reviewer_nJbr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145594303, "cdate": 1761145594303, "tmdate": 1762915561948, "mdate": 1762915561948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "Dear Reviewers,\n\n\n\nWe sincerely thank you for your valuable suggestions, which have helped us improve and strengthen the paper. In this revision, we have updated the following items:\n\n- We have updated Table 1 to provide a more comprehensive survey of state-of-the-art 3D human reconstruction methods. （**[Tab. 1, Lines 117-132]，[Lines 147-149]**）\n\n- We provide more detailed descriptions of the training loss to avoid potential confusion. （ **Lines 347-348**）\n- We corrected a testing-time typo for LHM: we had copied their reported value, but they measured inference time on an RTX 4090 while our tests were run on a single NVIDIA A100. Accordingly, we updated the value from \"3.17 s\" to \"5.73 s\" (**Tab. 3, Line 423**).\n- We have added a more detailed efficiency analysis in the main text to enable reviewers to clearly observe the performance gap between our proposed framework and LHM in terms of computational efficiency. This analysis includes two new tables: Table 4 and Table 5. **Table 4** reports the inference time required by LHM and our method to fuse 2D and 3D tokens (excluding image tokenization and preprocessing) under varying input configurations and different numbers of geometric sampling points. **Table 5** illustrates the reduction in the number of 3D tokens across the encoder's layers of our PIT framework. (**[Lines 467-485], [Tab. 4 Lines475-482]**, **[Tab. 5 Lines486-494]** )\n- We added a detailed study on the effectiveness of multi-modal fusion, including Table 6, which shows our model can efficiently fuse large numbers of 3D tokens and achieve overall quality improvements, whereas LHM struggles to fuse 2D and 3D tokens efficiently when the 3D token count is high. (**[Lines 500-512], [Tab.6 Lines 501-507]**)\n- We also presented a comprehensive ablation study of the attention modules in our encoder–decoder PIT, analyzing the contributions of image-wise and point-wise attention to the performance gains. (**[Lines 513-528], [Tab.7 Lines 513-520]**)\n- We discussed general image-to-3D methods (e.g., Hunyuan3D and Rodin) for avatar generation, as well as the use of Mixamo for automatic rigging, and include a comparative experiment evaluating our approach against these pipelines. ([**Appendix D.4 Lines 1028-1038**], [**Appendix D.4, Fig.8 Lines 1039-1060**])\n- We added a figure illustrating failure cases of our method. ([**Appendix D.5, Lines 1065-1068**], [**Appendix D.5, Fig.9 Lines 1080-1095**])\n- We corrected errors in the Model Efficiency table: the LHM efficiency value was computed using 40K points, not 160K. We replaced \"160 K\" with \"40K\" in the \"# Points\" column and added a new row reporting LHM's efficiency with 160K points. ([**Appendix D.6, Tab. 12, Lines 1102-1103**])\n- We conducted an ablation study to verify that the 3DGS representation plays a important role in 3D-aware Neural Rendering. ([**Appendix D.6, Lines 1173-1204**], [**Appendix D.6, Fig.13 Lines 1206-1216**])\n- We added a figure illustrating human reconstructions in the canonical space (12 cases). ([**Appendix D.7, Lines 1219-1221**], [**Appendix D.7, Fig.14 Lines 1242-1295, the 24 page**])\n\n\n\nFor the specific concerns and questions raised by the reviewers, we have provided detailed and structured point-by-point responses under the corresponding comments.\n\n\n\nSincerely,\n\nLHM++ Authors"}}, "id": "rMIAr3aW9c", "forum": "tryFvnBKjy", "replyto": "tryFvnBKjy", "signatures": ["ICLR.cc/2026/Conference/Submission597/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission597/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission597/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763714113580, "cdate": 1763714113580, "tmdate": 1763714113580, "mdate": 1763714113580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LHM++, a network that reconstructs an animatable digital avatar in a feed-forward pass taking arbitrary number of images as inputs. It reduces the time cost of LHM by adopting token \"merge\" and \"unmerge\" operations. The token \"merge\" operation merges similar image tokens. By reducing the number of tokens, it speeds up the attention computations.\n\nAnother contribution is the neural renderer. Instead of rendering the predicted Gaussian directly, it renders the feature map in 2D and uses a DPT head to predict from the 2D feature map.\n\nThe modification in the number of tokens significantly speeds up the inference with more image inputs and reduces the memory cost. Meanwhile, the method outperforms the existing methods in terms of loose clothes animation due to the neural renderer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper demonstrates the ability to animate loose clothes and generalization, which is challenging in human rendering.\n* With \"merge\" and \"unmerge\", the model runs much faster than LHM with a lower cost in memory.\n* The paper is clearly written and highlights the contributions."}, "weaknesses": {"value": "* The paper claims that in LHM, the time complexity of self-attention operations scales quadratically with the number of image tokens (and thus with the number of input images). Meanwhile, as the number of input images increases, image tokens begin to dominate the attention computation. Although the proposed “merge” and “unmerge” operations help reduce memory and computational overhead, the overall self-attention complexity remains quadratic with respect to the number of images. These operations only reduce the time cost by a constant factor.\n\n* The proposed LHM++ is presented as an improvement over LHM; however, it is unclear where this improvement originates. The main contributions of the paper appear to be (1) the PIT block with the “token merge” mechanism and (2) the neural renderer. Since the “merge” and “unmerge” operations inevitably introduce information loss, they are more likely to degrade rather than enhance visual quality. Additionally, the paper’s ablation study in the appendix shows that the neural renderer only marginally improves PSNR and SSIM. I would appreciate it if the authors could clarify in more detail where the performance gain from LHM to LHM++ in Table 3 comes from."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "modMhfkYSt", "forum": "tryFvnBKjy", "replyto": "tryFvnBKjy", "signatures": ["ICLR.cc/2026/Conference/Submission597/Reviewer_ckeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission597/Reviewer_ckeM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761541635278, "cdate": 1761541635278, "tmdate": 1762915561270, "mdate": 1762915561270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LHM++: AN EFFICIENT LARGE HUMAN RECONSTRUCTION MODEL FOR POSE-FREE IMAGES TO 3D. Overall, the proposed method is well-motivated, and the experimental results seems good."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe paper is well-written with a logical structure that makes the technical contributions easy to follow.\n•\tThe proposed  framework is reasonable and well-justified. The experimental results convincingly demonstrate the effectiveness of the approach across different scenarios.\n•\t The demo videos are excellent supplementary materials."}, "weaknesses": {"value": "Could you please give a discussion about the diffirence with 3D generation model, such like CLAY (Rodin). I wonder can we use the Rodin to perform 3D avatar generation and then perform auto-rigging such as Mixamo?"}, "questions": {"value": "please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oR4SYGl93V", "forum": "tryFvnBKjy", "replyto": "tryFvnBKjy", "signatures": ["ICLR.cc/2026/Conference/Submission597/Reviewer_MuBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission597/Reviewer_MuBq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754309354, "cdate": 1761754309354, "tmdate": 1762915560824, "mdate": 1762915560824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LHM++, a feed-forward model to generate 3D human avatars from casually captured images. At the core of the method is a Encoder-Decoder Point-Image Transformer (PIT) module to fuse 3D and 2D features, which are decoded into 3D Gaussian parameters. The authors conducted experiments on different dataset to verify the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow.\n\nSynthesizing 3D/4D humans from images is an interesting task with practical applications.\n\nThe method is technically sound by leveraging a multimodal transformer architecture to fuse 3D and 2D feature for 3D Gaussian generation."}, "weaknesses": {"value": "Limited technical contribution. This paper is an extension for LHM, and the main difference is that LHM++ replaces the MBHT with PIT mode. However, both MBHT and PIT fuse 3D and 2D features for 3D Gaussian prediction. Does the LHM support multiple image processing by fusing multiple images using MBHT architecture? Why is the PIT required, and how does it outperform MBHT?\n\nThe paper proposes that the PIT architecture improves the results, whereas the results in Tab. 10 suggest that the number of 3D geometric points has a bigger impact on the results, i.e., for 40K points, LHM-0.7B even performs better. It’s not clear whether the PIT architecture or the number of query points improves the results.\n\n\nThe paper proposes DPT-head as the final renderer. In this case, why is the 3DGS representation required? Is it possible to just predict the SMPL offsets for LBS instead of the full 3DGS parameters?"}, "questions": {"value": "How does the method decouple the belongings (e.g., the bag in Fig. 2) and human clothing? \n\nDetails about the implementation. Are the Gaussian rendering and neural rendering jointly trained? The Eq. 8 loss is not clear. Are both the RGB and perception loss applied for Gaussian rendering and neural rendering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IviTepIIJ1", "forum": "tryFvnBKjy", "replyto": "tryFvnBKjy", "signatures": ["ICLR.cc/2026/Conference/Submission597/Reviewer_3ReR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission597/Reviewer_3ReR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002096278, "cdate": 1762002096278, "tmdate": 1762915560638, "mdate": 1762915560638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}