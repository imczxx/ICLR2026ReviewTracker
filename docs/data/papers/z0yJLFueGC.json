{"id": "z0yJLFueGC", "number": 13123, "cdate": 1758213827215, "mdate": 1759897462853, "content": {"title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning", "abstract": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model’s recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets—including AMC, AIME, and IMO-style problems—demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2$\\times$ and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.", "tldr": "", "keywords": ["Reinforcement Learning", "Curriculum Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8635931c860e9bff332c4d84ab96b3039dbdd222.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ADARFT, a method that enhances both the efficiency and final accuracy of RFT through adaptive curriculum learning. ADARFT dynamically adjusts the difficulty of training problems based on recent reward signals, thereby accelerating learning by maintaining an optimal difficulty range. Experimental results demonstrate consistent and positive improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Compared with existing online filtering schemes that repeatedly sample and discard data until rewards fall within a target range, ADARFT reduces computational overhead by estimating the difficulty level directly rather than relying on extensive resampling."}, "weaknesses": {"value": "The algorithm calibrates the target difficulty so that the target reward is beta (often set to 0.5). However, focusing solely on samples with a reward around 0.5 may be problematic. When all samples have reward higher than 0.5, the algorithm will be slow to make progress on those higher reward samples. A more effective approach might involve selecting samples whose rewards fall within a broader range, such as 0.2–0.8, and using a soft selection rule to maintain diversity and stability during training."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LYzjy5zG6o", "forum": "z0yJLFueGC", "replyto": "z0yJLFueGC", "signatures": ["ICLR.cc/2026/Conference/Submission13123/Reviewer_mKvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13123/Reviewer_mKvd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504594701, "cdate": 1761504594701, "tmdate": 1762923847350, "mdate": 1762923847350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ADARFT (Adaptive Curriculum Reinforcement Finetuning) to improve the sample efficiency of reinforcement finetuning for LLMs. ADARFT first estimated a static difficulty of each problem using an auxiliary model. Then, it dynamically updates a target difficulty during training and prioritizes problems whose static difficulty is close to this target, aiming to focus training on tasks that are challenging yet solvable. On mathematical reasoning datasets, ADARFT achieves moderate improvements compared with baseline methods."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis linking reward variance to learning signal strength (Section 3.4) provides a rationale for targeting a 50% success rate.\n- The results section includes a thoughtful discussion on when curriculum learning is most beneficial (e.g., under imbalanced data distributions)."}, "weaknesses": {"value": "- The rationale behind the core design lacks justification. The comparison between a static per-task difficulty and a dynamically updated target difficulty is questionable. In practice, the actual problem difficulty $d_i$ evolves as the model is refined, while the statistical meaning of the target difficulty T remains ambiguous. Moreover, since the target difficulty update relies on the average success rate of non-uniformly sampled data, it contains a biased estimate of the overall success rate, further complicating its interpretation.\n- The proposed difficulty estimation method—based on a precomputed difficulty score for each problem—has limited practical value: (1) The difficulty estimate is static, whereas actual problem difficulty evolves with the refined model. (2) It requires a separate model carefully matched to the difficulty distribution of each dataset. (3) To estimate difficulty, the authors pre-generate multiple responses for every data sample (in the main experiment, n = 128), which incurs significant computational cost.\n- The sampling algorithm and the target difficulty update equation are highly empirical and introduce many hyperparameters. However, the overall performance gains from using ADARFT are not significant (Table 2).\n- For experiment results, there is no analysis of sampling behavior or training dynamics beyond performance metrics.\n- Estimating problem difficulty is part of the method and should not be presented in the experimental section (Section 4.1)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MuGhPF05Ty", "forum": "z0yJLFueGC", "replyto": "z0yJLFueGC", "signatures": ["ICLR.cc/2026/Conference/Submission13123/Reviewer_sAx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13123/Reviewer_sAx2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910271463, "cdate": 1761910271463, "tmdate": 1762923847091, "mdate": 1762923847091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to implement a difficulty-based curriculum over prompts in RL finetuning of LLMs on reasoning tasks, which should in principle improve sample efficiency by avoiding wasted inference on tasks that are too easy or difficult. The specific method, AdaRFT, works by assigning a fixed difficulty score to each problem, maintaining an online target difficulty estimate, and sampling the closest B problems to that target difficulty at each RL step. The target difficulty is updated online so that the empirical mean reward is close to a target value of 0.5. The authors provide some weak theoretical and empirical evidence that this value of 0.5 is ideal. To estimate the difficulty of each problem offline, the authors propose two methods: 1. use fail@1 (i.e. 1 - pass@1) over n=128 attempts using a fixed Qwen 2.5 MATH 7b model, or 2. prompting GPT-4o to assign a difficulty to each model, and show that there is a reasonable correlation between the two metrics. When tested on math reasoning tasks, the authors find that AdaRFT achieves noticeable improvements in training efficiency/accuracy as well as test accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The fundamental idea of an adaptive difficulty-based curriculum is definitely sound and merits investigation. The paper includes some nice experiments, including ones difficulty-skewed prompt distributions and model sizes that help provide understanding of in which scenarios the method helps the most. The paper is also very clearly-written and easy to read."}, "weaknesses": {"value": "I found the paper to be weak in three related aspects: missing baselines, overly simple method without comprehensive analysis of design choices, and relative lack of improvement. \n\n1. The idea of difficulty-based curriculum learning is not new, and multiple ideas have been proposed in the literature. Out of all of these, the authors evaluate only a single baseline: removing all prompts with initial model pass@k in [0, 0.1] and [0.9, 1.0]. Simple baselines such as curriculum/prioritized sampling in the Kimi k1.5 report need to also be included to understand the contribution of the adaptive part of the method.\n\n2. The results are a bit incremental (+2%), especially on Qwen 2.5 7b models, even without comparing to reasonably strong baselines. One reason for why this might be the case is that the fixed/offline difficulty estimation seems like a poor design choice - why not use online empirical pass@1 to be the difficulty estimate for a problem? Difficulty estimation is fundamentally model-dependent, so using fixed base models to evaluate it seems like a poor choice."}, "questions": {"value": "1. Figure 3 should clearly state it is training accuracy. \n2. I would like to see test or validation accuracy curves over training as well (i.e. Figure 3), instead of just final performance.\n3. I'd suggest removing GSM8K, MATH-500, and AMC23 from the test sets (old/contaminated) and adding AIME 25."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ThWN6cR5ip", "forum": "z0yJLFueGC", "replyto": "z0yJLFueGC", "signatures": ["ICLR.cc/2026/Conference/Submission13123/Reviewer_os5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13123/Reviewer_os5J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037450082, "cdate": 1762037450082, "tmdate": 1762923846835, "mdate": 1762923846835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adaptive curriculum strategy for reinforcement fine-tuning. It scores problem difficulty once (via model rollouts or an LLM rater), then, during training, dynamically targets a difficulty level whose success rate stays near 50% and updates this target using observed batch rewards. The training loop itself uses a standard policy-optimization setup; ADARFT only changes how batches are selected. Experiments on multiple math benchmarks with Qwen-family models show faster early progress and  final-accuracy gains versus baselines, with analyses suggesting the largest benefits when the dataset’s difficulty distribution is skewed or the model is relatively weak."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The studied problem is important. The motivation of this paper is clear.\n\n* The proposed method is compatible to different RL methods.\n\n* This paper is well-written."}, "weaknesses": {"value": "* This paper does not compare against many existing curriculum-learning methods for reinforcement learning, despite a growing body of existing works (see references below). The related work section also listed many existing RL curriculum learning methods, but they are not compared in the experiments. The baselines used in the experiments are naive PPO and PPO with filtering data, which makes it hard to compare the proposed method and other advanced curriculum learning methods. A better evaluation should includes more existing RL curriculum learning methods.\n\n* Although the method is in principle orthogonal to the choice of RL optimizer, the empirical evidence focuses almost entirely on PPO. Results for other RFT variants (e.g., GRPO, Reinforcement++) appear only in Fig. 7 and are based on a single run each. It is suggested to expand these experiments with more RFT methods on different settings.\n\n* The experiments use only Qwen models on math reasoning benchmarks. Although Qwen is widely-used open-sourced model family, recent works point out that it may have exposure to some math benchmark data (Wu et al.), which could influence results. It is suggested to broaden coverage to additional domains (e.g., coding) and include more model families to make the empirical results more comprehensive.\n\n* This paper highlights the proposed methods is more efficient than existing methods, however, it requires additional rollouts to estimate the difficulities of all samples. Whlie the average time cost for each step is discussed, it does not discuss the cost from the additional rollouts for difficulity estimation. Although an LLM-judge shortcut is proposed to reduce cost, Fig. 6 shows a notable performance drop versus rollout-based difficulity labels. Please provide end-to-end time cost including the difficulity estimation phase.\n\n\nZhang et al., Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation. EMNLP 2025.\n\nTzannetos et al., Proximal Curriculum for Reinforcement Learning Agents. TMLR 2023.\n\nParashar et al., Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning\n\nWu et al., Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9CkJPHl6XQ", "forum": "z0yJLFueGC", "replyto": "z0yJLFueGC", "signatures": ["ICLR.cc/2026/Conference/Submission13123/Reviewer_pDAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13123/Reviewer_pDAr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160501459, "cdate": 1762160501459, "tmdate": 1762923846510, "mdate": 1762923846510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}