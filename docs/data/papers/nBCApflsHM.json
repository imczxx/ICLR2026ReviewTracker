{"id": "nBCApflsHM", "number": 6631, "cdate": 1757990913058, "mdate": 1759897904007, "content": {"title": "RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging", "abstract": "Model merging aims to combine task-specific models into a unified model that is capable of multi-tasking, without any computational overhead of re-training. Regression Mean (RegMean), an approach that formulates model merging as a linear regression problem, aims to find the optimal weights for each linear layer in the merge model by minimizing the discrepancy in predictions between the merge and candidate models. RegMean provides a precise closed-form solution for the merging problem; therefore, it offers explainability and computational efficiency. However, RegMean merges each linear layer independently, overlooking how the features and information in the earlier layers propagate through the layers and influence the final prediction in the merge model. In this paper, we introduce RegMean++, a simple yet effective alternative to RegMean, that explicitly incorporates both intra- and cross-layer dependencies between merge models' layers into RegMean's objective. By accounting for these dependencies, RegMean++ better captures the behaviors of the merge model. Extensive experiments demonstrate that RegMean++ consistently outperforms RegMean across diverse settings, including in-domain (ID) and out-of-domain (OOD) generalization, sequential merging, large-scale tasks, and robustness under several types of distribution shifts. Furthermore, RegMean++ achieves competitive or state-of-the-art performance compared to various recent advanced model merging methods.", "tldr": "Enhancing the effectiveness and generalization capabilities of  Regression Mean for model merging.", "keywords": ["Model Merging", "Image Classification", "Regression Mean for Model Mering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c68f2b139d7e4a53ec2da35798405318bfe7ca4.pdf", "supplementary_material": "/attachment/3f522f705959cfe3c6b5d54ad4f2d6c67e51d9be.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of model-merging: combining multiple task-specific models into a unified model without retraining. The authors engage with an existing method called RegMean (Regression Mean) which treats each linear layer in the merge model independently to minimize prediction error. They argue that RegMean neglects cross-layer and intra-layer dependencies (i.e., how features propagate through layers and how earlier layers affect later ones). This paper makes the following contributions:\n\n\n* A new method called RegMean++ which extends RegMean by explicitly incorporating both intra-layer dependencies and cross-layer dependencies into the regression objective (i.e., modelling how layer outputs and later layers interact).\n\n\n* Extensive empirical evaluation showing that RegMean++ outperforms RegMean across diverse settings: in-domain (ID) and out-of-domain (OOD) generalization, sequential merging (adding tasks one after another), large-scale tasks, and robustness under various distribution shifts. \n\n* A demonstration that this methodology competes with or even beats recent advanced model-merging methods (beyond just the baseline RegMean) in some cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors identify a meaningful shortcoming in RegMean: the layer-wise independence assumption ignores downstream effects and feature propagation. The articulation of this gap is clear.\n\n\n\n* According to the submission summary, the method is evaluated across multiple axes (ID vs. OOD, sequential merging, large-scale tasks, robustness) and consistently outperforms the baseline (RegMean)."}, "weaknesses": {"value": "* While the regression‐mean merging paradigm is attractive, it implicitly assumes linear relationships between the candidate models and the merged model’s weights. The world of deep networks is highly nonlinear and features propagate in complex ways; how valid is the linear regression assumption in deep networks? Extra discussion needed.\n\n\n* The paper claims to model intra‐ and cross‐layer dependencies, but it may be unclear exactly how much additional modelling is done (e.g., are inter‐layer weights estimated, correlation structures learned?) and how scalable that is to very deep architectures.\n\n\n* It would strengthen the work to include analyses of when RegMean++ fails, e.g., tasks that are very dissimilar or when layer dependencies are minimal, or whether the benefit is marginal in some cases—and what trade‐offs exist."}, "questions": {"value": "* Compared with baseline RegMean (which is relatively cheap), how much extra computation or memory cost does RegMean++ incur (both at merging time, and at inference time if relevant)? Are there any deployment concerns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xxv82AC4Uy", "forum": "nBCApflsHM", "replyto": "nBCApflsHM", "signatures": ["ICLR.cc/2026/Conference/Submission6631/Reviewer_tzZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6631/Reviewer_tzZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622435867, "cdate": 1761622435867, "tmdate": 1762918948893, "mdate": 1762918948893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of model-merging: combining multiple task-specific models into a unified model without retraining. The authors propose RegMean++ which treats each linear layer in the merge model independently to minimize prediction error. The authors show that existing approach neglects cross-layer and intra-layer dependencies. The paper's contributions can be summarized as following:\n\n\n* The authors propose RegMean++ which extends RegMean by explicitly incorporating both intra-layer dependencies and cross-layer dependencies into the regression objective.\n\n\n* The authors conduct extensive experiments, showing that  RegMean++ outperforms RegMean across diverse model merge settings: including in-domain (ID) and out-of-domain (OOD) generalization, sequential merging, large-scale tasks, and robustness under various distribution shifts. \n\n* The experiments demonstrate significant improvements compared to state-of-art approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors identify a meaningful shortcoming in RegMean: the layer-wise independence assumption ignores downstream effects and feature propagation, illustrating a research gap in existing researches.\n\n\n\n* According to the submission summary, the method is evaluated across multiple axes (ID vs. OOD, sequential merging, large-scale tasks, robustness) and consistently outperforms various model merge approaches."}, "weaknesses": {"value": "* While the regression‐mean merging techniques are reasonable, it implicitly assumes linear relationships between the candidate models and the merged model’s weights. However, in practice, the relations among different neural networks are highly nonlinear. Does the linear regression assumption still hold in deep networks? \n\n\n* The paper identify model intra‐ and cross‐layer dependencies, but it may be unclear that what is the performance that when scaling the method  to very deep architectures. Does the proposed method still work in this case?\n\n\n* It would be better to include analysis and discussions on the theoretical conditions that when RegMean++ would or would not work."}, "questions": {"value": "* Compared with existing baseline approaches, it would be better to discuss how much more memory/computation cost does RegMean++ need, e.g., extra merging time? This may provide a more comprehensive evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xxv82AC4Uy", "forum": "nBCApflsHM", "replyto": "nBCApflsHM", "signatures": ["ICLR.cc/2026/Conference/Submission6631/Reviewer_tzZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6631/Reviewer_tzZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622435867, "cdate": 1761622435867, "tmdate": 1763649691357, "mdate": 1763649691357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called RegMean++ for multi-task model merging, which is built upon the previous RegMean work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper points out that RegMean considers only single-layer information during merging, while ignoring the role of cross-layer information flow.\n- The paper compares the proposed method with existing approaches in terms of accuracy, out-of-distribution generalization, and performance under distribution shift scenarios."}, "weaknesses": {"value": "- This paper is an improvement based on RegMean, and the main difference lies in the input features; thus, its novelty is limited.\n- The paper only validates the method on ViT architectures and simple image classification tasks, lacking verification on LLMs and text generation tasks.\n- The experiments do not provide a comparison of time costs among different model merging methods.\n- In Table 2, it is unclear why the proposed method outperforms RegMean on OOD data; this conclusion lacks deeper analysis or theoretical explanation."}, "questions": {"value": "In Table 1, the original paper reports 85.86 for TSV-M and 86.3 for ISO-C, while this paper reports 83.1 and 82.5, respectively. What causes this discrepancy in the reported results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OhfK7acYwy", "forum": "nBCApflsHM", "replyto": "nBCApflsHM", "signatures": ["ICLR.cc/2026/Conference/Submission6631/Reviewer_uBzW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6631/Reviewer_uBzW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724237916, "cdate": 1761724237916, "tmdate": 1762918948502, "mdate": 1762918948502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an improvement over RegMean. However, it lacks clear motivation, presents limited novelty, and provides an insufficiently explained methodology."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Extensive and well-controlled empirical campaign: Main benchmark covers 8 diverse image tasks, 3 model sizes, and 11 baselines. Sustainability under scale tested up to 20 tasks and sequential arrival. Robustness evaluated on seven corruption types and class-imbalanced or ImageNet-OOD samples."}, "weaknesses": {"value": "1)This paper lacks clear motivation, presents limited novelty, and provides an insufficiently explained methodology.\n2)No theoretical grounding for the “corrected” statistics: The manuscript claims RegMean++ “incorporates intra- and cross-layer dependencies” but offers no proof or even informal argument that using merged-model activations minimises a meaningful objective that couples layers (Sec. 3.1–3.2). Consequently, convergence, optimality, or error bounds with respect to the true multi-task risk are absent.\n3)Computational cost brushed aside: RegMean++ needs an extra forward pass through the growing merged model for every layer to collect statistics (Algorithm 1, line 5). No wall-clock or FLOP comparison is given; the abstract claims “no computational overhead of re-training” but omits this non-trivial overhead relative to RegMean.\n4)Scalability to larger models unaddressed: All experiments use ViT-B/32, B/16, L/14 (≤303 M params). The limitation section itself flags billion-scale models as future work, so current evidence does not support generalisability to LLM or large multimodal scenarios.\nCode and checkpoints are not yet released, which limits immediate reproducibility."}, "questions": {"value": "What is the motivation of this paper, and what are its main contributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vWPQCV3Eq1", "forum": "nBCApflsHM", "replyto": "nBCApflsHM", "signatures": ["ICLR.cc/2026/Conference/Submission6631/Reviewer_sT1A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6631/Reviewer_sT1A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933023450, "cdate": 1761933023450, "tmdate": 1762918948115, "mdate": 1762918948115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RegMean++, an extension of RegMean that augments the merging objective with both intra-layer and cross-layer dependency modeling. The method is evaluated across diverse scenarios—ID and OOD settings, sequential merging, and merging with corrupted data—and is accompanied by a detailed layer-wise importance analysis of the merging process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[1] The paper is clearly written. The idea of using representations from the merged model—rather than the originals—is a neat and effective trick that substantially boosts merging performance.\n\n[2] The evaluation is comprehensive, covering multiple vision benchmarks and providing thoughtful analyses."}, "weaknesses": {"value": "[1] The study is confined to vision tasks. Demonstrating results on LLMs—where model merging is widely practised—would significantly strengthen the paper’s impact.\n\n[2] While RegMean++ advances over RegMean, the sequential-merging results should also be compared to state-of-the-art methods tailored for this setting to establish competitiveness."}, "questions": {"value": "(1) The reported performance for SOTA baselines such as Iso-C [1] and TSV-M[2] appears considerably lower than in their original papers. Is this due to a different set of model checkpoints being used? If so, could you verify whether the same trends hold when evaluating on the exact checkpoints used in those works?\n\nReferences:\n\n[1] Marczak, D., Magistri, S., Cygert, S., Twardowski, B., Bagdanov, A. D., & van de Weijer, J. (2025). No task left behind: Isotropic model merging with common and task-specific subspaces. arXiv preprint arXiv:2502.04959.\n\n[2] Gargiulo, A. A., Crisostomi, D., Bucarelli, M. S., Scardapane, S., Silvestri, F., & Rodola, E. (2025). Task singular vectors: Reducing task interference in model merging. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 18695-18705)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9wT1WtkTF1", "forum": "nBCApflsHM", "replyto": "nBCApflsHM", "signatures": ["ICLR.cc/2026/Conference/Submission6631/Reviewer_cLJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6631/Reviewer_cLJb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6631/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989080873, "cdate": 1761989080873, "tmdate": 1762918947684, "mdate": 1762918947684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}