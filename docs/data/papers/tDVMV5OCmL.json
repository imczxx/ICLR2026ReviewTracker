{"id": "tDVMV5OCmL", "number": 16959, "cdate": 1758270588243, "mdate": 1759897207625, "content": {"title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "abstract": "Not only the classical methods of neural network pruning but also most importance-based pruning methods rely too much on parameter magnitudes to prune effectively. We propose a novel pruning strategy, named IPPRO, using projective space to alleviate the unfair advantage given to parameter magnitudes. We use gradient of loss in the projective space to construct PROscore, which is a magnitude-indifferent score that is in turn used by IPPRO, our novel importance-based structured pruning algorithm. Extensive experiments on Convolutional Neural Networks (CNNs), Vision Transformers (ViT), and Large Language Models (LLMs) demonstrate that IPPRO consistently outperforms, especially in high compression scenarios. Our results establish IPPRO as a task-agnostic and architecture-agnostic pruning paradigm, offering both a new theoretical foundation and a practical tool for magnitude-indifferent structured pruning.", "tldr": "", "keywords": ["structured pruning", "importance criteria", "projective geometry", "model compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bc75394e4bd0cae1f473377faea82d597ace545.pdf", "supplementary_material": "/attachment/25022ff0469c5fb0c4615a4a7c25052245d42659.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes IPPRO (Importance-based Pruning with PRojective Offset), a magnitude-indifferent structured pruning method for neural networks. The key innovation lies in using projective geometry to evaluate filter importance through angular displacement under gradient descent, rather than relying on filter magnitudes. \n\nThe authors introduce PROscore, computed as the tangent of angular distance in projective space after one gradient descent step, as the importance criterion. Extensive experiments demonstrate IPPRO's effectiveness across CNNs, Vision Transformers, and LLMs, showing consistent performance improvements especially in high compression scenarios and without fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The projective geometry framework provides a principled alternative to magnitude-based heuristics. \n\n2. Testing on 7+ architectures (ResNet, MobileNet, DeiT, EfficientFormer, DeepLabV3, LLaMA).\n\n3. Robust performance without fine-tuning."}, "weaknesses": {"value": "1. Why should angular distance in projective space correspond to filter importance? Is there a theoretical justification that angular distance is better than geometric distance or norm? A formal theorem would strengthen the contribution. It would also be important to theoretically justify why angular distance is superior, rather than merely demonstrating that it performs better.\n\n2. No wall-clock time comparisons with baselines. This is important for pruning papers.\n\n3. The performance improvement is marginal. For example, in Table 2 (Cityscapes – DeepLabV3-ResNet50), when comparing IPPRO (ours) with SIRFP (Wu et al., 2025), the improvement in mIoU is very small. Under the base pruning setting, SIRFP achieves 81.3 mIoU, while IPPRO reaches 81.5 mIoU, giving only a +0.2 gain. In terms of FLOPs reduction, IPPRO obtains 61.8 %, just 0.5 % higher than SIRFP’s 61.3 %. This slight edge shows that the performance improvement is marginal, i.e., the proposed method performs almost on par with the previous state-of-the-art under the same compression ratio.\n\n4. No analysis on modern architectures (e.g., Swin Transformer). \n\n5. Experiments on Imagenet should be put in the main text, not supplementary."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CIsllmGJhw", "forum": "tDVMV5OCmL", "replyto": "tDVMV5OCmL", "signatures": ["ICLR.cc/2026/Conference/Submission16959/Reviewer_8L8K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16959/Reviewer_8L8K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916139036, "cdate": 1761916139036, "tmdate": 1762926979859, "mdate": 1762926979859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IPPRO (Importance-based Pruning with PRojective Offset), a structured pruning framework that overcomes the limitations of magnitude-based pruning by focusing on directional importance. The method embeds each filter into a projective space, where importance depends on direction rather than scale, ensuring scale-invariant comparison across layers. A new PROscore measures how much each filter’s direction changes under a gradient step, and filters that move closer to the origin are pruned. A simple parameter injection trick allows this computation without affecting model outputs. Experiments on CNNs, Vision Transformers, and LLMs show that IPPRO achieves slightly better accuracy and greater stability than previous direction-based pruning methods even without fine-tuning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "**1. Clear and principled formulation of scale invariance through projective geometry**\n\nThe paper formalizes magnitude-independent pruning not as heuristic normalization but as a property of the underlying space. By embedding filters into a projective space, the method guarantees scale invariance at the definition level, providing a clean and mathematically grounded justification for direction-based importance.\n\n**2. Unified pruning framework applicable across architectures**\n\nThe same projective formulation applies consistently to CNNs, Vision Transformers, and large language models. This unification makes the approach architecture-agnostic and highlights that the proposed importance measure is not tied to a specific model design or normalization scheme.\n\n**3. Comprehensive empirical validation across diverse tasks**\n\nExperiments cover image classification, semantic segmentation, and language modeling, using models such as ResNet-50, DeepLabV3, DeiT, and LLaMA-2-7B. The breadth of evaluation supports the claim that IPPRO is a general framework rather than a task-specific trick.\n\n**4. Robustness and fine-tuning-free performance**\n\nIPPRO maintains accuracy even when computed with limited data sampling and performs competitively without any fine-tuning after pruning. This property makes the method practical for large-scale models where retraining is expensive or infeasible.\n\n**5. High clarity and reproducibility**\n\nThe paper is well-organized, with precise notation, clear pseudo-code, and detailed ablations that enhance transparency. The paper’s presentation quality and completeness make reproduction straightforward and strengthen the empirical credibility of the results."}, "weaknesses": {"value": "**1. Direction-based pruning has been extensively explored in prior work**\n\nSeveral recent methods such as Torque, Catalyst, and geometric pruning already focus on gradient direction rather than magnitude. IPPRO provides a cleaner mathematical reformulation but does not introduce a fundamentally new optimization insight.\n\n**2. Limited theoretical gain from adopting projective geometry**\n\nAlthough the paper frames pruning in the language of projective geometry, the practical effect largely reduces to normalizing vectors and measuring angular displacement. The framework adds elegant terminology but yields little new theoretical understanding beyond existing direction-based normalization schemes.\n\n**3. Increased computational overhead with limited performance improvement**\n\nComputing PROscores requires additional gradient accumulation and parameter injection, which significantly increases pruning cost. However, the resulting accuracy gain over previous direction-based methods is minimal, raising concerns about the efficiency–benefit trade-off.\n\n**4. Unclear behavior in hybrid architectures without manual layer-wise control**\n\nWhile the method is claimed to be unified across CNNs and Transformers, it is unclear how well the approach generalizes to mixed architectures such as ConViT or hybrid CNN–ViT models when global pruning is applied without manually setting layer-wise ratios. This raises questions about the true level of architectural unification achieved by the framework."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CRnV74ZUfc", "forum": "tDVMV5OCmL", "replyto": "tDVMV5OCmL", "signatures": ["ICLR.cc/2026/Conference/Submission16959/Reviewer_VazZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16959/Reviewer_VazZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950503297, "cdate": 1761950503297, "tmdate": 1762926979503, "mdate": 1762926979503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop an architecture-agnostic pruning algorithm (although it is not entirely clear how it would work on a simple MLP) that is based on ideas from projective geometry. Concretely, the method uses a score called PROscore (defined in Eq (4)) involving a modified version of the model under consideration that includes some fresh parameters (parameter injection, the meaning of which isn’t entirely clear) and the gradient of the loss function of this modified model.\n\nThe authors then explain how to use this score function for pruning in the case of CNNs, vision transformers and LLMs and proceed to show that the method works well in a very wide range of experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Based on the empirical section of the paper, the method clearly works very well.\n- The experiments and results are convincing and well put together.\n- The scoring function (4) is interesting."}, "weaknesses": {"value": "- The paper starts from the following axiom: when it comes to pruning a NN, the idea that the magnitude of the parameters of the NN is important is a myth (143-145), and “magnitude-invariant” methods must be developed. There is a mathematical and a motivational problem with this perspective:\n\t- Mathematical: What operation are involved in a forward pass through a NN? Lots of additions and multiplications, ReLU and softmax to name the most important. Addition, multiplication by positive numbers, ReLU and softmax are isotone in their arguments, multiplication by negative numbers is antitone; in particular they are all monotone. Therefore, inputs with bigger magnitudes (i.e. absolute values) produce outputs with bigger magnitudes. Thus magnitude matters at a very fundamental level in any NN. The onus is therefore on the authors to substantiate their claim that the importance of magnitude is a myth on the face of this very basic mathematical fact. Which brings me to the second point,\n\t- Motivational: one cannot do science by simply stating that a certain way of doing things is a myth and taking another approach. If magnitude is not, or less, important than one might think, this must be documented and explained. This paper doesn't do either.\n- Despite the claim that the proposed method is “magnitude-invariant” (a term that is never defined precisely), the proposed solution is very much magnitude dependent. The expression (4) -- which has a typo, it should $\\lVert F_i\\rVert$ in the denominator, not $D_i$ -- is large when the numerator $\\lVert F_i-\\lambda\\nabla_{F_i}\\mathcal{L} \\rVert$ is large, i.e. the updated $F_i$ with learning rate $\\lambda$ has a large magnitude, and the denominator $\\lvert \\lVert F_i\\rVert-\\lambda\\frac{\\partial\\mathcal{L}}{\\partial D_i} \\rvert$ is small (the interpretation of this term is more complicated, see below). In which sense is this magnitude-invariant?\n- The expression (4) could have been written without any reference to projective space and projective geometry. It really plays no role in this story. Eq (4) provides a gradient-based method with an unusual denominator, and this denominator is the heart of the story. \n- I wish more time and care had been spent on this denominator $\\lvert \\lVert F_i\\rVert-\\lambda\\frac{\\partial\\mathcal{L}}{\\partial D_i} \\rvert$. To start to understand it one must jump to the next section to find the definition of $\\frac{\\partial\\mathcal{L}}{\\partial D_i}$ (this is not a great way to present things…). But this is not enough to understand the meaning of this term. What exactly is $\\psi$ in (5)? What does “modifying element-wise computation layer $\\sigma$” mean? If $\\sigma(x)$ returns a tuple of dimension N and $x$ is a tuple of dimension $M\\neq N$ how are we supposed to understand (5)? Since $\\frac{\\partial\\psi}{\\partial D_i}=x$ (and the second derivative will therefore vanish), what can we say about the general shape of $\\frac{\\partial\\mathcal{L}}{\\partial D_i}$ from the chain rule? And what does this mean for the proposed method and the meaning of the denominator of (4)?\n- The paper is littered with grammatical errors, making it quite hard to read in some places."}, "questions": {"value": "If the proposed method does work better than other pruning strategies, it looks like it is due to the denominator in (4) which penalises certain behaviours. Which behaviours and how? What is the role of $\\lambda$ (and how is it chosen)? What is the intuition behind $\\frac{\\partial\\mathcal{L}}{\\partial D_i}$? What rate of change does it measure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FmlsGitO27", "forum": "tDVMV5OCmL", "replyto": "tDVMV5OCmL", "signatures": ["ICLR.cc/2026/Conference/Submission16959/Reviewer_z984"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16959/Reviewer_z984"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762352862795, "cdate": 1762352862795, "tmdate": 1762926979117, "mdate": 1762926979117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a structured pruning approach for neural nets, that uses projective space. This attempts to address some of the shortcomings of magnitude based pruning. It projects the filters on latent space and then notices its movement during gradient descent. An importance score is then computed leveraging the movement information, which in turn forms the basis for pruning the filters."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The only strength of the article, in my opinion, is that it attempts to address a very timely problem is neural net."}, "weaknesses": {"value": "The papers has several major weaknesses. \n\n1. First and foremost, the description of the proposed method is very poor. It is very difficult to understand what is going on. I do not think a reader would be able to implement the algorithm from the description given in the paper. Specifically, I do not find the following critical information. \n  A) When does it stop taking the filters out from the net? \n  B) What happens if all filters are removed from a layer and how does the approach handle layer collapse? \n  C) Does it need pre-trained model always? \n\n2. Projective geometry seems the key idea of the paper. Yet, there is hardly any clarity in the paper why and how does it help. The description seems too superficial and cursory. \n\n3. Algorithm 1 does not add any value, in my opinion. Rather the authors should use the space to better justify why and how of projective geometry. \n\n4. Results: Finally, the results are nowhere close to the state of the art. For example, on CIFAR 10, CURL (Neural network pruning with residual-connections and limited-data, 2020), SPvR (SPvR: Structured Pruning via Ranking , 2025), Hrank ( Hrank:Filter pruning using high-rank feature map, 2020), OTOv2 achieves the same performance as the proposed method with 40% (absolute) lesser parameters. Overall, I think the proposed approach uses much more parameters than the state of the art models to achieve the same performance."}, "questions": {"value": "I have no additional question. In weakness section, I have detailed the issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AuBGaKzSNz", "forum": "tDVMV5OCmL", "replyto": "tDVMV5OCmL", "signatures": ["ICLR.cc/2026/Conference/Submission16959/Reviewer_4m1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16959/Reviewer_4m1G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762692715871, "cdate": 1762692715871, "tmdate": 1762926978663, "mdate": 1762926978663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}