{"id": "S2X9A4q06U", "number": 17800, "cdate": 1758280644786, "mdate": 1759897152888, "content": {"title": "Efficient Prior Selection in Gaussian Process Bandits with Thompson Sampling", "abstract": "Gaussian process (GP) bandits provide a powerful framework for performing blackbox optimization of unknown functions. The characteristics of the unknown function depend heavily on the assumed GP prior. Most work in the literature assume that this prior is known but in practice this seldom holds. Instead, practitioners often rely on maximum likelihood estimation to select the hyperparameters of the prior - which lacks theoretical guarantees. In this work, we propose two algorithms for joint prior selection and regret minimization in GP bandits based on GP Thompson sampling (GP-TS): Prior-Elimination GP-TS (PE-GP-TS) and HyperPrior GP-TS (HP-GP-TS). We theoretically analyze the algorithms and establish upper bounds for the regret of HP-GP-TS. In addition, we demonstrate the effectiveness of our algorithms compared to the alternatives through experiments with synthetic and real-world data.", "tldr": "We propose, analyze and evaluate two algorithms for joint prior selection and regret minimization based on Gaussian process Thompson sampling.", "keywords": ["Multi-armed bandits", "Gaussian processes", "Thompson sampling", "Hyperparameter optimization", "Bayesian optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d03c5ad79514a6beb356cb4717b6a11f731db789.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analysis two algorithms for Bayesian Optimisation with unknown prior: PE-GP-TS - a combination of Thompson Sampling and existing PE-GP-UCB and HP-GP-TS - a combination of Fully-Bayesian treatment of unknown prior and Thompson Sampling. Authors provide an incomplete regret bound for PE-GP-TS, leaving one term unbounded. They then proceeded to develop an incomplete bound on HP-GP-TS following the analysis of Srinivas et al, with one term unbounded, and another bound independent of MIG of the kernel following the analysis of van Roy and Russo. They then proceed to empirically evaluate the proposed algorithm on a number of benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper address the topic of BO with unknown prior, which is an important, understudied topic."}, "weaknesses": {"value": "**All algorithms are straightforward combinations of existing ones**\n\nTo me it seems, like PE-GP-TS is just PE-GP-UCB [3] with the acquisition function changed from UCB to Thompson Sampling, as such the novelty of that approach is marginal, at best. When it comes to HP-GP-TS, this is again, a combination of Thompson Sampling and well-known Fully Bayesian BO [2], where a hyper prior is used. In fact, Thompson Sampling with hyper prior was already proposed by prior work [1]. As such, I think the paper offers little to no novelty when it comes to proposed algorithms.\n\n**Theoretical results not particularly strong and incomplete**\n\nTheorem 4.3 leaves one term unbounded being the sum of predictive variances at the optimal point under the true prior. As the optimal point might never get queried, it is likely impossible to bound this term, meaning it could, in principle be linear. Although the term is inside square root, there is one more $T$ inside it, so together that would mean there is a linear $\\mathcal{O}(T)$ outside of the square root, rendering the whole bound trivial.\n\nTheorem 4.5 again leaves one crucial term unbounded, and there are further results bounding its growth. As such, this term could in principle, grow linearly and thus make the whole bound trivial. Without further analysis, I find little to no value in such a result.\n\nTheorem 4.6 does not depend on the MIG of the kernel nor on any other properties of the kernel except for scale. This means the scaling of this bound is same no matter if the true objective was sampled from the simplest slowly-varying kernel or from an extremely complex kernel with short length-scale. As such, such a bound must be very worst-case, which is reflected in scaling with $|\\mathcal{X}|$. The whole problem with choosing the right prior/kernel for BO is that if we choose prior/kernel that is too simple, then we risk functional mismatch, whereas if we choose one that is too complex, we risk slow convergence. Thus, omitting the kernel complexity from the analysis misses the point entirely . \n\nAs such, unfortunately, I believe none of the Theorems provides a useful, complete bound for any of the algorithms.\n\n**Empirical results not particularly useful**\n\nAll the experimental settings are crafted so that there is only a finite number of priors to choose from, whereas in reality e.g. in case of unknown lengthscale, there space of possible priors corresponding to possible lengthscale is continuous and infinite. I understand that previous work [3] also considered discrete number of priors, however, it seems to me that was a \"first line of work\" on this topic and focused primarily on novel theoretical results. As such, I would expect the following work in the stream to \"raise the bar higher\", either by providing stronger theoretical guarantees (which as I discussed before I do not think the paper does) or by making the setting more realistic and easily applicable in practice, e.g. by lifting some assumptions used for theoretical analysis and comparing with the strongest empirical baselines in literature, such as MetaBO[4], transferBO[5] and self-correcting BO[6]. As I believe the paper does neither of these two things, I believe it is incremental at best. \n\n**References**\n\n[1] Hong, Joey, et al. \"Thompson sampling with a mixture prior.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\n[2] De Ath, George, Richard M. Everson, and Jonathan E. Fieldsend. \"How Bayesian should Bayesian optimisation be?.\" Proceedings of the Genetic and Evolutionary Computation Conference Companion. 2021.\n\n[3] Ziomek, Juliusz, Masaki Adachi, and Michael A. Osborne. \"Time-Varying Gaussian Process Bandits with Unknown Prior.\" arXiv preprint arXiv:2402.01632 (2024).\n\n[4] Maraval, Alexandre, et al. \"End-to-end meta-bayesian optimisation with transformer neural processes.\" Advances in Neural Information Processing Systems 36 (2023): 11246-11260.\n\n[5] Tighineanu, Petru, et al. \"Transfer learning with gaussian processes for bayesian optimization.\" International conference on artificial intelligence and statistics. PMLR, 2022.\n\n[6] Hvarfner, Carl, et al. \"Self-correcting bayesian optimization through bayesian active learning.\" Advances in Neural Information Processing Systems 36 (2023): 79173-79199."}, "questions": {"value": "- Why do you study standard (frequentist) regret for PE-GP-TS, but Bayesian regret for HP-GP-TS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2PWlszegdw", "forum": "S2X9A4q06U", "replyto": "S2X9A4q06U", "signatures": ["ICLR.cc/2026/Conference/Submission17800/Reviewer_RXxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17800/Reviewer_RXxU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756123325, "cdate": 1761756123325, "tmdate": 1762927644928, "mdate": 1762927644928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Publication of code"}, "comment": {"value": "As discussed in the reproducibility statement, we have made the code for the experiments available in an anonymous repository [here](https://anonymous.4open.science/r/Prior-Selection-GP-Bandits-C95D)."}}, "id": "p74YiEJm85", "forum": "S2X9A4q06U", "replyto": "S2X9A4q06U", "signatures": ["ICLR.cc/2026/Conference/Submission17800/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17800/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17800/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763032126450, "cdate": 1763032126450, "tmdate": 1763032126450, "mdate": 1763032126450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses Gaussian Process (GP) bandits where the prior is unknown. To mitigate the over-exploration of existing UCB methods (e.g., PE-GP-UCB), the authors propose two Thompson Sampling (TS) algorithms. PE-GP-TS adapts prior elimination to TS. HP-GP-TS is a Bayesian approach using bi-level sampling: sampling a prior from a hyperposterior, then sampling a function."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. HP-GP-TS outperforms PE-GP-UCB. Crucially (Fig. 5), its regret does not scale with $|P|$, a significant practical advantage.\n2. The analysis shows theoretical advantages: dependence on average MIG (Thm 4.5) and a tighter $\\mathcal{O}(\\sqrt{T})$ bound via information-theoretic analysis (Thm 4.6).\n3. Fig. 4 clearly demonstrates that UCB methods fixate on flexible priors to maximize optimism, leading to over-exploration, while TS methods identify the true prior more accurately."}, "weaknesses": {"value": "1. for PE-GP-TS (Critical). Theorem 4.3 includes the term $\\sqrt{... \\sum_{t}\\sigma_{t,p^{*}}^{2}(x^{*})}$. This depends on the cumulative posterior variance at the optimum $x^*$. If $x^*$ is rarely sampled, this variance remains high, implying linear regret $\\mathcal{O}(T)$.\n2. HP-GP-TS has a cumulative complexity of $\\mathcal{O}(T^{4}|P|)$, which is prohibitive for long horizons.\n3. The UCB-based bound (Thm 4.5) includes an unbounded term representing the cost of learning the prior in the general case."}, "questions": {"value": "1. How is $\\sum_{t}\\sigma_{t,p^{*}}^{2}(x^{*})$ bounded in Thm 4.3? Please clarify how this avoids linear regret for PE-GP-TS.\n2. Given the $\\mathcal{O}(T^4|P|)$ complexity, are there approximations to mitigate the computational cost of HP-GP-TS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H1WNLiytzC", "forum": "S2X9A4q06U", "replyto": "S2X9A4q06U", "signatures": ["ICLR.cc/2026/Conference/Submission17800/Reviewer_zenE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17800/Reviewer_zenE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898201915, "cdate": 1761898201915, "tmdate": 1762927643983, "mdate": 1762927643983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Bayesian optimization with an unknown prior known to be one of a finite number of candidates.  First a prior elimination + Thompson Sampling approach is given complementing an existing prior elimination + UCB approach.  Then a fully Bayesian approach is given based on Thompson sampling.  The main contributions are regret analyses for these, and experiments are also given."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an important topic that probably deserves more attention.  The paper is well written and the mathematical analysis seems to be sound (though I didn't check the appendices closely).  One of the results hints at a result with average information gain instead of worst-case, which would be a good contribution (but see below for non-minor caveats).  The experiments are also appreciated."}, "weaknesses": {"value": "I have potentially significant concerns with each of the the main results:\n- Theorem 4.3 has an extra sum of variance terms at $x^*$ which does not appear in regular analyses.  Moreover, the authors do not comment on this extra term, and do not provide any comparison to PE-GP-UCB (where it seems there is no such term).  I suspect this term could be linear in $T$, or at least hard to prove it's sublinear.\n- Similarly, Theorem 4.5 contains a sum of differences of U values that looks potentially linear in $T$ or at least hard to bound.  Again, no comment is made on this term.\n- Lemma 4.4 is a bit complicated / hard to grasp, and I'm not sure of its purpose.  If the purpose is just for the 4 lines of discussion after, then I think a fair bit of editing should be done to make its meaning and role clearer.\n- Theorem 4.6 looks quite weak in the sense that it matches what I would expect if the $|\\mathcal{X}|$ arms were all independent without correlations.  Exploiting correlations is crucial in GP settings, and one is almost always interested in settings with much more domain points than the time horizon.\n\nSince this paper is primarily theoretical, I view these concerns as quite significant.\n\nSome less important notes:\n- Finite $P$ is quite restrictive, but OK since handling continuous sets might be out of reach for now.\n- The authors mention linear bandits, and the reader might question whether any of these can be \"kernelized\" (I guess not easily)\n- First line of Section 4.1: I don't know what \"larger\" means (larger than what?)"}, "questions": {"value": "The weaknesses above have implicit questions.  The main one is -- can you prove the extra terms in Theorems 4.3 and 4.5 are sublinear, at least in certain specific scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CT4az3JRgo", "forum": "S2X9A4q06U", "replyto": "S2X9A4q06U", "signatures": ["ICLR.cc/2026/Conference/Submission17800/Reviewer_J9PU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17800/Reviewer_J9PU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17800/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762477304760, "cdate": 1762477304760, "tmdate": 1762927643598, "mdate": 1762927643598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}