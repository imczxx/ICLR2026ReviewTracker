{"id": "eZO38vANPM", "number": 11595, "cdate": 1758202261465, "mdate": 1763727295558, "content": {"title": "IMSE: Intrinsic Mixture of Spectral Experts Fine-tuning for Test-Time Adaptation", "abstract": "Test-time adaptation (TTA) has been widely explored to prevent performance degradation when test data differ from the training distribution.\nHowever, fully leveraging the rich representations of large pretrained models with minimal parameter updates remains underexplored.\nIn this paper, we propose a novel approach, Intrinsic Mixture of Spectral Experts (IMSE), that leverages the spectral experts inherently embedded in Vision Transformers. \nWe decompose each linear layer via singular value decomposition (SVD) and adapt only the singular values, referring to each decomposed rank-1 component as a spectral expert while keeping the singular vectors fixed.\nWe further identify a key limitation of entropy minimization in TTA: it often reduces feature variance, causing the model to rely on domain-specific cues rather than class-discriminative features. \nTo address this, we propose a diversity maximization loss based on singular vector–input alignment, which maximizing diversity of response pattern.\nIn the continual test-time adaptation (CTTA) scenario, beyond preserving pretrained knowledge, it is crucial to retain and reuse knowledge from previously observed domains. We introduce Domain-Aware Spectral Code Retrieval, which estimates input distributions to detect domain shifts, and retrieves adapted singular values for rapid adaptation.\nExtensive experiments show that our method achieves state-of-the-art performance on ImageNet-C/R/A under single-domain TTA. In CTTA, it improves accuracy by 3.4pp with 2,000$\\times$ fewer trainable parameters.", "tldr": "We propose a test-time adaptation method that leverages the intrinsic spectral structures of pretrained Vision Transformers, addressing underexplored challenges in both TTA and CTTA.", "keywords": ["test time adaptation", "continual learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eac8213dd0e2a5b88e3a392678f553056965e92b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces IMSE (Intrinsic Mixture of Supervised Experts), a test-time adaptation method that decomposes linear layers in a pretrained model using singular value decomposition (SVD), treating each rank-1 component as a spectral expert. During adaptation, only the singular values are updated while the bases remain fixed, allowing parameter-efficient updates. The method combines entropy minimization with confidence-based filtering, a diversity maximization loss to promote balanced expert activation, and a domain descriptor mechanism for detecting distribution shifts and retrieving previously adapted spectral parameters from a memory bank."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of spectral decomposition to form a mixture-of-experts representation is an interesting approach to test-time adaptation."}, "weaknesses": {"value": "1. The assumption that spectral experts can generalize across domains by only adapting singular values lacks theoretical justification and is not thoroughly evaluated in challenging domain-shift scenarios.\n2. The interplay between the different components (e.g., diversity loss, entropy minimization, and domain memory) is complex, but the paper lacks sufficient ablation studies to isolate their individual contributions.\n3. The scalability and stability of the domain memory mechanism under long-term continual adaptation are not fully explored, leaving questions about its robustness in practice."}, "questions": {"value": "1. he framework assumes that adapting only the singular values of pretrained linear layers is sufficient for capturing domain shifts. Could the authors provide theoretical or empirical evidence supporting this assumption, especially for shifts that may require new basis directions?\n2. The method combines entropy minimization, diversity regularization, and domain memory. Have the authors performed detailed ablations to assess the individual and joint impact of each component? For example, how does performance change when the diversity loss is removed?\n3. The KL-divergence-based domain descriptor mechanism is used to trigger new adaptations and retrieve stored parameters. How robust is this approach under gradual or noisy distribution shifts? What mechanisms are in place to prevent over-segmentation or memory bloat when many small shifts occur?\\\n4. The diversity regularization loss penalizes widespread activation of individual spectral experts. Could this unintentionally suppress task-relevant features that happen to be common in a new domain? How does the method balance between promoting diversity and retaining important features?\n5. Could the authors discuss whether the IMSE framework or its components (e.g., diversity loss, spectral adaptation) can be extended to multi-source domain generalization or continual learning settings where test-time updates are not allowed?\n6. While the method is evaluated on common TTA benchmarks, it would be helpful to include results on tasks involving more gradual or subtle domain shifts, or with more fine-grained domain boundaries. Could the authors include such evaluations in a revision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nCNzgO4rW6", "forum": "eZO38vANPM", "replyto": "eZO38vANPM", "signatures": ["ICLR.cc/2026/Conference/Submission11595/Reviewer_Q6jF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11595/Reviewer_Q6jF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696356144, "cdate": 1761696356144, "tmdate": 1762922677279, "mdate": 1762922677279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the test-time adaptation (TTA) problem by leveraging the knowledge in pretrained networks. Specifically, it decomposes each linear layer via singular value decomposition (SVD) and adapts only the singular values. For continual test-time adaptation (CTTA), the method estimates input distributions to detect domain shifts and retrieves previously adapted singular values for rapid adaptation. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using $\\text{Std}_{i}^{(l)}$ to measure whether each expert captures domain-specific patterns is an interesting design.\n\n2. Detecting new domains by comparing the current input-level descriptor $ϕ$ with the accumulated descriptor $\\phi_{(t)}$ is an effective way.\n\n3. This paper analyzes, in an unsupervised manner, that entropy minimization tends to capture domain-related rather than class-discriminative information."}, "weaknesses": {"value": "1.  In lines 161–163, since $\\mathbf{v}_i^{(l)}$ fixed, it is unclear how it can define each spectral expert’s response. Could the authors provide a more detailed explanation of this point.\n\n2. The paper conducts experiments only on three OOD datasets. Could authors also evaluated the method on classical domain adaptation benchmarks such as Office-Home[1] or DomainNet[2]?\n\n3. The paper introduces two loss components, entropy minimization $L_{entmin}$ and diversity maximization $L_{dm}$, but lacks an ablation study isolating their individual contributions. Could the author show the effect of using only entropy minimization, only diversity maximization, and their combination to better understand each term’s impact on adaptation performance.\n\n4. The paper does not include experiments on time analysis (e.g., runtime efficiency) or memory analysis (e.g., computational cost and storage of spectral codes). Could authors provide such experiments to better demonstrate the efficiency of the proposed method?\n\n[1] Venkateswara, Hemanth, et al. \"Deep hashing network for unsupervised domain adaptation.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[2] Peng, Xingchao, et al. \"Moment matching for multi-source domain adaptation.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019."}, "questions": {"value": "Please see the above weaknesses. If you can conduct additional experiments to further evaluate your method, I would be willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mlgFDm5gFu", "forum": "eZO38vANPM", "replyto": "eZO38vANPM", "signatures": ["ICLR.cc/2026/Conference/Submission11595/Reviewer_wHid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11595/Reviewer_wHid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819630614, "cdate": 1761819630614, "tmdate": 1762922676836, "mdate": 1762922676836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Intrinsic Mixture of Spectral Experts, a novel framework for test-time adaptation. IMSE updates only the singular values while keeping the singular vectors fixed, thus enabling parameter-efficient adaptation. To prevent the entropy minimization objective from collapsing feature diversity, the authors introduce a diversity maximization loss based on spectral vector–input alignment. For CTTA, they propose a Domain-Aware Spectral Code Retrieval mechanism, which stores domain-specific singular values and retrieves them according to domain similarity to mitigate forgetting. Experiments on ImageNet-C  and CLIP  backbones show that IMSE achieves state-of-the-art results while using fewer trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper correctly identifies two practical problems in TTA: the overfitting tendency of entropy minimization and catastrophic forgetting in CTTA.\n2. The idea of reinterpreting linear layers as mixtures of rank-1 “spectral experts” is interesting. It provides a compact and interpretable way to perform fine-grained adaptation.\n3. Updating only singular values makes IMSE computationally efficient and easily applicable to existing pretrained models such as ViT or CLIP.\n4. The reported improvements on ImageNet-C and the low parameter count are convincing, and the experiments are well organized."}, "weaknesses": {"value": "1. How robust is the retrieval process when encountering a completely new domain that differs substantially from all stored domains?\n2. Updating only singular values implicitly constrains parameter updates to a diagonal submanifold of the low-rank space. Does this constraint reduce optimization expressivity when confronted with severe domain shifts?\n3. How much extra computational cost is introduced by SVD?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o8GraPplnp", "forum": "eZO38vANPM", "replyto": "eZO38vANPM", "signatures": ["ICLR.cc/2026/Conference/Submission11595/Reviewer_4qVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11595/Reviewer_4qVn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990832167, "cdate": 1761990832167, "tmdate": 1762922676504, "mdate": 1762922676504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IMSE (Intrinsic Mixture of Spectral Experts), a new framework for continual test-time adaptation (CTTA). IMSE decomposes each linear layer in Vision Transformers using singular value decomposition (SVD), interpreting the orthogonal basis matrices as a mixture of spectral experts and the diagonal matrix of singular values as spectral weights. During adaptation, IMSE fine-tunes only these spectral weights while keeping the orthogonal bases fixed, enabling highly parameter-efficient adaptation.\n\nTo guide unsupervised adaptation, the authors propose a combined loss function that integrates entropy minimization with a diversity maximization term to prevent feature collapse. IMSE achieves state-of-the-art performance on ImageNet-C, -R, and -A benchmarks, outperforming strong TTA baselines such as TENT, SAR, and DPAL while requiring significantly fewer trainable parameters.\n\nFor continual TTA, the paper further introduces a Domain Bank that stores domain-specific spectral codes and descriptors, enabling retrieval and reuse of prior adaptations to mitigate catastrophic forgetting. Comprehensive ablation studies demonstrate the contribution of each proposed component."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: IMSE offers a novel interpretation of test-time adaptation through the lens of spectral experts, extending SVD-based parameter-efficient fine-tuning (e.g., LoRA, SVDiff) to the unsupervised TTA setting. The idea of freezing orthogonal bases and adapting only singular values is conceptually simple yet highly effective.\n\nEfficiency and Practicality: By updating only singular values, IMSE achieves orders-of-magnitude parameter reduction (~2000× fewer trainable parameters) while maintaining or exceeding SOTA performance.\n\nContinual Adaptation Innovation: The introduction of the Domain Bank provides a simple but powerful mechanism for mitigating catastrophic forgetting across sequential domains.\n\nComprehensive Ablation Analysis: The paper includes well-designed ablation studies that isolate the impact of each component."}, "weaknesses": {"value": "Domain Bank scalability:\nUsing simple KL divergence over mean–variance descriptors might struggle to discriminate fine-grained domain shifts, potentially leading to incorrect retrievals.\n\nNo direct measure of forgetting:\nAlthough the Domain Bank is designed to mitigate catastrophic forgetting, the paper does not include quantitative evidence, e.g. Backward Transfer (BWT), to verify that performance on previously adapted domains remains stable after subsequent adaptations."}, "questions": {"value": "Could the authors quantify the computational cost of performing full SVDs across all linear layers in large-scale ViTs (e.g., CLIP or MAE)? Have they considered using truncated or randomized SVD to reduce preprocessing overhead, and if so, how would this affect adaptation performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zzuRwqZh4L", "forum": "eZO38vANPM", "replyto": "eZO38vANPM", "signatures": ["ICLR.cc/2026/Conference/Submission11595/Reviewer_23Fr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11595/Reviewer_23Fr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11595/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193551742, "cdate": 1762193551742, "tmdate": 1762922675804, "mdate": 1762922675804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}