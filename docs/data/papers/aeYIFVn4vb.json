{"id": "aeYIFVn4vb", "number": 19402, "cdate": 1758295932827, "mdate": 1759897041112, "content": {"title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries", "abstract": "Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.", "tldr": "We propose future summary pretraining to better capture long range dependencies and enhance reasoning capabilities of LLMs", "keywords": ["Teacher Forcing", "Multi-Token Prediction", "Pretraining", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d488ba376836da091bbbf70457271e3406fa19de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes an improved multi-token prediction objective for LLMs. Instead of just predicting the next token, or next few tokens, the method proposes to predict some summary of the future tokens. The summary can either be a simple bag of words set, or a learned embedding from another transformer that encodes the future. They show at the 3B and 8B scale, that future summary prediction outperforms conventional multi-token approaches in a variety of reasoning tasks (ARC, GSM8K, etc.) as well as on some pedagogical tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting idea of predicting a learned summary of the future, instead of raw tokens, to make representations capture more of the future.\n- Good evaluations of their multi-token prediction objective against other multi-token prediction baselines, at somewhat large scales (3B-8B) and over serious reasoning benchmarks.\n- Paper is generally well written."}, "weaknesses": {"value": "**This method has increased param / training budget, while baselines do not**\n\nThis method requires the training of an additional encoder, RevLM, and the experiments do not mention the additional size of this encoder or the computational cost incurred from using it. \n\n There should be some baseline that is fair in the number of training flops / parameters, since your method gets access to an additional encoder. For example, you should train the MTP baseline with the same number of parameters / flops as your method.\n\n\n**Learned Future Summary is still susceptible to failure modes** \n\nIt's also not clear to me how the learned future summary is \"adaptive\", in the sense that the future summary embedding is being trained on a \"fixed\" objective of previous token prediction, given the future. I suspect it's possible to come up with some adversarial sequence for the RevLM to destroy its representation. \n\nHere's one speculative idea. Imagine we also have a stargraph where the paths lead to other stargraphs.  The forward LM starts in the origin of the 1st stargraph, and the RevLM starts at the origin of another stargraph at the end of the path of the first stargraph. Then the RevLM will learn a shortcut solution for predicting the previous token, and making the learned future summary useless. \n\n### Minor\nSibling discovery task: the evaluation metric is vague, where the method's convergence speed is shown to be faster than NTP. But what is the definition of a converged method, is there a raw metric like accuracy score? The relative convergence speed to NTP convergence is not useful if NTP is bad, for example."}, "questions": {"value": "Aside from my concerns above,\n\nIs there a connection between this objective and JEPA / SSL approaches, where we are trying to enforce similarity between a view of the context, and a view of the future?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7DA6nPFyg", "forum": "aeYIFVn4vb", "replyto": "aeYIFVn4vb", "signatures": ["ICLR.cc/2026/Conference/Submission19402/Reviewer_WozN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19402/Reviewer_WozN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761099361009, "cdate": 1761099361009, "tmdate": 1762931321760, "mdate": 1762931321760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Future Summary Prediction (FSP) to address the limitations of next-token prediction (NTP) and multi-token prediction (MTP). The authors introduce two variants of FSP and conduct experiments on 3B and 8B models, demonstrating consistent improvements over NTP and MTP across mathematical reasoning, general reasoning, and coding benchmarks.\n\nKey Reasons:\n1. The paper presents a well-motivated and principled pretraining objective that addresses genuine limitations of existing approaches.\n2. The interpretability of the learned representations and the computational cost of the approach are insufficiently analyzed.\n\nSupporting Arguments\n\nFSP-RevLM achieves consistent gains across multiple benchmarks, validating the empirical effectiveness of the method. However, the underlying mechanism—why future summary prediction improves model performance—remains underexplored."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed FSP shifts the modeling focus from predicting individual tokens to capturing the global structure of future content—a fresh and principled direction for language modeling.\nExperiments at the 3B and 8B scales are comprehensive, with detailed comparisons against NTP, MTP, and DeepSeek-MTP across multiple benchmarks."}, "weaknesses": {"value": "Although FSP-RevLM achieves strong empirical results, the learned future representations lack interpretability, making it unclear what specific future information is being encoded.\nThe computational cost of training the reverse language model is not discussed in detail. The paper does not include comparisons of training time or resource consumption."}, "questions": {"value": "1. What is the computational overhead of training the reverse language model? Can it be jointly optimized with the main model?\n2. Have you considered applying FSP to other popular open-source LLM architectures?\n3. Why does FSP-RevLM slightly underperform NTP on GSM8K? Have you analyzed task-specific failure cases?\n4. Do you plan to evaluate FSP on larger models (e.g., 70B) or alternative architectures (e.g., MoE, RWKV)?\n5. Is there a way to interpret or visualize the learned future summaries from RevLM to better understand the captured information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1N2QGX6sHQ", "forum": "aeYIFVn4vb", "replyto": "aeYIFVn4vb", "signatures": ["ICLR.cc/2026/Conference/Submission19402/Reviewer_YWeo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19402/Reviewer_YWeo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531741640, "cdate": 1761531741640, "tmdate": 1762931321386, "mdate": 1762931321386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents two methods for improving transformer training.  One predicts the future bag-of-words with an additional head (FSP-BCE) and another predicts the hidden representation of a backwards running transformer (FSP-RevLM).  There are some results on path-star, sibling discovery, and 8b scale pre-training.  I think this is a nice topic, but I see a few issues with the paper.  First, the presentation of two very different methods gives the papers a somewhat disjointed feel.  The advantages and disadvantages of each method feel like they are not discussed enough.  For example, on some datasets, FSP-BCE will converge to predicting a marginal distribution over all the tokens in the dataset, if the sequences are sufficiently long and diverse.  In this case, it seems like it would not be very useful.  It also seems like this the effectiveness of this method will be sensitive to how tokenization is done.  For example, if we used a character-level model, it seems like it would be almost completely useless.  I don't think that's a decisive disadvantage by itself, but a paper focusing on this method exclusively could explore these tradeoffs.  \n\nAnother serious issue is that I believe that the RevLM method has been proposed before in the literature, (Serdyuk et. al 2017), and while the method here has some differences, it seems like the paper should be written so that the novelties in the proposed method can be properly highlighted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper deals with an important topic, which is how to move beyond the next-token prediction objective in a practical way.  I believe that the experiments are also correctly implemented and described in a clear way.  Some of the empirical improvements, especially with FSP-RevLM are also impressive."}, "weaknesses": {"value": "In my opinion, the biggest issue with this paper is that I don't believe that the FSP-RevLM method's novelty is correctly explained with respect to the existing literature.  There is a paper called Twin Networks, published in ICML 2017 (Serdyuk et. al), which I believe is essentially the same approach, of predicting the hidden state of a backward-trained RNN.  Now, there may be a difference here, in that this paper uses transformers instead of RNNs.  However, those differences should be discussed and this paper should be cited.  I found another paper, not cited in this work, called \"ProphetNet\" from (Qi et. al) in 2020 which predicts future n-grams, which also seems quite closely related to the bag-of-words prediction in this paper.    The z-forcing paper (Goyal et. al 2017) also predicts the hidden states of a backwards running RNN.  See section 3.3 in the arxiv paper.  The fact that this very relevant and fairly old work is not discussed, gives me concerns.  \n\nIt's less important, but there is a recent paper \"Joint Token Prediction\" (Ahn et. al 2025) which deals with learning belief states using a multi-token prediction method.  However, I am willing to accept this not being cited due to the work being quite recent.  \n\nAnother issue I have is that FSP-BCE seems like it could be sensitive to the horizon that you use, depending on how the dataset is constructed.  Perhaps empirically it's not an issue, but it seems like you can run into trouble with method if the sequences become long enough, and eventually the distribution over future tokens become similar (for example if our dataset is drawn from one very long sequence, then this would be an issue).  However, if the dataset splits all the sequences into different documents, perhaps this won't be an issue."}, "questions": {"value": "The paper has experiments on the \"path star\" task, but why didn't you also run on the more general (and much harder) star graph task?  \n\nWhat's the added computational cost of training the RevLM model on the right-to-left sequences?  Presumably it's twice the number of total parameters used during training and a little over twice the cost to train?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FbHbEQuVGD", "forum": "aeYIFVn4vb", "replyto": "aeYIFVn4vb", "signatures": ["ICLR.cc/2026/Conference/Submission19402/Reviewer_QvQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19402/Reviewer_QvQ8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881475784, "cdate": 1761881475784, "tmdate": 1762931320637, "mdate": 1762931320637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel pretraining objective called Future Summary Prediction (FSP). Instead of predicting specific future tokens, FSP trains an auxiliary head on the LLM to predict a single, compact summary of the long-term future of a sequence. This forces the model to develop a more global understanding and plan ahead, significantly reducing its reliance on myopic, shortcut-based learning.\n\nFSP augments the standard NTP loss with an auxiliary loss that predicts a summary of a long future window. The paper explores two ways to create this future summary:\nHand-crafted Summary (FSP-BCE): This is a simple but effective approach where the summary is a \"bag-of-words\" (a multi-hot vector) of all unique tokens that will appear in a long future window. The model is trained to predict the presence of these future tokens, without needing to know their exact order or position. This forces the model to look far ahead, making it much harder to rely on simple shortcuts.\nLearned Summary (FSP-RevLM): This is the paper's main and most powerful contribution. It addresses a weakness in handcrafted summaries, which can be \"noisy\" by including irrelevant future information. A separate reverse language model (RevLM) is trained to read sequences from right to left.The hidden state of this RevLM, after it has processed the future part of a sequence, serves as a rich, compact, and adaptive summary of that future. The main (forward) model is then trained to predict this learned summary vector. The RevLM naturally learns to emphasize what is important and predictable about the future, filtering out noise."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Novelty: This paper proposes a new dimension towards thinking about multi-token prediction in LLMs\nSuperior Performance on Benchmarks: At the 8B scale, FSP-RevLM consistently outperforms NTP, MTP, and DeepSeek-MTP across a range of benchmarks. It shows significant improvements on:\nReasoning: Up to a 5% absolute gain on ARC (AI2 Reasoning Challenge).\nMath: A 4.2% gain on MATH and 3.5% on GSM8K compared to standard MTP.\nCoding: Achieves the highest score on MBPP and is competitive on HumanEval+."}, "weaknesses": {"value": "The speedup due to this change is not mentioned in the main paper."}, "questions": {"value": "1. Does this technique only boost model accuracy or do you see any boost in inference speed per token by generating multiple tokens at a time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bayEfX4K35", "forum": "aeYIFVn4vb", "replyto": "aeYIFVn4vb", "signatures": ["ICLR.cc/2026/Conference/Submission19402/Reviewer_r6NQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19402/Reviewer_r6NQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19402/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959434140, "cdate": 1761959434140, "tmdate": 1762931320183, "mdate": 1762931320183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}