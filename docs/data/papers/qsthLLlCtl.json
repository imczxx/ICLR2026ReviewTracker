{"id": "qsthLLlCtl", "number": 17072, "cdate": 1758271826891, "mdate": 1763753848091, "content": {"title": "PRISM: Enhancing PRotein Inverse Folding through Fine- Grained Retrieval on Structure-Sequence Multimodal Representations", "abstract": "Designing protein sequences that fold into a target three-dimensional structure, known as the inverse folding problem, is central to protein engineering but remains challenging due to the vast sequence space and the importance of local structural constraints. Existing deep learning approaches achieve strong recovery rates, yet they lack explicit mechanisms to reuse fine-grained structure-sequence patterns that are conserved across natural proteins. \nWe present PRISM, a multimodal retrieval-augmented generation framework for inverse folding that retrieves fine-grained representations of potential motifs from known proteins and integrates them with a hybrid self-cross attention decoder. PRISM is formulated as a latent-variable probabilistic model and implemented with an efficient approximation, combining theoretical grounding with practical scalability. \nAcross five benchmarks (CATH-4.2, TS50, TS500, CAMEO 2022, and the PDB date split), PRISM establishes new state of the art in both perplexity and amino acid recovery, while also improving foldability metrics (RMSD, TM-score, pLDDT), demonstrating that fine-grained multimodal retrieval is a powerful and efficient paradigm for protein sequence design.", "tldr": "We present PRISM, a multimodal retrieval-augmented generation framework that enhances protein inverse folding by dynamically integrating fine-grained structure-sequence multimodal representations from a larger protein database.", "keywords": ["Retrieval Augmented Generation", "Protein Language Modeling", "Protein Inverse Folding", "Protein Sequence Design", "Multimodal Representation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0af5631617ebc82ae788f2020219b3a92425ec3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the protein inverse folding problem: designing a sequence for a target 3D structure. The authors identify that existing methods lack mechanisms to reuse conserved, fine-grained structure-sequence patterns. They propose PRISM, a retrieval-augmented generation (RAG) framework that retrieves fine-grained \"potential motif\" representations from a vector database. These retrieved embeddings are integrated with global backbone context using a hybrid self-cross attention (MHSCA) decoder. The method is formulated as a latent-variable probabilistic model. Experiments across five benchmarks show that PRISM achieves new state-of-the-art results in sequence recovery and perplexity, while also improving structural foldability metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates strong empirical results, consistently outperforming existing methods, including its own base model (AIDO.Protein-IF), across five challenging benchmarks (CATH-4.2, TS50, TS500, CAMEO 2022, PDB date split).\n\nThe application of a retrieval-augmented generation (RAG) framework to inverse folding at a fine-grained, residue-level (\"potential motif\") is a novel contribution. This provides an explicit mechanism to reuse conserved local patterns, which is a limitation in prior work\n\nThe paper includes an extensive set of ablations that validate the core design choices, including the impact of the number of retrieved entries, the contribution of the hybrid decoder, the effect of aggregation depth, and the saturation of the vector database. The analysis of the recovery-diversity trade-off is also a valuable addition."}, "weaknesses": {"value": "Presentation needs improvement. Some tables and figures overlap with text body, e.g., tab 3 and fig 3. Sec. 4.2 is missing. ‘Fig.’ and ‘Figure’ are both used in text body.\n\nSome baselines, like SPDesign and VFN-IFE, that report higher metrics (on CATH), are not compared.\n\nThe training objective (Eq. 20) optimizes the parameters of the aggregation module ($\\theta_Z$) and a structure encoder ($\\theta_B$). However, the joint encoder $\\mathcal{G}$ (identified as AIDO.Protein-IF) that produces the query embeddings $\\hat{\\mathcal{E}}^q$ and populates the database appears to be frozen, as the TopK retrieval step is non-differentiable. This prevents end-to-end training and means the model cannot learn to improve its representations for better retrieval; it can only learn to use the results from a fixed, and potentially suboptimal, retrieval set.\n\nThe \"PRISM (str. enc.)\" ablation (Tables 1, 2, 3) is vaguely described. The paper states this variant replaces the joint encoder with a \"purely structure-based encoder (ProteinMPNN-CMLM)\" and retrieval operates \"only over structural embeddings\". This seems to contradict the framework's core \"multimodal representation\" $\\mathcal{E} = \\mathcal{G}(B,S)$ (Eq. 5) and the definition of the database $D$. It is unclear how the multimodal vector database is constructed or queried in this \"structure-only\" setting."}, "questions": {"value": "Let’s denote the training set as A, the RAG database as B. The paper compares models trained on A and augmented with B, with models trained on A. However, since the information in B is also used, could you compare models trained on A and augmented with B, with models trained on A+B?\n\nCould you explain the independences for Eq. 3?\n\nIs the joint encoder $\\mathcal{G}$ (AIDO.Protein-IF) kept frozen during training? If so, have the authors considered methods to make the retrieval step differentiable (e.g., via relaxation) to enable end-to-end fine-tuning of the query encoder?\n\nAt inference, an \"initial sequence estimate $\\tilde{S}^q$\" is required to generate the query embedding $\\hat{\\mathcal{E}}^q$. How sensitive is the model's final performance to the quality of this initial sequence? For instance, what is the drop in AAR if $\\tilde{S}^q$ is generated by a weaker baseline like ProteinMPNN instead of AIDO.Protein-IF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rqXfxaI5eC", "forum": "qsthLLlCtl", "replyto": "qsthLLlCtl", "signatures": ["ICLR.cc/2026/Conference/Submission17072/Reviewer_CkVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17072/Reviewer_CkVi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554197181, "cdate": 1761554197181, "tmdate": 1762927081694, "mdate": 1762927081694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a RAG system for the protein inverse folding task. It uses AIDO.Protein-IF as a joint protein sequence-structure encoder to generate residue-level embeddings for every protein in the knowledge base, which are then used to construct a RAG system. During inference, an initial guess of the protein sequence is first generated from a base estimator, then used to index and retrieve top-k candidates from the RAG. The paper claims SOTA performance of their approach on various benchmarks compared to existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses the protein inverse folding problem, which is a core challenge in proteomics and biology. \n* The residue-level RAG system allows flexible reuse of local structure and sequence patterns. \n* The performance of the model is impressive on different benchmarks for its SOTA amino-acid recovery rate, perplexity and foldability."}, "weaknesses": {"value": "* The model relies heavily on the AIDO.Protein-IF model in both RAG system construction and inference. The not-so-significant improvement in performance of the proposed method compared to AIDO.Protein-IF makes the entire work more like a second-stage refinement rather than a completely novel end-to-end framework. \n* I suspect that the performance of the model might rely heavily on the quality of the initial guess from the base estimator AIDO.Protein-IF. This may be a particular problem when the retrieval system is at residue-level. The paper provides little further interpretation or ablation study on this matter."}, "questions": {"value": "Please address my two major concerns in the “Weaknesses” section first. I will reassess after the rebuttal. Two other miscellaneous questions are as follows: \n* The RAG system likely limits the generalization ability of the model on unseen protein backbones. Have you tested the performance of the model on novel or orphan proteins that are dissimilar to any known protein in the RAG? \n* How much disk space do we need to store residue-level embeddings for every protein in the knowledge base? And what size of RAM did you use for experiments in section 4.6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hlJjsgF02M", "forum": "qsthLLlCtl", "replyto": "qsthLLlCtl", "signatures": ["ICLR.cc/2026/Conference/Submission17072/Reviewer_1DYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17072/Reviewer_1DYG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834535849, "cdate": 1761834535849, "tmdate": 1762927081404, "mdate": 1762927081404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PRISM, a multimodal retrieval-augmented generation (RAG) framework for protein inverse folding. The key idea is to augment a generative inverse-folding model with fine-grained structure–sequence retrieval from a large protein database. Instead of relying solely on end-to-end transformer inference, PRISM retrieves localized structural motifs or fragments and conditions decoding through a hybrid self-cross-attention decoder that integrates both query (target structure) and retrieved (reference) contexts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem setup and latent-variable formulation provide theoretical grounding for the proposed method.\n- The ablation studies and runtime analysis provide clear evidence of the effectiveness of the proposed method."}, "weaknesses": {"value": "- The author did not provide the database construction process. What structures have been used in the retrieval? Even without exact overlap, similar folds from the same CATH family may leak local sequence priors, effectively making the task easier.\n- Equation (13): The $p(Z \\mid B, \\mathcal{R} )$ should be $p(Z \\mid B, \\mathcal{R} , \\mathcal{E})$, where $\\mathcal{E}$ is the encoding of the retrieved fragments.\n- The PGM derivation seems to prove something trivial. The actual training process is just a dense retrieval with some next-token prediction loss."}, "questions": {"value": "- Some of the figures have overlaps with the text, and some tables are too small to read.\n- Section 4.2 is empty.\n- The figures look too busy to read. You can either extend it to full size or remove some details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KgjEbia7Ev", "forum": "qsthLLlCtl", "replyto": "qsthLLlCtl", "signatures": ["ICLR.cc/2026/Conference/Submission17072/Reviewer_3CLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17072/Reviewer_3CLu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762076831339, "cdate": 1762076831339, "tmdate": 1762929544973, "mdate": 1762929544973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRISM, a novel framework for protein inverse folding.\nThe authors identify a key limitation in existing deep learning models: they lack an explicit mechanism to reuse conserved, fine-grained structure-sequence patterns (e.g., motifs) found in known proteins. To address this, PRISM is proposed as a multimodal retrieval-augmented generation (RAG) framework.\nThe core idea is to supplement end-to-end generation with a memory-based approach. The method works in several steps:\n(1) construct a vector database by encoding known protein structures and sequences into contextualized representations.\n(2) given a target 3D structure, leverage a base model to generate an initial sequence estimate, which is used to retrieve the vector database to obtain the most similar potential motifs.\n(3) a novel hybrid self-cross attention (MHSCA) decoder then integrates the backbone information with the retrieved motifs to generate the sequence.\nThe method is theoretically grounded as a latent-variable probabilistic model. Empirically, PRISM improves upon all baselines in both sequence accuracy (AAR) and structural foldability (RMSD and TM-score) across five major benchmarks (CATH-4.2, TS50, TS500, CAMEO 2022, and PDB date split). Extensive and thorough ablation and analysis demonstrate the effectiveness of the proposed modules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of explicitly retrieving fine-grained, residue-level motifs is an intuitive and novel solution to reuse the conserved local patterns which are central to protein function.\n2. The method demonstrates clear and consistent improvement over existing models across all five evaluation benchmarks. This improvement is shown not only in sequence recovery metrics but also in more practical foldability scores.\n3. The authors show that the significant improvement brought by retrieval-based method are achieved with a negligible runtime overhead (only a 14.3% compared to the base estimator), making PRISM a practical and scalable solution.\n4. The authors provide extensive ablation studies to justify their design choices. They successfully prove the contribution of the retrieval mechanism and improvement over varying sequence lengths, optimize the number of retrieved entries, validate the hybrid MHSCA decoder design, and analyze the effect of decoder depth."}, "weaknesses": {"value": "1. The main results in Table 1 should include structure-related metrics (e.g., scTM and RMSD) instead of relying solely on AAR and PPL for evaluation. Although scTM and RMSD are assessed in Table 4, this comparison is limited to DPLM2-3B and AIDO and lacks a broader set of baselines.\n2. Several critical details are missing from the main body text:\n- What protein library was the vector database built upon?\n- In Figure 2, do the structure embedding and joint embedding originate from the same model? Are they kept fixed during training?\n- A new term, \"base estimator,\" appears in line 315. I speculate it is used to estimate the initial sequence during inference. If so, the authors should explicitly clarify it.\n3. The sampling process relies on the \"base estimator\" to generate an initial sequence, which is then used for subsequent retrieval. This raises a key question: does the quality of this initial sequence significantly impact the final generated sequence? The authors have not provided an analysis of this potential dependency.\n4. The retrieval-based generation method may cause the generated protein sequences to be overly similar to natural proteins (as suggested by the high AAR of ~70% in Table 2), which will limit the novel protein design. Furthermore, it's uncertain if the method's performance will drop significantly when used on a novel backbone that not similar to any entries in the database.\n5. Minor issue: there is a formatting error where Table 3 overlaps with the main text."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rix9i8cGTa", "forum": "qsthLLlCtl", "replyto": "qsthLLlCtl", "signatures": ["ICLR.cc/2026/Conference/Submission17072/Reviewer_5aXk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17072/Reviewer_5aXk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293640790, "cdate": 1762293640790, "tmdate": 1762927080331, "mdate": 1762927080331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}