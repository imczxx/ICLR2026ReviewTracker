{"id": "OAXECnLxuk", "number": 16513, "cdate": 1758265438256, "mdate": 1763754402662, "content": {"title": "DaVinci: Reinforcing Visual-Structural Syntax in MLLMs for Generalized Scientific Diagram Parsing", "abstract": "Parsing raster-based scientific diagrams into structured representations is critical for editability and reusability. However, existing multimodal LLMs (MLLMs) struggle with the diverse visual primitives, complex structural layouts, and strict syntax involved. To address this, we introduce DaVinci, a novel MLLM that learns diagram parsing based on a two-stage framework—supervised learning of visual primitives followed by reinforcement learning of their structural relationships. Our model learns visual-structural syntax through supervised training on TikZ30K, a newly curated dataset of high-quality diagram-TikZ code pairs that features abundant visual primitives and structurally optimized drawing sequences. We further refine the model via reinforcement learning, guided by a hybrid reward function that jointly optimizes for visual fidelity, structural consistency, and code correctness. Extensive experiments show that DaVinci significantly outperforms existing open-source MLLMs and surpasses leading proprietary models like GPT-5 and Claude-Sonnet-4.", "tldr": "", "keywords": ["Scientific diagram parsing", "multimodal large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ea2724530863dd515859e4b2ecbcb43d69d68be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a new MLLM for synthesizing scientific figures as TikZ graphics programs conditioned on raster images. To facilitate this, the authors introduce a highly curated dataset of 30k TikZ images. Compared to related work, this dataset might seem small, but the automated curation process yields higher-quality training examples that lead to better downstream performance. The authors also introduce a new set of task-specific reward functions that can further improve the model via GRPO. Their final model outperforms both proprietary and open-weight baselines on a wide range of automated metrics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The central contribution of the work is the dataset and its curation pipeline. Through manual analysis, the authors identify shortcomings of existing datasets and improve their training examples through automated methods. These findings will be useful for future work (such as injecting comments as an \"inline\" reasoning trace).\n* Likewise, the defined reward functions seem effective and scalable and will be useful for future work.\n* In general, the paper is well-written and easy to follow."}, "weaknesses": {"value": "* The comparison between DaVinci and DeTikZify is not entirely fair, as DaVinci is based on a stronger base model. This means that conclusions about the effectiveness of the data curation strategy (l.375f) could also be due to this difference.\n* Even though the dataset is a central contribution, its creation is not entirely clear. For example, in l.154, the authors mention that they originally collect 360k TikZ snippets. It is not clear at which step these examples are filtered down to only 30k examples.\n* While the findings are interesting, there are few technical novelties presented."}, "questions": {"value": "* In l.61, Belouadi et al. use the MCTS algorithm primarily to find outputs with higher perceptual similarity, not to improve the compile success rate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IkS2920yAB", "forum": "OAXECnLxuk", "replyto": "OAXECnLxuk", "signatures": ["ICLR.cc/2026/Conference/Submission16513/Reviewer_zdpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16513/Reviewer_zdpU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794926702, "cdate": 1761794926702, "tmdate": 1762926603994, "mdate": 1762926603994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the timely topic of diagram-to-code generation by proposing a two-stage learning method: a supervised fine-tuning (SFT) \"cold start\" followed by GRPO post-training. It also explores two overlooked features, code ordering and descriptive comments. However, the paper's key findings lack support from rigorous experimental data, leading to ungrounded claims about the technical method's efficacy. Moreover, the dataset construction and evaluation methodology present a potential risk of bias, indicating that the work requires further improvement before its conclusions can be considered reliable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The proposed two-stage learning approach with SFT + GRPO is a timely fit for the field.\n\nS2. This work draws attention to the often-overlooked features of code ordering and descriptive comments in diagram-to-code generation.\n\nS3. The paper is well written, and the methodology is solid."}, "weaknesses": {"value": "W1. The main concern lies in the data construction process, which contains only 30k samples, whereas previous work (DATIkZ v1: 117k, v2: 360k, v3: 456k) used much larger datasets. Though the authors provide more details in Appendix C, key information is still missing. In line 795, they refer to 225k samples, but after stratified sampling by token length, only 58k remain (line 978), and it is unclear how the final 30k samples (line 149) were selected. Moreover, stratified sampling by token length may introduce bias, making the proposed dataset focus more on the long-tail data of the original dataset.\n\nW2. We encourage the authors to provide a concise and clear description of the data construction process in the main content, showing how 366,075 (line 764) valid pairs were reduced to the final TikZ-30k dataset. It is also necessary to provide comprehensive statistical analyses of the dataset at each step to ensure that no bias is introduced.\n\nW3. Bias in the evaluation method is another critical weakness. In line 338, the authors mention sampling 542 instances from 456,000 instances in DATIkZ v3. This small-scale test set cannot provide statistically convincing results, and the unclear sampling method raises concerns about potential distortion. Moreover, the authors should include evaluation metrics consistent with previous studies for better performance comparison.\n\nW4. The reviewer is curious about the “error-free precise feedback” (line 076); however, the authors only introduce it as “a method” (line 263) without experimental support or technical details.\n\nW5. Format/typos: \n- In Figure 1, the Tikz code is changing from “yellow” to “red” while the text is describing “from green to light red” in the top right part.\n- In Table 1, did not bold the best score for SigLIP"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kwln1kRxUm", "forum": "OAXECnLxuk", "replyto": "OAXECnLxuk", "signatures": ["ICLR.cc/2026/Conference/Submission16513/Reviewer_jdjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16513/Reviewer_jdjw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914984923, "cdate": 1761914984923, "tmdate": 1762926603648, "mdate": 1762926603648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the problem of parsing raster-based scientific figures into TikZ code. The authors introduce TikZ30K, a high-quality dataset constructed through extensive preprocessing using large language models (LLMs) and vision-language models (VLMs). The data curation pipeline includes low-quality data removal via VLMs, code reordering via LLMs, and comment injection to enhance planning capabilities. A Qwen2.5-VL multimodal LLM is fine-tuned using a two-step training pipeline consisting of supervised fine-tuning (SFT) followed by reinforcement learning (RL). The authors design novel reward functions evaluating textual information, geometric elements, image fidelity, and rendering success rate. The trained model is evaluated on a range of proprietary and open-source MLLMs, demonstrating that the SFT+RL approach outperforms baselines such as GPT-5 on automatic metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The data preprocessing pipeline is novel and intuitive. The use of VLMs for labeling and filtering, LLMs for code reordering, and comment injection for planning is well-motivated. Ablation studies confirm that reordering improves performance.\n- The reward design is comprehensive, capturing textual, geometric, and rendering aspects. The inclusion of ablations on different reward settings is appreciated.\n- The analysis provides interesting insights, particularly regarding reasoning vs. non-reasoning behavior and the observation that code similarity decreases while performance improves after RL. Evaluation includes both proprietary and open-source MLLMs."}, "weaknesses": {"value": "- It would be helpful to include a code efficiency metric, such as average token or character length, to better understand the complexity and verbosity of the generated TikZ code.\n- There is no information about the data timeframe or sourcing, which raises potential concerns about data contamination or leakage.\n- The lack of human evaluation is a major limitation. Relying solely on automatic metrics makes it difficult to assess the real-world quality and fidelity of generated figures. Even a small-scale human evaluation would substantially increase the credibility of the results and help correlate human judgments with automatic metrics."}, "questions": {"value": "- What is the exact difference between Reorder30K and TikZ30K? Is it simply the inclusion of comments introduced during data curation?\n- For DaTikZ-V3, is the preprocessing pipeline the same as TikZ30K? Also, is TikZ30K a subset of DaTikZ-V3, or was it sourced independently? Clarifying the dataset relationships and data collection process in the main paper would improve transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YqrevOhI2d", "forum": "OAXECnLxuk", "replyto": "OAXECnLxuk", "signatures": ["ICLR.cc/2026/Conference/Submission16513/Reviewer_1soJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16513/Reviewer_1soJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942396074, "cdate": 1761942396074, "tmdate": 1762926603244, "mdate": 1762926603244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two-stage learning framework for diagram generation using TikZ, consisting of supervised fine-tuning (SFT) followed by reinforcement learning (RL).\nIn the first stage, the authors investigate the effects of drawing order and comment placement, aspects that have not been thoroughly examined in previous studies.\nIn the second stage, they introduce a composite reward function in the RL phase that incorporates multiple factors, including image quality, code correctness, and structural consistency, and demonstrate its effectiveness through experimental results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors analyze previously overlooked properties of the data itself—specifically, the order in which data are generated and the use of comments within the code—and demonstrate their positive effects.\n- They further propose a method for defining the RL reward that incorporates text alignment, bounding-box consistency, and element matching directly extracted from the vector images."}, "weaknesses": {"value": "- Although a new dataset has been constructed, there is no description of its license information, making the details of the dataset’s licensing unclear.\n- The evaluation relies solely on automatic metrics, and no human evaluation is conducted. Since the proposed automatic metrics partially overlap with the reward function used in RL, verifying through human evaluation whether the improvement is genuinely meaningful—or merely a result of metric hacking—would greatly enhance the credibility and reliability of the results."}, "questions": {"value": "- In Section E.3 of the paper, which describes the training details, it is mentioned that the ViT and MLP projector are frozen during the SFT stage but unfrozen during the GRPO stage. Did you experiment with both configurations—freezing and unfreezing—in the GRPO stage, and find that unfreezing these components led to better performance?\n- Will the model, code, and dataset be released to the public?\n- The dataset is referred to as TikZ30K, but according to Appendix C.4, there are 58,000 training samples. Were the data further filtered during the optimization or post-verification steps? Does this mean that the final dataset actually consists of 30,000 samples?\n- The code augmentation process does not seem trivial for an LLM. The paper mentions that the procedure was repeated when rendering failed or when the rendered image was dissimilar to the original. On average, how many attempts were required per sample to obtain a valid code conversion? Moreover, were all samples successfully converted into valid code, or were some data filtered out due to unsuccessful conversions?\n- Could you please clarify which specific tools or libraries were used to extract the elements from the PDF files when computing the geometric similarity reward?\n- In the computation of the geometric similarity reward, I assume that matching the corresponding elements is not a trivial task. Could you please elaborate on how reliable the matching results are when inspected by humans? In other words, to what extent does the proposed algorithm produce visually or semantically reasonable correspondences between elements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZyFh9QIpNt", "forum": "OAXECnLxuk", "replyto": "OAXECnLxuk", "signatures": ["ICLR.cc/2026/Conference/Submission16513/Reviewer_modw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16513/Reviewer_modw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984719845, "cdate": 1761984719845, "tmdate": 1762926602823, "mdate": 1762926602823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}