{"id": "xf2JC4531b", "number": 1080, "cdate": 1756835050991, "mdate": 1759898229438, "content": {"title": "Alita-G: Self-Evolving Generative Agent for Agent Generation", "abstract": "Large language models (LLMs) perform better when scaffolded into agents with memory, tools, and feedback. Beyond this, self-evolving agents have emerged, but current work largely limits adaptation to prompt rewriting or failure retries. Therefore, we present Alita-G, a self-evolution framework that transforms a general-purpose agent into a domain expert by systematically generating, abstracting, and curating Model Context Protocol (MCP) tools. In this framework, a generalist agent executes a curated suite of target-domain tasks and synthesizes candidate MCPs from successful trajectories. These are then abstracted to parameterized primitives and consolidated into a MCP Box. At inference time, Alita-G performs retrieval-augmented MCP selection with the help of each tool’s descriptions and use cases, before executing an agent equipped with the MCP Executor. Across several benchmarks GAIA, PathVQA, and Humanity's Last Exam, Alita-G attains strong gains while reducing computation costs. On GAIA validation, it achieves 83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result while reducing mean tokens per example by approximately 15% relative to a strong baseline agent. Alita-G thus provides a principled pathway from generalist capability to reusable, domain-specific competence, improving both accuracy and efficiency on complex reasoning tasks.", "tldr": "", "keywords": ["Agent", "Self-evolving"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d630e3be624ecd475006c87658bbd07249dfbe48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ALITA-G, a self-evolution framework that turns a general-purpose LLM agent into a domain specialist by automatically generating, abstracting, and curating Model Context Protocol (MCP) tools from successful task trajectories. It repeatedly runs a master agent over a target task set to synthesize candidate MCPs, abstracts them into parameterized, standardized primitives, and consolidates them into an MCP Box with rich descriptions and use cases. At inference, a specialized agent performs retrieval-augmented MCP selection (threshold or top-k) and executes with a Task Analyzer, MCP Retriever, and MCP Executor, yielding targeted tool use and lower compute. Across GAIA, PathVQA, and Humanity’s Last Exam, ALITA-G improves accuracy while reducing token consumption, achieving new state-of-the-art results on GAIA validation (83.03% pass@1, 89.09% pass@3) with ~15% fewer tokens than a strong baseline. Analyses show performance scales with MCP Box richness up to diminishing returns and that gains are driven by selective, higher MCP usage on hard instances with minimal regressions. The work differs from prior self-evolving agents by coupling MCP abstraction with MCP-level RAG to enable end-to-end, task-conditioned specialization that boosts both accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes to generate domain-relevant MCPs, which could benefit future task solving. The idea holds the potential as an effective adaptation approach to generalize pre-trained LLMs.\n2. The paper is in general well-written."}, "weaknesses": {"value": "1. The evaluation is conducted in a relative small subset of the original benchmark without repeat experiments. There may be bias and variance in the results.\n2. The authors use part of the benchmark data to tune hyperparameters. It would be better to develop a more principled and generalizable way determine them if they have a large influence.\n3. It is unclear which data the authors used in the process to generate MCPs, and whether the MCPs across 3 benchmarks are the same. I am also curious about how the authors define target domain? Is each benchmark considered as a target domain? Correct me if I miss anything.\n4. I would like to see more examples of generated MCPs. From the example in Figure 2, I feel that download_and_extract_pdf seems a function commonly implemented in deep research systems. It would be great to see more examples and understand why the generated MCPs are superior in achiving better performance and efficiency.\n5. The authors need more explanations on the difference from Alita (https://arxiv.org/pdf/2505.20286), given that both of them do self-evolution by generating MCPs."}, "questions": {"value": "1. It is suggested to try more models and see whether the findings hold.\n2. Do you evaluate GAIA test set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R1VTlxEipm", "forum": "xf2JC4531b", "replyto": "xf2JC4531b", "signatures": ["ICLR.cc/2026/Conference/Submission1080/Reviewer_6RMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1080/Reviewer_6RMu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761256487088, "cdate": 1761256487088, "tmdate": 1762915672669, "mdate": 1762915672669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes ALITA-G, a novel framework for self-evolving LLM agents that addresses the limitations of narrow, single-task methods. It enables a general-purpose \"master\" agent to become a domain-specific expert by systematically creating a library of reusable tools. At inference time, a Retrieval-Augmented Generation (RAG) system efficiently selects the most relevant tools from this box for a specialized agent to use. Experimental results confirm the effectiveness of ALITA-G."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe manuscript is well-motivated and well-written. The paper tackles a problem of significant importance. The ability to self-evolve is important for developing powerful LLM-based agent systems. Moreover, the proposed pipeline is well-presented and can be easily understood.\n\n2.\tThe authors validate their framework across three diverse and challenging benchmarks. Moreover, the detailed analysis of MCP Box scalability in Section 4.1 provides a nuanced investigation into the diminishing returns of adding more tools. This analysis is mature and provides valuable practical insights.\n\n3.\tThe paper includes a suite of well-designed ablation studies in the appendix that convincingly demonstrate the contribution of key components. For instance, the analysis of RAG content components (Table 4) validates the choice to use both MCP descriptions and use cases for retrieval."}, "weaknesses": {"value": "1.\tThe paper claims a \"new state-of-the-art result\" on the GAIA benchmark. However, this result is reported on the GAIA validation set. State-of-the-art claims for major benchmarks like GAIA are adjudicated on the private test set via official leaderboards to ensure fair, robust, and non-overfit comparisons.\n\n2.\tThe framework's success hinges on a powerful \"master agent\" composed of top-tier proprietary models (Claude-Sonnet and GPT). It remains unclear whether this approach is viable with less capable, or even the strongest open-source, models (Llama and Qwen etc.)."}, "questions": {"value": "1.\tCould you report the results on the official GAIA leaderboard's private test set to ensure a fair and non-overfit comparison?\n\n2.\tHow viable is this tool-generation approach when using open-source models, such as Llama or Qwen, as the master agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AYE4R92MpV", "forum": "xf2JC4531b", "replyto": "xf2JC4531b", "signatures": ["ICLR.cc/2026/Conference/Submission1080/Reviewer_VVWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1080/Reviewer_VVWH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710933070, "cdate": 1761710933070, "tmdate": 1762915672193, "mdate": 1762915672193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to allow LLMs to create their own set of tools (in the form of MCP interfaces) by first analyzing traces of successful past rollouts. Those new tools create a MCP pool which the model can tap in in the future.\nThe results on a variety of benchmarks show strong improvements, although this reviewer has some questions on the specific setup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Allowing LLMs to create their own tool is an important step in the creation of more autonomous specialized agents, and this paper is a step into that direction\n\nThe benchmarks results show consistent gains"}, "weaknesses": {"value": "I have some doubts on the set-up (see below), and that the current setting does not leak past in-domain information into the test set"}, "questions": {"value": "The MCP tool is obtained from successful past execution. How is this done on benchmarks that do not contain training sets? Eg, where do the MCP tools for HLE come from? Are those from past test instances?\n\nFigure 2 in the Appendix provides one example of an abstracted MCP tool. Could you provide others? Those might be quite insightful"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "keTCK2RaVr", "forum": "xf2JC4531b", "replyto": "xf2JC4531b", "signatures": ["ICLR.cc/2026/Conference/Submission1080/Reviewer_t2qM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1080/Reviewer_t2qM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860200166, "cdate": 1761860200166, "tmdate": 1762915671325, "mdate": 1762915671325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}