{"id": "RyQ25bGKDs", "number": 8122, "cdate": 1758066160809, "mdate": 1759897805696, "content": {"title": "Subjective Neural Networks: Bayesian Dropout with Trust-Aware Opinions", "abstract": "Deep neural networks achieve remarkable predictive accuracy but often fail to convey meaningful uncertainty, which limits their reliability in safety-critical applications. Existing approaches such as Evidence Deep Learning (EDL) and Bayesian dropout either treat uncertainty as deterministic evidence or approximate it with sampling, but they lack an explicit interpretation of subjective trust. In this work, we introduce the Subjective Neural Network (SNN), a framework that combines Bayesian variational inference with subjective logic. Neuron activations are controlled by Beta–Bernoulli dropout, where the Beta distribution encodes a subjective trust opinion and the Bernoulli mask determines whether a neuron participates in inference. During prediction, we apply a nested sampling procedure: sampling trust probabilities from Beta distributions, generating dropout masks, and aggregating outputs into Dirichlet distributions. This process produces predictions that can be directly mapped into subjective opinions of beliefs and uncertainty over class labels. Empirical results on image classification benchmarks show that SNN achieves competitive accuracy while providing calibrated and interpretable uncertainty estimates. Our work establishes a principled connection between Bayesian deep learning and subjective logic, offering a pathway toward trust-aware neural networks.", "tldr": "We propose Subjective Neural Networks, a Bayesian framework where Beta–Bernoulli dropout encodes subjective trust and predictions are mapped into Dirichlet-based opinions, yielding competitive accuracy with calibrated and interpretable uncertainty.", "keywords": ["Bayesian deep learning", "Dropout", "Beta–Bernoulli distribution", "Subjective logic", "Uncertainty", "Evidence theory", "Trust-aware inference", "Dirichlet distribution"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d101fa4270172ce77479df78fd532277fa3389df.pdf", "supplementary_material": "/attachment/e35c0a237712bfad46bdc748cc38beeea125bf34.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduce the Subjective Neural Network (SNN), a framework that combines Bayesian variational inference with subjective logic."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper proposed a novel framework to unify the Bayesian Variational Inference with subjective logic."}, "weaknesses": {"value": "The proposed method is mainly evaluated on small-scale datasets, which makes it hard to convey the effectiveness of the proposed model. \n\nThe motivation is not strong, as to why we need to combine the Bayesian Variational Inference with subjective logic.\n\nThe experiment result is weak."}, "questions": {"value": "My main concern lies in why you want to combine the Bayesian variational inference with subjective logic."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nDfPKm9Wyn", "forum": "RyQ25bGKDs", "replyto": "RyQ25bGKDs", "signatures": ["ICLR.cc/2026/Conference/Submission8122/Reviewer_273u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8122/Reviewer_273u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908622872, "cdate": 1761908622872, "tmdate": 1762920100250, "mdate": 1762920100250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study uncertainty estimation for image classification. They propose Subjective Neural Networks (SNNs), an approach that combines Beta–Bernoulli Dropout with subjective logic.\n\nThey evaluate the approach on MNIST and CIFAR-5 using small CNNs, comparing with MC-dropout and EDL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written in the sense that it contains basically no typos or similar issues.\n- The proposed approach is simple and makes some sense overall."}, "weaknesses": {"value": "- The paper is just ~7.5 pages long.\n- The experimental evaluation is extremely limited, the proposed approach is just evaluated on MNIST and CIFAR-10 using very small CNNs.\n- Even in the very limited evaluation, the proposed method does not seem to perform particularly well, the predictive performance on CIFAR-5 is significantly below MC-dropout in Table 1."}, "questions": {"value": "- Could you please extend the paper to 9 full pages?\n- Could you please significantly extend the experimental evaluation with up-to-date datasets and models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EVpCGOQ09b", "forum": "RyQ25bGKDs", "replyto": "RyQ25bGKDs", "signatures": ["ICLR.cc/2026/Conference/Submission8122/Reviewer_3Nub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8122/Reviewer_3Nub"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976599696, "cdate": 1761976599696, "tmdate": 1762920099809, "mdate": 1762920099809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of the paper introduce Subjective Neural Network (SNN), a framework that combines Bayesian variational inference with the domain of subjective logic, in order to tackle the problem of uncertainty quantification within the context of deep neural networks (DNNs). Their method adopts principles from Subject logic, in order to make more accurate predictions, namely the representation of beliefs opinions (instead of fixed probabilities), the epistemic uncertainty and prior information.\nIn order to do so they introduce a framework at which (a) neuron activations are controlled by Beta–Bernoulli Dropout, an extension of classical dropout where, instead of a fixed rate across all neurons, each neuron has a distribution over dropout probabilities (b) the representation of opinion maps to a Dirichlet distribution. Given that direct inference of the posterior over latent trust probabilities is intractable, they adopt a variational approach and also employ the Kumaraswamy distribution, in order to enable gradient-based learning.\nAt inference time, they approximate the posterior predictive distribution through nested sampling.\nThey conclude their work with an experiments section on CIFAR-10 and MNIST data where they test the performance of their method against Dropout and Evidential Deep Learning (EDL)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduce the idea of combining two frameworks, namely Bayesian variational inference with subjective logic. \nThey develop at which dropout probabilities for each neuron are not fixed, but rather stem from a hierarchical distribution ( $z_j \\sim Beta(a_j, b_j),   \\ p_j \\sim Bernoulli(z_j), \\foreach j $)\nIn order to avoid the problem of intractable posterior they use they approximate each $p_j$ with a variational Beta distribution and enable gradient learning through Kumaraswamy distribution which approximates the Beta.\nFinally, they use nested sampling for inference."}, "weaknesses": {"value": "-- The introduction of the paper has no references;\n-- Although the idea is interesting, the method does not seem to outperform existing ones at the experiments section (eg. Table 1);\n-- Figure 1 is not very clear; moreover dropout is tested against different sets of digits at the digit 9 case;\n-- Figure 2 does not include Dropout;\n-- In CIFAR case (which is presented at the Appendix) SNN is outperformed across all tests (also same problem is observed as at Figure 5, methods are not tested against the same set of labels)"}, "questions": {"value": "Authors would be kindly asked to\n\n1) please provide computational times of Dropout, EDL and SNN;\n2) please include more results with respect to MNIST and CIFAR (more digits and labels at the classification of rotated digits and samples, respectively;\n3) please regenerate Figure 1, bottom row so that (a) all methods are tested against the same set of digits; (b) symbols are consistent to digits across graphs;\n4) please regenerate Figure 2, including Dropout curve"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "slQafXP9Tc", "forum": "RyQ25bGKDs", "replyto": "RyQ25bGKDs", "signatures": ["ICLR.cc/2026/Conference/Submission8122/Reviewer_yjur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8122/Reviewer_yjur"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762280721871, "cdate": 1762280721871, "tmdate": 1762920099456, "mdate": 1762920099456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}