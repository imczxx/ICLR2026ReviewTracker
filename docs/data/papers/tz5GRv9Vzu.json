{"id": "tz5GRv9Vzu", "number": 3224, "cdate": 1757380904195, "mdate": 1759898101284, "content": {"title": "Durian: Dual Reference Image-Guided Portrait Animation with Attribute Transfer", "abstract": "We present Durian, the first method for generating portrait animation videos with cross-identity attribute transfer from one or more reference images to a target portrait. Training such models typically requires attribute pairs of the same individual, which are rarely available at scale. To address this challenge, we propose a self-reconstruction formulation that leverages ordinary portrait videos to learn attribute transfer without explicit paired data. Two frames from the same video act as a pseudo pair: one serves as an attribute reference and the other as an identity reference. To enable this self-reconstruction training, we introduce a Dual ReferenceNet that processes the two references separately and then fuses their features via spatial attention within a diffusion model. To make sure each reference functions as a specialized stream for either identity or attribute information, we apply complementary masking to the reference images.\nTogether, these two components guide the model to reconstruct the original video, naturally learning cross-identity attribute transfer.\nTo bridge the gap between self-reconstruction training and cross-identity inference, we introduce a mask expansion strategy and augmentation schemes, enabling robust transfer of attributes with varying spatial extent and misalignment. Durian achieves state-of-the-art performance on portrait animation with attribute transfer. Moreover, its dual reference design uniquely supports multi-attribute composition and smooth attribute interpolation within a single generation pass, enabling highly flexible and controllable synthesis.", "tldr": "Given a portrait and one or more reference images specifying target attributes, Durian generates a portrait animation with attribute transfer in a single-stage pipeline.", "keywords": ["Attribute Transfer", "Portrait Animation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f65e1e81f25426deabadb84e7a984e80b3d2507.pdf", "supplementary_material": "/attachment/64a9baca16cf1ba225e7fcc9e9ed966cad821bf0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes DURIAN, a diffusion-based framework for dual-reference image-guided portrait animation, which enables attribute transfer across identities (e.g., hairstyle, glasses, beard, hat) while maintaining the target person’s identity consistency and generating high-fidelity dynamic videos.The paper introduces four core innovations:\n1. Dual ReferenceNet architecture – separately processes the attribute reference image and the identity reference image, and fuses their representations through a Spatial Attention mechanism within the diffusion process.\n2. Self-Reconstruction Training Strategy – trains solely on natural portrait videos by sampling two frames from the same video as pseudo attribute–identity pairs, enabling unpaired attribute transfer learning.\n3. Attribute-Aware Mask Expansion and Augmentation – bridges the gap between self-reconstruction training and real cross-identity inference, improving robustness to spatial misalignment and diverse attribute layouts.\n4. Emergent Abilities – DURIAN exhibits zero-shot multi-attribute composition and interpolation in a single forward pass without any additional training.\nExtensive experiments on CelebV-Text, VFHQ, and Nersemble datasets show that DURIAN significantly outperforms multiple attribute transfer baselines both quantitatively and visually."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Clever self-reconstruction strategy: The proposed self-supervised training method effectively eliminates the dependence on paired attribute data, which is scarce in real-world scenarios, making it an important advancement in attribute transfer.\n2. High technical originality: The dual-reference architecture independently encodes identity and attribute information, and fuses them via cross-attention, representing a novel design in diffusion-based frameworks.\n3. Robust identity preservation: The introduction of mask expansion and augmentation significantly improves robustness under cross-identity attribute generation, ensuring consistent identity preservation.\n4. Excellent writing and structure: The paper clearly motivates the research problem, presents the method with logical flow, and provides solid reasoning for each design choice. The methodology section follows a top-down structure, clearly explaining design rationale and detailed implementations.\n5. Detailed equations and reproducibility: The mathematical formulations are clear and complete. The appendix provides comprehensive implementation details, enhancing reproducibility.\n6. Comprehensive experiments: Quantitative metrics include both reconstruction fidelity and perceptual quality; Ablation studies verify the effect of each module (mask expansion, augmentation, dual-branch design, etc.); Comparison with strong baselines shows clear superiority.\n7. Emergent properties: The model demonstrates emergent multi-attribute composition and smooth interpolation in a single forward pass, indicating strong representational power and generalization."}, "weaknesses": {"value": "1. Handling of overlapping attributes: The paper does not explicitly describe how the model handles the interaction between overlapping attributes such as hair and hat regions. Providing a detailed explanation or visualization of how the spatial attention mechanism resolves these conflicts would make the method more transparent and convincing.\n2. Heavy dependency on external tools: The approach heavily relies on multiple external pretrained modules, including Sapiens, EMOCA, FLUX, and SDXL. While this integration improves practicality, it may reduce the model’s reproducibility and independence. Since the mask quality from Sapiens directly affects transfer results (for example, incomplete glasses detection can cause attribute failure), it would be beneficial to conduct a sensitivity analysis or robustness evaluation with alternative segmentation tools.\n3. Limited evaluation domain: The current evaluation focuses only on human facial portraits, without testing on stylized or non-human avatars. Including results on cartoon or 3D-rendered portraits would help demonstrate the model’s generalization ability across different visual domains.\n4. Empirical design of mask expansion: The mask expansion strategy, which leverages SDXL and ControlNet with prompt engineering to simulate variations such as long hair or reshaped hairstyles, appears somewhat heuristic. It would strengthen the technical rigor to include quantitative results or introduce a more automatic and controllable expansion mechanism.\n5. Lack of semantic consistency validation in interpolation: The interpolation experiments (Fig. 7) only show visual examples, without verifying whether the interpolation path corresponds to a semantically smooth transformation. Incorporating semantic distance or latent-space curvature analysis would help confirm that the interpolation indeed reflects continuous attribute transitions."}, "questions": {"value": "1. Could the proposed method be extended to audio-driven scenarios, where an attribute reference is applied to a portrait and animated according to an input audio signal?\n2. How does the model handle overlapping attributes during multi-attribute transfer (e.g., hat and hair regions)?\n3. Can the model generalize to non-human domains such as stylized or 3D avatars?\n4. How sensitive is DURIAN to the accuracy of Sapiens masks? Does noisy segmentation significantly degrade results?\n5. Could the self-reconstruction setup be extended to multi-frame attribute co-training (e.g., three or more frames to jointly learn multiple attributes)?\n6. Could the authors provide examples of failure cases under extreme conditions (occlusion, conflicting lighting, overlapping attributes, etc.)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rh6b24MlCB", "forum": "tz5GRv9Vzu", "replyto": "tz5GRv9Vzu", "signatures": ["ICLR.cc/2026/Conference/Submission3224/Reviewer_3NqY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3224/Reviewer_3NqY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300192276, "cdate": 1761300192276, "tmdate": 1762916608393, "mdate": 1762916608393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Durian, a diffusion-based framework for generating portrait animation videos with cross-identity facial attribute transfer (e.g., hairstyles, eyeglasses, beards, hats) from one or more reference images. A core challenge in this field is the scarcity of large-scale paired data (same individual with/without target attributes), which limits existing methods to specialized tasks (e.g., hairstyle-only transfer) or multi-stage pipelines. Durian addresses this by introducing a self-reconstruction training strategy that leverages uncurated in-the-wild portrait videos (CelebV-Text, VFHQ, Nersemble) instead of paired data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Existing attribute transfer methods (e.g., HairFusion, Chung et al. 2025; StableHair, Zhang et al. 2025) rely on attribute-paired data or synthetic pipelines (e.g., generating bald portraits to pair with hairstyles). In contrast, Durian’s self-reconstruction strategy uses uncurated videos (2,747 total) to simulate attribute transfer, eliminating the need for expensive paired data. This makes it scalable to diverse attributes without dataset curation.\n- Single-reference models (e.g., CAT-VTON, Chong et al. 2024; TriplaneEdit, Bilecen et al. 2024) concatenate attribute and identity inputs into a shared encoder, leading to attribute-identity blending. Durian’s ARNet (attribute) and PRNet (identity) separate feature extraction, enforced by complementary masking. Ablation studies confirm this design outperforms single ReferenceNet variants (L₁: 0.0744 vs. 0.0813; SSIM: 0.6527 vs. 0.6314).\n- Most SOTA methods are task-siloed: static attribute transfer (e.g., Stable-Makeup, Zhang et al. 2024b) cannot animate, while animation models (e.g., LivePortrait, Guo et al. 2024; X-Portrait, Xie et al. 2024) cannot transfer attributes. Durian uniquely enables multi-attribute composition (e.g., combining hair + glasses + hat) and attribute interpolation (smooth hairstyle transitions) in one generation pass—critical for real-world styling applications (e.g., virtual try-ons).\n- The self-reconstruction paradigm inherently suffers from a domain gap (training on single identities vs. inferring across identities). Durian’s mask expansion (union of original/generated attribute masks) and augmentations (affine misalignment, FLUX outpainting) outperform fixed-heuristic methods (e.g., HairFusion’s hair-specific mask expansion). This explains its strong cross-identity results (ID-Sim: 0.7098 vs. baseline 0.2776)."}, "weaknesses": {"value": "- Durian only supports four attributes (hair, eyeglasses, beard, hat), narrowing its applicability compared to methods that handle makeup (Stable-Makeup, Zhang et al. 2024b), clothing (AnyFit, Li et al. 2024), or accessories beyond hats (e.g., necklaces). The paper provides no rationale for excluding these attributes, nor discusses modifications needed for deformable attributes (e.g., scarves) or subtle attributes (e.g., lipstick).\n- Durian relies heavily on third-party tools for critical components:\nMask generation: Sapiens (Khirodkar et al. 2024) for attribute segmentation. Outpainting: FLUX (Labs, 2024) to fill regions after affine transformations. Keypoint alignment: LivePortrait (Guo et al. 2024) to fix shape mismatch in cross-identity inference. This lack of end-to-end integration limits deployment flexibility and introduces potential errors (e.g., Sapiens failing to segment small attributes like thin eyeglasses).\n- The two-stage training (60,000 steps per stage, ~6 days on 8 NVIDIA RTX A6000 GPUs) is computationally expensive. For comparison, single-stage animation models (e.g., Animate Anyone, Hu 2024) train faster while maintaining temporal consistency. The paper does not explore optimization (e.g., adaptive step scheduling) to reduce training time, which is a barrier for real-world adoption.\n- Insufficient Theoretical and Ablation Depth\n  - The “full ref. image input” ablation (unmasked training) achieves better self-transfer metrics (L₁: 0.0670 vs. Durian’s 0.0744) but fails in cross-identity transfer. However, the paper does not analyze why unmasked inputs break disentanglement (e.g., do they allow the model to copy identity cues instead of learning transfer?).\n  - Ablations for temporal attention (critical for animation) are missing. The paper only confirms temporal layers improve consistency but does not quantify their impact (e.g., VFID without temporal layers).\n- Durian is only evaluated on 3-second clips. For longer videos (e.g., 10+ seconds), its temporal attention may struggle with drifting attributes (e.g., hairstyle shape changing over time). SOTA video diffusion models (e.g., VideoCrafter2, Chen et al. 2024a) use factorized spatiotemporal attention to maintain long-horizon consistency, which Durian does not adopt.\n- For blurred images or unrecognizable faces (e.g., low resolution, occlusions), Durian fails to extract valid attribute masks or align references. This is a critical gap compared to robust methods like HunyuanPortrait (Xu et al. 2025), which uses an enhanced appearance encoder to handle low-quality inputs.\n- When combining attributes from references with large pose differences (e.g., a hat from a left-angled face and glasses from a front-facing face), Durian suffers from misalignment. Unlike MimicMotion (Zhang et al. 2024b), which uses pose-aware guidance to align multi-reference features, Durian lacks explicit pose correction for compositional tasks."}, "questions": {"value": "- Can Durian be extended to deformable attributes (e.g., clothing, scarves) or subtle attributes (e.g., makeup)? What modifications to masking (e.g., dynamic masks for moving clothing) or feature extraction (e.g., finer-grained ARNet layers) would be required?\n- If external tools (e.g., Sapiens, LivePortrait) are replaced with a built-in masker/aligner, how would performance change? Is end-to-end integration feasible without sacrificing quality?\n- Cross-identity transfers with large lighting differences (e.g., a dark-haired reference and a bright-portrait identity) produce subtle artifacts. Would adding an illumination alignment module (e.g., as in PERSE, Cha et al. 2025) improve attribute-identity coherence?\n- Training data is imbalanced (2,086 hair samples vs. 129 hat samples). Does this imbalance hurt transfer quality for rare attributes (e.g., hats)? Could few-shot learning or data augmentation for rare attributes improve generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sluoRHVGTj", "forum": "tz5GRv9Vzu", "replyto": "tz5GRv9Vzu", "signatures": ["ICLR.cc/2026/Conference/Submission3224/Reviewer_VRMj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3224/Reviewer_VRMj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383000588, "cdate": 1761383000588, "tmdate": 1762916608204, "mdate": 1762916608204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces portrait animation system while transferring facial attributes based on reference images. To avoid the attribute-paired dataset issues, it propose self-reconstruction strategies by designing dual reference branches upon diffusion models, significantly outperforming existing SoTAs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- High fidelity on attribute transfer: The proposed system exhibits significantly faithful outcomes on attribute transfer with high quality and relatively no visual artifacts.\n- Single generation pass for multiple attributes: For multiple attributes transfer, the concat-based module produces pretty plausible results."}, "weaknesses": {"value": "- Discussion about masking step: It is well known that extracting particular pixels alongside given attributes often fails within in-the-wild condition. It is better to discuss how such scenarios can be further alleviated for practicality.\n- Width-wise concatenations: The system propose width-wise concatenation to mix the reference information from both paths in main denoising process. However, in reference-based editing manner, there are some common approaches such as cross attention, per-layer attention map swapping and so forth. It is recommended to discuss how the proposed width-concatenation works compared to existing approaches."}, "questions": {"value": "In multi-attributes transfer, the main component is to concatenate the feature $F^{l, N_{attr}}_{attr}$ with attention module. However, in general, projecting features into key and value vectors relies on learnable projection layer. Since multiple-attributes transfer increase the width of the features thus requiring variable size of linear projections, how the model handles this?\n\nDo the system use separate $N_{attr}$-specific models than single attribute transfer or use some inference-level adaptive manner in single generation pass?\n\nAlso, it is recommended to discuss is there any degradation according to the $N_{attr}$ and the maximal number for this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BWgL5ZHZMP", "forum": "tz5GRv9Vzu", "replyto": "tz5GRv9Vzu", "signatures": ["ICLR.cc/2026/Conference/Submission3224/Reviewer_2dwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3224/Reviewer_2dwE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977415349, "cdate": 1761977415349, "tmdate": 1762916608010, "mdate": 1762916608010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Durian, which generates portrait animation videos with cross-identity attribute transfer from one or more reference images to a target portrait"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The task may be new and the method does not need paired data."}, "weaknesses": {"value": "- Figure 2 shows the proposed method use the guidance video frames only. But in A.3, it is stated that the methods use LivePortrait to generate a \"self-reenactment\" video. This video preserves the shape of the target portrait while matching the movements of the driving video.\nThen, extract key points from this intermediate generated video, which will serve as the actual input for the Durian model. The proposed Durian seems to rely on the ability of LivePortrait on the self-reenactment video. The comparison with LivePortrait may not be fair.\n\n- “full ref. image input” uses unmasked portrait and attribute images during training. This variant achieves the best quantitative scores in Table 2. The paper provide explanation in Fig 5. This may show the used metrics are not reliable and the performance in Table 1 is in question. In Figure 10 shows liveportrait with StableHair performs better.\n\n- The paper mention that \"no previous work has directly addressed this task\". Therefore, it constructs a \"two-stage\" baseline: Using an image editing model (such as StableHair) to transfer attributes, and then using a video animation model (such as LivePortrait) to create animations. Such comparison is unfair. The two-stage method inherently has disadvantages (error accumulation, domain mismatch). It is expected that the authors' end-to-end model outperforms these \"patched-together\" baselines in terms of metrics. This is not a solid contribution.\n\n- The core critique of the novelty of the Durian architecture lies in the fact that its \"Dual ReferenceNet\" is a combination of Adapters. The core mechanism of IP-Adapter (Ye et al., 2023) is as follows: it uses a (typically frozen) image encoder (CLIP) to extract features from a reference image, and injects these features into the cross-attention layers of a pre-trained diffusion model. This enables additional control over the styel.Durian’s ARNet performs exactly the same function. It takes an attribute reference image (hairstyle), encodes it also with CLIP , and injects its spatial featureinto the diffusion U-Net to control the style again. Besides, InstantID (Wang et al., 2024) designs IdentityNet that integrates facial and key-point images into the generation process by imposing strong semantic and weak spatial conditions, thereby achieving high-fidelity id preserve. Durian also performs an identical function. It takes a portrait reference image, encodes it also with ArcFace and injects its feature Diffusion to ensure identity preservation. It seems such two branches are, in fact, a combination of existing adapters.\n\n\n## Comprehensive Analysis  \nThe so-called \"novel architecture\" of Durian is essentially a **parallel combination** of an IP-Adapter (for attributes) and an InstantID-like module (for identity). This is a **compositional and incremental engineering step**, not a fundamental architectural invention. In their paper, the authors avoid using the standard term \"Adapter\" and instead coin the new term \"Dual ReferenceNet\"—to some extent, this obscures the limited architectural novelty of their method."}, "questions": {"value": "Pls see weakness."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Portrait generation may include bias."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "07JMWwptT4", "forum": "tz5GRv9Vzu", "replyto": "tz5GRv9Vzu", "signatures": ["ICLR.cc/2026/Conference/Submission3224/Reviewer_5oxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3224/Reviewer_5oxU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990566063, "cdate": 1761990566063, "tmdate": 1762916607813, "mdate": 1762916607813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}