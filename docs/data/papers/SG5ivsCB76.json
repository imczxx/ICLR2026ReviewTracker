{"id": "SG5ivsCB76", "number": 560, "cdate": 1756746631351, "mdate": 1763748021874, "content": {"title": "BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Recordings for Speech Decoding", "abstract": "Decoding speech directly from neural activity is a central goal in brain-computer interface (BCI) research. In recent years, exciting advances have been made through the growing use of intracranial field potential recordings, such as stereo-ElectroEncephaloGraphy (sEEG) and ElectroCorticoGraphy (ECoG). These neural signals capture rich population-level activity but present key challenges: (i) task-relevant neural signals are sparsely distributed across sEEG electrodes, and (ii) multiple neural components (e.g., tongue \\& jaw \\& lips control in vSMC) are often entangled within the task-relevant functional groups in both sEEG and ECoG recordings. To address these challenges, we introduce a unified speech decoding framework enhanced by Coarse-to-Fine disentanglement, BrainStratify, which includes (i) identifying functional groups through spatial-context-guided temporal-spatial modeling, and (ii) disentangling neural components within the target functional group using Decoupled Product Quantization (DPQ). We evaluate BrainStratify on six datasets (including sEEG, (epidural) ECoG, etc.), spanning tasks like vocal production, speech perception, etc. Extensive experiments show that BrainStratify, as a unified framework for decoding speech from intracranial neural signals, significantly outperforms previous decoding methods. Overall, by combining data-driven stratification with neuroscience-inspired modularity, BrainStratify offers a robust and interpretable solution for decoding speech from intracranial recordings. Code and dataset will be publicly available.", "tldr": "We propose the BrainStratify framework for decoding speech from intracranial neural signals, which disentangles intracranial neural signals to identify fine-grained states in a Coarse-to-Fine way.", "keywords": ["speech decoding", "sEEG", "epidural ECoG", "self-supervision", "neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6cc84194f1b61158992e1ddd0fbc59d3d992d034.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents BrainStratify, a unified framework for decoding speech from intracranial recordings (sEEG and ECoG) that employs a coarse-to-fine neural disentanglement approach. The framework addresses two key challenges in intracranial neural decoding: (1) task-relevant neural signals are sparsely distributed across electrodes, and (2) multiple neural components (e.g., tongue, jaw, and lips control) are often entangled within functional groups. The proposed solution consists of two stages: (i) Coarse Disentanglement Learning (BrainStratify-Coarse), which identifies functional channel groups through spatial-context-guided temporal-spatial modeling and learns sparse inter-channel attention graphs via self-supervised pre-training, and (ii) Fine Disentanglement Learning (BrainStratify-Fine), which introduces Decoupled Product Quantization (DPQ) to disentangle neural components within target functional groups by using multiple codexes with partial-correlation constraints. The authors evaluate their method on six datasets spanning various modalities (sEEG, epidural ECoG, MEA, sEMG) and tasks (vocal production, speech perception, motor intention), including two newly collected epidural ECoG datasets from patients with ALS and spinal cord injury. Experimental results demonstrate that BrainStratify achieves state-of-the-art performance across all evaluated tasks, with particularly strong improvements on word classification and syllable sequential classification tasks.\n\nThe main contributions include: (1) a neuroscience-inspired design that leverages modular brain computation principles to identify functional groups based on learned inter-channel connectivity rather than low-level correlations, (2) a label-efficient approach that requires minimal labeled data for channel selection (600 samples vs. 3000 for supervised methods), (3) a DPQ mechanism that successfully disentangles neural components as validated through articulator control experiments, and (4) comprehensive experimental validation showing consistent improvements over baseline methods including LaBraM, CBraMod, PopT, EEG-CFMR, Du-IN, and H2DiLR. The framework demonstrates strong interpretability through visualizations of learned channel clusters and codex contributions, and achieves real-time inference speeds (≤20ms per trial) suitable for practical BCI applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a genuinely novel approach to intracranial neural decoding by addressing two critical yet under-explored challenges simultaneously: sparse electrode distribution and entangled neural components. The originality lies in the elegant coarse-to-fine disentanglement framework that mirrors hierarchical brain organization principles. Unlike existing methods (CCM, DUET) that rely on low-level correlation-based clustering—which the authors convincingly demonstrate to be inadequate for multimodal intracranial signals where similar firing patterns hold different functional meanings by location—BrainStratify learns functional groupings through spatial-context-guided self-supervised pre-training. The introduction of Decoupled Product Quantization (DPQ) for neural component disentanglement is particularly innovative, adapting product quantization from computer vision to the neuroscience domain with partial-correlation constraints that encourage codex independence. The articulator control validation experiments (Figure 4) provide compelling evidence that learned codexes capture interpretable neural components (jaw, lips, tongue control), demonstrating that the method discovers meaningful neuroscientific structure rather than arbitrary feature decompositions.\n\nThe experimental evaluation demonstrates outstanding thoroughness across multiple dimensions. The evaluation spans six diverse datasets covering different modalities (sEEG, epidural ECoG, MEA, sEMG), recording scenarios (vocal production, speech perception, motor intention), and subject populations (12 subjects in Du-IN, 10 in Brain Treebank, plus newly collected clinical patients), providing strong evidence of generalizability. The authors collect two new well-annotated clinical datasets from patients with ALS and spinal cord injury, contributing valuable resources to the community. The statistical analysis is rigorous, reporting results with standard errors across six random seeds and conducting paired t-tests against all baselines. The ablation studies are comprehensive, systematically evaluating the impact of cluster numbers, codex groups, codex sizes, codex dimensions, and individual DPQ components. The label-efficiency analysis (Figure 1) demonstrates a critical practical advantage: while supervised methods require 3000 samples to identify task-relevant channels, BrainStratify-Coarse achieves comparable performance with only 600 samples, addressing a fundamental constraint in medical data collection where large-scale annotation is often prohibitively expensive.\n\nThe paper is exceptionally well-written with clear motivation, logical flow, and comprehensive documentation. The problem formulation is precise, clearly distinguishing between coarse-grained functional group identification and fine-grained neural component disentanglement. Figure 2 provides an excellent overview of the entire framework, effectively illustrating both training stages and downstream applications. The visualization of channel connectivity matrices (Figure 3b) offers direct comparison with baselines, making the superiority of the learned sparse attention graphs immediately apparent. The anatomical visualizations (Figure 3c-d) ground the learned clusters in neuroscientific interpretability by showing weighted anatomical counts relative to the Desikan-Killiany atlas. The disentanglement evaluation (Figure 4) is particularly compelling, showing channel tuning maps across articulators and quantifying each codex's contribution to different articulator control tasks. The extensive appendices provide complete implementation details, hyperparameters, subject-wise visualizations, and convergence curves, ensuring full reproducibility.\n\nThe work makes important contributions toward clinically deployable speech BCIs for patients with communication impairments. The state-of-the-art results across all evaluated tasks demonstrate consistent and meaningful performance improvements over strong baselines including Du-IN, PopT, and other foundation models. The framework's data efficiency (requiring only 600 samples for effective channel selection) and fast inference speed (≤20ms per trial) make it suitable for real-world deployment where calibration time and response latency are crucial. The successful application to epidural ECoG—a minimally invasive recording modality positioned outside the dura mater that reduces tissue damage compared to traditional intracranial placements—is particularly noteworthy for clinical translation. The results on patients with severe motor impairments (ALS patient who lost communication abilities, SCI patient with complete limb paralysis) demonstrate real-world applicability. The interpretability provided by learned functional groups and disentangled neural components enhances clinical trust and enables neuroscientists to validate and refine the system based on known brain organization principles."}, "weaknesses": {"value": "While the paper presents a comprehensive framework, several theoretical and methodological concerns warrant deeper investigation. First, the theoretical justification for why the spatial context pre-training task (replacing 10\\% of channels with unrelated temporal activity) should lead to learning functional connectivity remains somewhat underspecified. The authors claim this avoids shortcuts in mask-based reconstruction where models might over-rely on intra-channel temporal patterns, but the connection between detecting temporal mismatches and learning inter-channel functional relationships is not rigorously established. The rapid convergence to ≥95\\% accuracy with only 1-hour data suggests the task may be relatively superficial, detecting obvious temporal discontinuities rather than learning deep functional relationships. A more thorough analysis comparing the learned attention patterns with established neuroscientific measures of functional connectivity (e.g., coherence, Granger causality, phase-locking value) would strengthen the claim that BrainStratify-Coarse genuinely captures functional groupings rather than simpler statistical dependencies. Additionally, the paper lacks theoretical analysis of what properties the learned channel connectivity matrix P should possess to ensure meaningful clustering, and there is no discussion of convergence guarantees or sensitivity to initialization.\n\nThe experimental setup and comparison framework present several limitations that may overstate the improvements. Most critically, the comparison with baseline methods is not entirely fair in terms of architectural choices and pre-training strategies. For instance, the authors pre-train PopT across all subjects in a dataset but pre-train BrainStratify-Coarse on individual subjects, making it difficult to disentangle whether improvements stem from the proposed method or from subject-specific tuning. The paper claims PopT \"struggles to converge with limited data\" and thus requires multi-subject pre-training, but this asymmetric setup raises questions about whether PopT would perform better with subject-specific pre-training given sufficient data augmentation or optimization adjustments. Similarly, for DUET and CCM, the authors use raw (non-rereferenced) signals while using rereferenced signals for BrainStratify, justified by claiming rereferencing \"disrupts correlations\" needed by correlation-based methods. However, this inconsistency means the methods are operating on fundamentally different input representations, making direct performance comparison problematic. A more rigorous evaluation would either (1) adapt all methods to work with the same input representation, or (2) provide extensive ablations showing that each method performs best with its chosen representation. The paper also lacks comparison with more recent neural decoding methods or domain adaptation techniques that might handle cross-subject variability more effectively.\n\nThe generalization and scalability claims require more careful scrutiny. While the paper evaluates on six datasets, several critical limitations constrain the generalizability conclusions. First, five of the six datasets involve speech-related tasks (vocal production, speech perception, reading), with only one motor intention dataset, limiting evidence that the method generalizes beyond the speech domain. The authors acknowledge this limitation, stating the framework's \"design affords clear interpretability grounded in brain organization\" and anticipating generalization to other cognitive states, but this remains speculative without empirical validation on diverse cognitive tasks like visual perception, memory, decision-making, or emotional processing. Second, the epidural ECoG datasets contain only single subjects each, making it impossible to assess inter-subject variability for this clinically important modality. Third, all sEEG/ECoG datasets involve relatively sparse electrode coverage (averaging ~110-128 channels), and it remains unclear how the method would scale to higher-density recordings (e.g., 256+ channels) or whether the identified functional groups would remain stable. The authors note that BrainStratify is \"currently limited to identifying functional modules rather than full functional networks\" due to sparse electrode distribution, but provide no analysis of how coverage density affects performance or what minimum coverage is required for reliable functional group identification. Fourth, the cross-dataset evaluation is limited, models are trained and tested within the same dataset rather than evaluating transfer across datasets, which would provide stronger evidence of learning generalizable neural representations.\n\nThe Decoupled Product Quantization component, while innovative, raises several conceptual and practical concerns. The choice of using G=4 codex groups for sEEG versus G=8 for ECoG is justified post-hoc through ablation studies (Figure 10b), but the paper provides no principled method for determining the optimal number of codex groups a priori for new datasets or modalities. The ablation shows performance is relatively stable across a range of values (4-16 for ECoG), but this sensitivity analysis does not address the fundamental question of how many true neural components should exist within a functional group, a question that likely depends on the specific brain region, task demands, and recording modality. The partial correlation constraint L_pc encourages independence between codexes, but the paper does not analyze whether the learned codexes satisfy independence statistically (e.g., through mutual information analysis) or whether independence is the appropriate inductive bias. In neuroscience, neural components may exhibit structured dependencies (e.g., coordinated jaw-tongue movements during speech), and enforcing strict independence might discard meaningful covariance structure. The disentanglement evaluation (Figure 4) on articulator control tasks is compelling but limited in scope, it shows that different codexes contribute differentially to jaw/lips/tongue control, but does not rigorously quantify disentanglement quality using established metrics from the disentanglement learning literature (e.g., MIG, SAP, DCI scores). Moreover, this evaluation uses only ~2k trials with 6 movement types from a single subject, and it is unclear whether the discovered decomposition would remain consistent across subjects or tasks.\n\nThe paper's treatment of related work and positioning within the broader literature could be strengthened. The discussion of disentanglement representation learning (Section 2.3) mentions several relevant methods (QLAE, Tripod, pi-VAE, PDisVAE) but does not empirically compare against any of them, making it difficult to assess whether DPQ offers advantages over existing disentanglement techniques when applied to neural data. The claim that \"neural signals require disentanglement along the channel dimension, similar to how RGB channels are treated in images\" is conceptually interesting but somewhat misleading, as RGB channels represent different wavelengths of the same spatial location, while neural channels represent different spatial locations with potentially different functional properties. This distinction matters for justifying architectural choices. Additionally, the paper does not adequately discuss recent work on multimodal disentanglement or structured latent variable models that might be applicable. The comparison with time series forecasting methods (CCM, DUET) is somewhat tangential since these methods optimize for prediction rather than decoding, and their failure on neural decoding tasks does not necessarily invalidate their core ideas about channel clustering. A more relevant comparison would be with other neural decoding methods that explicitly model channel interactions or functional connectivity.\n\nSeveral implementation details and design choices lack sufficient justification. The paper uses spectral clustering with default scikit-learn parameters for grouping channels, but provides no analysis of robustness to this choice—would other clustering algorithms (e.g., hierarchical clustering with different linkage criteria, community detection algorithms, Louvain modularity optimization) yield similar functional groups? The choice of k=10 clusters appears arbitrary, justified only through post-hoc ablation showing performance is relatively stable across 5-20 clusters, but there is no principled method for selecting k or determining whether the true number of functional groups varies by subject. The weighted anatomical counts metric (Appendix I) is introduced as a quantitative measure of cluster quality, but its interpretation is unclear. why should lower N_anatomy indicate better clustering, and what is the expected value under a null model? The paper reports that \"most models surpass the PopT baseline with SC selection\" when evaluated on channels selected by BrainStratify-Coarse, but this comparison conflates channel selection quality with model performance, and it is unclear whether improvements stem from better channel selection or from the specific channels selected happening to be easier for certain model architectures. The training computational cost is mentioned briefly (6 hours on V100 GPU), but there is no analysis of how this scales with dataset size, number of channels, or number of subjects, which is relevant for practical deployment.\n\nFinally, the clinical validation and real-world applicability claims require more cautious interpretation. While the results on ALS and SCI patients are promising, these are single-subject cases with relatively short-term evaluation, and the paper does not address critical questions for clinical deployment such as long-term stability of the learned functional groups across days or weeks, robustness to electrode impedance changes, or performance degradation in the presence of neural plasticity or disease progression. The claim that epidural ECoG is \"minimally invasive\" and \"reduces tissue damage\" compared to traditional placements is important for clinical translation, but the paper does not discuss other critical clinical considerations such as infection risk, hardware reliability, or patient acceptance. The fast inference speed (≤20ms) is promising for real-time BCI, but the paper does not analyze latency breakdown (data transfer vs. computation) or discuss whether this latency is acceptable for natural communication, which typically requires <100ms speech decoding latency for fluid conversation. The lack of online closed-loop evaluation. where the BCI system provides real-time feedback to users—limits conclusions about practical usability, as offline decoding performance often overestimates online performance due to non-stationarities and adaptation effects."}, "questions": {"value": "Please see your weaknesses and I will adjust the final score based on your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X1KbGpGqn0", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Reviewer_xusM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Reviewer_xusM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555563665, "cdate": 1761555563665, "tmdate": 1762915546757, "mdate": 1762915546757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal by Authors (1/2)"}, "comment": {"value": "Thanks to all reviewers for careful reading and thoughtful feedback. We are grateful for acknowledging our method presents “a clear, modular design that mirrors neurophysiology intuition (group $\\rightarrow$ sub-component)”,“the codex-wise analyses on the articulation benchmark (jaw/lips/tongue) that add interpretability and are a nice touch toward making the discrete units meaningful” and “identifying relevant channels is interesting and a potentially useful procedure for improving downstream performance” (Reviewer 5Gy2 & Reviewer cnbL). We are also excited to hear that our work “is generalizable to different tasks beyond speech decoding and various types of biosignals” and “the topic is interesting and important” (Reviewer 721d). Further, we are delighted to hear our method “mirrors hierarchical brain organization principles” (Reviewer xusM).\n\nIn this rebuttal, we will clarify the problems our work aims to solve, underscore their importance, and further detail our method's novelty in this context.\n\n# The problems and importance\n\nThis work aims to tackle two fundamental challenges in intracranial BCIs: (1) **data-efficient channel selection for sEEG**, and (2) **learning generalizable representations that enhance performance across diverse decoding paradigms** (e.g., classification, regression, sequential classification) in intracranial recordings (e.g., sEEG, ECoG).\n\nThe brain's modular computation means task-relevant signals are typically distributed sparsely across the brain [1-4]. Since sEEG recordings capture highly localized and non-redundant signals, selecting task-relevant channels is critical for both decoding performance and computational efficiency, especially in complex tasks like word classification in [1]’s Figure 4b. However, current methods (e.g., SC and MC) rely heavily on supervised data, which is a major limitation in clinical practice where the vast majority of intracranial recordings are unlabeled.\n\nThis reality exposes a key gap: no work has systematically addressed channel selection in a realistic, label-scarce clinical setting. Our work directly confronts this by asking:\n\n1. Can we leverage abundant unlabeled data to drastically reduce the labeled sample requirement for channel selection?\n\n2. Can we learn a unified representation from unlabeled data that boosts performance across diverse decoding paradigms (e.g., classification, sequence labeling, regression)?\n\nFor vocal production in the Du-IN dataset [1], different decoding paradigms mean word classification, syllable sequential classification (Table 2), and speech synthesis (Table 12). By solving these problems, we enable more efficient and high-performance decoders, which is a crucial step toward practical clinical BCIs.\n\n# The novelty of our method\n\nCurrent research on intracranial decoding (especially speech decoding) primarily follows two paths: spatiotemporal models to build neurofoundation models [2,3] and temporal models to model brain dynamics [1,4]. While temporal models excel in cognitive-state decoding performance, they are notoriously labeled-data-hungry for channel selection. Spatiotemporal models, though less effective for intracranial cognitive-state decoding, possess a unique strength: an inherent flexibility in modeling channel dependencies. We leverage this specific strength to validate our novel functional-group-based channel selection approach. \n\nOur work, BrainStratify, integrates these approaches to create a unified framework, which combines the temporal modeling strengths of temporal models with the powerful spatial dependency capture of spatiotemporal models, achieving optimal performance in speech decoding and other similar tasks (e.g., motor intention).\n\nOne of our key innovations lies in using the spatial attention graphs from an unsupervised spatiotemporal Transformer to identify functional channel groups that align with the brain's modular organization (Figure 3). To our knowledge, this is the first work to **extract and validate such functional groups directly from model attention, a method that proves highly data-efficient**. This approach drastically reduces the labeled data required for channel selection -- e.g., from 3,000 to 600 samples in the Du-IN dataset [1] (Figure 1). This is **of immediate practical value, as many clinical datasets are similarly label-scarce** (e.g., ~1,500 samples in H2DiLR [4], ~500 samples in Singh et al. [7], ~500 samples in Angrick et al. [5]).\n\nFurthermore, while disentangled representation learning is established in computer vision and emerging in neural encoding studies with neural spike data [6], we are the first to adapt it for intracranial decoding in a clinical BCI context. We specifically **collected an articulation benchmark to quantitatively validate its interpretability via codex-wise analysis**."}}, "id": "XUirxUpAdE", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763228081449, "cdate": 1763228081449, "tmdate": 1763228081449, "mdate": 1763228081449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal by Authors (1/2)"}, "comment": {"value": "Thanks to all reviewers for careful reading and thoughtful feedback. We are grateful for acknowledging our method presents “a clear, modular design that mirrors neurophysiology intuition (group $\\rightarrow$ sub-component)”,“the codex-wise analyses on the articulation benchmark (jaw/lips/tongue) that add interpretability and are a nice touch toward making the discrete units meaningful” and “identifying relevant channels is interesting and a potentially useful procedure for improving downstream performance” (Reviewer 5Gy2 & Reviewer cnbL). We are also excited to hear that our work “is generalizable to different tasks beyond speech decoding and various types of biosignals” and “the topic is interesting and important” (Reviewer 721d). Further, we are delighted to hear our method “mirrors hierarchical brain organization principles” (Reviewer xusM).\n\nIn this rebuttal, we will clarify the problems our work aims to solve, underscore their importance, and further detail our method's novelty in this context.\n\n# The problems and importance\n\nThis work aims to tackle two fundamental challenges in intracranial BCIs: (1) **data-efficient channel selection for sEEG** (Line 55), and (2) **learning generalizable representations that enhance performance across diverse decoding paradigms** (e.g., classification, regression, sequential classification) in intracranial recordings (e.g., sEEG, ECoG).\n\nThe brain's modular computation means task-relevant signals are typically distributed sparsely across the brain [1-4]. Since sEEG recordings capture highly localized and non-redundant signals, selecting task-relevant channels is critical for both decoding performance and computational efficiency, especially in complex tasks like word classification in [1]’s Figure 4b. However, current methods (e.g., SC and MC) rely heavily on supervised data, which is a major limitation in clinical practice where the vast majority of intracranial recordings are unlabeled.\n\nThis reality exposes a key gap: few works have attempted to systematically address channel selection in a realistic, label-scarce clinical setting. Our work directly confronts this by asking:\n\n1. Can we leverage abundant unlabeled data to drastically reduce the labeled sample requirement for channel selection?\n\n2. Can we learn a unified representation from unlabeled data that boosts performance across diverse decoding paradigms (e.g., classification, sequence labeling, regression)?\n\nFor vocal production in the Du-IN dataset [1], different decoding paradigms mean word classification, syllable sequential classification (Table 2), and speech synthesis (Table 12). By solving these problems, we enable more efficient and high-performance decoders, which is a crucial step toward practical clinical BCIs.\n\n# The novelty of our method\n\nCurrent research on intracranial decoding (especially speech decoding) primarily follows two paths: spatiotemporal models to build neurofoundation models [2,3] and temporal models to model brain dynamics [1,4]. While temporal models excel in cognitive-state decoding performance, they are notoriously labeled-data-hungry for channel selection. Spatiotemporal models, though less effective for intracranial cognitive-state decoding, possess a unique strength: an inherent flexibility in modeling channel dependencies. We leverage this specific strength to validate our novel functional-group-based channel selection approach. \n\nOur work, BrainStratify, integrates these approaches to create a unified framework, which combines the temporal modeling strengths of temporal models with the powerful spatial dependency capture of spatiotemporal models, achieving optimal performance in speech decoding and other similar tasks (e.g., motor intention).\n\nOne of our key innovations lies in using the spatial attention graphs from an unsupervised spatiotemporal Transformer to identify functional channel groups that align with the brain's modular organization (Figure 3). To our knowledge, this is the first work to **extract and validate such functional groups directly from model attention, a method that proves highly data-efficient**. This approach drastically reduces the labeled data required for channel selection -- e.g., **from 3,000 to 600 samples in the Du-IN dataset [1] (Figure 1 & Line 378)**. This is **of immediate practical value, as many clinical datasets are similarly label-scarce** (e.g., ~1,500 samples in H2DiLR [4], ~500 samples in Singh et al. [7], ~500 samples in Angrick et al. [5]).\n\nFurthermore, while disentangled representation learning is established in computer vision and emerging in neural encoding studies with neural spike data [6], we are the first to adapt it for intracranial decoding in a clinical BCI context. We specifically **collected an articulation benchmark to quantitatively validate its interpretability via codex-wise analysis**."}}, "id": "XUirxUpAdE", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763228081449, "cdate": 1763228081449, "tmdate": 1763316210259, "mdate": 1763316210259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a unified framework for speech decoding via disentangling fine and coarse neural information from intracranial recordings in two complementary stages, i.e., focusing on spatial and temporal information, respectively. Its effectiveness is evaluated comprehensively across six datasets consisting of different signal types and tasks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method for neural disentanglement demonstrates improved performance over previous methods considered. Also the framework is generalizable to different tasks beyond speech decoding and various types of biosignals. Model configuration is well written in detail and also the authors claim the dataset and code will be publicly available upon publication."}, "weaknesses": {"value": "The framing of ‘speech decoding’ as presented is confusing, and can be a bit misleading based on the downstream tasks actually performed. It looks like ‘speech decoding’ is just one part of downstream tasks, and the proposed framework looks like it is suitable for other types of tasks, too, so I do not see any part of the framework that is specific to speech. \n\nAlso, from results (Table 2 and 3), it is difficult to capture the effectiveness of ‘BrainStra.-Coarse’. It appears that the only rows to compare the ‘BrainStra.-Coarse’ condition are the last two rows of Table 2, but it presents contradicting results. A review of additional results in the Appendix did not appear to show any firm evidence to support the effectiveness of ‘Coarse’.\n\nRegarding the articulatory regions (tongue, lips, and jaw), it is not clear how those data are annotated. According to the description, it looks like subjects were instructed to ‘intend’ to move those parts, but unclear whether there was actual articulatory movements. Also, neural underpinning and muscles involved in movements of different parts of tongue (e.g., tongue tip, body, or root) are slightly different, but this is either not properly addressed or if only the tongue tip is taken for analysis. If it is the latter case, I believe it should be clearly stated that ‘tongue tip’, not just ‘tongue’ in general. Also, those articulatory movements are correlated to each other, that is, for instance, jaw or lip movements are related to each other, rather than totally separate, but these points were not properly addressed in the paper.\n\nSome results in some tables do not indicate which types of metrics were used for evaluation. It looks accuracy, but they may also be balanced accuracy, F1 scores, or some other metric,  what exact metrics are being reported is not clear.\n\nWhile the topic is interesting and important, I think this paper is not yet ready for publication and would benefit from significant revision."}, "questions": {"value": "Were there any methods to address subject invariability or generalizability? Also, were there any analysis on different performance among the subjects?\nWhy is this framework framed as ‘speech decoding’ as it is not speech-specific and generalizable to other tasks too?\nWhat do the authors intend by a ‘unified’ framework?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "I do not see any major ethics concerns, but as it involves human subjects, an additional ethics review by designated ethics reviewers would be beneficial."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CThCpNoKti", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Reviewer_721d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Reviewer_721d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868254816, "cdate": 1761868254816, "tmdate": 1762915546641, "mdate": 1762915546641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a two-stage intracranial speech decoding method that (1) identifies relevant channels in the recording via a neural clustering approach and (2) applies a regular deep learning method to classify from the selected channels. Stage (1) does not seem to help improve downstream performance, though stage (2) shows interesting relationships between the learned codex and articulatory actions and performs well in comparison to alternative approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Identifying relevant channels is interesting and a potentially useful procedure for improving downstream performance\n- Interesting analyses of learned codex signals directly relating to articulatory actions\n- Good use of statistical significance and error bars in all analyses"}, "weaknesses": {"value": "- Improvement over multi-channel with the channel selection method is weak based on ablations, suggesting it may not be helpful for downstream decoding (though it does not hurt)\n- Since BrainStratify-Fine is essentially a supervised method, could the authors please also compare to a simple supervised deep learning baseline, e.g. training EEGNet and an MLP baseline\n- Minor: Line 387. Suppresses → surpasses\n- Table 2 and 3 colouring is misleading. Green should not indicate lack of statistical significance\n\nI am willing to raise my score if the authors can address the weaknesses satisfactorily."}, "questions": {"value": "- When evaluating the pre-trained models, did the authors consider end-to-end fine-tuning them rather than linear probing? This might be a fairer comparison to the end-to-end training used for BrainStratify-Fine"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XcirNkQcWn", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Reviewer_cnbL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Reviewer_cnbL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920921170, "cdate": 1761920921170, "tmdate": 1762915546496, "mdate": 1762915546496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a two-stage intracranial speech decoding method that (1) identifies relevant channels in the recording via a neural clustering approach and (2) applies a regular deep learning method to classify from the selected channels. Stage (1) does not seem to help improve downstream performance, though stage (2) shows interesting relationships between the learned codex and articulatory actions and performs well in comparison to alternative approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Identifying relevant channels is interesting and a potentially useful procedure for improving downstream performance\n- Interesting analyses of learned codex signals directly relating to articulatory actions\n- Good use of statistical significance and error bars in all analyses"}, "weaknesses": {"value": "- Improvement over multi-channel with the channel selection method is weak based on ablations, suggesting it may not be helpful for downstream decoding (though it does not hurt)\n- Since BrainStratify-Fine is essentially a supervised method, could the authors please also compare to a simple supervised deep learning baseline, e.g. training EEGNet and an MLP baseline\n- Minor: Line 387. Suppresses → surpasses\n- Table 2 and 3 colouring is misleading. Green should not indicate lack of statistical significance\n\nI am willing to raise my score if the authors can address the weaknesses satisfactorily."}, "questions": {"value": "- When evaluating the pre-trained models, did the authors consider end-to-end fine-tuning them rather than linear probing? This might be a fairer comparison to the end-to-end training used for BrainStratify-Fine"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XcirNkQcWn", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Reviewer_cnbL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Reviewer_cnbL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920921170, "cdate": 1761920921170, "tmdate": 1763340169907, "mdate": 1763340169907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BrainStratify, a two-stage framework for speech decoding from intracranial signals (sEEG/ECoG): Coarse stage learns an inter-channel attention graph with a spatial-context objective and clusters channels into functional groups; Fine stage introduces Decoupled Product Quantization (DPQ) within a VQ-VAE/MAE pipeline to disentangle neural components and guide masked code prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a clear, modular design that mirrors neurophysiology intuition (group → sub-component). The pipeline is well explained with task definitions and training details.\n\n2. The paper is well written. I particularly liked the codex-wise analyses on the articulation benchmark (jaw/lips/tongue) that add interpretability and are a nice touch toward making the discrete units meaningful."}, "weaknesses": {"value": "1. Novelty is incremental relative to known building blocks. There is little algorithmic innovation beyond integrating well-known components; DPQ is essentially PQ with independence encouragement and used to supervise MAE via code indices.\n\n2. The new epidural ECoG datasets each contain one subject; most modeling is subject-specific. Therefore, the claims of robust generalization and neuroscience-inspired modularity are not convincingly demonstrated across patients or sessions. Cross-subject transfer (train on N−1, test on held-out subject) would better substantiate robustness.\n\n3. While conceptually appealing, this two-stage design is highly complex, integrating transformers, convolutional encoders, spectral clustering, a VQ-VAE with multiple codebooks, and a masked autoencoding step. The sheer number of components makes it difficult to pinpoint which aspects drive improvements. \n\n4. Moreover, certain design choices are not fully justified. The authors mention adding “either learnable or MNI-based” spatial embeddings before the spatial transformer, implying a choice between using standardized brain coordinates (MNI space) or learning positional encodings. It remains unclear which option was used in final experiments and how much this choice impacts performance or interpretability. \n\nWithout explicit ablations or rationale for these architectural decisions, the rigor of the design is in question. The method appears over-engineered, stitching together many recent techniques (transformers, product quantization, masked modeling) and this raises the possibility that a far simpler architecture might achieve similar results.\n\nMinor comment: Fig. 2 caption is missing (c)."}, "questions": {"value": "I am a bit curious as to how the functional group of channels is ultimately selected for decoding. The coarse stage clusters electrodes into groups, but the paper admits that it select[s] and combine[s] these groups \"based on their performance in downstream decoding tasks.”. In other words, after unsupervised clustering, the authors use supervised task performance to decide which cluster (or combination of clusters) is relevant. This procedure risks information leakage or overfitting if not handled carefully. It blurs the line between an unsupervised pretext step and the supervised evaluation. The paper does not detail whether this selection was done using a held-out validation set, how many labeled examples were used, or how they avoided biasing the final results. Can the authors please explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "anUm71uvAN", "forum": "SG5ivsCB76", "replyto": "SG5ivsCB76", "signatures": ["ICLR.cc/2026/Conference/Submission560/Reviewer_5Gy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission560/Reviewer_5Gy2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762476235457, "cdate": 1762476235457, "tmdate": 1762915546305, "mdate": 1762915546305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}