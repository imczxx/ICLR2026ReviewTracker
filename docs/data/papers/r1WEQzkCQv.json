{"id": "r1WEQzkCQv", "number": 7696, "cdate": 1758032507465, "mdate": 1759897838802, "content": {"title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "abstract": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.", "tldr": "", "keywords": ["large language model", "reasoning", "latent reasoning", "chain of thought"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff0285579ca4549f8dd9d9b25a15e6ed5f3fc9ce.pdf", "supplementary_material": "/attachment/fd9ea89ab1ac3dce2ba548f214b8f9dd1f33c3d0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces LIPO, a test-time optimization framework that enhances LLM’s reasoning abilities without updating its parameters. It uses internal confidence as a reward to optimize latent thought, which is then integrated with the prompt to generate a solution. They empirically show robust performance on the reasoning benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper considers using internal reward to optimize the latent thought, which is a test-time method, and does not need additional data."}, "weaknesses": {"value": "see questions."}, "questions": {"value": "How to choose the variance hyperparameter during updating is not explained.\nFor confidence, there are many metrics to measure it, like entropy, KL over a uniform distribution. What is the advantage of using negative log probability?\nThe Monte Carlo estimation of the gradient has large variance, as it only uses one sample.\nAlthough many papers use internal confidence as a reward, I'm not sure if maximizing confidence is really meaningful, as it will reduce the diversity of the model.\nWhy use greedy decoding to generate the final answer? The technical report of Qwen mentions the optimal parameters, like temperature = 0.7, top-p = 0.8, and top-k = 20. Also, for the generation length, can you make it larger, as the Qwen3-14B can achieve good performance like 79.3 on AIME24 and 70.4 on AIME25, when the generation length equals 64k. Using a small generation length on these challenge reasoning tasks (AIME) is meaningless, as most methods didn’t complete the generation, achieving very low scores.\nIs the optimal number of thought tokens and number of iterations the same for different reasoning tasks and different models? \nCan you compare more baselines? Such as Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space or SLOT: Sample-specific Language Model Optimization at Test-time, which is also test-time for latent optimization.\nWhy is LTPO faster than zero-thot COT, as it needs optimization, but later does not need optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kxEMVFqmWD", "forum": "r1WEQzkCQv", "replyto": "r1WEQzkCQv", "signatures": ["ICLR.cc/2026/Conference/Submission7696/Reviewer_JXR4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7696/Reviewer_JXR4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874481962, "cdate": 1761874481962, "tmdate": 1762919755909, "mdate": 1762919755909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Latent Thought Policy Optimization (LTPO), a framework for improving the reasoning ability of Large Language Models at test-time. The core idea is concatenating the prompt with K tokens as latent thoughts and improving the latent representation via policy gradient with confidence as reward, derived from the model’s output probabilities on these latent tokens.\nThe initial latent tokens are set to be the embedding of [THINK] token and policy is conditional Gaussian with controlled variance.\nThe method is evaluated on five reasoning benchmarks: GSM8K, MATH-500, ASDiv-Aug, AIME2024, and AIME2025. The results outperform Zero-Shot CoT, CoT-Unk, and SoftCoT baselines on above benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear Motivation and Problem Framing: The paper is well-motivated, presenting a clear method for test-time latent space optimization.\n- The proposed algorithm LTPO is sound, lightweight with broad applicability.\n- The empirical result shows robust improvement over several latent reasoning baselines."}, "weaknesses": {"value": "1. Missing Comparison to Highly Relevant Contemporaneous Work. The major weakness is missing comparison to highly relevant work LatentSeek (Li et al., 2025). Both frameworks introduce a near-identical method: test-time optimization of latent representations using a policy gradient to enhance reasoning. The primary distinction lies in the reward function. LTPO proposes a logit-based confidence score specifically to avoid text generation during the optimization loop. In contrast, LatentSeek requires decoding the entire sequence at each step. Given that LTPO's main claimed advantage is its efficiency compared to LatentSeek, a direct empirical comparison and trade-off discussion against LatentSeek is essential to validate the paper's central contribution.\n2. Questionable Justification for Using a Policy Gradient. The paper chooses R that is \"non-differentiable with respect to the input latent vectors\", which in turn necessitated the use of a policy optimization method like REINFORCE. This is technically true as top-k is involved in the loss calculation, and the paper conduct ablation study that shows small k is necessary for good performance. However, it fails to justify this fundamental design choice over a simpler, fully-differentiable alternative. For instance, the goal of \"improving confidence\" could be achieved by directly minimizing the entropy of the output distribution using standard, and typically more stable, backpropagation. And with backpropagation, the dependence on k might be different and might offer a more stable and efficient path to optimizing the latent tokens. The paper should have included a discussion on the design choice as policy gradient method provides higher variance and design bias/complexity."}, "questions": {"value": "1. Could you provide an empirical comparison with LatentSeek (Li et al., 2025)? Given the similar test-time optimization paradigm, a detailed discussion is necessary to distinguish the pros and cons of both method.\n2. Could you justify the choice of the non-differentiable top-k reward function? This choice necessitates using a policy gradient, and it would be helpful to understand why this was preferred over a fully-differentiable metric.\n\nI will raise my score once any of the questions above are properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "m0ycJCjMiw", "forum": "r1WEQzkCQv", "replyto": "r1WEQzkCQv", "signatures": ["ICLR.cc/2026/Conference/Submission7696/Reviewer_Xm7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7696/Reviewer_Xm7C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890589803, "cdate": 1761890589803, "tmdate": 1762919754368, "mdate": 1762919754368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LTPO - a method that adds certain latent thought token vectors to the input and at test time tries to find optimal vectors via an intrinsic model confidence based reward. After certain iterations at test time, the optimal though vectors are concatenated with the input tokens to obtain the outputs. The paper shows that LTPO outperforms existing methods like zero shot and soft CoT in most cases on math datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Adding thought tokens that can be optimized via model's own confidence based rewards is a powerful way to improve reasoning.\n2. The methods overcomes the efficiency issue of producing long CoT traces.\n3. LTPO method outperforms popular techniques like zero shot and soft CoT on math tasks\n4. Paper is well written and easy to follow"}, "weaknesses": {"value": "1. One major drawback is lack of sufficient experiments. While experiments are conducted on math datasets, the paper can be strengthened by showing the method works on other kinds of tasks as well\n2. Lack of generalization experiments for LTPO: extend to other domains"}, "questions": {"value": "1. Are the latent thought tokens initialized to [THOUGHT] for every test instance x, or is the optimal thought vector for one instance used as a starting initialization for the next test instance?\n2. Howcome the accuracy remains stable when 1 to 16 thought tokens are used? Any thought? Does that mean the more thought tokens being used do not learn any additional signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AHcKWfnDO6", "forum": "r1WEQzkCQv", "replyto": "r1WEQzkCQv", "signatures": ["ICLR.cc/2026/Conference/Submission7696/Reviewer_3uYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7696/Reviewer_3uYZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965307578, "cdate": 1761965307578, "tmdate": 1762919754071, "mdate": 1762919754071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LTPO, a parameter-free, test-time method that improves LLM reasoning by optimizing latent thought token embeddings with an online policy-gradient loop, guided by a confidence-based intrinsic reward computed from the model’s own output distributions. The method perturbs latent thought vectors with a Gaussian policy, scores them via a top-k log-probability reward, and ascends the estimated gradient; the best latent state found is then used to decode the final answer. Experiments across GSM8K, MATH-500, ASDiv-Aug, and AIME24/25 show consistent gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written and structured, followed by the motivation and explanation.\n2. LTPO formalizes latent-token search as test-time RL with a simple Gaussian exploration policy and confidence reward.\n3. On AIME24/25 benchmarks, LTPO avoids the collapse seen in trained latent methods and surpasses Zero-Shot CoT on several models."}, "weaknesses": {"value": "1. In Figure 2, the best performance is achieved with 1 or 2 thoughts, and increasing the number of thought tokens does not necessarily lead to better results. Since the base model, Llama 3.1 8B, is not capable of generating long chains of thought, I suggest evaluating the method using models with stronger reasoning ability, such as the DeepSeek Distill series.\n2. The reward is computed only at latent token positions, but it would be helpful for the authors to justify why this design choice can predict the correctness of the final answer.\n3. In Line 190, the authors mention that the reward is “not a perfect proxy for correctness” and refer to Appendix B for discussion. I suggest adding a hybrid reward function that combines model confidence with a verifier to reduce overconfidence bias and better align the reward with true correctness."}, "questions": {"value": "What is the rollout setting during evaluation? Do the authors generate the response only once per question, or perform multiple rollouts? I suggest reporting results using pass@k."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ieS6cC7NMw", "forum": "r1WEQzkCQv", "replyto": "r1WEQzkCQv", "signatures": ["ICLR.cc/2026/Conference/Submission7696/Reviewer_gpmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7696/Reviewer_gpmg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201793986, "cdate": 1762201793986, "tmdate": 1762919753676, "mdate": 1762919753676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}