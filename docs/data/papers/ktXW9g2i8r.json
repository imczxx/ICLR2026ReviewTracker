{"id": "ktXW9g2i8r", "number": 628, "cdate": 1756757170507, "mdate": 1763119748683, "content": {"title": "Directional Ensemble Aggregation for Actor-Critics", "abstract": "Reliable $Q$-value estimation is central to off-policy reinforcement learning in continuous control. Standard actor-critic methods often address overestimation bias by aggregating ensembles of $Q$-values conservatively, for example by taking their minimum. While effective at reducing bias, these static rules discard useful information, cannot adapt to training dynamics, and generalize poorly across learning regimes. We propose Directional Ensemble Aggregation (DEA), a fully learnable aggregation method that replaces static aggregation with a dynamic mechanism capable of interpolating between conservative and explorative strategies as training progresses. DEA introduces two learnable directional parameters, one regulating critic conservatism and the other guiding actor exploration. Both are learned using disagreement-weighted Bellman errors, where updates depend only on the sign of each sample’s error. This decoupled design allows DEA to adjust automatically to task-specific uncertainty, ensemble size, and update frequency in a data-driven manner. Empirically, DEA generalizes across MuJoCo and DeepMind Control Suite benchmarks in both interactive and sample-efficient learning regimes.", "tldr": "Directional Ensemble Aggregation (DEA) is a fully learnable actor-critic method that adaptively balances conservatism and exploration by aggregating $Q$-values based on ensemble disagreement.", "keywords": ["reinforcement learning", "off-policy", "actor-critic", "adaptive ensemble learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3f7e81ff889e79311e6c61095f8fb25896d2b3bd.pdf", "supplementary_material": "/attachment/96600b49e932e7dd5c8a22a73a7c2813292062bb.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Directional Ensemble Aggregation (DEA), a novel method designed to improve Q-value estimation in off-policy reinforcement learning (RL). Traditional actor-critic methods  often rely on static aggregation strategies, such as taking the minimum Q-value from an ensemble of critics, which can lead to  loss of valuable information. DEA aims to replace these static rules with a dynamic, learnable approach that adjusts based on training dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. DEA presents a unique, learnable aggregation method that can learn from  task-specific uncertainties and ensemble dynamics. This is a significant advancement over traditional methods that use fixed aggregation strategies, which can hinder performance and adaptability.\n2. The paper provides empirical evidence demonstrating DEA's effectiveness across various benchmarks, outperforming existing methods like SAC and REDQ.\n3. The use of metrics such as Final Return, InterQuartile Mean (IQM), and Area Under the Learning Curve (AULC) provides a comprehensive evaluation of the method's effectiveness"}, "weaknesses": {"value": "1. The paper lacks a  theoretical analysis to prove the convergence of new TD update. Specifically, as shown in Equation (4), an extra term $\\bar{k}\\cdot \\bar{\\delta}(s,a)$ is introduced for TD update. This may affect the convergence of the operator.\n  2. The introduction of learnable parameters for aggregation increases the complexity of the algorithm. This complexity may require more careful tuning and could pose challenges in terms of computational efficiency and stability during training.\n  3. In ensemble disagreement, $\\delta$ is calculated by averaging the differences between the valuations of any two Q-nets in the Q-net set. The computational complexity of is $O(N^2)$. Why not choose the standard deviation of multiple Q-net estimation?\n  4. Why choose to train two k and two $\\delta$?"}, "questions": {"value": "1. Why are the results in Figure 3 and Table 2 inconsistent?\n 2. Why choose to train two k and two $\\delta$? Are the learnable parameters redundant?\n 3. Is the current delta calculation method reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j9dwzVAXpk", "forum": "ktXW9g2i8r", "replyto": "ktXW9g2i8r", "signatures": ["ICLR.cc/2026/Conference/Submission628/Reviewer_zbpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission628/Reviewer_zbpB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186826830, "cdate": 1761186826830, "tmdate": 1762915570045, "mdate": 1762915570045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "HvU0B9d0bh", "forum": "ktXW9g2i8r", "replyto": "ktXW9g2i8r", "signatures": ["ICLR.cc/2026/Conference/Submission628/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission628/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119747412, "cdate": 1763119747412, "tmdate": 1763119747412, "mdate": 1763119747412, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Directional Ensemble Aggregation (DEA), a fully learnable aggregation method that replaces static aggregation with a dynamic mechanism, allowing interpolation between conservative and explorative strategies as training progresses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes Directional Ensemble Aggregation for DRL and conducts extensive experiments across various tasks."}, "weaknesses": {"value": "1. Can the definition of Ensemble disagreement vary? How would different definitions impact performance?  \n2. How does Ensemble disagreement and the performance of the proposed method change as the number of critics increases? It is recommended to analyze this through experiments.  \n3. It is suggested that the authors provide theoretical proof to demonstrate: 1) the convergence of the proposed method, and 2) why the proposed method outperforms SOTA solutions.  \n4. The baseline comparisons are too few; at least five more SOTA solutions should be included for comparison.  \n5. Table 3 should also present the standard deviation from 10 seed runs to show the variability of each method.  \n6. Intermediate results should be presented to demonstrate how the proposed method improves exploration capabilities (e.g., by showing policy entropy) or Q-value estimation (the difference from true Q-values) compared to SOTA solutions."}, "questions": {"value": "1. Can the definition of Ensemble disagreement vary? How would different definitions impact performance?  \n2. How does Ensemble disagreement and the performance of the proposed method change as the number of critics increases? It is recommended to analyze this through experiments.  \n3. It is suggested that the authors provide theoretical proof to demonstrate: 1) the convergence of the proposed method, and 2) why the proposed method outperforms SOTA solutions.  \n4. The baseline comparisons are too few; at least five more SOTA solutions should be included for comparison.  \n5. Table 3 should also present the standard deviation from 10 seed runs to show the variability of each method.  \n6. Intermediate results should be presented to demonstrate how the proposed method improves exploration capabilities (e.g., by showing policy entropy) or Q-value estimation (the difference from true Q-values) compared to SOTA solutions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mhQv8jiJ4l", "forum": "ktXW9g2i8r", "replyto": "ktXW9g2i8r", "signatures": ["ICLR.cc/2026/Conference/Submission628/Reviewer_h1ne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission628/Reviewer_h1ne"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551043401, "cdate": 1761551043401, "tmdate": 1762915569466, "mdate": 1762915569466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Directional Ensemble Aggregation (DEA) for off-policy actor–critic RL. Instead of using a fixed rule to combine critic heads, DEA learns two small scalar parameters to build the target critic used for Bellman updates and the actor-update critic used for policy improvement respectively. Both parameters are adapted online from the ensemble’s internal disagreement signal, and their learning uses only the direction of the prediction error (over/under-shoot) to keep updates stable and noise-tolerant. Conceptually, DEA decouples how conservative the target should be from how optimistic the actor update can be, allowing the method to interpolate automatically between cautious and exploratory behavior as uncertainty changes during training. The approach integrates into a standard SAC pipeline with minimal code changes and no architectural overhead. Experiments on MuJoCo and DeepMind Control report improvements over SAC and REDQ when results are aggregated across the interactive and sample-efficient regimes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple and well-grounded intuition: The method introduces a clean, decoupled aggregation mechanism for the target and actor critics that integrates seamlessly into SAC/REDQ pipelines with minimal engineering effort. Using ensemble disagreement to adjust optimism and conservatism is conceptually sound, and the sign-only adaptation is a practical way to stabilize updates and reduce variance.\n2. Clear and interpretable: The paper is easy to follow, and the explanations of how the two learned parameters evolve during training help readers understand the method’s behavior in practice."}, "weaknesses": {"value": "1. Unfair aggregation across regimes: The paper reports results by averaging performance across the interactive and sample-efficient regimes, but these two settings differ substantially in both algorithmic design intent and compute budgets (UTD ratio, ensemble size, and wall-clock cost). SAC is primarily designed for the interactive regime, emphasizing stability and low update-to-data ratios, while REDQ is optimized for sample efficiency with higher update ratios and larger ensembles. Aggregating their results into a single metric mixes incompatible evaluation conditions and obscures each method’s actual strengths. As a result, the reported average performance does not reflect a fair or meaningful comparison—DEA’s improvement could stem from outperforming SAC in one regime and underperforming against REDQ in the other, yet the aggregation hides this distinction. A clear, per-regime comparison is needed to judge where DEA truly provides benefits.\n2. Insufficient ablations despite the claim: The paper states that spanning two regimes “naturally” serves as ablations over ensemble size and UTD. In practice, only the aggregated performance of two points is shown (ensemble size 2 vs. 10 with paired UTD settings). This does not characterize sensitivity or robustness\n3. Unclear magnitude of gains: Even in the aggregated results, DEA’s improvement is not clearly demonstrated. While the method shows noticeable gains in area-under-the-learning-curve (AULC), the improvements on final performance and IQM are marginal. The paper should report the mean and median percentage improvements as well as the standard deviations across runs to better illustrate the actual magnitude of performance gains."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u7ZLqDJa7H", "forum": "ktXW9g2i8r", "replyto": "ktXW9g2i8r", "signatures": ["ICLR.cc/2026/Conference/Submission628/Reviewer_nfo9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission628/Reviewer_nfo9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705027159, "cdate": 1761705027159, "tmdate": 1762915569326, "mdate": 1762915569326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses reliable Q-value estimation in off-policy reinforcement learning for continuous control and highlights limitations of standard actor-critic methods: they mitigate overestimation bias by conservatively aggregating Q-value ensembles (e.g., taking the minimum), which discards useful information, cannot adapt to training dynamics, and generalizes poorly across learning regimes. To address these issues, this paper proposes Directional Ensemble Aggregation (DEA), a fully learnable aggregation method that replaces static rules with a dynamic mechanism capable of interpolating between conservative and explorative strategies as training progresses. DEA uses a decoupled aggregation with two learnable directional parameters: $\\(\\bar{\\kappa}\\)$, which constructs the aggregation that guides critic learning, and $\\(\\kappa\\)$, which constructs the aggregation that guides actor learning. Both parameters are learned directly from data using disagreement-weighted Bellman errors, with updates depending only on the sign of the error for each sample. This design allows DEA to adjust automatically to task-specific uncertainty, ensemble size, and update frequency. \n\nEmpirically, across MuJoCo and the DeepMind Control Suite, DEA maintains reliable learning dynamics and outperforms static ensemble aggregation methods in both interactive and sample-efficient learning regimes, demonstrating strong generalization across settings where static rules often fail."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is clearly written and provides a well-structured explanation of the motivation behind DEA and its design, making the proposed dynamic aggregation mechanism easy to follow and understand.\n2) The paper conducts extensive experiments on two benchmark suites, MuJoCo and the DeepMind Control Suite, and the results demonstrate that DEA consistently outperforms both interactive methods (SAC) and sample-efficient methods (REDQ)."}, "weaknesses": {"value": "1) The paper lacks theoretical analysis or convergence guarantees to support the proposed method, making its stability and effectiveness rely solely on empirical evidence.\n2) The proposed method introduces high computational complexity, especially due to the need to compute ensemble disagreement, yet the paper does not provide any analysis or discussion of the associated computational or resource overhead."}, "questions": {"value": "1) Figures 10-13 show that the ensemble disagreement decreases significantly as training progresses. However, in Figure 1, the two directional parameters $\\kappa$ and $\\bar{\\kappa}$ do not exhibit a clear trend of change, especially after 0.2M interactions. Intuitively, as the ensemble disagreement decreases over training, $\\kappa$ is expected to gradually increase to promote more optimistic exploration. Could the authors clarify why this expected pattern is not observed?\n2) At the beginning of training, $\\kappa$ is initialized to zero while $\\bar{\\kappa}$ is initialized to a negative value. Could the authors explain the motivation behind these choices? A brief justification would help readers better understand the initialization strategy and the intended behavior of DEA during early training.\n3) Furthermore, ensemble disagreement is typically large during the early phase of training. In this case, one might expect $\\bar{\\kappa}$ to be small or even negative to avoid overestimation. However, the results show that $\\bar{\\kappa}$ is relatively large at the beginning compared to its later values. Could the authors elaborate on why this occurs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1cvecP1tRJ", "forum": "ktXW9g2i8r", "replyto": "ktXW9g2i8r", "signatures": ["ICLR.cc/2026/Conference/Submission628/Reviewer_oa1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission628/Reviewer_oa1b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741707027, "cdate": 1761741707027, "tmdate": 1762915569194, "mdate": 1762915569194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}