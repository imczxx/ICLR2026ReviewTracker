{"id": "rP6LGu8qPo", "number": 19072, "cdate": 1758293309510, "mdate": 1763542109752, "content": {"title": "Consistent Low-Rank Adaptation of Two-layer Neural Networks: A Nonparametric Statistics Approach", "abstract": "Low-Rank Adaptation (LoRA) is a powerful technique for fine-tuning Large Language Models (LLMs), offering greater parameter efficiency and improved generalization in data-constrained settings. While its advantages makes it highly promising for general transfer learning, its reliance on iterative optimization methods such as SGD still demands substantial computation and poses a challenge for theoretical analysis.We propose a novel two-step, closed-form approach for LoRA in two-layer feedforward neural networks (FNN) that mitigates the reliance on iterative algorithms. First, by leveraging Stein’s lemma, a classical statistical tool, we derive an analytical estimator for the first-layer LoRA parameters. Second, we solve for the second-layer parameters via reduced-rank ridge regression. We provide theoretical guarantees for the low-rank parameter estimation under a projection adaptation assumption: the optimal first layer adaptation removes irrelevant directions via subspace projection. This generalizes the concept of rank pruning, which removes irrelevant low-rank components from a weight matrix.Crucially, our solution is non-iterative and computationally efficient, computing the full adaptation in seconds—a fraction of the time required by SGD-based LoRA. Numerical experiments on MNIST suggest that our method not only significantly reduces computational cost and achieves prediction performance comparable to that of a fully trained LoRA model, but also serves as a good initialization for SGD-based LoRA.", "tldr": "", "keywords": ["Low-rank model", "Transfer Learning", "Deep Neural Networks", "Stein's Lemma", "Ridge regression"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4d0d87884beb269235c2dbb189727e073bb22a07.pdf", "supplementary_material": "/attachment/3872a152ba6036fb6dec29dd5fde4f1d4d17a200.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates LoRA (Low-Rank Adaptation) in the context of a two-layer neural network. By applying Stein’s second-order lemma, the authors derive a closed-form solution, eliminating the need for iterative SGD optimization and offering significant computational efficiency. Experiments on MNIST are presented to demonstrate the effectiveness of the proposed solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "*  The paper takes a clever and analytically tractable approach by simplifying the setup to make use of Stein’s second-order lemma.\n*  The approach provides computational advantages, avoiding iterative training loops."}, "weaknesses": {"value": "*  The formulation may be an over-simplification, limiting the practical applicability of the findings to real-world deep networks.\n*  As a result, the theoretical contribution might have narrow scope, primarily useful as a pedagogical or illustrative case rather than as a general method.\n*  Experimental validation is weak: MNIST is a toy dataset, and no additional empirical evidence is provided to support generalization beyond simple settings.\n*  The claim that the closed-form solution can serve as a strong initialization for LoRA is not empirically verified."}, "questions": {"value": "See weakness, and\n1. Line 131: The function $f$ appears to be undefined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C43VmKrA0b", "forum": "rP6LGu8qPo", "replyto": "rP6LGu8qPo", "signatures": ["ICLR.cc/2026/Conference/Submission19072/Reviewer_2EjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19072/Reviewer_2EjR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884582101, "cdate": 1761884582101, "tmdate": 1762931097666, "mdate": 1762931097666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Yb6Ai0h8ZR", "forum": "rP6LGu8qPo", "replyto": "rP6LGu8qPo", "signatures": ["ICLR.cc/2026/Conference/Submission19072/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19072/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763542109012, "cdate": 1763542109012, "tmdate": 1763542109012, "mdate": 1763542109012, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LoRA is the method of choice to finetune LLMs or any DN in general. This is typically done by SGD since the nonlinearity of the DN prevents any closed-form. The paper looks at an extremely simplified model--under which it is possible to obtain a closed-form solution. Having a closed-form solution is great, but as it can only be applicable to two-layer networks which are not used in practice, the actual impact of the study feels absent. It is also unclear if even within that space of models the method has any impact on challenging datasets.\n\n** Strength **\n- While I do not believe the considered setup can be useful in practice (see below), when looking only at this particular setup the work is novel\n- The presentation provides clear mathematical results that can be used in practice--albeit in a non-realistic setup\n- The need for more theory around LoRA is high hereby making the paper quite well timed with today's research problem\n\n** Weakness **\n- while there are some minor typos, and some grammatical issues, the entire writing is OK to follow\n- the major issue is the limitation of the method to two-layer models which is extremely far from the LoRA setup with attention and multiple blocks. This is a key limitation since none of the results can be leveraged as of today in existing models--and insights from shallow networks do not extend to deep networks as well demonstrated by the numerous NTK and similar works.\n- beyond the shallow network limitation, the datasets considered are extremely simple to solve for any algorithm and thus do not provide meaningful signal to assess the quality of the propose method\n- lastly, the presentation is not friendly to practitioners from different fields which reduces the impact of the submission even further"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Please see summary"}, "weaknesses": {"value": "Please see summary"}, "questions": {"value": "Please see summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sHyCQOZQo6", "forum": "rP6LGu8qPo", "replyto": "rP6LGu8qPo", "signatures": ["ICLR.cc/2026/Conference/Submission19072/Reviewer_tHrb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19072/Reviewer_tHrb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971363396, "cdate": 1761971363396, "tmdate": 1762931097203, "mdate": 1762931097203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel, non-iterative method for LoRA as an alternative to gradient-based fine-tuning. The method is restricted to two-layer neural networks and computes a closed-form solution in two steps: \n\nLayer 1: It uses Stein's lemma to derive an analytical, closed-form estimator for the first-layer adaptation matrix $\\hat{\\Delta}_1$.\n\nLayer 2: It solves for the second-layer adaptation $\\hat{\\Delta}_2$ using standard reduced-rank ridge regression.\n\nThe authors provide theoretical consistency guarantees under a projection adaptation assumption and claim the method is comparable to standard LoRA while being significantly faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper preposes Stein-based subspace recovery for layer 1 and reduced-rank regression for layer 2. This statistical analysis is novel and has a nice view. If the assumptions were to hold, the computational speedup is a significant practical benefit.\n\n2. The paper demonstrates that its closed-form solution potentialy serves as a high-quality, non-zero initialization for standard gradient-based LoRA. This \"SGD-I\" baseline consistently outperforms standard LoRA with zero initialization ."}, "weaknesses": {"value": "1. The proposed method only applies to a simple two-layer feedforward network. This is not a general-purpose LoRA replacement and has almost no applicability to adapting modern LLMs. Framing as “consistent LoRA” feels incremental relative to existing dimension-reduction regression and reduced-rank literature. \n\n2. The proposed method is limited to some restrictive assumptions. For example, it requires knowing the second-order score function $S(z)$ of the input representation $Z = \\phi(X)$. This is impossible in a practical setting (e.g. for an LLM's hidden states). The paper circumvents this on MNIST only by assuming the latent space is Gaussian, which is a major simplification .\n\n3. The paper lacks experiments that represent real world problems. The only \"real-world\" experiment is on MNIST which is a toy problem that does not support the paper's claims about LLM fine-tuning. Furthermore, the network being adapted is tiny (input dim 20, hidden dim 1024), which is not representative of any modern task."}, "questions": {"value": "Please see Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6rO58PsRef", "forum": "rP6LGu8qPo", "replyto": "rP6LGu8qPo", "signatures": ["ICLR.cc/2026/Conference/Submission19072/Reviewer_nCFn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19072/Reviewer_nCFn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999147240, "cdate": 1761999147240, "tmdate": 1762931096734, "mdate": 1762931096734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a non-iterative alternative to Low-Rank Adaptation (LoRA) for two-layer neural networks. By combining Stein’s lemma with reduced-rank ridge regression, the method analytically estimates low-rank adaptation parameters without iterative optimization. Theoretical consistency guarantees under a projection adaptation assumption are provided, and comparable accuracy to SGD-based LoRA is achieved while being orders of magnitude faster. The approach also serves as an effective initialization for iterative fine-tuning, offering a fast, theoretically grounded framework for efficient transfer learning"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical guarantees are very complete, and the algorithm is very nice. The 2-layer structure is taken advantage of to the fullest extent. The experiments with 2-layer networks (and fixed feature maps) also consistently show improvement upon the standard SGD training."}, "weaknesses": {"value": "I'm really unsure about the generalizability of the knowledge provided in this work. In practice, the neural networks will have more than 2 layers. The proposed training algorithm seems difficult to generalize to the setup of multiple layers (the authors mention that the generalization to 3 or more layers is possible when second-order score functions for hidden activations are available, but this seems like an unrealistic assumption).\n\nI understand that deep learning theory often relies on stylized and restrictive assumptions, but the setup being studied should have some shared common structure with the practical setting. In this case, the setup being studied has a different architecture and a different algorithm compared to the standard deep network with LoRA modules trained via SGD."}, "questions": {"value": "In what sense is the analysis of this work informative for the standard practical LoRA training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "36cnvm6j6u", "forum": "rP6LGu8qPo", "replyto": "rP6LGu8qPo", "signatures": ["ICLR.cc/2026/Conference/Submission19072/Reviewer_9z6j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19072/Reviewer_9z6j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245119941, "cdate": 1762245119941, "tmdate": 1762931096415, "mdate": 1762931096415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}