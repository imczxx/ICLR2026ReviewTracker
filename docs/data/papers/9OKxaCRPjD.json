{"id": "9OKxaCRPjD", "number": 5925, "cdate": 1757946656795, "mdate": 1759897944462, "content": {"title": "Mitigating Surgical Data Imbalance with Dual-Prediction Video Diffusion Model", "abstract": "Surgical video datasets are essential for scene understanding, enabling procedural modeling and intra-operative support. However, these datasets are often heavily imbalanced, with rare actions and tools under-represented, which limits the robustness of downstream models. We address this challenge with SurgiFlowVid, a sparse and controllable video diffusion framework for generating surgical videos of under-represented classes. Our approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples. In addition, a sparse visual encoder conditions the generation process on lightweight signals (e.g., sparse segmentation masks or RGB frames), enabling controllability without dense annotations. We validate our approach on three surgical datasets across tasks including action recognition, tool presence detection, and laparoscope motion prediction. Synthetic data generated by our method yields consistent gains of 10‚Äì20% over competitive baselines, establishing SurgiFlowVid as a promising strategy to mitigate data imbalance and advance surgical video understanding methods.", "tldr": "SurgiFlowVid: A dual-prediction diffusion framework using RGB and optical flow with sparse conditioning to address surgical video data imbalance, yielding 10‚Äì20% downstream performance gains.", "keywords": ["Synthetic data", "Surgical Data Science", "Data imbalance", "Video diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fb1804b498348b5a1e66d2e688ca6a19d258589.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents SurgiFlowVid, a controllable video diffusion model that generates synthetic surgical videos to address data imbalance. It jointly denoises RGB frames and optical flow for better motion modeling and uses sparse visual conditioning for controllable generation. Experiments on three datasets show 10‚Äì20% gains across surgical video understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Addresses an important issue of data imbalance in surgical video datasets.\n2) Incorporates optical flow as an additional modality, which reasonably enhances downstream performance.\n3) Shows consistent quantitative improvement across tasks, demonstrating the potential of data synthesis for surgical video analysis."}, "weaknesses": {"value": "1) Two-branch design (RGB + optical flow) is computationally heavy and lacks efficiency analysis.\n2) Technical novelty is limited; using multi-branch or multi-task setups to boost performance is a common approach.\n3) Insufficient technical details‚Äîunclear how the control branch conditions generation or how intermediate frames are produced from sparse inputs.\n4) Unclear definition of ‚Äúunder-represented‚Äù frames or classes.\n5) Table 1 results not convincing, simple data augmentation baselines could achieve comparable gains.\n6) Missing ablation studies (e.g., one vs. two branches, with vs. without sparse guidance) to isolate the source of improvement.\n7) No computational efficiency comparison, especially considering the added cost of optical flow processing."}, "questions": {"value": "Optimization and efficiency details are missing. What is the computational overhead introduced by optical flow in inference stage? And others refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qEKs5mbe0Z", "forum": "9OKxaCRPjD", "replyto": "9OKxaCRPjD", "signatures": ["ICLR.cc/2026/Conference/Submission5925/Reviewer_R83Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5925/Reviewer_R83Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805057338, "cdate": 1761805057338, "tmdate": 1762918355231, "mdate": 1762918355231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SurgiFlowVid, a controllable video diffusion framework to mitigate severe class imbalance in surgical video datasets by synthesizing short clips for under-represented actions/tools. The two main technical ideas are: (i) a dual-prediction diffusion U-Net that jointly denoises RGB frames and optical flow, injecting a temporal motion prior during training; and (ii) a sparse visual encoder that conditions generation on lightweight signals (text, sparse segmentation masks, or RGB keyframes) without requiring dense per-frame labels. The method builds on a two-stage video DM pipeline (spatial SD + temporal transformers) and freezes spatial layers while training temporal layers and the dual-head I/O. Experiments on SAR-RARP50, GraSP, and AutoLaparo show consistent boosts (typically ~10‚Äì20%) for rare classes across action recognition, tool presence detection, and laparoscope motion prediction versus Endora, SurV-Gen (¬± rejection sampling), and SparseCtrl variants; ablations indicate that temporal consistency and spatial structure matter, whereas naive duplication/shuffling hurts. Limitations include short (‚âà4 s) clips and occasional tool misplacement under sparse conditioning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-motivated objective (rare-class augmentation) with domain relevance; aligns with documented imbalance in SAR-RARP50/GraSP.\n\nTemporal inductive bias via flow: predicting motion jointly with appearance is principled and consistent with general-domain findings that explicit motion priors improve coherence.\n\nSparse masks/RGB keyframes are realistic for surgical datasets and extend SparseCtrl-style conditioning to this domain\n\nThree datasets (SAR-RARP50, GraSP, AutoLaparo) and three downstream tasks show consistent, practically relevant gains; class-wise reporting is appreciated.\n\nDemonstrate that duplication ‚â† diversity; shuffling/noise degrade performance; sparse-only is insufficient‚Äîsupporting the method‚Äôs design choices."}, "weaknesses": {"value": "The paper encodes optical flow to RGB and uses it only at training time, but does not quantify sensitivity to (a) the flow estimator (e.g., RAFT vs. alternatives), (b) flow noise, or (c) the weighting ùúÜùëù. Motion quality metrics (e.g., warping error, flow-consistency) are absent.\n\nResults are for ~4-second clips; impact on longer sequences (phase recognition) remains untested. An autoregressive extension is suggested but not validated.\n\nBaselines include Endora, SurV-Gen, SparseCtrl, but SurgSora (RGBD+flow controllable) and other recent controllable methods are only cited; direct side-by-side comparisons would strengthen claims.\n\nQualitative failure (tool position drift) is shown; quantitative analyses of conditioning faithfulness (mask overlap over time, keyframe similarity) are missing.\n\nAll datasets are from related laparoscopic/robotic procedures; cross-procedure transfer (e.g., cholecystectomy) is not studied."}, "questions": {"value": "Long-horizon behavior. What breaks when generating ‚â•16 frames? Is an autoregressive strategy (as in FlowVid) feasible with your frozen-spatial setup? \n\nWill you release the synthetic videos per class and the visual encoder checkpoints to enable reproducibility on SAR-RARP50/GraSP/AutoLaparo? \n\nHow do you prevent label leakage and ensure synthetic samples do not overfit to specific sites/instrument brands? Any filtering for tool hallucinations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vW6BKdm5St", "forum": "9OKxaCRPjD", "replyto": "9OKxaCRPjD", "signatures": ["ICLR.cc/2026/Conference/Submission5925/Reviewer_fb7U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5925/Reviewer_fb7U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970354598, "cdate": 1761970354598, "tmdate": 1762918354759, "mdate": 1762918354759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of class imbalance in surgical video datasets by proposing SurgiFlowVid, a sparse and controllable video diffusion framework for generating videos of under-represented classes. Its core contributions are:\n\nDual-Prediction Diffusion U-Net: A novel module that jointly denoises RGB frames and optical flow maps, providing explicit temporal inductive biases to improve motion modeling from limited data.\n\nSparse Visual Encoder: Enables controllable generation using lightweight conditional signals (e.g., sparse segmentation masks or RGB frames), reducing the reliance on costly, dense annotations that are rarely available in surgical datasets.\n\nExtensive Validation: The method is rigorously evaluated on three surgical datasets (SAR-RARP50, GraSP, AutoLaparo) across three downstream tasks (action recognition, tool presence detection, laparoscope motion prediction). Results demonstrate that augmenting real datasets with synthetic data from SurgiFlowVid yields consistent and significant performance gains of 10-20% over strong, competitive baselines.\n\nThis work presents a effective and practical strategy to mitigate data imbalance, thereby advancing the development of robust surgical video understanding models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The integration of optical flow as an explicit supervisory signal within a video diffusion model to enhance temporal coherence is a novel and clever idea. The sparse conditioning approach is aptly designed for the constraints of surgical data annotation.\n\nQuality: The experimental work is thorough and compelling. The method is validated across three distinct datasets and three different downstream tasks, supported by comprehensive ablations and baseline comparisons.\n\nClarity: The methodology, particularly the dual-prediction module and sparse encoder, is described clearly. The experimental section is well-structured, facilitating reproducibility.\n\nSignificance: It directly addresses a key bottleneck in deploying surgical AI models. The approach has high practical utility and can be directly applied to improve the performance of existing systems."}, "weaknesses": {"value": "Limited Generation Length: As noted in the limitations, the method currently generates short clips (~4 seconds). This is a constraint for tasks requiring long-term temporal modeling (e.g., surgical phase recognition). While autoregressive generation is mentioned as future work, it is not explored here.\n\nLimitations of Sparse Conditioning: When using very sparse segmentation masks, the model can generate tools with incorrect positions (Figure 5). This indicates room for improvement in understanding complex spatial relationships from sparse guidance alone.\n\nInsufficient Computational Cost Analysis: While Appendix B.6 provides a comparison of parameters and inference time, a deeper discussion of the training/inference cost versus performance trade-off, especially in the context of resource-constrained healthcare environments, would strengthen the practical deployment analysis."}, "questions": {"value": "On the Role of Optical Flow: Optical flow is used only as a training-time signal. Have you explored or considered using optical flow as an additional conditioning signal during inference to potentially further improve generation quality or control?\n\nOn \"Individual Class Modeling\": The results show that modeling each under-represented class separately (IC) sometimes leads to significant gains (e.g., for class A7 in SAR-RARP50). Could you elaborate more on the advantages and ideal scenarios for this strategy? Does it increase the risk of overfitting or training complexity?\n\nOn the Realism-Diversity Trade-off: The paper emphasizes both spatial-temporal consistency and diversity. Are there more quantitative metrics (e.g., feature diversity extracted using a pre-trained model) to evaluate the diversity of the generated videos and compare it against the baselines?\n\nOn Future Work Directions: Beyond autoregressive generation, for mitigating \"tool position shifts,\" the paper suggests \"richer conditional signals such as tool kinematics.\" Could you propose one or two concrete technical pathways to implement this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3NI50QqEuE", "forum": "9OKxaCRPjD", "replyto": "9OKxaCRPjD", "signatures": ["ICLR.cc/2026/Conference/Submission5925/Reviewer_fdAM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5925/Reviewer_fdAM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991405984, "cdate": 1761991405984, "tmdate": 1762918354491, "mdate": 1762918354491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address this challenge with SurgiFlowVid, a sparse and controllable video diffusion framework for generating surgical videos\nof under-represented classes. Their approach introduces a dual-prediction diffusion module that jointly denoises RGB frames and optical flow, providing temporal inductive biases to improve motion modeling from limited samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The research question is well positioned and made clear.\n2. The experiments are extensive and well analyzed."}, "weaknesses": {"value": "It is not entirely clear whether this paper includes a comparison with other state-of-the-art methods. Were there no existing studies or similar approaches available for comparison in this line of research? If comparable methods do exist, the authors are encouraged to include a quantitative comparison and perform appropriate statistical significance testing to strengthen the scientific rigor and credibility of the results."}, "questions": {"value": "I noticed that some of the references include URL links. Could you clarify whether citing websites directly is appropriate, or if this might affect the scientific rigor of the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BRgPN25Ipn", "forum": "9OKxaCRPjD", "replyto": "9OKxaCRPjD", "signatures": ["ICLR.cc/2026/Conference/Submission5925/Reviewer_xutZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5925/Reviewer_xutZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107925051, "cdate": 1762107925051, "tmdate": 1762918354200, "mdate": 1762918354200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}