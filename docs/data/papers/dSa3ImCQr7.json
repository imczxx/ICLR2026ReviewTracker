{"id": "dSa3ImCQr7", "number": 7638, "cdate": 1758030056860, "mdate": 1759897841777, "content": {"title": "Out of the Memory Barrier: A Highly Memory-Efficient Training System for LLMs with Million-Token Contexts", "abstract": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint ($\\mathcal{O}(1)$) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available for review at https://anonymous.4open.science/r/oomb/README.md.", "tldr": "We built a system called OOMB that uses chunk-based processing and smart CPU offloading to train LLMs on million-token contexts using a single GPU, a task that previously required a large computer cluster.", "keywords": ["LLM", "NLP", "Long-Context LLM", "Memory Efficient Training"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8560d4fd94da0255eab4d4de43c106331937ad38.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents OOMB, a system for memory-efficient training of LLMs on long contexts. The approach combines chunk-recurrent training with activation recomputation, paged KV cache management, asynchronous CPU offloading, and sparse attention. The authors claim to enable training of Qwen2.5-7B with 4M-token contexts on a single H200 GPU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses an Important Problem: GPU memory is indeed a critical bottleneck for long-context LLM training, making this a relevant research direction.\n\n2. The integration of multiple techniques (paging, offloading, sparse attention, custom kernels) shows good systems engineering.\n\n3. Strong Memory Efficiency: The claimed 10MB per 10K tokens overhead is impressive if validated.\n\n 4. Clear Presentation: The paper is generally well-written with good use of figures to illustrate the approach."}, "weaknesses": {"value": "The paper demonstrates memory efficiency but never validates that the trained models actually work. In particular, the paper did not provide evaluations on long-context benchmarks (LongBench, RULER, BABILong, InfiniteBench), comparisons of final model quality between dense and sparse attention. Meanwhile the training curves stop at ~250 steps with loss ~1.2-4.7, and no evidence of convergence, and only used synthetic data. It would be great if the authors can provide complete training runs with evaluation on established benchmarks, and compare dense vs. sparse attention on major tasks.\n\nThe paper claimed that it can \"training Qwen2.5-7B with a 4M-token context on a single H200 GPU\". However, figure 8 (4M experiment) uses 4×H200 with tensor parallelism, not single GPU, 4M requires sparse attention (Table 2 shows dense attention OOMs at 256K on single GPU), and table 1 lists \"OOMB + Dense Attn\" at 4M—contradicts the data. Can the author clarify w.r.t this matter?\n\nThe paper has limited technical novelty, as each component exists: chunk-wise training: Standard for RNNs, used by SeCO for transformers; paged KV cache: directly from vLLM; CPU offloading: common practice; Sparse attention: well-established."}, "questions": {"value": "Table 3 shows OOMB is only marginally faster than Ring Flash Attention per-device. When is OOMB preferable vs. context parallelism on a cluster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "csRNn2T0XZ", "forum": "dSa3ImCQr7", "replyto": "dSa3ImCQr7", "signatures": ["ICLR.cc/2026/Conference/Submission7638/Reviewer_qtG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7638/Reviewer_qtG7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760909535017, "cdate": 1760909535017, "tmdate": 1762919714510, "mdate": 1762919714510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OOMB, a highly memory-efficient training system designed to overcome the prohibitive GPU memory overhead associated with training Large Language Models (LLMs) on million-token contexts. The core of the system is a chunk-recurrent training framework that employs on-the-fly activation recomputation, achieving a constant ($\\mathcal{O}(1)$) memory footprint for activations. To manage the remaining bottleneck of the linearly growing KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients, asynchronous CPU offloading, and page-level sparse attention. The authors demonstrate that their system can train a 7B model with a 4-million-token context on a single H200 GPU, a significant advance in resource efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- While the core modules used in OOMB (e.g., chunking, paged memory, CPU offload, sparse attention) are existing concepts, the paper does an excellent job of integrating them into a cohesive and interdependent system. \n\n- The paper provides a comprehensive set of experiments to validate the system's efficiency. The performance benchmarks against standard parallel training and state-of-the-art context parallelism methods are convincing. Furthermore, ablation studies on chunk size and scalability tests up to 8 million tokens effectively demonstrate the system's robustness and practicality.\n\n- The OOMB framework is designed not only for single-GPU scenarios but also supports multi-GPU training via Tensor Parallelism. This flexibility significantly increases the practical value of the work for a wider range of researchers and institutions."}, "weaknesses": {"value": "- Although the authors analyze the performance of sparse attention, I still have concerns about its impact on model quality. A key missing piece of analysis is an ablation study on the page size. The representative vector for each key page is calculated by averaging. It is plausible that a larger page size would average out more details, reducing the distinctiveness between different page-representative vectors and thus degrading the quality of the Top-K retrieval. This seems like a critical hyperparameter that was not explored.\n\n- The training loss curve for the 1M context length in Figure 7 (top-left) shows some instability, especially for the larger budget configuration. The authors briefly attribute this to the learning rate and limitations in positional encoding extrapolation. This explanation is not sufficient to be fully convinced. Could this instability be an artifact of the sparse approximation itself at such an extreme scale? Further analysis is needed to clarify this point."}, "questions": {"value": "1.  The KV cache is also a form of activation. The OOMB framework discards other intermediate activations but retains the KV cache to be passed between chunks . Is the primary reason for this special treatment to ensure the attention mechanism has access to the full history for a mathematically correct forward pass, while other activations can be recomputed locally within a chunk without affecting the model's final output?\n2.  In Table 4a, the results for 16K, 64K, and 256K contexts generally show that the loss decreases as the sparse attention budget increases, approaching the performance of dense attention. However, for the 1M context, the loss is higher with larger budgets (4K+8192 and 4K+32768) than with smaller ones. This trend is counter-intuitive. Could the authors provide a potential explanation for this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tXsK1TU3PF", "forum": "dSa3ImCQr7", "replyto": "dSa3ImCQr7", "signatures": ["ICLR.cc/2026/Conference/Submission7638/Reviewer_dZ2Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7638/Reviewer_dZ2Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761047195684, "cdate": 1761047195684, "tmdate": 1762919714142, "mdate": 1762919714142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Training large language models (LLMs) on long contexts is constrained by exorbitant GPU memory overhead, primarily due to the linear growth of memory usage with sequence length. This paper proposes a memory-efficient training system named OOMB, which adopts a scheme combining a \"block-wise cyclic training framework\" with \"on-the-fly activation recomputation\". To efficiently manage the KV cache, OOMB incorporates a set of collaborative optimization strategies: (1) a paged memory manager for both the KV cache and its gradients to eliminate fragmentation; (2) asynchronous CPU offloading to hide data transfer latency; (3) page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paged KV Cache and Gradient Management. This paper introduces a paged memory manager for the key-value (KV) cache and its gradients to eliminate exorbitant memory operation overhead, while alleviating memory fragmentation when appending new KV pairs.  \n2. Specialized Kernels. This paper implement custom operators that execute all operations on the KV cache and its gradients directly, making them opaque to PyTorch’s autograd system. This avoids storing the KV cache as an activation and allows for direct gradient accumulation.\n2. Asynchronous KV Cache CPU Offloading. This paper designed an asynchronous mechanism to preemptively offload the KV cache and its gradients, whose memory grows with context length, to CPU memory. The resulting data transfer latency is effectively masked by computation.\n3. Page-level Sparse Attention. This technique not only reduces the complexity of attention computation but also minimizes the communication overhead introduced by the offloading mechanism to the greatest extent."}, "weaknesses": {"value": "1. The description of the workflow of the OOMB training system lacks clarity. Could pseudocode be provided for illustration, similar to FlashAttention?\n2. The experiments are only conducted on small model, and validation on larger or more diverse models is anticipated."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LnXeoIJ2Ko", "forum": "dSa3ImCQr7", "replyto": "dSa3ImCQr7", "signatures": ["ICLR.cc/2026/Conference/Submission7638/Reviewer_ZwBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7638/Reviewer_ZwBd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902764716, "cdate": 1761902764716, "tmdate": 1762919713745, "mdate": 1762919713745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OOMB (Out Of the Memory Barrier) to train LLM efficiently with ultra-long contexts up to 4–8 million tokens using a single GPU. For GPU memory usage, activations and KV caches scale linearly with sequence length. OOMB combines chunk-recurrent training with a series of memory and compute optimizations that make activation memory constant and KV cache growth manageable.\n\nChunk-Recurrent Training: it splits long sequences into smaller chunks processed serially. Activation recomputation keeps activation memory constant to sequence length. Shifts the main bottleneck from activations to the KV cache.\n\nPaged KV cache and gradient management: it proposed a custom paged memory manager for KV caches and gradients, accumulating gradients directly on GPU memory to reduce I/O and fragmentation.\n\nAsynchronous KV cache offloading: Transfers KV caches between GPU and CPU memory asynchronously using pinned memory and DMA\n\nPage-level sparse attention: introduces top-K page retrieval for dense models (e.g., Qwen2.5) and native sparse attention for GPT-OSS"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "originality\nIt showcased the effectiveness of paged KV cache manager that support forward and backward passes, combined with activation recomputation and asynchronous offloading. OOMB innovates on single-GPU training feasibility for million-token contexts. It's quite different to mainstream approaches\n\nQuality\nThe paper has solid benchmarking results across context lengths, chunk sizes, and sparse retrieval budgets. Comparing to FlashAttention and Ring Flash Attention, improvements in runtime and memory are consistent and reproducible. It has clear formulations for chunk-wise training, sparse attention retrieval, and ablation studies up to 8M-token contexts. It also openly discussed latency tradeoffs with dependency on CPU–GPU bandwidth\n\nClarity: Figures 1–3 and 6–9 effectively illustrate the system architecture and memory scaling behavior. The writing style is clear and professional, with minimal ambiguity.\n\nSignificance: The work addresses memory constraints in long-context training. It's critical for long-tail users with limited GPU resource. Achieving 4M-token context training on a single GPU is a concrete result that could democratize access to long-context modeling. The system offers a practical path forward for future foundation model training"}, "weaknesses": {"value": "Whether baselines are state-of-the-art: The authors state (Section 5.3) that “for more optimal sparse attention strategies, we refer readers to the established literature,” which underscores that the sparse attention component remains under-developed. The analysis only reports gradient approximation errors (Fig. 7) without investigating why the sparse retrieval patterns emerge\n\n\nLimited discussion of evaluation results: Section 6 acknowledges that “chunk-wise serial processing introduces latency,” but the magnitude of this penalty relative to GPU compute budget or wall-clock training time is unclear."}, "questions": {"value": "It would be great to include a breakdown analysis of time spent on recomputation vs. data transfer vs. forward/backward compute.\n\nClarify what “O(1)” precisely means in practice (e.g., per-chunk constant, independent of total context but dependent on hidden size)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bQDdrxY93D", "forum": "dSa3ImCQr7", "replyto": "dSa3ImCQr7", "signatures": ["ICLR.cc/2026/Conference/Submission7638/Reviewer_TNui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7638/Reviewer_TNui"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985008330, "cdate": 1761985008330, "tmdate": 1762919713285, "mdate": 1762919713285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}