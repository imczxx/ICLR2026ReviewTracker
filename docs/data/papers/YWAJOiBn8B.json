{"id": "YWAJOiBn8B", "number": 13887, "cdate": 1758224428075, "mdate": 1759897406362, "content": {"title": "Learning What to Say and How Precisely: Efficient Communication via Differentiable Discrete Communication Learning", "abstract": "Effective communication in multi-agent reinforcement learning (MARL) is critical for success but constrained by bandwidth, yet past approaches have been limited to complex gating mechanisms that only decide whether to communicate, not how precisely. Learning to optimize message precision at the bit-level is fundamentally harder, as the required discretization step breaks gradient flow. We address this by generalizing Differentiable Discrete Communication Learning (DDCL), a framework for end-to-end optimization of discrete messages. Our primary contribution is an extension of DDCL to support unbounded signals, transforming it into a universal, plug-and-play layer for any MARL architecture. We verify our approach with three key results. First, through a qualitative analysis in a controlled environment, we demonstrate \\textit{how} agents learn to dynamically modulate message precision according to the informational needs of the task. Second, we integrate our variant of DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth by over an order of magnitude while matching or exceeding task performance. Finally, we provide direct evidence for the \"Bitter Lesson\" in MARL communication: a simple Transformer-based policy leveraging DDCL matches the performance of complex, specialized architectures, questioning the necessity of bespoke communication designs.", "tldr": "We created a universal, plug-and-play DDCL layer for MARL that learns message precision, cutting bandwidth by >10x without performance loss. It also lets a simple Transformer match complex, bespoke architectures.", "keywords": ["Multi-Agent Reinforcement Learning (MARL)", "Differentiable Communication", "Communication Efficiency", "Discrete Communication", "Message Precision", "Unbiased Gradients"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b89d86577cf9f2ff062885c763122b42d9d19368.pdf", "supplementary_material": "/attachment/cdd407c8ab0a5558d63744a2a51548c1a1746f2f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the generalized DDCL framework to optimize communication efficiency in Multi - Agent Reinforcement Learning (MARL). Through a differentiable discrete communication mechanism and adaptive communication cost optimization, DDCL solves high bandwidth consumption and gradient issues in MARL communication. Experiments on multiple MARL tasks show DDCL significantly reduces bandwidth while improving collaboration performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1.Core Technical Contribution: A theoretically grounded and practically validated generalization of DDCL to unbounded signals.\n2.Plug-and-Play Utility: Can be seamlessly integrated into multiple existing MARL algorithms without major architectural changes.\n3.Clear Differentiable Objective: Well-justified communication cost term derived from information-theoretic principles.\n4.Strong Empirical Results: Demonstrates bandwidth reduction of up to 10000× with competitive or superior task performance.\n5.Elegant Demonstration of “Bitter Lesson”: Transformer + DDCL performs on par or better than complex specialized comms systems.\n6.Clean Writing and Illustrations: Well-organized, with clear figures showing success vs. bandwidth tradeoffs, communication distributions, etc.\nThe paper is the first to empirically validate the Bitter Lesson in MARL communication, showing that simple and general architectures (e.g., Transformer + DDCL) can match or outperform heavily-engineered designs, reinforcing the importance of scalable learning methods over hand-crafted communication priors."}, "weaknesses": {"value": "The paper assumes synchronized randomness between agents to reconstruct discrete messages. It would strengthen the work to analyze how DDCL performs under desynchronized or noisy communication conditions.\n\nWhile the current DDCL design uses a fixed uniform grid, future work could explore learned non-uniform quantization or variational encoding for further compression efficiency.\n\nThe empirical evaluation could be further enhanced by testing DDCL under more dynamic or heterogeneous communication topologies.\nto summarize:\n(1) The current method assumes synchronized randomness between agents. It would be useful to analyze robustness under desynchronization or channel noise.\n\n(2) While DDCL uses a fixed quantization grid, exploring non-uniform or learnable encoding schemes could yield further improvements in compression efficiency.\n\n(3) The evaluation focuses on standard MARL benchmarks; testing on more dynamic or heterogeneous communication graphs could enhance the scope of applicability."}, "questions": {"value": "Expand discussion or experiments on desynchronized noise, possibly testing its effect on message decoding.\nClarify how λ is tuned across different tasks — is it task-specific or globally set?\nwhole paper need more simple figure or decent figure to show the workflow of your work, help to better understand this whole mechanism:\nfor example, first show the architecture of multi-agent's information communication. and show how you figure with a clear figure to help reader to understand."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7xu1TOsHid", "forum": "YWAJOiBn8B", "replyto": "YWAJOiBn8B", "signatures": ["ICLR.cc/2026/Conference/Submission13887/Reviewer_mnfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13887/Reviewer_mnfL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760467044442, "cdate": 1760467044442, "tmdate": 1762924398792, "mdate": 1762924398792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper generalizes *Differentiable Discrete Communication Learning* (DDCL; Freed et al., 2020) to support unbounded, signed, real-valued signals, enabling differentiable optimization of discrete message length in multi-agent reinforcement learning (MARL).  \nThe resulting framework is plug-and-play, integrates seamlessly with diverse MARL architectures and achieves significant bandwidth savings without sacrificing task performance.  \nExperiments span both interpretable toy domains and large-scale benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear conceptual improvement.**  \n  The paper makes a clean and well-motivated generalization of Freed et al.’s DDCL to unbounded real-valued signals, removing the restrictive assumption \\(z \\in [0,1]\\).  \n  This substantially broadens applicability to a wide range of MARL architectures.\n\n- **Strong experimental validation.**  \n  The evaluation suite is extensive, covering both controlled and high-dimensional domains.  \n  Integrations into four established MARL+Comms baselines are convincing, and the cross-architecture comparisons are handled carefully.\n\n- **Theoretical and empirical consistency.**  \n  The *CommunicatingGoalEnv* analysis successfully bridges theory and practice:  \n  the emergent frequency-aware communication protocol and rate–distortion frontier plots are particularly illustrative.\n\n- **Empirical support for the “Bitter Lesson.”**  \n  The finding that a simple Transformer-based architecture with DDCL can match or outperform specialized communication systems provides an elegant and meaningful conclusion."}, "weaknesses": {"value": "- **Clarity and presentation.**  \n  Figures are visually dense and captions occasionally unclear and confusing.  \n  - *Figure 1:* The caption is not well aligned with the figure. The term *“episodic plot”* is unclear, and it is not evident where “success rate remains perfect (1.0).”  \n  - *Figure 2:* The numerous STE baselines (`STE_[4,8,16]`) clutter the Pareto plots; consider reducing them or highlighting key configurations more clearly.\nThe Pareto frontier is said to be “indicated by thick black borders,”  \n  but the markers with thick borders are not consistently on the Pareto front — clarification is needed.\n\n- **Statistical reliability.**  \n  Several experiments (notably GRF) exhibit very wide confidence intervals,  \n  with overlapping CIs that make many improvements statistically insignificant.  \n  The paper would benefit from **additional runs** or **variance analysis** to support claims more robustly."}, "questions": {"value": "- **Readability and layout.**  \n  Figures are often placed far from their discussion (e.g., Fig. 1 and Fig. 2), making the narrative hard to follow.\n\n- **Missing or unclear references.**  \n  Lines 104–105:  \n  “Treating messages as discrete actions naturally handles discrete channels, but learns inefficiently and often converges to inferior policies.” This statement requires supporting references.\n\n- **Terminological precision.**  \n  The term *“Rate–Distortion Frontier”* (line 317) is used somewhat loosely;  \n  *“Efficiency–Performance Frontier”* (as in line 360) may better reflect what is empirically measured.\n\n- **Incomplete text and unclear visuals.**  \n  Line 342 (*“Details about the environment in appendix E”*) appears truncated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DIy9uyA2VH", "forum": "YWAJOiBn8B", "replyto": "YWAJOiBn8B", "signatures": ["ICLR.cc/2026/Conference/Submission13887/Reviewer_He3m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13887/Reviewer_He3m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882790032, "cdate": 1760882790032, "tmdate": 1762924398434, "mdate": 1762924398434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends DDCL by relaxing the assumption of bounded, positive values of messages to unbounded messages. The paper proposes to constrain the length of quantized messages through a communication loss. The paper integrates the proposed loss into several MARL with communication methods and evaluates them in several benchmark environments. The results show that a short length (fewer bits) of messages can achieve better/similar performance in some tasks. The analysis is interesting and could provide interesting insights about quantized messages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The idea and the relaxation of the assumption in communication seems to be interesting.\n2. The proof sounds correct.\n3. The analysis is extensive and pretty interesting."}, "weaknesses": {"value": "However, I found the paper has shortcomings in: 1) the core issues (see my detailed comments below) when relaxing the assumption are not considered and addressed; 2) the theoretical analysis seems to have marginal modification from Freed's work [1,2]; 3) the claims in the experiments are too strong, which is not sufficiently evidenced by the results.\n\n[1] Discrete communication learning via backpropagation on bandwidth-limited communication networks. Master’s Thesis, Carnegie Mellon University, Pittsburgh, PA, USA, August 2020. CMU-RI-TR-20-45. \n\n[2] Benjamin Freed, Guillaume Sartoretti, Jiaheng Hu, Howie Choset. Communication Learning via Backpropagation in Discrete Channels with Unknown Noise. AAAI 2020: 7160-7168."}, "questions": {"value": "In the introduction, NQD focuses on calibrating messages (sending messages as short as possible), and IMAC specifically focuses on compressing messages to satisfy bandwidth limitations. How does your work differentiate from these works?\n\nLine 79: What is the \"bitter lesson\" in MARL communication?\n\nIn section 2.1: why POMG is needed. Since you assume shared rewards and use observations only, why not use Dec-POMDP\n\nline 105: This is not clear to me. Why do discrete messages learns inefficiently and converge to inferior policies?\n\nline 131: The result of the communication cost (line 130) was not provided in the referenced paper. Please provide the correct reference.\n\nline 137: The message to be non-positive could be interesting for a general neural network design and potentially useful for gradient backpropagation. But why are unbounded values interesting and helpful? Communication is something injected into the MARL structure, which may satisfy particular requirements of designers, so that the messages can be bound.\n\nSection 3: The related work section introduces MARL with CTDE and value decomposition methods in particular. However, since you never mention CTDE and value decomposition methods later. Why is this subsection needed? I suggest removing this part and directly starting with Communication in MARL\n\nline 162: IMAC was not introduced properly. In fact, the main contribution of IMAC is not graph-based scheduling. IMAC primarily focuses on compressing messages to satisfy bandwidth limitations, which, in my view, is pretty relevant to this paper.\n\nline 194: In the inequality at line 194, there is a double number of messages in the upper bound, why not define the set of messages including negative values? Moreover, why is |M| not used? Moreover, I feel a bit tricky by the proof in Appendix B. The proof actually follows Freed's work, while incorporating sign bits. As stated in the proof, the sign bits can be transformed into non-negative integers. However, this can be simply done by offsetting the values of messages and following the same theory of Freed. How does the assumption of read-valued messages make the theory distinguished? \n\nBesides, Theorem A.1 follows Freed's result while only removing (mod 1). Theorem A.2 follows Freed's result by only modifying the assumption that z is a read-value. These theorems create a new issue that if z is an unbounded value, the quantized integer message m can go to infinity (and obviously, the upper bound is meaningless). The theorems do not touch the core issues of the assumption relaxed by this paper.\n\nLine 199, the communication edges are not defined, which follows a graphic view of communication. Does this follow a directed communication topology?\n\nline 287: This claim is not supported by the expected bits you propose to achieve. In fact, you mentioned in line 290 that the average length is 4.75, which is similar to the fixed-precision code.\n\nIn line 306, how do you compare/view your work and NDQ which uses an entropy-based loss for calibrating continuous messages?\n\nLine 356: The confidence interval of IC2Net with your proposal in GRF 3v1 overlaps with the comparison. Could you provide the statistical report about this to confirm the significance?\n\nline 391: You claim the DDCL creates the Pareto frontier. But how do you identify/compute the _global Pareto frontier?_\n\nLines 449-451 are a big claim that I couldn't draw from your results. The results in Figure 2 show that GAComm and MAGIC (using gating, scheduling, and graph attention) can achieve similar performance to MAPPO"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HxkwKaXn3f", "forum": "YWAJOiBn8B", "replyto": "YWAJOiBn8B", "signatures": ["ICLR.cc/2026/Conference/Submission13887/Reviewer_Gbbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13887/Reviewer_Gbbd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761222184241, "cdate": 1761222184241, "tmdate": 1762924397983, "mdate": 1762924397983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the challenge of efficient, low-bandwidth communication in Multi-Agent Reinforcement Learning (MARL), noting that prior approaches mainly focused on gating messages (deciding if to communicate) rather than modulating message precision (deciding how precisely). The main contribution is generalizing the Differentiable Discrete Communication Learning (DDCL) framework to support unbounded, signed, real-valued signals. This transformation makes DDCL a universal, plug-and-play module for any MARL architecture by proving that the unbiased gradient estimation property holds for unbounded signals and deriving a new differentiable communication cost $L_{comms}$.\nThe validation is robust. First, qualitative analysis shows agents learn a frequency-aware compression protocol in a toy task, assigning fewer bits to high-probability events, demonstrating 24x efficiency for the most frequent goal compared to fixed codes. Second, by integrating DDCL into four state-of-the-art MARL+Comms baselines (IC3Net, TarMAC, GA-Comm, MAGIC) across challenging benchmarks (TJ, PP, GRF), the framework is shown to reduce bandwidth by one to five orders of magnitude while consistently maintaining or improving task performance, frequently establishing the Pareto frontier. Finally, the work provides direct evidence for the \"Bitter Lesson\" in MARL communication: a simple MAPPO Transformer policy empowered by DDCL matches or exceeds the performance of complex, specialized architectures, challenging the necessity of hand-crafted communication mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality\n\n First, the generalization of the DDCL framework itself is a crucial technical step. Prior DDCL work was limited to positive, bounded signals, which imposed artificial architectural constraints (like requiring a sigmoid activation on policy outputs). This generalization to unbounded, signed, real-valued vectors removes those constraints, fundamentally changing DDCL from a niche technique to a universal, plug-and-play module for any MARL architecture. The authors back this up by proving that the unbiased gradient property holds for the generalized, unbounded signal and deriving a new, appropriate communication loss, $L_{comms}$, that accounts for signed integers. This technical expansion is essential for DDCL's practical applicability across diverse MARL agents. Second, the paper offers direct, compelling evidence for the \"Bitter Lesson\" in MARL communication. The established trend in MARL+Comms has been to develop complex architectures employing hand-crafted mechanisms for gating, scheduling, or graph optimization (e.g., IC3Net, MAGIC, GA-Comm). The work strongly counter-argues this trend by showing that a simple, general-purpose architecture (MAPPO Transformer) combined with DDCL's learnable precision mechanism matches or exceeds the performance of these specialized models across nearly all benchmarks. This suggests that the foundational ability to learn how precisely to communicate (DDCL) provides a larger return than complex architectural priors designed to learn when to communicate. This is a conceptually significant and original finding that should shift research focus.\n\n\n- Quality\n\nthe paper proves the key result that the reconstruction error e is statistically independent of the original signal z for the unbounded quantization scheme. The communication cost derivation uses established information-theoretic principles (variable-length coding, Jensen’s inequality) to justify the differentiable surrogate loss $L_{comms}$. Empirically, the three-pronged validation is comprehensive. The qualitative analysis convincingly demonstrates that DDCL leads to the emergence of a sophisticated, frequency-aware coding protocol—an efficient, variable-length encoding that allocates minimal bits (0.25 average bits) to the most frequent goal (51.5% frequency), resulting in a 24x compression advantage over fixed uniform codes.\n\nThe quantitative utility tests integrate DDCL into four baselines and evaluating them across three distinct and challenging environments designed to expose communication bottlenecks (TJ, PP, GRF). The results consistently show DDCL-enhanced variants forming the global Pareto frontier. The comparisons against fixed-quantization baselines (STE) highlight DDCL's robustness; for instance, MAGIC DDCL achieves 155.6% greater success than its STE4 counterpart in Predator-Prey Hard, proving adaptive precision is superior to simple low-bit communication in complex tasks. The inclusion of detailed sensitivity analyses for both the communication penalty λ and quantization granularity δ further demonstrates careful experimentation and stability testing of the framework.\n\n\n- Clarity and Significance\n\nThe paper is clear and well-organized, effectively explaining the technical difficulty of discrete communication and how the reparameterization mechanism resolves the non-differentiability issue via shared noise. The use of Rate-Distortion (Pareto) plots is the correct method for demonstrating the core trade-off, allowing for straightforward interpretation of the efficiency gains. The significance is substantial for both theory and practice in MARL. Practically, DDCL serves as a universal efficiency multiplier. The consistent reduction in communication bandwidth by orders of magnitude (e.g., MAPPO compression up to 5.26 OOM in Traffic Junction Hard with no significant performance loss) while maintaining or improving episodic rewards is critical for deploying MARL in real-world systems with limited bandwidth. Conceptually, the work's strongest significance lies in its support of the \"Bitter Lesson\". By showing that general-purpose architectures coupled with a learned efficiency mechanism can outperform complex, specialized communication designs, the paper provides a clear path for future research that focuses on general, scalable algorithms over niche architectural engineering. This is a field-shaping conclusion. It suggests that resources currently spent designing bespoke communication layers might be better allocated to improving the backbone architecture and the principled communication regularization mechanisms. This realization challenges existing conventions and points MARL communication research toward more scalable solutions."}, "weaknesses": {"value": "- Originality\n\nThe framework's core novelty is arguably limited to the boundary conditions of the DDCL formulation. The foundational innovation was established in prior work (Freed et al., 2020c). The generalization to unbounded signals is crucial for applicability, but the mechanism itself is inherited. A more significant constraint on the originality is the sub-optimality of the derived communication cost, $L_{comms}$. The paper acknowledges that $L_{comms}$ is a surrogate loss derived using Jensen's inequality, which provides a differentiable upper bound on the expected message length. Minimizing this upper bound does not guarantee minimizing the true message length. This results in a substantial \"Shannon Gap\": in the qualitative analysis, the agent's average communication length (4.75 bits) is significantly above the theoretical minimum (1.81 bits).\n\n\n- Significance\n\nRegarding significance, two issues limit its claim of universal utility. First, the DDCL mechanism inherently requires shared pseudorandom number generators between agents to perform the noise subtraction necessary for unbiased gradient estimation. This is a strong constraint on its real-world applicability in truly asynchronous, desynchronized systems. The paper lists investigating robustness to desynchronized noise as future work, confirming this practical limitation of the current framework. Second, while the paper provides strong evidence for the \"Bitter Lesson,\" it relies on a simplification: for architectures like IC3Net, the evaluation adheres to a simplified star-shaped communication graph to isolate the impact of DDCL. This choice, while useful for ablation, limits the demonstration of DDCL's ability to improve efficiency in more complex, dynamic, or fully-connected communication topologies inherently supported by other baselines like GA-Comm and TarMAC."}, "questions": {"value": "1. The primary weakness acknowledged is the gap between the learned message length (4.75 bits) and the theoretical Shannon limit (1.81 bits) due to minimizing the surrogate upper bound $L_{comms}$. Given that $L_{comms}$ indirectly couples communication cost to the L1-norm ∣z∣, have the authors explored alternative differentiable objectives that might better approximate an entropy-based loss, decoupling the cost from the latent vector magnitude, as suggested in the Limitations section?\n\n2. The generalized DDCL relies on synchronized pseudorandom number generators between agents for the noise subtraction step required for unbiased gradient estimation. In a real-world setting where synchronization might be imperfect, how sensitive is the training process to desynchronized noise, and what magnitude of gradient bias would be introduced if the shared randomness assumption were slightly violated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jJF7Kq2lRk", "forum": "YWAJOiBn8B", "replyto": "YWAJOiBn8B", "signatures": ["ICLR.cc/2026/Conference/Submission13887/Reviewer_sFcA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13887/Reviewer_sFcA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762179205981, "cdate": 1762179205981, "tmdate": 1762924397338, "mdate": 1762924397338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}