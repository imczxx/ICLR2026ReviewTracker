{"id": "zJnPyb2xrp", "number": 7334, "cdate": 1758016435155, "mdate": 1759897859323, "content": {"title": "PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such as image captioning and visual question answering. However, they often suffer from over-reliance on spurious correlations, primarily due to linguistic priors that distract the model from leveraging actual visual information. To address these issues, we introduce MMG-PostAlign, a post-multimodal alignment framework designed to enhance the visual understanding capabilities and mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal grounding module for both visual grounding, which identifies the referred object in the image, and textual grounding, which generates the rationale for the final answer, ensuring that outputs are anchored in both visual and textual evidence. To mitigate the hallucinations, we introduce a negative rejection mechanism in the visual grounding module to distinguish grounded entities from non-existent objects influenced by linguistic biases. On the textual grounding side, we propose a selective reasoning mechanism that adjusts the model’s reasoning strategy based on query complexity. Extensive evaluations are conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench showing significant improvements in fine-grained visual understanding and hallucination suppression.", "tldr": "", "keywords": ["multimodal grounding", "MLLM hallucination", "alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c433c5adec4dfdc25862d183d0cef5b5d9dfb605.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMGrounded-PostAlign, a post-multimodal alignment framework designed to mitigate hallucinations and enhance the visual understanding capabilities of Multimodal Large Language Models (MLLMs). The core idea is to use multimodal grounding as a \"corrective lens\" to anchor the model's outputs in actual visual and textual evidence, thereby reducing its over-reliance on spurious linguistic priors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a novel and compelling perspective. Instead of using MLLMs for grounding tasks (the common approach), it inverts the relationship by leveraging grounding to enhance the MLLM itself.\n- The paper is generally well-written and well-structured. The motivation is clearly established, the framework is explained with the aid of a pipeline diagram, and the findings are presented logically."}, "weaknesses": {"value": "- the `<SIMPLE>`/`<COMPLEX>` labels are assigned at the dataset level, not per sample. This is a coarse-grained heuristic that may misclassify individual queries.\n- It is uncertain whether the performance gains are due to the novel grounding-as-a-lens concept or simply the introduction of any additional post-processing signal. It is possible that a simpler method like contrastive decoding could achieve similar hallucination suppression on POPE without the need for a full segmentation model.\n- The framework is a multi-component system where the visual grounding, textual grounding, and final answer generation are interlinked. What happens if the visual grounding module fails (e.g., produces an incorrect or low-confidence mask for a present object, or fails to trigger <REJ> for an absent one)?\n\n- Some figures have relatively small, which affects readability. For example, Figure 3(a) and (c)."}, "questions": {"value": "See weakness please"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i5NAeL13K1", "forum": "zJnPyb2xrp", "replyto": "zJnPyb2xrp", "signatures": ["ICLR.cc/2026/Conference/Submission7334/Reviewer_DgK7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7334/Reviewer_DgK7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555437016, "cdate": 1761555437016, "tmdate": 1762919453206, "mdate": 1762919453206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Addressing the hallucination problem and insufficient fine-grained visual understanding of Multimodal Large Language Models (MLLMs) caused by their over-reliance on linguistic priors, this paper proposes the MMGrounded-PostAlign post-multimodal alignment framework. The framework integrates visual grounding (incorporating a negative rejection mechanism to distinguish between real and non-existent objects) and textual grounding (incorporating a selective reasoning mechanism to adjust reasoning strategies based on query complexity). Built on the base models of LLaVA-1.5-7B/13B and ViT-H SAM, and optimized through LoRA fine-tuning and multi-loss function, the framework’s effectiveness—including suppressing hallucinations, enhancing visual understanding, and preserving the reasoning capabilities of MLLMs—has been validated on benchmarks such as HaloQuest, POPE, VQAv2, MME, MMBench, RefCOCO, and ReasonSeg."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It accurately identifies two key issues of Multimodal Large Language Models (MLLMs): \"hallucinations\" (generating non-existent content) and \"insufficient fine-grained visual understanding\", which are caused by the models' over-reliance on linguistic priors. These two types of issues serve as core bottlenecks that undermine the robustness and reliability of current MLLMs in vision-language tasks, making the research direction highly practically significant and necessary.\n- While enhancing visual understanding and suppressing hallucinations, it does not compromise the inherent reasoning and generalization capabilities of MLLMs (e.g., achieving performance equal to or better than the baseline on MME and MMBench). It also avoids the problem of degraded reasoning ability in some grounding methods (such as BTL-Generation) due to overfitting to visual information, demonstrating an excellent balancing effect."}, "weaknesses": {"value": "- In textual grounding, the <SIMPLE>/<COMPLEX> labels are categorized at the \"dataset level\" (e.g., queries in the COCO dataset are classified as <SIMPLE>, while those in the ReasonSeg dataset are classified as <COMPLEX>), rather than being annotated at the \"sample level\". Although this approach reduces annotation costs and ensures training stability, it fails to handle scenarios where \"simple queries and complex queries are mixed\" within the same dataset. This may lead to inaccurate matching of reasoning strategies for some samples (e.g., complex reasoning queries expressed in a simple form are misjudged as <SIMPLE>).\n- The <REJ> samples used to train the \"negative rejection mechanism\" are only sourced from the gRefCOCO dataset (containing 32,202 queries that \"refer to non-existent objects\") and do not cover more scenarios (such as negative samples of different object types, different image styles, and different linguistic expressions). When facing unseen \"false query-image\" combinations, the effectiveness of the negative rejection mechanism may decrease, and its robustness needs further verification.\n- During training, the model is enabled to automatically judge the complexity of queries through \"self-reflection prompting\". However, the model's judgment of difficulty may also introduce biases and hallucinations. For instance, it is common for the model to be overconfident, which leads to the generation of hallucinations."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IpjYCCtomi", "forum": "zJnPyb2xrp", "replyto": "zJnPyb2xrp", "signatures": ["ICLR.cc/2026/Conference/Submission7334/Reviewer_CYza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7334/Reviewer_CYza"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761731921690, "cdate": 1761731921690, "tmdate": 1762919452225, "mdate": 1762919452225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMGrounded-PostAlign, a framework to reduce MLLM hallucinations by grounding outputs in evidence instead of unreliable text priors. It features a visual grounding module with a \"negative rejection mechanism\" to deny non-existent objects and a textual grounding module that uses \"selective reasoning\" to add rationales only for complex queries , thereby improving visual accuracy and suppressing hallucinations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies and addresses a critical problem in MLLMs: the over-reliance on linguistic priors, which leads to hallucinations and a failure to ground responses in visual evidence. The proposed \"post-alignment\" framework is a well-motivated and logical approach to re-center the model's outputs on visual information.\n\n2. The experimental analysis provides valuable insights. \"Finding 1\" (Figure 3), which empirically demonstrates how linguistic priors can override visual information in the model's later layers, offers a strong motivation for the method. Furthermore, the ablation studies (e.g., Tables 1 and 2) are thorough, providing a solid comparison of different grounding strategies (segmentation, detection, BTL vs. explicit grounding) and validating the paper's design choices.\n\n3. The `<REJ>` token is a practical and effective mechanism for negative grounding. By giving the model an explicit option to \"abstain\" from grounding a non-existent object, this method directly targets object hallucination."}, "weaknesses": {"value": "1. Insufficient baseline comparisons: A significant weakness is the lack of comparison against the original, unmodified baseline model, as well as other well-established MLLMs. The model is built on LLaVA-1.5, yet Tables 1-3 primarily compare variants of the proposed method against an internal baseline (the framework with modules removed), not against the original LLaVA-1.5. This makes it difficult to assess the true impact (including any potential performance trade-offs) of the added components. This is particularly concerning given \"Finding 3\" (retaining reasoning abilities), which cannot be fully verified without this comparison. For instance, the reported 63.9 on MMBench-EN (7B) may not be competitive with the public LLaVA-1.5-7B score (64.3).\n\n2. Unclear architectural novelty: The novelty of the visual grounding module's architecture is not well-explained. The Method section (Section 3) describes a model (with SAM-based decoder, `<LOC>` and `<REJ>` tokens) that seems to reimplement established paradigms from prior works like LISA, GLaMM, and GSVA. While these are cited in Related Work, the Method section itself does not attribute these design choices or clearly differentiate what is adopted from prior work versus what is a new architectural innovation. The contribution appears to be more in the application of this module, but the presentation makes the architectural contribution ambiguous.\n\n3. Dataset-level reasoning labels: A significant limitation, as acknowledged by the authors in Appendix, is that the `<SIMPLE>`/`<COMPLEX>` labels for selective reasoning are applied at the dataset level, not the sample level. This is a very coarse heuristic. A dataset labeled \"complex\" may contain many simple queries, and vice-versa. This design choice weakens the \"selective reasoning\" strategy, as the model isn't learning to distinguish query complexity on a case-by-case basis but is instead learning a bias associated with the data source prior.\n\n4. (Minor) Clarity of \"Selective Reasoning\": The \"selective reasoning\" mechanism is not very clearly explained in the Introduction. The Introduction is vague. The reader must wait until Section 3.3 to understand the concrete implementation (i.e., the `<SIMPLE>` and `<COMPLEX>` tokens). Briefly explaining this mechanism earlier would improve the paper's readability and flow.\n\n5. (Minor) Citation formatting: The paper does not consistently follow the ICLR template's citation guidelines (`\\citep` and `\\citet`). This should be corrected for the final version."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mUMFK4l6he", "forum": "zJnPyb2xrp", "replyto": "zJnPyb2xrp", "signatures": ["ICLR.cc/2026/Conference/Submission7334/Reviewer_WDPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7334/Reviewer_WDPN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892815430, "cdate": 1761892815430, "tmdate": 1762919451341, "mdate": 1762919451341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MMGrounded-PostAlign, a post-alignment framework that augments an MLLM with  \n(1) visual grounding (segmentation + bounding box) driven by a special LOC token and a negative rejection token REJ, and  \n(2) textual grounding via selective reasoning that emits rationales only for complex queries (SIMPLE/COMPLEX gate).\n\nThe method aims to reduce hallucinations caused by linguistic priors and improve fine-grained visual understanding.  \nExperiments on HaloQuest, POPE, VQAv2, MMBench, MME, RefCOCO series, and ReasonSeg show consistent gains; ablations compare to BTL (boxes-as-tokens) variants."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation: Tackles language-prior-driven hallucination via explicit multimodal grounding; neat idea of \"grounding as a corrective lens\".  \n- practical design: Simple LOC/REJ interface to a multi-task decoder; selective reasoning avoids unnecessary rationale generation.  \n- Broad evaluation: Covers hallucination, general V+L, and grounding benchmarks with meaningful ablations."}, "weaknesses": {"value": "- Limited generality:Only tested on LLaVA-1.5 (7B/13B) + SAM-ViT-H; cross-backbone evidence (e.g., Qwen-VL, other grounding encoders) missing.  \n\n- The idea of labeling queries as SIMPLE vs COMPLEX is good, but doing so at the dataset level rather than per sample raises concern. Some “simple” dataset queries might still require reasoning, and vice-versa. \n\n-  The REJ token is an interesting idea, but the paper does not show the  cases where referent exists but system rejects."}, "questions": {"value": "I note that the Related Work section references reinforcement learning (RL) approaches in the vision–language modelling domain, yet the paper does not include any empirical comparison involving RL.  \n\nMy thought is that a more general RL-based training paradigm—one in which the vision-language model learns in a sequential decision-making setting—might offer better generalization across vision-language tasks rather than designing a task-specific chain-of-thought process solely for the grounding task.  \n\nSo my question is: what are the advantages of your approach compared to a more general reinforcement learning method for MLLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JmlKsiMPqP", "forum": "zJnPyb2xrp", "replyto": "zJnPyb2xrp", "signatures": ["ICLR.cc/2026/Conference/Submission7334/Reviewer_PNjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7334/Reviewer_PNjX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016391992, "cdate": 1762016391992, "tmdate": 1762919450507, "mdate": 1762919450507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}