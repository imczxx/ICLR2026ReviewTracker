{"id": "Zk5jinDNGE", "number": 24536, "cdate": 1758357769741, "mdate": 1759896761382, "content": {"title": "Stochastic Gaussian Zeroth-Order Optimization: Improved Convergence Analysis under Skewed Hessian Spectra", "abstract": "This paper addresses large-scale finite-sum optimization problems, which are particularly prevalent in the big data era. \nIn the field of zeroth-order optimization, stochastic methods have become essential tools. \nNatural zeroth-order stochastic methods primarily rely on stochastic gradient descent (SGD).\nPreprocessing the stochastic gradient using a Gaussian vector defines the method ZO-SGD-Gauss (ZSG), whereas estimating coordinate-wise partial derivatives defines ZO-SGD-Coordinate (ZSC).\nCompared to ZSC, ZSG often demonstrates superior performance in practice.\nHowever, the underlying mechanisms behind this phenomenon remain unclear in the academic community.\nTo the best of our knowledge, our work is the first to theoretically analyze the potential advantages of ZSG compared to ZSC.\nTo facilitate convergence analysis, the quadratic regularity assumption is introduced to generalize the smoothness and strong convexity to the Hessian matrix.\nThis assumption makes it possible to integrate Hessian information into the complexity analysis.\nIn the quadratic case, we provide a theoretical analysis and proof of the significant convergence improvement achieved by ZSG. Finally, experiments on both synthetic and real-world datasets validate the effectiveness of our theoretical analysis.", "tldr": "", "keywords": ["stochastic zeroth-order optimization", "quadratic regularity", "skewed Hessian spectra"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52980119f394614b44d04c02b15fdd4d2315ed1f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the zero-order SGD with gaussian gradient estimation. The authors refine the convergence rates, showing that convergence depends on $\\text{tr}(\\nabla^2 f)$, instead of $\\lambda_{\\max}(\\nabla^2 f)$. They compare the performance of the analyzed algorithm on quadratic functions, as well as on logistic regression on LibSVM datasets."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1)The authors improve the existing rates for zero-order optimization, that benefits the skewed Hessian spectra, when $\\text{tr}(\\nabla^2f) \\leq d\\lambda_{max}(\\nabla^2 f)$."}, "weaknesses": {"value": "1)The authors claim that the terms $P_1(\\alpha)$ and $Q_1(\\alpha)$ are negligible with the small choice of $\\alpha$. However, these terms contain multipliers $\\lambda_{\\max}^2(\\nabla^2 f)d^3$ and $\\lambda_{\\max}^2(\\nabla^2 f)d^3T$, which are frequently large.\n\n2)Unclear writing -- no description of ZSC was given, though, the authors compare the obtained results with it throughout the paper, differences between Theorems 4.5, 4.7, 4.8 are hard to distinguish.\n\n3)Considering $\\text{tr}(\\nabla^2 f)$ as $\\max_{z^t}\\nabla^2 f(z^t)$ wuth similar definitions for $\\lambda_{\\min}(\\nabla^2 f)$ and $\\lambda_{\\max}(\\nabla^2 f)$ is a rough estimate. With this analysis, most convex problems might be considered as strongly convex, when $\\lambda_{\\min} > 0$. \n\n4)The plots do not contain confidence intervals; however, stochastic methods are considered. Also, more complex setups than LibSVM datasets are missing."}, "questions": {"value": "1)The derived stepsize depends on $\\text{tr}(\\nabla^2 f)$ and $\\lambda_{\\min}(\\nabla^2 f)$. How are they obtained in practice?\n\n2)If we access the objective's Hessian during the training process, why do we consider the derivative-free optimization at the first place?\n\n3)Do we demand gaussian distributions in the scheme proposed in Algorithm 1? What if we consider arbitrary (a)symmetric distribution? \n\n4)Does corollary 4.6 result in sublinear convergence for strongly convex functions? According to Assumption 2.1  $\\gamma_l > 0$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No additional ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kToXq5UkHV", "forum": "Zk5jinDNGE", "replyto": "Zk5jinDNGE", "signatures": ["ICLR.cc/2026/Conference/Submission24536/Reviewer_XZDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24536/Reviewer_XZDu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829123464, "cdate": 1761829123464, "tmdate": 1762943115914, "mdate": 1762943115914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes **Gaussian-based zeroth-order stochastic gradient descent (ZSG)** and compares it with **coordinate-based zeroth-order SGD (ZSC)**.  \nIt introduces **Assumption 2.1 : Quadratic Regularity (QR)**, a deterministic Hessian-metric condition generalizing smoothness and strong convexity.  \nUnder this assumption the authors derive iteration-complexity bounds suggesting that ZSG enjoys milder dimensional dependence (\\(\\operatorname{tr}(M)\\)) than ZSC (\\(d\\,\\lambda_{\\max}(M)\\)), particularly for skewed Hessian spectra.  \nExperiments on synthetic quadratics and logistic regression qualitatively support the idea that Gaussian perturbations help under ill-conditioning.\n\nHowever, several technical and presentation issues seriously weaken the results.  \nMost importantly, **Theorems 4.5 and 4.8 mis-state convergence rates**: the quantities \\(Q_1\\) and \\(Q_2\\) depend on the total iteration count T, so the bounds do **not** imply convergence to zero for stochastic functions.  \nCombined with ambiguous assumptions and missing discussion of validity domains, the paper‚Äôs theoretical claims are overstated."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a careful deterministic analysis under a clear quadratic-regularity assumption.  \n- Offers intuition on why Gaussian perturbations can mitigate poor conditioning.  \n- Experiments qualitatively match the deterministic predictions."}, "weaknesses": {"value": "1. **Mis-stated main theorems (4.5 & 4.8).**  \n   The ‚Äúconstants‚Äù \\(Q_1\\) and \\(Q_2\\) depend explicitly on T through cumulative step-size and variance terms.  \n   This destroys asymptotic convergence: the error bound does not vanish as T ‚Üí ‚àû.  \n   Despite this, the text claims a ‚Äúsublinear‚Äù convergence in the *stochastic* case.  \n   The presentation conceals the dependence, giving the impression of a stronger result than actually proved.\n\n2. **Failure to handle stochastic functions.**  \n   Because the variance term grows with T, the analysis effectively applies only to deterministic or finite-sum settings.  \n   There is no uniform bound on stochastic noise, so the claimed results do **not** establish convergence for genuinely stochastic oracles.  \n   The theory should have been presented as deterministic analysis rather than stochastic convergence.\n\n3. **Ambiguity in Assumption 2.1.**  \n   The constants \\(\\gamma_u,\\gamma_l\\) are written as if they may depend on x,y,z, which would make the inequalities tautological.  \n   For the theorems to hold, they must be global constants independent of those points.  \n   This appears to be a typographical error that needs correction.\n\n4. **Lack of discussion of when assumptions hold.**  \n   The paper should explicitly identify and justify classes of functions satisfying QR (e.g., quadratics, certain regularized GLMs).  \n   Beyond the trivial quadratic case, examples are only hinted at and never proved.\n\n5. **Limited novelty and scope.**  \n   Algorithmically, ZSG is standard (Gaussian SPSA / NES).  \n   Experiments are small-scale and deterministic; no tests on high-variance or nonconvex settings.\n\n6. **Lack of transparency.**  \n   By labeling \\(Q_1,Q_2\\) as constants and not clarifying their T-dependence, the manuscript obscures a fundamental limitation of the analysis."}, "questions": {"value": "1. Can you formally characterize non-quadratic functions (e.g., logistic or least-squares objectives) that satisfy the QR condition with global constants?  \n2. Would a fully deterministic framing (œÉ = 0) strengthen the paper?  \n3. How could the variance term be controlled to extend the results to true stochastic settings?  Maybe use momentum?\n4. Can you restate Theorems 4.5 and 4.8 with explicit T-dependence and honest asymptotic interpretation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKrxhSm6Qe", "forum": "Zk5jinDNGE", "replyto": "Zk5jinDNGE", "signatures": ["ICLR.cc/2026/Conference/Submission24536/Reviewer_ivSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24536/Reviewer_ivSN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924257899, "cdate": 1761924257899, "tmdate": 1762943115416, "mdate": 1762943115416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes an accelerated convergence rate for ZSG and theoretically analyze the potential advantages of ZSG compared to ZSC. The paper evaluates on both synthetic and real-world datasets and demonstrate the performance of ZSG outperforms that of ZSC."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel analysis:\nNovel theoretical contribution by being the first to rigorously analyze why ZSG outperforms ZSC in practice\n\nThe theoretical gap addressed is important:\nThe work addresses why ZSG is preferred in practice despite identical O(d) complexity bounds\n\nClarity:\nThe paper is generally well written with clear problem setup and algorithmic description. The contributions and stated clearly and the main results are stated precisely with appropriate assumptions."}, "weaknesses": {"value": "Notations:\nThere is heavy notation that accumulates through the paper that can make it difficult to parse.\n\n\nAssumptions do not match motivating examples:\nAll results assume strongly convex objectives (Assumption 2.1), but the examples used to motivate the analysis such as LLM fine tuning involve non-convex deep learning problems. This means the analysis cannot be directly applied to the examples it states\n\n\nExperiments:\nSome experimental details are sparse such as how are step sizes chosen. The experimental evaluations are very toy settings. However this may be fine because the work mainly fills a theoretical gap. \n\nMissing comparisons:\nThe paper does not provide theoretical or empirical comparison to variance reduced zeroth-order methods or adaptive/momentum based zeroth order methods."}, "questions": {"value": "How should users set the step size when $tr(M)$, $\\lambda_\\min(M)$ are unknown? \n\nIs there a way to construct an experiment for LLM fine tuning and other motivating examples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WVhFAtIGOv", "forum": "Zk5jinDNGE", "replyto": "Zk5jinDNGE", "signatures": ["ICLR.cc/2026/Conference/Submission24536/Reviewer_HDZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24536/Reviewer_HDZg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947398670, "cdate": 1761947398670, "tmdate": 1762943115034, "mdate": 1762943115034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies stochastic zeroth-order optimization and compares Gaussian perturbation ZO-SGD (ZSG) with coordinate finite-difference ZO-SGD (ZSC). Under a quadratic regularity assumption that lifts smooth/strong-convex conditions to a Hessian-norm form, it proves that ZSG enjoys weaker dimension dependence and faster convergence than ZSC, especially under skewed Hessian spectra."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is easy to navigate: assumptions and notation are stated upfront, the notion of quadratic regularity is introduced with intuition before formal use.  And the Theorems show ZSG attains iteration/query bounds that avoid the explicit factor ùëë that appears for ZSC."}, "weaknesses": {"value": "**Question 1.** The bounds hinge on $\\gamma_u, \\gamma_l$ and on quantities like $\\operatorname{tr}(M), \\lambda_{\\min }(M), \\lambda_{\\max }(M)$, which may be unknown or hard to estimate. Thus, practical guidance for choosing $\\eta_t$ that depends on these is limited.\n\n**Question 2.** The empirical comparison is primarily ZSG vs. ZSC; other ZO baselines (e.g., two-point random directions with mini-batching/importance sampling) are not reported, making it harder to gauge practical significance.\n\n**Question 3.**  Although the theory targets skewed spectra, experiments do not directly measure Hessian anisotropy on real data (only synthetic constructions), so the claimed mechanism is not empirically verified on those tasks.\n\n**Question 4.** Sensitivity to $\\alpha$ and noise assumptions. Theory requires \"sufficiently small\" $\\alpha$ and a bounded variance $\\sigma^2$; experiments fix $\\alpha=10^{-6}$ without sensitivity analysis, so robustness is unclear."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nYaZOlmn09", "forum": "Zk5jinDNGE", "replyto": "Zk5jinDNGE", "signatures": ["ICLR.cc/2026/Conference/Submission24536/Reviewer_hEb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24536/Reviewer_hEb7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987168194, "cdate": 1761987168194, "tmdate": 1762943114770, "mdate": 1762943114770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}