{"id": "FSDxP3ZpAx", "number": 10259, "cdate": 1758165320455, "mdate": 1759897662557, "content": {"title": "LoRA Provably Reduces Forgetting and Enables Adapter Merging in Multiclass Linear Classification", "abstract": "Low–Rank Adaptation (LoRA) has become the de-facto parameter-efficient fine-tuning algorithm.  \nBesides training-efficiency, practitioners observe two striking benefits: *(i)* remarkable resistance to *catastrophic forgetting*, and *(ii)* independently trained adapters can be *merged* into a single model that performs well on multiple tasks. Despite their practical importance, these phenomena have lacked rigorous theoretical explanation. In this work, we provide the first theoretical justification for the two aforementioned phenomena by analyzing the structure of LoRA solutions in multiclass linear classification problems for orthogonal tasks.  \nOur analysis shows that, under suitable weight regularization, the optimal LoRA adapter aligns exactly with the \\emph{max-margin} (hard-margin SVM) solution for the fine-tuning data. This alignment lets us track in closed form how the normalized margins on the pre-training data, fine-tuning data and their union vary with the regularization parameter.\nFor *(i)*, we observe a trade-off: decreasing the regularization parameter enlarges the fine-tuning margin while proportionally shrinking the pre-training margin, never collapsing it to zero.\nConcerning *(ii)*, we view the merged weights through the same margin lens, we prove why merging succeeds and derive optimal mixing coefficients that maximize the margin on the union of all tasks.\nFinally, we numerically validate our theory across multiple deep learning architectures and task configurations. The empirical results closely match our theoretical predictions. \nTaken together, our results give the first principled explanation for LoRA’s resistance to forgetting and its surprising merging ability.", "tldr": "", "keywords": ["fine-tuning", "low rank adaptation", "max margin solution"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f48503975b0c776208aed9f3636937ce61bf1f8f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical analysis of the Low-Rank Adaptation (LoRA) algorithm in the context of multiclass linear classification. The authors aim to offer a principled explanation for two important empirical findings: (i) LoRA's resistance to catastrophic forgetting, and (ii) the ability to merge independently trained adapters into a single model that performs well on multiple tasks. The paper builds on the concept of maximizing the margin in linear classification and derives closed-form expressions for optimal LoRA adapters, showing that LoRA’s effectiveness in forgetting reduction and adapter merging can be understood through the lens of margin theory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a good the high-level motivation, i.e., to explain LoRA’s empirical behavior from first principles, which is timely and relevant to the community.\n2. Given the strong simplifying assumptions, the theoretical derivations are internally consistent. The link between the regularized LoRA objective and hard-margin SVM theory is clearly explained."}, "weaknesses": {"value": "1. The Assumption 2.3 is a strong and unrealistic assumption in practical scenarios. The author provide the some numerical evidence in Appendix I.1 by using the pearson correlation coefficient, it may not be sufficient to support such a restrictive orthogonal assumption. Same for the Assumption 3.1. \n2. This paper only consider the multiclass linear classification task, but the usage of LoRA are often focus on the large scale model such as Large Language Models on the generation tasks.\n3. The experiments are conducted on LoRA with only the one layer of the model (final linear), can the result expand to multiple layer as many exsiting usage of LoRA?"}, "questions": {"value": "1. This paper are based on the ''orthogonal tasks'', which lacks of the detailed explaination. Are the orthogonal tasks refers to the task that satisfied the Assumptions 2.3? If that, the reference that ''across all original tasks'' in the introduction (line 58) seems problematic as many of them do not have this assumption.\n2. While the paper provides a deep theoretical analysis, it could benefit from a more detailed discussion of the practical implications of LoRA's findings for real-world applications, such as incremental learning or continual learning. How do the theoretical insights apply to large-scale models like LLMs (Large Language Models)? \n3. How performance changes as correlation of task increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBuykCrX8k", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Reviewer_85Bk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Reviewer_85Bk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561439825, "cdate": 1761561439825, "tmdate": 1762921615603, "mdate": 1762921615603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General responses to all reviewers"}, "comment": {"value": "We appreciate the reviewers’ concerns regarding the linear setting and the restrictive assumptions. Below we clarify our contributions and explain why these modeling choices are both standard and necessary for developing the **first theoretical** foundations of LoRA adapter merging and the choices of regularization parameters. We respectfully note that simplified assumptions are common and often essential in the deep learning theory community; evaluating theory work primarily based on the generality of its assumptions rather than the insights it provides would rule out much of the progress the community has made in understanding modern neural networks.\n\n**(1) Restatement of our contributions:** The paper makes two primary *theoretical* contributions:\n\n- **Characterizing the implicit bias of LoRA.** \nWe provide the first characterization of the *stationary points* of LoRA under weight decay. This reveals the structure of the solutions induced by LoRA’s low rank parameterization and explains how LoRA adapts the pre-trained model to the fine-tuning tasks.\n\n- **Using the implicit bias to derive optimal regularization and optimal merging coefficients.** Based on the closed-form characterization of the limiting solution, we analytically determine how to optimally choose the regularization parameter during fine-tuning and how to set the mixing coefficients for adapter merging.  To our knowledge, no prior work provides theoretical guidance on these choices.\n\nThus, our analysis characterizes the exact optimality conditions and stationary solutions, and uses them to derive principled guidelines for LoRA training and merging.\n\n**(2) Why linear models and orthogonal data? This is standard in deep learning theory**\n\nWe acknowledge that the orthogonal-data assumption is idealized. However, similar simplifying assumptions are widely adopted in theoretical deep learning to isolate core mechanisms that would otherwise be analytically intractable. Representative examples include:\n\n- **Neural collapse theory**  Despite neural collapse phenomenon has been observed for deep neural networks, existing theoretical works [1,2,3,4,5] study a *surrogate model* (a matrix factorization objective with regularization) to explain the neural collapse theory. \n\n- **Implicit bias of gradient descent** Classical results on the implicit bias of neural networks [6]  begin with *linear binary classification problems* under linear separable assumptions before extending to more complex settings, such as multi-class linear classification problems [7,8].\n\n- **Theoretical studies of LoRA** Recent theory on LoRA follows the same pattern.  [9] analyze LoRA initialization in the context of matrix factorization;  [10] study gradient flow learning dynamics in matrix factorization;  [11] analyze the LoRA loss landscape for linearized deep networks (effectively linear models);  [12] study one-step LoRA updates for one-layer models;  [13] analyze two-layer ReLU networks under orthogonal features and rank-one assumptions. \n\nOur assumptions are no stronger than those used in prior theoretical work and serve the same purpose: enabling exact, theoretical characterization of LoRA, which is not feasible for general correlated data or deep nonlinear networks.\n\n**(3) Future work:** Relaxing orthogonality and extending the framework to nonlinear features are important next steps. Our work provides the theoretical foundation needed for such generalization, and we view it as a necessary first step toward a deeper understanding of LoRA in modern deep architectures.\n\n[1] Prevalence of neural collapse during the terminal phase of deep learning training\n\n[2] Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Features Model\n\n[3] Imbalance trouble: Revisiting neural-collapse geometry\n\n[4] A geometric analysis of neural collapse with unconstrained features\n\n[5] An Unconstrained Layer-Peeled Perspective on Neural Collapse\n\n[6] The implicit bias of gradient descent on separable data\n\n[7] The implicit bias of gradient descent on separable multiclass data\n\n[8] Unified binary and multiclass margin-based classification\n\n[9] On the crucial role of initialization for matrix factorization\n\n[10] Understanding the learning dynamics of lora: A gradient flow perspective on low-rank adaptation in matrix factorization\n\n[11] Lora training in the ntk regime has no spurious local minima.\n\n[12] Lora-one: One-step full gradient could suffice for fine-tuning large language models, provably and efficiently\n\n[13] Gradient dynamics for low-rank fine-tuning beyond kernels"}}, "id": "IZ1Zw2kMeJ", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763688319382, "cdate": 1763688319382, "tmdate": 1763688319382, "mdate": 1763688319382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a theoretical study on how LoRA exhibits resistance to catastrophic forgetting and enables the effective merging of separately fine-tuned models. Starting from a set of simplifying assumptions and leveraging the theory of soft and hard margins in SVMs, they develop formal theorems and proofs showing that, when models are fine-tuned with LoRA under norm regularization on the LoRA matrices, three distinct training regimes emerge, each leading to different model behaviors. The analysis further demonstrates that LoRA imposes specific boundaries between pretraining and finetuning phases depending on the strength of the regularization term, and that optimal scaling factors exist which *i)* allow the resulting model to find the most effective boundary between pretraining and finetuning tasks, thus mitigating catastrophic forgetting, or *ii)* yield optimal merging performance across multiple fine-tuned models. Theoretical insights are supported by experiments on various vision architectures using the CIFAR-100 dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Mathematical completeness: assumptions, lemmas and theorems are clearly stated, and proofs are provided\n- Overall, the paper is well written, the notation is precise and not too complex at the same time"}, "weaknesses": {"value": "1. Experimental section: in my opinion, this is the main weakness of the paper. The authors make several simplifying assumptions to obtain more tractable mathematics (see next weakness), which, while potentially too restrictive and simplistic, could be acceptable if the experimental section demonstrated that the theoretical insights generalize to more realistic and complex settings. However, this is not the case. To validate their findings, the authors conduct linear probing experiments: *i.e.*, they fine-tune only the final classification layer with LoRA while keeping the rest of the model frozen. This setup is both unrealistic, since LoRA is typically applied to fine-tune the backbone (particularly the Attention modules) rather than the classifier head, and overly simplistic. Consequently, the empirical results have limited practical relevance. Moreover, given that the stated goal of the paper is to explain how and why LoRA mitigates catastrophic forgetting and facilitates model merging, the comparison baseline should include Full Fine-Tuning (FFT). The authors briefly mention FFT in Section 3.2, noting that comparing LoRA to FFT is not their goal: this, however, appears inconsistent with the paper’s central motivation.\n\n\n2. Strong assumptions, which may rarely hold in practice:\n   - Assumption 2.1, *i.e.* \"The input data dimension is larger than the total number of classes\", is reasonable.\n   - Assumption 2.2, while somewhat restrictive, is still acceptable if it helps maintain mathematical tractability: LoRA can and has been used on datasets with more classes than LoRA’s rank.\n   - Assumptions 2.3 and 2.4 are overly restrictive, and lack justification that they are met in realistic scenarios. Even Section I.1 (page 31 of the Appendix), which attempts to validate part of Assumption 2.3 by showing that features are orthogonal, does so in a toy setting (see previous weakness) and thus fails to demonstrate that these assumptions hold in real-world cases.\n\nMinor weaknesses (did not affect my overall evaluation):\n\n3. Figures 2 and 5 reference Theorem 4, which does not appear anywhere in the paper; this is likely a typo, and the authors probably meant Theorem 3.\n\n4. The distinction between $M_K^{(\\overline{K})}$ and $M_{\\overline{K}}$ is unclear. The authors define $M_K^{(\\overline{K})}$ as the first $\\overline{K}$ columns of the $K$-ETF matrix $M_K$; however, it is not evident how this differs from $M_{\\overline{K}}$."}, "questions": {"value": "Does the theory provided by the authors also translate into more practical settings and use cases, for instance those in the Model Merging Literature? Especially considering the finetuning operations (finetuned backbone, in many cases with LoRA) and datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8jEIL6fYmW", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Reviewer_Qs8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Reviewer_Qs8n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754674760, "cdate": 1761754674760, "tmdate": 1762921614538, "mdate": 1762921614538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Weak Experiment Setup"}, "comment": {"value": "**Regarding the experimental setup**\nReviewers noted that our experiments apply LoRA only to the final linear classifier of a pretrained model, rather than to multiple layers. We chose this setting intentionally, for the following reasons:\n\n- **The experiments are designed to evaluate the theory, not benchmark LoRA.** Our theoretical results are derived for a linear model with orthogonal features. The experimental goal is therefore to test whether the *theoretical predictions remain valid when the assumptions are violated*, i.e., on real, (nearly) orthogonal data with realistic feature representations for linear classifier. This is why we use pretrained features with a linear model. Since in the feature space, the data from different classes are nearly orthogonal (see Figure 3 and Figure 4).\n\n- **The results support the robustness of the theory.** Even though real datasets do not satisfy the orthogonal data assumption, our empirical results show that the theoretically predicted optimal regularization parameters and optimal merging coefficients still qualitatively match the empirical optima. This is precisely the type of validation the theory aims for.\n\nThus, the experimental choices are *deliberate* and aligned with the purpose of the paper: to examine whether our theoretical insights remain predictive in practical scenarios. Extending the framework to multi-layer LoRA is an important but separate direction, and falls outside the scope of the theoretical model studied here."}}, "id": "llpmbohQqW", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763688585460, "cdate": 1763688585460, "tmdate": 1763688585460, "mdate": 1763688585460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a theoretical analysis of LoRA under a multiclass linear classification setting to explain two empirical observations: its resistance to catastrophic forgetting and the success of adapter merging. The authors show that under certain regularization regimes, the LoRA solution aligns with the hard-margin SVM direction, and derive margin-based expressions to justify these behaviors. Limited experiments on linear-probe settings support the analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses two important empirical properties of LoRA with an explicit mathematical formulation.\n2. Clear presentation and rigorous derivations under simplified assumptions.\n3. Offers interpretable margin-based insights and closed-form solutions for merging coefficients."}, "weaknesses": {"value": "1. The theoretical foundation is too shallow, restricted to a single linear layer without deeper or nonlinear analysis, which weakens its relevance to real LoRA behavior in large models.\n2. The experimental validation is limited and cannot convincingly demonstrate how LoRA reduces forgetting or benefits merging in practice.\n3. The regularization analysis is intuitive rather than novel; showing that a stronger regularizer constrains updates is already well understood.\n4. Strong orthogonality assumptions further reduce practical applicability."}, "questions": {"value": "Does the proposed analysis or conclusion still hold when LoRA is applied across all layers or on larger-scale models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJn0N94S3T", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Reviewer_F1QV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Reviewer_F1QV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838240849, "cdate": 1761838240849, "tmdate": 1762921614195, "mdate": 1762921614195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical study of LoRA in a simplified multi-class linear classification setting. Under strong orthogonality and separability assumptions, it shows that, with appropriate regularization, LoRA can remain resistant to catastrophic forgetting, and that independently trained LoRA adapters can be merged into a single model that maintains good performance across multiple tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis is clear and logically structured.\n- The margin-based analysis yields closed-form expressions for both the optimal regularization parameter and the adapter-merging coefficients.\n- The paper further distinguishes different penalty regimes to reveal the trade-off between preserving the pre-trained task and fitting the new task."}, "weaknesses": {"value": "- The analysis relies on a number of very strong and idealized assumptions. These assumptions are unlikely to hold under realistic data distributions, and the paper currently does not provide large-scale empirical evidence to indicate how robust the theory is once these assumptions are relaxed.\n- The paper only treats a simple multi-class linear classification head with frozen features, whereas in practice LoRA is mostly used inside attention blocks on non-orthogonal, high-dimensional representations; it would be helpful to at least discuss how the results might extend to that setting.\n- The experimental section is relatively light and limited to small vision-style settings."}, "questions": {"value": "Could the authors analyze how the current conclusions change when the orthogonality and related assumptions are relaxed to a more realistic, approximately orthogonal setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYFNo5E85F", "forum": "FSDxP3ZpAx", "replyto": "FSDxP3ZpAx", "signatures": ["ICLR.cc/2026/Conference/Submission10259/Reviewer_odn8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10259/Reviewer_odn8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919471285, "cdate": 1761919471285, "tmdate": 1762921613779, "mdate": 1762921613779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}