{"id": "2qn6AOIkL1", "number": 10698, "cdate": 1758179878922, "mdate": 1759897634703, "content": {"title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts", "abstract": "Mixture-of-Experts (MoE) architectures are rapidly becoming the standard for building scalable, \nefficient large language models (LLMs). However, \nthe core principle of MoE, which routes tokens to specialized experts, creates a largely overlooked attack surface. In this work, we propose BadMoE, a novel backdoor attack that exploits this vulnerability. We first prove the existence of ''dominating experts'' in MoE that can determine the output. Building on this insight, BadMoE poisons ``dormant'' (underutilized) experts and utilizes routing-aware triggers to activate them, enabling stealthy and effective manipulation. \n Specifically, BadMoE involves three steps: 1) identifying dormant experts unrelated to the target task, 2) optimizing a routing-aware trigger toward these experts, and 3) promoting them to dominating roles through training data. \nExtensive experiments on three MoE LLMs across multiple backdoor tasks show that BadMoE, using only two injected experts, can reliably control outputs, outperform existing attacks, and evade current defenses. \nBy exploiting architectural sparsity and dynamic usage profiling, our approach uncovers backdoor vulnerabilities overlooked by traditional methods, highlighting critical risks in MoE architectures and calling for a reevaluation of current LLM security paradigms.", "tldr": "", "keywords": ["Mixture-of-Experts LLMs", "backdoor attack", "Routing Optimization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bba317e0e145e0b9d212a7d1b127c2d5dfc46200.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces BADMOE, a backdoor attack targeting Mixture of Experts MoE LLMs, and reveals a new vulnerability in expert routing. It proves the existence of dominating experts that can dictate outputs and exploit this by poisoning low-usage experts and designing routing aware triggers to activate them. BADMOE can achieve up to 100% attack success with only two infected experts while preserving benign performance and evading common defenses. The results expose that sparse expert activation enables stealthy and robust backdoors in MoE models and motivate the development of fundamentally new defences beyond conventional parameter and data-centric methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The technical design of BADMOE is grounded in both theory and practice, combining a formal proof of expert dominance with a well-structured three-stage attack pipeline and evaluation across multiple MoE architectures and tasks.\n- The findings highlight a critical and previously overlooked vulnerability in a fast-adopting class of LLM architectures, emphasizing the need for new MoE-specific security defenses.\n- The paper introduces a previously unexplored attack surface in Mixture-of-Experts LLMs by formulating the notion of dominating experts and demonstrating how their routing behavior can be exploited for backdoor injection."}, "weaknesses": {"value": "- While the paper evaluates several existing defenses, it does not explore or analyze potential MoE-specific defensive strategies in depth. For instance, the discussion of dormant expert pruning is shallow and lacks a systematic defense design or evaluation.\n- The evaluation primarily uses standard NLP benchmarks and tasks. The paper does not assess whether BADMOE can persist under domain adaptation or large-scale instruction tuning, which are common in practical LLM reuse scenarios.\n- The attack assumes that adversaries can inject poisoned experts and release modified MoE checkpoints publicly, but the paper provides limited evidence of how feasible such manipulations are in real-world model supply chains or open-source ecosystems. Some public repositories have applied verification when users upload models.\n- The paper does not demonstrate BADMOE in a complete deployment pipeline, e.g., a hosted API or plugin ecosystem, thus its persistence and exploitability under model updates or reinforcement fine-tuning remain unclear."}, "questions": {"value": "1. Can BADMOE persist after instruction tuning, domain adaptation, or RLHF-style fine-tuning?\n2. How sensitive is the attack to router retraining or expert replacement? Would re-initializing the router or randomizing expert selection mitigate BADMOE’s effect without heavy accuracy loss?\n3. Could the authors include quantitative metrics that connect the theoretical dominance score (e.g., KL divergence) with observed ASR, to demonstrate a stronger causal link between theory and practice?\n4. Could the authors expand the exploration of MoE-specific defenses beyond dormant expert pruning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "of98uzcbnM", "forum": "2qn6AOIkL1", "replyto": "2qn6AOIkL1", "signatures": ["ICLR.cc/2026/Conference/Submission10698/Reviewer_CD4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10698/Reviewer_CD4H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478815619, "cdate": 1761478815619, "tmdate": 1762921939395, "mdate": 1762921939395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new backdoor attack targeting Mixture-of-Experts LLMs: BadMoe. Specifically, given a predetermined model layer $l$, they identified *dormant experts* (i.e., experts that are underutilized by the model on a given dataset). Then, using GCG, they find a trigger prompt that preferentially activates the previously identified dormant experts. Lastly, they jointly finetune the dormant experts and the non-MoE components of the model to (i) preserve utility when the trigger is not present and (ii) display the targeted behavior when the trigger is present.\n\nThey evaluate their method on different tasks and show that they outperform prior methods in ASR and utility preservation. They demonstrate that their method is robust to various practical scenarios and prior defenses. These results suggest that BadMoe poses a significant threat to MoE models and reveals a new attack vector on these architectures."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and the method is clearly presented. Numerous illustrations (Fig. 1 and Fig. 2) help the reader understand the attack and its components, which in turn makes diving into the details of the method easier. Furthermore, all the necessary preliminaries on MoE needed to understand the method are clearly presented in the paper.\n- While the different components of their attack are not novel per se, combining them into a successful attack is non-trivial and represents a novel contribution, as it uncovers a new potential threat vector in MoE models.\n- The empirical evaluation is extensive and comprehensive: several tasks are evaluated and multiple baselines are used for comparison. Moreover, most components of the method are ablated, and potential defenses as well as realistic deployment scenarios are evaluated."}, "weaknesses": {"value": "- I think the utility evaluation is a bit sparse; using standard LLM benchmarks would improve it. More specifically, the dormant experts are selected because they are dormant on a specific task $\\mathcal{D}$. I am wondering what would happen if I evaluate the model’s clean accuracy on a task $\\mathcal{D}'$ for which the previously dormant experts are actually dominant (or at least often activated).\n- I do not see the contributions of Theorem 4.1 from Section 4.2. First, I think the Gaussian assumption is unrealistic, given that prior works have shown outliers in the activation distribution have a significant impact on LLM behavior ([1]). Second, the components of the attacks do not specifically leverage the insights from Theorem 4.1. The loss in Equation (9) is a standard backdooring loss with regularization, and it turns out that the injected experts dominate.\n- While the attack is clearly successful and robust, the results are mostly incrementally better than prior baselines (except on robustness to domain shift, where BadMoe shows a significant improvement compared to all baselines).\n\n[1] Systematic Outliers in Large Language Models, An et al., ICLR 2025."}, "questions": {"value": "- Could the authors evaluate BadMoe models on standard LLM benchmarks (e.g., MMLU, HumanEval, ...) that span a very wide range of tasks?\n- Are dormant experts inactive for all tasks, or are there tasks for which they would be active? If such tasks exist, would BadMoe retain high clean accuracy on those tasks?\n- Assume the attacker's trigger is $z$. What would happen if I optimize a new trigger $z' \\neq z$ that also activates only the experts from $S_{\\alpha}$? Would it activate the backdoor? If so, could optimizing a prompt to activate dormant experts in a model and then measuring accuracy be a targeted way to identify a BadMoe model?\n- How can Theorem 4.1 help improve or guide the design of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8aanOwY552", "forum": "2qn6AOIkL1", "replyto": "2qn6AOIkL1", "signatures": ["ICLR.cc/2026/Conference/Submission10698/Reviewer_foeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10698/Reviewer_foeM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725619273, "cdate": 1761725619273, "tmdate": 1762921938884, "mdate": 1762921938884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a backdoor attack against mixture of experts LLMs. The idea is based on the existence of dominating experts, that can determine the output of the LLM. The backdoor attack is based on the idea of poisoning underutilized experts that are unrelated to the question, but can be activated with a routing-aware trigger. Experimental study shows that the proposed approach can reliably control outputs and evade current defenses with two injected experts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper shows a valid approach of backdoor attack against MOE LLMs that specifically relies on the internal structure of the MOE. \n* The experimental study shows that the proposed backdoor can successfully attack the chosen LLMs in a very large set of cases."}, "weaknesses": {"value": "* The definition of a dominating expert (4.1) does not appear to take into consideration the other experts, which is an unusual definition for \"dominating\" something. \n* Given that in the definition of this paper, the experts are combined additively, and limits on the internal structure are not considered, it seems that Theorem 4.1 only says that you can always have a large enough output that it will be larger than the other models. This appears to be a very simple observation. \n* The step based on infecting dormant experts (4.5) appears to require training: all the parameters of the model outside the experts + the adversarial experts. It is not clear how much of the model is actually not trained. Also, the paper claim that in this procedure the theta_0 is trained to maintain normal model behavior and theta_a for dominating the target outputs - but actually nothing in the training objective implies this. The training objective in formula (9) is symmetrical in \\theta_0 and \\theta_a as well as in poisoned and clean data."}, "questions": {"value": "* Given that in the specified scenario the attacker needs to have access to the whole LLM, what would be the advantages of this particular approach of identifying dormant MOE, and only modifying them - as opposed to modifying all the experts?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The paper presents a technique to provide a backdoor into an LLM, raising security questions. On the other hand, knowing about potential attack vectors can also help the defense against them."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YYHyYUK47Y", "forum": "2qn6AOIkL1", "replyto": "2qn6AOIkL1", "signatures": ["ICLR.cc/2026/Conference/Submission10698/Reviewer_MgCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10698/Reviewer_MgCa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951586731, "cdate": 1761951586731, "tmdate": 1762921938496, "mdate": 1762921938496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new backdoor attack that works by targeting specific experts in an MoE model, so that the poisoned backdoored behavior is activated only when some experts are active. This makes the attack much more stealthy and hard to defend against. The attacks themselves are highly effective with often near-100% attack success rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A new poisoning attack that's effective at poisoning MoE models. Because it targets the MoE specifically, the attack is much more robust to defenses that try to remove backdoors because the poisons can be added to experts that don't usually activate and so it's hard to know why the model is malicious.\n\nHigh attack success rates---much higher than prior work. This is interesting in its own right, but I wonder if the baselines couldn't have been tuned to be somewhat stronger. Because poisoning is a tradeoff of utility-vs-asr I wonder what the full curve would look like.\n\nGood evaluation both with and without defenses."}, "weaknesses": {"value": "The paper is not very clear about why this work is interesting. The main introduction frames the work around being around MoE poisoning as something that hasn't been done, but is not very well motivated. This is true, but saying \"this hasn't been done before\" doesn't make a compelling paper. Most things haven't been done before. What's interesting about this attack is that, because the poisoning is done to rarely-activated experts, it's much harder to remove the poisoning via fine-tuning because these experts see very little gradient signal.\n\nThis directly ties in to my other concern with this work, though: there are probably simple defenses that would test each expert one-by-one in order to see if the backdoor is present, or defenses that make sure they've tested each expert one-by-one. The attack here is good motivation for doing things like this, but once you know that this is necessary the solutions are somewhat straightforward. This doesn't invalidate the utility of the attack, but it does make it less compelling if it's easily fixed.\n\nMy other concern with this work is that this attack works well for MoE models that have a small number of experts, but the recent trend (cf. deepseek) is to train many more experts and activate even fewer of them. Do the results still work in this setting?"}, "questions": {"value": "Could a defender implement techniques that test each expert independently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r6TxaqNJPK", "forum": "2qn6AOIkL1", "replyto": "2qn6AOIkL1", "signatures": ["ICLR.cc/2026/Conference/Submission10698/Reviewer_JVWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10698/Reviewer_JVWK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018564710, "cdate": 1762018564710, "tmdate": 1762921938106, "mdate": 1762921938106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}