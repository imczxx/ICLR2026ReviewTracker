{"id": "0USd4azknv", "number": 5162, "cdate": 1757858138754, "mdate": 1762940973740, "content": {"title": "VST-SD: Versatile Style Transfer with Content-Style Statistics Disentanglement", "abstract": "Recent works in versatile style transfer have achieved impressive results in both content preservation and style fidelity. However, optimizing models solely with content and style losses often fails to match the real image distribution, leading to suboptimal stylization quality. In this paper, we propose a novel self-supervised framework, VST-SD, which disentangles content and style representations to enhance stylization performance. Specifically, we separate content and style from the input and train the model to reconstruct the original image. To facilitate effective disentanglement, we leverage feature statistics: a content encoder is designed with perturbation and compression to remove style-related statistics, while a style encoder employs magnitude preservation to capture style-specific information. A cascade of diffusion models are introduced to integrate content and style into new images. To support multi-modal capabilities in versatile style transfer, we construct a paired text-style dataset and design a pipeline enabling flexible, text-guided stylization. Experimental results across artistic, photorealistic, and text-guided stylization demonstrate the effectiveness and versatility of our approach.", "tldr": "A general framework for image and text-guided style transfer", "keywords": ["versatile style transfer", "content-style disentanglement"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7f5b4ae4dd430611beaa7d76b4e3be452b1ceffa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents VST-SD, a diffusion-based versatile stylization method leveraging feature statistics disentanglement. This method involves a self-supervised learning framework that trains a diffusion model to reconstruct from the disentangled content and style representations. Traditional statistical methods rely on auxiliary objectives to learn the disentanglement of style and content features, while suffering from poor image quality. Instead, the authors propose to directly learn the disentangled representations in an unsupervised manner, and leverage the diffusion generative model to learn the stylized image distribution. Additionally, the authors introduce a diffusion-transformer to generate style representations from text to further enable flexible text-guided image stylization. This generation pipeline is a cascade of three separate diffusion models: 1) a main stylized image generator; 2) a refiner initialized with the first stage coarse generation; 3) a text-to-style generator to generate style representations from text."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method seems to be lightweight and fast.\n- The design of using diffusion models for text-to-style representation token generation, instead of directly extracting text representation, is novel.\n- The presented qualitative samples are high-quality with plausible style transfer. The quantitative comparisons demonstrate the advantages of the proposed method."}, "weaknesses": {"value": "- Legend is missing in Figure 2. The red line in the left chart is confusing, does it represent split operation?\n- The overall experimental results seem to be sparse. Specifically, the quantitative evaluations are conducted on 100 images sampled from the training set. Moreover, only photorealistic stylization is tested in Section 4.3, I would like to see how the proposed method behave on other styles like digital art etc.. I believe more evaluation results on a public benchmark like IMAGStyle [1] will better showcase the effectiveness of the proposed method.\n- In addition to the sparsity in test data, the employed evaluation metrics are also constrained. There are four quantitative metrics reported, three of which evaluate the style consistency and the last one is CLIP-IQA which evaluates image quality. I believe there should be some metrics measuring the content similarity as well. For instance, leveraging the proposed content encoder for such evaluation will not only demonstrate the effectiveness of the encoder itself, but also be a good quantitative metric for comparing content similarity.\n- Besides the limitation in quantitative metrics, the proposed method fails on the CLIP-based style similarity evaluation. Notably, both of the other two style-consistency metrics are related with Gram Loss. I wonder if these two advantageous metrics are correlated with each other, and in that case the superiority of the proposed method would be much unclear. Overall, I believe there should be more and better designed evaluations.\n- The analysis of the entanglement of style and content representation in Section 4.4.1 is interesting. The authors implement a smart way to visualize the extracted representations. But if the inverted content and style are entangled, it is super unclear how the proposed method works since it is built on the assumption of ‘a generator conditioned on the disentangled content and style representations can reconstruct the original image’ (line 58-60).\n- The ablation setting is ambiguous. Are the reported results in Table 3 evaluated on separate models trained w/ and w/o the listed components? Or did the author directly evaluate the model performance without statistical perturbation, for instance, even though the model was trained with it. Moreover, since the visualization analysis in 4.4.1 couldn’t demonstrate the disentanglement effect. Maybe another set of ablation studies on the compression rate in Section 3.2 can help us figure out how the encoder learns the disentanglement of style and content features?\n- Finally, limitations of the proposed method are not discussed.\n\n[1] Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024."}, "questions": {"value": "In line 199-201, the authors said ‘By visualizing the features at different layers, we observe that the feature from layer 4 captures more global structure’. Can we have some visualized samples presented here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I believe there is no special ethics concerns in this paper."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P84dSN92SX", "forum": "0USd4azknv", "replyto": "0USd4azknv", "signatures": ["ICLR.cc/2026/Conference/Submission5162/Reviewer_CtAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5162/Reviewer_CtAs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760849428074, "cdate": 1760849428074, "tmdate": 1762917917893, "mdate": 1762917917893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new versatile style transfer framework called VST-SD, which can perform artistic, photorealistic, and text-guided stylization in a unified model. VST-SD consists of two key components: a statistics-removal content encoder that removes style statistics using perturbation and channel-wise compression, and a magnitude-preservation style encoder that robustly captures and stabilizes style representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method supports three kinds of style transfer: artistic, photorealistic, and text-guided style transfer, in a unified model.\n2. The images generated by this method for artistic and photorealistic style transfer are impressive.\n3. Extensive experiments are conducted to evaluate the performance of the proposed method."}, "weaknesses": {"value": "1. The proposed method shows unsatisfactory performance in text-guided style transfer. For example, in the first row of Figure 4, the description in the text instruction, \"Luminous white sandy shores. Sunset orange and twilight blue skies\" is not reflected in the generated image. The quantitative results in Table 2 also demonstrate that the proposed method performs the worst in text alignment compared to all other baseline methods.\n\n2. In Section 3.2, the paper states, 'By visualizing the features at different layers, we observe that the feature from layer 4 captures more global structure and thus use its output as the original content representation,' but no specific experimental results are provided to support this conclusion.\n\n3. In the ablation experiment shown in Figure 3, the difference between using and not using refinement diffusion is not significant. In this case, it is recommended to provide quantitative experiments to further demonstrate the effectiveness of the refinement diffusion.\n\n4. In the ablation experiment in Section 4.4.2, there is a lack of qualitative experimental results, which are the most direct way to observe the effects of each component.\n\n5. Some state-of-the-art style transfer methods are not introduced and compared in this paper, such as SaMam [1], HIS [2], OmniStyle [3], and StyleSSP [4]. \\\n[1] SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer. CVPR 2025. \\\n[2] HSI: A Holistic Style Injector for Arbitrary Style Transfer. CVPR 2025. \\\n[3] OmniStyle: Filtering High Quality Style Transfer Data at Scale. CVPR 2025. \\\n[4] StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer. CVPR 2025."}, "questions": {"value": "Please see **Weaknesses**.\n\nOthers:\n\n1. Section 3.2 states, \"This perturbation can be realized by using mini-batch statistics, and we implement it by reactivating the batch normalization layers for efficiency.\" Why does reactivating the batch normalization layers help achieve perturbation?\n\n2. Section 3.3 states, \"we flatten the lower triangular elements into a vector.\" Why were the lower triangular elements chosen? What is the purpose of doing this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r4cWJTNgcF", "forum": "0USd4azknv", "replyto": "0USd4azknv", "signatures": ["ICLR.cc/2026/Conference/Submission5162/Reviewer_4tVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5162/Reviewer_4tVJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141715315, "cdate": 1761141715315, "tmdate": 1762917917623, "mdate": 1762917917623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a self-supervised framework for image/text-based style transfer. The core innovation lies in its approach to content-style disentanglement by leveraging feature statistics. The authors' motivation for designing the content-style disentanglement method stems from previous research on the relationship between image style and feature statistics. By employing channel-wise compression and perturbation, they encourage the disentanglement of style information. Through comparative experiments, the authors claim that their method achieves state-of-the-art performance in style transfer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I appreciate the paper's motivation to abandon traditional style loss and adopt a self-supervised approach for style disentanglement. Style transfer has long lacked convincing evaluation metrics, making it necessary to revisit this task from fundamental principles. Additionally, the paper achieves not only image-guided style transfer but also text-guided style transfer."}, "weaknesses": {"value": "- While numerous prior studies have demonstrated the effectiveness of style transfer by manipulating statistical features, they have not proven that style information strictly corresponds to these statistics. Therefore, using this as a self-supervised objective cannot achieve complete disentanglement of style information. As shown in Figure 7, the content image still retains abstract stylistic attributes from the original image, such as brushstrokes.\n\n- The experimental details are insufficient—for instance, implementations and hyperparameters of compression and perturbation, as well as the construction details of the text-style dataset (Figure 4 appears to only illustrate object or object-color pair).\n\n- The refinement diffusion is the most computationally heavy component in the pipeline, yet its necessity lacks further discussion. There are no quantitative metrics provided, and the qualitative results in Figure 3 show almost no discernible difference."}, "questions": {"value": "Please address the concerns raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xIdrGoSQxb", "forum": "0USd4azknv", "replyto": "0USd4azknv", "signatures": ["ICLR.cc/2026/Conference/Submission5162/Reviewer_pSmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5162/Reviewer_pSmz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657908619, "cdate": 1761657908619, "tmdate": 1762917917298, "mdate": 1762917917298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks the reviewers for taking the time to review this paper.\n\nI have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "D0vycmTQZE", "forum": "0USd4azknv", "replyto": "0USd4azknv", "signatures": ["ICLR.cc/2026/Conference/Submission5162/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5162/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762938058294, "cdate": 1762938058294, "tmdate": 1762938058294, "mdate": 1762938058294, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel self-supervised framework named VST-SD for Versatile Style Transfer (VST), which unifies artistic style transfer, photorealistic style transfer, and text-guided style transfer. Experimental results demonstrate that the proposed method outperforms existing approaches on both qualitative and quantitative metrics across all three tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method achieves strong performance across three distinct style transfer tasks.\n2. The paper's core contributions statistics removal in the content encoder and magnitude preservation in the style encoder are key to achieving high-quality self-supervised disentanglement. This approach of explicitly disentangling statistics is more fundamental and effective than traditional methods that rely on predefined loss functions."}, "weaknesses": {"value": "1. The paper states that statistics perturbation is achieved by reactivating the batch normalization layers. This is unclear. Does this simply mean using training mode during inference? If so, this is just standard batch normalization, not a novel perturbation.\n2. The paper lacks a discussion on abstract style texts. The text-to-style module appears to be limited to descriptive styles . This is an important limitation that the paper does not discuss.\n3. Is \"Dstulet\" in Table 3 a typo for \"Dstyle\"?"}, "questions": {"value": "1. The paper mentions that \"statistical perturbation is achieved by reactivating the batch normalization layers\", but the mechanism is not clearly described. Does \"reactivating\" here specifically refer to using the training mode of batch normalization during inference? If it is only this operation, what is the difference between it and standard batch normalization, and how to reflect the innovation of \"perturbation\"?​\n2. The existing research does not involve discussions on abstract style texts, and the text-to-style module only shows adaptability to descriptive styles. Could you supplement the explanation of the module's limitations in processing abstract style texts, and whether there are optimization directions for this limitation in the future?\n3. The term \"Dstulet\" in Table 3 is different from the context and the common term \"Dstyle\". Is \"Dstulet\" a typo of \"Dstyle\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SoxKsPgbZr", "forum": "0USd4azknv", "replyto": "0USd4azknv", "signatures": ["ICLR.cc/2026/Conference/Submission5162/Reviewer_WkK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5162/Reviewer_WkK1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804593780, "cdate": 1761804593780, "tmdate": 1762917916995, "mdate": 1762917916995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}