{"id": "JfhxzTASJr", "number": 5297, "cdate": 1757895014412, "mdate": 1762986604390, "content": {"title": "Continuous Hand Gesture Spotting through Deep Sequential Encoding and Probabilistic Time-Series Modeling", "abstract": "Continuous hand gesture spotting in real time is a challenging problem because ambiguous gesture boundaries and abundant non-gesture motions often confound recognition systems. Unlike isolated recognition, spotting requires detecting both the onset and offset of gestures while rejecting irrelevant transitions, making robustness crucial for practical human–computer interaction. We present a hybrid framework that integrates MediaPipe Hands for extracting 3D landmarks, an LSTM Autoencoder for compact spatiotemporal encoding, and Gaussian Hidden Markov Models (HMMs) for probabilistic sequence modeling. To further suppress spurious detections during transitions, we introduce an ergodic threshold mechanism that adaptively filters low-likelihood segments. On a vocabulary of 10 command gestures, the system achieves 96.56% recognition accuracy, 97.89% segmental F1, and 6.55% word error rate (WER) in continuous input streams, while remaining lightweight enough to run on a CPU-only device. These results show that combining deep representation learning with probabilistic dynamics yields reliable boundary detection without heavy computational overhead. Beyond empirical gains, the framework is data-efficient and readily extensible to new vocabularies, enabling rapid adaptation with limited training data. Overall, these findings demonstrate the practical feasibility of robust gesture spotting, bridging the gap between controlled research settings and real-world applications in VR/AR environments and customizable user interfaces.", "tldr": "A hybrid framework with MediaPipe Hands, LSTM Autoencoder, and Gaussian HMMs achieves 96.6% accuracy, 97.9% F1, and 6.6% WER in continuous gesture spotting, enabling accurate, data-efficient VR/AR interaction.", "keywords": ["Continuous Hand Gesture Spotting", "Gesture recognition", "Hidden Markov Model", "LSTM Autoencoder", "MediaPipe Hands", "Deep sequential encoding", "Probabilistic Time-Series Modeling", "Threshold -based Filtering"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dc8e09e46c63760429beff74914a1784927edb0f.pdf", "supplementary_material": "/attachment/fdafcad0931066a307a1238509dc107d51c05de7.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a hybrid framework that integrates MediaPipe Hands, an LSTM Autoencoder, Gaussian Hidden Markov Models (HMMs), and an ergodic threshold mechanism for continuous hand gesture spotting. Specifically, MediaPipe Hands is used to extract 3D landmarks, the LSTM Autoencoder performs compact spatiotemporal encoding, Gaussian HMMs are employed for probabilistic sequence modeling, and the ergodic threshold mechanism adaptively filters low-likelihood segments. The effectiveness of the proposed method is validated through experiments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is lightweight and can be deployed on CPU-only devices for continuous hand gesture spotting."}, "weaknesses": {"value": "The paper shows very limited originality, as it merely combines several existing methods without introducing substantial innovation.\n\nThe experimental validation is weak, being conducted only on a simple dataset without any comparisons to existing approaches.\n\nThere are writing issues, and the citation style is clearly incorrect."}, "questions": {"value": "How does the proposed method compare with the current state-of-the-art approaches for continuous hand gesture spotting?\n\nWhat are the speed and resource consumption characteristics of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a0whccEoYx", "forum": "JfhxzTASJr", "replyto": "JfhxzTASJr", "signatures": ["ICLR.cc/2026/Conference/Submission5297/Reviewer_HVaK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5297/Reviewer_HVaK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716525836, "cdate": 1761716525836, "tmdate": 1762917993870, "mdate": 1762917993870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "In this work, I attempted to extend my earlier HMM-only approach to the ‘gesture spotting’ task by integrating it with deep learning and enabling vector-level inputs - rather than scalar inputs - so that fine-grained finger-level motion could be incorporated. This was intended to address the inefficiencies of deep learning when handling sequence data. However, due to time constraints, I was unable to complete the necessary experiments, and therefore I am withdrawing the submission."}}, "id": "Aqw5b95WGF", "forum": "JfhxzTASJr", "replyto": "JfhxzTASJr", "signatures": ["ICLR.cc/2026/Conference/Submission5297/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5297/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762986603689, "cdate": 1762986603689, "tmdate": 1762986603689, "mdate": 1762986603689, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on detecting gestures in continuous movements by identifying the onset of these gestures from their boundary and offset windows. It uses MediaPipe for pose estimation, then employs LSTM and graphical models for spatio-temporal modelling. The model then uses gesture-specific transitional scores and models to localize their onsets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper explicitly tackles continuous gesture spotting (onset/offset detection plus non-gesture rejection), which is more challenging and practically relevant than isolated gesture classification for HCI.\n\n- The system is designed to run in real time on a CPU-only machine, which is useful for many HCI scenarios where GPU resources are not available."}, "weaknesses": {"value": "- The temporal modelling relies on LSTM autoencoders and Gaussian HMMs with manually designed thresholding. While these choices are defensible from an efficiency standpoint, the field has largely moved toward attention-based and transformer models for continuous sign and gesture recognition. The paper cites such works but does not provide empirical comparisons, nor does it compare against simpler yet strong baselines. Without these baselines, it is challenging to justify the added complexity of the LSTM-encoder + HMM + threshold architecture or to support the claim that this hybrid design is indeed necessary.\n\n- The experimental setup is restricted to 10 command gestures for PowerPoint control, using only the right hand. This is a narrow, application-specific vocabulary, and many gestures are relatively simple. The paper does not evaluate its approach on standard continuous-hand benchmarks (e.g., the datasets mentioned in the related work), so it is unclear how the approach would fare on more diverse, noisy, or complex gesture sets. \n \n- The paper claims that the framework is “readily extensible to new vocabularies” and that it is data-efficient. However, there is no experiment where new gestures are added to the vocabulary to measure how performance scales with vocabulary size.\n\n- Given that the threshold model complexity grows with the number of gesture classes, the scalability claim is particularly important to validate experimentally."}, "questions": {"value": "I do not have questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "76NT602C1a", "forum": "JfhxzTASJr", "replyto": "JfhxzTASJr", "signatures": ["ICLR.cc/2026/Conference/Submission5297/Reviewer_n6VW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5297/Reviewer_n6VW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762262834323, "cdate": 1762262834323, "tmdate": 1762917993667, "mdate": 1762917993667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new integrated method for gesture spotting, i.e., the segmentation and recognition of gestures from a continuous stream of images. To this end, four methods are combined: \"MediaPipe Hands\" for localization, an LSTM autoencoder, a HMM for recognition, and an \"ergodic\" threshold mechanism to suppress spurious detections. This system is validated on a self-created dataset of continuous hand gestures with very good accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is easy to read and describes all parts of the proposed system well. Results are rather credible, and the fact that this runs without GPU acceleration is pretty amazing."}, "weaknesses": {"value": "- It would be better to introduce what \"spotting\" means, exactly. \n- The related work section could be more extensive about \"spotting\", not just gesture recognition\n- The novelty is rather incremental, as you \"just\" piece together elements that have been described already\n- It would be better to test you method on publicly available datasets to allow a comparison to other methods\n- The dataset you test you method on is not public, making the claims hard to verify\n- The path from raw data to system response is not quite detailed enough (feature encoding, cropping, rescaling, ...). This should at least be explained in the appendix."}, "questions": {"value": "- Do you assume that all gestures have the same length? If so, please explain why this is not a problem in practice\n- Is the dataset you use for evaluation publicly available? I found no link or similar..."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KXq9Ldny60", "forum": "JfhxzTASJr", "replyto": "JfhxzTASJr", "signatures": ["ICLR.cc/2026/Conference/Submission5297/Reviewer_XrpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5297/Reviewer_XrpH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762325668532, "cdate": 1762325668532, "tmdate": 1762917993407, "mdate": 1762917993407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}