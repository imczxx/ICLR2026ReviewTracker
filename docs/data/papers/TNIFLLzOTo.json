{"id": "TNIFLLzOTo", "number": 6311, "cdate": 1757966704988, "mdate": 1763267683122, "content": {"title": "Differentially Private Conditional Text Generation with RL-Boosted Control", "abstract": "Generating high-quality synthetic text under differential privacy (DP) is critical for training and evaluating language models without compromising user privacy. Prior work on synthesizing DP *datasets* often fail to preserve key statistical attributes, suffer utility loss from the noise required by DP, and lack fine-grained control over generation. To address these challenges, we make two contributions. First, we introduce a hierarchical framework that decomposes DP synthetic text generation into two subtasks: *feature learning* and *conditional text generation*. This design explicitly incorporates learned features into the generation process and simplifies the end-to-end synthesis task. Through systematic ablations, we identify the most effective configuration: a rich tabular schema as feature, a DP tabular synthesizer, and a DP fine-tuned conditional generator, which we term ACTG (**A**ttribute-**C**onditioned **T**ext **G**eneration). Second, we propose Anchored RL (ARL), a post-training method that improves the instruction-following ability of ACTG for conditional generation. ARL combines RL to boost control with an SFT anchor on best-of-$N$ data to prevent reward hacking. Together, these components form our end-to-end algorithm **ACTG-ARL**, which advances both the quality of DP synthetic text (+20\\% MAUVE over prior work) and the control of the conditional generator under strong privacy guarantees.", "tldr": "We propose a recipe for differentially private conditional text generation that advances the state of the art.", "keywords": ["differentially private synthetic data", "conditional generation", "reinforcement learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76a86d2e99d9634b42ec021e638e08ffdbaef4f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduced a hierarchical framework (with ACTG as the optimal configuration) and a novel Anchored RL recipe that, together, form our end-to-end algorithm ACTG-ARL. \nExperimental results show improvements over strong baselines (DP-FT, CTCL, Aug-PE) across two datasets and three privacy budgets. The paper is well-motivated, with ablation studies that validate each design choice. It contributes to advancing DP text generation by emphasizing fine-grained controllability—a valuable new dimension alongside utility and privacy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel and well-structured framework (ACTG) for differentially private (DP) text generation, which decomposes the synthesis process into feature learning and conditional text generation. This modular approach leads to improved privacy-utility trade-offs and provides interpretability benefits.\n\nThe introduction of Anchored Reinforcement Learning (ARL) further enhances instruction-following capabilities under privacy constraints, successfully addressing the problem of reward hacking that often plagues RL-based alignment."}, "weaknesses": {"value": "1.\tThis paper aims to design a differentially private text generator, however, the argument for differential privacy is not sufficient:\n\n①\tAuthors say Stage 0 doesn’t consume any privacy budget. Why treat Stage 0 with a trusted component and other stages not. I didn’t find the discussions in Appendix C.1\n\n②\tHow to employ the framework and how to prove the framework is differentially private are not clear. For the DP feature generator, DP-FT is a text generator, how to generate feature here? For DP conditional generator, how to perform DP-FT, and the method( “prompting a powerful LLM…”) didn’t protect privacy.\n\n③\tThe usage of ARL is not clear. Authors did not the illustration of privacy for this step.\n\n2.\tThe baseline coverage could be broader. The paper mainly compares against CTCL, DP-FT, and Aug-PE; including recent diffusion-based or graphical-model-based DP synthesizers (e.g., Ochs & Habernal 2025; DeSalvo et al. 2024) would further solidify the empirical claims. \n\n3.\tSome technical details—such as PPO background, reward signal stability, and computational overhead of best-of-N sampling—could be expanded for clarity. Additionally, all experiments are performed using a single model size (gemma-3-1b-pt), leaving scaling behavior unexplored.\n\n4.\twhile ARL is effective, its reliance on an LLM oracle for annotation and evaluation may limit reproducibility or accessibility for smaller labs.\n\n5.\tThe conclusion in Figure 4(a) does not illustrate the issue because the difference in ε=1/∞ is significant. (The experiment for eps=∞ does not illustrate the issues, as in this case, this is no privacy guarentee.)"}, "questions": {"value": "1.\tHow sensitive is the performance of ARL to the hyperparameter γ and the number of candidates N in best-of-N sampling?\n\n2.\tCould the authors clarify the computational cost of the ARL fine-tuning stage relative to ACTG and ACTG-RL?\n\n3.\tIs it possible to integrate diffusion-based DP generators into the Stage-1 feature generation step to improve diversity?\n\n4.\tHow would the proposed method scale if larger or smaller base models were used?\n\n5.\tIn the ablation study of 5.3.3.2, why is it said that ground-truth features of D^x_priv is not available? This is just a comparative experiment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cqfqXZvH64", "forum": "TNIFLLzOTo", "replyto": "TNIFLLzOTo", "signatures": ["ICLR.cc/2026/Conference/Submission6311/Reviewer_g64a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6311/Reviewer_g64a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535201695, "cdate": 1761535201695, "tmdate": 1762918609606, "mdate": 1762918609606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new hierarchical framework for generating high-quality synthetic text under differential privacy (DP). Prior approaches to DP synthetic text generation often struggle with preserving key statistical attributes, suffer from significant utility loss due to injected noise, and lack fine-grained control during generation.To address these issues, the authors decompose the DP synthetic text generation task into two subtasks: feature learning and conditional text generation. Their framework uses a rich tabular schema as a feature representation, a DP tabular synthesizer to ensure privacy during feature learning, and a DP fine-tuned conditional generator for text synthesis. Another key contribution is Anchored Reinforcement Learning (Anchored RL), a post-training method that enhances the instruction-following capability of the conditional generator, ACTG, under DP constraints. Empirically, the proposed method improves both text quality and control compared to prior work. It also offers strong privacy guarantees, allowing the resulting DP synthetic datasets to be reused without additional privacy costs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and easy to follow, with a logical flow that makes the main ideas and contributions understandable.\n\n- The arguments are well structured, and the overall organisation effectively supports the proposed framework and experimental results.\n\n- The inclusion of code significantly enhances the validity and reproducibility of the work. The codebase is well structured, sufficiently documented, and provides a strong foundation for others to build upon."}, "weaknesses": {"value": "- The reported performance improvements, while consistent, are modest compared to baselines such as vanilla DP-FT and CTCL. The authors could strengthen their claims by including statistical measures of variability (e.g., variance bars or standard deviations) in the plots to show robustness across multiple runs. \n\n- It would be valuable to include an additional baseline that applies Anchored RL directly to vanilla DP-FT. This would help isolate and clarify the contribution of Anchored RL to the overall performance gains.\n\n- All experiments were conducted using a single model, which the authors acknowledge as a limitation. \n\n\nThe paper presents a solid contribution with clear writing, strong methodological grounding, and commendable reproducibility. However, I remain uncertain about how much of the reported improvements can be attributed to noise, given the absence of variance bars or discussion of experimental variability. Therefore, I am assigning a score of 6. I would be open to reconsidering my score based on the rebuttal, particularly if the authors can clarify the robustness of their results across multiple runs and provide additional statistical evidence supporting the observed gains."}, "questions": {"value": "- I could not locate the script used to produce Figure 6, Appendix C (schema identification) in the provided codebase. Including this script would make the work fully reproducible and ensure that others can replicate the S3 approach end-to-end.\n\n- Could the authors clarify whether the number or quality of extracted features influences the effectiveness of the S3 approach? An ablation study examining how varying the number of features impacts performance would provide deeper insights into the method’s sensitivity and limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dE9i42ej6w", "forum": "TNIFLLzOTo", "replyto": "TNIFLLzOTo", "signatures": ["ICLR.cc/2026/Conference/Submission6311/Reviewer_9s83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6311/Reviewer_9s83"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898466422, "cdate": 1761898466422, "tmdate": 1762918609276, "mdate": 1762918609276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new approach to generating DP synthetic text using a hierarchical framework. The model is designed to address the challenges of privacy, data utility, and controlled generation. The framework decomposes the generation task into two stages: feature learning and conditional text generation. The authors introduce ACTG (Attribute-Conditioned Text Generation), a method that optimizes both DP synthetic text quality and fine-grained control over the generation. They also propose a post-training method, Anchored RL (ARL), which improves the instruction-following ability of ACTG by addressing control degradation under DP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The hierarchical framework (ACTG) combines a DP tabular synthesizer with a DP fine-tuned conditional generator. This separation of tasks allows for improved optimization.\n\n2.The introduction of ARL to address instruction-following is a significant contribution. It demonstrates the balance between privacy and control in DP settings, achieving better performance than previous methods.\n\n3.ACTG-ARL outperforms prior methods in multiple metrics, including MAUVE and attribute distribution matching. This provides solid evidence of its practical utility."}, "weaknesses": {"value": "1.The framework depends heavily on accurate feature extraction, but does not provide a detailed description of the feature extractor, especially regarding how attributes are selected or normalized before DP processing. \n\n2.The difference between this work and CTCL isn’t made very clear. Although the paper introduces some refinements, it largely follows CTCL’s existing structure of “public pretraining → private fine-tuning → synthetic generation.”  The overall idea and workflow feel quite similar, and the updates seem more like incremental technical improvements than a fundamentally new approach.\n\n3.The evaluation is limited to one model configuration, leaving open the question of how well the approach generalizes across different model sizes and capacities."}, "questions": {"value": "1.The evaluation is conducted on a single model configuration, but the chosen model and experimental setup raise questions. In the Aug-PE stage, the paper replaces the original GPT-3.5 (used in prior work) with Qwen, which undermines one of the core advantages of the approach,its compatibility with black-box LLMs. This substitution also makes it unclear whether the reported improvements are due to the framework itself or to differences in the underlying model. Furthermore, Aug-PE should theoretically outperform DP-FT given its reinforcement of privacy-preserving generalization, yet this expected advantage is not reflected in the experimental results.\n2.How can the trade-off between control and text fidelity be better balanced during the joint optimization of SFT and ARL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BJNSlPOnxN", "forum": "TNIFLLzOTo", "replyto": "TNIFLLzOTo", "signatures": ["ICLR.cc/2026/Conference/Submission6311/Reviewer_hwX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6311/Reviewer_hwX2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928609057, "cdate": 1761928609057, "tmdate": 1762918608782, "mdate": 1762918608782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present ACTG-ARL, a novel framework for differentially private (DP) conditional text generation that aims to improve both the quality of synthetic text and fine-grained control over generation while maintaining strong privacy guarantees. The authors propose a hierarchical framework that decomposes DP synthetic text generation into feature learning and conditional text generation. They also introduce Anchored RL (ARL), a post-training method that uses reinforcement learning (RL) with a supervised fine-tuning (SFT) anchor to boost instruction-following ability and mitigate reward hacking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The authors clearly highlight the weaknesses in existing DP based text generation techniques. The motivation for the paper is strong.\n\nS2. ACTG is modular and hierarchical potentially allowing better privacy utility tradeoffs.\n\nS3. I like the Anchored RL approach for better instruction following in DP-trained conditional generations. The empirical results look strong and the evaluations are well grounded."}, "weaknesses": {"value": "W1. The complexity and computational cost of the overall framework might be challenging to implement. More discussion on this should be added in the paper. \n\nW2. The quality of the generated features and rewards is directly tied to the capabilities of these oracle LLMs. The paper acknowledges this with the discussion on extraction error but doesn't fully explore potential limitations if a less capable or open-source LLM is used as the oracle.\n\nW3. While the structured tabular schema (S3) performs well, the process of designing such a schema (LLM-assisted) is described as \"dataset-specific.\" This raises questions about how much manual effort or expert knowledge is required to create effective schemas for new domains, and whether the LLM assistance is truly robust across highly diverse data."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7HNvsTJSp4", "forum": "TNIFLLzOTo", "replyto": "TNIFLLzOTo", "signatures": ["ICLR.cc/2026/Conference/Submission6311/Reviewer_RyGE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6311/Reviewer_RyGE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055343393, "cdate": 1762055343393, "tmdate": 1762918608243, "mdate": 1762918608243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global response"}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback, including comments on the choice of oracle LLM, the use of a single model, and schema design. We have incorporated these suggestions and conducted additional experiments accordingly. All new results are included in the revised draft. Some highlights include:\n\n\n  - In **Appendix F.8**, we replace Gemini with Qwen2.5-32B-Instruct (an open-source model that can be run locally) for feature extraction, and repeat the ACTG pipeline. The resulting DP synthetic data achieves nearly identical MAUVE scores, demonstrating that ACTG is robust to the choice of feature extractor and can work effectively with reasonably capable open-source models.\n  - In **Appendix F.10**, we evaluate a larger model, gemma-3-4b-pt. While scaling up improves performance for all methods, ACTG maintains a clear and substantial advantage over other baselines, confirming that our performance gains persist at larger scales.\n  - In **Appendix F.7**, we examine a simpler 3-field schema and find that even this minimal schema offers meaningful conditioning signals and outperforms all baselines. At the same time, a richer schema more faithfully captures key information of the private dataset and therefore performs better. \n\n\nWe are grateful for these suggestions, which help us further strengthen the paper. We view them as complementary to our main contributions and the broader significance of our work, summarized below.\n\n\n  - **Structured insights in unstructured data synthesis.**\nWe reframe part of the task of DP text synthesis as DP tabular data synthesis, enabling us to combine the strengths of both fields within a unified hierarchical framework. This perspective of *leveraging structured insights to guide unstructured data synthesis* represents a major conceptual advance.\n  - **Substantial and consistent performance improvements.**\nAcross all datasets, privacy regimes and evaluation metrics, ACTG delivers large, consistent improvements over prior methods. These results validate the effectiveness of our hierarchical design and the hybrid tabular-text synthesis strategy. \n  - **Introducing control as a third fundamental dimension.**\nOur work is the first to explicitly elevate *control*, alongside utility and privacy, as a third fundamental dimension in DP synthetic text generation. This conceptual shift reframes the problem space and opens new possibilities for fine-grained generation under privacy constraints. Building on this principle, we propose the Anchored RL (ARL) algorithm, which adapts state-of-the-art LLM training and inference techniques to the DP setting, a key technical contribution that operationalizes this new dimension.\n  - **Strong reproducibility and community impact.**\nAs Reviewer 9s83 remarked, our codebase is “well structured, sufficiently documented, and provides a strong foundation for others to build upon”. This reproducibility is intentional: our goal is to make future extensions straightforward. To further support the community, **we have released all of our DP synthetic datasets on our anonymous GitHub repository**.\n  - **Critical re-evaluation of prior practices.**\nAlthough not highlighted in the reviews, our paper also revisits prevailing evaluation practices (Remark 3.3) and identifies critical issues, such as reliance on weak embedding models and short context lengths. We address these shortcomings and establish a more rigorous evaluation pipeline for future work.\n\n\nTaken together, our work introduces new perspectives, tools, and evaluation protocols for DP synthetic text generation. We believe these contributions represent a meaningful step forward and will be of broad interest to the community."}}, "id": "pSBc9WyOxK", "forum": "TNIFLLzOTo", "replyto": "TNIFLLzOTo", "signatures": ["ICLR.cc/2026/Conference/Submission6311/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6311/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6311/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763267729051, "cdate": 1763267729051, "tmdate": 1763267729051, "mdate": 1763267729051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}