{"id": "SJ2WK6kdPJ", "number": 16730, "cdate": 1758268139939, "mdate": 1759897222380, "content": {"title": "NABLA: Neighborhood-Adaptive Block-Level Attention for Efficient Video Diffusion Transformers", "abstract": "Full self‑attention in video diffusion transformers scales quadratically with the spatio‑temporal token count, making processing the high‑resolution clips prohibitively slow and memory‑heavy. We introduce NABLA, a Neighborhood‑Adaptive Block‑Level Attention mechanism that builds a per‑head sparse mask in three steps: (i) average‑pool queries and keys into $N\\times N$ blocks, (ii) keep the highest‑probability blocks via a cumulative‑density threshold, and (iii) optionally union the result with Sliding‑Tile Attention (STA) to suppress border artefacts. NABLA drops straight into PyTorch's FlexAttention with no custom kernels or extra losses. On the Wan 2.1 14B text‑to‑video model at 720p, NABLA accelerates training and inference by up to $2.7\\times$ while matching CLIP ($42.06\\rightarrow42.08$), VBench ($83.16\\rightarrow 83.17$) and FVD ($68.9\\rightarrow 67.5$) scores. During pre‑training of a 2B DiT at $512^2$, iteration time falls from 10.9s to 7.5s ($1.46\\times$) with lower validation loss. A link to the code and model weights will be published in the camera-ready version of the paper.", "tldr": "A novel sparse attention approach for transformer-based architectures in video generation tasks", "keywords": ["Video generation", "diffusion transformers", "sparse attention", "adaptive computation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9eb902ca0ae5bc1736d15f4054aabbbe8069dd20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a Neighborhood-Adaptive Block-Level Attention mechanism (NABLA) designed to accelerate video diffusion transformers by exploiting sparsity in the attention map. The method operates in three steps: (1) average-pool queries and keys into N×N blocks; (2) apply a cumulative density threshold to select high-importance blocks; (3) optionally combine the result with Sliding Tile Attention (STA) to mitigate boundary artifacts. NABLA can be integrated directly into PyTorch’s FlexAttention without custom CUDA kernels. The authors claim up to 2.7× acceleration on the WAN 2.1 (14B) model, with comparable CLIP/FVD/VBench metrics to full attention."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. NABLA is designed to fit into PyTorch’s FlexAttention API with minimal engineering cost, which makes it practically useful.\n\n2. Although not deep, the experiments are at least conducted on a realistic backbone, lending some credibility to deployment feasibility."}, "weaknesses": {"value": "1. The overall presentation of the paper is poor. For instance, Figure 1 is disproportionately large and occupies excessive space, reducing readability. In addition, the Introduction section devotes most of its content to related work, while providing only a brief, single paragraph describing the proposed method, which makes it difficult for readers to grasp the core contribution.\n\n2. The proposed approach (average pooling + CDF thresholding) is a straightforward engineering heuristic, not grounded in theory or optimization principles. Could the author provide some theory guarantee?"}, "questions": {"value": "1. How does NABLA differ mathematically from prior dynamic sparse attention methods like DSV (Tan et al., 2025) or AdaSpa (Xia et al., 2025)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "723sP4hAZN", "forum": "SJ2WK6kdPJ", "replyto": "SJ2WK6kdPJ", "signatures": ["ICLR.cc/2026/Conference/Submission16730/Reviewer_fc3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16730/Reviewer_fc3H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556172740, "cdate": 1761556172740, "tmdate": 1762926782355, "mdate": 1762926782355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the computational bottleneck of full self-attention in video diﬀusion transformers (DiTs). The authors propose NABLA, a training-free, adaptive block-level attention mechanism. The core algorithm works by average-pooling Q/K matrices to compute a cheap, low-resolution attention map. It then applies a Cumulative Density Function (CDF) threshold to dynamically select the most important blocks. It proposes to combine the dynamic sparse mask with STA mask. The proposed method outperforms the STA baseline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper identifies a consistent failure mode of static sparse attention (such as STA): object duplication and boundary artifacts. It attributes this issue to limited global attention coverage. This observation clearly explains why fixed sparsity patterns can harm video generation quality.\n  \n- The proposed method is easily implemented using PyTorch without requiring custom CUDA kernels or model retraining."}, "weaknesses": {"value": "**Contribution and novelty are limited.** The main idea (downsample Q/K, compute coarse attention, and guide sparse masking) is not new; it closely resembles SpargeAttention, MInference, and SeerAttention, etc. The proposed CDF-based thresholding is also previously used in SpargeAttention, FlexPrefill, Twilight, etc. In other words, the proposed main method is exactly the same as other works, and can not be considered a contribution.\n\n**Misaligned self-positioning and unfair comparisons.** Although NABLA is a dynamic sparse method, all comparisons are made against static methods such as STA. Moreover, the mix of NABLA and STA, makes the results look better but unfair. It is hard to know how well NABLA works on its own. Proper baselines such as SparseVideoGen, SparseVideoGen2, SpargeAttention, and RadialAttention should be included.\n\n**Unbalanced narrative and poor writing.** Section 2 (Background) spends excessive space reiterating standard attention equations (Eqs. 13) and describing STA in full detail. The writing and formatting of the paper should be improved. The presentation is sometimes unclear, and the structure and layout make it difficult to follow the main ideas.\n\n**The experiments are not well designed.** They should report both end-to-end quality metrics and efficiency metrics together, rather than evaluating them separately. The baseline only includes one component of the proposed method, namely STA, and lacks comparisons with other relevant baselines.\n\n**Missing empirical analysis.** The key hyperparameter $thr$ is not analyzed, and its chosen values (0.4 and 0.2) lack rationale. Table 3 shows that NABLA with threshold 0.4 already performs as well as or even better than full attention. This result disagrees with the paper’s claim that NABLA alone produces visible artifacts. The statements about border artifacts are only based on examples and are not supported by measurements."}, "questions": {"value": "Why use CDF thresholding instead of Top-K or ﬁxed-threshold binarization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RSBykLmRxr", "forum": "SJ2WK6kdPJ", "replyto": "SJ2WK6kdPJ", "signatures": ["ICLR.cc/2026/Conference/Submission16730/Reviewer_mKfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16730/Reviewer_mKfN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861561172, "cdate": 1761861561172, "tmdate": 1762926781883, "mdate": 1762926781883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NABLA, a Neighborhood-Adaptive Block-Level Attention mechanism designed to improve the efficiency of video diffusion transformers. By constructing a sparse attention mask through block-wise pooling and adaptive thresholding, NABLA reduces the cost of full self-attention. The method integrates seamlessly into PyTorch’s FlexAttention and achieves substantial training and inference acceleration (up to 2.7×) while maintaining comparable quality across several benchmarks, including CLIP, VBench, and FVD scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work addresses an important problem in the video generation field—scaling self-attention efficiently. Given the rising cost of video diffusion models, efficiency-focused contributions are timely and valuable.\n- The paper is well organized and easy to follow."}, "weaknesses": {"value": "- The core idea of using pooling-based approximations and block-sparse attention is not new. Similar strategies have been explored in both large language model acceleration methods such as MInference and video diffusion models such as SparseVideoGen. The conceptual overlap reduces the originality of the contribution.\n- The paper lacks comparisons with other block-sparse or spatially adaptive attention methods for video generation, such as SpargeAttention, SparseVideoGen, PowerAttention, RadialAttention, and XAttention, which are necessary to evaluate NABLA’s relative performance.\n- The evaluation is limited to a single model (Wan 2.1-14B) and a small set of metrics. Broader experiments—including other video generation models (e.g., Hunyuan Video) and additional evaluation metrics like VisionReward—would provide stronger evidence of generalization and robustness."}, "questions": {"value": "1. How does NABLA fundamentally differ from existing block-sparse attention mechanisms (e.g., MInference) beyond its adaptation to video data?\n\n2. Can the authors provide quantitative or qualitative comparisons against other block sparse attention to better contextualize NABLA’s efficiency and quality trade-offs?\n\n3. Have the authors tested NABLA on other video diffusion architectures, such as Hunyuan and CogvideoX, to verify model-agnostic performance improvements?\n\n4. Could the authors include additional perceptual metrics (e.g., VisionReward, PickScore) or generated video samples to assess generation quality more comprehensively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nse3Ub35a6", "forum": "SJ2WK6kdPJ", "replyto": "SJ2WK6kdPJ", "signatures": ["ICLR.cc/2026/Conference/Submission16730/Reviewer_m2hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16730/Reviewer_m2hF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931480729, "cdate": 1761931480729, "tmdate": 1762926781373, "mdate": 1762926781373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dynamic sparse-attention mechanism for video DiTs. NABLA computes a low-resolution attention map and uses it to derive a dynamic attention mask. Because this alone can introduce visible seams at block boundaries, the method is combined with Sliding Tile Attention (STA). Using the union of STA and NABLA as the attention mask, the authors claim to accelerate both inference and training while preserving output quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Despite its simplicity and the fact it can be implemented with FlexAttention, the method shows promising strong practical gains and is highly effective.\n\n- It can be introduced via fine-tuning into models originally trained with full attention, which is very convenient and increases its applicability.\n\n- The experiments cover the key axes of speed and quality to a reasonable extent."}, "weaknesses": {"value": "- In Table 2, the runtime comparison between the Baseline (full attention) and the STA/NABLA variants appears to evaluate all settings with FlexAttention. However, the Baseline could (and in practice often would) leverage FlashAttention. Using FlashAttention for the Baseline would be a more realistic and informative comparison.\n\n- Quantitative comparisons against other dynamic sparse-attention methods (e.g., AdaSpa, Sparse-VideoGen, SpargeAttention) are limited, making it hard to highlight NABLA’s relative advantages."}, "questions": {"value": "- The explanation of Figure 2 feels somewhat brief. Some questions remain about the specific patterns shown and how they are derived/interpreted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2t6PA6FFS6", "forum": "SJ2WK6kdPJ", "replyto": "SJ2WK6kdPJ", "signatures": ["ICLR.cc/2026/Conference/Submission16730/Reviewer_svGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16730/Reviewer_svGJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972372550, "cdate": 1761972372550, "tmdate": 1762926780852, "mdate": 1762926780852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}