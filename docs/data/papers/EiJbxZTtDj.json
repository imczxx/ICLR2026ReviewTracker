{"id": "EiJbxZTtDj", "number": 13581, "cdate": 1758219405212, "mdate": 1759897427047, "content": {"title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "abstract": "With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM \"assistants'' specialized to particular types of queries. However, some tasks (i.e., queries) may benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires selecting a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.", "tldr": "", "keywords": ["neural bandits", "online LLM selection"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7805798c21e75a860355029129e42e5f1f9df4e.pdf", "supplementary_material": "/attachment/3af728b5150736d43afd6b98934f70b7725995b7.zip"}, "replies": [{"content": {"summary": {"value": "With the increasing use of large language models (LLMs) for a wide range of tasks, there is growing interest in strategies that can efficiently predict which LLMs will succeed in completing a task, especially in a cost-effective manner. This is becoming even more relevant as platforms like Microsoft and OpenAI offer users the ability to create custom LLM assistants, specialized for specific types of queries. However, some tasks might benefit from breaking them down into smaller subtasks, each of which can be performed by an LLM tailored to handle that specific subtask.\n\nFor example, in medical diagnosis extraction, the process could involve selecting one LLM to summarize the medical record, another to validate the summary, and a third to extract the diagnosis from the validated summary. Unlike traditional LLM selection algorithms, this process requires selecting a sequence of LLMs, where each LLM’s output feeds into the next, creating a chain of dependencies that influence the cost and success rate of the task.\n\nThe challenge lies in selecting the right sequence of LLMs, where the quality of each subtask’s output directly affects the next LLM's performance. To solve this, the authors propose a neural contextual bandit-based algorithm that learns to model LLM success on each subtask in an online manner, guiding the selection of LLMs without the need for historical performance data. Experiments on datasets related to telecommunications question answering and medical diagnosis prediction show that the proposed algorithm outperforms existing LLM selection algorithms in terms of efficiency and success."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of neural contextual bandits is interesting and this allows the model to adapt to new queries and subtasks dynamically, ensuring better decision-making in LLM selection.\n\n2. The paper introduces the idea of breaking down complex tasks into smaller subtasks and selecting specialized LLMs for each one. This approach leverages the strengths of different LLMs for specific tasks, leading to more efficient and effective solutions.\n\n3. The paper is well-written and clear to read. Abundant theoretical proofs and visualizations are provided to enhance the readability."}, "weaknesses": {"value": "1. it may introduce challenges in terms of training the model efficiently, especially when there is no historical data available. This could make the training process more computationally intensive, particularly for large-scale systems.\n\n2. The idea in the paper is not very novel, as a similar concept is presented in the paper \"Cost-efficient knowledge-based question answering with large language models,\" which also addresses the selection of models in a cost-efficient manner.\n\n3. The need to select a sequence of LLMs for each subtask introduces a layer of decision-making that might introduce delays, especially in time-sensitive applications.\n\n4. It would be helpful to see more generalizable results across a wider range of tasks to assess the broader applicability of the method."}, "questions": {"value": "The model could benefit from more efficient training methods or pre-training strategies that reduce the computational cost of learning to select LLMs for new subtasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6s2svI6B6P", "forum": "EiJbxZTtDj", "replyto": "EiJbxZTtDj", "signatures": ["ICLR.cc/2026/Conference/Submission13581/Reviewer_y6C9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13581/Reviewer_y6C9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960864437, "cdate": 1760960864437, "tmdate": 1762924174582, "mdate": 1762924174582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses **cost-effective LLM selection for tasks decomposed into sequential pipelines**, where different LLMs handle each subtask and outputs feed forward. This creates complex interdependencies: earlier LLM choices affect input quality and optimal selections for downstream subtasks.\n\nThe authors propose **Sequential Bandits**, a neural contextual bandit algorithm with two key innovations: (1) maintaining **separate neural networks for each (subtask, LLM) pair** to model rewards, and (2) **sequentially** selecting LLMs after observing previous outputs rather than committing to all choices simultaneously. The UCB-based selection strategy balances exploration-exploitation while incorporating costs via tunable parameters $\\alpha_i$.\n\n**Main contributions** include: (1) novel problem formulation for sequential LLM pipeline selection, (2) the Sequential Bandits algorithm grounded in neural contextual bandits, (3) a new medical diagnosis dataset from MIMIC-III, and (4) experiments showing 7.6% and 6.5% improvements over baselines on medical and telecommunications tasks. The work assumes task decomposition is pre-defined rather than learned."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:** The paper identifies a new problem that sits between existing LLM routing and cascading work. The key insight—that LLM choices for earlier subtasks affect optimal selections downstream—is well-motivated. \n\n**Quality:** The experimental methodology demonstrates careful design in several respects. The baselines are thoughtfully constructed to isolate specific contributions: Random shows learning value and fixed Llama/Tele shows adaptation value.\n\n**Clarity:** The paper is well-written with clear motivation and straightforward presentation. Figure 2 effectively illustrates the problem, and Algorithm 1 is easy to follow.\n\n**Significance:** The problem is timely given the rise of agentic AI and custom LLM assistants. The framework is general and shows meaningful practical improvements."}, "weaknesses": {"value": "**Limited Baseline Comparisons:** The most significant experimental weakness is the absence of comparisons to recent LLM routing and cascading methods extensively cited in related work, such as FrugalGPT, Hybrid LLM, and many other routing/cascading frameworks. While many routing methods require historical training data—and learning without such data is indeed a key motivation for the online approach—several evaluation strategies could address this gap: (1) provide a warm-start period where methods collect initial training data before comparison, or (2) evaluate after the online learning phase to compare converged performance. More fundamentally, demonstrating when and by how much online learning outperforms offline-trained routers (in terms of sample efficiency, final performance, or adaptability) would significantly strengthen the contribution. Without these comparisons, readers cannot assess whether the observed improvements come from genuinely superior algorithmic design.\n\n**Task Decomposition Assumption:** The paper assumes task decomposition is given, which is a major practical limitation explicitly acknowledged but not addressed. In real deployments, determining *how* to break down tasks is often harder than selecting models. The medical example (summarize → diagnose) seems intuitive, but for arbitrary complex queries, the optimal decomposition is non-obvious and likely query-dependent. The paper provides no guidance on when pipelines help versus hurt, or how to design good decompositions.\n\n**Lack of Theoretical Analysis:** While the paper adapts NeuralUCB's framework, it provides no formal regret bounds for the sequential setting. The theoretical contribution is essentially \"use existing neural bandit algorithms but with separate networks per arm.\" It's unclear whether standard NeuralUCB regret guarantees transfer to this setting where: (1) contexts change dynamically based on previous LLM outputs, (2) subtask rewards are interdependent, and (3) the super-arm reward is a complex function of base rewards. The exploration-exploitation trade-off in sequential decisions differs fundamentally from standard bandits. \n\n**Limited Experimental Scale and Scope:** The evaluation uses only 100 medical reports and 100 rounds of online learning, which is quite modest. More critically, there's no analysis of: (1) how performance scales with pipeline depth (only 2-3 subtasks tested), (2) sensitivity to the cost parameter α (no ablation shown), (3) what happens with more LLMs per subtask (experiments use 5-7 total models), or (4) performance in non-stationary settings where LLM capabilities or costs change. The token length prediction model's accuracy isn't reported—if predictions are poor, the cost-aware objective could be misleading. \n\n**Cost Model Limitations:** The cost model assumes costs are proportional to tokens, but ignores latency, which can be a critical \"cost\" in interactive applications. The token predictor is trained on LMSYS-Chat-1M, which may not generalize well to specialized domains (medical, telecom). The fixed token predictor never adapts during online learning, potentially leading to systematic errors."}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4VZVDflGQ", "forum": "EiJbxZTtDj", "replyto": "EiJbxZTtDj", "signatures": ["ICLR.cc/2026/Conference/Submission13581/Reviewer_CrjG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13581/Reviewer_CrjG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966283572, "cdate": 1761966283572, "tmdate": 1762924174237, "mdate": 1762924174237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Sequential Bandits, a contextual MAB algorithm variant adapted to selecting a pipeline of LLMs for each subtasks in task decomposing paradigm. Specifically, Sequential Bandits is designed for the scenario of complicated tasks that cannot be well-solved in single step by one LLM alone and rely on selecting suitable LLMs to perform decomposed subtasks in order to achieve the optimal results. Different from traditional neural MAB methods, Sequential Bandits employs separate reward neural networks for each LLM combinations to enrich the exploration while keeping the training overhead. Experiments on MIMIC-III and TeleQnA datasets exhibit Sequential Bandits has a better net reward and a lower economic cost compared to other adapted traditional MAB methods and random baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper first formulate selecting LLM pipeline into an instance of the classical contextual MAB problem.\n\nS2: The method design and experiment considered cost-effectiveness during LLM selection pipeline.\n\nS3: Comparison with naively adapted traditional MAB baselines is conducted to show the effectiveness of separate neural network design in Sequential Bandits."}, "weaknesses": {"value": "I'm not expert in online LLM selection technique and MAB algorithm, thus I can only provide some literal and experiment suggestions for the paper. I hope the other knowledgeable reviewers can provide more technical feedback.\n\nW1: Fig 2 could provide more elaborated or distinguished details for Sequential Bandits to make it friendly to the audiences from other fields.\n\nW2: As provided in related works, it seems there exist other recent studies for LLM selection pipeline, while the experiment only employed adapted traditional MAB baselines.\n\nW3: Although illustrate reward and cost for each baseline, a more detailed results on task accuracy, success rate, etc. are not provided to give a more clear review of the method effectiveness or usability."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Awfb2yJ1EW", "forum": "EiJbxZTtDj", "replyto": "EiJbxZTtDj", "signatures": ["ICLR.cc/2026/Conference/Submission13581/Reviewer_23oe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13581/Reviewer_23oe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987777812, "cdate": 1761987777812, "tmdate": 1762924173870, "mdate": 1762924173870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach called \"Sequential Bandits,\" which adapts the contextual multi-armed bandit (MAB) algorithm to select the best large language models (LLMs) for sequential, multi-step tasks. The main idea is to model the task decomposition as a pipeline of subtasks, each tackled by a different LLM. The algorithm aims to maximize the accuracy of task completion while also minimizing costs, providing a framework for intelligent LLM selection in complex tasks that cannot be handled by a single model. The proposed method is evaluated on medical diagnosis prediction and telecommunications question answering tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces the \"Sequential Bandits\" algorithm, which effectively addresses the problem of selecting the best LLMs for multi-step tasks, providing a solution that is more practical than existing methods.Experiments show that the proposed method outperforms baseline algorithms in medical diagnosis and telecommunications question answering tasks, demonstrating its ability to balance accuracy and cost."}, "weaknesses": {"value": "The details of the cost prediction model, particularly in terms of output token length prediction and overall cost estimation, are not fully explained. The paper compares the method to a few baselines but does not offer a comprehensive comparison with other state-of-the-art LLM selection algorithms.The approach uses separate neural networks for each subtask, which could lead to scalability challenges for larger systems."}, "questions": {"value": "The details of the cost prediction model, particularly in terms of output token length prediction and overall cost estimation, are not fully explained. The paper compares the method to a few baselines but does not offer a comprehensive comparison with other state-of-the-art LLM selection algorithms.The approach uses separate neural networks for each subtask, which could lead to scalability challenges for larger systems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ERbaiKcByR", "forum": "EiJbxZTtDj", "replyto": "EiJbxZTtDj", "signatures": ["ICLR.cc/2026/Conference/Submission13581/Reviewer_4YAL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13581/Reviewer_4YAL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999121794, "cdate": 1761999121794, "tmdate": 1762924173588, "mdate": 1762924173588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}