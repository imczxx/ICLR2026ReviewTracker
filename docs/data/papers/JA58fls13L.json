{"id": "JA58fls13L", "number": 19661, "cdate": 1758298063749, "mdate": 1759897027514, "content": {"title": "Preserving Ignorance Awareness in LLM Fine-Tuning", "abstract": "Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model’s ability to faithfully express epistemic uncertainty (a property we term \"Ignorance Awareness\"). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning.", "tldr": "", "keywords": ["LLM Fine-tuning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9847d582a4cdef1ea5631afcfaae733e917d7f5f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out that fine-tuning can make LLMs lose their ability to say “I don’t know.”\nIt shows that normal fine-tuning shifts activations and blurs the line between what the model knows and doesn’t know.\nTo fix this, the authors propose SEAT, a sparse and entity-aware tuning method that keeps knowledge boundaries clear and better preserves uncertainty while keeping performance strong."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel problem formulation: The notion of Ignorance Awareness and its degradation through fine-tuning is both original and safety-relevant.\n\nPrincipled theoretical grounding: Provides formal definitions, propositions, and Lipschitz-bounded theorems linking sparsity to activation stability.\n\nStrong empirical validation: Convincing results across datasets and models (Llama3-8B, Qwen2.5-7B) with ablations clearly isolating each SEAT component."}, "weaknesses": {"value": "The approach’s scalability and implementation practicality for multi-round continual learning are not evaluated.\n\nThe work focuses on knowledge acquisition scenarios; the broader effects on refusal calibration or harmful prompt alignment remain unexplored."}, "questions": {"value": "Can SEAT be extended to refusal calibration or safety alignment tasks, where the goal is to preserve ethical uncertainty rather than factual ignorance, as mentioned in the main paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gtmtfGb0Kz", "forum": "JA58fls13L", "replyto": "JA58fls13L", "signatures": ["ICLR.cc/2026/Conference/Submission19661/Reviewer_ZnQP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19661/Reviewer_ZnQP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589538765, "cdate": 1761589538765, "tmdate": 1762931512671, "mdate": 1762931512671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Fine-tuning an already aligned language model deteriorates its tendency to reply “I don’t know” to a question it does not know the answer to. This paper proposes SEAT, a method that updates a randomly masked subset of model parameters at each gradient step, and minimizes the KL divergence between the original and fine-tuned model on perturbed variants of instances with entity names replaced. Experiments on a GPT-4o-generated dataset of real-world events, and on two synthetic datasets (TOFU and PISTOL) show that SEAT can preserve more of the ability to acknowledge not knowing than conventional fine-tuning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Language models should indeed acknowledge ignorance when applicable; this is an important problem to study.\n2. Isolating an ‘ignorance’ direction in model activations might inform analyses of its behavior."}, "weaknesses": {"value": "I will group my critiques into three sections.\n1. **Setting and problem**.\n \n    a. The proposed issue is a non-problem. One is typically worried about continual training of base (pretrained) and not instruction-tuned models, since we can always perform light alignment/RL/… at the end. If it is claimed that base models can acknowledge lack of knowledge, then one should work with base models (Llama-3.1-8B or Qwen-2.5-7B, say). The proposed issue only exists—as far as the paper shows—with instruction-tuned versions (Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct).\n\n    b. Therefore the connection with “continual learning” category, as the related work section suggests, is quite tenuous. Even if it were in this category, several references are missing—for example, the original definitions of catastrophic interference [1,2], or even the original replay methods [3].\n\n2. **Method**\n\n    a. I don’t think the theoretical results amount to a meaningful result. Equations 3-5 basically (1) assume that the loss is l-Lipschitz (ok) (2) bound the difference in activation in terms of the supremum of the gradient norm (an incredibly loose upper bound that may not even exist—it only exists here because they assume that the parameters cannot grow apart more than a fixed amount) (3) claim that this upper bound will be lower if less parameters are updated since fewer of them will now change. This result does not mean much—I might as well say that no difference will be observed if I do not update any parameters; the set of equations is meaningless unless optimized and solved under some constraint of performance preservation.\n\n    b. It is a big assumption to assume that the data is neatly packaged in terms of (subject, relation, object) tuples and also that one can find other, reasonable, replacements object’ to form perturbations. Most natural data is not packaged in a neat format as this, and finding a suitable replacement may not always be feasible; and of course, there might be a massive computational cost to rewriting the data in this format based on, e.g., an LLM.\nTherefore, the method proposed here is not practical.\n\n3. **Experiments**\n\n    a. No baselines are compared to. In fact, even early on, the paper mentions that LoRA is insufficient, so at least LoRA must be included as a baseline. Additional baselines might include continual learning methods like replay techniques.\n\n    b. Other sparse training methods should also be compared to, including, e.g., model editing methods.\n\n[1] Michael McCloskey and Neal J. Cohen. “Catastrophic interference in connectionist networks: The sequential learning problem”. Psychology of Learning and Motivation - Advances in Research and Theory, 24:109–165, 1989.\n\n[2] R. Ratcliff. “Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions”. Psychological Review, 97(2):285–308, 1990.\n\n[3] Anthony Robins. “Catastrophic forgetting, rehearsal and pseudorehearsal”. Connection Science, 7(2):123–146, 1995."}, "questions": {"value": "1. Did you try your experiments with base (non-instruct) language models?\n\n2. How does LoRA perform on the three benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NSiutO5Z18", "forum": "JA58fls13L", "replyto": "JA58fls13L", "signatures": ["ICLR.cc/2026/Conference/Submission19661/Reviewer_8Sdd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19661/Reviewer_8Sdd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765936529, "cdate": 1761765936529, "tmdate": 1762931512167, "mdate": 1762931512167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of preserving Ignorance Awareness (IA), a model’s ability to recognize when it does not know, during fine-tuning. The authors argue that standard fine-tuning disrupts epistemic boundaries, causing models to produce overconfident or hallucinatory answers. They propose Sparse Entity-aware Tuning (SEAT), combining sparse parameter updates and entity perturbation to maintain IA while retaining task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an underexplored yet important aspect of model alignment (epistemic humility) and proposes a clear conceptual framework to formalize it.\n\n2. The theoretical analysis provides a general justification for stability under sparse updates, formalizing how smaller parameter movements constrain activation drift.\n\n3. Experiments demonstrate consistent improvements on IA-related metrics, showing practical relevance."}, "weaknesses": {"value": "1. The framework implicitly assumes that large pretrained models already possess a “refusal-to-answer”. Demonstrating that the pretrained model indeed encodes this capability requires more direct evidence. Of course, the paper demonstrates that the pretrained model possesses a refusal capability, but only on synthetic data. It would strengthen the work if the authors could also show evidence that such capability exists when dealing with real-world entities or natural data.\n\n2. The experimental setup assumes a difficult fine-tuning scenario where only task-specific data are available. While this setting is realistic, it also restricts generality. \n\n3. The theoretical proofs are mathematically consistent but very general. They describe overall knowledge preservation through limited parameter movement, not IA-specific stability. The masking operation may preserve all representations, including those that should adapt, potentially impeding learning. It would help to explicitly connect the mathematical derivation and method to IA-specific features or scores.\n\n4. The empirical section lacks strong baselines. Random masking, although theoretically justified (Corollary 1-2), contributes little beyond a regularization effect. More meaningful comparisons with alternative stability-preserving methods (e.g., LoRA variants, EWC, or alignment-oriented fine-tuning such as DPO or ORPO) are necessary to demonstrate the distinctiveness of SEAT."}, "questions": {"value": "See Weekneses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nhzzVJCWAz", "forum": "JA58fls13L", "replyto": "JA58fls13L", "signatures": ["ICLR.cc/2026/Conference/Submission19661/Reviewer_ytXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19661/Reviewer_ytXg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817507564, "cdate": 1761817507564, "tmdate": 1762931511541, "mdate": 1762931511541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SEAT, a LLM fine-tuning method designed to preserve the model’s *Ignorance Awareness*, its ability to recognize and express uncertainty about unknown information, while learning new knowledge. The authors argue that conventional fine-tuning often causes activation displacement, which erodes this property and leads to hallucinations. SEAT addresses this issue with two key components: (1) *sparse tuning*, which limits activation drift during fine-tuning, and (2) *entity perturbation*, which mitigates entanglement by preventing inadvertent generalization to neighboring data points. Experiments on both real and synthetic datasets show that SEAT maintains ignorance awareness effectively, without sacrificing the ability to learn new information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- [S1] **Timely and relevant topic.** Preserving uncertainty calibration and avoiding hallucinations with a focus on ignorance awareness during fine-tuning is an important challenge for LLM safety and reliability.\n- [S2] **Combination of practical and theoretical perspectives.** The inclusion of both theoretical analysis (Section 3) and empirical experiments is a positive aspect, showing the authors’ effort to ground their approach in both intuition and formal reasoning."}, "weaknesses": {"value": "- [W1] **Limited novelty in both problem formulation and methodology.** The idea of maintaining ignorance awareness during fine-tuning appears to overlap heavily with existing research in continual learning (as suggested in the Related Works section). Responding with “I don’t know” can be viewed simply as learning another data distribution, so it is unclear if this warrants a distinct formulation. As a result, the proposed techniques, sparse tuning and entity perturbation, appear to be adaptations of existing methods: the former has been explored in continual learning (e.g., [A]) and the latter parallels adversarial perturbation and decision-boundary preservation methods from machine unlearning (e.g., [B]). Therefore, the methodological contribution feels incremental.\n- [W2] **Inadequate experimental comparisons.** The experimental section claims that directly comparable baselines are unavailable, yet it seems existing continual fine-tuning methods can be tested seamlessly within the framework. Comparing SEAT only to Full FT and Sparse FT baselines does not convincingly demonstrate its advantages.\n- [W3] **Weak and unclear theoretical development.** Theoretical results in Section 3 (Theorems 1–3) are difficult to follow and contribute limited new insight. The early theorems state rather trivial results about sparse updates, while the main theorem (Theorem 3) relies on an assumption of Lipschitz continuity for the score functional, a property that does not hold for self-attention networks [C]. Moreover, the argument appears to depend on an empirical observation from unrelated unlearning work [D], weakening its rigor. The section would benefit from a clearer statement of assumptions and a more intuitive discussion of implications.\n\n[A] Continual Learning with Node-Importance based Adaptive Group Sparse Regularization. NeurIPS 2020.\\\n[B] Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers. AAAI 2024.\\\n[C] The Lipschitz Constant of Self-Attention. ICML 2021.\\\n[D] LLM Unlearning via Neural Activation Redirection. NeurIPS 2025."}, "questions": {"value": "- [Q1] In the ablation studies, it appears that the configuration “Full FT + KL w/o EP” (i.e., without both sparse tuning and entity perturbation) is missing. Would including this help clarify the relative contribution of each component?\n- [Q2] Citation formatting seems inconsistent (mixing \\cite{} and \\citep{}). Please ensure consistent use of citation style throughout the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaYsiOWk5q", "forum": "JA58fls13L", "replyto": "JA58fls13L", "signatures": ["ICLR.cc/2026/Conference/Submission19661/Reviewer_WJrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19661/Reviewer_WJrx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995852467, "cdate": 1761995852467, "tmdate": 1762931511104, "mdate": 1762931511104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}