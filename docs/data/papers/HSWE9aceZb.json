{"id": "HSWE9aceZb", "number": 16376, "cdate": 1758263924549, "mdate": 1759897244641, "content": {"title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios", "abstract": "Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency.\n\nTo bridge this gap, we introduce CXMdataset, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasetsâ€”such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation.\n\nBuilding on this, we release CXMdataset, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools.\n\nOur baseline experiments underscore the benchmark's difficulty: even state-of-the-art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.", "tldr": "We introduce a large-scale synthetic benchmark dataset to evaluate AI performance on critical operational tasks in realistic Customer Experience Management (CXM) scenarios.", "keywords": ["Benchmark Dataset", "Customer Experience Management", "CXM", "Large Language Models", "Synthetic Data Generation", "Contact Center AI", "Retrieval Augmented Generation", "RAG", "Intent Prediction", "LLM", "CXMArena"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b60e2e527e0c164616b568d3be01ca6bbbd8d143.pdf", "supplementary_material": "/attachment/c9e16a0c3802eb2cb465ab3c08cf2a061d11b36b.zip"}, "replies": [{"content": {"summary": {"value": "The authors developed a new way to automatically generate a complete, brand-specific Knowledge Base (KB) for Customer Experience Management (CXM) problems. They have publicly released the entire dataset, which includes articles, synthesized user queries, and tool definitions.\n\nThe paper argues that existing benchmarks fall short for CXM because they focus on general, open-domain knowledge and generic tools. This work tackles that gap by creating a benchmark that includes a full suite of core tasks relevant to CXM, which requires specialized brand/product knowledge and the use of tailored tools, just like in real-world customer support scenarios.\n\nUsing their synthetic KB, the authors designed and benchmarked a suite of tasks to evaluate different LLM capabilities. This includes an Intent Prediction task for classification of user needs, an Article Retrieval task for retrieval of relevant documents, a Tool Use task for executing API calls, and a Quality Adherence task to evaluate if responses follow brand guidelines. The benchmark also includes an end-to-end Multi-turn RAG with Tool Use task, which is a generation problem that integrates all these skills to produce a final, context-aware response."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a critical gap in existing benchmarks, which typically focus on open-domain tasks and fail to address the unique challenges of Customer Experience Management (CXM), such as specialized knowledge retrieval, tailored tool use, and quality adherence.\n\nA key contribution is the novel data synthesis pipeline that generates a complete and realistic brand-specific Knowledge Base. Synthesizing such data is valuable & practical as it enables the creation of a rich CXM environment for research without exposing sensitive company data or compromising user privacy.\n\nThe work provides a comprehensive benchmark by designing and evaluating performance on a suite of diverse tasks relevant to CXM, including retrieval, tool use, and end-to-end multi-turn RAG, thereby establishing important baselines for future research."}, "weaknesses": {"value": "The benchmark's primary weakness is a lack of a clear rationale for performing several isolated, synthesized tasks on a synthesized knowledge base. For example, both the article retrieval and tool-calling tasks are designed in isolation. While this is a valid approach, the paper does not explain the unique advantages of this synthetic setup over existing approaches that synthesize search queries, or tool calls from openly available articles and real-world tools (e.g. ToolBench).  A stronger justification would help frame the benchmark as a cohesive suite of tasks designed to address novel CXM challenges, rather than a collection of disconnected components. (e.g. any unique challenge around jointly performing KB retrieval, tool call during a multi-turn RAG problem?)\n\nWhile the paper does involve an evaluation on an end-to-end RAG task, a key methodological detail that remains unclear is how tool outputs are simulated during the multi-turn RAG evaluation, or if tool calls are simulated at all. In addition, while the paper mentions llm-as-a-judge is used to evaluate the final multi-turn RAG output, it is not clear if the evaluation is grounded in the golden response or the correct sources."}, "questions": {"value": "In the multi-turn RAG evaluation, how are tool outputs simulated and provided to the model after a tool call is made?\n\nFor the LLM-as-a-judge evaluation, is the judge grounded against the knowledge base or golden responses to verify factual accuracy?\n\nIt seems that KB_3211 is referenced in the multi-turn set but is missing from the articles subset. Could you clarify this data discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RvTPVFtpRK", "forum": "HSWE9aceZb", "replyto": "HSWE9aceZb", "signatures": ["ICLR.cc/2026/Conference/Submission16376/Reviewer_nXhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16376/Reviewer_nXhr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760836767310, "cdate": 1760836767310, "tmdate": 1762926501716, "mdate": 1762926501716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CXMArena: a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational Customer Experience Management contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Real-world distribution because of controlled noise injection (simulated ASR errors, interaction fragments) from SMEs and rigorous automated validation.\n\n- Authors introduce five tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools.\n\n- Pipeline applied to different domains and languages.\n\n- The authors introduce a pipeline to synthetically generate the knowledge base specific to a fictional brand and then uses these KBs to generate realistic conversations along with noise."}, "weaknesses": {"value": "- My main concern is on the correctness of the synthetic data using an LLM (Gemini in this case) and the LLM as a judge evaluation without a human in the loop. \n\n- Contradiction detection baseline would be very insightful since this is one of the tasks needed for Knowledge Base refinement.\n\n- Do you generate all this dataset synthetically given a seed prompt about the brand name and its type? I'm not convinced how you can ensure highly fidelity data since there is no human in the loop. How do you avoid hallucinations? For instance for the KB refinement task how do you know that similar KBs are correctly labelled? \n\n- GPT 4o as a judge and no human evaluation.\n\n- LLM was used for checking the dataset correctness with no human expert in the loop."}, "questions": {"value": "In the abstract you mention \"The entities closely represent real-world distribution because of controlled\nnoise injection (informed by domain experts)\". What kind of information do the experts provide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IwK4UbUeub", "forum": "HSWE9aceZb", "replyto": "HSWE9aceZb", "signatures": ["ICLR.cc/2026/Conference/Submission16376/Reviewer_wsXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16376/Reviewer_wsXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761077753185, "cdate": 1761077753185, "tmdate": 1762926500891, "mdate": 1762926500891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new dataset for evaluation of LLM response in Customer Service. They tackle important problems like Knowledge base based questions, tool calling, multi-turn question answers etc. They provide limitations of current datasets and propose a system which can address these limitations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors bring forth a very important problem and one which practitioners constantly face. One very important aspect of public datasets and papers is that they very rarely to industry situations and the authors point it out very well. They also do a comprehensive study of the different challenges situations in Customer Service via LLMs in Section 2 and bring about limitations of exisiting datasets very well."}, "weaknesses": {"value": "The authors highlight that most public datasets have limitations. This is a very valid concern and they have given significant citations establishing the limitations of existing datasets in Section 2. What is not clear is how is this alleviated in their work. Looking at one example they mention \"We simulate real-world data quality issues by introducing controlled redundant and contradictory information from one article to another, creating data for developing KB maintenance techniques\" - it is not clear how is this going to solve the very real problems in lines 088-103.  Similarly, their description on Multi-turn RAG in lines 252-257 do not explain how the limitations discussed in Section 2 are addressed. The paper largely glosses over details (some information is present in Appendix but that is not under detailed review) and more importantly even from Appendix it is not clear how the concerns are addressed.\n\nNext the authors provide multiple evaluation results with different models however it is hard to understand if this is good or bad without a benchmark. They provide some examples in the appendix however they do not give examples of how they are different from existing datasets making an evaluation of the marginal improvement challenging."}, "questions": {"value": "Please address the concerns in the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bhd5KybxcC", "forum": "HSWE9aceZb", "replyto": "HSWE9aceZb", "signatures": ["ICLR.cc/2026/Conference/Submission16376/Reviewer_52hR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16376/Reviewer_52hR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495421349, "cdate": 1761495421349, "tmdate": 1762926499714, "mdate": 1762926499714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. CXMArena manuscript introduces a unified, large-scale which is basically synthetic benchmark for evaluating AI models in Customer Experience Management (CXM). The synthetic data benchmark will be used in many real world scenarios\n\n2. The manuscript tells about five operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools which are all pretty well explained and in depth.\n\n3. All the dataset which are created using a scalable and high value LLM-powered pipeline that simulates realistic customer-agent interactions with controlled noise injection for authenticity. The LLMs used are pretty in depth with metrics supported. The synthetic data can be used in very critical in CXM scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive benchmark covering core CXM tasks beyond usual fluency. The CXM tasks usually have real word scenario use cases so the manuscript is useful.\n \n2. Synthetic yet realistic data with strong alignment to real-world metrics. The realistic nature of this data has very strong alignment and large scale value in very in depth metrics.\n\n3. Cross-domain and multilingual support (English, French, German). The cross domain languages are very well used in here.\n\n4. Provides baseline results for multiple models. Multiple models are been used like GPT and others."}, "weaknesses": {"value": "1. Although the manuscript is being used for synthetic, it may miss subtle nuances of real human behavior. Also LLM with human as a judge could be helpful to explore that might strengthen the findings\n2. Dependent on biases and limitations of LLMs used for generation.\n3. Currently focused on one domain with limited real-world diversity. Also multi domain alignment could be helpful"}, "questions": {"value": "1. How will CXMArena handle continuous domain drift in real CXM data? The drift can be measured and how can be used.\n2. Can the synthetic pipeline generalize to non-English, multi-brand, or emotion-rich interactions?\n3. How could MCP be used and how some performance benchmarks on MCP be used"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N7iERIZaRx", "forum": "HSWE9aceZb", "replyto": "HSWE9aceZb", "signatures": ["ICLR.cc/2026/Conference/Submission16376/Reviewer_gNnY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16376/Reviewer_gNnY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036089132, "cdate": 1762036089132, "tmdate": 1762926498816, "mdate": 1762926498816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}