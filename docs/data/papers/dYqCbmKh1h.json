{"id": "dYqCbmKh1h", "number": 12025, "cdate": 1758205251107, "mdate": 1763350342757, "content": {"title": "Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration", "abstract": "In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row dominating diagonal (SNRDD) assumption and a condition motivated by the convergence of AVI. The SNRDD assumption also ensures the convergence of linear Q-learning, and its relationship with the convergence of AVI is examined. Lastly, several interesting observations on the solution of PBE are provided when using $\\epsilon$-greedy policy.", "tldr": "", "keywords": ["reinforcement learning", "q-learning", "projected bellman equation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76a0c0d6b2c8b1c12efb575979ec7158ac82b1e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the projected Bellman equation (PBE) and two algorithms used to solve it: linear Q-learning and approximate value iteration (AVI). \n\nIt proposes (i) an SNRDD (strictly negatively row-dominating diagonal) condition as a sufficient condition for existence/uniqueness of PBE solutions and convergence of linear Q-learning, (ii) a second sufficient condition motivated by AVI (two infinity-norm contraction conditions), and (iii) examples highlighting pathological behaviors under $\\epsilon$-greedy policies. \n\nThe contributions are summarized in the introduction (existence/uniqueness under SNRDD; a second AVI-motivated condition; convergence proofs; examples where AVI converges but linear Q-learning does not, and vice-versa; $\\epsilon$-greedy pathologies)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear examples illustrating ε-greedy pathologies.**\n\nSection 6 presents counterexamples showing how changing ε can lead to the existence of no, one, or multiple PBE solutions. This illustrates that discontinuous policies can undermine fixed-point guarantees even under SNRDD conditions.\n\n2. **Comparative visualization of algorithm behavior.**\n\nThe examples and figures demonstrate cases where AVI converges but linear Q-learning diverges, and vice versa. These visual results help highlight subtle differences between projection-based and iterative schemes.\n\n3. **Technically correct foundational results.**\n\nThe use of fixed-point theorems and local Lipschitz arguments in proving existence (Theorem 3.2) is mathematically sound."}, "weaknesses": {"value": "1. **Lack of practical motivation for SNRDD and AVI conditions.**\n\n   The paper repeatedly states that the theoretical properties of PBE, linear Q-learning, and AVI are “not well understood” (line 45) but does not clarify *why* such understanding is important. It remains unclear what practical benefit arises from identifying the SNRDD or AVI contraction conditions. Do they guide algorithm design, help diagnose convergence failure, or provide insight into stability under function approximation? Without this link, the results feel detached from practical reinforcement learning. For instance, while SNRDD guarantees uniqueness of the PBE solution (Theorem 3.2), the paper never demonstrates how this insight could be used to construct or modify algorithms in practice.\n\n2. **No discussion of necessity or minimality of conditions (4), (6), and (7).**\n\n   The analysis provides only *global sufficient* conditions for convergence, SNRDD in Eq. (4) and the AVI-motivated inequalities in (6)–(7). However, these are imposed uniformly for all $\\theta$ in $\\mathcal{D}$, which may be stronger than necessary. For example, Lemma 3.6 follows almost directly from the definition in (4), suggesting that the assumptions are conservative rather than tight. The paper never investigates whether local conditions near a fixed point would suffice, nor does it analyze the borderline cases when these inequalities fail. This omission leaves unclear what the true boundary of convergence is and whether the results could be sharpened.\n\n3. **Lack of empirical or verifiable interpretation.**\n\n   While the paper introduces two mathematical conditions (SNRDD and AVI contraction), it does not discuss how practitioners could verify them in realistic reinforcement learning settings. For instance, Eq. (4) requires checking a row-wise dominance property of a matrix involving the unknown transition structure and feature representation, which is something infeasible to compute in practice. The paper provides no numerical examples demonstrating whether these conditions approximately hold, nor any heuristic indicators that could help identify when a system might violate them.\n\n4. **Unclear takeaway from ε-greedy pathologies (Section 6).**\n\n   Section 6 presents examples where $\\epsilon$-greedy policies lead to multiple or nonexistent PBE solutions as $\\epsilon$ varies, illustrating discontinuities in the induced operator. While these examples are interesting, the paper does not extract a clear lesson: should $\\epsilon$-greedy be avoided, or can the results motivate a modification (e.g., replacing it with softmax or continuous exploration)? Moreover, the examples are disconnected from the main theory: the paper does not explain whether SNRDD or AVI contraction fails in those cases or whether these examples reveal a fundamental limitation of the proposed conditions.\n\n5. **Weak connection to prior convergence literature and unclear technical novelty.**\n\n   The paper positions itself as deepening the theoretical understanding of PBE, yet many results (e.g., Lemma 3.6 and Proposition 3.13) appear to follow directly from standard definitions or fixed-point arguments. It is unclear what the main technical difficulty is or how it compares to earlier analyses such as Li et al. (2024) (*Operations Research*), Tsitsiklis and Van Roy (1997), or Baird (1995). Those works already explore convergence, bias, and sample complexity of Q-learning with general function approximation. In contrast, this paper’s focus on the *existence* of a fixed point lacks discussion on how such existence results affect convergence guarantees, estimation bias, or sample efficiency. Without this comparison, it is hard to see what new theoretical challenge the paper truly addresses."}, "questions": {"value": "1. Beyond “not well understood,” what practical benefit arises from identifying SNRDD or AVI contraction conditions?\n\n2. Are conditions (4)/(6)/(7) necessary near fixed points, or can weaker conditions suffice?\n\n3. How could practitioners empirically verify these conditions?\n\n4. What actionable takeaway should one draw from ε-greedy pathologies (Section 6)?\n\n5. How do these results extend or differ from prior convergence analyses (e.g., Meyn 2024; Li et al. 2024)? What is the technical difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8ldJPDmgY7", "forum": "dYqCbmKh1h", "replyto": "dYqCbmKh1h", "signatures": ["ICLR.cc/2026/Conference/Submission12025/Reviewer_EKy9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12025/Reviewer_EKy9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500336507, "cdate": 1761500336507, "tmdate": 1762923008056, "mdate": 1762923008056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the projected Bellman equation (PBE) and examines its theoretical connections to linear Q-learning and approximate value iteration (AVI). It establishes sufficient conditions for the existence and uniqueness of PBE solutions—most notably through a strictly negatively row-dominant diagonal (SNRDD) property and an additional condition motivated by AVI. The authors analyze convergence of both AVI and linear Q-learning under these assumptions using contraction mappings and fixed-point arguments. They also present illustrative examples, including cases where convergence fails or yields suboptimal solutions under ε-greedy policies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a fundamental and underexplored theoretical topic: when and why the projected Bellman equation admits a unique solution and how that affects linear Q-learning and AVI. The formal analysis seems to be technically sound, and the examples highlighting pathological convergence behaviors are informative. The paper is well-written and easy to follow in general."}, "weaknesses": {"value": "The existence and uniqueness of PBE solutions under diagonal dominance (SNRDD) are not particularly surprising—such conditions are standard in numerical linear algebra and reinforcement learning theory. Consequently, Section 3, while technically correct, feels incremental and could be condensed substantially. Additionally, some of the technical assumptions, such as requiring relatively large regularization constants for certain guarantees, seem restrictive and analytically convenient rather than broadly insightful. These conditions limit the perceived generality and practical relevance of the results.\n\nFurther, given that MDP and RL is a very classical and well-sturdied field, many related works seems missing, to name a few: Convergence of Q-learning with Linear Function Approximation (Melo, Meyn & Ribeiro, 2008), and An Analysis of Linear Models and Value-Function Approximation (Parr et al., 2008). A lack of comparison and acknowledgement of previous works makes it difficult to evaluate how this paper advances beyond well-established results. It remains unclear which aspects are novel relative to known stochastic approximation and ODE-based analyses. The manuscript would benefit from clearer statements of what is newly proven here versus what is already established in the literature."}, "questions": {"value": "See the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qPkwIoOiEE", "forum": "dYqCbmKh1h", "replyto": "dYqCbmKh1h", "signatures": ["ICLR.cc/2026/Conference/Submission12025/Reviewer_CbLt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12025/Reviewer_CbLt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960389186, "cdate": 1761960389186, "tmdate": 1762923007641, "mdate": 1762923007641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the projected Bellman equation (PBE) under linear function approximation and analyzes two solvers—linear Q-learning and approximate value iteration (AVI). It introduces a sufficient condition based on “strictly negatively row-dominating diagonals” (SNRDD) to guarantee existence/uniqueness of PBE solutions, connects that condition to AVI-style contraction conditions, and gives convergence proofs for several Q-learning variants via contraction/ODE arguments. It also presents examples showing (i) divergence/convergence mismatches between AVI and linear Q-learning and (ii) pathological behaviors under $\\epsilon$-greedy policies (solution multiplicity, non-existence, and emergence of optimal yet unattainable solutions)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n\nClear new sufficient condition (SNRDD) for PBE solvability that applies across tabular, linear, and regularized settings, and accommodates off-policy cases and (locally) Lipschitz policies. This is positioned as broader/different from prior on-policy or tamed-Gibbs results. \n\nUnifying convergence view for Q-learning variants (tabular asynchronous, linear, and regularized) via contraction theory/ODE analysis—reducing auxiliary assumptions (e.g., feature positivity/orthogonality; no target network/projection required for their regularized variant). \nExplicit comparison to AVI-motivated conditions and a formal relationship result (Proposition 3.13) clarifying when the SNRDD-based criterion aligns with an AVI-style norm bound. \n\nInsightful counter-examples: concrete constructions where AVI converges but linear Q-learning oscillates and vice-versa (Figure 2), and where Q-learning converges to a unique but sub-optimal fixed point. These help map the frontier between the two methods."}, "weaknesses": {"value": "Scope restricted to linear function approximation: While the linear regime is foundational, many modern RL systems are nonlinear; the paper stops short of indicating which parts of the analysis might transfer (even qualitatively) to nonlinear function classes. (Authors briefly note this as future work.)"}, "questions": {"value": "Nonlinear approximation outlook: Which pieces of your ODE/contraction proof strategy seem most likely to carry over to nonlinear function classes (e.g., local SNRDD-like Jacobian conditions), and what are the main obstacles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "47fHDj8WiP", "forum": "dYqCbmKh1h", "replyto": "dYqCbmKh1h", "signatures": ["ICLR.cc/2026/Conference/Submission12025/Reviewer_DW32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12025/Reviewer_DW32"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976408429, "cdate": 1761976408429, "tmdate": 1762923007336, "mdate": 1762923007336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the existence and uniqueness of solutions for two projected Bellman-equation-based algorithms: linear Q-learning and AVI. The main conditions used in the paper are SNRDD, a matrix condition for determining the stability of dynamical systems, for linear Q-learning, and a matrix norm condition for AVI. The paper also provides a convergence analysis using ODE-based stochastic approximation (Borkar and Meyn, 2000) for the linear Q-learning algorithm, leveraging the SNRDD condition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present a unifying tool (SNRDD) for analyzing the convergence of tabular, linear, and regularized linear Q-learning.\n- An interesting contrast is provided, with conditions showing when AVI converges while linear Q-learning does not, and vice versa, as well as a condition under which both converge (Proposition 3.13).\n- An extensive appendix with theoretical rigor is provided that helps guide readers through the definitions and results."}, "weaknesses": {"value": "- My main concern is the novelty of the results provided. The paper centers on the SNRDD condition, which was already used by (Lim and Lee, 2024) for a similar purpose in regularized linear Q-learning. Although (Lim and Lee, 2024) required two additional conditions (e.g., orthogonal and non-negative features), as the authors indicated in Remark 4.5, I believe this is a bit incremental.\n- The fixed behavior policy condition, although assumed in related works, is restrictive in my opinion. The authors mention that a replay buffer can be considered a fixed distribution, but the standard use of a replay buffer involves continual updates with recent experience. Therefore, I am not convinced that a replay buffer argument applies here."}, "questions": {"value": "- Why is the fixed-behavior policy $\\beta_\\theta$ denoted by $\\theta$ in Section 2.2?\n- In the e-greedy case, how do the existence of an optimal solution and Q-learning’s ability to converge to it relate to the SNRDD condition?\n- What are the implications of the conditions for the existence and uniqueness of the PBE solution (SNRDD and the AVI condition)? For example, can we make design choices in linear Q-learning based on the SNRDD condition to ensure convergence?\n- (minor) In Definition 3.1 (SNRDD), is there a missing absolute value on the entries $A_{ij}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wpj7z89yUB", "forum": "dYqCbmKh1h", "replyto": "dYqCbmKh1h", "signatures": ["ICLR.cc/2026/Conference/Submission12025/Reviewer_qNwA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12025/Reviewer_qNwA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998381355, "cdate": 1761998381355, "tmdate": 1762923006912, "mdate": 1762923006912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all the reviewers for their constructive feedback, which has greatly helped us improve the quality of the manuscript. All changes in the revised version are highlighted in ${\\color{red}red}$. Below, we summarize the common concerns raised by the reviewers and our corresponding responses:\n\n**Technical Novelty and Our Contribution:** Our contribution lies in three fold: First, offering a new perspective on analyzing linear Q-learning through the lens of contraction theory. In details, we identify the one-sided Lipschitz condition of linear Q-learning and show a condition on regularization coefficient $\\eta$ that does not depend on the knowledge of model parameters. Second, we investigate the convergence relationship with AVI—an open question not previously addressed in the literature. Moreover, our divergence examples provide complementary insights and also represent novel findings. Lastly, we show the hardness of extending the analysis to $\\epsilon$-behavior policy by presenting an example where the SNRDD condition holds, yet no solution—or multiple solutions—may exist. We have clarified this in the contribution part of our revised manuscript.\n\n**Key Practical Insights:** Let us first illustrate how our theoretical understanding can provide help in algorithm design. In particular, our analysis provides explicit guidance on the practical use of regularization. By adding an additional $-\\eta\\theta$ term to PBE or the update of Q-learning to ensure stability--analogous to $\\ell_2$ regularization in supervised learning--we can guarantee stability. As discussed in Lemma 3.6 and Remark 3.7 of the manuscript, with appropriate choice of $\\eta$, the SNRDD condition can be easily met. The condition on the coefficient $\\eta>3$ is sufficient when the feature values scale at the order of $\\frac{1}{\\sqrt{p}}$, where $p$ is the feature dimension. **This condition does not require knowledge of MDP.** This provides additional insight on how using $\\ell_2$-type regularization should be helpful in RL. We have clarified this in Remark 3.7 of the revised manuscript.  \n\n\nNow, let us illustrate how our theoretical findings can indicate where practical improvements may be pursued. Since AVI can be viewed as a primitive form of deep Q-networks (DQN), the fact that we identify regimes in which Q-learning converges while AVI fails highlights meaningful structural differences between the two. This observation encourages further investigation into Q-learning–type updates, consistent with recent efforts to revisit DQN without target networks—essentially reverting to a form of Q-learning [R1, R2].\n\n[R1] Gallici, Matteo, et al., “Simplifying Deep Temporal Difference Learning,” ICLR 2025.\n\n[R2] Vasan, Gautham, et al., “Deep policy gradient methods without batch updates, target networks, or replay buffers,” NeurIPS 2024, pp. 845–891.\n\n\n**Implication of Section 6($\\epsilon$-greedy behavior):** In Example 14.1 of Section 6, which uses $\\epsilon$-greedy behavior policy, we show that existence of solution to PBE is irrelevant with the SNRDD condition. This example shows hardness of extending the analysis to $\\epsilon$-greedy behavior policy. Therefore, it is difficult to prove convergence of Q-learning to a single point under $\\epsilon$-greedy policy. Nonetheless, we believe that under SNRDD condition, the boundedness of the iterate can be proved using the differential inclusion framework by [R3]. We have included the discussion in the Section 6 and conclusion part of the revised manuscript.\n\n[R3] Gopalan, Aditya, and Gugan Thoppe. \"Demystifying Approximate RL with $\\epsilon $-greedy Exploration: A Differential Inclusion View.\""}}, "id": "2LX6124X6o", "forum": "dYqCbmKh1h", "replyto": "dYqCbmKh1h", "signatures": ["ICLR.cc/2026/Conference/Submission12025/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12025/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission12025/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763353753869, "cdate": 1763353753869, "tmdate": 1763353753869, "mdate": 1763353753869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}