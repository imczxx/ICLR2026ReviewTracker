{"id": "I4hx723NT5", "number": 2446, "cdate": 1757089205249, "mdate": 1759898147678, "content": {"title": "FreqCa: Accelerating Diffusion Models via Frequency Decomposition Caching", "abstract": "The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timesteps. \nHowever, previous feature caching assumes that features in adjacent timesteps are similar or continuous, which does not always hold in all settings.\nTo investigate this, this paper begins with an analysis from the frequency domain, which reveal that \\emph{different frequency bands in the features of diffusion models exhibit different dynamics across timesteps.} Concretely, low-frequency components, which decide the structure of images, exhibit higher \\emph{similarity} but poor continuity. In contrast, the high-frequency bands, which decode the details of images, show significant continuity but poor similarity. These interesting observations motivate us to propose  \\textbf{Freq}uency-aware \\textbf{Ca}ching (\\textbf{FreqCa})\n which directly reuses features of low-frequency components based on their similarity, while using a second-order Hermite interpolator to predict the volatile high-frequency ones based on its continuity.\n Besides, we further propose to cache Cumulative Residual Feature (CRF) instead of the features in all the layers, which reduces the memory footprint of feature caching by \\textbf{99\\%}. \n Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit demonstrate its effectiveness in both generation and editing. \\emph{Codes are available in the supplementary materials and will be released on GitHub.}", "tldr": "FreqCa accelerates diffusion models by reusing low-frequency features and predicting high-frequency ones, slashing memory usage by 99% with a novel cumulative caching strategy.", "keywords": ["Diffusion Models", "Image generation", "Video generation", "Model Acceleration", "Feature Cache"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd3cdec5d3e07e1a0311434070eb83708a3a0f2c.pdf", "supplementary_material": "/attachment/6cce21878ef0ce745dfb8afcc19add7a8354aa56.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a frequency-aware caching framework called FreqCa to accelerate the sampling efficiency of diffusion models. FreqCa decomposes cached features into low-frequency and high-frequency components, then reuses the low-frequency part and predicts the high-frequency part using historical high-frequency information and a Hermite interpolator. Additionally, this paper proposes to cache only the Cumulative Residual Features (CRF), which significantly reduces memory overhead. Experimental results show that FreqCa consistently demonstrates strong competitiveness in terms of speed and memory usage in benchmark tests of text-to-image generation and image editing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is based on a sound motivation. Figure 2 illustrates the different behaviors of low-frequency and high-frequency components during the diffusion model sampling process, proving the rationality and effectiveness of the differentiated caching strategy for low and high frequencies.\n2. FreqCa combines differentiated frequency processing with memory-efficient CRF caching, outperforming previous methods that cache entire features.\n3. The experimental results are sufficient. Extensive experiments are conducted on different models and tasks, fully verifying the effectiveness of FreqCa."}, "weaknesses": {"value": "1. The different behaviors of low-frequency and high-frequency components during diffusion model sampling have already been discovered in previous work (FreeU [1]), although that study focused on UNet models.\n2. The paper lacks elaboration on the implementation of its methods. For example, it does not explain how FFT and DCT work in detail, nor how the m-th order difference $\\Delta^{m}F(z_{t}^{High})$ of the second-order Hermite interpolator is calculated.\n3. Although experimental results show that Hermite interpolation works well, there is a lack of theoretical analysis on error bounds or convergence, as well as comparisons with other interpolation methods.\n\n[1] Si C, Huang Z, Jiang Y, et al. Freeu: Free lunch in diffusion u-net[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 4733-4743."}, "questions": {"value": "1. In Figure 3(d), why are $\\Delta^{1}F(z_{t}^{High})$ and $\\Delta^{2}F(z_{t}^{High})$ connected bidirectionally to $\\Delta^{1}F(z_{t+N}^{High})$ and $\\Delta^{2}F(z_{t+N}^{High})$ respectively? Do these two pairs of components influence each other?\n2. Why is a second-order Hermite interpolator chosen to predict the volatile high-frequency components? Have different methods been tried for predicting high-frequency components? How do their performances compare to the second-order Hermite interpolator, and are there specific comparison results?\n3. What is the maximum acceleration multiple of FreqCa? Is there an acceleration threshold beyond which the quality of generated images degrades sharply?\n4. Are there special cases where FreqCa fails to achieve acceleration? If yes, can the reasons for this failure be explained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qe5ox1Zg6p", "forum": "I4hx723NT5", "replyto": "I4hx723NT5", "signatures": ["ICLR.cc/2026/Conference/Submission2446/Reviewer_wkDH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2446/Reviewer_wkDH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722630613, "cdate": 1761722630613, "tmdate": 1762916240412, "mdate": 1762916240412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FreqCa, a frequency-aware feature-caching framework aimed at accelerating diffusion transformers during inference. The authors first conduct a Fourier/DCT-based analysis and observe that low-frequency components of the denoising features exhibit high similarity but low continuity across timesteps, whereas high-frequency components show the opposite behavior. Leveraging this observation, FreqCa\n\n1.\tcaches only the Cumulative Residual Feature (CRF) instead of all layer outputs, reducing cache memory from O(L) to O(1);\n\n2.\tdecomposes the CRF into low- and high-frequency parts;\n\n3.\tdirectly reuses the low-frequency part and predicts the high-frequency part with a second-order Hermite interpolator;\n\n4.\treconstructs the feature and skips the heavy forward pass for the chosen timesteps.\n\nExperiments on four diffusion models show 6–7× speed-ups less degradation in ImageReward or GEdit scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n\n2. Frequency-domain analysis reveals a compelling disparity between low- and high-frequency temporal dynamics in diffusion features, providing an intuitive rationale for the proposed split strategy.\n\n3. FreqCa achieves latency–quality trade-offs on both text-to-image generation and image-editing benchmarks without any fine-tuning."}, "weaknesses": {"value": "1. The paper does not discuss whether the proposed method can be extended to text-to-video generation. Additional discussion would be valuable. \n\n2. For text-to-image tasks, the paper mainly reports metric tables. Qualitative comparison with other baselines is missing. \n\n3. In Figure 6 row 1, the non-edited areas (e.g., car license plate and body text) appear significantly changed compared with the base model. \n\n4. Recent papers like OmniCache(ICCV2025), BlockDance(CVPR 2025) also decouple structural and textural signals for feature reuse. A discussion clarifying the differences with FreqCa might be necessary."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CO37Piep4D", "forum": "I4hx723NT5", "replyto": "I4hx723NT5", "signatures": ["ICLR.cc/2026/Conference/Submission2446/Reviewer_AmyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2446/Reviewer_AmyC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828181729, "cdate": 1761828181729, "tmdate": 1762916240269, "mdate": 1762916240269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FreqCa, which exploits and empirically proves a key intuition that the updates made by each block during a denoising step with a diffusion transformer are cumulative, and can be effectively represented by a cumulative residual feature (CRF) which can be separated into low and high frequency bands (with FFT or DCT). More compute is used to try to model change/error for high frequency bands (with a taylor or hermite predictor, the final work preferring a hermite predictor). A key outcome is that while having comparable quality/speed to prior training-free caching methods, this method in this work has a much smaller memory footprint."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "S1. This addresses an important aspect of efficiency that is often neglected in the prior work: memory efficiency. \n\nS2. At the same time, it matches or outperforms prior art in terms of acceleration (wall time) and quality (Image Reward).\n\nS3. The intuitions driving this performance make sense, they are well-analyzed, and the method seems quite novel."}, "weaknesses": {"value": "W1. The writing is poor. It is very difficult to understand the method without reading the provided code, and the code itself does not come with clear documentation, so one must try to make decent guesses of which files are relevant, and then trace from there. For example, N, which defines which blocks are cached for standard methods, is never clearly defined, nor is it clearly explained how the CRF cache can be used for the relevant layers on subsequent steps. The intuitions for these components are clearly explained, but their usage for the caching is not.\n\nW2. It's not clear if FreqCa consistently would outperform competing methods outside of the high steps, high speedups regime. In fact, Figure 8 seems to have some evidence to the contrary. Additionally, 25 steps is essentially the same as 50 (Table 2). It would be ideal to evaluate with caching in these settings as well, where we first get decent speedups by reducing the steps, and then apply the caching. How does FreqCa compare to other methods in these cases, when the possible speedups are 2x-3x instead of 5x-7x?"}, "questions": {"value": "The presentation of the work needs to improve substantially, and I would appreciate clarification of how the method works. \n\n1. How are the stored high frequency and low frequency components actually used in the FreqCa caching framework? I understand how they are computed, but not how they are used, in particular how they interact with the hyperparameter N.\n\n2. Can this be used for text-to-video generation? Most of the prior works evaluate with some T2V, so why was this omitted? \n\n3. Figure 8 shows better results for TaylorSeer with less aggressive acceleration. Additionally, 50 steps seems to be quite high as a baseline. In many cases some of these models can give good results with 20 or 30 steps. Does FreqCa still perform well in such settings, where high accelerations (5x and 7x) are not possible? Or is TaylorSeer better in these cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M7KNzBA9hX", "forum": "I4hx723NT5", "replyto": "I4hx723NT5", "signatures": ["ICLR.cc/2026/Conference/Submission2446/Reviewer_itww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2446/Reviewer_itww"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866882541, "cdate": 1761866882541, "tmdate": 1762916240091, "mdate": 1762916240091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FreqCa, a training-free method to accelerate Diffusion Transformers (DiTs) by introducing frequency-aware feature caching. Unlike prior caching approaches that treat all features uniformly—either reusing them based on similarity or forecasting them based on continuity—FreqCa recognizes that different frequency components of diffusion features behave differently over timesteps. Through Fourier or cosine decomposition, it separates each feature map into low-frequency components, which are stable and highly similar but discontinuous, and high-frequency components, which are volatile yet continuous. FreqCa then reuses the low-frequency parts directly while predicting the high-frequency parts using a Hermite polynomial interpolator, allowing accurate extrapolation without retraining or modifying the model.\n\nTo further enhance efficiency, the paper proposes Cumulative Residual Feature (CRF) caching, which stores only a single aggregated feature tensor instead of per-layer activations—reducing cache memory usage by up to 99% and achieving constant  O(1) memory complexity. Experiments on multiple benchmarks, including FLUX.1-dev, Qwen-Image, and Qwen-Image-Edit, show that FreqCa delivers 6–7× faster inference while maintaining image quality within 2% of the original model. Overall, FreqCa unifies the reuse- and forecast-based caching paradigms under a frequency-aware framework, offering a theoretically grounded and practically efficient solution for accelerating large diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The key strengths of this work lie in its innovative frequency-domain insight and practical efficiency gains. By analyzing diffusion model features through their frequency components, the authors uncover that low- and high-frequency signals evolve differently across timesteps—low frequencies are highly similar but discontinuous, while high frequencies are volatile yet continuous. This observation leads to a unified and theoretically grounded frequency-aware caching strategy that combines the advantages of reuse-based and forecast-based methods. The resulting dual-path design—reusing low-frequency features while predicting high-frequency ones via Hermite interpolation—achieves both stability and precision without retraining or architectural changes.\n\nAnother major strength is the introduction of Cumulative Residual Feature (CRF) caching, which compresses layer-wise activations into a single tensor, reducing memory usage by up to 99% and achieving constant O(1) complexity. Combined with extensive empirical validation across diverse models such as FLUX.1-dev and Qwen-Image, FreqCa delivers 6–7× acceleration with less than 2% quality loss, surpassing prior caching-based accelerators like TaylorSeer and FORA. Overall, the method is both conceptually elegant and practically powerful, offering a training-free, plug-and-play solution that meaningfully advances the efficiency of large diffusion transformers."}, "weaknesses": {"value": "The main weaknesses of the paper relate to its scope, assumptions, and evaluation breadth, rather than flaws in execution.\n\nFirst, FreqCa’s effectiveness depends heavily on the frequency-domain assumption—that diffusion features can be cleanly decomposed into separable low- and high-frequency components with distinct temporal dynamics. While this holds empirically for the tested visual models (e.g., FLUX and Qwen-Image), it may not generalize to other modalities or architectures, such as U-Net–based diffusion models, text-conditioned latent diffusion, or video models with nonstationary frequency spectra. The approach may also be sensitive to the chosen decomposition method (FFT vs. DCT) and cutoff parameters, yet the paper provides limited analysis of these choices’ effects on accuracy or stability.\n\nSecond, although the Cumulative Residual Feature (CRF) design greatly improves memory efficiency, it implicitly assumes that residual features preserve enough information for high-fidelity reconstruction. This approximation might degrade when applied to deeper or non-residual architectures, and the paper does not fully quantify how caching fidelity changes under extreme compression. Moreover, the Hermite predictor introduces additional hyperparameters (e.g., order and interval length), but their tuning process and generalizability are not deeply analyzed.\n\nFinally, the experimental scope, while extensive for image generation and editing, focuses mainly on large diffusion transformers under controlled GPU setups. Broader validation—on smaller models, other tasks, or hardware platforms—would better demonstrate robustness and portability. Overall, the paper’s weaknesses are in assumption generality, parameter sensitivity, and cross-domain evaluation, rather than methodological soundness."}, "questions": {"value": "The followings are questions:\n1. Have you evaluated FreqCa on other architectures such as U-Net–based diffusion models or multimodal systems like text-to-audio or video diffusion? Since the frequency decomposition is image-centric, how well would the framework generalize to non-visual or non-grid-structured data?\n2. How sensitive are the results to the choice between FFT and DCT, or to the frequency cutoff between low and high bands? Could an adaptive or learned decomposition improve robustness across different models?\n3. While CRF caching reduces memory by 99%, does it ever lose critical layer-specific information, particularly in deeper or non-residual architectures? Have you tested scenarios where the compression trade-off leads to visible artifacts or quality drops?\n4. Why was the Hermite predictor chosen specifically over other nonlinear extrapolators (e.g., spline or neural predictors)? How stable is the Hermite interpolation at higher acceleration ratios (e.g., 8× or 10×), and where does degradation first appear?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "60uNg811Mg", "forum": "I4hx723NT5", "replyto": "I4hx723NT5", "signatures": ["ICLR.cc/2026/Conference/Submission2446/Reviewer_Xrjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2446/Reviewer_Xrjg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961529283, "cdate": 1761961529283, "tmdate": 1762916239854, "mdate": 1762916239854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}