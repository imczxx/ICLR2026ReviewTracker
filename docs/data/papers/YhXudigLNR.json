{"id": "YhXudigLNR", "number": 12188, "cdate": 1758206217144, "mdate": 1759897526483, "content": {"title": "Optimal Scaling Needs Optimal Norm", "abstract": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling over model and dataset size is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(\\eta^\\ast, B^\\ast)$ consistently shares the same operator norm value – a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a unique $(\\eta^\\ast, B^\\ast)$ achieves the best loss. We provide the first measurement of $(\\eta^\\ast, B^\\ast)$ scaling with the dataset size for Scion and find the scaling rules consistent with those of Adam. Tuning learning rates per layer group can also improve the model performance, with the output layer driving the sensitivity and hidden layers benefiting from slightly lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.", "tldr": "", "keywords": ["optimal scaling", "hyperparameter transfer", "scion"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a67b0ee0da590040b8d4b7d6d23d9ebbd270f4c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies optimal tuning of LLMs as LR, model size, dataset size, and batch size vary.  Specifically, they consider tuning of norm-based optimizers such as Scion, where the operator norm of different layers is controlled as part of the optimization process.  Over a number of training runs, they identify that, at optimal settings of LR and B, the operator norm of the output layer is fairly consistent as both model and dataset size scale.  Further, they derive empirical power laws for optimal LR and B.  They also show that per-layer-group tuning of LRs can also help with Scion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "It's interesting to consider optimal hyperparameter scaling rules for norm-based optimizers, since this has previously mostly been done in AdamW, with and without maximal update parameterization.\n\nSince norm-based approaches give new metrics that you can monitor, it is well-motivated to monitor them, and to derive insights from them.\n\nI also don't mind the idea of stress-testing at a constant learning rate (even though this is definitely not state-of-the-art), as doing so can make things obvious that might not be when we use LR decay.  Similarly, it's perhaps okay to not use weight decay (but again, this deviates from the SOTA).\n\nSome of the observations are definitely intriguing and thought-provoking.  For example, recent work is showing collapse/consistency in training loss curves (after normalization) across model sizes (https://arxiv.org/abs/2507.02119, https://arxiv.org/abs/2509.25087).  Here the operator norm of the output layer --- the first layer to get gradient from the loss --- is also consistent in a kind of normalized sense across scales as well.  I wonder if there’s likewise a connection between the output norm and scaling laws.  Especially since you use a constant LR, power laws for loss should hold at every step."}, "weaknesses": {"value": "My overall view is that the way things are presented here is a bit of an overclaim, or a bit misleading for practitioners.  I have concerns about the real cause-and-effect, and whether the insights gained here are actionable.\n- For example, we say that scaling is “governed” by the output layer operator norm, and we plot it as the independent variable, but it’s just something we observe, right?  We don’t scale or control it directly.  So I’m not sure how to use this, and secondly, correlation is not causation, right?\n- In terms of actionable insights: the output norm seems like something you measure after a large-scale training run.  If you're training various runs, you wouldn't choose the one with lowest output norm, you'd choose the one with lowest loss, right?\n- Moreover, even if your other hypotheses prove valid, how can you actually operationalize this?  How do you know the region of low norm sensitivity a priori, where you can exchange η for B?  Does this low-sensitivity region also transfer across scales?  “we cannot fully rely on the output norm as a guide to selecting optimal hyperparameters” – as a practitioner, how can you rely on it at all?\n\nAt least in the main paper, I would have liked more discussion of the case where you actually do use LR decay.  Do you still see output norm transfer?\n\nLooking at the results as a whole, I'm not sure I *do* actually see norm transfer.  For example, Figure 1(a) and Figure 1(b) have different optimal output norms.  Perhaps they get even more different in other situations.  So it seems to be that output norm alignment is neither necessary nor sufficient.  So what *can* we really say about it???\n\nMore nitpicky:\n- Methods section: How can we say that spectral norms (or MUP) “guarantee” hyperparameter transfer?  We already know that the amount of data affects the optimal HP settings, right?  So under what conditions do the HPs transfer?  With the same amount of data?  With a certain tokens-per-parameter ratio?  So in what sense is it a guarantee?\n- “We briefly explain the idea behind each of them below.” – each of what?\n- Tuning on a proxy model citing “(OpenAI et al., 2024; Gunter et al., 2024; Dey et al., 2024; Meta AI, 2025; Zuo et al., 2025)” – can you make the semantics of these citations more clear?  Like, does GPT-4 use MUP, and if not, what does it tune on a proxy model specifically?\n- Why do we italicize “a single optimal batch size” in 3.2?  Is that surprising or notable somehow?\n- Typo: “Later, a deeper understand* has been built”"}, "questions": {"value": "- Why isn’t there the same optimal output norm in Figure 1(a) and 1(b)?  One curve (d=256, D=2^33) is even plotted on both plots, right?\n- What is the significance of RMS-norming the inputs to all layers?  Do we still need the same control on the operator norm?  Does it affect, e.g., how LR correlates with output norm?  You mention this might be one reason we get depth transfer --- could it be a reason we get norm transfer???\n- Can we explain via theory, or even via intuition, why different layers would have different optimal LRs, when using norm-based optimizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3pReBJqA1j", "forum": "YhXudigLNR", "replyto": "YhXudigLNR", "signatures": ["ICLR.cc/2026/Conference/Submission12188/Reviewer_NKpP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12188/Reviewer_NKpP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499985524, "cdate": 1761499985524, "tmdate": 1762923138056, "mdate": 1762923138056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at how the optimal learning rate and batch size scale across model dimensions (width, depth) and dataset size for the Scion optimizer. In particular, they investigate how the norm of the final layer seems to be preserved across the optimal hyperparameter configurations, which they call norm transfer. These experiments are based on grid searches for relatively small (69M) Llama 3 models with additional normalization layers and generally no momentum or weight decay."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and clearly has a lot of effort put into it.\n* The topic of the paper (HP scaling across multiple dimensions) is interesting and relevant to the community.\n* Good breadth of experiments and ablations."}, "weaknesses": {"value": "* Some aspects of the experimental configuration choices may limit the relevance of the finding to typical training setups (e.g. no weight decay, no momentum, no learning rate schedule).\n* It seems very likely that the norm transfer is simply a correlation with some other measure, rather than directly causing any interesting behaviors.\n* I feel the paper somewhat lacks a clear practical takeaway. The norm observations can not directly be used (and may not hold with weight decay which is standard practice) and the HP scaling rules may also not hold for more typical training settings."}, "questions": {"value": "* Could you clarify exactly which scaling rules and results hold in a more standard training setting (LR warmup + decay to zero, momentum, weight decay)?\n* I think the paper would be stronger if you made an attempt to explain why the norm should transfer and when it does not. With weight decay it seems less likely, especially if you consider different (LR, WD) pairs. For the PyTorch AdamW version where the total decay scaling is (1 - LR * WD), different configurations with a constant LR*WD value often give similar final performance while affecting the norm differently.\n* Do you believe your findings hold for other optimizers, e.g. AdamW?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "inY7HLeyWQ", "forum": "YhXudigLNR", "replyto": "YhXudigLNR", "signatures": ["ICLR.cc/2026/Conference/Submission12188/Reviewer_f6ch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12188/Reviewer_f6ch"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642077790, "cdate": 1761642077790, "tmdate": 1762923137397, "mdate": 1762923137397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the optimal learning rate and batch size scaling in LLM training. Using the scion optimizer, they find that the optimal learning rate and batch size share the same operator norm of the output layer. This condition, however, is necessary and not sufficient, as different learning rates and batch sizes share the same operator norm. Through large-scale experiments, they provide empirical scaling laws for learning rate and batch size as functions of the dataset. They recover the known result that the optimal learning rate is proportional to the square root of batch size."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Large-scale experiments\n* The work unifies learning rate transfer and learning rate-batch size scaling laws\n* The norm transfer result is intriguing (but I am unsure about the implications)"}, "weaknesses": {"value": "* The work is done on Scion optimizer, which is not well adopted in the field yet. \n* The results are empirical, and it's unclear why norm transfer phenomena occur."}, "questions": {"value": "* I would like to request that the authors help me understand the implications of this work --- as an optimal norm may not imply optimal performance, what is the impact of norm transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hNeNy9aA17", "forum": "YhXudigLNR", "replyto": "YhXudigLNR", "signatures": ["ICLR.cc/2026/Conference/Submission12188/Reviewer_HkV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12188/Reviewer_HkV3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684235639, "cdate": 1761684235639, "tmdate": 1762923136758, "mdate": 1762923136758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates optimal hyper-parameter scaling for LLM training. The authors claim that the joint optimal scaling over model and dataset size is governed by a single invariant, the operator norm of the output layer. They study this optimal norm, specifically for Scion/Muon optimizer. The authors also mention that having an optimal norm is a necessary but not sufficient condition."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- The paper has a clear experimental methodology, with strong theoretical explanations about the approach. \n- The paper tests norm transfer across a range of width, depth and data scales.\n- The paper poses some very interesting questions and theoretical gaps for the research community towards the end of the main text.\n- The authors release their Distributed Scion implementation, which can be helpful for the broader research community."}, "weaknesses": {"value": "Scope for improvement:\n- Since the paper only looks at the invariant optimal norm for the Scion/Muon optimizer, the applicability is narrow and cannot be generalized to other widely used optimizers.\n- Most of the experiments in the paper are performed on a 69M parameter model, which is really small. The authors should shed some light on why they didn’t use bigger models which are more representative of the current SOTA model size. I am also curious how the optimal norm changes across different data regimes for a fixed set of models.\n- The paper evaluates optimality solely through training loss (cross-entropy), without downstream task benchmarks. While training loss is standard for scaling law research, validating that norm-optimized configurations also optimize downstream performance would strengthen the claims. Even for small models (69M-1.3B), evaluating trends on standard benchmarks (e.g., HellaSwag, LAMBADA) would confirm that norm transfer reflects meaningful capability improvements, not just training dynamics artifacts. This is particularly important given the paper's claim of discovering a 'unifying principle' for optimal scaling.\n- The anonymous github repo (https:// anonymous.4open.science/r/disco_iclr2026-E11D) seems empty, which raises some reproducibility concerns."}, "questions": {"value": "Points 1, 2, 3 from weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3jhMRkNuB8", "forum": "YhXudigLNR", "replyto": "YhXudigLNR", "signatures": ["ICLR.cc/2026/Conference/Submission12188/Reviewer_y4hf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12188/Reviewer_y4hf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958701617, "cdate": 1761958701617, "tmdate": 1762923136178, "mdate": 1762923136178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}