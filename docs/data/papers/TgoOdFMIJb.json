{"id": "TgoOdFMIJb", "number": 14725, "cdate": 1758242491474, "mdate": 1759897352708, "content": {"title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression", "abstract": "We introduce SIRI, **S**caling **I**terative **R**einforcement Learning with **I**nterleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempt to reduce them at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The *compression phase* cuts the rollout length, forcing the model to make precise and valuable decisions in limited context, which effectively reduces redundant tokens and increases reasoning density. The *expansion phase* then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. \nRemarkably, we find that after each compression–expansion cycle, the model’s performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance–efficiency trade-off.\nTraining on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2\\% while reducing token usage by 46.9\\% after three iterations, and SIRI-high achieves the highest accuracy\ncompared to all other methods (Figure 1).\nOur findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two.", "tldr": "", "keywords": ["LLM", "RL", "efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ede75fa5a7ded1d8901c081becb82beaec4bcdd5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Oscillating response length helps the iterative on-policy RL mechanism such as GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* A simply designed length scheduler helps improve accuracy as iterations go by, as illustrated in Figures 1 and 4.\n* If it is indeed true that accuracy increases when training with GRPO using responses generated through compression and expansion cycles, this represents a quite intriguing discovery."}, "weaknesses": {"value": "* While the paper provides empirical validation (Fig 4, 5, 7), the core mechanism in Figure 2(b) is a hypothesis without theoretical justification. \n* Unsure that the finding is statistically valid—Standard error or confidence interval is not reported in any experiments. \n* The stylized dynamics in Figure 2(b) do not align well with the actual training curves in Figures 4 and 5. The paper appears to be drawing connections between the hypothesis and results that may not genuinely exist.\n* The way of using the length scheduler is not clear. Did you include the term in the GRPO objective and add additional parameter to control its effect?"}, "questions": {"value": "* How can we determine the causal relationship between compression and efficiency gains and between expansion and exploration capability?\n* Recently, I read the paper below [1], which showed that GRPO leads to longer responses. The authors removed two terms to make it more stable and control the response length. This might be relevant to your work on analyzing response length.\n\n[1] Understanding R1-Zero-Like Training: A Critical Perspective, COLM 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HnRY5ZMIHZ", "forum": "TgoOdFMIJb", "replyto": "TgoOdFMIJb", "signatures": ["ICLR.cc/2026/Conference/Submission14725/Reviewer_DQ7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14725/Reviewer_DQ7k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761052648821, "cdate": 1761052648821, "tmdate": 1762925087004, "mdate": 1762925087004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIRI, a framework for training LRMs that enhances reasoning accuracy while reducing token usage by dynamically alternating between compression and expansion phases during RL. SIRI employs a scheduler to adjust maximum rollout lengths, forcing concise decision-making in compression phases to eliminate redundancy and enabling exploration in expansion phases for long-horizon planning. SIRI improves performance on math benchmarks, outperforming baselines such as DeepScaleR and AdaptThink by pushing the Pareto frontier of efficiency and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well presented, and easy to read\n- Detailed empirical analyses, empirically validating the ideas of the proposed method\n- Proposed method is easy to implement, and leads to a SOTA performance in terms of Pareto frontier.\n- Compares multiple scheduler choices"}, "weaknesses": {"value": "- The main idea of the paper does not seem very novel, as it is extending DeepScaleR's compression-extension approach into the iterative training framework.\n- Including a new scheduler adds a number of hyperparameters, e.g., scheduler type, L_max, L_min, T. Tuning such hyperparameters can significantly increase the computational burden.\n- Only validated on DeepSeek R1 Distill Qwen models"}, "questions": {"value": "- Some recent papers on RLVR report that compressing the number of thinking tokens can degrade the model's general performance on different tasks other than the one being trained. Is there any severe degradation on model performance on general tasks when the compression is repeatedly done?\n\n- While in the ablations it is discussed that scheduler with a longer cycles performs best, DeepScaleR is outperformed by proposed method where DeepScaleR can also be seen as a form of SIRI with extremely long cycles. What would be the potential reasons that the proposed method outperforms DeepScaleR? How would the performance trend be if we further increase the cycle lengths more than those experimented in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w6LApvUFaw", "forum": "TgoOdFMIJb", "replyto": "TgoOdFMIJb", "signatures": ["ICLR.cc/2026/Conference/Submission14725/Reviewer_y5FW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14725/Reviewer_y5FW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491508501, "cdate": 1761491508501, "tmdate": 1762925086415, "mdate": 1762925086415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SIRI, a training method for Large Reasoning Models. The core idea is to improve both reasoning accuracy and token efficiency through an iterative process. During training, the model alternates between a compression phase with a short generation limit and an expansion phase with a longer limit. The compression phase forces the model to generate more concise and dense reasoning, while the expansion phase allows for exploration and planning. The authors show empirically that this cyclical training pushes the model towards better performance and efficiency, outperforming existing methods on mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple, intuitive, and easy to implement. It introduces a novel training curriculum by dynamically adjusting the maximum generation length using a scheduler. \n\n2. The paper provides strong empirical evidence to support its claims. The experiments are conducted on two different model sizes and evaluated on multiple standard mathematical reasoning benchmarks. The results in Table 1 clearly demonstrate that SIRI improves both accuracy and token efficiency, surpassing strong baselines.\n\n3. This paper presents a thorough analysis of the method's behavior. It goes beyond final performance numbers and investigates the training dynamics, the effect of different schedulers, and the changes in the model's output patterns."}, "weaknesses": {"value": "1. The conceptual novelty of the method appears to be an incremental extension of prior work. The idea of a compression phase followed by an expansion phase was already present in the DeepScaleR baseline. The primary contribution here is making this process iterative, which feels more like a refinement than a fundamentally new approach.\n\n2. The comparison with baseline methods could be more robust. The results for several key baselines are incomplete in Table 1, which weakens the comparative claims. Additionally, the main competitive baseline, DAPO-DeepScaleR, is an author implementation, which raises questions about whether it was optimally tuned for a fair comparison.\n\n3. The explanation for why the method works is based on indirect evidence. The analysis linking performance gains to the frequency of specific keywords like \"wait\" is correlational. It does not provide a deep, causal understanding of how the model's reasoning structure is fundamentally improved by the iterative training."}, "questions": {"value": "1. The method can be viewed as a form of curriculum learning. How does the proposed iterative oscillation compare to a more standard curriculum that simply starts with a short generation length and gradually increases it over time without the compression cycles?\n\n2. Regarding the DAPO-DeepScaleR baseline, could you provide more detail on its implementation? Specifically, was its single expansion phase given a comparable number of training steps to one of your expansion phases to ensure that the performance difference is due to the iterative nature of SIRI?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zz7nQpP0nB", "forum": "TgoOdFMIJb", "replyto": "TgoOdFMIJb", "signatures": ["ICLR.cc/2026/Conference/Submission14725/Reviewer_bwAV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14725/Reviewer_bwAV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805702687, "cdate": 1761805702687, "tmdate": 1762925085977, "mdate": 1762925085977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SIRI, a training regime for Large Reasoning Models (LRMs) that aims to solve the performance-efficiency trade-off1. The core idea is to iteratively alternate between \"compressing\" and \"expanding\" the model's reasoning budget by dynamically adjusting the maximum rollout length $L$ during reinforcement learning. The authors use a \"length scheduler,\" (e.g., a cosine scheduler) to manage this alternation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The empirical results on benchmarks like AIME24 are strong—achieving high accuracy with reduced token counts"}, "weaknesses": {"value": "Lack of Theoretical Guarantee or Novel Algorithm:\n\nEntirely Heuristic: The method is motivated by a hypothesis based on a visual inspection of a previous paper's training curve. There is no formal analysis of why this compression-expansion cycle is beneficial.\n\nNo New Algorithm: The paper does not introduce a new loss function or make any fundamental modifications to the RL algorithm. It builds on existing methods (GRPO/DAPO) and primarily proposes a novel scheduling strategy for one of the training hyperparameters—the maximum length $L$. The proposed schedule resembles a cosine learning rate schedule. \n\nWeak Post-Hoc Analysis: The analysis provided is purely observational. For example, the entropy analysis in Appendix A.2 is interesting, but it merely observes that entropy oscillates. Worse, it even notes that the baseline model also shows periodic entropy fluctuations, which confuses the claim that this oscillation is the unique, driving factor behind SIRI's success ."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qIq9PjurQK", "forum": "TgoOdFMIJb", "replyto": "TgoOdFMIJb", "signatures": ["ICLR.cc/2026/Conference/Submission14725/Reviewer_v74n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14725/Reviewer_v74n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938544576, "cdate": 1761938544576, "tmdate": 1762928056464, "mdate": 1762928056464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}