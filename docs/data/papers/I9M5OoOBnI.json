{"id": "I9M5OoOBnI", "number": 576, "cdate": 1756749259995, "mdate": 1763748765594, "content": {"title": "Universal Properties of Activation Sparsity in Modern Large Language Models", "abstract": "Activation sparsity is an intriguing property of deep neural networks that has been extensively studied in ReLU-based models, due to its advantages for efficiency, robustness, and interpretability. \nHowever, methods relying on exact zero activations do not directly apply to modern Large Language Models (LLMs), leading to fragmented, model-specific strategies for LLM activation sparsity and a gap in its general understanding. \nIn this work, we introduce a general framework for evaluating sparsity robustness in contemporary LLMs and conduct a systematic investigation of this phenomenon in their feedforward~(FFN) layers.\nOur results uncover universal properties of activation sparsity across diverse model families and scales.\nImportantly, we observe that the potential for effective activation sparsity grows with model size, highlighting its increasing relevance as models scale. \nFurthermore, we present the first study of activation sparsity in diffusion-based LLMs. \nOverall, our work provides a comprehensive perspective and practical guidance for harnessing activation sparsity in LLM design and acceleration.", "tldr": "We provide an overview of activation sparsity in modern LLM architectures and highlight a set of interesting properties of the activations across different model families.", "keywords": ["LLMs", "activation sparsity", "efficiency", "representations"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12d57afe62174398a79a78df2265a6d1380990a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive study on the properties of sparsification in the FFN layers of transformer-based language models and diffusion language models. Specifically, they use a simple top-$p$ sparsification strategy and analyze the robustness against sparsity of various models from the Gemma3, Qwen2.5, and LLaMa3.1/3.2 family, by measuring at what sparsity levels does downstream task performance drop below 99% of its original performance.\n\nSome interesting findings from this paper include: input-based sparsification is often more attractive than gate-based sparsification, larger models are more robust to sparsification, and diffusion models are more robust to sparsification. The findings of this paper are valuable for designing language models with better robustness against sparsification or methods that better leverage sparsification to achieve inference speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-presented and generally easy to follow.\n- The findings of this paper is very interesting and highly valuable to researchers interesting in language models with sparse activations. \n- The methods and experimental setup is well designed, making the results reliable and convincing."}, "weaknesses": {"value": "- How sparse activation patterns evolve across diffusion steps is an interesting phenomenon and the paper does a good job in bringing it up (starting with Line 406, and Figure 6). However, the conclusions of the experiments in this regard is inconclusive and it is unclear how they are useful for the community.\n- Considering how sparse mixture-of-experts (MoE) models have become very popular, I would love to see investigations in how the findings of this paper transfers to MoE models. I suggest repeating some of the experiments with models such as MoE variants of the Qwen3 series, GPT-OSS, GLM4.5, etc.\n- While the top-$p$ sparsification strategy is reasonable, I would like to see a comparison of it against an even simpler, top-$k$ sparsification strategy."}, "questions": {"value": "- What does \"Normalized accuracy\" in Figure 5 mean?\n- Can you provide more insights into why MMLU-Redux is less robust to sparsification compared to GSM8K and TruthfulQA?\n- It seems that you apply the same $p$ value across all layers. However, I think using a different $p$ for different layer is more reasonable. Is it possible to repeat the experiments, but with a layer-specific $p$ value such that each layer can have its own critical sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3f4MMHsCUG", "forum": "I9M5OoOBnI", "replyto": "I9M5OoOBnI", "signatures": ["ICLR.cc/2026/Conference/Submission576/Reviewer_BibT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission576/Reviewer_BibT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545071374, "cdate": 1761545071374, "tmdate": 1762915552019, "mdate": 1762915552019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a unified framework for evaluating sparsity robustness in large language models and systematically examines activation sparsity in their feedforward layers. The study identifies consistent sparsity patterns across different model families and sizes, revealing that larger models exhibit greater potential for effective activation sparsity. It also provides the first analysis of sparsity in diffusion-based LLMs. Overall, the work offers insights and guidance for leveraging sparsity to improve LLM efficiency and design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper uncovers some universal properties of activation sparsity across diverse model families and scales. This could be quite helpful to the community.\n- The paper is well written.\n- Experiments are quite quantitive and informative."}, "weaknesses": {"value": "- Pure experimentation-driven work without any theoretical analysis or others.\n- Larger model leads to higher critical sparsity. This point is actually very straightforward. For instance, usually larger model and smaller model are trained on the same pile of data. Of course the larger model admit more redandancy."}, "questions": {"value": "- What is  \"functional sparsity\"? The authors not seem to provide good explanation across the paper, though this term is very important.\n- What is \"effective rank\"? Need short explanation in the paper to be self-contained. For instance, what \"effective rank of 0.1\" means? How it corresponds to your conclusion.\n\nPersonally, I like this type of paper giving inspiration. Please work on the quesitons and weakness points if appropriate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "te0WqYasAP", "forum": "I9M5OoOBnI", "replyto": "I9M5OoOBnI", "signatures": ["ICLR.cc/2026/Conference/Submission576/Reviewer_PkEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission576/Reviewer_PkEJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877407150, "cdate": 1761877407150, "tmdate": 1762915551768, "mdate": 1762915551768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the lack of a general understanding of activation sparsity in modern Large Language Models (LLMs) that utilize non-ReLU activations, which prohibits the use of traditional zero-based sparsity methods. To solve this, the authors introduce a novel, general framework to evaluate sparsity robustness in contemporary LLMs, focusing on the Feedforward Network (FFN) layers. Through systematic investigation across diverse models and scales, the work reveals a key universal property: the potential for effective activation sparsity increases with model size. Furthermore, the study presents the first analysis of activation sparsity in diffusion-based LLMs, ultimately offering a comprehensive perspective and practical guidance for leveraging this phenomenon in LLM design and acceleration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a simple, training-free top-p sparsification method and the metric of critical sparsity (maximum sparsity retaining 99\\% performance), providing a unified and fair way to compare sparsity tolerance across different LLM architectures and FFN components.\n\n2. The work systematically confirms that the critical sparsity increases with model size. This is consistently reinforced by the finding that the effective rank of activations decreases with model size, offering strong evidence that larger models inherently possess greater exploitable redundancy.\n\n3. The investigation into diffusion-based LLMs (LLaDA) is novel, revealing that they exhibit substantial activation sparsity and even slightly more favorable sparsity-performance trade-offs than autoregressive models, highlighting a new acceleration opportunity.\n\n4. Based on empirical data, the authors provide the practical insight that input activation sparsification is the most effective training-free approach, as its sparsity tolerance is comparable to or greater than gate or up-projection activations."}, "weaknesses": {"value": "1. A major finding is that the critical sparsity varies substantially across different downstream tasks and training recipes (e.g., instruction-tuning). This challenges the core assumption of many prior acceleration methods that sparsification rules calibrated on an auxiliary dataset will generalize universally without overfitting\n\n2. The paper acknowledges that the effective speedups from activation sparsity methods are practically limited to a factor of 1.3x to 1.5x, which is less compelling when compared to alternative lossless techniques like speculative decoding that can achieve up to 4x speedups.\n\n3. The analysis is explicitly constrained to only the FFN layers, intentionally excluding the Multi-Head Attention (MHA) module. While a cost justification is provided, this limits the comprehensiveness of the \"universal properties\" claim within the entire Transformer architecture.\n\n4. The use of effective rank as a theoretical proxy for redundancy is weakened by the observation that gate activations show a high effective rank yet exhibit a low empirical capacity for sparsification, suggesting that this metric is insufficient to fully capture robustness to sparsification"}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ktjQmA39VO", "forum": "I9M5OoOBnI", "replyto": "I9M5OoOBnI", "signatures": ["ICLR.cc/2026/Conference/Submission576/Reviewer_YvVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission576/Reviewer_YvVz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989426189, "cdate": 1761989426189, "tmdate": 1762915551215, "mdate": 1762915551215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates activation sparsity in modern large models. Beyond ReLU-based networks that produce exact zeros, using SiLU/GELU activations in FFNs yields functional or approximate sparsity, with many activations near zero. The authors address the current fragmented understanding of this phenomenon by introducing a general, training-free framework to evaluate sparsity robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "They propose a simple top-p sparsification to induce sparsity in various activations (input, gate, up-projection, intermediate). This allows for the introduction of critical sparsity, i.e., the maximum sparsity level that causes less than 1% performance drop. Through extensive experiments on models like Gemma, Llama, and Qwen across different scales (e.g., 1B to 32B parameters), they find that the potential for effective activation sparsity increases with model size. They also find that input-based sparsification is as effective as, or even better than, the more commonly studied gate-based methods, making it a more practical, predictor-free approach. The study also shows that critical sparsity is task-dependent, varying significantly across different downstream evaluations. It also persists across different model types, including instruction-tuned and reasoning-specialized variants. All these provide a comprehensive perspective and practical guidance for harnessing activation sparsity in model design and acceleration."}, "weaknesses": {"value": "This paper's methodology introduces sparsity via the top-p rule and defines critical sparsity as the level at which performance degradation is less than 1%. This is similar to the paper \"Sparsing Law: Towards Large Language Models with Greater Activation Sparsity\". Further comparison of different sparsity definitions is crucial, as a better definition of sparsity will result in less reduction in model performance."}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MDi8pNIiDU", "forum": "I9M5OoOBnI", "replyto": "I9M5OoOBnI", "signatures": ["ICLR.cc/2026/Conference/Submission576/Reviewer_7CAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission576/Reviewer_7CAX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission576/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991702219, "cdate": 1761991702219, "tmdate": 1762915550825, "mdate": 1762915550825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Joint response and rebuttal summary"}, "comment": {"value": "We thank all Reviewers and the Area Chairs for their time, hard work, and inspiring suggestions. We are glad to note that all the Reviewers have acknowledged the soundness of our approach and praised our comprehensive empirical results. We have responded to all the Reviewers, carefully addressed the raised points, and updated a revised version of the paper, which incorporates the proposed improvements (marked in color). The feedback has significantly helped us improve the paper’s readability, clarity, and practical value.\n\nWe summarize the most important rebuttal points and updates made to the paper below:\n\n1. We have extended the paper with an additional analysis of activation sparsity in MoE models in Section 4.5, using Qwen3-30B-A3B as our case study.\n\n2. We significantly improved the readability of the end of Section 4.6 by expanding the analysis of activation sparsity throughout the diffusion process. The added results report sparsity at each diffusion step, providing a clearer context and strengthening the motivation for this experiment.\n\n3. We expanded the analysis of alternative sparsification rules with additional Appendix A2, which includes ablations of the generalized top-p rule. \n\n4. We added Appendix E, which contains a detailed explanation of effective rank computation and makes the paper more self-contained.\n\n5. We clarified the contributions of our work, positioned it more clearly relative to prior research, and addressed several smaller issues raised by the Reviewers, including clarifications on normalization of metrics and the use of the “functional sparsity” term.\n\nWe thank all Reviewers once again and remain eager to continue the discussion if needed. We hope our responses address the Reviewers’ concerns and further reinforce the largely positive initial assessment of our work, underscoring the value and relevance of our work."}}, "id": "P2WJGbXhbM", "forum": "I9M5OoOBnI", "replyto": "I9M5OoOBnI", "signatures": ["ICLR.cc/2026/Conference/Submission576/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission576/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission576/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763749714878, "cdate": 1763749714878, "tmdate": 1763749714878, "mdate": 1763749714878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}