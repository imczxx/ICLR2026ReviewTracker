{"id": "h9b5h69v3p", "number": 12949, "cdate": 1758211959701, "mdate": 1759897474681, "content": {"title": "Discrete Diffusion Trajectory Alignment via Stepwise Decomposition", "abstract": "Discrete diffusion models have demonstrated great promise in modeling various sequence data, ranging from human language to biological sequences. Inspired by the success of RL in language models, there is growing interest in further improving the models by alignment with a certain reward. In this work, we propose an offline preference optimization method to approach trajectory alignment for discrete diffusion models. Instead of applying the reward on the final output and backpropagating the gradient to the entire denoising process, we decompose the problem into a set of stepwise alignment objectives by matching the per-step posterior. This framework enables efficient diffusion optimization, is compatible with arbitrary reward functions, and importantly, yields an equivalent optimal solution under additive factorization of the trajectory reward. Experiments across multiple domains including DNA sequence design, protein inverse folding, and language modeling consistently demonstrate the superiority of our approach. Notably, it achieves an up to 12\\% improvement over the most competitive RL-based baseline in terms of predicted activity on DNA sequence design, and further improves the GSM8K score from 78.6 to 81.2 on LLaDA-8B-Instruct for language modeling.", "tldr": "", "keywords": ["discrete diffusion models", "preference optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea1ab1b6cccce326bc0bed8e885e394e4000a7e7.pdf", "supplementary_material": "/attachment/d861b685eb14960cc441a8814ada6c8476c0956f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an offline fine-tuning method for discrete diffusion, without rely on the reward gradient calculations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall framework and results are technical sound. The proposed fine tuning methods without gradient calculation are important in scientific area."}, "weaknesses": {"value": "- Performances: in Table 1, it’s reported that DRAKES achieve Pred-Activity as 5.61 while proposed SDPO is 6.30. However, in the original paper, DRAKES have two versions (with and without KL), which is 5.61 and 6.44, respectively. Why the authors only pick up the lower value one? If DRAKES can achieve 6.44, does it mean the performance gain by SDPO is low? Since the primary optimization reward here is Pred-Activity, I think this is a big experimental flaw.\n- Baselines: authors may miss some important baselines. For fine-tuning discrete diffusion without gradient, DDPP [1] and DDPO [2] serve as strong baselines. Why not add them as the baseline methods?\n\n[1] Rector-Brooks J, Hasan M, Peng Z, et al. Steering masked discrete diffusion models via discrete denoising posterior prediction[J]. arXiv preprint arXiv:2410.08134, 2024.  \n[2] Black K, Janner M, Du Y, et al. Training diffusion models with reinforcement learning[J]. arXiv preprint arXiv:2305.13301, 2023."}, "questions": {"value": "- In Line 262-263, the authors show that $x_t$ cannot be calculated in offline setting, so that the authors drop it. Although it’s understandable, could the authors provide more in-depth analysis on this? Will this cause big problems?\n- In iterative labeling, authors present that they will update the datasets from latest models. Does it mean that the overall framework is not completely offline anymore?  \n- While I appreciate the authors add diversity quantity in fine-tuning diffusions, in Table 2, why the pretrained model doesn’t have the highest entropy? My understanding is that, no matter how you can keep diversity, during fine-tuning of reward maximization, adding diversity measure from pretrained model of 35.2 to 42.3 seems strange to me. \n- One important measurement is the computation cost. Compared with your baselines, did your methods add additional computation costs, especially the reward calculations? (since scientific reward calculation may be very cost) How about the convergence rate under same computation compared with the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wn9x4D2o5Y", "forum": "h9b5h69v3p", "replyto": "h9b5h69v3p", "signatures": ["ICLR.cc/2026/Conference/Submission12949/Reviewer_KTkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12949/Reviewer_KTkz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981360169, "cdate": 1761981360169, "tmdate": 1762923708375, "mdate": 1762923708375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a preference optimization method for discrete diffusion models using diffusion trajectory alignment. It decompose the alignment problem into a set of stepwise objectives, instead of directly optimization for the reward on the final output. The authors provide the optimality guarantee under the assumption of the additive factorization of the trajectory reward. Experiments on DNA sequence, protein sequence, and language demonstrate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Experiments on different domains demonstrate strong performance across different metrics.\n- The idea to leverage the intermediate step information for alignment of discrete diffusion models is well-motivated."}, "weaknesses": {"value": "- Although claimed as a trajectory alignment algorithm to maximize the stepwise reward, the algorithm does not really use the stepwise reward. Instead, it only uses the reward of the clean sample $r(x\\_0,c)$ . Actually, the trajectory alignment here is more like a simple decomposition of the joint distribution for the data itself, while irrelevant to the reward-tilted part.\n- In Theorem 4.1, the optimality of the trajectory alignment objective is achieved when the reward $\\hat{r}(x_{0:T},c)$ is defined as the complicated chain reward formulation. Therefore, the theorem actually defines a very strong condition specialized for the trajectory alignment objective to prove that the trajectory alignment objective is optimal under this specialized condition. Although we may gain some interesting insights from the condition, the theorem itself is not very meaningful.\n- The final objective relies on the coefficient w(t). Could the author provide more ablations on different choices of w(t) to better demonstrate its effect?\n- The authors turn this trajectory alignment objective into an off-policy preference optimization objective. For general RL algorithms of LLMs, it is known that on-policy RL works better than off-policy RL methods. Why does the author choose off-policy preference optimization as the final objective, instead of an on-policy one, and does SDPO suffer from the same issue as off-policy RL for LLMs?"}, "questions": {"value": "Please refer to the **weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yqULWfOKIw", "forum": "h9b5h69v3p", "replyto": "h9b5h69v3p", "signatures": ["ICLR.cc/2026/Conference/Submission12949/Reviewer_pkbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12949/Reviewer_pkbk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983046255, "cdate": 1761983046255, "tmdate": 1762923707890, "mdate": 1762923707890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work aims to fine-tune a discrete diffusion model (with trajectory-level distribution $p_\\theta(x_{0:T})$) to sample from the tilted distribution  $p_{\\theta}(x_{0:T}) \\exp(\\frac{1}{\\beta}r(x_{0:T}))$. It proposes a surrogate objective which decomposes the trajectory-level problem into subproblems where each posterior over $x_0$: $p_\\theta(x_0|x_t)$ is fine-tuned to the target $p_\\theta(x_0 | x_t) \\exp(\\frac{1}{\\beta_t}r(x_0))$, which side-steps issues with expensive likelihood evaluations over the trajectory. The work focuses on the offline-setting where a reward-labeled dataset (with data drawn from $p_\\theta$) is available, and solves each subproblem using a variant of forward-KL/cross-entropy loss, where samples from the data-set are importance weighted using the reward. The method is shown to outperform other baselines on the tasks of DNA sequence design, protein inverse folding, and preference alignment for language modelling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Side-stepping the trajectory-level nature of discrete diffusion fine-tuning is an important problem, and the proposed method is a useful step in this direction.\n2. The work is well organized.\n3. The preference alignment task in language modelling demonstrates scalability of the method to larger models.\n4. The extension to iterative training is useful.\n5. The ablations section is well-done, and clarifies important aspects of the method (namely its sensitivity to the sample size $N$). \n6. The evaluations are done on a comprehensive set of metrics for DNA sequence design and protein inverse folding."}, "weaknesses": {"value": "1. The method assumes access to $p_\\theta(x_0|x_t)$ - however, in discrete diffusion, we only have access to a factorized approximation (or equivalently, the **mean**) (eg. see section 3 of (Shi et al., 2024)) of this posterior (through $f_\\theta(x_t,t)$). Equation 12 conflates these two things as being equal. \n    - This additionally calls into question the applicability of the theoretical analysis in Theorem 4.1 - does the result still hold for the factorized approximation of the posterior $p_\\theta(x_0 | x_t)$?\n2. The variance of the importance weights (or equivalently the effective sample size) should be evaluated for various tasks - since a reduced sample size appears to be a potential limitation of the method for more challenging rewards.\n3. The bias in using the stepwise approximation should be better analyzed:\n    - Are there real rewards for which the method is unbiased (at sampling from the trajectory level reward-tilted distribution)?\n    - If not, a visualization of how the method is biased would clarify the methods drawbacks (for instance on a toy task where the exact reward-tilted posterior can be visualized).\n\n4. The choice of baselines lack a more standard policy gradient RL method. I understand that the method tackles an offline setting, but I think at least one comparison to such a method (such as diffu-GRPO (Zhao et al., 2025)) would be helpful in understanding the sample efficiency of the proposed method. In particular this is because such methods have been shown to be more scalable to challenging problems than DRAKES.\n\n5. The work is in need of proofreading for various grammatical mistakes.\n\nFor now I am recommending a weak reject, mainly due to point 1 and its impact on the theoretical results (and secondarily, point 4 as an important missing baseline) above. \n\nJiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K Titsias. Simplified and\ngeneralized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024.\n\nSiyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion\nlarge language models via reinforcement learning. 2025. arXiv:2504.12216."}, "questions": {"value": "1. Why not formulate Equation (8) as $D_{\\mathrm{KL}}( p_\\mathrm{ref}(x_0 | x_t, c) \\exp(r(x_0, c)/\\beta_t) \\mid \\mid p_\\theta(x_0 | x_t, c))$ directly? Is there an advantage to the loss as its presented?\n\n2. In Appendix B, it is mentioned that the first stage of training is done on existing/original datasets for fine-tuning. This in theory creates a mismatch with the training objective since it assumes the pretrained model’s distribution perfectly coincides with its pretraining dataset, which is likely not true. Does the performance of the method degrade if samples from the pretrained model are used instead, for the initial fine-tuning stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pHfe62OGx1", "forum": "h9b5h69v3p", "replyto": "h9b5h69v3p", "signatures": ["ICLR.cc/2026/Conference/Submission12949/Reviewer_Gnse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12949/Reviewer_Gnse"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991045172, "cdate": 1761991045172, "tmdate": 1762923707357, "mdate": 1762923707357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an offline preference optimization method for aligning the discrete diffusion models with rewards. By decomposing the loss for trajectory alignment into a set of stepwise objectives, the proposed method achieved efficient optimization. Fine-tuned diffusion models outperform RL-based and inference-time guidance methods in terms of reward optimization while maintaining strong sequence quality/naturalness. The efficacy of the method was verified across different domains including DNA, protein, and natural language."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured and clearly written. The overall presentation flows smoothly and is easy to follow. Most details are clear. \n2. The experiments covered diverse relevant domains and are comprehensively evaluated. Beyond achieving the best rewards among all compared methods, the sequences generated by the propose method also maintain comparable naturalness (close to natural sequences, or low NLL) and diversity (entropy)."}, "weaknesses": {"value": "1. Missing parts in the related work section. Although the authors did experiments with some inference-time guidance methods in the experiments for DNA and protein, I think it is still worth to mention such baselines in the related work about diffusion alignment. The pros and cons of these two different alignment paradigms, i.e., adding guidance during the inference time without training diffusion models like TDS or CG, or fine-tuning diffusion model through preference optimization like SDPO, could be further clarified in the main text. \n2. Although the authors mentioned that dividing a trajectory alignment loss into the objectives for individual steps would make the optimization more efficient, it is not well supported by the experimental results. It could be good if some proxies for training efficiencies, e.g., training speed per batch/convergence speed , are used for comparison between previous preference optimization methods like D2-DPO or DRAKES and SDPO\n3. It would be clearer if the complete training algorithm was listed in the paper."}, "questions": {"value": "1. Some clarification questions about methods details.\n- Are you directly using CG which involves a time-dependent classifier as proposals in TDS? It seems that in the original TDS paper, they were using the reward model on the clean data $x_0$ which was approximated from $x_t$.\n- Is $p_{ref}$ the pertained diffusion model?\n- Is each token of $X_0$ a one-hot vector?\n- In the first two tasks, DNA and protein, what is the number of Monte-Carlo samples $N$? is $c$ in $r(x,c)$ empty?\n2. In the proposed framework, do the reward models always need to be differentiable? From my reading of Eq. 11, differentiability is not necessary.  If not, did you try to apply your method on non-differentiable targets like the docking scores of small molecules?\n3. Could you please provide more intuitions about why the ATAC-Acc of the generated DNA sequences decreases but\nPred-Activity increases with a small β, despite their being positively correlated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f1SffJ8obj", "forum": "h9b5h69v3p", "replyto": "h9b5h69v3p", "signatures": ["ICLR.cc/2026/Conference/Submission12949/Reviewer_WErC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12949/Reviewer_WErC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12949/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762799647862, "cdate": 1762799647862, "tmdate": 1762923706750, "mdate": 1762923706750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}