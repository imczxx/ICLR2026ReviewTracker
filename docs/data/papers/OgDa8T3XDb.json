{"id": "OgDa8T3XDb", "number": 20686, "cdate": 1758309008582, "mdate": 1759896963940, "content": {"title": "Deformable Contact-Aware 3D Object Placement", "abstract": "We study language-guided object placement in real 3D scenes when contact is \\emph{deformable and frictional}. Rather than guessing a rigid pose that “looks right,” we cast placement as a \\emph{drop-to-equilibrium} problem: if the support, scale, and a reasonable pre-drop pose are provided, physics should determine where the object actually rests. Our pipeline, \\textbf{DCAP}, couples language/vision priors with simulation. An LLM extracts the intended support and a realistic size prior; a minimal three-view VLM query returns a single rotation; and a sub-part–aware LLM selects the exact target region, after which we raycast to place the object 1cm above it—no “upward-facing” constraint required. We assign per-part materials by \\emph{hard} mapping of semantic labels to a curated library, split parts into rigid vs.\\ MPM by stiffness, fill soft parts with particles, and then drop to equilibrium with a corotated-elastic MPM solver. To evaluate deformable placement, we convert 186 high-fidelity indoor scenes to watertight meshes by rendering multi-view images from InteriorGS and extracting surfaces with SuGaR. We score methods along two axes—\\emph{Right Place} and \\emph{Physics \\& Naturalness}—using both a human-aligned VLM protocol and forced-choice human studies. DCAP substantially outperforms language-only and rigid-constraint baselines on both axes, produces visible, material-consistent deformations, and correctly flags infeasible instructions. Finally, using DCAP’s settled geometry as conditioning improves downstream 2D insertions, indicating that physically justified final states are valuable beyond simulation.", "tldr": "N/A", "keywords": ["3D Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61739cb3596ced68e869af97deb4a1c6133279af.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for arranging deformable objects using large language models (LLMs). Previous similar studies only considered rigid bounding boxes without accounting for contact-induced deformation. This paper addresses this issue by proposing a collaborative solution involving multiple components. However, the overall writing, formatting, diagramming, and font sizes in the figures severely hinder readability. The provided qualitative experimental demos are also too sparse to reliably assess their generalization capabilities. Furthermore, there is a lack of evidence demonstrating whether the employed LLM can effectively meet the requirements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The research question is novel and meaningful."}, "weaknesses": {"value": "1. The paper writing is too poor. Both the writing and diagrams include significant issues. For example, the font size in the images is too small. These problems make the paper hard to read.\n2. The provided qualitative experimental demos are also too sparse to reliably assess their generalization capabilities. The authors need to provide more evidence to prove the robustness of such a complex system.\n3. There is a lack of evidence demonstrating whether the employed LLM can effectively meet the requirements. The authors need to provide a more complete evaluation of the LLM to prove that the selected LLM has a strong capability to understand the setting."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QbHNYGZ2dw", "forum": "OgDa8T3XDb", "replyto": "OgDa8T3XDb", "signatures": ["ICLR.cc/2026/Conference/Submission20686/Reviewer_57bo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20686/Reviewer_57bo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702984783, "cdate": 1761702984783, "tmdate": 1762934068423, "mdate": 1762934068423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Uses segmentation and VLM queries to try and solve object placement in the problem of 2D image editing, trying to simulate properties of the different objects to give good contacts and realistic deformations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Lots of description on what is being done helps the readers through a difficult method improving the clarity, but this significant usage of page space limits your ability to demonstrate the significance of the method."}, "weaknesses": {"value": "Lack of qualitative evaluations for a visual task, makes it hard to judge this work and understand the improvement over prior works. Even some in the supmat would be sufficient. The results shown so far look interesting and I would like to see more. Any more diagrams that can help more quickly convey the method so that you can spend more time demonstrating your method's results would also be beneficial."}, "questions": {"value": "Are there more evaluations that can be run aside from human preference and VLM evals? This is a task I'm not as familiar with but I'd love to see more quantitative evals if any more benchmarks exist."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "adDrT7qZGP", "forum": "OgDa8T3XDb", "replyto": "OgDa8T3XDb", "signatures": ["ICLR.cc/2026/Conference/Submission20686/Reviewer_k4op"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20686/Reviewer_k4op"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881539188, "cdate": 1761881539188, "tmdate": 1762934067958, "mdate": 1762934067958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an approach to place objects into simulated 3D scenes using language prompts specifying the desired position of the object. From the prompt, the approach makes use of LLMs and VLMs to visually locate the intended support of the object in a rendered image of the scene, as well as segment the object and its support into parts and infer their physical parameters (e.g. density, Young's modulus) to condition the simulator. The approach further uses LLMs to determine the initial position and rotation of the object for a drop into the scene, then rolls out the simulation until a set of convergence criteria are satisfied. The results are presented to a VLM and a set of human study participants for evaluation, outperforming all of the 3 considered baselines on the \"right placement\" and \"physics & naturalness\" metrics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The formulation of placement as \"drop-to-equilibrium\" is a useful idea for applications such as animation or interior design.\nThe considered baselines are deemed to be outperformed by human judges.\nIt is commendable that the authors reimplemented the FirePlace baseline for an additional comparison with a method whose code is not publicly accessible.\nThe work includes an honest discussion of its limitations."}, "weaknesses": {"value": "The presentation of the work can be improved in several points: improving the citation style, spending more effort on visually appealing figures. Several citations have been formatted incorrectly. \nInsufficient examples of the output are provided to allow a sensible assessment of the method's output and performance against baselines. The work could have greatly benefited from the inclusion of supplementary materials in form of videos with qualitative examples of baselines' vs. the proposed method's results, to allow reviewers to form a more informed opinion about its efficacy.\nThe work cites its appendix in several places, yet no appendix has been uploaded.\nThe abstract is too detailed. The textual abstract still contains LaTeX code."}, "questions": {"value": "What is the size of the human study in Table 2?\nIn line 226, you mention that \"More views look thorough but often introduce near–duplicate evidence and confusion\". It seems counterintuitive to me that providing more evidence to the VLM will produce worse results given a reasonable aggregation scheme such as a majority vote. Do you have more evidence to support this claim? \nWhat is the quality of the LLM semantic label-to-material mapping in the \"Giving the simulator honest materials\" step?\nWhich segmentation model is used to segment the image into labeled object instances? How often does this step fail, e.g. due to failure of detection of the object by the segmentation method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hl0CgU3FEs", "forum": "OgDa8T3XDb", "replyto": "OgDa8T3XDb", "signatures": ["ICLR.cc/2026/Conference/Submission20686/Reviewer_YM5r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20686/Reviewer_YM5r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961088207, "cdate": 1761961088207, "tmdate": 1762934067522, "mdate": 1762934067522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DCAP pipeline which combines vision/language priors with simulation. The main contributions of the work are as follows:\n\n-   A novel problem formulation which formulates placement as drop-to-equilibrium, adhering to the physical properties of the materials.\n-   DCAP pipeline that couples LLM/VLM with physical simulators.\n-   A new benchmark that converts 186 high-fidelity InteriorGS scenes into watertight, simulation-ready meshes using SuGaR."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-   [S1] **Novel Problem Formulation:** The primary strength of the proposed work is the novel formulation of object placement as a physics-based equilibrium problem. This formula addresses a clear gap in the existing framework that ignores the physics-awareness of the generation.\n-   [S2] **New Benchmark:** The proposed method makes a valuable contribution by proposing a high-fidelity benchmark for this task. The authors reconstruct the scenes from InteriorGS and extracted meshing using SuGaR.\n-   [S3] **Novel Pipeline:** In the proposed method, high-level semantic reasoning (intent, size and location) is performed by LLMs/VLMs. They utilize a physics simulator to handle the complex contacts and deformations. The proposed method is highly scalable."}, "weaknesses": {"value": "-   [W1] Total inference time is not reported.\n-   [W2] \"Filling Soft Parts so They Behave Like Solids\": The authors do not provide any supporting figures for this. Further, there is no ablation study for this.\n-   [W3] The argument of 1cm is not backed by empirical evidence. What was the reasoning behind choosing this value? This design choice should be thoroughly investigated."}, "questions": {"value": "-   [Q1] Please provide more details on the curated material library (Section 3.3). How many materials are included, and what are the ranges and median values for the key parameters\n-   [Q2] Can the following experiment be done with the TanksandTemples dataset? Train 3DGS on the dataset, obtain the mesh, and use the proposed pipeline. Please provide reasoning on why it cannot be done.\n-   [Q3] Can the proposed pipeline handle out-of-distribution materials?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UNcL8IjrHy", "forum": "OgDa8T3XDb", "replyto": "OgDa8T3XDb", "signatures": ["ICLR.cc/2026/Conference/Submission20686/Reviewer_Mv2Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20686/Reviewer_Mv2Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021597434, "cdate": 1762021597434, "tmdate": 1762934067221, "mdate": 1762934067221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}