{"id": "ERJd7dMN6U", "number": 14847, "cdate": 1758244664627, "mdate": 1763355568398, "content": {"title": "Riemannian Optimization on Relaxed Indicator Matrix Manifold", "abstract": "The indicator matrix plays an important role in machine learning, but optimizing it is an NP-hard problem. We propose a new relaxation of the indicator matrix and compared with other existing relaxations, it can flexibly incorporate class information. We prove that this relaxation forms a manifold, which we call the Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian geometry, we develop a Riemannian toolbox for optimization on the RIM manifold. Specifically, we provide several methods of Retraction, including a fast Retraction method to obtain geodesics. We point out that the RIM manifold is a generalization of the double stochastic manifold, and it is much faster than existing methods on the double stochastic manifold, which has a complexity of \\( \\mathcal{O}(n^3) \\), while RIM manifold optimization is \\( \\mathcal{O}(n) \\) and often yields better results. We conducted extensive experiments, including image denoising, with millions of variables to support our conclusion, and applied the RIM manifold to Ratio Cut, we provide a rigorous convergence proof and achieve clustering results that outperform the state-of-the-art methods. Our Code in here.", "tldr": "", "keywords": ["Optimization", "Clustering", "Graph Cut"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72b7a00b9ff11c8185926a26c115142758444cb7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the RIM (Relaxed Indicator Matrix) manifold as a generalization of the doubly stochastic and single stochastic manifolds. It proves Riemannian geometric properties of this set and develops a full Riemannian toolbox for optimization, including gradient, Hessian, and multiple retraction methods. The method is applied to large-scale tasks like Ratio Cut clustering and image denoising."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a manifold that interpolates between existing relaxations of indicator matrices.\n\n2. The authors develops practical Riemannian algorithms (including a retraction via projection).\n\n3. The authors shows speedups over doubly stochastic manifold optimization in experiments."}, "weaknesses": {"value": "1. The paper defines the RIM manifold as a subset of Euclidean space with strict inequalities: M = {X | X1 = 1, l < Xᵀ1 < u, X > 0}\nHowever, this is an open set, and hence optimization problems posed over this set may have no solution, since the infimum may lie on the boundary (e.g., when X^T 1=u). The paper does not discuss this issue, and Theorem 1 claims it's an \"embedded submanifold\" without clarifying what happens near the boundary. This has major implications — e.g., the projection-based retraction (Theorem 5) may not be well-defined if the minimum lies at the boundary where feasibility breaks.\n\n2. The paper claims that optimization on the Stiefel manifold has time complexity \nO(n^3), but this is misleading. For typical tall matrices (n≫c), the complexity is only O(nc^2), as shown in Wen and Yin (2013). Please correct this and cite:\n\n       -Wen and Yin, A feasible method for optimization with orthogonality constraints, Math. Program. (2013)\n\nAlso, for retraction definitions, please cite standard sources:\n\n       -Boumal, “An Introduction to Optimization on Smooth Manifolds”, 2023, or\n\n       -Absil et al., “Optimization Algorithms on Matrix Manifolds”, Princeton, 2008\n\nIn Theorem 5, the projection used as a retraction may not always be well-defined — the feasibility set is open, and projection may fail to produce a point inside. The paper should acknowledge this and cite related projection-based retractions such as:\n\n       Absil & Malick, Projection-like retractions on matrix manifolds, SIAM J. Optimization, 2012.\n\n3. Every minor step is called a \"Theorem\", including obvious projections and inner product definitions. This makes the paper harder to read. Please distinguish between core theoretical contributions and auxiliary results."}, "questions": {"value": "1. How do you guarantee the solution remains in the interior of the RIM manifold? Do you clip or project back when the optimizer moves outside?\n\n2. Can the projection in Theorem 5 fail if the retracted point lies on the boundary (e.g., due to positivity or sum constraints)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "myrIbHzVUu", "forum": "ERJd7dMN6U", "replyto": "ERJd7dMN6U", "signatures": ["ICLR.cc/2026/Conference/Submission14847/Reviewer_m9ij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14847/Reviewer_m9ij"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840065455, "cdate": 1761840065455, "tmdate": 1762925203392, "mdate": 1762925203392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Relaxed Indicator Matrix Manifold (RIM). The authors prove (M) is an embedded submanifold. They equip (M) with the Euclidean metric restricted to the manifold and derive a simple projection formula for the Riemannian gradient. \nFor retraction, they give two options: (1) a norm‑minimizing projection that they show is a geodesic for small steps (Dykstra‑style algorithm, Theorem 5, and (2) a Sinkhorn‑style mapping characterized by diagonal scalings.\n\nA central claim is lower per‑step complexity on RIM vs the doubly stochastic manifold (DSM) (Table 1).\n  \nExperiments: \n(i) retraction timing favors Dykstra as size grows (Table 2). \n(ii) “Experiment 2” compares RIM vs DSM on two problems: a convex norm‑approximation and TV image denoising.   \n(iii) “Experiment 3” applies RIM to Ratio‑Cut with closed‑form Euclidean gradient/Hessian (Appendix A.9) and shows losses/times vs several baselines\n(iv) “Experiment 4” reports clustering metrics (ACC/NMI/ARI) across datasets (Table 5). \n\nAuthors also provide a convergence proof of RIM‑RGD to stationarity at (O(1/T)) under standard smoothness and Armijo/Wolfe conditions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The RIM construction interpolates between single‑stochastic and doubly‑stochastic models via ([l,u]) bounds, letting practitioners encode prior class information. \n2. Clean Riemannian operators on RIM reduce to simple column‑mean corrections for the gradient and avoid DSM’s pseudoinverses, yielding the (O(nc)) vs (O(n^3)) complexity gap (Table 1).  \n3. Multiple retractions are implemented and compared; Dykstra is both fast at large sizes and, by Theorem 5, induces a geodesic in the small‑step regime.  \n4. Useful coverage of Ratio‑Cut with explicit Euclidean gradient and Hessian (Appendix A.9) that can be projected to RIM. \n5. Convergence guarantees for RIM‑RGD with Armijo/Wolfe steps are provided."}, "weaknesses": {"value": "1. No approximation guarantees to the discrete indicator problem. The paper proves geometry and algorithmic convergence, not approximation quality: there is no integrality gap, rounding guarantee, or conditions under which RIM recovers the discrete optimum. The main theory sections and appendices focus on the toolbox, complexity, convergence, not on approximation bounds. A relaxation cannot just be fast, it must actually be a good approximation to the original problem (discrete indicator).\n2. TV denoising evidence is weak. The claim relies on an ~10% objective gap (1.05e5 vs 1.17e5) and “zoomed‑in” visual inspection; no PSNR/SSIM/LPIPS or even data‑term MSE are reported near those figures/tables [p.8, text around Table 3; Fig.4]. \n3. Compute fairness needs clarification. Speedups depend on stopping rules, retraction choice, and any hyperparameter tuning (e.g., (\\xi) in TV); these details are not fully standardized across RIM vs DSM in the description of Experiment 2. \n4. Choice of ([l,u]) is heuristic and dataset‑dependent. Appendix D.3.5 admits the difficulty, proposes using (n/c) or K‑means proportions, and shows a sensitivity table only for one dataset (MnistData05). \n5. Claims that “RIM images are clearer (regardless of noise level)” (page 47) are again qualitative and not well justified."}, "questions": {"value": "1. Can you provide any approximation or rounding guarantee that connects a stationary point on RIM to a discrete indicator solution (or to DSM) with a bounded increase in the objective?\n2. How should ([l,u]) be picked in practice without prior labels Do you recommend a data‑driven estimator beyond K‑means, and how sensitive are results across datasets?\n3. In Experiment 2, did both manifolds use identical stopping rules and the same retraction and line‑search settings If not, how do the results change when matched?\n4. Can authors provide concrete denoising metrics such as PSNR/SSIM/LPIPS/MSE for RIM vs DSM denoising experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tL9B9ZwA2R", "forum": "ERJd7dMN6U", "replyto": "ERJd7dMN6U", "signatures": ["ICLR.cc/2026/Conference/Submission14847/Reviewer_dz1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14847/Reviewer_dz1d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939217382, "cdate": 1761939217382, "tmdate": 1762925202774, "mdate": 1762925202774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A new relaxation is introduced for the indicator matrix optimization problem, leading to a manifold with a simple structure. Algorithms are designed based on this manifold."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "New retractions proposed and a class of efficient manifold algorithms are developed. Extensive numerical experiments have been conducted to validate the effectiveness of the proposed algorithms."}, "weaknesses": {"value": "Presentation can be improved, e.g., \"Our Code is presented in Appendix H\", \"The proof is included in A.5\", etc."}, "questions": {"value": "It is claimed after (1) that when $l=u=r$, the relaxation becomes $\\\\{X | X1_c=1_, X^T1_n=r, X>0\\\\}$. However, it seems to me that when $l=u=r$, the set in (1) is empty. What the tangent space of the manifold when $l=u$? Can you verify that the proposed retraction is still valid for this case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cRy9YyORVl", "forum": "ERJd7dMN6U", "replyto": "ERJd7dMN6U", "signatures": ["ICLR.cc/2026/Conference/Submission14847/Reviewer_Y5CU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14847/Reviewer_Y5CU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163231632, "cdate": 1762163231632, "tmdate": 1762925202001, "mdate": 1762925202001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank all reviewers"}, "comment": {"value": "Dear AC and PC,\n\nWe sincerely thank all reviewers for their thorough and constructive feedback. We appreciate the opportunity to clarify our work and address the points raised.\n\nWe have provided detailed responses to each reviewer, including new experimental results and theoretical clarifications, which we believe fully address their concerns. We are confident that these clarifications will resolve any misunderstandings and demonstrate the contribution of our paper. It was truly a fantastic experience.\n\nMany thanks again to Y5CU and m9ij for their unique insights and academic discoveries, and to dz1d for rigor and pragmatism. All of this left a deep impression on us. They are definitely top reviewers. \n\nWe look forward to a productive discussion."}}, "id": "It1MdtR6To", "forum": "ERJd7dMN6U", "replyto": "ERJd7dMN6U", "signatures": ["ICLR.cc/2026/Conference/Submission14847/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14847/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission14847/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763023973492, "cdate": 1763023973492, "tmdate": 1763099232400, "mdate": 1763099232400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Authors' Response to Reviews"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank you for your insightful comments and valuable feedback on our paper. We have thoroughly addressed all points raised and have submitted a revised manuscript. All major revisions are marked in green in the updated PDF.\n\nBelow are our point-by-point responses:\n\n### **For Reviewer Y5CU:**\n\n  * We have carefully revised the manuscript's presentation. For instance, Line 24 now reads, \"Our code is presented in Appendix H,\" and the proof statement is, \"The proof is included in A.1.\"\n  * In Lines 192-195, we have briefly addressed the concern raised by both you and Reviewer m9ij, and we have directed readers to Appendix G for a detailed discussion.\n\n### **For Reviewer dz1d:**\n\n  * We now point to our analysis of the optimal solution relationship in the main paper (Lines 317-319), with the full proof provided in the appendix (Lines 1748-1780).\n  * We have added supplementary experiments and a sensitivity analysis for $(l, u)$ in Lines 2461-2490.\n  * In Lines 369-371, we clarify that our other conditions remain consistent with those in RIM and DSM. \n  * We have supplemented the description of the denoising process evaluation (Lines 429-451) and provided the final results (Lines 487-491).\n\n### **For Reviewer m9ij:**\n\n  * In Lines 192-195, we acknowledge the boundary point issue you raised and point to a detailed discussion in Appendix G. We have also added a \"Limitation\" section (Lines 470-478) to clarify that an $\\epsilon$-correction is needed for strict boundary requirements.\n  * We have corrected the time complexity for Stiefel manifold optimization (Lines 38-39, 107-108) and included the correct citation.\n  * We have added the appropriate citation for the definition of \"retraction\" (Lines 160-161). We also cite projection-based retractions and discuss your related query in Lines 192-195.\n  * We have relabeled the original \"Theorems 1, 2, 3, and 4\" as \"Lemmas 1, 2, 3, and 4,\" respectively.\n\n-----\n\nWe once again thank all reviewers for their time and effort in reviewing our manuscript. We hope our revisions have satisfactorily addressed all concerns and look forward to your re-evaluation."}}, "id": "MiNeO8RwfF", "forum": "ERJd7dMN6U", "replyto": "ERJd7dMN6U", "signatures": ["ICLR.cc/2026/Conference/Submission14847/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14847/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission14847/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763355481851, "cdate": 1763355481851, "tmdate": 1763355481851, "mdate": 1763355481851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}