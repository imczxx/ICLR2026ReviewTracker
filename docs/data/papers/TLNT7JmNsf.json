{"id": "TLNT7JmNsf", "number": 2899, "cdate": 1757299061014, "mdate": 1759898120292, "content": {"title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "abstract": "Real robot data collection for imitation learning has led to significant advances in robotic manipulation. \nHowever, the requirement for robot hardware in the process fundamentally constrains the scale of the data.\nIn this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with \\benchmarkName and show significant improvements over baselines and ablate the importance of human data", "tldr": "", "keywords": ["vision-language-action-model", "VLA", "manipulation", "robotics", "human_video", "EgoCentric", "egocentric"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8157c8260eb0ecfe41a8fad9255d5065cfddee01.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents EgoVLA, a new vision-language-action (VLA) model that is trained on large-scale egocentric human video data. The authors have preprocessed the existing egocentric human data to get MANO annotations. Then, EgoVLA is trained to predict the ground-truth MANO parameters, including wrist poses and hand joint angles. Finally, EgoVLA is fine-tuned on a small amount of robot demonstration data. This process involves 1) converting the robot data to human representations for training and 2) mapping from human hand to robot hand in the inference stage. Furthermore, the authors have proposed a new benchmark for evaluating EgoVLA, which is based on NVIDIA Issac Lab. Experiments show that pretraining VLAs on egocentric video data improves task success and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] The paper explores a novel direction how we can train VLA models using egocentric human video data.\n- [S2] The paper proposes a new benchmark for evaluating VLA models, especially based on humanoid robots.\n- [S3] The paper is well-written and easy to understand."}, "weaknesses": {"value": "- [W1] As the authors note in their limitations section, egocentric video data still requires hand and wrist pose annotations, which are difficult to collect. This suggests that egocentric video data faces similar data scalability challenges as robot data collection.\n- [W2] In the experiments, the authors only compare EgoVLA against a simple non-VLA baseline—ACT [1]—making it difficult to assess the true performance gains and whether the proposed method outperforms other strong alternatives, such as π0 [2] and OpenVLA [3]. Benchmarking against such competitive models would strengthen both the validation of the proposed method and the credibility of the benchmark itself.\n\n\n\n**References**\n- [1] Zhao et al., Learning fine-grained bimanual manipulation with low-cost hardware, 2023.\n- [2] Black et al., $\\pi_0 $: A Vision-Language-Action Flow Model for General Robot Control, 2024.\n- [3] Kim et al., Openvla: An open-source vision-language-action model, 2024."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ijE7XdVMHG", "forum": "TLNT7JmNsf", "replyto": "TLNT7JmNsf", "signatures": ["ICLR.cc/2026/Conference/Submission2899/Reviewer_9rjm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2899/Reviewer_9rjm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725706511, "cdate": 1761725706511, "tmdate": 1762916435014, "mdate": 1762916435014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of learning vision-language-action (VLA) models from egocentric human videos. The proposed model predicts human wrist and hand poses, which are subsequently retargeted to robot actions using inverse kinematics. A small number of robot demonstrations are employed for model fine-tuning. To assess the framework’s effectiveness, the authors introduce a simulation benchmark for systematic evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Leveraging human videos to learn dexterous manipulation policies is a promising direction, particularly given the high cost and effort required to collect teleoperation data.  \n\nAligning the action space between humans and robots through rigid 3D transformations and retargeting is a sound and well-motivated design choice.  \n\nOverall, the proposed method demonstrates strong performance based on the reported evaluations."}, "weaknesses": {"value": "The full system is evaluated only in simulation, and the domain gap between simulation and the real world is not adequately addressed.  \n\nMoreover, the method still requires task-specific robot data for fine-tuning, which limits its scalability and generalization potential."}, "questions": {"value": "Human motions are highly versatile, and some may be infeasible to transfer to robotic embodiments through inverse kinematics or retargeting due to morphological or dynamic mismatches. How do the authors plan to handle such cases? \n\nWhat are the common failure modes of the proposed method? A detailed analysis of these limitations would help readers better understand the strengths and weaknesses of the approach. \n\nHave the authors explored zero-shot deployment of the system---i.e., using only human data without robot fine-tuning---to assess its generalization capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Aa4VOaqLuS", "forum": "TLNT7JmNsf", "replyto": "TLNT7JmNsf", "signatures": ["ICLR.cc/2026/Conference/Submission2899/Reviewer_4urF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2899/Reviewer_4urF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851121291, "cdate": 1761851121291, "tmdate": 1762916434853, "mdate": 1762916434853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a VLA model capable of learning bimanual robot manipulation from egocentric human demonstration videos. This is performed by defining a common representation consisting of wrist pose and hand pose.\n\nTo this end, the authors build a dataset consisting of four publicly available subsets and perform preprocessing making the data suitable for training. This consists of harmonizing data, performing camera motion compensation, adding language placeholders, and providing hand pose in the MANO format.\n\nIn a first stage, the VLA model is trained on human data, predicting the future sequence of wrist pose and hand pose. In a finetuning stage, the model is trained on a small robot dataset, predicting handposes that are retargeted to the robot pose. For deployment on a humanoid robot, hand poses are converted to robot joint parameters using an MLP.\n\nThe model is evaluated on the proposed Ego Humanoid Manipulation Benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The model allows joint learning from robotic and non-robotic data, approaching a core problem of VLA training\n- The proposed VLA is well engineered matching human hand representations with dexterous robot manipulators at each training stage\n- The model shows improved results over baseline methods on the proposed Ego Humanoid Manipulation Benchmark"}, "weaknesses": {"value": "- The method only utilizes simple human demonstrations with clearly visible hands in simple environments. No results on day-to-day demonstrations are presented, limiting the potential advantage of reduced dataset collection cost. Showing generalization to these scenarios could potentially be done on datasets like Ego4D or Epic Kitchens.\n- EgoVLA is purely evaluated on simulation results. With VLA models often showing considerably different performance on real-word deployment, this shows little evidence of the model's actual performance.\n- The proposed Ego Humanoid Manipulation Benchmark is not validated to correlate with real-world robot performance. While a direct match between simulation and real-world performance is often not given, the benchmark should be validated to ensure method rank consistency."}, "questions": {"value": "- The visualization show well detected hand poses. It would be interesting to understand how sensitive model training is to incorrect detections in real-world scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JjokdLVfmG", "forum": "TLNT7JmNsf", "replyto": "TLNT7JmNsf", "signatures": ["ICLR.cc/2026/Conference/Submission2899/Reviewer_v3pr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2899/Reviewer_v3pr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997124451, "cdate": 1761997124451, "tmdate": 1762916434573, "mdate": 1762916434573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores whether we can train manipulation policies without relying on large amounts of robot data.  The authors argue that robot action spaces can be close enough. Human hand and wrist motions can be converted into robot actions using simple geometric transforms like inverse kinematics and hand retargeting. The authors train a VLA model purely on egocentric human videos and then do a small amount of robot fine-tuning, resulting in a robot-ready policy called EgoVLA. They also introduce a new bimanual manipulation benchmark in Isaac Sim with diverse tasks and demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addressed an interesting and important problem.\n\n* The method proposed in this paper is intuitive, and the paper is easy to follow.\n\n* The proposed benchmark, although limited, is valuable. Within this benchmark, the proposed method outperforms both specialist and generalist baselines and generalizes better across viewpoints and object positions."}, "weaknesses": {"value": "* The main limitation is that validation is mostly in simulation. While the contributions may still be meaningful for scaling robot learning with accessible human video data, the current justification based on only the simulation is not convincing. This is particularly important because the argument made in this paper is bold, which can also be justified through real-world experiments. In other words, the made claims may be true only within this paper's experimental settings. \n\n* While being intuitive and desirable, it is hard to admit the closeness of the robot and human action spaces. This is primarily due to the make of the human hands being unlike of robots, therefore the offered degree of freedoms and the articulation, flexibility, and dexterity of human hands. Certain actions may simply be infeasible for robots that are very simple for human hands. In some constrained setting, it may, however, be true what the paper argues. However, that line is not made clear; therefore, such a bold claim is not to be taken lightly.\nThis claim is particularly worrisome, as it turns out to be empirical  (besides being intuitive, thus possibly deceptive) and the experimental claims are made through the self-made simulation benchmark."}, "questions": {"value": "* Please delineate the claim similarity of the action spaces of humans and robots in more formal terms, while acknowledging the fact that the current claim is not true in general settings with current robots (or the one used in the experiments).\n\n* The main limitation is the real robot experimental results. This is particularly important for the argument put forward in this paper. Without those experiments, the paper cannot be accepted at its current stage, in addition to the requested delination. \n\n* Please provide the details on how the ego-motion is compensated to bring the hand pose in the first view reference frame. Are the ground truth camera poses available at all times?\n\n* Ablation study: while using the wrist pose, is the location of the wrist sufficient to achieve the final results? Do the other pose variables (rotation) play a vital role as well? Is the wrist pose and end effector assumed to be aligned? If so,  how easy is such alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gBD1LWDP8S", "forum": "TLNT7JmNsf", "replyto": "TLNT7JmNsf", "signatures": ["ICLR.cc/2026/Conference/Submission2899/Reviewer_heQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2899/Reviewer_heQB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067535785, "cdate": 1762067535785, "tmdate": 1762916434408, "mdate": 1762916434408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}