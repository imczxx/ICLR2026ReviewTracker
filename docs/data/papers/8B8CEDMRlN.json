{"id": "8B8CEDMRlN", "number": 18001, "cdate": 1758282782982, "mdate": 1759897140094, "content": {"title": "Rethinking Cross-Lingual Gaps From A Statistical Viewpoint", "abstract": "Any piece of knowledge is usually expressed in one or a handful of natural languages on the web or in any large corpus. Large Language Models (LLMs) act as a bridge by acquiring knowledge from a source language and making it accessible when queried from target languages. Prior research has pointed to a cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a target language compared to when the query is in the source language. Existing research has rationalized divergence in latent representations in source and target languages as the source of cross-lingual gap. In this work, we take an alternative view and hypothesize that the variance of responses in the target language is the main cause of this gap. For the first time, we formalize the cross-lingual gap in terms of bias-variance decomposition. We present extensive experimental evidence which support proposed formulation and hypothesis. We then reinforce our hypothesis through multiple inference-time interventions that control the variance and reduce the cross-lingual performance gap.", "tldr": "Cross-Lingual Gaps are not due to Knowledge Barriers", "keywords": ["Cross-lingual gaps", "LLMs", "bias-variance", "domain adaptation", "statistical learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46ad867dac53d1a5cc23f304913e592a9d5da594.pdf", "supplementary_material": "/attachment/7b33649908151fdb2dc5082614319fc882b73a4f.zip"}, "replies": [{"content": {"summary": {"value": "The study aims to understand the cause for poor cross-lingual transfer performance from a bias-variance perspective. The study claims that variance in the responses in the target language causes the cross-lingual transfer gap. Formalize bias and variance in cross-lingual transfer from source and target language responses and provide experimental results using inference-time method to reduce variance in the target language."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- identifying the cause for the cross-lingual transfer gap is very relevant and interesting\n- the study views the problem from a variance-bias perspective, which is novel to the best of my knowledge \n- the proposed inference-time approaches exposing the model to multilingual inputs (and asking them to explicitly translate inputs) show promising results in cross-lingual transfer performance"}, "weaknesses": {"value": "- The manuscript is not well structured and hard to follow. Experimental setup and evaluation methodologies are presented in the results section. The discussion of the results is very short and relevant parts of the discussion were moved to the appendix.\n- The plots in Figure 6 seem misleading: the x-axis are inconsistent, as they should all range from 0.0 to 1.0. Also, there is no description in the caption as to how the displayed variance functions were computed. While there seems to be a trend, it is not as clear as the figures may suggest in (a) and the sample size in (b) is very low\n- Inference-time methods TrEn-k and TTA are not well motivated and embedded in the rest of the manuscript and discussions. \n- The derived conclusions, i.e. reducing variance in the source language reduces variance in the target language, does not hold following the conclusions in Appendix I."}, "questions": {"value": "- PCA plot in figure 1, b is underspecified: what representations are used to compute this pca plot?\n- Why would you expect responses in different target languages to exhibit similar variance as responses in a single source language?\n- To test the hypothesis that target language variance is proportional to source language variance, I'd suggest to analyze languages pairwise, instead of the aggregated figures and values presented in table 1 and figure 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PKMM35wYOF", "forum": "8B8CEDMRlN", "replyto": "8B8CEDMRlN", "signatures": ["ICLR.cc/2026/Conference/Submission18001/Reviewer_RNgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18001/Reviewer_RNgW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844619138, "cdate": 1761844619138, "tmdate": 1762927797032, "mdate": 1762927797032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, this paper solved the problem of understanding the causes behind cross-lingual performance gaps in multilingual LLMs. This paper proposed a statistical framework based on bias–variance decomposition, hypothesizing that cross-lingual gaps arise primarily from increased variance in target languages, rather than biases. Through formal analysis and experiments on multiple benchmarks (ECLeKTic, MMLU with mixup), the authors show that ensembling and inference-time variance control significantly reduce these gaps, suggesting that knowledge itself transfers well but confidence does not."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) Presents a novel and rigorous statistical perspective (bias–variance decomposition) on the cross-lingual gap problem.\n\n2) Strong empirical validation using multiple benchmarks and large models (Gemini, GPT-5, DeepSeek).\n\n3) Well-designed experiments with both response and input ensembling to test the hypothesis.\n\n4) Clear contributions and discussion showing practical mitigation strategies that are inference-time only."}, "weaknesses": {"value": "1) This work focuses on inference-level variance, I'm curious if this variance/bias stil exists during training.\n\n2) The assumption that representation bias is negligible may oversimplify real-world multilingual disparities, especially for low-resource languages.\n\n3) Experimental scope is limited to well-represented languages; findings may not generalize to unseen or underrepresented ones."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gRH8TSWqUT", "forum": "8B8CEDMRlN", "replyto": "8B8CEDMRlN", "signatures": ["ICLR.cc/2026/Conference/Submission18001/Reviewer_goJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18001/Reviewer_goJz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872031431, "cdate": 1761872031431, "tmdate": 1762927796446, "mdate": 1762927796446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate gaps in knowledge (and therefore performance) on tasks where the only difference is the language in which the query is posed in and adopt a bias-variance framework to explain the gap. \n\n# Method\n\nLogits from source language modeled as RVs from a Gaussian distribution and target logit drawn from bias and variance components, linearly weighed. \nFrom Prop 1 and 2: \n\n\"Reducing the radii (√variance) will make the average responses from source and\ntarget agree more often only when there are no biases\"\n\nFrom Prop 3.:\n\"\n1.When the source confidence is high, i.e., (μ0 −μ1)/pσ2s + 2 ≫ 1 then the target confidence must\nalso be high based on Proposition 3.\n2.Since source and target confidence are related, we should see increasing agreement (or suppressed\ncross-lingual gap) as confidence in source increases.\n\"\n# Results\n\nThe authors test on ECLeKTic and MMLU (w/ language mixup)\n \nTo show the gap is due to variance: embedding the L2 distance between source and target languages reduces with increasing ensemble size. Similarly, authors show decreasing Chi-square distance over multiple-choice with increasing ensemble size. Similalry, the authors do a variant of ensembling i) multiple translations presented at once and ii) translated then answer; only i) improves performance.\n\nOn ECLeKTic, authors show \"High confidence in source leads to high confidence in target\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors take a principled approach to investigate the variance hypothesis and show the results on frontier performance. \n2. Section 4.2 and Prop 3. results are strong and align with the presentation and inference around it."}, "weaknesses": {"value": "1. The authors skip a lot of existing strategies (train and inference time) that people have looked at [1-3, and many others]. Discussion of the proposed framework and analyzing results from these works seems critical\n2. In-line with the above comment, Section 4.1 results, especially 4.1.2, the results are a) not novel; b) novelty aside, TTA-results are extremely surprising, especially with the bigger models and attributing it failure to follow instructions doesn't seem satisfactory (unless all prompt optimizations were conducted and the models still fail to do so, which is not detailed anywhere and unlikely that is happening). \n3. There is more to the results in 4.1 - there is everything from in-context learning, few-shot examples, impact of language similarity (intermediate languages), etc., all considered in previous works have to be investigated, and similar results /observations seem important. \n4. Another missing analysis is correlation to language maps, where the distance between languages is calculated [4]. Previous works have shown these strategies to work, and this framework can help explain some of those empirical observations.\n5. Analysis around scale of models - deeper analysis across model families should strengthen the findings.\n\n\n[1] Kumar, Somnath, et al. \"Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs.\" Proceedings of the 31st International Conference on Computational Linguistics. 2025.\n[2] Agrawal, Ashish Sunil, Barah Fazili, and Preethi Jyothi. \"Translation errors significantly impact low-resource languages in cross-lingual learning.\" arXiv preprint arXiv:2402.02080 (2024).\n[3] Wang, Weixuan, et al. \"Bridging the language gaps in large language models with inference-time cross-lingual intervention.\" arXiv preprint arXiv:2410.12462 (2024).\n[4] Littell, Patrick, et al. \"URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors.\" Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. 2017."}, "questions": {"value": "Please look at the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0TJ7lF9H7", "forum": "8B8CEDMRlN", "replyto": "8B8CEDMRlN", "signatures": ["ICLR.cc/2026/Conference/Submission18001/Reviewer_kN1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18001/Reviewer_kN1s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943437008, "cdate": 1761943437008, "tmdate": 1762927796086, "mdate": 1762927796086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the cross-lingual transfer by examining the output distribution. The authors argue that the source and target languages in cross-lingual transfer share the response space, and the variance in the source language transfers to the target language. As a result, if the variance in the source language is low, the cross-lingual transfer is strong."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tCross-lingual transfer is a key feature of multilingual LMs, but it is not fully understood. Studies of this feature help the community design multilingual LMs to support more languages. \n\n2.\tThe authors study the cross-lingual transfer in a black box and connect the sampling process with cross-lingual transfer, which is interesting. \n\n3.\tPresentation is clear."}, "weaknesses": {"value": "1.\tWhile the idea is interesting, I have some general concerns or questions:\n\n- The language bias or the language modeling performance is not identical for all languages. This might be a confounding factor in the study as the entropy or the output variance is different for all languages. One actionable suggestion here, consider studying a high-resource language to another high-resource language, high-resource to low-resource,  low-resource to high-resource, and low-resource to high-resource.\n- The experimental design for ECLeKTic is not clear. Throughout all the paper, I assume the authors attempt to analyze the variance in the logits. However, for ECLeKTic, the authors consider the embedding distance via an external embedding model. This is confusing as it does not support the main claim of this paper.\n\n2.\tThe prompts in the experiments are not intuitive and clear.  My understanding here (via multiple reading rounds; correct me if necessary ),  you prompt ten times for each language and compute the variance across languages. What is the intuition of using TrEn-k and TTA baselines as baselines?"}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gqm8fgM2KZ", "forum": "8B8CEDMRlN", "replyto": "8B8CEDMRlN", "signatures": ["ICLR.cc/2026/Conference/Submission18001/Reviewer_Z4rW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18001/Reviewer_Z4rW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762880847635, "cdate": 1762880847635, "tmdate": 1762927795320, "mdate": 1762927795320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}