{"id": "I3W8PynQU0", "number": 19759, "cdate": 1758299065204, "mdate": 1759897021096, "content": {"title": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning", "abstract": "Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce **MEAL** (**M**ulti-agent **E**nvironments for **A**daptive **L**earning), the first benchmark tailored for continual multi-agent learning. Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of up to 100 tasks on a standard desktop PC within a few hours. Evaluating popular CL and MARL methods reveals that naïvely combining them fails to preserve network plasticity or prevent catastrophic forgetting of cooperative behaviors.", "tldr": "The first benchmark for continual multi-agent reinforcement learning", "keywords": ["continual learning", "multi-agent", "overcooked", "benchmark", "reinforcement learning", "cooperative"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fb7d6c1922c9b49e08094a9d9af058f7e6c4321.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a benchmark for a Continual Multi-agent Reinforcement Learning. The benchmark environment is based on the “Overcooked” environment, written in JAX to enable massive learning time speedup, which in turn allows for the testing of more tasks. Authors also implemented several multi-agent RL methods and various methods used in the Continual RL setting, and evaluated their implementations across multiple configurations of the environment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clear, well-written, and coherent.\n* Due to the nature of the Continual RL domain, there is a need for speeding up the environments, as training has to span multiple tasks, which drives training times to be long. The speed achieved by MEAL seems to be impressive, and allows for in-depth algorithm analysis with a data-rich setup\n* Authors included several ablations and smaller experiments, which help to paint a more complete picture of how good current baselines are, and what challenges future algorithms will have to address."}, "weaknesses": {"value": "I have several smaller issues with the paper that I believe should be addressed by the authors.\n\n* The authors claim that MEAL is the first Continual Multi-agent RL benchmark that uses JAX for end-to-end training. I have found another GitHub project that appears to address the same setting, also utilizing an overcooked environment: [https://github.com/aialt/overcooked-jax](https://github.com/aialt/overcooked-jax). This project appears to differ significantly from the author-provided code in the anonymous GitHub repository, so I assume it is not the same project. Could the authors clarify what the differences are between this project and their contribution, and how the novelty claim holds in the presence of that other repository?  \n* My understanding is that JaxMARL([https://github.com/FLAIROx/JaxMARL](https://github.com/FLAIROx/JaxMARL)) already provides an overcooked environment for Multi-agent RL setup, and the authors acknowledge that their contribution is built on top of that project. Effectively, the benchmark provided is simply a level generator for the JaxMARL overcooked environment. In my opinion, while this benchmark can be used to assess different baselines and is useful, it should provide more than just a level generator for one environment. I would love to see at least one more environment.  \n* In section 4.5 authors write:  \n  “We therefore normalize the delivery count by the optimal cook-deliver cycle for a single agent on any given task”.  \n  I see the value in that metric, and indeed, in many scenarios, it can provide meaningful information on whether RL agents can effectively cooperate. The level validator that authors use rejects any level that cannot be solved by a single agent. This makes sense in the context of the above metric, because if a level is unsolvable by a single agent, the metric loses its meaning. But I believe that authors should include another level generator that forces the cooperation between the agents, i.e., the level is unsolvable by a single agent. This could be treated as a separate environment or incorporated as a level that is generated as part of the benchmark. Forced cooperation appears to be an important feature in a benchmark like this, and I don’t see any reason not to include it.  \n* The paper is well-written and clear for the most part, but I think that in section 4.1, the authors should add 1-2 sentences of explanation of the goal of the overcooked environment, and what is the meaning of onions, plates, deliveries, etc. Alternatively, authors could place this explanation in the appendix and refer to it; however, either way, the reader's understanding of the basic environment dynamics should not require them to look beyond the reviewed paper.  \n* The fact that the main experiments use multi-headed architectures (one head for each task) is mentioned for the first time in section 5.2 \\- Ablation Study. I think that this should be mentioned earlier (section 5.1 perhaps?), as it is an important (although natural) choice.  \n* In the abstract, the authors claim:  \n  “MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours.”  \n  However, most tasks used in the paper are 20\\. There should be at least one experiment in which authors push the benchmark to the limits and use 100 tasks, which they claim is possible.  \n* In Appendix B2, the hyperparameter for parallel envs is only 16\\. This seems peculiar, as the primary speedup from using JAX is the parallelization across a number of environments. In similar works that use JAX for fast benchmarks, the authors typically use environments in the order of thousands or tens of thousands (for example, in the JaxMarl blog post, the authors compare 100, 1k, and 10k environments). What is the reason for such a small number? Have the authors tested larger numbers to achieve higher GPU utilization, and consequently greater learning speed?"}, "questions": {"value": "Although I don’t think it is necessary in this work, I would like to see a study of how scaling of the network sizes impacts performance. In the current version, the authors use 2x128 MLP networks, which are small even for RL algorithms. Especially in light of recent scaling results (for example, https://arxiv.org/abs/2405.16158, https://arxiv.org/abs/2410.09754), I think it is important to leverage the speed of JAX to provide some indication of whether scaling the networks achieves any performance gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mfQCVyDQLJ", "forum": "I3W8PynQU0", "replyto": "I3W8PynQU0", "signatures": ["ICLR.cc/2026/Conference/Submission19759/Reviewer_tMtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19759/Reviewer_tMtN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665938258, "cdate": 1761665938258, "tmdate": 1762931598665, "mdate": 1762931598665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MEAL (Multi-agent Environments for Adaptive Learning), a benchmark for continual cooperative multi-agent reinforcement learning (CMARL). MEAL builds upon the Overcooked environment and leverages JAX for GPU-accelerated simulations, enabling scalable training across sequences of up to 100 tasks. The benchmark offers procedural environment generation, controllable difficulty levels, and supports evaluation of both full and partial observability. Six continual learning (CL) baselines (EWC, Online EWC, L2, MAS, AGEM, FT) are implemented and systematically compared. The authors show that while classical CL methods can retain performance in simple settings, they struggle as environment complexity and coordination demands increase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Timely and relevant benchmark: Addresses the underexplored intersection of continual learning and cooperative multi-agent reinforcement learning, filling a clear gap in existing benchmarks.\n- GPU-accelerated simulation: The use of JAX for end-to-end GPU-based training is a notable practical advancement, drastically reducing wall-clock training time and enabling long task sequences (up to 100 tasks on a single GPU).\n- Comprehensive baseline coverage: Six continual learning baselines implemented in JAX provide a solid foundation for reproducible comparisons and future benchmarking.\n- Clear and organized structure: The paper is well-written and systematically walks through environment generation, difficulty scaling, and evaluation metrics.\n- Strong empirical evaluation: Provides clear analyses of forgetting, forward transfer, and average performance across difficulty levels, and meaningful visualizations (Figures 3–5).\n- Procedural task generation: Ensures a potentially infinite task space with controlled difficulty, supporting reproducibility through seeded generation.\n- Thoughtful design of variants: Inclusion of curriculum and repetition settings\n- Insightful ablations: The analysis of multi-head architectures and plasticity loss (Figures 6 and 8) provides valuable insights into continual adaptation in MARL.\n- Technical rigor and reproducibility: The inclusion of environment generation algorithms (Appendix A) and ablation studies (e.g., Figure 6) shows commendable transparency and attention to implementation detail.\n- Code is provided and contains relevant details, including scripts to reproduce the plots."}, "weaknesses": {"value": "- Claim of first CMARL benchmark requires clarification: The authors state (line 039) that MEAL is the first continual MARL benchmark, but works such as “Multi-Agent Continual Coordination via Progressive Task Contextualization” (Yuan et al., 2024) already explore continual multi-agent coordination.\n- Task ID dependence (line 305 & Figure 6): Since the ablation suggests task IDs have negligible effect, it would strengthen the benchmark if baseline results without task ID access were also reported for completeness and fairness.\n- Limited task diversity: The benchmark is confined to a single Overcooked cooperative domain with discrete action spaces, which contrasts with more general-purpose benchmarks (e.g., Melting Pot, Continual World) that support broader environmental and control variations.\n- Accessibility for new readers: Line 047: The paper assumes prior familiarity with Overcooked; a brief intuitive explanation of its mechanics and why layout changes matter would improve clarity.\n- Lines 080–090: Acronyms such as EWC, MAS, and PackNet appear without definition in the main text; a short reminder or a glossary table would help readers less familiar with continual learning."}, "questions": {"value": "- Clarification on novelty claim: Please explicitly discuss how MEAL differs from Yuan et al. (2024) and other CMARL-like studies that included continual coordination experiments.\n- Baselines without task IDs: Since Figure 6 shows the negligible effect of task IDs, a results table for “no-task-ID” settings would provide valuable insight into real continual adaptability.\n- Explanation of Overcooked: Add a short introductory paragraph early in the paper (around line 047) explaining what Overcooked is and why its layout variability makes it suitable for continual learning.\n- Define acronyms early: Ensure EWC, MAS, PackNet, etc., are defined upon first use (lines 080–090).\nPer-task evaluation diversity: Why is EWC the only baseline visualized in per-task evaluation (line 297)? Showing others could reveal different forgetting or transfer dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vW3whDh2kd", "forum": "I3W8PynQU0", "replyto": "I3W8PynQU0", "signatures": ["ICLR.cc/2026/Conference/Submission19759/Reviewer_JNx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19759/Reviewer_JNx1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814628573, "cdate": 1761814628573, "tmdate": 1762931597339, "mdate": 1762931597339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MEAL, a new benchmark for continual multi-agent reinforcement learning based on the Overcooked cooperative multi-agent RL environment. MEAL adds procedural generation to Overcooked, a grid world, to vary grid size and obstacle density. MEAL also uses JAX for GPU acceleration, enabling the benchmarking of continual MARL over longer task sequences on a single desktop GPU in a few hours. Experiments by naively combining continual RL methods with standard MARL methods show that approaches fail in more complex continual MARL settings where non-stationarity arises from both multi-agent dynamics and the task distribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The problem setting is interesting and novel. I am not aware of any prior work considering the continual multi-agent reinforcement learning paradigm.\n\n- The paper is generally well-written and organized. The experiments use standard continual RL evaluation metrics including forgetting and forward transfer."}, "weaknesses": {"value": "1. MEAL only considers non-stationarity in changing environment layouts (Line1326) while the observation space and action space are the same between \"tasks\" in MEAL. Given this, I am not certain if the setting studied in this paper is truly continual RL. In particular, this distribution shift from procedural generation is most similar to individual environments in Procgen [1]. For comparison, Jelly Bean World [2] uses an infinite grid world to achieve non-stationarity, Continual World [3] uses different objects in the scene, and CORA [4] uses different tasks within game environments (which have different observation space, action space, and environment dynamics altogether). \n\n2. Table 1 claims that MEAL covers infinite tasks, yet only 3 difficulty levels are studied. Use of procedural generation is not sufficient to claim infinite tasks, given that some of the other benchmarks in the table also use environments with procedural generation such as Procgen.\n\n3. The paper primarily focuses on a multi-head output setting, instead of using a shared output head. Ablation in Figure 6 shows that EWC completely fails on the MEAL benchmark when using a shared output head. Using individual heads while claiming that MEAL spans an infinite number of tasks (Table 1) would result in unbounded memory usage to store additional model parameters. Furthermore, careful evaluation of the shared output head setting may show that other methods (ie. replay-based vs. penalty-based) outperform EWC. CLEAR, a replay-based method, drastically outperforms CLEAR on single-agent continual RL when using a shared output head.\n\n[1] https://arxiv.org/abs/1912.01588 \n\n[2] https://arxiv.org/abs/2002.06306\n\n[3] https://arxiv.org/abs/2105.10919 \n\n[4] https://arxiv.org/abs/2110.10067\n\n[5] https://arxiv.org/abs/1811.11682"}, "questions": {"value": "1. It would be beneficial for MEAL to also evaluate changing environment dynamics, rather than simply environment layout within Overcooked. This may be achieved by adding \"kitchen\" / \"recipe\" formats beyond the current onion soup recipe. Another path to consider would be to add multi-agent support for other JAX-based grid world environments such as Craftax [1], Naxvix [2], or XLand-Minigrid [3]. Or to directly incorporate other multi-agent RL environments built in Jax such as MAC [4] or SocialJax.\n\n2. Regarding Table 3, how easy is it to scale MEAL to more than 3 agents? Given the performance boost from Jax, what about training with 10 agents? 100 agents? \n\n[1] https://arxiv.org/abs/2402.16801\n\n[2] https://arxiv.org/abs/2407.19396\n\n[3] https://arxiv.org/abs/2312.12044\n\n[4] https://arxiv.org/abs/2503.14576\n\n[5] https://arxiv.org/abs/2503.14576"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7onRRjfA6h", "forum": "I3W8PynQU0", "replyto": "I3W8PynQU0", "signatures": ["ICLR.cc/2026/Conference/Submission19759/Reviewer_n4Rh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19759/Reviewer_n4Rh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927690706, "cdate": 1761927690706, "tmdate": 1762931596015, "mdate": 1762931596015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MEAL is a benchmark for continual multi-agent reinforcement learning. This suite of JAX environments evaluates continual learning methods in 100 tasks and demonstrates that simple combination of MARL and CL, struggle to complex settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "There is a growing need for increasingly numerous tasks in CRL and Multi-task RL, which this benchmark addresses. The presentation of results and discussions are strong and clear, with useful ablations and performance measures (forgetting, dormant ratio, etc). The paper proposes numerous interesting appendices and adjacent settings such as curriculum learning are a nice addition."}, "weaknesses": {"value": "The claim it is the first continual RL library to leverage JAX should carefully distinguish itself from existing, non-benchmark libraries, such as ReDo (https://github.com/google/dopamine/tree/master/dopamine/labs/redo). Although PyTorch based, Plasticine is also a relevant comparison (https://github.com/RLE-Foundation/Plasticine/tree/main). Whilst there are clearly defined contributions in procedural generation and establishing CL, POMDP and Curriculum settings, the framework seems to also re-implement a lot of functionality from the Overcooked Carrol et al. (2019) paper which limits it's novelty. The claim that MEAL has infinite tasks, whilst true, is somewhat misleading as the diversity between tasks is also of significance. I.e the tasks in Continual World are very distinct, which is perhaps why algorithms trained on MEAL are able to perform well even after numerous task changes. The relatively minor increment in dormancy and plasticity-loss makes it harder to evaluate CL methods against each other.\n\nSimilarly using different output heads for different tasks means a portion of the parameters aren't experiencing any distribution shifts, which is why many works omit this privileged information. Whilst the proposed baselines are relevant (albeit slightly dated), it would be interesting to see an optimization method (CBP, ReDo, or related)."}, "questions": {"value": "Are you able to elaborate on the intuition as to why harder tasks cause greater plasticity loss? Is there greater shifts in task distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AOGupKrCzV", "forum": "I3W8PynQU0", "replyto": "I3W8PynQU0", "signatures": ["ICLR.cc/2026/Conference/Submission19759/Reviewer_gMyk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19759/Reviewer_gMyk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19759/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933461286, "cdate": 1761933461286, "tmdate": 1762931595030, "mdate": 1762931595030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}