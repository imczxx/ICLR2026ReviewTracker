{"id": "DcHGEcqdFf", "number": 15985, "cdate": 1758258071100, "mdate": 1763706360953, "content": {"title": "Disentanglement of Variations with Multimodal Generative Modeling", "abstract": "Multimodal data are prevalent across various domains, and learning robust representations of such data is paramount to enhancing generation quality and downstream task performance. To handle heterogeneity and interconnections among different modalities, recent multimodal generative models extract shared and private (modality-specific) information with two separate variables. Despite attempts to enforce disentanglement between these two variables, these methods struggle with challenging datasets where the likelihood model is insufficient. In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to explicitly address this issue, with rigorous mutual information-based regularizations, including cross-view mutual information maximization for extracting shared variables, and a cycle-consistency style loss for redundancy removal using generative augmentations. We further introduce diffusion models to improve the capacity of latent priors. These newly proposed components are complementary to each other. Compared to existing approaches, IDMVAE shows a clean separation between shared and private information, demonstrating superior generation quality and semantic coherence on challenging datasets.", "tldr": "", "keywords": ["Multimodal Variational Autoencoder", "Disentanglement", "Multi-view Information Bottleneck", "Diffusion Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cda600f40420f3ef2868b68ec0aed41141d153ef.pdf", "supplementary_material": "/attachment/75eff227445919da6c2a2f98d74e36227b8559ca.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces Information-Disentangled Multimodal VAE (IDMVAE), a new multimodal generative framework designed to improve disentanglement between shared and modality-specific representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The cross-view regularization for disentangling the shared latent variable z and modality-specific w is well-motivated.\n\n2. The experiments consider cross-domain data, and the ablation studies are complementary.\n\n3. Using diffusion to model latent priors results in better alignment between prior and posterior distributions, leading to improved unconditional generation and coherence."}, "weaknesses": {"value": "1. Compared to multimodal VAEs, the novelty is based on the cross-view regularization. For the diffusion part, CMVAE also integrates the diffusion model for improved performance. The conceptual advance is relatively modest.\n\n2. The paper does not provide training efficiency comparisons or runtime analyses (especially for the diffusion part). It unclear whether the performance comes from the model complexity.\n\n3. While figures illustrate disentanglement, generated samples on CUB are of relatively low fidelity, suggesting the model’s generative capacity is still limited."}, "questions": {"value": "1. Can you provide a way to visualize the learned shared latent variable and modality-specific variables? Comparing the visualizations for the MMVAE+ and the proposed method could be straightforward.\n\n2. Can this method scale up to a large-scale Multimodal dataset (e.g., MSCOCO)? \n\n3. How does the proposed method compare to other diffusion models in terms of, for example, CLIP score?\n\n4. How do you handle the text input? Any pre-trained embedding models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8qlKlRSIB3", "forum": "DcHGEcqdFf", "replyto": "DcHGEcqdFf", "signatures": ["ICLR.cc/2026/Conference/Submission15985/Reviewer_wgHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15985/Reviewer_wgHU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408458786, "cdate": 1761408458786, "tmdate": 1762926193348, "mdate": 1762926193348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Information-disentangled Multimodal VAE (IDMVAE), a new model for learning from data with multiple modalities. IDMVAE achieves this improved disentanglement by using Mutual Information (MI) regularizations and latent diffusion priors. They try to learn a common latent z that captures all the shared information encouraged by the added new regularization losses introduced in this work. In addition, they utilize diffusion models to create more complex and capable latent priors, thereby moving beyond simple Gaussian priors and enhancing the model's representational capacity. The authors demonstrate that IDMVAE achieves better shared and private information, resulting in superior generation quality and enhanced semantic coherence on some simple datasets compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and well-presented. The paper also seems to have strong theoretical grounds for the idea they presented in this work. Their MI-based regularization methods could be useful for multimodal VAE works in general. The proposed losses are also useful for learning a shared representation, which is important for downstream tasks. Their proposed idea of using practical contrastive loss for cross-view MI maximization and their disentanglement learning with generative augmentation losses is a good contribution. They propose using diffusion models for learning complex latents, but this has already been proposed and implemented in other works (look at the weaknesses part)."}, "weaknesses": {"value": "Even though the paper is well presented, there are considerable weaknesses in this paper that render the contribution marginal.\n\n\n1. $ \\textbf{Used datasets are toy datasets} $  \n\nThe datasets used in this work are very small and not significant enough to show the practicality of the presented method. I encourage the authors to look at [1], which is also a work done on multimodal VAEs using a score-based model that used the CelebAMask-HQ multimodal dataset, which is larger than the datasets used in this paper, as a starting point. The paper is also not cited and not compared as a baseline. The authors can also add other larger datasets than this to prove their method is still useful in these settings.\n\n2. $\\textbf{Latent diffusion proposal}$\n\n Using a latent diffusion/score-based model to learn complex latents for multimodal VAEs is also presented in [2]. But it hasn't been cited in this work and presented as a new thing.\n\n3. $\\textbf{Limited baselines}$\n\n The baselines compared in this work are limited. Additional recent baselines should also be added to the work. E.g., [1,2,3,4]. \n\n4. $\\textbf{Evaluation of Quality}$\n\n The paper doesn't use FID for evaluating the quality of generated unconditional and conditional images as in [1]. I believe a quantifiable measure of quality is important in multimodal VAE works.\n\n5. $\\textbf{Evaluation of method of PoE Multimodal Models}$\n\n The paper proposes that the method can be used in PoE/MoPoE multimodal VAE models but those haven't been explored as ablations. Can the authors try their method in at least one setup of these multimodal VAEs as a proof of concept or ablation study.\n\n\nReferences\n1. Wesego, Daniel, and Amirmohammad Rooshenas. \"Score-based multimodal autoencoders.\" arXiv preprint arXiv:2305.15708 (2023).\n2. Wesego, Daniel, and Pedram Rooshenas. \"Multimodal ELBO with Diffusion Decoders.\" arXiv preprint arXiv:2408.16883 (2024).\n3. Bounoua, Mustapha, Giulio Franzese, and Pietro Michiardi. \"Multi-modal latent diffusion.\" Entropy 26, no. 4 (2024): 320.\n4. Palumbo, Emanuele, Laura Manduchi, Sonia Laguna, Daphné Chopard, and Julia E. Vogt. \"Deep generative clustering with multimodal diffusion variational autoencoders.\" In The Twelfth International Conference on Learning Representations. 2024."}, "questions": {"value": "Please review the Weaknesses section and attempt to address the points raised there."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m19zYntSzr", "forum": "DcHGEcqdFf", "replyto": "DcHGEcqdFf", "signatures": ["ICLR.cc/2026/Conference/Submission15985/Reviewer_bE3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15985/Reviewer_bE3u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461219372, "cdate": 1761461219372, "tmdate": 1762926192702, "mdate": 1762926192702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Disentanglement of Variations with Multi-Modal Generative Modeling (IDMVAE)” proposes a new multimodal variational autoencoder designed to cleanly separate shared and private sources of variation across data modalities. Standard multimodal VAEs, such as MMVAE and MoPoE, often suffer from latent leakage, where modality-specific information contaminates the shared latent space, causing incoherent cross-modal generation and poor disentanglement. IDMVAE introduces three complementary mechanisms to address this.\n\n1.\tA cross-view mutual information maximization term aligns shared representations by encouraging different modality-specific encoders to capture the same underlying latent factors.\n2.\tA generative augmentation cycle removes redundancy between shared and private latents by decoding “mixed” latent pairs into synthetic samples and enforcing that shared latents remain invariant while private latents retain modality-specific structure.\n3.\tA latent diffusion prior replaces the standard Gaussian prior with a learned diffusion model that can represent complex, multimodal latent distributions, reducing prior–posterior mismatch and improving unconditional generation quality.\n\nThe resulting objective extends the MMVAE+ ELBO with information-theoretic and generative-consistency regularizers, all trainable end-to-end. Experiments on PolyMNIST-Quadrant, CUB (image–text), and TCGA (multi-omics) show that IDMVAE achieves substantially cleaner separation of shared vs. private factors, higher conditional and unconditional coherence, and improved latent alignment compared to strong baselines. Ablation studies demonstrate that cross-view MI alignment improves cross-modal reconstruction, the generative augmentation reduces leakage, and the diffusion prior enhances sample quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\\\nIntroduces a principled formulation of multimodal disentanglement by combining cross-view mutual-information maximization with generative augmentation, directly targeting the separation of shared and private factors.\\\nConceptually unifies ideas from contrastive learning, information bottleneck, and generative modeling in a single probabilistic framework.\n\nQuality:\\\nMethodologically robust: each loss term is clearly motivated by known weaknesses of multimodal VAEs (e.g., posterior collapse, shared–private leakage).\\\nThe generative augmentation mechanism leverages the model’s own decoder for cycle consistency, avoiding handcrafted data augmentations\\\nComprehensive experiments across synthetic (PolyMNIST-Quadrant) and real (CUB, TCGA) datasets demonstrate consistent improvements in disentanglement, coherence, and generation quality.\\\nAblation studies isolate the contribution of each component (CrossMI, GenAug, diffusion prior) and show interpretable, consistent improvements.\\\n\nClarity:\\\nThe paper is well-structured and clearly written, with each component (CrossMI, GenAug, diffusion prior) well motivated and described.\\\nMathematical notation and objective formulation are concise and consistent.\n\nSignificance:\\\nAddresses a challenge in multimodal learning of separating  shared vs. private information with a practical, modular solution applicable to existing architectures.\\\nDemonstrates that learned latent priors (diffusion models) can substantially improve multimodal VAEs, opening a new direction for hybrid diffusion–VAE research. However, it lacks a clear comparison with other multi-modal (latent) diffusion models, such as [1,2].\n\nRefs: \n[1] Bounoua, Mustapha, Giulio Franzese, and Pietro Michiardi. \"Multi-modal latent diffusion.\" Entropy 26.4 (2024): 320. \\\n[2] Chen, Changyou, et al. \"Diffusion models for multi-modal generative modeling.\" arXiv preprint arXiv:2407.17571 (2024)."}, "weaknesses": {"value": "The connection between maximizing $I(z_m; z_n)$ and achieving conditional sufficiency $I(x_m;x_n|z)=0$ remains heuristic. Under what assumptions (e.g., conditional independence or additive latent structure) do the suggested losses approximate or minimal-sufficiency conditions.\\\nThe InfoNCE estimator can be sensitive to negative-sample size, temperature, and batch composition. The paper fixes these hyperparameters without ablation or analysis.\\\nAdding contrastive, augmentation, and diffusion losses likely increases computational cost. The paper lacks a discussion of these computation costs.\\\nYou compare to MMVAE variants but not to contrastive approaches such as [3-4], or as mentioned above, latent diffusion models [1-2]. \\\nThe generative augmentation term is described as reducing redundancy between z and w_m, but the paper does not analyze how effectively this enforces $I(z; w_m) \\to 0$ in the experiments.\n\n\n\nRefs: \\\n[3] Liang, Paul Pu, et al. \"Factorized contrastive learning: Going beyond multi-view redundancy.\" Advances in Neural Information Processing Systems 36 (2023): 32971-32998.\\\n[4] Dufumier, Benoit, et al. \"What to align in multimodal contrastive learning?.\" arXiv preprint arXiv:2409.07402 (2024)."}, "questions": {"value": "1.\tIs the diffusion prior applied only to the shared latent z, or also to private w_m? If only to z, why is a simple Gaussian sufficient for the private components?\n2.\tHow do the MI and cycle constraints correspond to orthogonality or covariance restrictions in linear Bayesian joint additive factor models such as [5]?\n3.\tThe paper does not mention a β-VAE coefficient. Is β fixed at 1, and do $\\lambda_{\\text{MI}}$, $\\lambda_{\\text{GenAug}}$ effectively play this role? How do these hyperparameters influence the average posterior entropies $H[q(z|x_m)] $ and $H[q(w_m|x_m)] $ over training.\n4.\tSome VAE models e.g. [6] address the inference-consistency problem in multimodal VAEs, i.e. the inability to obtain coherent subset posteriors such as $p(w_2 \\mid x_1)$ via a tighter ELBO and flexible encoder aggregation. Does IDMVAE provide an alternative route to this goal by enforcing independence between $w_m$ and the shared latent z, thereby avoiding aggregation bias or looser bounds for modalities with high conditional mutual information?\n5.\tThe CrossMI loss is motivated by encouraging $I(z_m;x_n)\\approx I(x_m;x_n)$, implying that the shared latent fully explains cross-modal dependence. Would this not render private latents redundant? How is this balanced to prevent collapse of private variation, and does this depend on dataset redundancy?\n6.\tIntegrating a diffusion prior end-to-end into a multimodal VAE can introduce instability, as seen in recent works like REPA-E [7] or uses some annealing tricks [8]. The paper does not discuss training schedules, warm-up strategies, or whether gradients from the diffusion loss are stopped for encoder updates.\n\nRefs:\n[5] Anceschi, Niccolo, et al. \"Bayesian joint additive factor models for multiview learning.\" arXiv preprint arXiv:2406.00778(2024).\\\n[6] Hirt, Marcel, et al. \"Learning multi-modal generative models with permutation-invariant encoders and tighter variational objectives.\" Transactions on Machine Learning Research.\\\n[7] Leng, Xingjian, et al. \"Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers.\" arXiv preprint arXiv:2504.10483 (2025).\\\n[8] Vahdat, Arash, Karsten Kreis, and Jan Kautz. \"Score-based generative modeling in latent space.\" Advances in neural information processing systems 34 (2021): 11287-11302."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VWd5X953d5", "forum": "DcHGEcqdFf", "replyto": "DcHGEcqdFf", "signatures": ["ICLR.cc/2026/Conference/Submission15985/Reviewer_1dPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15985/Reviewer_1dPy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986934796, "cdate": 1761986934796, "tmdate": 1762926192362, "mdate": 1762926192362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to all reviewers"}, "comment": {"value": "We thank the reviewers for constructive feedback. We will post individual responses to each reviewer soon. Here we address the common concerns. \n\n1. Datasets and High quality generation \n\nThe dataset we used are commonly found in Multi-modal VAE papers published in top ML venues in just the past 2-3 years, as shown by references in the main paper. \n\nNow we would like the reviewers to check out our new experiments on high-resolution CUB (CUB-HQ), in Appendix E of the updated **supplementary material**. We will replace the previous CUB experiments with CUB-HQ experiments in the next version. In CUB-HQ, we use the original CUB images without cropping out the bird portion using bounding boxes, and use a high 256x256 resolution. This means the high-resolution images contain richer backgrounds, and CUB-HQ is therefore more challenging for representation learning. We preprocess the dataset using pretrained VAE as in DiT, and train IDMVAE with relatively small resnet architecture. We then train DiT denoisers on top of IDMVAE samples, to produce high resolution images, following [1] and [2].\n\nThe results show that our method outperforms baselines by a clear margin, and each regularization term contributes. We provide not only latent classification but also FID and CLIPScore as requested by reviewers to assess the quality of generation; again our method beats important baselines. We hope these results clarify reviews’ concerns on scaling up to high resolution generation: with careful use of pretrained models, IDMVAE can efficiently learn disentangled representations that lead to controllable generation of complex modalities. \n\n2. Related works on diffusion for multi-modal data \n\nWe thank the reviewers for bringing up papers on latent diffusion for multimodal learning, most importantly [3,4]. \n\n- We emphasize that most of these references do not couple latent diffusion and representation learning as tightly as we do: they tend to first learn individual VAEs for each modality, without resolving the correlation between different modalities, and use a second stage of diffusion modeling to couple the separately learned representations. In contrast, our latent prior is jointly trained with the rest of objective and regularization; the gradient of latent diffusion loss propagates to the encoders. \n\n- More importantly, none of these works perform disentanglement of the shared versus private variables. So even if we could evaluate their generative coherence, their methods do not offer the same level of control as we do, e.g., we can combine z and w from different data points or samples from the prior to generate new inputs. \n\n- We are not claiming to be the first one to use latent diffusion for multi-modal data, our novelty is instead the use of more flexible latent distribution to learn structured latent space (e.g., clusters) and *disentangled* representation. Nonetheless, we will cite these papers and add discussions in a future version. We will also include empirical comparisons with [4] in the next version. \n\n- Our application of DiT denoisers in CUB-HQ is similar to the method described in [2,5]; their works previously demonstrated that conditional diffusion yields high-quality results for multi-modal data, and we will acknowledge these contributions in the next version.\n\nAll in all, the existence of prior work on diffusion models for multi-modal data does not undermine our novelty in multi-modal disentanglement. \n\n \n[1] Pandey et al. DiffuseVAE: Efficient, controllable and high-fidelity generation from low-dimensional latents. TMLR, 2022. \n\n[2] Palumbo et al. Deep generative clustering with multimodal diffusion variational autoencoders. In ICLR, 2024. \n\n[3] Bounoua et al. \"Multi-modal latent diffusion.\" Entropy 26.4 (2024): 320. \n\n[4] Wesego, Daniel, and Amirmohammad Rooshenas. \"Score-based multimodal autoencoders.\" arXiv preprint arXiv:2305.15708 (2023). \n\n[5] Wesego, Daniel, and Pedram Rooshenas. \"Multimodal ELBO with Diffusion Decoders.\" arXiv preprint arXiv:2408.16883 (2024)."}}, "id": "1eWCqnFQ28", "forum": "DcHGEcqdFf", "replyto": "DcHGEcqdFf", "signatures": ["ICLR.cc/2026/Conference/Submission15985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15985/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705774842, "cdate": 1763705774842, "tmdate": 1763706461914, "mdate": 1763706461914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}