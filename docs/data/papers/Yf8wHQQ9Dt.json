{"id": "Yf8wHQQ9Dt", "number": 7502, "cdate": 1758025139315, "mdate": 1759897849063, "content": {"title": "Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever", "abstract": "Tool-augmented large language models (LLMs) leverage external functions to extend their capabilities, but inaccurate function calls can lead to inefficiencies and increased costs. Existing methods address this challenge by fine-tuning LLMs or using demonstration-based prompting, yet they often suffer from high training overhead and fail to account for inconsistent demonstration samples, which misguide the model’s invocation behavior. In this paper, we trained a behavior-aligned retriever (BAR), which provides behaviorally consistent demonstrations to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus including different function-calling behaviors, i.e., calling or non-calling. We use the contrastive learning framework to train the BAR with customized positive/negative pairs and a dual-negative contrastive loss, ensuring robust retrieval of behaviorally consistent examples. Experiments demonstrate that our approach significantly reduces erroneous function calls while maintaining high task performance, offering a cost-effective and efficient solution for tool-augmented LLMs.", "tldr": "", "keywords": ["Function Calling", "Tool-augmented Large Language Model", "Retrieval-based Augmentation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d05f5064daa15ed73da72a7bd4db2f44de369009.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BAR (Behavior-Aligned Retriever), a retrieval module designed to improve LLMs' tool-use decisions. The authors find that retrieving behaviorally and semantically related examples can enhance LLMs’ function-calling capabilities. To train the retriever, they construct positive and negative example pairs and optimize them using a dual-negative contrastive loss. The effectiveness of BAR is evaluated on the H2A dataset across three dimensions—helpfulness, harmlessness, and autonomy—showing notable improvements in each aspect."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed BAR leverages the model’s in-context learning abilities to reduce unnecessary API calls—a factor often overlooked in conventional tool-augmented LLMs.\n2. Empirical results show that BAR consistently improves the model’s function-calling performance on both the H2A and ToolDEER datasets, demonstrating the effectiveness and generality of the approach."}, "weaknesses": {"value": "1. The authors evaluate their approach only on relatively small LLMs, which raises concerns about scalability to larger models such as DeepSeek 3.1 or GPT-5. Including results from larger models would provide stronger evidence for the method’s general effectiveness.\n\n2. The dataset coverage is limited, as the experiments are conducted on only two datasets. Incorporating additional tool-calling benchmarks such as BFCL or Tau-Bench would help further validate and strengthen the conclusions.\n\n3. The proposed method does not account for multi-turn tool calling, as indicated by the provided templates. This limitation suggests a gap between the current implementation and real-world multi-turn tool-use scenarios common in modern LLM applications."}, "questions": {"value": "1. How would the performance change if we randomly selected the same number of examples for the LLM, instead of using BAR’s behavior-aligned retrieval? Would BAR still outperform such random retrieval baselines?\n2. Does integrating BAR affect model performance on standard tool-calling benchmarks such as BFCL? Additionally, how well does BAR handle multi-turn tool-calling scenarios compared to existing methods?\n3. What are the computational costs and additional latency introduced by BAR during inference? Evaluating this trade-off would help assess its practicality in real-world applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5dxkG0v0zD", "forum": "Yf8wHQQ9Dt", "replyto": "Yf8wHQQ9Dt", "signatures": ["ICLR.cc/2026/Conference/Submission7502/Reviewer_ctRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7502/Reviewer_ctRF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743184072, "cdate": 1761743184072, "tmdate": 1762919614014, "mdate": 1762919614014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main motivation of this work is that tool-use agents, when equipped with correct demonstrations (prompting examples), exhibit strong performance with a lower incorrect tool-calling rate, as analyzed in a preliminary experiment. On top of this empirical study, the authors train a retriever that can select appropriate in-context learning examples for tool-use agents during the task-solving process. The training loss is dual-negative contrastive loss, teaching the retriever to distinguish good and bad examples.  Experiments on several datasets, as well as a lot of analysis, demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Augmenting tool-use agents with a plug-in retriever is elegant and efficient. The LLMs can be improved without the cost- and time-intensive post-training. Compared with LLM post-training, fine-tuning a smaller retriever is both cheaper and faster.\n\n2. This paper is well-organized with clear formulation and writing structure, which makes it easy for the reader to understand."}, "weaknesses": {"value": "1. In-context example selection is a well-known technique. Although this paper adopt this to augment tool-use agents and achieve good performance, the novelty concerns still remain. Could the author explain the main contributions of the techniques?\n\n2. It seems like a customized example corpus has been built first, thereby enabling the retriever to retrieve from it. However, the details of building such a corpus are unclear. I suggest that the author provide more explanation.\n\n3. The author only experiments on smaller LLMs, while powerful LLMs such as GPT or Claude are not covered. Therefore, there is concern about whether the proposed method is really necessary for powerful LLMs. As models' foundational abilities improve, do in-context examples become less important since LLMs may be less sensitive to them?\n\n4. It seems like in-context learning demonstrations should be created first, which will be put into LLMs' context during the task-solving stage. However, what if the tool scale (the number of overall toolsets) is large and the tool description exceeds LLMs’ context length? In other words, I suggest the author discuss potential concerns about the lengthy context. In which scenarios does it occur, and is there any solution?\n\n5. This paper augments LLMs with an in-context selector, which can select suitable demonstrations for tool-use LLMs. However, a more intuitive solution is to adopt a tool retriever, which can retrieve or re-rank the most relevant tools from the overall toolset [1,2]. I suggest that the author carefully compare these two techniques and explain the advantages of the proposed solution compared with existing tool retrieval solutions.\n\n---\n\n### Reference\n\n[1] Benchmarking Tool Retrieval for Large Language Models\n\n[2] Enhancing Tool Retrieval with Iterative Feedback from Large Language Models"}, "questions": {"value": "Mainly see the above weakness.\n\nThe overall idea is elegant and easy to implement. However, the differences with the conventional in-context selection seem vague. I am happy and open if the author can provide more discussion (as mentioned in Weaknesses 1 and 3)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5GqXFnBGJ", "forum": "Yf8wHQQ9Dt", "replyto": "Yf8wHQQ9Dt", "signatures": ["ICLR.cc/2026/Conference/Submission7502/Reviewer_Hd9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7502/Reviewer_Hd9m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793929900, "cdate": 1761793929900, "tmdate": 1762919613583, "mdate": 1762919613583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Behavior-Aligned Retriever for tool-augmented LLMs. It addresses the problem that semantically similar examples retrieved for in-context demonstrations may display inconsistent tool-calling behaviors, thereby confusing the LLM. The authors introduce a contrastive training framework with dual-negative loss to retrieve semantically similar but behaviorally consistent examples. Experiments on H2A and ToolDEER datasets show that BAR reduces unnecessary API calls and improves function-calling accuracy.\nThe idea of aligning retrieval examples not only semantically but also behaviorally is novel and relevant to the growing area of LLM tool use. However, the paper currently suffers from several presentation and clarity issues that reduce its readability and impact."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies an underexplored but important issue in retrieval-augmented function-calling that semantically close examples can have opposite tool behaviors.\n2. The proposed dual-negative contrastive loss to enforce behavioral consistency is a solid and interesting technical contribution.\n3. The experiments show consistent gains across multiple datasets and models, indicating robustness and general applicability."}, "weaknesses": {"value": "1. Clarity of Figure 1 and Motivation\nThe description around Figure 1 is currently confusing. The text says that “Many queries that are lexically or topically close require opposite tool-invocation behavior. Figure 1 illustrates such a clash…”, but the figure only shows a general retrieval pipeline rather than the clash itself. The authors should explicitly annotate Figure 1 (or provide an additional subfigure) to illustrate a concrete example of the mismatch: e.g., “What’s the weather in Paris?” (requires API) vs. “What’s your favorite kind of weather?” (no API). The caption should clearly explain how the figure demonstrates this semantic-behavior conflict rather than restating the pipeline.\n\n2. Undefined Variable in Equation (1)\nIn Eq. (1), k (the number of retrieved demonstrations) is used but not defined. Please define k explicitly as “the number of demonstrations retrieved per query,” to avoid ambiguity.\n\n3. Findings 1 and 2 Lack Detail and Supporting Results\nThe section summarizing Finding 1 and Finding 2 is too brief and mostly qualitative. There are no quantitative results or examples that clearly support these “findings.” I suggest including a table or plot (possibly moving part of Figure 2’s analysis here) to show how LLM performance changes with semantic similarity and with behavioral consistency. The text should also clarify how these findings were derived — from ablation experiments, correlation analyses, or preliminary runs?\n\n4. Methodological Details Are Missing (Major)\nThe retrieval function defines z_i as “similar queries,” but the paper does not explain how semantic similarity is computed or how the threshold t is chosen in Eq. (3). (Probably I missed several part, please notice in the rebuttal if I missed anything. ). Similarly, the process of determining “behavioral compatibility” (call/no-call) during retrieval is underspecified. How is this integrated into similarity scoring?\nMore details on zi similarity computation, threshold tuning, and implementation specifics would make the method reproducible.\n\n5. Behavior Alignment Step Needs More Explanation\nThe term “behavior alignment” is central but remains vague in the main text.\nPlease add a subsection or a short paragraph describing concretely how behavioral alignment is achieved during retrieval — is it through embedding projection, label conditioning, or post-filtering?\nFigure 3 and Figure 4 (t-SNE plots) could be better connected to this explanation, e.g., by visually linking clusters to behavioral categories.\n\n7. Missing Related Work on Tool Learning\nImportant recent works on tool learning and introspective alignment should be discussed, including:\n1. Confucius: Iterative Tool Learning from Introspection Feedback (AAAI 2024)\n2. Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents (WWW 2025)\n3. Iterative Self-incentivization Empowers LLMs as Agentic Searchers (NeurIPS 2025)\nThese works also address the problem of learning when and how to use tools, and their discussion could strengthen the paper’s positioning."}, "questions": {"value": "1. Pseudo Query Generation has been proposed in recent years to leverages LLM as a few-shot query generator, and creates task-specific\nretrievers based on the generated data (see https://arxiv.org/pdf/2209.11755). I was wondering if the authors can discuss if such kind of strategy can be also adaptable to the task they are working in this paper.\nMethods like pseudo query generation (e.g., Gao et al., 2022; Bonifacio et al., 2023; Li et al., 2023 — corresponding to your suggested papers [arXiv:2209.11755, 2305.11841, 2206.10128]*) can also improve retrieval quality through augmentation.\nThe authors could discuss whether such approaches could achieve similar effects, or how BAR complements them (e.g., BAR focuses on behavioral rather than purely semantic alignment).\nA short comparative discussion in Related Work or Discussion would make the contribution clearer.\n\n2. See my comments in the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kRTbSGrQcE", "forum": "Yf8wHQQ9Dt", "replyto": "Yf8wHQQ9Dt", "signatures": ["ICLR.cc/2026/Conference/Submission7502/Reviewer_Mhtb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7502/Reviewer_Mhtb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994402083, "cdate": 1761994402083, "tmdate": 1762919613178, "mdate": 1762919613178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets a practical weakness in tool-augmented LLMs: semantically similar in-context demonstrations can recommend opposite tool-use behaviors (call vs. no-call), leading to unnecessary or missing API calls. The authors propose a Behavior-Aligned Retriever (BAR) trained with contrastive learning that explicitly encodes invocation behavior in addition to semantics. They construct positive pairs that are both semantically close and behavior-matched, and introduce dual-negative contrastive loss (DNCL) that mixes (i) negatives with the same behavior (to sharpen semantic resolution) and (ii) hard negatives with different behavior (to sharpen behavioral boundaries). Evaluated on H2A and ToolDEER, BAR purportedly improves exact function match (helpfulness), refusal rates (harmlessness), and, most notably, direct response (autonomy) (i.e. fewer redundant API calls) compared to BM25, BERT, and Contriever retrieval. Ablations show DNCL outperforms CE/InfoNCE/SCL/Triplet, and behavior-consistency increases with training scale (plateauing around ~95-100%)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem formulation and motivation. The paper convincingly shows that semantic-only retrieval can mislead tool decisions and empirically correlates behavior-consistency with downstream quality. \n\n2. Methodological simplicity with practical impact. BAR is a lightweight retriever that can be dropped into existing RAG-for-tools pipelines without re-training the backbone LLM; the contrastive formulation is clean and reproducible (hyperparameters, alpha, tau, batch/epochs are provided). \n\n3. Comprehensive model coverage. Results span vanilla chat models and tool-tuned models (Functionary, ToolLLaMA, ToolAlpaca, ToolAlign), showing broadly consistent gains, especially in Autonomy where avoiding unnecessary calls matters most. \n\n4. Useful diagnostics. Behavior-consistency ratio, retrieval distribution tables, and t-SNE plots offer interpretable evidence that BAR clusters call/no-call behaviors more faithfully. \n\n5. Ablations that matter. The DNCL vs. baselines and negative-sampling studies are targeted and illuminate why BAR works (hard inter-behavior negatives are crucial)."}, "weaknesses": {"value": "1. Evaluation reliance on LLM judges / proxies. Parts of Harmlessness/Helpfulness rely on GPT-4-style judgment prompts or indirect metrics; robustness to judge variance and prompt framing is not analyzed. Statistical significance and inter-rater reliability are missing. \n\n2. Cost & latency not quantified. The core pitch is \"reducing unnecessary API calls -> efficiency\", but there is no end-to-end accounting of latency, monetary cost, or energy savings across workloads. Gains are reported as rates (e.g., autonomy/direct-response), not as operational cost reductions. \n\n3. Limited safety granularity. Authors acknowledge BAR struggles to separate \"helpful-but-needs-tools\" from \"harmful-and-should-refuse\" due to similar surface forms; the method uses binary call/no-call supervision rather than risk-tiered labels, which constrains safety improvements. \n\n4. Domain generalization / leakage risks. The training corpus mixes function-calling and general QA; more details are needed on (i) de-duplication vs. test sets, (ii) domain shifts (e.g., unseen APIs), and (iii) whether lexical artifacts from particular datasets inflate behavior-consistency. \n\n5. Mixed results in places. Some rows (e.g., Llama-3.1-8B on ToolDEER #SearchAPI) show weaker or regressive performance with BAR relative to strong baselines; the paper does not deeply probe these regressions or provide significance testing. \n\n6. Narrow decision framing. \"Call vs. no-call\" is a coarse abstraction. Many real tasks involve which tool(s) and with what parameters; BAR’s behavior labels don’t address multi-tool composition or parameter selection beyond the helpfulness exact-match proxy."}, "questions": {"value": "1. Data hygiene and leakage: How do you ensure zero overlap (near-duplicates, paraphrases) between training positives and test queries/demonstrations, especially given semantic thresholds for positives and \"top-l\" hard negatives? Please report near-duplication audits and performance under deduped settings. \n\n2. Operational savings: Can you report wall-clock latency, dollar cost, and energy deltas (with/without BAR) for representative workloads? This would substantiate the efficiency claim beyond accuracy-style metrics. \n\n3. Safety-aware variants: Have you tried adding risk labels (e.g., harmless/helpful/harmful) to the behavior vector so BAR can prioritize \"refusal-like\" demonstrations for potentially unsafe queries? \n\n4. k-sensitivity and retrieval drift: You choose top-5 as default; how sensitive are results to k across models/datasets, and how stable is BAR when the datastore composition shifts (new domains/APIs added)? Please include confidence intervals. \n\n5. Beyond binary behavior: Can BAR be extended to multiclass behavior (no-call, call-single-tool, call-multi-tool) and to parameter-selection hints? Any preliminary results? \n\n6. Failure modes & regressions: In rows where BAR underperforms (e.g., some #SearchAPI cases), what patterns cause degradation—semantic over-fitting, conservative bias, or label noise? Could a temperature/α schedule mitigate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l36OyUodff", "forum": "Yf8wHQQ9Dt", "replyto": "Yf8wHQQ9Dt", "signatures": ["ICLR.cc/2026/Conference/Submission7502/Reviewer_bGHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7502/Reviewer_bGHN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7502/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183012938, "cdate": 1762183012938, "tmdate": 1762919612853, "mdate": 1762919612853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}