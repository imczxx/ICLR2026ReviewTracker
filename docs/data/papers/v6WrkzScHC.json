{"id": "v6WrkzScHC", "number": 158, "cdate": 1756729841216, "mdate": 1763670863475, "content": {"title": "Riemannian Networks over Full-Rank Correlation Matrices", "abstract": "Representations on the Symmetric Positive Definite (SPD) manifold have garnered significant attention across different applications. In contrast, the manifold of full-rank correlation matrices, a normalized alternative to SPD matrices, remains largely underexplored. This paper introduces Riemannian networks over the correlation manifold, leveraging five recently developed correlation geometries. We systematically extend Multinomial Logistic Regression (MLR), Fully Connected (FC), and convolutional layers to these geometries. Additionally, we present methods for accurate backpropagation for two correlation geometries. Experiments comparing our approach against existing SPD and Grassmannian networks demonstrate its effectiveness.", "tldr": "We extend core deep learning layers to correlation manifolds under five Riemannian geometries, introducing CorNets that outperform existing SPD and Grassmannian networks", "keywords": ["Correlation matrices", "Riemannian neural networks", "Matrix manifolds", "Riemannian manifolds"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f13254e99efa7407073058b01bee2f0b3520003a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends MLR, FC, and convolutional layers in Riemannien Networks to correlation manifold under five geometries: ECM, LECM, OLM, LSM and PHCM, and discuss backpropagation of Riemannian computations under OLM and LSM. The authors evaluate thier method on 3 benchmarks Radar, HDM05 and FPHA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is coherent and methodologically sound. The mathematical formulations are clearly presented and internally consistent. The extension of fully connected layers, multinomial logistic regression, and convolutional operations to the correlation manifold represents, in my view, an original contribution."}, "weaknesses": {"value": "The experiments are limited, only main results and an ablation study are presented. The contributions are fair, adopting Correlation matrix in Riemannian Networks is mainly an extension of existing SPD or hyperbolic formulations."}, "questions": {"value": "1. In main results, the authors report the best results comparing to methods, Grassmannian and SPD manifolds, I wonder if the authors compare the proposed CorNet with other methods in Correlation Manifold?\n2. The authors claim accurate backpropagation of Riemannian computations under OLM and LSM as one main contribution, but I didn't see experiments evaluating this point.\n3. The authors needs more interpretations on fig.2 and fig.5\n4. Can the authors provide more evidence justifying the novolties"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2o8MENvuM0", "forum": "v6WrkzScHC", "replyto": "v6WrkzScHC", "signatures": ["ICLR.cc/2026/Conference/Submission158/Reviewer_xKjc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission158/Reviewer_xKjc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481248610, "cdate": 1761481248610, "tmdate": 1762915459744, "mdate": 1762915459744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Comment responses (1/2)"}, "comment": {"value": "We thank all the reviewers for their constructive suggestions and valuable feedback. Below, we address the common questions (CQs).\n\n**Remark:** All references cited in the rebuttal correspond to the numbering in the revised manuscript.\n***\n\n# CQ#1: Experiments on the large NTU120 dataset (Reviewers $\\textcolor{blue}{PcBq}$, $\\textcolor{red}{jw93}$, and $\\textcolor{green}{xKjc}$)\n\nTab. A: Performance and fit time (s/epoch) on NTU120 dataset.\n| Manifold     |     Method      |  Mean ± STD  | Fit Time|\n|:------------:|:---------------:|:------------:|:-------:|\n| Grassmannian |      GrNet      | 57.59 ± 0.22 |  50.97  |\n| Grassmannian |      GyroGr     | 53.76 ± 0.18 | 136.96  |\n| Grassmannian |  GyroGr-Scaling | 43.90 ± 0.23 | 338.01  |\n|      SPD     |      SPDNet     | 51.25 ± 0.36 |  12.77  |\n|      SPD     |     SPDNetBN    | 54.35 ± 0.43 |  19.78  |\n|      SPD     |  SPDResNet-AIM  | 57.33 ± 0.35 |  23.84  |\n|      SPD     |  SPDResNet-LEM  | 61.34 ± 2.02 |  13.00  |\n|      SPD     | SPDNetLieBN-AIM | 58.20 ± 0.46 |  31.10  |\n|      SPD     | SPDNetLieBN-LCM | 57.96 ± 0.43 |  22.06  |\n|      SPD     |    SPDNetMLR    | 58.59 ± 0.13 |  22.48  |\n|      SPD     |      GyroLE     | 59.29 ± 0.42 |  22.08  |\n|      SPD     |      GyroLC     | 59.29 ± 0.42 |  14.14  |\n|      SPD     |      GyroAI     | 62.21 ± 0.29 |  98.31  |\n|      SPD     |    GyroSPD++    | 61.57 ± 0.30 | 216.46  |\n|  Correlation |    CorNet-ECM   | **65.04 ± 0.14** |  **12.06**  |\n|  Correlation |   CorNet-LECM   | 65.03 ± 0.10 |  12.68  |\n|  Correlation |    CorNet-OLM   | 64.41 ± 0.23 |  16.07  |\n|  Correlation |    CorNet-LSM   | 60.69 ± 0.85 |  16.28  |\n|  Correlation |   CorNet-PHCM   | 60.01 ± 0.22 |  16.92  |\n\nWe additionally evaluate our methods on the larger NTU120 dataset [a]. Following [b, c], we focus on the mutual-action classes.\n- **Accuracy.** As shown in Tab. A, CorNets continue to outperform both SPD and Grassmannian networks. The strongest SPD competitors GyroAI and GyroSPD++ obtain 62.21 ± 0.29 and 61.57 ± 0.30, while our CorNet-ECM and CorNet-LECM achieve 65.04 ± 0.14 and 65.03 ± 0.10, respectively.\n- **Efficiency.** CorNet-ECM is also the fastest one, with a training time of 12.06 s/epoch. It is slightly faster than the simplest SPD models such as SPDNet (12.77 s/epoch) and SPDResNet-LEM (13.00 s/epoch), and much faster than more expressive SPD baselines like GyroLE, GyroAI, GyroLC and GyroSPD++. This is attributed to the fast and simple computations of the correlation geometries.\n\nThese results have been added to Tab. 2 in the main paper. More details on the dataset and preprocessing can be found in App. I.2–I.3."}}, "id": "TQMSv6g9LX", "forum": "v6WrkzScHC", "replyto": "v6WrkzScHC", "signatures": ["ICLR.cc/2026/Conference/Submission158/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission158/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission158/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763671014167, "cdate": 1763671014167, "tmdate": 1763674520509, "mdate": 1763674520509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Correlation Networks (CorNets): a new family of deep learning architectures operating on the manifold of full-rank correlation matrices. The authors extend key neural network components to this manifold under five distinct Riemannian geometries (four zero-curvature and one hyperbolic). They also propose accurate backpropagation schemes for their layers. Experimental results on radar and human action recognition datasets demonstrate that CorNets outperform existing SPD and Grassmannian networks in both accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a wide framework to implement neural networks on full-rank correlation matrices. It considers multiple geometries and layer types and distinguishes between them. Thus it introduces a wide toolbox that could be useful in various scenarios.\n- This framework is presented in a detailed manner. The paper includes all the necessary details to understand and reimplement both the method and the experiments.\n- The experiments are thorough and well-documented.  \n- The introduced CorNets show improved performance over SPD and Grassmanian method on a variety of datasets."}, "weaknesses": {"value": "For me, the paper has two crucial weaknesses:\n\na) The presentation is in some parts very dense. Especially, when introducing the various layers it is mostly a quick listing of facts and for me it was hard to form a conceptual picture that lasts beyond the specific methods. At the same time, I also found this makes it less pleasant to read for me. Maybe the authors could introduce more structure lists and tables in the main text, move the propositions etc to the appendix and discuss more conceptual things in the main text?\n\nb) To me the examples in the experimental validation do not seem to be that interesting. The datasets are rather old, and the performance of previous methods seems to already be very good on two of them. It is interesting to see, that the method is noticeably better than competitors on HDM05. However, to me, it is not clear why this is and I would recommend the authors to discuss this more in the paper and at least propose some hypothesis. Furthermore, I miss a comparison to method not based on matrix manifolds for the same tasks. This is, for me, a common issue with paper working on neural networks for matrix manifolds and makes it hard to learn when such networks are appropriate tools. To mitigate this, the authors could include a more thorough discussion on when correlation matrices are most appropriate, when other matrices, and when maybe purely Euclidean methods suffice.\n\nI still think that the paper deserves to be published at ICLR. Nevertheless, these concerns move me closer to the decision boundary."}, "questions": {"value": "- Suggestion: I think it would be helpful to provide pseudo-code on how to implement the method. For me, this would drastically reduce the time necessary for going from reading the paper to using it in my context. Furthermore, I believe that pseudo-code is also a more permanent way to document implementations than a link to some repository.\n  - Suggestion: I think tables for the layers introduced in  (7) and (9)-(12) containing their parameters and formulas would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IxwA9BNXym", "forum": "v6WrkzScHC", "replyto": "v6WrkzScHC", "signatures": ["ICLR.cc/2026/Conference/Submission158/Reviewer_jw93"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission158/Reviewer_jw93"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921605142, "cdate": 1761921605142, "tmdate": 1762915459141, "mdate": 1762915459141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends Multinomial Logistic Regression (MLR), Fully Connected (FC), and convolutional layers to the geometry of full-rank correlation matrices. The proposed networks are compared to Symmetric Positive Definite (SPD) and Grassmannian neural networks to show their effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper concerns with the manifold of correlation matrices that is underexplored in deep learning.\n- The construction of network building blocks is supported by theoretical results."}, "weaknesses": {"value": "- The novelty is limited.\n- More experiments are needed to validate the proposed networks."}, "questions": {"value": "The aim of this paper is to construct MLR, FC, and convolutional layers for neural networks on the manifold of correlation matrices. Unfortunately, I failed to see new ideas in the construction of these building blocks for the following reasons:\n- Riemannian MLR layers on hyperbolic space were proposed in [Ganeal et al., 2018], then extended to matrix manifolds in [Nguyen & Yang, 2023].\n- FC and convolutional layers on hyperbolic space were proposed in [Shimize et al., 2021], then extended to matrix manifolds in [Nguyen et al., 2024].\n\nThe present work follows the same approach in [Shimize et al., 2021; Nguyen & Yang, 2023; Nguyen et al., 2024] to adapt the layers on hyperbolic space to the setting of matrix manifolds. What is new in the present paper is that they authors deal with the geometry of correlation matrices. However, since correlation matrices are normalized SPD matrices, and the geometry of the former is thoroughly studied [David & Gu, 2019], it is straighforward to derive formulas and theoretical results in the considered setting. \n\nRegarding experimental evaluations, I am not convinced by the comparison of the proposed networks and SPD neural networks for the following reasons:\n- All datasets are of small size.\n- Intuitively, I can't quite see why the proposed neural networks can outperform SPD neural networks in terms of accuracy, since correlation matrices are nothing but normalized SPD ones. I think improvements can be obtained in specific cases but it is not systematic. \n\nTo summary, the greatest weakness of the paper is its limited novelty which explains my rating for the paper. \n\nQuestion: \n\n1. Could you give an intuitive reason why neural networks on the manifold of correlation matrices can be more effective than their SPD counterparts in terms of accuracy ?\n\n2. I am wondering if the proposed networks and their SPD counterparts share the same architecture ? This question is related to one of my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4bgqTetPFx", "forum": "v6WrkzScHC", "replyto": "v6WrkzScHC", "signatures": ["ICLR.cc/2026/Conference/Submission158/Reviewer_PcBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission158/Reviewer_PcBq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987326509, "cdate": 1761987326509, "tmdate": 1762915458835, "mdate": 1762915458835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}