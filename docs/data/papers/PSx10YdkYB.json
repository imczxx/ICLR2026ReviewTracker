{"id": "PSx10YdkYB", "number": 18273, "cdate": 1758285868626, "mdate": 1759897114854, "content": {"title": "Fairness-Aware Test-Time Prompt Tuning", "abstract": "Vision-language models have displayed remarkable capabilities in multi-modal understanding and are increasingly used in critical applications where economic and practical deployment constraints prohibit re-training or fine-tuning. However, these models can also exhibit systematic biases that disproportionately affect protected demographic groups and existing approaches to addressing these biases require extensive model retraining and access to demographic attributes. There is a clear need to develop test-time adaptation (TTA) approaches that improve the fairness characteristics of pretrained models under distributional shift. In this paper, we evaluate how episodic TTA affects fairness in CLIP classification under subpopulation shifts and develop FairTPT, a novel fairness-aware episodic TTA method that jointly minimizes target marginal entropy while maximizing spurious marginal entropy through soft-prompt tuning. We find that standard episodic TTA generally exacerbates disparities between majority and minority groups, that blinding a model to spurious attributes without degrading target performance is inherently challenging, and that excessive blinding can lead to catastrophic forgetting. This model collapse can be prevented by monitoring test-time changes in target loss within the linear regime, while still achieving fairness improvements on reactive data and preserving overall performance. Thus refined, FairTPT outperforms all state-of-the-art episodic test-time debiasing methods and establishes a foundation for robust TTA—essential for achieving fairness in practice.", "tldr": "We introduce FairTPT, a novel fairness-aware test-time adaptation method for vision-language models that reduces bias by jointly optimizing target-attribute and sensitive-attribute entropy during episodic prompt tuning.", "keywords": ["test-time adaptation", "test-time debiasing", "prompt-tuning", "vision-language models", "algorithmic fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edfc3023fe2edaed4850a20efeb20d49b602764b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose a fairness-aware test-time prompt tuning method by minimizing the entropy of target predictions while reducing bias by maximizing the entropy of sensitive attribute predictions. It achieves balanced adaptation through a lightweight learning-rate heuristic that prevents over-debiasing, all in a fully unsupervised, episodic setting without needing sensitive attribute labels."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of this paper is clear, and the objective function (min target entropy + max spurious entropy) is very intuitive and easy to understand.\n\n- The method does not require access to sensitive attributes or retraining the model and it's easy and practical for deployed models since it only injects in the test stage through soft prompting.\n\n- The empirical evaluation covered most of the benchmark datasets in fairness for image classification tasks, which makes the results rigorous."}, "weaknesses": {"value": "- The method is limited on one attribute spurious correlation, not extend to complex or multi-sensitive attributes, for example, if the spurious correlation is on a combination of age, gender and race. Also, the method depends on pre-define the sensitive attribute and in practice, sometimes it may not reflect the true spurious correlation.\n\n- Lack of analysis of explainability, for example, using attention map to illustrate the change of sensitive attribute after applying the method (is it really reduce the dependence of sensitive attribute).\n\n- No ablation study about if remove ELRA or view filtering or simply maximising spurious entropy would work, lack of demonstrate of each component of the objective function."}, "questions": {"value": "- If predefined sensitive attribute is gender -> y, the actual spurious is gender+race -> y or race -> y, under such situation, is the method still effective? For example, would the soft prompting still successfully reduce the model's reliance on the true spurious factors, or might it fail to capture the intertwined biases and even introduce unintended distortions in the predictions?\n\n- Does there exist a generalizable way to tune $\\lambda_{fair}$ across datasets or tasks, and how sensitive the model’s performance is to $\\lambda_{fair}$"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPIo4R5zjB", "forum": "PSx10YdkYB", "replyto": "PSx10YdkYB", "signatures": ["ICLR.cc/2026/Conference/Submission18273/Reviewer_YdNt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18273/Reviewer_YdNt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761373306190, "cdate": 1761373306190, "tmdate": 1762927997324, "mdate": 1762927997324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Briefly summarize the paper and its contributions. You can incorporate Markdown and Latex into your review. See https://openreview.net/faq.\n\nThis work tackles a common problem in large-scale vision-language models (VLMs) like CLIP: balancing predictive performance with the mitigation of spurious correlations, i.e. fairness in this article. Building upon the Test-Time Prompt Tuning (TPT) framework—which optimizes prompt embeddings by minimizing prediction entropy over augmented inputs—the authors introduce a sophisticated dual-objective approach. Specifically, they build upon this by introducing a dual objective, adding what is effectively a \"reverse TPT\" for spurious attributes. While the TPT objective minimizes entropy to improve accuracy on the target task, this new, opposing objective simultaneously maximizes the entropy for spurious features. This strategy is designed to actively \"unlearn\" or eliminate these biases at test time, thereby achieving enhanced fairness while preserving the model's overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results.\n\nThey address a meaningful challenge of mitigating spurious correlations in VLMs without largely sacrificing accuracy, a critical issue for fair and robust deployment. Its originality lies in the clever extension of Test-Time Prompt Tuning (TPT) into a dual-objective framework. The introduction of a \"reverse TPT\" to actively maximize entropy for spurious attributes is an intuitively effective method for test-time bias disentanglement. Furthermore, the clarity of the paper is great as for me, presenting its \"push-pull\" logic and technical design with precision."}, "weaknesses": {"value": "A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.\n\nThe primary weakness lies in its perceived lack of substantial novelty as for me. The core algorithm is somewhat an extension of the existing Test-Time Prompt Tuning (TPT) framework. The objective is an intuitive application of TPT's entropy-based mechanism rather than a new paradigm. As such, the overall novelty of the technical contribution is somewhat limited as for me."}, "questions": {"value": "For the final methods FAIRTPT and FAIRTPT (MO), does the latter refer to the algorithm + Multi-Objective Optimization engineering technique? And your Equation 3 and Equation 4 are actually two different designs, but for these two algorithms, did you default to choosing Equation 4 since I didn't find the specifies. I want to know what effect of Equation 3 is. Have you ever done related experiments to try it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OdFEBlcTBh", "forum": "PSx10YdkYB", "replyto": "PSx10YdkYB", "signatures": ["ICLR.cc/2026/Conference/Submission18273/Reviewer_hjYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18273/Reviewer_hjYo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842535565, "cdate": 1761842535565, "tmdate": 1762927996956, "mdate": 1762927996956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method which aims at test-time tuning of zero-shot image classification tasks in visual-language models. The authors introduce two loss terms associated with the prompt generation. The authors evaluate their method against a zero-shot baseline,fine-tuned model, and a debiasing baseline, on 4 datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Overall, this paper is well-motivated and presents the technical details well. It would be suitable for a general ai researcher outside the area. \n\n2. The evaluation is suitable thorough, the datasets and baselines are reasonable. Had there been a stronger delta against the baselines, this would be a complete paper.\n\n3. The authors give detailed outline of all method pseudocode (but no included code) in the appendix. Also significant secondary results in the appendix. This is therefore an acceptable (but not exceptional) standard of reproducibility."}, "weaknesses": {"value": "1. Overall, the evaluation is underwhelming. As is common in debiasing work, the authors don't provide a qualitative evaluation on a downstream application. For example, is +2.0% WGA qualitatively different than +1.1 WGA when comparing OrthCali v. FairTPT on average performance (Table 1)? Further, the authors don't provide variance over the trials (more than 5 trials would be best to estimate variance).  \n\n2. The results against the fairness baseline are similar enough that things like training or inference cost are relevant, e.g. is the delta improvement over the baseline due to better hyperparameter tuning, over a larger set of candidate models or higher training runtime? Is there a large difference in inference-time cost that would justify a small delta improvement?\n\n3. The results show significant parameter sensitivity on η. This is concerning, as this may increase training time on arbitrary datasets where we can't assume prior hyperparameters are suitable.  \n\nTogether, the authors haven't demonstrated a significant improvement in terms of efficiency or qualitative results."}, "questions": {"value": "From above weaknesses:\n\n1. What is the qualitative value of these delta improvements observed against the fair baseline?\n\n2. Do you have std results that can be reported? At least in the average columns (if readability is a concern)?\n\n3. Do you have runtime comparisons, both in terms of training/tuning time, and inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2qDh5a30zv", "forum": "PSx10YdkYB", "replyto": "PSx10YdkYB", "signatures": ["ICLR.cc/2026/Conference/Submission18273/Reviewer_jwNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18273/Reviewer_jwNW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019809232, "cdate": 1762019809232, "tmdate": 1762927996430, "mdate": 1762927996430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates the fairness of existing test-time adaptation (TTA) methods for VLMs and proposes a new TTA approach to reduce spurious correlations. First, the paper shows that the existing TTA methods do not improve subgroup robustness, can amplify disparities, and are highly sensitive to hyperparameters. Then, a new TTA method, FAIRTPT, is proposed that jointly minimizes the target-attribute entropy to preserve accuracy and maximizes the sensitive-attribute entropy to reduce the spurious correlations. Empirically, FAIRTPT achieves SOTA/competitive results across different fairness benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an important problem and proposes a rigorous solution. \n\n- Empirically, FAIRTPT outperforms other TTA methods across various fairness benchmarks."}, "weaknesses": {"value": "- I find the hyperparameter sensitivy aspect a bit irrelevant to the main theme of the paper, which is fairness. While it is nice that FAIRTPT is a method with more favorable hyperparameter sensitivy than other TTA methods, a presentation where this is introduced as a nice-to-have property would make the overall story more coherent. \n\n- The method is specifically developed and empirically validated on zero-shot classification asks, but the title sounds more general. This specific focus/limitation should be more explicit in the title."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3wxeaIuAeu", "forum": "PSx10YdkYB", "replyto": "PSx10YdkYB", "signatures": ["ICLR.cc/2026/Conference/Submission18273/Reviewer_1PZi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18273/Reviewer_1PZi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18273/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156709100, "cdate": 1762156709100, "tmdate": 1762927995987, "mdate": 1762927995987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}