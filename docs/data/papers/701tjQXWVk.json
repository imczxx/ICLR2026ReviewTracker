{"id": "701tjQXWVk", "number": 10195, "cdate": 1758163557222, "mdate": 1759897667872, "content": {"title": "ExGRPO: Learning to Reason from Prior Successes", "abstract": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose  ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.", "tldr": "We present the first systematic study of what makes reasoning experiences valuable in zero RLVR and propose a framework that leverages these insights to exploit high-value experiences for efficient RLVR.", "keywords": ["Reinforcement Learning", "Large Reasoning Model", "Reinforcement Learning with Verifiable Rewards"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fcf7fd7ae0ba39715be61ad68f8684d8832e25d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ExGRPO (Experiential Group Relative Policy Optimization), a reinforcement learning method tailored for large language models (LLMs) in the RLVR (Reinforcement Learning from Verifiable Rewards) paradigm. ExGRPO addresses the inefficiencies of on-policy RLVR, which discards prior successful trajectories, by designing a principled experience management pipeline. The key innovations include:\n\nIdentifying rollout correctness and trajectory entropy as indicators of experience quality;\n\nOrganizing trajectories by correctness buckets and selecting low-entropy paths for replay;\n\nA mixed-policy optimization objective that blends off-policy experience replay with on-policy rollouts, using importance reweighting and policy shaping to control exploration–exploitation tradeoffs;\n\nA delayed start mechanism to avoid low-quality replay at early stages.\n\nExtensive experiments on multiple reasoning benchmarks (AIME, MATH-500, Olympiad, ARC-c, GPQA, etc.) and across backbone models (Qwen, LLaMA, LUFFY, etc.) demonstrate consistent performance gains (+3.5/+7.6 points in-domain/OOD) and improved stability over strong baselines like GRPO and RePO."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated problem: The paper targets a real bottleneck in RLVR—wasteful discard of prior trajectories—and connects this with known issues in experience replay for RL and LLMs.\n\nPrincipled design of experience value: The use of rollout correctness buckets and entropy as quality proxy is backed by a detailed analysis (Fig.1), offering clear guidance for trajectory selection.\n\nStrong methodology: ExGRPO is conceptually elegant and practically implementable, combining well-established principles (importance weighting, entropy shaping) into a novel RLVR-specific pipeline.\n\nThorough evaluation:\n\nTested on five models (1.5B–8B);\n\nCovers both in-distribution (math) and out-of-distribution reasoning tasks;\n\nIncludes ablation studies (Fig.7), training stability analysis (Fig.4), and replay dynamics (Fig.5–6);\n\nAchieves SOTA or near-SOTA results (Table 1), including on strong baselines (Oat-Zero, PRIME-Zero, RePO-Zero).\n\nStabilizes weak models: The method prevents collapse in Llama-3.1 8B, a previously unstable model under on-policy RLVR.\n\nOpen-sourcing & reproducibility: Authors commit to releasing code and weights and provide training details and an extensible GitHub repo."}, "weaknesses": {"value": "Although ExGRPO effectively improves efficiency and stability in RLVR training, it remains heuristic?. The method is also limited to verifiable reasoning tasks and has not been validated on open-ended domains like dialogue or code generation."}, "questions": {"value": "How would ExGRPO perform on tasks with non-verifiable or sparse rewards, such as creative reasoning or open-ended dialogue?\n\nCould the valuable-but-incorrect trajectories (those with correct reasoning but wrong final answers) also be leveraged instead of being discarded?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bj3cUR4s8x", "forum": "701tjQXWVk", "replyto": "701tjQXWVk", "signatures": ["ICLR.cc/2026/Conference/Submission10195/Reviewer_wSTN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10195/Reviewer_wSTN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760474270819, "cdate": 1760474270819, "tmdate": 1762921558379, "mdate": 1762921558379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational inefficiency and instability of standard on-policy Reinforcement Learning from Verifiable Rewards (RLVR) for training large reasoning model. The authors first investigate what constitutes a valuable reasoning experience, identifying rollout correctness (favoring medium-difficulty problems) and trajectory entropy (favoring low-entropy chains) as effective indicators of quality. Based on these insights, they propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes these high-value experiences in a replay buffer. ExGRPO employs a mixed-policy objective that strategically balances fresh exploration with the exploitation of these selected past successes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple but Clear motivation\n\n2. Simple implementation\n\n3. Clear improvement compared to GRPO\n\n4. Various experiments"}, "weaknesses": {"value": "1. Depending on highly heuristic results"}, "questions": {"value": "1. It seems difficult to regard a task with 25% accuracy as having the same learning value as one with 75% accuracy. Have you considered adopting a more fine-grained categorization or another prioritization scheme?\n\n2. For each sampled query from the buffer, it is mentioned that the entropy of all stored candidate trajectories needs to be recomputed at every step with respect to the current policy ($\\pi_{\\theta}$). I’m wondering whether the computational overhead of this process is negligible.\n\n3. To validate the core heuristic of “low entropy,” we used a much more powerful model, Qwen3-32B, as an external judge compared to the 7B target model. I’m curious whether similar patterns would emerge with other models — does this imply that a significantly more capable “teacher” model is required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nst3TTLKHt", "forum": "701tjQXWVk", "replyto": "701tjQXWVk", "signatures": ["ICLR.cc/2026/Conference/Submission10195/Reviewer_B8SA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10195/Reviewer_B8SA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760890656555, "cdate": 1760890656555, "tmdate": 1762921558002, "mdate": 1762921558002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ExGRPO (Experiential Group Relative Policy Optimization), a reinforcement learning framework designed to enhance the reasoning ability of large language models (LLMs) under the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm. Unlike conventional on-policy RLVR methods that discard rollout experiences after each update, ExGRPO reuses past successful experiences through a structured experience management and replay mechanism. The framework partitions experiences by rollout correctness and trajectory entropy, selecting medium-difficulty, low-entropy samples for replay. Experiments across multiple backbones (1.5B–8B models, including Qwen and Llama families) demonstrate consistent improvements in mathematical reasoning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Quality: The empirical dynamics of the algorithm are well investigated by the analysis in Section 6, where theoretical justifications are also provided to ensure the unbiasedness of the proposed formula in Equation 4. Experimental settings follow the standard setup in this field.\n\n2. Clarity: This paper is well written and easy to follow.\n\n3. Novelty: The proposed experience replay management and its combination with on-policy training look new to me, though the accuracy- and entropy-based approach is not new, given the existence of similar literature [1][2] in the past."}, "weaknesses": {"value": "1. Significance:\n  - The selection criteria depend on the metric of correctness, which requires the reward signal to be verifiable. This introduces additional challenges when extending to more general open-domain tasks.\n  - While ExGRPO achieves better performance than the on-policy baseline, it also incurs additional time and memory costs for accuracy recomputation and experience storage. It is worth discussing whether those costs will pose practical challenges or if there are trade-offs between cost and performance.\n\n2. Quality: \n  - More baselines are encouraged to be compared with, such as [1][2][3].\n\n  - Minor issues:\n    + line 276: typo: \"a advantage group\" -> \"an advantage group\"\n\n### References\n[1]: Zhang, Jixiao, and Chunsheng Zuo. \"Grpo-lead: A difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models.\" arXiv preprint arXiv:2504.09696 (2025).\n\n[2]: Chen, Minghan, et al. \"Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization.\" arXiv preprint arXiv:2505.12346 (2025).\n\n[3]: Wei Xiong, et al. A minimalist approach to llm reasoning: from rejection\nsampling to reinforce. arXiv preprint arXiv:2504.11343, 2025.\n\n[4]: Qiying Yu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025"}, "questions": {"value": "* line 258: The choice of the Gaussian distribution seems heuristic here. I am wondering if there is any theoretical justification or empirical evidence to support this choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RD8KLjvZr6", "forum": "701tjQXWVk", "replyto": "701tjQXWVk", "signatures": ["ICLR.cc/2026/Conference/Submission10195/Reviewer_o61x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10195/Reviewer_o61x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856326322, "cdate": 1761856326322, "tmdate": 1762921557590, "mdate": 1762921557590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiency of on-policy RLVR for LLM reasoning, where valuable experience is wasted. The authors first identify that intermediate-difficulty problems and low-entropy or high-certainty trajectories are the most valuable training data. Based on this, they propose ExGRPO, a framework with principled experience replay. It maintains a buffer, groups experiences into buckets by real-time correctness, and prioritizes sampling medium-difficulty tasks, selecting the lowest-entropy trajectory for replay. Experiments show ExGRPO significantly outperforms on-policy baselines, especially in out-of-distribution generalization and training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's primary strength is its methodical approach. It doesn't just propose experience replay; it first runs a diagnostic study in Section 3.2 to prove that medium-difficulty questions and low-entropy trajectories are the most valuable. This provides a strong, data-driven empirical foundation for the ExGRPO framework's design.\n\n2. The average gain of +7.6 OOD points across all models is significant. It suggests that learning from a curated buffer of past successes helps the model form more robust and generalizable reasoning patterns compared to noisy on-policy exploration, which may overfit to in-distribution data.\n\n3. The paper's novelty lies in the principled application of experience replay to RLVR. The idea of dynamically bucketing experiences by online correctness and then selecting by trajectory entropy is a new and intelligent combination of ideas that goes beyond naive replay.\n\n4. The method is not based on an empirical heuristic. The appendix provides a solid theoretical analysis of the mixed-policy objective, its variance bounds, and the role of policy shaping. This adds significant depth and soundness to the proposed framework.\n\n5. The paper is well-written. It logically builds its argument from the preliminary study to the final method, and its explanations of complex concepts are intuitive. The figures and tables are clean, informative, and effectively support the main claims."}, "weaknesses": {"value": "1. The method introduces a replay buffer and additional computation for entropy calculation and sampling per step. A discussion on the impact on memory usage and overall training throughput is missing, which makes it hard to assess the method's true efficiency trade-offs.\n\n2. The entire method hinges on the \"medium-difficulty\" bucket, defined as 25%-75% correctness. This range is presented without justification and is not tested in the ablations. It's unclear how sensitive the model's performance is to this definition. A sensitivity analysis, for example, showing results for 30-70 or 40-60, is a critical missing piece.\n\n3. The method exclusively replays low-entropy successes. This strategy risks two problems: a) it discards \"instructive failures\" which are known to be valuable for learning, and b) it may reduce policy exploration by only reinforcing high-confidence, correct paths, potentially leading to a less diverse policy. The authors do not sufficiently justify this \"success-only\" design choice over other possibilities.\n\n4. The reliance on a binary \"correctness\" signal for bucketing makes the method's application to standard RLHF unclear. These tasks use continuous rewards from a preference model, such as for helpfulness, and it's not obvious how to define \"difficulty\" buckets in that context. This limits the method's generalizability"}, "questions": {"value": "1. As the replay buffer grows to millions of items, is the \"lowest-entropy\" selection (Section 4.1) recalculated for all trajectories in a sampled bucket at every step? If so, this step will become a significant computational bottleneck of this method.\n2. How sensitive is the method to the definition of the difficulty buckets? For instance, what happens to performance if the \"medium\" bucket is defined more narrowly or more broadly?\n3. The paper focuses on replaying low-entropy successes. Do the authors experiment with replaying other types of experiences, such as high-entropy successes or even \"instructive failures\"? Is there a risk of the policy becoming too narrow by only focusing on confident, correct solutions?\n4. Regarding the \"Retired Set\" (Section 4.1): questions are retired after 100% success. Does this risk \"catastrophic forgetting\" of these skills? Would it be beneficial to periodically reintroduce a small fraction of \"Easy\" or \"Retired\" problems back into the training mix to ensure the model retains these capabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Y99uEdq29", "forum": "701tjQXWVk", "replyto": "701tjQXWVk", "signatures": ["ICLR.cc/2026/Conference/Submission10195/Reviewer_7NcB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10195/Reviewer_7NcB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10195/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899403531, "cdate": 1761899403531, "tmdate": 1762921557073, "mdate": 1762921557073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}