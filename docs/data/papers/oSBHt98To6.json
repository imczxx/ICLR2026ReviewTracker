{"id": "oSBHt98To6", "number": 17935, "cdate": 1758282153941, "mdate": 1759897144117, "content": {"title": "Mitigating Watermark Forgery in Generative Models via Randomized Key Selection", "abstract": "Watermarking enables GenAI providers to verify whether content was generated by their models. A watermark is a hidden signal in the content, whose presence can be detected using a secret watermark key. A core security threat are forgery attacks, where adversaries insert the provider's watermark into content \\emph{not} produced by the provider, potentially damaging their reputation and undermining trust. Existing defenses resist forgery by embedding many watermarks with multiple keys into the same content, which can degrade model utility. However, forgery remains a threat when attackers can collect sufficiently many watermarked samples. We propose a defense that is provably forgery-resistant \\emph{independent} of the number of watermarked content collected by the attacker, provided they cannot easily distinguish watermarks from different keys. Our scheme does not further degrade model utility. We randomize the watermark key selection for each query and accept content as genuine only if a watermark is detected by \\emph{exactly} one key. We focus on the image and text modalities, but our defense is modality-agnostic, since it treats the underlying watermarking method as a black-box. Our method provably bounds the attacker's success rate and we empirically observe a reduction from near-perfect success rates to only $2\\%$ at negligible computational overhead.", "tldr": "Novel multi-key watermarking defense method accepts content only with exactly one watermark detected, reducing forgery success from 100% to 2%.", "keywords": ["Content Watermarking", "Large Language Models", "Diffusion", "Forgery", "Security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d91adcfe371ced41e90c6917390258b300589b71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors proposes a defense against watermark forgery attacks in generative AI systems. It introduces a randomized key selection mechanism, where each generation uses a randomly sampled watermark key from a pool, and authenticity is confirmed only if exactly one key detects a watermark. This approachs are provably forgery-resistant even when attackers have access to many watermarked samples, provided they cannot distinguish which key was used. The method preserves model utility and is modality-agnostic, applying to both text and image generation. Empirical results show a reduction in forgery success rates from nearly 100% to as low as 2%, with negligible computational overhead. The authors also provides theoretical bounds, extensive experiments against adaptive and blind attackers, and an open-source implementation, demonstrating the method’s robustness and practicality"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper demonstrates strong originality by reframing watermark forgery prevention as a key-randomization problem, introducing a simple yet powerful defense that does not rely on increasing watermark density or retraining models. Its theoretical contribution, which can be a provable bound on forgery success independent of sample size, representing a conceptual advance over prior multi-key and statistical approaches. The work is of high quality, combining rigorous analysis with extensive empirical validation across text and image modalities, including comparisons to adaptive and blind attackers. I the presentation clarity is systematic, with clear algorithmic descriptions (e.g., Algorithm 1), theoretical assumptions, and visualizations that make the defense mechanism easy to follow. The paper’s significance lies in its practical applicability for securing generative AI outputs, as it provides a low-overhead, easily deployable solution that enhances trust and accountability in content attribution systems, addressing an increasingly critical issue for AI providers. The exposition is clear and well-structured, with intuitive figures (e.g., Figure 1) that help visualize the threat model and defense workflow. The writing maintains a good balance between formalism and accessibility—technical sections are precise, while the main ideas remain easy to follow. The inclusion of algorithms, assumptions, and calibration procedures aids reproducibility."}, "weaknesses": {"value": "The problem is a bit of artificial -- foragaing the content to hurt the reputation of model providers, which can be less important than the standard settings. \nThe adaptive attacks are implemented as either a blind adaptive (averaging) attacker or a stronger informed/key-classification attacker, suggesting. This rests on the key assumption that attacker must obtain either many unlabeled samples (for averaging) or many labeled samples per key (for the informed attack), and the defender’s detection API is assumed inaccessible to the attacker (private detector). Moreover, a better training methodology for distinguishing watermarked content with different keys can be used for designing adaptive attack, such as using reward models trained via Bradley-Terry loss as in Watermarks In the Sand paper.\nIt is better to add latest models like Qwen3, Gemma2 and Llama3 if possible. In general the experiments can be strengthened in various ways.\nFigure 7 has image captions that are overlapped and it is hard to interpret the results."}, "questions": {"value": "I understand the setting can be new but is there any baseline you can add to the plot such as the one using reward models or recursive paraphrasing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aknf9y1idh", "forum": "oSBHt98To6", "replyto": "oSBHt98To6", "signatures": ["ICLR.cc/2026/Conference/Submission17935/Reviewer_h8s7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17935/Reviewer_h8s7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760846036654, "cdate": 1760846036654, "tmdate": 1762927743854, "mdate": 1762927743854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces an add-on for generative content watermarking approaches: using a randomization procedure to sample a key from  a pool of keys $K$ for each user query; during inference, the content is treated as genuinely watermarked iff the presence of exactly one key is significant. The method has a theoretical upper bound for the forgery success of a blind attacker; experimentally a notable decrease in forgery rates are demonstrated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a simple approach: generate a pool of watermarking keys and treat the content as watermarked iff the presence of  exactly one key is not rejected. For considered text watermarking approaches, the method yields negligible detection cost and low false negatives. The experimental evaluation is broad and demonstrates the limits of applicability of an approach."}, "weaknesses": {"value": "1) Independence assumption. For the combinatorial bound to hold, the indicator functions $Z_i$ has to be i.i.d. random variables that, in some schemes, may not be case; the independence under $H_0$ should be carefully verified theoretically, possibly, by restricting the randomization procedure used for key generation. \n\n2) Forgery resistance against attackers who can differentiate between keys is unsatisfactory, making the method barely feasible against adaptive attacks; the only solution is to keep keys private. \n\n3) The overall robustness and quality / perplexity of the generated content is within responsibility of the underlying watermarking approach(es). More than that, watermark detection threshold correction reduces per-key robustness that introduces additional utility-robustness trade-off of the underlying watermarking approach. \n\n4) For the used parameters (namely, the number of keys) the upper bound of the probability of false detection is close to $0.4$ which seems too loose."}, "questions": {"value": "Please elaborate to the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AkGubzjAnl", "forum": "oSBHt98To6", "replyto": "oSBHt98To6", "signatures": ["ICLR.cc/2026/Conference/Submission17935/Reviewer_a6tt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17935/Reviewer_a6tt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571648090, "cdate": 1761571648090, "tmdate": 1762927743350, "mdate": 1762927743350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses an important security problem: watermark forgery. It proposes randomized key selection where the provider has a pool of watermarking keys and randomly selects one for each generation. During verification, the system accepts content as authentic if exactly one key produces a statistically significant signal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper combines cryptographic reasoning and ML experiments. It gives provable guarantees with formal upper bound on forgery success independent of attacker query budget. The paper has done ablations on watermarking datasets and attackers.\nIt offers unified defense for text and image generation"}, "weaknesses": {"value": "The assumption of independent key detectors may not be realistic as there are correlations among watermark signals. This can increate the false positive rates.\nThe paper doesn’t discuss practical deployment aspects like key management, auditing, and scalability."}, "questions": {"value": "1. How might the paper's approach interact with watermark-removal attacks?\n2. How does the method perform when the number of keys r grows large, e.g., changes to false positive rate correction ( Šidák)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J8FHJmfbLs", "forum": "oSBHt98To6", "replyto": "oSBHt98To6", "signatures": ["ICLR.cc/2026/Conference/Submission17935/Reviewer_PkWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17935/Reviewer_PkWN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977332075, "cdate": 1761977332075, "tmdate": 1762927742951, "mdate": 1762927742951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a defense against forgery and watermark-stealing attacks on watermarking schemes for large language models (LLMs). Existing\nschemes often rely on multiple independent keys to resist forgery, but this approach can degrade output quality and utility. The authors introduce a method\nthat samples one key from a distribution for each generation, and detects ownership by statistically verifying that exactly one key is significantly present in\nthe suspect text. If one key dominates, the text is accepted as genuine, if multiple keys appear, it is flagged as a forgery, and if none appear, it is considered unwatermarked. Detection uses a multiple-hypothesis test with Šidák correction to control false positives. Experiments on text and image watermarking demonstrate decreased forgery success compared to baseline single-key and prior multi-key methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles an important practical concern, how to improve robustness without degrading output quality. The main technical argument, detecting “exactly one key” using hypothesis testing, appears sound, and the derivation of false-positive control using Šidák correction is appropriate. The paper also provides theoretical bounds on the success rate of blind attackers. The experimental evaluation is reasonable."}, "weaknesses": {"value": "The theoretical framing is limited to an information-theoretic setting, neglecting the computational model where the problem has already been formally addressed.\n\nThe threat model is mostly clear, but the relationship to cryptographic definitions of unforgeability (e.g., in Christ and Gunn, Eurocrypt 2024) should be discussed explicitly.\n\nWhile the paper’s motivation is well-intentioned, it appears somewhat misplaced in light of recent cryptographic developments. The authors motivate their work by citing Jovanovi´c et al. (2024) and Zhao et al. (2024b), claiming that resisting forgery attacks when the adversary collects N or more watermarked samples remains an open problem. However, this framing overlooks the computational model introduced by Christ and Gunn (CRYPTO 2024).\nIn that line of work, watermarking is defined and analyzed within a cryptographic pseudorandomness framework, where security against forgery follows directly from the pseudorandomness of the underlying code family. Under this model, the “multi-sample forgery” issue treated here as open is already resolved\nsingle-key watermarking schemes can achieve strong unforgeability guarantees against efficient adversaries and even admit practical instantiations by Gunn, Zhao and Song (ICLR 2025 poster)."}, "questions": {"value": "Why did you restrict your analysis to the information-theoretic setting? Given that the computational pseudorandomness framework of Christ and Gunn already captures unforgeability and there is a practical construction based on Pseudorandom error-correction codes.\n\nAlthough the information-theoretic setting is stronger than the computational one, in practice the latter is preferred due to its greater efficiency and ease of applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ReiSjNg6iV", "forum": "oSBHt98To6", "replyto": "oSBHt98To6", "signatures": ["ICLR.cc/2026/Conference/Submission17935/Reviewer_CnUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17935/Reviewer_CnUp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984610979, "cdate": 1761984610979, "tmdate": 1762927742545, "mdate": 1762927742545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}