{"id": "NQ8wms41IJ", "number": 13092, "cdate": 1758213499744, "mdate": 1759897465943, "content": {"title": "Stochastic Truncation for Multi-Step Off-Policy RL", "abstract": "Multi-step off-policy reinforcement learning is crucial for reliable policy evaluation in long-horizon settings. However, extending beyond one-step temporal-difference learning remains challenging due to distribution mismatch between behavior and target policies. This mismatch becomes more severe at longer horizons, resulting in compounding bias and variance.\nExisting approaches generally fall into two categories: conservative methods (e.g., Retrace), which guarantee convergence but often suffer from high variance, and non-conservative methods (e.g., Peng’s $Q(\\lambda)$ and integrated algorithms such as Rainbow), which often achieve strong empirical performance but lack convergence guarantees under arbitrary exploration.\nWe identify horizon selection as a central obstacle and connect it to the mixing time of policy-induced Markov chains. Since mixing time is difficult to estimate online, we derive a practical upper bound through a coupling-based analysis to guide adaptive truncation.\nBuilding on this insight, we propose T4 (Time To Truncate Trajectory), a stochastic and adaptive truncation mechanism within the Retrace framework. We prove that T4 is non-conservative yet converges under arbitrary behavior policies, and is robust to cap-length tuning. Empirically, T4 improves both policy evaluation and control performance over strong baselines on standard RL benchmarks.", "tldr": "", "keywords": ["multi-step off-policy reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/186de90aa8f0fd5e74469cd4f69bca49a40da204.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses key challenges in multi-step off-policy RL: distribution mismatch between behavior/target policies amplifies error with longer horizons, existing conservative (e.g., Retrace) and non-conservative (e.g., Peng’s \\(Q(\\lambda)\\)) methods face trade-offs between convergence and variance, and truncation horizon selection (tied to Markov chain mixing time) is hard to estimate online.\n\nThe authors proposes T4, a stochastic adaptive truncation mechanism under Retrace. T4 uses coupling analysis to derive a mixing time upper bound, employs Bernoulli variables to truncate trajectories at the first state match of behavior/target policies, and is non-conservative yet convergent under arbitrary behavior policies, with robustness to cap length tuning.\n\nEmpirically, T4 (with SAC/TD3) outperforms baselines (one-step, uncorrected n-step, Retrace, Peng’s \\(Q(\\lambda)\\)) in convergence and performance on 5 MuJoCo tasks, and excels in sparse-reward scenarios and matches model-based MBPO’s sample efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It addresses the core issue of multi-step off-policy RL (truncation horizon selection) by proposing T4, an adaptive stochastic truncation mechanism guided by mixing time upper bounds, avoiding the limitations of fixed cap lengths.\n\nT4 achieves a theoretical breakthrough: it is non-conservative (unlike Retrace with high variance) yet guarantees convergence under arbitrary behavior policies (solving the defect of non-conservative methods like Peng’s Q(λ) lacking guarantees)"}, "weaknesses": {"value": "1, The paper fails to clarify the necessity of proposing T4, as it does not analyze the limitations of mature off-policy methods (e.g., GTD2, TDC, ETD,Truncated ETD) in multi-step scenarios or explain T4’s advantages over them\n\n2, It relies on strict, impractical assumptions (uniformly d-bounded kernel, cross-Doeblin condition) for T4’s convergence derivation, with no guidance on parameter tuning in practice\n\n3, Experiments are only conducted on 5 MuJoCo continuous control tasks, lacking validation in discrete-action or sparse-reward environments, and counterexamples such as 2 states counterexample, Baird's counterexample.\n\n4, Baseline comparisons are incomplete—only 4 methods (One-step RL, Retrace, etc.) are included, excluding state-of-the-art off-policy algorithms"}, "questions": {"value": "1, The paper fails to clarify the necessity of proposing T4, as it does not analyze the limitations of mature off-policy methods (e.g., GTD2, TDC, ETD,Truncated ETD) in multi-step scenarios or explain T4’s advantages over them\n\n2, It relies on strict, impractical assumptions (uniformly d-bounded kernel, cross-Doeblin condition) for T4’s convergence derivation, with no guidance on parameter tuning in practice\n\n3, Experiments are only conducted on 5 MuJoCo continuous control tasks, lacking validation in discrete-action or sparse-reward environments, and counterexamples such as 2 states counterexample, Baird's counterexample.\n\n4, Baseline comparisons are incomplete—only 4 methods (One-step RL, Retrace, etc.) are included, excluding state-of-the-art off-policy algorithms"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DThm3tKTVV", "forum": "NQ8wms41IJ", "replyto": "NQ8wms41IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13092/Reviewer_uLFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13092/Reviewer_uLFM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760768582680, "cdate": 1760768582680, "tmdate": 1762923818423, "mdate": 1762923818423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces T4 (Time To Truncate Trajectory), an adaptive method for multi-step off-policy reinforcement learning that addresses the instability caused by long-horizon return. The core idea is to dynamically truncate each trajectory's return based on an estimate of the \"mixing time\" of the underlying policies, which is the point where the data-generating (behavior) policy and the target policy's state distributions are likely to converge. This avoids the bias and variance issues that arise from using fixed, long n-step returns."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It provides a new, principled way to determine the horizon for off-policy returns by linking it to Markov chain mixing times, moving beyond ad-hoc fixed horizons.\n2. The T4 algorithm is proven to converge to the optimal value function, even when using arbitrary, changing behavior policies, a significant guarantee that many multi-step methods lack.\n4. When integrated with state-of-the-art algorithms like SAC and TD3 on continuous control benchmarks, T4 consistently improves performance and learning speed compared to one-step and other multi-step methods.\n4. The method is shown to be insensitive to the maximum truncation length hyperparameter, reducing the need for environment-specific tuning."}, "weaknesses": {"value": "1. The convergence proofs rely on strong assumptions (like uniform ergodicity) that may not hold in all real-world environments, creating a gap between theory and practice.\n2. The validation is confined to continuous control tasks (MuJoCo). Its effectiveness in other domains like discrete action spaces (e.g., Atari) or tasks with very sparse rewards has not been demonstrated.\n3. The empirical analysis could be deeper. It doesn't fully explore the behavior of the truncation mechanism itself (e.g., how the chosen horizon adapts during training) or directly measure the bias-variance trade-off.\n4. While robust to the main truncation cap, the method relies on a specific heuristic to estimate policy overlap, and its sensitivity to this choice or the discount factor (\\gamma) is not fully explored."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4veuocqvVO", "forum": "NQ8wms41IJ", "replyto": "NQ8wms41IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13092/Reviewer_pYMM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13092/Reviewer_pYMM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848022767, "cdate": 1761848022767, "tmdate": 1762923818044, "mdate": 1762923818044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose T4 (Time To Truncate Trajectory), a stochastic and adaptive truncation mechanism designed to address the core difficulty of horizon selection in off-policy multi-step settings. Traditional fixed-length truncation either amplifies distribution mismatch or loses long-horizon information, leading to instability and bias. T4 resolves this by connecting truncation length to the mixing time of policy-induced Markov chains and estimating an adaptive cutoff through a coupling-based analysis. Technically, T4 reformulates the Retrace operator using Bernoulli random variables that represent stepwise disagreement between behavior and target trajectories, stochastic ally terminating credit propagation once the trajectories are expected to align. The paper establishes that T4 is non-conservative yet convergent under arbitrary behavior policies, offering both theoretical guarantees and practical robustness. Empirical results on MuJoCo and sparse-reward benchmarks demonstrate consistent gains over Retrace, Peng’s $Q(\\lambda)$, and n-step baselines, with comparable sample efficiency to model-based MBPO while remaining fully model-free. Overall, the paper provides a principled integration of coupling-based theory and algorithmic design, yielding an adaptive, theoretically grounded alternative to fixed-horizon multi-step RL, though the reliance on bounded-kernel and ergodicity assumptions may restrict its direct applicability to more complex or non-stationary environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a principled stochastic truncation method (T4) that links horizon selection in multi-step off-policy reinforcement learning to the mixing time of the policy-induced Markov chain. This connection provides a theoretical basis for adaptive horizon control and helps address the long-standing problem of balancing bias and variance in multi-step updates.\n2. The T4 operator reformulates the Retrace framework using Bernoulli random variables to stochastically determine the truncation point. It is shown to be non-conservative yet convergent under arbitrary behavior policies, offering a unified and flexible solution that avoids the instability and heavy tuning required by fixed-length multi-step methods.\n3. The experiments are comprehensive and well-structured, covering standard continuous control and sparse-reward tasks. T4 consistently improves both sample efficiency and stability over baselines such as Retrace, Peng’s $Q(\\lambda)$, and n-step methods, while approaching the performance of model-based algorithms like MBPO despite remaining entirely model-free."}, "weaknesses": {"value": "1. There seems to be a semantic mismatch between the theoretical quantity $p_i = \\Pr(S_i^\\beta \\neq S_i^\\pi)$, introduced in Sec. 3 as the probability that the two coupled chains driven by $\\beta$ and $\\pi$ have not yet met at step $i$, and the practical estimator $\\hat p_i = 1 - \\min(\\beta(a_i \\mid s_i), \\pi(a_i \\mid s_i))$used in Sec. 4.1. The former is a state-/kernel-level disagreement that depends on the whole transition kernel under both policies (cf. Eq. (7)), and it is exactly this quantity that appears in the coupling argument leading to the truncated operator and the contraction statement in Thm. 1. The latter, however, is a single-sample, action-level proxy computed from the sampled pair $(s_i, a_i)$, and in general it does not control the total-variation overlap of the next-state distributions induced by $\\beta$ and $\\pi$. Since the theoretical guarantees hinge on the kernel-level bound, could the authors justify that replacing $p_i$ by $\\hat p_i$ still yields the same contraction (or an equivalent domination inequality) required by Thm. 1? If this cannot be shown, please make explicit what the practical algorithm in Sec. 4.1 is actually guaranteed to approximate (e.g., a biased but stable truncation, or a contraction with a weaker constant).\n2. It seems the experiments do not align the method in the setting it is designed for, i.e., when the behavior policy is far from the target policy or mixes slowly. The results in Sec.  5 are all on standard continuous-control tasks with SAC/TD3-like training and replay, where the behavior is close to the current policy and the off-policy gap is small, so fixed-$n$ targets and Retrace are already fairly stable. This does not show that the proposed truncation helps when $T_{\\beta,\\pi}$ is large or the data come from older/heterogeneous policies. Can the authors add runs with deliberately stale or mismatched behavior to demonstrate the claimed advantage?\n3. The stochastic truncation intuitively introduces additional randomness on top of trajectory sampling. Please quantify its variance compared to deterministic truncation (e.g., fixed $n$) and show that the overall MSE is indeed smaller in the new scenario."}, "questions": {"value": "Most of my concerns are already detailed in the cons section. I would be happy to raise the score if the authors can address them convincingly. One remaining question: Can the author provide a practical example when assumption 1 is satisfied?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "adWKCaqqed", "forum": "NQ8wms41IJ", "replyto": "NQ8wms41IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13092/Reviewer_PfVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13092/Reviewer_PfVi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924402886, "cdate": 1761924402886, "tmdate": 1762923817640, "mdate": 1762923817640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes T4 (Time To Truncate Trajectory), a stochastic and adaptive truncation mechanism for multi-step off-policy reinforcement learning. The core idea is to treat the truncation horizon as a stochastic variable determined by the meeting time between trajectories sampled under the behavior and target policies.\nBy linking this meeting time to the mixing time of the underlying Markov chain through coupling analysis, the paper provides a principled framework for adaptive truncation within the Retrace formulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel theoretical framing: The paper introduces a coupling-based view of trajectory truncation, connecting horizon length to mixing time — an elegant and original perspective that generalizes Retrace.\n2. Solid theoretical guarantees: The contraction and convergence results (Theorems 1 & 2) are well-motivated and broadly applicable under mild assumptions ($p_i \\le \\xi$, $\\gamma < \\tfrac{1}{1+\\xi}$).\n3. Relevance: Horizon selection and cumulative variance amplification remain open issues in multi-step off-policy RL, and this work moves the discussion toward adaptive control of truncation length."}, "weaknesses": {"value": "1. Empirical ambiguity:\n- Figure 1 claims to show “cumulative error scaling,” yet it only displays average return vs. environment steps; no error metric or bias/variance quantification is presented.\n- Figure 3 shows marginal improvements—SAC-T4 only occasionally underperforms SAC. Also, I would suggest changing the color of either MBPO or SAC, as they are hard to distinguish visually.\n- Figure 4 is less naturally integrated: the paper switches from MuJoCo continuous control to CartPole without explaining why, how λ-values are chosen, or how sparse rewards relate to previous experiments.\n\n2. Fairness and tuning:\nIn Figure 4, T4 is tested with $\\lambda \\in \\{0.3, 0.7, 0.9, 1\\}$ while Peng $Q(\\lambda)$ is shown only for $\\lambda \\in \\{0.9, 1\\}$, producing an unbalanced comparison. The figure shows better performance for T4 at $\\lambda = 0.3, 0.7$, but Peng $Q(\\lambda)$ with the same $\\lambda$ values is never tested. Besides, when sharing  the same $\\lambda$ ($0.9, 1$), T4 is clearly outperformed by the Peng baseline.\n\n3. Missing definitions:\nEquation (3) introduces the total-variation (TV) norm without defining it or referring readers to Appendix A, where the definition appears later. Equation (4) uses $\\Omega(\\tau_{\\text{mix}})$ without explaining that it is asymptotic notation (e.g., meaning “at least proportional to $\\tau_{\\text{mix}}$.”)\n\n4. Figure clarity and aesthetics:\nIn Figure 3, like mentioned above, SAC and MBPO curves are rendered in very similar colors, making them difficult to distinguish.\nCaptions throughout the paper are a bit terse and omit key experimental details (e.g., environment, reward structure, hyperparameter settings)."}, "questions": {"value": "1. Could you please clarify what exactly “cumulative error scaling” refers to in Figure 1? If this corresponds to bias or variance accumulation, consider including an explicit quantitative measure (e.g., mean-squared TD error) to strengthen the connection between theory and experiment.\n2. Please elaborate on the motivation for switching from MuJoCo to CartPole in Figure 4. A brief explanation of the sparse-reward setting and its relevance to your overall argument would make the comparison clearer.\n\nOthers: see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZddyjFYJQv", "forum": "NQ8wms41IJ", "replyto": "NQ8wms41IJ", "signatures": ["ICLR.cc/2026/Conference/Submission13092/Reviewer_nSE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13092/Reviewer_nSE5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962454251, "cdate": 1761962454251, "tmdate": 1762923817180, "mdate": 1762923817180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}