{"id": "MKxKKsz0cx", "number": 232, "cdate": 1756731990842, "mdate": 1763236906248, "content": {"title": "Multi-LCB: Extending LiveCodeBench to Multiple Programming Languages", "abstract": "LiveCodeBench (LCB) has recently become a widely adopted benchmark for evaluating large language models (LLMs) on code-generation tasks. By curating competitive programming problems, constantly adding fresh problems to the set, and filtering them by release dates, LCB provides contamination-aware evaluation and offers a holistic view of coding capability. However, LCB remains restricted to Python, leaving open the question of whether LLMs can generalize across the diverse programming languages required in real-world software engineering.\n\nWe introduce Multi-LCB, a benchmark for evaluating LLMs across twelve programming languages, including Python.\nMulti-LCB transforms Python tasks from the LCB dataset into equivalent tasks in other languages while preserving LCB’s contamination controls and evaluation protocol.\nBecause it is fully compatible with the original LCB format, Multi-LCB will automatically track future LCB updates, enabling systematic assessment of cross-language code generation competence and requiring models to sustain performance well beyond Python.\n\nWe evaluated 20 LLMs for instruction and reasoning on Multi-LCB, uncovering evidence of Python overfitting, language-specific contamination, and substantial disparities in multilingual performance. Our results establish Multi-LCB as a rigorous new benchmark for multi-programming-language code evaluation, directly addressing LCB’s primary limitation and exposing critical gaps in current LLM capabilities. All prompts, source code and experimental configurations are publicly available at https://anonymous.4open.science/r/Multi-LiveCodeBench-C627/", "tldr": "We introduce a contamination-aware benchmark that evaluates code LLMs across 12 programming languages.", "keywords": ["Code Benchmark; Code LLMs; Cross Language Evaluation; Contamination; Overfitting"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22312912d6ef48bef422e649457efbfd61c1ce2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends LiveCodeBench (LCB) from Python to twelve programming languages and proposes Multi-LCB, a contamination-aware, continuously updating benchmark for multilingual code generation.\nThe authors convert LCB tasks into a unified STDIN/STDOUT evaluation pipeline and evaluate 20 recent LLMs.\nTheir findings include (i) Python overfitting; (ii) language-specific contamination signals via post-cutoff time slicing; and (iii) cross-language performance disparities.\nThe paper argues that Multi-LCB preserves LCB’s contamination controls while enabling side-by-side, same-task comparisons across languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Meaningful benchmark**, which can be directly used to evaluate and compare LLMs' code generation capability across different PLs \n2. **Large-scale experiments with insightful findings**. Cover diverse programming languages and evaluate 20 LLMs, providing findings to reveal real multilingual gaps in coding tasks.\n3. **Additional results and Open-sourced code**. Provide additional results and implementation details in the appendix and also release the scripts and datasets in an anonymous repository."}, "weaknesses": {"value": "1. **Limited Measurement of the Generalization on Different PL**. This paper aims to answer the question of whether LLMs can generalize across diverse programming languages (in abstract).\nWhen evaluating a model's capability in a specific language, it's crucial to assess its capability in utilizing that language's unique features and built-in libraries  (e.g., Rust ownership/lifetimes, Go concurrency primitives, JS/TS async patterns, use of standard libraries). However, since all tasks were converted from Python tasks, one concern is that these tasks may be language-specific and thus fail to effectively measure the model's performance in using syntax and features related to other languages. Consequently, it cannot effectively measure the LLMs' capability in using different programming languages. This may limit the contribution of this benchmark.\n2. **Insufficient handling of the differences between the syntax/features of different languages**. The outputs of certain tasks may depend on language-specific syntax/features (e.g., the measurement of Unicode length is different in Python, JS, and TS (code points VS code units), and modulo with negative inputs may have different results across languages). If such samples are directly translated from a Python dataset, the evaluation may unfairly penalize other languages and understate LLMs' competence on them.\n3. **Contamination detection across languages**. Beyond filtering by the release date of the original Python problem, the paper does not provide other contamination detection methods. One of my concerns is that the rewritten/converted samples may have textual or semantical overlap with samples in the prior dataset, leading to data contamination and inflating scores of certain models."}, "questions": {"value": "1.  This paper seems to only measure LLMs' capability to use different programming languages ​​to solve the same algorithm problem, which may not comprehensively reflect  LLMs' capability to use syntax and features unique to other languages, thereby providing a one-sided measurement.\nPlease discuss and clarify the contribution of this paper and what specific research questions this paper wants to answer.\n2. How does the author filter or process these samples related to the unique features of the programming language? What are their effects on the assessment of models' capability on different PLs in your experiments?\n3. Besides the release-date filtering, what contamination detection methods are used on the converted samples in different languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X4H2gb6EnW", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_EauN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_EauN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441404455, "cdate": 1761441404455, "tmdate": 1762915476031, "mdate": 1762915476031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends LiveCodeBench (LCB) from Python to twelve programming languages and proposes Multi-LCB, a contamination-aware, continuously updating benchmark for multilingual code generation.\nThe authors convert LCB tasks into a unified STDIN/STDOUT evaluation pipeline and evaluate 20 recent LLMs.\nTheir findings include (i) Python overfitting; (ii) language-specific contamination signals via post-cutoff time slicing; and (iii) cross-language performance disparities.\nThe paper argues that Multi-LCB preserves LCB’s contamination controls while enabling side-by-side, same-task comparisons across languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Meaningful benchmark**, which can be directly used to evaluate and compare LLMs' code generation capability across different PLs \n2. **Large-scale experiments with insightful findings**. Cover diverse programming languages and evaluate 20 LLMs, providing findings to reveal real multilingual gaps in coding tasks.\n3. **Additional results and Open-sourced code**. Provide additional results and implementation details in the appendix and also release the scripts and datasets in an anonymous repository."}, "weaknesses": {"value": "1. **Limited Measurement of the Generalization on Different PL**. This paper aims to answer the question of whether LLMs can generalize across diverse programming languages (in abstract).\nWhen evaluating a model's capability in a specific language, it's crucial to assess its capability in utilizing that language's unique features and built-in libraries  (e.g., Rust ownership/lifetimes, Go concurrency primitives, JS/TS async patterns, use of standard libraries). However, since all tasks were converted from Python tasks, one concern is that these tasks may be language-specific and thus fail to effectively measure the model's performance in using syntax and features related to other languages. Consequently, it cannot effectively measure the LLMs' capability in using different programming languages. This may limit the contribution of this benchmark.\n2. **Insufficient handling of the differences between the syntax/features of different languages**. The outputs of certain tasks may depend on language-specific syntax/features (e.g., the measurement of Unicode length is different in Python, JS, and TS (code points VS code units), and modulo with negative inputs may have different results across languages). If such samples are directly translated from a Python dataset, the evaluation may unfairly penalize other languages and understate LLMs' competence on them.\n3. **Contamination detection across languages**. Beyond filtering by the release date of the original Python problem, the paper does not provide other contamination detection methods. One of my concerns is that the rewritten/converted samples may have textual or semantical overlap with samples in the prior dataset, leading to data contamination and inflating scores of certain models."}, "questions": {"value": "1.  This paper seems to only measure LLMs' capability to use different programming languages ​​to solve the same algorithm problem, which may not comprehensively reflect  LLMs' capability to use syntax and features unique to other languages, thereby providing a one-sided measurement.\nPlease discuss and clarify the contribution of this paper and what specific research questions this paper wants to answer.\n2. How does the author filter or process these samples related to the unique features of the programming language? What are their effects on the assessment of models' capability on different PLs in your experiments?\n3. Besides the release-date filtering, what contamination detection methods are used on the converted samples in different languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X4H2gb6EnW", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_EauN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_EauN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761441404455, "cdate": 1761441404455, "tmdate": 1763605157206, "mdate": 1763605157206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This project extends the LiveCodeBench (LCB) benchmark to support multiple programming languages. Specifically, the authors translated the original Python-based problems into 11 additional languages, resulting in a total of 12 supported languages. They also developed an automated mechanism to track and incorporate future LCB updates, ensuring long-term maintainability of the benchmark. Using the new Multi-LCB dataset, the authors conducted a comprehensive evaluation of 20 large language models (LLMs), revealing that most models exhibit substantially lower performance on languages other than Python due to overfitting, and uneven distribution of pertaining corpora in terms of programming language diversity. The authors publicly release their benchmark extension, including prompts, source code, and configurations under MIT license to facility reproduction and future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The work extends a state-of-the-art (SOTA) benchmark while preserving the core strengths of the original LiveCodeBench. The design also emphasizes long-term maintainability through automated update tracking.\n* The evaluation framework is relevant for practitioners, addressing key technical challenges such as automatically converting code problems and hidden test cases into the appropriate format, as well as supporting sandboxed execution.\n* The paper provides valuable insights into benchmark contamination by using release date metadata to perform experiments, highlighting an important issue in LLM evaluation.\n* The evaluation includes a sanity check by comparing Python results on both the original LCB and the proposed Multi-LCB, demonstrating methodological rigor."}, "weaknesses": {"value": "* The work does not introduce new task types. Since certain programming languages have inherent advantages for specific problems, adding novel or language-agnostic tasks could have strengthened the contribution’s originality.\n* While the overall presentation is solid, there are opportunities to enhance transparency in specific methodological aspects (see Questions section).\n* It is unclear whether the proposed benchmark covers all existing LCB releases or only the latest one.\n* The benchmark appears less challenging for LLMs on widely used programming languages — strong performance is reported for C++, Java, PHP, C#, and JavaScript.\n* The evaluation omits GPT-OSS and leading proprietary models, limiting the completeness of the comparative analysis."}, "questions": {"value": "1. My understanding is that LCB releases are non-overlapping and cumulative, each representing a temporal evaluation slice. Did you convert tasks from all releases or only from the latest one? The paper notes that your pipeline supports the conversion of multiple versions, but it is not explicit whether all versions were actually converted.\n2. Lines 205–215 mention adopting a zero-shot strategy, yet Step 2 refers to the inclusion of illustrative examples. Could you clarify how these examples fit within a zero-shot setup?\n3. Please elaborate on the infrastructure or practical constraints that prevented inclusion of the Swift programming language.\n4. I recommend adding evaluations for GPT-OSS as well as leading closed-source models to strengthen the study’s benchmarking scope.\n5. In your view, which factor contributes more to improving cross-language performance — multi-language training or enhanced reasoning capabilities?\n6. Please explain how the evaluation framework supports long-term maintainability through automated update tracking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DnxZB0Brbs", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_52p9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_52p9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587854060, "cdate": 1761587854060, "tmdate": 1762915475925, "mdate": 1762915475925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MULTI-LCB, an extension of the existing LiveCodeBench (LCB) benchmark from Python to 12 programming languages. This paper design an automated pipeline that converts function-based LeetCode problems into standardized STDIN/STDOUT tasks and execute them within an isolated sandbox environment. The paper reports Pass@1 results for 20 publicly available large language models, analyzing cross-language differences, contamination effects, model scaling, and fine-tuning strategies.\n\nThe major contributions are:\n1. Expands LCB from a single-language (Python) setup to 12 languages.\n2. Standardizes problem I/O specifications to enable consistent cross-language evaluation.\n3. Preserves LCB’s time-based cutoff to mitigate training-data leakage.\n4. Benchmarks 20 code generation models under a unified framework.\n\nOverall, the work demonstrates strong engineering and community value, providing a practical foundation for multi-language evaluation of code models. However, its methodological novelty is limited."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High engineering quality: Large-scale multi-language extension with reproducible infrastructure.\n\n2. Comprehensive coverage: Evaluation across 20 models and 12 languages under consistent settings.\n\n3. Community relevance: Provides a standardized and contamination-controlled environment for fair model comparison.\n\n4. Clear presentation: Tables and figures effectively summarize results, aiding interpretability."}, "weaknesses": {"value": "1. The authors extend LiveCodeBench from Python to 12 languages (e.g., C++, Java, Rust, Go, Kotlin), building a unified STDIN/STDOUT interface and Docker-based sandbox for consistent evaluation. The implementation is robust and reproducible.\n\n2. The benchmark covers 20 models (instruction-tuned, reasoning-tuned, etc.) under identical settings. Results across tables and heatmaps show consistent model ranking and meaningful trends in language difficulty.\n\n3. The continuation of LCB’s time-based contamination control adds credibility, and the planned release of code and conversion scripts will make this a useful resource for the code-generation community.\n\n4. Figures and tables are easy to interpret, and the comparison with the original Python subset (differences within a few points) supports reproducibility."}, "questions": {"value": "1. The work mainly represents a large-scale re-engineering of an existing benchmark rather than a conceptual or methodological innovation.\n\n2. The paper does not quantify whether converting function-based problems to STDIN/STDOUT truly preserves difficulty, leaving uncertainty about potential format-induced biases across languages.\n\n3. Only Pass@1 is reported; missing Pass@k and error breakdowns (e.g., compilation vs. runtime) limit interpretability of language gaps.\n\n4. The “Python advantage” is discussed but not supported by controlled analysis; other confounding factors such as type strictness or dataset differences are not explored.\n\n5. Some deviations from the LCB Python results (up to ~8%) are acknowledged but not analyzed, reducing confidence in full cross-language equivalence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HPms2uFLmK", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_QT4h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_QT4h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796608082, "cdate": 1761796608082, "tmdate": 1762915475670, "mdate": 1762915475670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Multi-LCB a multilingual extension of LiveCodeBench (LCB) that evaluates large language models across 12 programming languages (instead of just Python). The Multi-LCB benchmark is built by converting LCB into a unified STDIN/STDOUT format — which works across all supported languages. The authors present a robust evaluation of 20 large language models, identifying important trends on programming language bias in performance and contamination trends."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "- Automatic conversion of LCB into 12 languages is a significant contribution to the community.\n- Presents clear evidence for a bias towards certain language, in particular with python performance being consistently higher than other languages. The results also show clear hierarchies (Figure 4) of performance with python, c++, and java in the highest performing category.\n- Moreover, the results of performance over time, highlighting important trends resulting from large pretrained models including various benchmarks in the training set. The proposed Multi-LCD offers to track such information.\n- The rigorous methods are also a strength:\n  - Evaluation setup: 20 diverse models (7B-685B parameters), evaluation within a sandboxed execution, and strict resource limits, and a balanced evaluation for each language via common task sets.\n  - Care was taken to create reproducible results. Specifically, the authors give compiler versions, inference parameters, and public code release.\n- Clear presentation: the writing is clear and the figures clearly communicate important findings."}, "weaknesses": {"value": "\"These results confirm that strong Python ability is not a reliable proxy for true cross-lingual code generation competence.\"\nIt's unclear why the authors make this claim — to me their results suggest the opposite. Specifically, Figure 3 shows a clear correlation between python performance and average performance across all other languages. The benchmarks can show a bias towards performing better on python while still be proxy for performance on other languages. This claim also contradicts prior works that show multilingual models perform best because performance is correlated across difference languages.\n\nMissing citation: [\"Multi-Lingual Evaluation of Code Generation Models\"](https://arxiv.org/pdf/2210.14868) is a well cited paper on converting monolingual datasets to multilingual code, including the popular python dataset MBPP. This is an important citation in this area and very close to your approach, so you may want to explicitly clarify how the two approaches differ. Athiwaratkun et. al provide already provide an approach for converting benchmarks which enables direct comparison across languages.\n\nFor benchmarking purposes it would have been nice to see how commercial models like GPT5, Claude, and Gemini perform.\n\nThe authors only present results from `pass@1`, which is valid, however showing results with a greater sampling budget would help: (1) add to significance of findings (which are missing), and (2) more importantly `pass@k` metrics tend to show a sigmoid relationship between `k` and performance. In other words, for higher values of `k` we're likely to see smaller gaps between \"easier\" and \"harder\" languages, and the gap between python and other languages may be noticeably smaller. Likewise, the authors use a temperature at 0.2, which is quite low and may be preventing more exploration and possibly negatively biasing languages that make up a small portion of the training data.\n\n\"we evaluate 20 recent large language models on Multi-LCB, restricting tasks to those released after 2025-02-01 to ensure live, post-cutoff evaluation and minimize any risk of training-data leakage\"\nThis effort to avoid contamination is good and likely significant helps avoid contamination, but it's unlikely to \"ensure\" contamination since problems may have existed in other areas previously. LLMs have such large and broad pretraining datasets that indirect contamination is always possible."}, "questions": {"value": "Please clarify how this approach is novel with regard to Athiwaratkun et. al. For example, why not just use the approach they introduced for creating MBXP? Are there advantages to the Multi-LCB conversion method or is the primary contribution the benchmark dataset itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FjXJVLYt1v", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_VdKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_VdKt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855129696, "cdate": 1761855129696, "tmdate": 1762915475516, "mdate": 1762915475516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Multi-LCB a multilingual extension of LiveCodeBench (LCB) that evaluates large language models across 12 programming languages (instead of just Python). The Multi-LCB benchmark is built by converting LCB into a unified STDIN/STDOUT format — which works across all supported languages. The authors present a robust evaluation of 20 large language models, identifying important trends on programming language bias in performance and contamination trends."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Automatic conversion of LCB into 12 languages is a significant contribution to the community.\n- Presents clear evidence for a bias towards certain language, in particular with python performance being consistently higher than other languages. The results also show clear hierarchies (Figure 4) of performance with python, c++, and java in the highest performing category.\n- Moreover, the results of performance over time, highlighting important trends resulting from large pretrained models including various benchmarks in the training set. The proposed Multi-LCD offers to track such information.\n- The rigorous methods are also a strength:\n  - Evaluation setup: 20 diverse models (7B-685B parameters), evaluation within a sandboxed execution, and strict resource limits, and a balanced evaluation for each language via common task sets.\n  - Care was taken to create reproducible results. Specifically, the authors give compiler versions, inference parameters, and public code release.\n- Clear presentation: the writing is clear and the figures clearly communicate important findings."}, "weaknesses": {"value": "\"These results confirm that strong Python ability is not a reliable proxy for true cross-lingual code generation competence.\"\nIt's unclear why the authors make this claim — to me their results suggest the opposite. Specifically, Figure 3 shows a clear correlation between python performance and average performance across all other languages. The benchmarks can show a bias towards performing better on python while still be proxy for performance on other languages. This claim also contradicts prior works that show multilingual models perform best because performance is correlated across difference languages.\n\nMissing citation: [\"Multi-Lingual Evaluation of Code Generation Models\"](https://arxiv.org/pdf/2210.14868) is a well cited paper on converting monolingual datasets to multilingual code, including the popular python dataset MBPP. This is an important citation in this area and very close to your approach, so you may want to explicitly clarify how the two approaches differ. Athiwaratkun et. al provide already provide an approach for converting benchmarks which enables direct comparison across languages.\n\nFor benchmarking purposes it would have been nice to see how commercial models like GPT5, Claude, and Gemini perform.\n\nThe authors only present results from `pass@1`, which is valid, however showing results with a greater sampling budget would help: (1) add to significance of findings (which are missing), and (2) more importantly `pass@k` metrics tend to show a sigmoid relationship between `k` and performance. In other words, for higher values of `k` we're likely to see smaller gaps between \"easier\" and \"harder\" languages, and the gap between python and other languages may be noticeably smaller. Likewise, the authors use a temperature at 0.2, which is quite low and may be preventing more exploration and possibly negatively biasing languages that make up a small portion of the training data.\n\n\"we evaluate 20 recent large language models on Multi-LCB, restricting tasks to those released after 2025-02-01 to ensure live, post-cutoff evaluation and minimize any risk of training-data leakage\"\nThis effort to avoid contamination is good and likely significant helps avoid contamination, but it's unlikely to \"ensure\" contamination since problems may have existed in other areas previously. LLMs have such large and broad pretraining datasets that indirect contamination is always possible."}, "questions": {"value": "Please clarify how this approach is novel with regard to Athiwaratkun et. al. For example, why not just use the approach they introduced for creating MBXP? Are there advantages to the Multi-LCB conversion method or is the primary contribution the benchmark dataset itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FjXJVLYt1v", "forum": "MKxKKsz0cx", "replyto": "MKxKKsz0cx", "signatures": ["ICLR.cc/2026/Conference/Submission232/Reviewer_VdKt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission232/Reviewer_VdKt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission232/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855129696, "cdate": 1761855129696, "tmdate": 1763570881924, "mdate": 1763570881924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}