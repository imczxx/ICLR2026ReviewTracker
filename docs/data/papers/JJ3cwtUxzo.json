{"id": "JJ3cwtUxzo", "number": 1169, "cdate": 1756857210586, "mdate": 1759898224097, "content": {"title": "Near-Optimal Online Deployment and Routing for Streaming LLMs", "abstract": "The rapid pace at which new large language models (LLMs) appear, and older ones become obsolete, forces providers to manage a streaming inventory under a strict concurrency cap and per-query cost budgets. We cast this as an online decision problem that couples *stage-wise deployment* (at fixed maintenance windows) with *per-query routing* among live models. We introduce *StageRoute*, a hierarchical algorithm that (i) optimistically selects up to $M_{\\max}$ models for the next stage using reward upper-confidence and cost lower-confidence bounds, and (ii) routes each incoming query by solving a budget- and throughput-constrained bandit subproblem over the deployed set. We prove a regret of $\\tilde{\\mathcal{O}}(T^{2/3})$ with a matching lower bound, establishing near-optimality, and validate the theory empirically: *StageRoute* tracks a strong oracle under tight budgets across diverse workloads.", "tldr": "StageRoute periodically redeploys LLMs and cost-aware routes queries online to track a streaming model frontier with near-optimal regret.", "keywords": ["online learning", "bandits", "LLM routing", "staged deployment", "streaming model arrivals", "regret bounds", "budget/capacity constraints"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8534f6eddbfac0220c4002ee6db59cf97591df33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of online deployment and routing for streaming large language models in a realistic cloud setting where new models arrive over time and operational constraints apply. The authors formalize a novel two-timescale decision framework: at infrequent deployment stages, the system chooses which models to keep deployed (up to $M_{\\max}$ models) and at the fine-grained level, each incoming query is routed to one of the deployed models to maximize quality under cost and throughput constraints. The paper proposes an algorithm called StageRoute, which (i) uses optimistic estimates of performance (UCB) and conservative estimates of cost (LCB) to select a near-optimal subset of models at each stage, incurring any one-time deployment costs, and (ii) routes each query among the chosen models by solving a linear program that respects the long-term budget and per-model rate limits. The authors prove a regret bound of $\\widetilde{O}(T^{2/3})$ for StageRoute (with a matching $\\widetilde{O}(T^{2/3})$ lower bound), indicating that the algorithm’s performance loss relative to an oracle is sublinear in the number of queries and essentially near-optimal for this problem setting. Empirically, StageRoute is evaluated on a benchmark (36k+ queries across 8 diverse datasets) and is shown to closely track the performance of an oracle under strict cost budgets while significantly outperforming baseline strategies, demonstrating both the practical effectiveness and robustness of the approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel Problem Formulation: The paper identifies and formalizes a unique and important problem at the intersection of LLM systems and online learning. It is, to the best of the authors’ knowledge, the first to address online LLM deployment with streaming model arrivals while accounting for a hard concurrency cap on active models, one-time deployment costs, per-model throughput limits, and a global cost budget. This formulation fills a clear gap in prior work and it mirrors real-world constraints.\n\nWell-Designed StageRoute Algorithm: The proposed StageRoute algorithm elegantly mirrors the two-level structure of the problem. It combines strategic exploration-exploitation at deployment times with tactical optimal routing for each query. The deployment phase uses upper-confidence bounds on reward and lower-confidence bounds on cost to select a set of up to $M_{\\max}$ models, ensuring a principled trade-off between trying new models and sticking with reliable ones under uncertainty. Then, within each stage, the routing phase solves a linear programming subproblem to dispatch queries in a way that maximizes expected quality while respecting the budget and per-model throughput caps. This hierarchical approach is conceptually sound and modular – for instance, the routing step could incorporate contextual features if available, without changing the deployment step. Overall, the algorithmic design addresses the identified challenges in a logical and effective manner.\n\nTheoretical Rigor: A key strength is the thorough theoretical analysis. The authors establish an $\\widetilde{O}(T^{2/3})$ regret bound for StageRoute and also prove a matching lower bound for any policy in this setting, implying that StageRoute achieves essentially the best possible learning rate for the given problem. This is a non-trivial result, as the problem’s structure (staged decisions, streaming new arms, budget and capacity constraints) is more complex than standard bandits. The regret decomposition provides insight into the costs of limited concurrency and delayed updates. Such theoretical guarantees lend strong support to the algorithm’s reliability and novelty. It’s commendable that the paper not only proposes a solution but also proves its near-optimality, meeting a high bar of technical depth.\n\nClarity and Presentation: The paper is clearly written and well-organized, which aids in understanding a complex problem. The authors provide a helpful conceptual diagram (Figure 1) and use it to explain the StageRoute workflow intuitively. The constraints and assumptions are well-motivated by real-world examples. Each component of the solution is described with sufficient detail, and important aspects like the modularity of the design are highlighted (e.g., noting that contextual routing or other extensions can be plugged in). The combination of theoretical and empirical sections is balanced, and the contributions are explicitly itemized early on. This clarity ensures that readers can follow the motivation, the algorithm, and the significance of results without confusion."}, "weaknesses": {"value": "Computational Complexity and Scalability: A potential concern is the computational overhead of StageRoute’s deployment phase. Selecting the active model set is formulated as an optimization problem with combinatorial constraints (respecting the $M_{\\max}$ cap, budget, etc.), which the authors solve using a mixed-integer programming (MIP) solver in their experiments. This approach may not scale well if the number of candidate models grows large or if deployment decisions need to be made very frequently. The paper does not report runtime or complexity analysis for this MIP step, so it’s unclear how feasible StageRoute would be in real-time or large-scale deployments without further engineering. While throughput limits help throttle query routing, the stage-wise optimization might become a bottleneck as the system scales, and discussing this would strengthen the paper.\n\nBaseline Limitations: Given the novelty of the problem setting, the baselines used for comparison are relatively simple. The paper compares StageRoute mainly against a greedy heuristic (select top-$M_{\\max}$ models by a UCB/LCB-derived utility) and a uniform random strategy, plus an oracle upper-bound. These are sensible reference points, but one might wonder how StageRoute fares against more sophisticated alternatives. For example, adapting a bandits with knapsacks approach or a standard multi-armed bandit algorithm with budget constraints could be attempted as a baseline. The absence of comparisons to any prior bandit or online learning algorithms makes it harder to pinpoint how much improvement comes from each novel aspect of StageRoute.\n\nAssumption of Immediate Reward Feedback: The framework assumes that after each query is routed to a model, the system obtains a reward/quality score for that model’s answer, which is used to update the UCB values. In the experiments, this is achieved by using a benchmark dataset where each query has pre-assessed scores for each model’s response. However, in a real deployment of LLMs serving live user queries, obtaining a numerical reward signal for each response can be challenging – user feedback may be implicit, delayed, or noisy. The paper does not discuss how robust the algorithm is to noisy feedback or how it might work with proxy metrics (like user click-through or satisfaction ratings). This is a potential gap between the experimental setup and real-world application. If the reward signal is sparse or delayed, the effectiveness of the UCB-based exploration could be affected. Clarifying this assumption and its impact would be helpful, as the need for a reliable performance metric per query could limit applicability in practice.\n\nScope of Evaluation and Real-World Factors: It would strengthen the paper to see evaluation under a wider range of conditions. For instance, how would StageRoute perform with a larger pool of models or more frequent model introductions? Similarly, the study focuses on cost and throughput constraints, but other practical considerations like variable latency or model reliability are not deeply explored. The concurrency cap $M_{\\max}$ is fixed, and it’s not fully clear how sensitive the system is to this parameter beyond what's in the theoretical bound."}, "questions": {"value": "Choosing the Update Interval: What guidance can the authors provide on setting the stage update frequency (interval length)? There is an inherent trade-off: updating deployments more frequently allows faster adaptation to new models but also incurs more frequent deployment costs and reduces per-model learning time, whereas longer stages mean more stability but risk sticking with suboptimal models. It would be useful to discuss how an operator might choose the update schedule in practice, and whether StageRoute’s performance degrades if this parameter is not well-tuned.\n\nIncorporating Contextual Information: The paper notes that the routing step can incorporate contextual estimators when features are available. Did the authors explore any form of contextual routing in their experiments, or could they provide examples of what contextual features might be useful? For instance, one could imagine features derived from the query to predict which model is likely to perform best. If context was not used in the current work, do the authors anticipate that adding context would further improve performance, and would any changes be needed in the theoretical analysis to accommodate contextual bandit estimators?\n\nDeployment Cost Parameter: The formulation includes a cost for deploying a model, presumably to discourage constantly swapping models in and out. However, the paper does not detail the values or impact of these deployment costs. Clarification on how deployment costs were set and whether they had any noticeable effect on the decisions would be helpful.\n\nAdaptation to New Models: How quickly and reliably does StageRoute adapt to a newly arrived model that has superior performance? Based on the description, the algorithm will explore new models via UCB if they appear promising, but the stage commitment means a wrong decision could delay using a good model for an entire stage. The heatmap comparison suggests StageRoute’s selection probabilities eventually align with the oracle’s optimal set. It would be insightful if the authors could elaborate on this: for example, do they observe that StageRoute sometimes sticks with an incumbent model for a stage or two before “realizing” a newcomer is better, or does the algorithm typically catch on almost immediately at the next update? Understanding this dynamic would shed light on the practical behavior of the system in a continuously evolving model pool."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BaqW7lJgWS", "forum": "JJ3cwtUxzo", "replyto": "JJ3cwtUxzo", "signatures": ["ICLR.cc/2026/Conference/Submission1169/Reviewer_ANEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1169/Reviewer_ANEN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510239219, "cdate": 1761510239219, "tmdate": 1762915696206, "mdate": 1762915696206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce *StageRoute* an algorithm for LLM routing that accounts for a continually updating family of models (reflecting how LLM routing must be done in practice). The authors provide theoretical guarantees for their method and an experimental analysis performed on Routerbench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Adapting LLM routing to an ever-changing landscape of models is a significant contribution.\n* StageRoute includes a minimax optimality guarantee that also provides practical guidance on the selection of $K$ (the number of model deployment stages)."}, "weaknesses": {"value": "* The theoretical guarantees seem to require that $r_t$ and $c_t$ are drawn from model-dependendent but *query* independent distributions. It is unclear to me that this assumption makes sense in the context of studying *per-query* routing; in fact if this does hold why do any per-query routing at all?\n* The empirical base lines comparisons seem lacking, the authors only compare to a greedy and random baseline, but no prior routing methods."}, "questions": {"value": "* Can the authors add more extensive empirical comparisons to prior work on routing?\n* Additionally, the authors might consider adding a study on the choice of $K$, since this seems to be the main guidance the theory provides."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wTv6F6OWIe", "forum": "JJ3cwtUxzo", "replyto": "JJ3cwtUxzo", "signatures": ["ICLR.cc/2026/Conference/Submission1169/Reviewer_G3fm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1169/Reviewer_G3fm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594024348, "cdate": 1761594024348, "tmdate": 1762915696037, "mdate": 1762915696037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of managing a streaming inventory of LLMs under concurrency and per-query cost constraints. The authors present StageRoute, a hierarchical online decision-making framework for adaptive, cost-aware, and scalable deployment and routing of large language models at scale. The algorithm achieves a regret bound of $\\tilde{O}(T^{2/3})$ and a matching lower bound, establishing near-optimality. Empirically, StageRoute tracks a strong oracle under tight budget constraints across diverse workloads."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a timely and practical challenge in LLM serving systems, where cost and scalability are key bottlenecks.\n2. The theoretical analysis is rigorous and clarifies the trade-off between adaptivity and learnability.\n3. The presentation and organization have improved since the NeurIPS version, with better discussion of parameter dependencies and clearer empirical exposition."}, "weaknesses": {"value": "*Disclosure:* I also reviewed this paper in its earlier NeurIPS submission. Compared with that version, I find that the current paper has made several substantial improvements:\n\n1. The authors now clearly explain why the dependence on $K$ becomes invalid when $K \\ge O(T^{1/3})$, addressing my previous concern both theoretically (line 317) and empirically (line 466).\n2. While the algorithmic design still combines several existing ideas, the paper now provides a more thoughtful discussion of how these perspectives fit together, making the unification more convincing.\n3. Although the experimental setup still involves only two relatively simple baselines, the added analysis clarifies why StageRoute can approach the optimal solution, which mitigates the earlier concern.\n\n--------------------------previous review------------------------------------------\n1. **Regret Upper Bound Interpretation**: The regret bound depends on K\\sqrt{K}, where KK is the number of deployment stages. While this appears reasonable under a fixed-stage model, it becomes problematic in fully adaptive environments where $K=T$. In that case, the regret becomes $O(T)$, which suggests linear regret—a degenerate and counterintuitive result. One would expect more adaptivity (i.e., frequent updates) to improve performance, not worsen it. Clarifying this behavior or bounding regret in fully adaptive settings would strengthen the theoretical contribution.\n2. **Algorithmic Novelty**: Although the problem is framed as hierarchical, the optimization subproblems in both stages reduce to similar formulations (e.g., Eq. (7)), and the algorithmic components largely resemble standard knapsack or budgeted bandit methods (e.g., [4]). It would be beneficial for the authors to clarify what aspects are truly novel beyond the hierarchical decomposition, such as any new learning techniques, budget handling strategies, or theoretical proof techniques introduced specifically for this setting.\n3. **Limited Empirical Comparisons**: The experimental section only compares StageRoute against two simple baselines. This is a missed opportunity, given the availability of related work on budgeted bandits (e.g., [4, 8, 19, 22]) and LLM routing (e.g., [13, 25, 27, 36]). Including stronger baselines from these domains would provide a more convincing evaluation of StageRoute’s practical advantages and generality."}, "questions": {"value": "I do not have major concerns for this version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LtVdBU3kfM", "forum": "JJ3cwtUxzo", "replyto": "JJ3cwtUxzo", "signatures": ["ICLR.cc/2026/Conference/Submission1169/Reviewer_bJS8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1169/Reviewer_bJS8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984057753, "cdate": 1761984057753, "tmdate": 1762915695803, "mdate": 1762915695803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper look at a new-ish problem - which models should be deployed given a tight budget. They introduce a new method called StageRoute, which allows incoming query to be routed to an ever-changing set of models.  The authors prove some regret bounds, establish near-optimalty and also provide experiments."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper looks at a very practical problem, how does one choose which models to deploy in a tight setup? This is very useful for practical settings\n2. I carefully checked several theorems and lemmas (not all), they look correct. \n3. The experiments are quite comprehensive. In figure 3, it is quite clear that cumulative regret is fairly low for StageRoute. The sensitivity analysis is quite nice too."}, "weaknesses": {"value": "1. The paper does not do a great job of connecting with existing explore-exploit literature. Please expand upon the section why existing analyses does not apply. This bleeds into the experiments. Why compare only with greedy and random? This to me is the biggest drawback of this paper, there is a ton of literature in the MAB space that could have been used to create stronger baselines\n2. In eq(3), it is important for the authors to describe the details of the problem. Is it a MIP? IP? LP? They briefly mention this in the experiments with MIP + Gurobi, a deeper discussion is warranted.\n3. The main body of the paper does not really talk about the cost of running optinmization. How does this optimization scale? Can you reuse solutions from previous runs? How often is the problem solved? Not everyone has access to Gurobi. This is quite important."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MIk6y2pGRI", "forum": "JJ3cwtUxzo", "replyto": "JJ3cwtUxzo", "signatures": ["ICLR.cc/2026/Conference/Submission1169/Reviewer_DHJM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1169/Reviewer_DHJM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1169/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762934762526, "cdate": 1762934762526, "tmdate": 1762934762526, "mdate": 1762934762526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}