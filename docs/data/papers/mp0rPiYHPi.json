{"id": "mp0rPiYHPi", "number": 14012, "cdate": 1758226860116, "mdate": 1759897396300, "content": {"title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "abstract": "We present MetaTT, a Tensor Train (TT) adapter framework for fine-tuning of\npre-trained transformers. MetaTT enables flexible and parameter-efficient model\nadaptation by using a single shared TT to factorize transformer sub-modules. This\nfactorization indexes key structural dimensions, including layer and matrix type,\nand can optionally incorporate heads and tasks. This design allows MetaTT’s pa-\nrameter count to scale with the sum, rather than the product, of the modes, resulting\nin a substantially more compact adapter. Our benchmarks compare MetaTT with\nLoRA along with recent state-of-the-art matrix and tensor decomposition based\nfine-tuning methods. We observe that when tested on single-task standard language\nmodeling benchmarks, MetaTT achieves competitive parameter efficiency to accu-\nracy tradeoff. We further demonstrate that MetaTT performs competitively when\ncompared to state-of-the-art methods on multi-task learning. Finally, we leverage\nthe TT-ansatz to design a rank-adaptive optimizer inspired by the DMRG method\nfrom many-body physics. Our results demonstrate that integrating this approach\nwith AdamW enhances optimization performance for a specified target rank.", "tldr": "Global adapters for fine-tuning based on tensor train decompositions", "keywords": ["Fine-tuning", "low-rank approximation", "tensor networks", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a06b754b57f8cbbef1cd7b1803b3e8b6c06a8f77.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MetaTT, a Tensor Train (TT)-based adapter framework for parameter-efficient fine-tuning of pre-trained transformers. MetaTT leverages a shared TT decomposition to factorize various transformer sub-modules across structural dimensions such as layer index, matrix type, and optionally heads and tasks. Additionally, they propose a rank-adaptive optimizer inspired by the DMRG method from quantum physics, demonstrating improved optimization when integrated with AdamW for fixed target ranks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of this paper is clear and easy to follow. The topic of using a tensor-train adapter is quite novel, and the proposed rank-adaptive optimizer appears to be an important and reasonable enhancement that can be effectively integrated with existing tensorized methods."}, "weaknesses": {"value": "- My main concern with this paper is the soundness of the proposed method. I understand that global compression can reduce the number of trainable parameters. However, I’m not convinced why this should lead to better performance. Intuitively, reweighting the entire transformer block should perform worse than reweighting individual linear layers within the block, since adjusting single layers allows more flexibility—especially when combined with in-block non-linearity. I didn’t see any discussion or justification from the authors on why the proposed method works in this regard.\n- The experimental results further reinforce my concern. For example, in Table 1, the proposed method shows improvements over baselines like LoRA, but the gain is usually less than 1%, which is not substantial enough to confidently claim a real improvement. A similar trend is observed across other tables.\n- I’m also wondering whether the DMRG optimizer is included in Tables 1–3. If not, why was it excluded? Based on the results and discussion, it seems that the DMRG-inspired method itself contributes significantly to the performance gains. In contrast, the improvements from the tensorized adapter alone appear to be limited.\n- BERT-based models feel somewhat out-of-date to me. I highly suggest that the authors focus more on the tasks in Table 1 for the ablation studies instead."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ed0beSJebb", "forum": "mp0rPiYHPi", "replyto": "mp0rPiYHPi", "signatures": ["ICLR.cc/2026/Conference/Submission14012/Reviewer_6E7j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14012/Reviewer_6E7j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545499895, "cdate": 1761545499895, "tmdate": 1762924507231, "mdate": 1762924507231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MetaTT, a parameter-efficient fine-tuning framework based on tensor train decomposition. MetaTT parameterizes all transformer sub-modules using a single shared TT, achieving global compression. The paper proposes two variants, MetaTT-4D and MetaTT-5D, for single-task fine-tuning and extends the architecture to joint multi-task learning with an additional tensor core. Furthermore, the paper leverages a rank-adaptive optimizer inspired by the DMRG method from quantum many-body physics to enhance optimization. Experimental results compare MetaTT against several PEFT baselines, including LoRA, VeRA, and LoTR, for both single-task and multi-task settings on standard benchmarks like GLUE and commonsense reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed use of a single shared TT for compressing all transformer layers and sub-modules is novel and shows promise in reducing the parameter count while maintaining competitive performance.\n2. The rank-adaptive training inspired by DMRG is an interesting integration of techniques from quantum physics into machine learning, showcasing interdisciplinarity and potential for further exploration.\n3. The paper compares MetaTT with several state-of-the-art PEFT methods, including LoRA and LoTR, across multiple tasks, and reports detailed results on parameter efficiency and accuracy."}, "weaknesses": {"value": "1. Despite the novelty of the approach, the reported performance improvements are marginal or absent in most cases compared to simpler baselines like LoRA, especially given the significant computational complexity added by TT decomposition and rank-adaptive training.\n2. While the DMRG-inspired optimizer is presented as a key contribution, its practical benefits over standard optimizers like AdamW are not convincingly demonstrated. The rank-adaptive approach introduces additional training complexity without a clear payoff in terms of accuracy or efficiency.\n3. The paper overlooks some recent works on tensor-based adapters and PEFT methods, particularly those focusing on computational trade-offs and scalability, such as AdaLoRA and other adaptive rank methods.\n4. While the authors provide detailed pseudocode, the implementation lacks clarity in critical areas like initialization strategies and hyperparameter choices, which are shown to influence MetaTT's performance heavily. This raises concerns about the reproducibility of results."}, "questions": {"value": "1. The results show that MetaTT performs similarly to LoRA and LoTR in many benchmarks, with only marginal improvements in some cases. Can the authors clarify the practical advantages of MetaTT over these simpler methods, particularly in real-world scenarios? How do the authors justify the significant computational overhead introduced by TT decomposition and rank-adaptive training in light of these modest gains?\n2. While the paper provides pseudocode and hyperparameter grids, the results seem highly dependent on initialization strategies and specific rank settings. Could the authors share more details about the exact initialization methods, hyperparameter tuning process, and any challenges encountered during experimentation? Are there plans to release the full implementation and training pipelines?\n3. The paper notes that MetaTT-5D is more sensitive to initialization and training instability than MetaTT-4D. Can the authors elaborate on why this is the case? Are there specific guidelines or heuristics for initialization and hyperparameter selection that can make MetaTT-5D more robust? How does this sensitivity impact the usability of MetaTT in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uqgKKF1yL8", "forum": "mp0rPiYHPi", "replyto": "mp0rPiYHPi", "signatures": ["ICLR.cc/2026/Conference/Submission14012/Reviewer_73xA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14012/Reviewer_73xA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654441575, "cdate": 1761654441575, "tmdate": 1762924506561, "mdate": 1762924506561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission introduces a Tensor Train adapter model that parameterizes LLM weight updates as a fourth or fifth order tensor. Moreover, they introduce an optimization scheme that starts with a larger rank and iteratively fits an adapter and applies (approximated) truncated SVD to reduce the rank."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written, easy to follow, the contribution and references to prior work are clear, the approach is sound and experiments not only include the standard benchmarks but illustrate particularities of their proposed methods.\n\nBoth TT parameterizations (e.g. LoTR) and fifth order tensor adapter models that use layers input output dimensions and heads  (e.g. LoRTA) have been proposed, but not their conjunction. Secondly, treating tasks as an additional dimension is, to the best of my knowledge, novel. \n\nThe iterative DMRG inspired training algorithm is novel and relevant contribution that motivates further research into designing optimization algorithms for low rank tensor adapters that dynamically adjust rank throughout optimization. Although truncated SVD has been proposed to initialise LoRA adapters, and some optimizers have been proposed specifically for low rank matrix adapters (e.g. GaLoRE), the low rank tensor literature primarily relies on standard (adamw) optimization tools and, more importantly, the rank is usually treated as a fixed hyper-parameter. This is relevant because when the rank is low - regardless of the adapter model - optimization dynamics usually becomes challenging and starting with a larger rank can mitigate this."}, "weaknesses": {"value": "I think that the rank adaptive optimization scheme is a strong contribution. The experiments that showcase its benefits are centered in the standard NLU setting with roberta, but I think it would be useful to extend the empirical analysis to (at least one, ideally all) of other benchmarks/tasks/models in order to further substantiate the empirical gains from this scheme in settings that are regarded as more challenging."}, "questions": {"value": "Why do most experiments show only meta-TT 4D (except for roberta in nlu tasks)?\n\nCan you comment on initialization and sensitivity - perhaps provide an ablation ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8jvse2yWqT", "forum": "mp0rPiYHPi", "replyto": "mp0rPiYHPi", "signatures": ["ICLR.cc/2026/Conference/Submission14012/Reviewer_Kszv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14012/Reviewer_Kszv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923462492, "cdate": 1761923462492, "tmdate": 1762924505699, "mdate": 1762924505699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use tensor-train factorisation to compress the weight matrices of PEFT models.  They show how this elegant framework can be used to share implicit structure in the weights across parameter types, layers, attention heads, and multiple tasks.  Strong reductions in the number of parameters are achieved while getting similar or better accuracies across a number of tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is an elegant unified approach to compressing PEFT matrices to reduce parameter counts.  It also elegantly extends to multi-task PEFT, allowing shared structure across tasks.\n\nEmpirical evaluations are done on a good variety of tasks, using three versions of the model, each an extension of the previous model.  Results are generally good or comparable to previous methods, but with greatly reduced parameter counts."}, "weaknesses": {"value": "The novelty is not high.  There has been a lot of work on PEFT already, and this work does not add much conceptual or theoretical novelty.  The contribution is in identifying a general-purpose mathematical framework which addresses PEFT in a consistent way, rather than a collection of ad-hoc methods.\n\nThe empirical results do not demonstrate any breakthroughs with respect to previous work."}, "questions": {"value": "There is earlier work than the papers you cite which seems directly relevant to your approach of factorising parameter matrices and multi-task PEFT, respectively:\n Mahabadi, Henderson, and Ruder. Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.  NeurIPS 2021.\n Mahabadi, Ruder, Dehghani, and Henderson.  Parameter-efficient Multi-task Fintuning for Transformers via Shared Hypernetworks.  ACL 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKIVXgcErQ", "forum": "mp0rPiYHPi", "replyto": "mp0rPiYHPi", "signatures": ["ICLR.cc/2026/Conference/Submission14012/Reviewer_ufrS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14012/Reviewer_ufrS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762430099913, "cdate": 1762430099913, "tmdate": 1762924504574, "mdate": 1762924504574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}