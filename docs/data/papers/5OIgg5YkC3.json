{"id": "5OIgg5YkC3", "number": 14635, "cdate": 1758240621369, "mdate": 1763603474331, "content": {"title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models", "abstract": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on large paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary $\\textit{unpaired}$ multimodal data to directly enhance representation learning in a $\\textit{target}$ modality? We introduce $\\textbf{UML}$: $\\textbf{U}$npaired $\\textbf{M}$ultimodal $\\textbf{L}$earner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the world than unimodal training. Empirically, we show that incorporating unpaired data from auxiliary modalities—such as text, audio, or images—consistently improves downstream performance across diverse unimodal targets such as image and audio.", "tldr": "We show that incorporating auxiliary unpaired multimodal data can significantly improve performance on an individual modality.", "keywords": ["Unpaired Multimodal Representation Learning", "Cross-modal Learning; Multimodal Learning from Unpaired Data"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/266ed5e5054a845cc4879ac9bc6cbf7cd8ea32ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose the Unpaired Multimodal Learner (UML), which leverages auxiliary unpaired multimodal data to enhance representation learning in a target modality. Specifically, the authors present two frameworks, one for self-supervised learning and one for supervised learning, both of which utilize multiple encoders and a shared network. Theoretical and empirical results indicate that adding unpaired data of modality Y can lead to better reconstruction in modality X compared to adding additional data from X itself."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic addressed in the paper is interesting.\n2. The paper is clearly written and easy to follow.\n3. The theoretical analysis is sound, and the proposed method is simple yet effective."}, "weaknesses": {"value": "1. The primary concern lies in the motivation: unpaired multimodal data is relatively uncommon, whereas missing-modality data is more frequently encountered. If the data is only unpaired, why not use existing models (e.g., CLIP) to align them and create paired data? Therefore, a more practical setting might be using images from one dataset alongside text from another dataset.\n2. It would strengthen the paper to include experiments using various encoders and various shared networks to validate the general effectiveness of the method."}, "questions": {"value": "1. The experiments do not include comparisons with related works. Do the authors think it is necessary to compare against other approaches mentioned in the Further Related Works section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IehNigMq5n", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Reviewer_mJFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Reviewer_mJFa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964085774, "cdate": 1760964085774, "tmdate": 1762925011769, "mdate": 1762925011769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank all of the reviewers for their insightful and helpful feedback. We are glad that they found:\n- The paper is well-written and clear [8Sv3, EcBF, mJFa]\n- The experiments are extensive [EcBF]\n- The theoretical results are strong and sound [EcBF, mJFa]\n\nBelow we summarize key revisions and clarify our paper’s contribution (and answer individual questions in reviewer-specific responses). \n\n**1. UPDATES TO THE PAPER**\n\nIn response to comments from the reviewers, we have made the following changes to our paper. **Updates are purple in our revised PDF**.\n\n- Revised Sections 1, 2, and 5 to clarify the extent and scope of our claims, and discuss limitations of multimodal learning found in the literature.\n- Added further theoretical results in Section C.4 to strengthen the connection between weight-sharing in UML and our Fisher information argument in Section 3.1.\n- Revised the presentation of our experiments in Section 4.1 and our theoretical results in Section C.3 to improve clarity.\n- Revised our limitations (Section 5) to comment further on natural extensions of our work.\n\n**2. CONTRIBUTION WITH RESPECT TO PRIOR WORK**\n\nExisting work typically falls into one of the following categories:\n\n- **Paired multimodal learning improving $X$ using paired $(X,Y)$**: Most multimodal methods rely on paired $(x,y)$ supervision and cross-modal losses where improvements on $X$ are driven by explicit pairing and alignment objectives [1,2].\n- **Unpaired data only in combination with paired or pre-aligned signals**: “Unpaired” methods usually still use some paired data [3], pseudo-alignments [4,5,6], or pre-aligned feature spaces (e.g., CLIP) to transfer signal across modalities [8].\n- **Unpaired multimodal data confined to intra-domain or heavily engineered alignment**: Existing unpaired approaches often stay within a single modality family (e.g., vision) [7] or require explicit pseudo-labeling / cross-domain matching that hard-codes inductive biases into the representation [4,5,6].\n\n\nIn contrast, our work shows that one can train on fully unpaired audio, vision, and text datasets using separate encoders with a single shared head. Our results show that this:\n-  Improves unimodal performance on the tested benchmarks,\n- Scales monotonically as more modalities are added,\n- Learns multimodal structure (neurons, prototype alignment) without ever seeing paired data, and\n- Is backed by a theoretical analysis motivating when/why this works.\n\n\n[1] Zhai, Xiaohua, et al. \"Lit: Zero-shot transfer with locked-image text tuning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Cijo Jose, et al. “A unified framework for image-and pixel-level vision-language alignment.” arXiv preprint arXiv:2412.16334 (2024).\n\n[3] Geng, Xinyang, et al. \"Multimodal masked autoencoders learn transferable representations.\" arXiv preprint arXiv:2205.14204 (2022).\n\n[4] Xi, Johnny, et al. \"Propensity score alignment of unpaired multimodal data.\" Advances in Neural Information Processing Systems 37 (2024): 141103-141128.\n\n[5] Demetci, Pinar, et al. \"SCOT: single-cell multi-omics alignment with optimal transport.\" Journal of computational biology 29.1 (2022): 3-18.\n\n[6] Ryu, Jayoung, et al. \"Cross-modality matching and prediction of perturbation responses with labeled Gromov-Wasserstein optimal transport.\" arXiv preprint arXiv:2405.00838 (2024).\n\n[7] Girdhar, Rohit, et al. \"Omnimae: Single model masked pretraining on images and videos.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[8] Gao, Peng, et al. \"Clip-adapter: Better vision-language models with feature adapters.\" International Journal of Computer Vision 132.2 (2024): 581-595."}}, "id": "FrebpG3XGF", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763494364249, "cdate": 1763494364249, "tmdate": 1763505946909, "mdate": 1763505946909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Unpaired Multimodal Learner (UML), which leverages auxiliary unpaired multimodal data to enhance representation learning in a target modality. Specifically, the authors present two frameworks, one for self-supervised learning and one for supervised learning, both of which utilize multiple encoders and a shared network. Theoretical and empirical results indicate that adding unpaired data of modality Y can lead to better reconstruction in modality X compared to adding additional data from X itself."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic addressed in the paper is interesting.\n2. The paper is clearly written and easy to follow.\n3. The theoretical analysis is sound, and the proposed method is simple yet effective."}, "weaknesses": {"value": "1. The primary concern lies in the motivation: unpaired multimodal data is relatively uncommon, whereas missing-modality data is more frequently encountered. If the data is only unpaired, why not use existing models (e.g., CLIP) to align them and create paired data? Therefore, a more practical setting might be using images from one dataset alongside text from another dataset.\n2. It would strengthen the paper to include experiments using various encoders and various shared networks to validate the general effectiveness of the method."}, "questions": {"value": "1. The experiments do not include comparisons with related works. Do the authors think it is necessary to compare against other approaches mentioned in the Further Related Works section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IehNigMq5n", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Reviewer_mJFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Reviewer_mJFa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760964085774, "cdate": 1760964085774, "tmdate": 1763606935179, "mdate": 1763606935179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method, **UML** (**U**npaired **M**ultimodal **L**earner), to improve the performance of unimodal models by leveraging abundant, unpaired data from other modalities.\nThe core idea is that even without direct pairings (like an image and its specific caption), data from an auxiliary modality (e.g., text) can provide complementary information to enhance a model focused on a target modality (e.g., images).\nThe **UML** method works by having a single model with shared parameters (weight sharing) alternately process inputs from the different modalities. This design allows the model to capture shared underlying concepts and structures from both datasets, even though they are not explicitly linked.\nMoreover, the paper theoretically demonstrates that this approach strictly increases the Fisher information under linear assumption of generating data, resulting in more informative and robust representations for the target modality than training on that modality alone."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents strong theoretical approaches demonstrating how unpaired multimodal datasets, or datasets with missing modalities, can be effectively synergized with existing datasets. It shows that these datasets can be linearly combined, leading to an increase in Fisher Information.\n- The paper provides extensive experiments on various benchmark datasets that support its theoretical framework, validating the proposed methods and concepts.\n- The paper is well written, particularly in the Introduction and the section explaining the main concept."}, "weaknesses": {"value": "**Major**\n\nThe paper’s effort to ground its empirical findings in theoretical analysis is commendable; however, the theoretical assumptions appear too restrictive to be fully explanatory. The authors provide valuable intuition for UML by presenting theorems (Sec. 3.1) derived under a linear data-generating process.\nNonetheless, this represents a significant simplification of the highly non-linear dynamics of the large Transformer-based models (e.g., DINOv2, OpenLLaMA) used in the experiments. While the idea that shared weights act as a practical analogue of *Fisher information linear combination* is intriguing, it remains more of an intuitive analogy than a formal justification.\nIn my view, this creates a potential gap between theory and practice. The empirical results are undeniably strong and well-presented, yet they seem to be motivated by the theoretical framework rather than explained by it.\n\n**Minor**\n\n- Although the theoretical motivation is clearly written, the derivations of the theorems are somewhat verbose and occasionally redundant, which reduces readability. A more concise and streamlined presentation could enhance the clarity of the theoretical section.\n- While the experimental validation is extensive, its presentation in Section 4 and the Appendix seem overly dense. For example, much of the dataset and model information is relegated to the Appendix and only briefly mentioned in Section 4.1, where it hinders readability. Additionally, the color scheme used in Table 1 and Table 2 (blue and pink) is identical, even though the categories differ (Table 1: datasets; Table 2: settings).\nAs a suggestion (not a requirement), reorganizing the experimental sections might improve clarity. (e.g., focusing on the supervised setting in Section 4.1 and moving the self-supervised setting to Section 4.2, and removing the color scheme from Tables 1 and 2 to avoid confusion)"}, "questions": {"value": "- What is the main difference from prior works, and what constitutes the key novelty of this paper? In my view, as the authors mention in Section 3 and Appendix A.1, UML might seem slightly incremental, as it combines concepts from previous studies, such as shared model parameters (e.g., [1]), and the use of unpaired datasets (e.g.,[2]).\n- What happens if the text or data (modality $\\text{Y}$) are randomly generated? For instance, what if the text description is entirely unrelated to the image (e.g., the image depicts a dog, but the text describes playing sports)? Would such mismatched modalities disrupt the increase in Fisher Information?\n- Yet the authors note in the limitations section that most experiments are conducted on classification tasks, I still raise a concern regarding the applicability of the proposed method to other tasks, such as image–text retrieval.\n\n\n[1]  Chada, et al. \"Momo: A shared encoder model for text, image and multi-modal representations.\" arXiv preprint 2023\\\n[2] Lee, Jae-Jun, and Sung Whan Yoon. \"Can One Modality Model Synergize Training of Other Modality Models?.\" ICLR 2025\n\n=======================================================\n\n**Note**: I acknowledge that I may have partially misunderstood certain aspects of the paper. Therefore, I am willing to raise my rating score if these questions and concerns are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Af2KkY0ZFO", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Reviewer_EcBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Reviewer_EcBF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549913601, "cdate": 1761549913601, "tmdate": 1762925011420, "mdate": 1762925011420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method, **UML** (**U**npaired **M**ultimodal **L**earner), to improve the performance of unimodal models by leveraging abundant, unpaired data from other modalities.\nThe core idea is that even without direct pairings (like an image and its specific caption), data from an auxiliary modality (e.g., text) can provide complementary information to enhance a model focused on a target modality (e.g., images).\nThe **UML** method works by having a single model with shared parameters (weight sharing) alternately process inputs from the different modalities. This design allows the model to capture shared underlying concepts and structures from both datasets, even though they are not explicitly linked.\nMoreover, the paper theoretically demonstrates that this approach strictly increases the Fisher information under linear assumption of generating data, resulting in more informative and robust representations for the target modality than training on that modality alone."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents strong theoretical approaches demonstrating how unpaired multimodal datasets, or datasets with missing modalities, can be effectively synergized with existing datasets. It shows that these datasets can be linearly combined, leading to an increase in Fisher Information.\n- The paper provides extensive experiments on various benchmark datasets that support its theoretical framework, validating the proposed methods and concepts.\n- The paper is well written, particularly in the Introduction and the section explaining the main concept."}, "weaknesses": {"value": "**Major**\n\nThe paper’s effort to ground its empirical findings in theoretical analysis is commendable; however, the theoretical assumptions appear too restrictive to be fully explanatory. The authors provide valuable intuition for UML by presenting theorems (Sec. 3.1) derived under a linear data-generating process.\nNonetheless, this represents a significant simplification of the highly non-linear dynamics of the large Transformer-based models (e.g., DINOv2, OpenLLaMA) used in the experiments. While the idea that shared weights act as a practical analogue of *Fisher information linear combination* is intriguing, it remains more of an intuitive analogy than a formal justification.\nIn my view, this creates a potential gap between theory and practice. The empirical results are undeniably strong and well-presented, yet they seem to be motivated by the theoretical framework rather than explained by it.\n\n**Minor**\n\n- Although the theoretical motivation is clearly written, the derivations of the theorems are somewhat verbose and occasionally redundant, which reduces readability. A more concise and streamlined presentation could enhance the clarity of the theoretical section.\n- While the experimental validation is extensive, its presentation in Section 4 and the Appendix seem overly dense. For example, much of the dataset and model information is relegated to the Appendix and only briefly mentioned in Section 4.1, where it hinders readability. Additionally, the color scheme used in Table 1 and Table 2 (blue and pink) is identical, even though the categories differ (Table 1: datasets; Table 2: settings).\nAs a suggestion (not a requirement), reorganizing the experimental sections might improve clarity. (e.g., focusing on the supervised setting in Section 4.1 and moving the self-supervised setting to Section 4.2, and removing the color scheme from Tables 1 and 2 to avoid confusion)"}, "questions": {"value": "- What is the main difference from prior works, and what constitutes the key novelty of this paper? In my view, as the authors mention in Section 3 and Appendix A.1, UML might seem slightly incremental, as it combines concepts from previous studies, such as shared model parameters (e.g., [1]), and the use of unpaired datasets (e.g.,[2]).\n- What happens if the text or data (modality $\\text{Y}$) are randomly generated? For instance, what if the text description is entirely unrelated to the image (e.g., the image depicts a dog, but the text describes playing sports)? Would such mismatched modalities disrupt the increase in Fisher Information?\n- Yet the authors note in the limitations section that most experiments are conducted on classification tasks, I still raise a concern regarding the applicability of the proposed method to other tasks, such as image–text retrieval.\n\n\n[1]  Chada, et al. \"Momo: A shared encoder model for text, image and multi-modal representations.\" arXiv preprint 2023\\\n[2] Lee, Jae-Jun, and Sung Whan Yoon. \"Can One Modality Model Synergize Training of Other Modality Models?.\" ICLR 2025\n\n=======================================================\n\n**Note**: I acknowledge that I may have partially misunderstood certain aspects of the paper. Therefore, I am willing to raise my rating score if these questions and concerns are adequately addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Af2KkY0ZFO", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Reviewer_EcBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Reviewer_EcBF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549913601, "cdate": 1761549913601, "tmdate": 1763611510670, "mdate": 1763611510670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to augment unimodal models with unpaired training data from other modalities. Their approach shares weights across all modalities while optimizing for the downstream task on a joint, unpaired, multimodal training dataset. They evaluate their approach on several supervised, self-supervised, and transfer learning settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well written - ideas and motivations are expressed clearly and in easy-to-understand terms; experimental results are stated and discussed in a well-organized manner.\n\n2. The experiment in Section 4.4 on marginal rate-of-substitution between modalities is quite interesting and the results are insightful.\n\n3. The authors empirically show that in the scenarios considered, it is indeed possible to achieve some practical performance improvement through joint/pre-training with data from modalities other than the target."}, "weaknesses": {"value": "1. The idea that data from multiple modalities can be used for unified pretraining without any special consideration for the nature of the modalities being integrated is a fundamentally flawed premise. It is well known in literature that modalities can often have conflicting information, and even multimodal models, which often operate on explicitly paired data and are trained with the objective of aligning the modalities, struggle with this integration. There are plenty of works dedicated to addressing specifically this problem [a, b]. At the low level, for instance, modalities can often have different convergence rates [c] and provide conflicting gradients to the model [d], none of which can be thought of as being helpful from a training perspective without special treatment [d]. In fact, [c] specifically established the general impossibility of what the authors propose in this work.\n\n2. Although the experiments show performance some improvements, the generality with which the paper is presented is misleading. Attempting to perform joint multimodal training using the proposed approach in the settings and datasets used in [a, b, c, d], is likely to falsify the findings reported, since special adjustments to cope with the challenges of integrating multiple modalities is needed to deal with those settings.\n\n3. No ablation or analytical studies have been reported for the proposed approach, which makes it difficult to evaluate the contribution of the various design choices, for instance, the proposed UML objectives, the contribution of unpaired samples from a different modality, convergence in the uni-modal vs multi-modal setting, etc.\n\n4. Finally, neither the idea of sharing model weights, nor doing pretraining with data from multiple modalities can be regarded as novel, since they have been around in the multimodal learning community for a long time [e, f, g]. In fact, this is also the problem that works on domain generalization attempt to solve, albeit framed in a different manner [h].\n\nMinor:\\\nLine 449: \"Further results on\" -> \"For further results on\"\n\nReferences:\\\n[a] Zhang et al., \"Robust Multimodal Large Language Models Against Modality Conflict\", ICML 2025.\\\n[b] Ma et al., \"Improving Multimodal Learning Balance and Sufficiency through Data Remixing\", ICML 2025.\\\n[c] Wang et al., \"What makes training multi-modal classification networks hard?\", CVPR 2020.\\\n[d] Javaloy et al., \"Mitigating Modality Collapse in Multimodal VAEs via Impartial Optimization\", ICML 2022.\\\n[e] Ngiam et al., \"Multimodal Deep Learning\", ICML 2011.\\\n[f] Hu et al., \"Towards Unsupervised Sketch-based Image Retrieval\", BMVC 2022.\\\n[g] Rastegar et al., \"MDL-CW: A Multimodal Deep Learning Framework with Cross Weights\", CVPR 2016.\\\n[h] Gulrajani et al., \"In Search of Lost Domain Generalization\", ICLR 2021."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ry1t6am6MR", "forum": "5OIgg5YkC3", "replyto": "5OIgg5YkC3", "signatures": ["ICLR.cc/2026/Conference/Submission14635/Reviewer_8Sv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14635/Reviewer_8Sv3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956965492, "cdate": 1761956965492, "tmdate": 1762925010904, "mdate": 1762925010904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}