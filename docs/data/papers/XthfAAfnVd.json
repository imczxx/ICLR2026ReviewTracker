{"id": "XthfAAfnVd", "number": 21993, "cdate": 1758324444412, "mdate": 1759896892019, "content": {"title": "BRAIN: Boltzmann Reinforcement For Analog Ising Networks", "abstract": "Analog Ising machines represent a transformative paradigm for combinatorial optimization, exploiting physical dynamics to achieve high speed, energy efficient operations over conventional digital electronics. However, existing optimization algorithms fail to harness these platforms' massive parallelism while tackling the inherent measurement noise for Analog Ising machines. We introduce BRAIN (Boltzmann Reinforcement for Analog Ising Networks), transforming the traditional sampling-based optimization to distribution learning framework. The Boltzmann distribution provides the fundamental link between statistical physics and Ising-type combinatorial optimization, establishing the theoretical framework that enables physical systems to solve NP-hard problems. Unlike Monte Carlo Markov Chain (MCMC) methods that sample states from Boltzmann distributions, BRAIN directly learns the Boltzmann distribution through variational reinforcement learning. This fundamental transformation makes the algorithm inherently resilient to the Gaussian measurement noise intrinsic to analog Ising systems. Our approach employs policy gradients to optimize a parametric state generator, naturally aggregating information across multiple noisy measurements without requiring precise energy differences. We benchmark BRAIN against MCMC methods across diverse combinatorial optimization problems, demonstrating three critical advantages. First, BRAIN generalizes across different interaction topologies, performing effectively on both Curie-Weiss and 2D nearest-neighbor Ising models. Second, it exhibits remarkable robustness under severe measurement noise up to 40\\%. Third, it scales efficiently to large systems of 65,536 (N) spins, scaling as $\\mathrm{O} (N^{1.55})$, with noisy energy evaluations. With realistic 3\\% Gaussian noise, BRAIN maintains 98\\% ground state fidelity while MCMC methods achieve only 51\\% fidelity with BRAIN arriving at the MCMC solution $192\\times$ faster. Beyond ground state optimization, BRAIN preserves the complete thermodynamic landscape, analyzing phase transitions and metastable states essential for robust large scale combinatorial optimization and complex many-body physics applications.", "tldr": "", "keywords": ["Reinforcement learning; Ising systems; MCMC"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42a5832529eb8b239f350247f33d82bdd63c6d68.pdf", "supplementary_material": "/attachment/ce23a72f5ae61d2ddf35d680751c7e04d6ffe0bf.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a method to approximate a Boltzmann distribution by minimizing the reverse KL divergence between a simple product distribution and the target distribution. The approach is evaluated on the Ising model, with the authors claiming superior performance over MCMC methods when noisy energy functions are observed."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a challenging problem in statistical physics and combinatorial optimization, which is relevant to both theoretical and applied research."}, "weaknesses": {"value": "- **Novelty:** The primary contribution appears to be the use of noisy observations of the energy value. However, the core methodology does not introduce substantial conceptual or technical advancements beyond existing literature.\n- **Model Expressivity:** The choice of a simple product distribution as the approximating model is concerning. Such distributions are inherently limited in expressivity and typically struggle to capture multimodal or complex target distributions [6]. This raises questions about the validity of results like those in Figure 1c, where the model seemingly approximates a bimodal distribution—a task that should theoretically exceed the capabilities of a product distribution.\n- **Literature Awareness:** The paper does not adequately engage with recent, relevant work in the field, such as:\n  - [6] Wu et al. (2019) and [7] Nicoli et al. (2020) who consider an autoregressive distributio on the Ising Model\n  - [1] Karalias and Loukas (2020), which explores neural frameworks for combinatorial optimization on graphs.\n  - [2] Sanokowski et al. (2024), which introduces a diffusion model framework for unsupervised neural combinatorial optimization.\n  - [3] Sanokowski et al. (2025), focusing on scaling discrete diffusion samplers and evaluates diffusion samplers in sampling settings\n  - [4] Holderriethet al. (2025) and [5] Ou et al. (2025), who propose discrete neural flow samplers with locally equivariant transformers and evaluate on the Ising model\n- **Technical Accuracy:** There are problematic statements in the paper, such as the claim in line 217 that a graph neural network serves as a model distribution. This is misleading, as GNNs are used as a parametrization of model distributions."}, "questions": {"value": "1. **Novelty:** How does this work differentiate itself from the approaches outlined in [1]–[5]? Could you clarify the specific technical or conceptual innovations introduced in this paper?\n2. **Model Capability:** How can a simple product distribution, which is theoretically limited to unimodal distributions, produce samples that approximate a bimodal distribution, as shown in Figure 1c? Are there additional components or assumptions not clearly stated in the paper?\n3. **Literature Engagement:** Why were the works [1]–[5] not discussed or compared against in the paper? How does your method relate to or improve upon these existing approaches?\n\n---\n\n[1] Karalias, Nikolaos, and Andreas Loukas. \"Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs.\" Advances in Neural Information Processing Systems 33 (2020): 6659-6672.\n\n[2] Sanokowski, Sebastian, Sepp Hochreiter, and Sebastian Lehner. \"A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization.\" International Conference on Machine Learning. PMLR, 2024.\n\n[3] Sanokowski, Sebastian, et al. \"Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics.\n\n[4] \"Holderrieth, Peter, Michael Samuel Albergo, and Tommi Jaakkola. \"LEAPS: A discrete neural sampler via locally equivariant networks.\" Forty-second International Conference on Machine Learning.\n\n[5] Ou, Zijing, Ruixiang Zhang, and Yingzhen Li. \"Discrete Neural Flow Samplers with Locally Equivariant Transformer.\" arXiv preprint arXiv:2505.17741 (2025).\n\n[6] Wu, Dian, Lei Wang, and Pan Zhang. \"Solving statistical mechanics using variational autoregressive networks.\" Physical review letters 122.8 (2019): 080602.\n\n[7] Nicoli, Kim A., et al. \"Asymptotically unbiased estimation of physical observables with neural samplers.\" Physical Review E 101.2 (2020): 023304."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MOhb4UUR6b", "forum": "XthfAAfnVd", "replyto": "XthfAAfnVd", "signatures": ["ICLR.cc/2026/Conference/Submission21993/Reviewer_swjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21993/Reviewer_swjF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760700518767, "cdate": 1760700518767, "tmdate": 1762942010599, "mdate": 1762942010599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This method employs the REINFORCE gradient estimator to learn an approximation of the Boltzmann distribution of Ising systems based on noisy energy measurement from an Analog Ising machines. The method uses the reverse KL divergence as an objective function and compares the sampling performance primarily against Metropolis Hastings MCMC that also uses the noisy energy measurements."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Clear problem motivation and good motivation for using analog hardware in these problems."}, "weaknesses": {"value": "- The paper does not sufficiently explain the role of the analog Ising machine clearly enough. Please explain what it is, what it does, and how it is used.\n\n- No comparison to any other ML/RL method: this paper only evaluates REINFORCE gradient estimator in conjunction with a variational distribution that is a Bernoulli product distribution. Please employ also state-of-the-art RL methods like PPO and please also consider more sophisticated parameterizations of the variational distribution like e.g. discrete diffusion models (Sanokowski et al. 2025). The chosen approach cannot be considered state-of-the-art and consequently also lacks novelty from an ML perspective.\n\n- Compare against methods that are not based on analog Ising machines (see Sanokowski et al. 2025 and references therein).\n\nSanokowski, S., Berghammer, W., Ennemoser, M., Wang, H. P., Hochreiter, S., & Lehner, S. (2025). Scalable discrete diffusion samplers: Combinatorial optimization and statistical physics. arXiv preprint arXiv:2502.08696."}, "questions": {"value": "- Why is MH chosen as the primary MCMC benchmark and not more sophisticated MCMC methods like Sequential MC or Parallel Tempering (which is only mentioned once in the appendix)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2SURVuIfA7", "forum": "XthfAAfnVd", "replyto": "XthfAAfnVd", "signatures": ["ICLR.cc/2026/Conference/Submission21993/Reviewer_ZEoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21993/Reviewer_ZEoy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302910540, "cdate": 1761302910540, "tmdate": 1762942010396, "mdate": 1762942010396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an optimization algorithm for analog Ising networks. Instead of sampling individual states and comparing them to determine the optimization direction, the proposed method directly learns to approximate the full Boltzmann distribution. Compared to MCMC, the experimental results demonstrate stronger noise resilience and better scalability to large systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is clearly structured.\n\n- The proposed method exhibits strong robustness to measurement noise."}, "weaknesses": {"value": "- The comparison is somewhat limited. Only MCMC is used as a baseline, while more advanced techniques (e.g., neural Boltzmann generators) are not considered.\n\n- The experimental section lacks ablation studies, such as analyses of different choices of $q_{\\theta}$ or different noise models."}, "questions": {"value": "As a non-expert in this subfield, I find it difficult to distinguish the key differences between the proposed method and a variational Boltzmann machine. Could the authors clarify this distinction more explicitly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BAtDAC0sTJ", "forum": "XthfAAfnVd", "replyto": "XthfAAfnVd", "signatures": ["ICLR.cc/2026/Conference/Submission21993/Reviewer_HR86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21993/Reviewer_HR86"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972395539, "cdate": 1761972395539, "tmdate": 1762942010173, "mdate": 1762942010173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}