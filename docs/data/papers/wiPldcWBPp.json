{"id": "wiPldcWBPp", "number": 24865, "cdate": 1758361343519, "mdate": 1762957697234, "content": {"title": "FINEdits : Precise Image Editing with Inferred Masks and Light Fine-tuning", "abstract": "Image editing with diffusion models faces a fundamental trade-off between edit fidelity and preservation of unedited regions. Training-free methods often suffer from imperfect inversion that degrades reconstruction quality, while training-based approaches require substantial computational resources and carefully curated datasets. We present FINEdits, a method that addresses these limitations through two key innovations: (1) automatic mask inference using cross-attention maps to explicitly preserve non-edited regions, and (2) lightweight fine-tuning to improve inversion quality without semantic drift. Our masking approach leverages transformer attention mechanisms to automatically identify editing regions using a parameter-free K-means clustering method, eliminating the need for manual hyperparameter tuning. To handle the inversion quality degradation at early timesteps required for large edits, we introduce a light fine-tuning strategy that balances reconstruction fidelity with semantic preservation. We introduce EditFFHQ, a new benchmark dataset of 2000 face images with sequential editing instructions, enabling quantitative evaluation of identity preservation and edit quality. Extensive experiments demonstrate that FINEdits achieves superior identity preservation while maintaining competitive edit fidelity and image quality. Our method provides an effective solution for precise image editing that preserves visual consistency without requiring extensive retraining or manual parameter adjustment.", "tldr": "we propose a new image editing method that better preserves elements from the original image", "keywords": ["computer vision", "generative modeling", "diffusion models", "image editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3e74d427cdca1dce566276f901715761cd7c4c82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an image editing method. To preserve unedited regions, this paper proposes to use cross-attention maps, which captures rich semantic information and is easy to obtain. Second, to improve inversion quality, a light fine-tuning strategy is introduced. By tuning on a single image, and limiting edited regions, the proposed method achieves good identity preserving scores and excellent editing quality on face image editing tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The core methodology is conceptually simple and direct."}, "weaknesses": {"value": "The manuscript suffers from three critical limitations: \n- First, its reliance on fine-tuning is inherently costly and time-consuming, limiting its real-world applicability. More importantly, this paradigm is highly prone to overfitting on the source image, which likely explains the narrow scope of the experiments (the method is only demonstrated on face image edits). It remains unproven whether the approach can handle more extensive edits or generalizes to diverse scenes. \n- Second, the empirical validation is insufficient to claim effectiveness. The exclusive focus on face image editing, without tests on broader tasks (e.g., background modification, or style change), fails to demonstrate that the method is a generally viable solution for image editing.\n- Third, the proposed technique of using cross-attention score maps to infer editing masks, as described in the \"Localized Editing with Inferred Masks\" section, lacks sufficient novelty. This core idea has been previously explored in [1][2].\n\n[1]Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing. WACV 2025. \n\n[2]DiffEdit: Diffusion-based semantic image editing with mask guidance. ICLR 2023."}, "questions": {"value": "More experiments on various editing scenes are necessary, and please also clarify the strengths of the proposed localized editing methods compared to previous inversion-based editing frameworks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zzXVS6GI6a", "forum": "wiPldcWBPp", "replyto": "wiPldcWBPp", "signatures": ["ICLR.cc/2026/Conference/Submission24865/Reviewer_px2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24865/Reviewer_px2m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948497531, "cdate": 1760948497531, "tmdate": 1762943226348, "mdate": 1762943226348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We withdraw the paper as it needs more work. We thank the reviewers for their constructive comments."}}, "id": "VEZeMIjNyW", "forum": "wiPldcWBPp", "replyto": "wiPldcWBPp", "signatures": ["ICLR.cc/2026/Conference/Submission24865/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24865/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762957696296, "cdate": 1762957696296, "tmdate": 1762957696296, "mdate": 1762957696296, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FINEdits, a new inversion-based image editing framework that combines inferred attention-based masks and light single-image fine-tuning to achieve precise, localized edits while preserving visual identity. The method leverages transformer cross-attention maps to automatically infer editing masks without manual tuning, and performs lightweight fine-tuning to enhance inversion quality. In addition, the authors introduce EditFFHQ, a new benchmark of 2000 face images annotated with sequential editing instructions for rigorous evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The methodology is technically sound, well-motivated, and experimentally validated. \n\nThe paper is well-structured, with clear mathematical formulations."}, "weaknesses": {"value": "The per-image fine-tuning step  introduces latency compared to purely training-free methods. \n\nIncluding more qualitative comparisons or representative failure cases would help readers better understand the strengths and limitations of the proposed approach, particularly in challenging or ambiguous editing scenarios.\n\nA more fine-grained ablation study—for instance, analyzing the effect of mask type, pooling kernel size, or fine-tuning duration—would provide deeper insights into the contribution of each design choice."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4FnjBCzK8R", "forum": "wiPldcWBPp", "replyto": "wiPldcWBPp", "signatures": ["ICLR.cc/2026/Conference/Submission24865/Reviewer_EV1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24865/Reviewer_EV1u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980506740, "cdate": 1761980506740, "tmdate": 1762943226173, "mdate": 1762943226173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the \"editability/reconstruction tradeoff\" in diffusion-based image editing—where large edits require strong noise (degrading non-edited content) and weak noise limits edit fidelity—by proposing FINEdits, a two-component framework. (1) Light fine-tuning: A single-image fine-tuning step (1000 steps on Stable Diffusion 3, SD3) aligns the input image with its text prompt, improving inversion quality and identity preservation without semantic drift. (2) Inferred attention-based masking: Cross-attention maps from SD3’s transformer layers (8–12, 10th layer selected) are processed via parameter-free K-means clustering (2 centroids) to isolate edit regions; max-pooling and Gaussian blur refine the mask to avoid artifacts.\nTo enable quantitative evaluation, the paper introduces EditFFHQ, a benchmark of 2000 FFHQ face images annotated for 7 edit attributes (beard, hair, earrings, etc.) with metrics including Identity Preservation (IP via ArcFace), edit success rate (Qwen-VL-27B), FID, CMMD, and LPIPS. Experiments on single/sequential edits show FINEdits outperforms baselines (UltraEdit, SDEdit, Kontext-dev) in IP (0.77 vs. Kontext-dev’s 0.72), FID (29.24 vs. Kontext-dev’s 31.23), and LPIPS (0.11 vs. Kontext-dev’s 0.20), while maintaining a high success rate (0.94 vs. Kontext-dev’s 0.96). The code details are provided in the appendix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Light Fine-Tuning Balances Inversion and Semantics\n    1. Single-image focus reduces compute. Section 3.2 uses 1000 steps (Adam8bit, batch size 1) on RTX4090, feasible for consumer hardware. Table 2 shows this lightweight step improves IP by 32% (0.52→0.77) vs. no fine-tuning. This matters for practicality, as it avoids large-scale retraining.\n    2. Semantic preservation is maintained. Figure 2 (N=1000 steps) shows edits (adding a hat) remain accurate, and Table 1 (success rate=0.94) is nearly on par with Kontext-dev (0.96). This ensures fine-tuning does not bias the model toward reconstruction over editing.\n    3. Generalizes to sequential edits. Figure 5 (5 consecutive edits) shows FINEdits maintains IP=0.7+ across steps, while Kontext-dev’s FID degrades faster. This confirms fine-tuning’s long-term benefit for edit sequences.\n2. Attention-Based Masking Enables Precise Localization\n    1. Parameter-free design avoids manual tuning. Section 3.3 uses K-means (2 centroids) on attention maps, no thresholds. Figure 3 shows this isolates edit regions (beard) without user input, unlike DiffEdit (requires mask thresholds).\n    2. Max-pooling/blur reduce artifacts. Figure 3(c) shows smooth transitions between edited/unedited regions, while unrefined masks (Figure 3(b)) leave visible boundaries. Table 2 shows masking reduces LPIPS by 54% (0.24→0.11) vs. no masking, confirming perceptual quality gains.\n    3. Layer selection is empirically validated. Section 3.3 tests layers 8–12, selecting 10 for best semantic signal. Appendix A.1’s edit-wise hyperparameters (e.g., kernel size=21 for glasses) further optimize mask performance.\n3. EditFFHQ Enables Rigorous Evaluation\n    1. Identity preservation metric fills a gap. IP (ArcFace cosine similarity) quantifies face identity, a critical unmeasured metric in prior benchmarks. Table 1 shows FINEdits’ IP=0.77 outperforms all baselines, including ICEdit (0.75).\n    2. Filtered metrics avoid misleading results. Only successful edits are included in IP/FID calculations, preventing trivial high IP from failed edits (e.g., SDEdit’s 0.13 IP includes failed edits). This ensures fair comparison.\n    3. Diverse attributes cover common edits. 7 attributes (add/remove beard, hat, etc.) test both addition and removal, unlike benchmarks focused on single edit types. Figure 6 (qualitative) shows FINEdits handles all attributes effectively.\n4. Strong Empirical Validation Across Edits\n    1. Single-shot edits outperform baselines. Table 1 shows FINEdits leads in IP (0.77), FID (29.24), CMMD (0.20), and LPIPS (0.11), with success rate=0.94 (near Kontext-dev’s 0.96). This confirms superiority in balanced performance.\n    2. Sequential edits maintain quality. Figure 5 shows FINEdits’ FID remains <30 after 5 edits, while Kontext-dev’s FID rises to 35+. IP stays >0.7, unlike UltraEdit’s 0.5 drop.\n    3. Ablations isolate component value. Table 2 shows fine-tuning drives IP gains, masking improves FID/LPIPS, and their combination optimizes all metrics. This validates both components’ necessity."}, "weaknesses": {"value": "1. Limited Generalization Beyond Face Editing\n   1. No non-face experiments. All tests use EditFFHQ (faces); no results on objects (e.g., \"add a handle to a mug\") or scenes (e.g., \"change sky to sunset\"). It is unclear if attention masks work for non-semantic regions (e.g., sky texture). Also, the dataset is constructed by authors, which may lead to potential bias and lack of diversity. Public datasets with could provide more comprehensive evaluation.\n   2. Ambiguous prompts are untested. Edits use clear prompts (\"add a hat\"); no tests on vague prompts (\"make hair look stylish\"). Masking may fail if attention maps cannot isolate ambiguous concepts.\n   3. Cross-model compatibility unproven. Only SD3 is tested; no results on Flux or SDXL or any other T2I models. It is unclear if fine-tuning/masking generalizes to different architectures or larger models. Fine-tuning/masking may behave differently on larger models with distinct attention patterns.\n2. Implementation Ambiguities\n   1. Mask layer selection lacks justification. Section 3.3 selects layer 10 \"empirically\". No analysis of why layer 10 has better semantic signal.\n   2. Fine-tuning hyperparameters are arbitrary. 1000 steps and 5e-5 learning rate are used, but no ablations on step count (500/2000) or LR (1e-5/1e-4) are reported. It is unclear if fewer steps yield similar gains.\n3. Mask Failure Modes and Robustness\n   1. Mask accuracy for small attributes is untested. Earrings (small) and glasses (thin frames) may be mislocalized, but no metrics for mask precision/recall are provided. Figure 3 only shows beard edits (large attribute).\n   2. No handling of overlapping attributes. Edits like \"add a hat and glasses\" (overlapping head regions) are untested. Masking may merge regions, leading to over-editing.\n   3. Sensitivity to prompt wording is unknown. Prompts use fixed phrasing (\"add a hat\"); no tests on variations (\"put on a cap\"). Attention maps may fail if token selection (contextual cues) is inconsistent.\n4. Reproducibility and Resource Gaps\n   1. Fine-tuning optimizer details are incomplete. Appendix A.1 mentions Adam8bit but not weight decay, gradient clipping, or learning rate scheduling. These affect fine-tuning stability.\n   2. VLM judge prompts are missing. Edit success rate uses Qwen-VL-27B, but the instruction prompt (e.g., \"Did the edit add a hat?\") is not provided. Without this, users cannot replicate success rate calculations.\n   3. Computational cost is unreported. No runtime for fine-tuning (per image) or mask computation. It is unclear how FINEdits’ overhead compares to training-free methods (e.g., RFSolver).\n5. Limited Technical Novelty\n   1. Applying mask on attention map for edition is not novel. Prior works (DiffEdit, InstructPix2Pix) use attention-based masks for editing.\n   2. Fine-tuning on single image for edition is not novel. Prior works (Textual Inversion, DreamBooth) fine-tune diffusion models on single image for editing."}, "questions": {"value": "1. **Does FINEdits work for non-face images and ambiguous prompts, and how can it be adapted?** All experiments use faces; could you add results on a non-face benchmark (e.g., COCO-Edit) with prompts like \"add a handle to a mug\" or \"change sky to sunset\"? For ambiguous prompts (\"make hair stylish\"), could you test if mask quality degrades and propose a fix (e.g., prompt parsing to refine c^q)? Additionally, could you compare mask precision/recall for small (earrings) vs. large (beard) attributes?\n2. **Why is layer 10 optimal for masking, and how do fine-tuning hyperparameters (steps, LR) affect performance?** Section 3.3 selects layer 10 empirically; could you add a table showing IP, FID, and mask IoU for layers 8–12? For fine-tuning, could you ablate step count (500/1000/2000) and LR (1e-5/5e-5/1e-4) to show if 1000 steps/5e-5 is optimal? This would guide users to tune parameters for their use cases.\n3. **Does FINEdits work on other models (e.g., Flux, DiT), and what is its computational overhead?** Only SD3 is tested; could you add results on Flux and DiT to confirm generalization? For runtime, could you report fine-tuning time per image (RTX4090/A100) and mask computation time, comparing to those training-free methods (e.g., RFSolver’s 2s/mask vs. FINEdits’)?\n4. **How does FINEdits handle overlapping edits (e.g., \"add hat + glasses\") and dynamic edits (e.g., \"smile wider\")?** Overlapping attributes may confuse masks; could you test \"add hat + glasses\" and report mask IoU for each attribute? For dynamic edits, could you test incremental changes (\"smile slightly\" → \"smile wider\") to see if fine-tuning maintains consistency? Additionally, could you add a metric for mask overlap (e.g., IoU between hat and glasses masks)?\n\nOverall, I will reject this paper in its current form. And, I sincerely recommend that author should significantly improve their paper (at least fill all 9 pages for ICLR) and resubmit it in the future."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uUAof3wiRt", "forum": "wiPldcWBPp", "replyto": "wiPldcWBPp", "signatures": ["ICLR.cc/2026/Conference/Submission24865/Reviewer_QwbN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24865/Reviewer_QwbN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989397036, "cdate": 1761989397036, "tmdate": 1762943225958, "mdate": 1762943225958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FINEdits, a text-guided image editing method for flow-matching models (specifically SD3) that aims to strengthen edits while preserving non-edited content. It combines single-image fine-tuning of the velocity network with cross-attention–derived spatial masks, which are used to restrict where edits can occur. The authors also propose EditFFHQ, a benchmark of FFHQ faces annotated by a vision-language model for seven attributes, designed to test both single-shot and sequential edits with explicit identity preservation metrics. Experiments on SD3 show that FINEdits improves identity preservation and perceptual quality over several recent baselines in both single and multi-step editing scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method delivers strong empirical gains in identity preservation and perceptual quality on SD3 while keeping edit success rates competitive with recent state-of-the-art systems.\n\n- The evaluation protocol explicitly conditions identity and quality metrics on successful edits, which addresses a common flaw in prior editing evaluations and leads to more meaningful comparisons.\n\n- The decomposition of the pipeline into inversion-aware fine-tuning and attention-based masking, together with ablation studies, provides a clear picture of how each component contributes to the overall performance.\n\n- EditFFHQ focuses on sequential edits and identity preservation, offering a benchmark that better matches practical editing scenarios than one-shot, single-attribute tests."}, "weaknesses": {"value": "- The conceptual novelty is limited, as both single-image fine-tuning and attention-based masking have been explored before in related diffusion editing frameworks.\n\n- The per-image fine-tuning stage requires substantial computation (1k steps), yet the paper does not report wall-clock runtimes or compare latency with training-free baselines.\n\n- All experiments are conducted on FFHQ faces with a small fixed set of attributes. There is no evidence that the approach generalizes to more complex scenes, object categories, or free-form prompts."}, "questions": {"value": "How much time does the 1k-step fine-tuning plus editing take per image, and how does that compare to training-free methods such as LEdits++ or other SD3-based baselines?\n\nDo you observe a clear trade-off curve between the number of fine-tuning steps and performance (identity, FID, edit success), and is 1,000 steps close to the knee of that curve?\n\nCan you provide any qualitative or quantitative evidence that FINEdits generalizes beyond faces—for example to object-centric or scene-centric images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D3t0XAkKoh", "forum": "wiPldcWBPp", "replyto": "wiPldcWBPp", "signatures": ["ICLR.cc/2026/Conference/Submission24865/Reviewer_eoKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24865/Reviewer_eoKi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248634012, "cdate": 1762248634012, "tmdate": 1762943225737, "mdate": 1762943225737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}