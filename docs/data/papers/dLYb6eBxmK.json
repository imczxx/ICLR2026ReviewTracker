{"id": "dLYb6eBxmK", "number": 14380, "cdate": 1758234195913, "mdate": 1763699698842, "content": {"title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage", "abstract": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR–LLM–TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue models have effectively mitigated this issue through tools such as web search and knowledge-graph APIs, extending such capabilities to speech-in speech-out systems remains underexplored. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (Stream RAG), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech,  even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to issue tool calls during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both accuracy and responsiveness. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our Stream RAG approach increases QA accuracy by over 200% relative and further enhances user experience by reducing tool use latency by 17%. Importantly, our Stream RAG approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.", "tldr": "We present Streaming RAG, a framework enabling low-latency tool use in speech-in speech-out dialogue by issuing tool queries parallel with user speech, doubling factual accuracy and reducing response latency by 20%.", "keywords": ["Speech-in Speech-out Dialogue Systems", "Speech Language Models", "Tool-Augmented Dialogue Systems", "Retrieval-Augmented Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf34e204e5e708ba8914f4fb576346e7e736095e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Stream RAG, a framework to make spoken dialogue systems both accurate and responsive. The authors address a core trade-off: end-to-end speech-in-speech-out systems are fast but prone to factual errors (hallucinations). While Retrieval-Augmented Generation (RAG) can ground them with external tools like web search, this traditionally adds significant latency, disrupting the conversational flow. The key innovation of Stream RAG is to predict and issue tool queries in parallel with the user's speech, even before the user has finished talking. Specifically, the paper proposes two methods, including an advanced \"Model-Triggered\" approach where the model is post-trained to learn the optimal moment to make a tool call. To evaluate their work, the authors created AudioCRAG, a new benchmark of spoken queries. Experiments show that Stream RAG improves question-answering accuracy by over 200% compared to a no-tool baseline, while also reducing the latency from tool usage by over 20%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1- The paper tackles a timely issue at the intersection of spoken dialogue systems and LLMs. While tool use in text-based systems is well-explored, this is the pioneer work to systematically address its integration into speech-in-speech-out models, regarding latency barriers to adoption. The central idea of tool queries in parallel, along with ongoing speech, is an effective way to mask the latency of external tool calls.\n\n2-  The authors provide AudioCRAG, a significant resource to the research community. This enables standardized evaluation and fosters future research on the problem.\n\n3- Extensive experiments (including those reported in the appendix) with promising results."}, "weaknesses": {"value": "1- The evaluation is built upon the CRAG and TriviaQA datasets, which are composed of single-turn, fact-seeking questions. Can the author elaborate on how to expand the solution to a multi-turn setup? The \"streaming\" nature of the solution might be less effective or even problematic in multi-turn contexts where the true intent depends on previous turns.\n\n2- In the Fixed Interval Streaming RAG section, the process still needs to process to the end of the input to get the tool query for the final block, and then can reflect all the previous ones. I see little improvement over the first token latency in Table 1 in the AudioCRAG-Synthetic partition (5.9 ⇒ 5.32) vs the AudioCRAG-Human (5.4 ⇒ 3.6). Can you elaborate on the difference?\n\n3- The Fixed-Interval approach relies on a \"reflector\" module with simple heuristics (e.g., matching top 5 web docs, identical KG results). These heuristics may not be robust. For example, two different web queries could return slightly different but equally valid sets of documents.\n\n4- While the relative accuracy improvements are impressive, the final absolute accuracy scores are still modest (e.g., 34.2% - 37.4% for Qwen2.5-7B in Table 1)\n\n5- Spoken language is messy and filled with disfluencies (e.g., um, uh), repetitions, and self-corrections (I want to fly to Boston; no, wait, to New York, …). The framework would likely fire a query for \"Boston\" before the user corrects themselves. This could lead to wasted queries and a need for a complex query cancellation/updating logic. Can the authors elaborate more on this scenario? The negative sampling strategy helps with ASR ambiguity, but may not be sufficient for explicit user intent changes.\n\n6- Finally, I just wonder whether this study is suitable for the scope of the ICLR conference? I mean, while this conference focuses on a novel theoretical method of learning representation, this study tries to solve a specific application. Please emphasize the main contribution of this work."}, "questions": {"value": "Please check the comments in the Weaknesses Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4g8oR0nwU", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Reviewer_Sboq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Reviewer_Sboq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640082870, "cdate": 1761640082870, "tmdate": 1762924798026, "mdate": 1762924798026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are encouraged that our experiments were recognized as convincingly demonstrating the effectiveness of our approach in improving both the accuracy and latency of tool calls (@R ECqA, @R Sboq), and that the proposed AudioCRAG benchmark was viewed as a valuable contribution to the community (@R BKtK, @R Sboq). We will incorporate the reviewers’ suggestions and clarifications into the revised version of the paper, as highlighted in blue in the updated pdf.\n***\n# Marginal Latency Improvement (R ECqA,  R Sboq):\n\nWe thank the reviewer for this thoughtful comment regarding latency improvements. **Our main clarification is that the latency reductions reported in Table 3 represent conservative estimates, and after incorporating standard optimizations (e.g., VLLM) as well as end-point detection latency, our updated system achieves substantially larger and practically meaningful improvements.**\nSpecifically, as shown in Table 3, our approach achieves a 9.8% relative reduction in P-50 latency on Audio CRAG-Synthetic. It is also important to note that our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production speech systems. Because our Stream RAG design allows processing to begin without waiting for explicit end-point detection, this factor would further amplify real-world latency gains, consistent with the larger reduction observed for human-spoken audio, where we reduce P-50 latency from 5.4 s to 3.6 s (a 33% relative improvement). These results indicate that our method particularly benefits scenarios involving natural speech, where trailing silences and variable end-points are common.\nIn response to the reviewer’s suggestion, we additionally integrated vLLM for optimized inference with Qwen-OMNI. This update led to substantial further improvements, yielding a 16.6% reduction in P-50 latency (3.5 → 2.92 s) for Audio CRAG-Synthetic and an even more significant 57% reduction (3.16 → 1.36 s) for Audio CRAG-Human. These updated results have been included in Table 13 in the revised manuscript.\n***\n# Absence of Human Evaluation to indicate if latency is noticeable (R ECqA,  R WeMu):\n\nWe thank the reviewer for this observation. **Our main clarification is that the latency reductions we report are not only statistically meaningful but also perceptually significant when viewed against well-established human turn-taking thresholds.** Prior work on human conversational timing [1, 2] shows that the average turn-taking gap in natural human dialogue is approximately 239 ms, with delays beyond 500 ms perceived as unnatural. Complementary industry analyses [3]  similarly report that speech latency exceeding ~500 ms begins to degrade user experience, causing users to interrupt or disengage.\nIn this context, our observed latency reductions, particularly a 1.8 s improvement for human-spoken audio, represent a substantial enhancement relative to perceptual thresholds in human conversation. Moreover, our integration of efficient inference backends (e.g., vLLM for Qwen-OMNI) achieves P-50 latency near 670 ms, comparable to production-quality voice AI systems that target sub-second responsiveness.\nWe have explicitly clarified this discussion and its perceptual implications in Section A.12 to make the relevance of the latency improvement clearer to readers.\nHence, we respectfully submit that the reported reductions are both statistically significant and perceptually relevant, contributing directly to smoother conversational turn-taking and a more natural user experience in speech-to-speech dialogue systems.\n***\nReferences:\n\n[1] Human Latency Conversational Turns for Spoken Avatar Systems (https://arxiv.org/html/2404.16053v1)\n\n[2] Universals and cultural variation in turn-taking in conversation (https://www.pnas.org/doi/full/10.1073/pnas.0903616106)\n\n[3] VAPI blog post (https://vapi.ai/blog/speech-latency)"}}, "id": "iCSdsBxu20", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699775287, "cdate": 1763699775287, "tmdate": 1763699775287, "mdate": 1763699775287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their thoughtful and constructive feedback. We are encouraged that our experiments were recognized as convincingly demonstrating the effectiveness of our approach in improving both the accuracy and latency of tool calls (@R ECqA, @R Sboq), and that the proposed AudioCRAG benchmark was viewed as a valuable contribution to the community (@R BKtK, @R Sboq). We will incorporate the reviewers’ suggestions and clarifications into the revised version of the paper, as highlighted in blue in the updated pdf.\n***\n# Marginal Latency Improvement (R ECqA,  R Sboq):\n\nWe thank the reviewer for this thoughtful comment regarding latency improvements. **Our main clarification is that the latency reductions reported in Table 3 represent conservative estimates, and after incorporating standard optimizations (e.g., VLLM) as well as end-point detection latency, our updated system achieves substantially larger and practically meaningful improvements.**\nSpecifically, as shown in Table 3, our approach achieves a 9.8% relative reduction in P-50 latency on Audio CRAG-Synthetic. It is also important to note that our latency calculations on synthetic audio exclude end-point detection latency, which is required in all production speech systems. Because our Stream RAG design allows processing to begin without waiting for explicit end-point detection, this factor would further amplify real-world latency gains, consistent with the larger reduction observed for human-spoken audio, where we reduce P-50 latency from 5.4 s to 3.6 s (a 33% relative improvement). These results indicate that our method particularly benefits scenarios involving natural speech, where trailing silences and variable end-points are common.\nIn response to the reviewer’s suggestion, we additionally integrated vLLM for optimized inference with Qwen-OMNI. This update led to substantial further improvements, yielding a **16.6% reduction in P-50 latency (3.5 → 2.92 s) for Audio CRAG-Synthetic and an even more significant 57% reduction (3.16 → 1.36 s) for Audio CRAG-Human**. These updated results have been included in Table 13 in the revised manuscript.\n***\n# Absence of Human Evaluation to indicate if latency is noticeable (R ECqA,  R WeMu):\n\nWe thank the reviewer for this observation. **Our main clarification is that the latency reductions we report are not only statistically meaningful but also perceptually significant when viewed against well-established human turn-taking thresholds.** Prior work on human conversational timing [1, 2] shows that the average turn-taking gap in natural human dialogue is approximately 239 ms, with delays beyond 500 ms perceived as unnatural. Complementary industry analyses [3]  similarly report that speech latency exceeding ~500 ms begins to degrade user experience, causing users to interrupt or disengage.\nIn this context, our observed latency reductions, particularly a 1.8 s improvement for human-spoken audio, represent a substantial enhancement relative to perceptual thresholds in human conversation. Moreover, our integration of efficient inference backends (e.g., vLLM for Qwen-OMNI) achieves P-50 latency near 670 ms, comparable to production-quality voice AI systems that target sub-second responsiveness.\nWe have explicitly clarified this discussion and its perceptual implications in Section A.12 to make the relevance of the latency improvement clearer to readers.\nHence, we respectfully submit that the reported reductions are both statistically significant and perceptually relevant, contributing directly to smoother conversational turn-taking and a more natural user experience in speech-to-speech dialogue systems.\n***\nReferences:\n\n[1] Human Latency Conversational Turns for Spoken Avatar Systems (https://arxiv.org/html/2404.16053v1)\n\n[2] Universals and cultural variation in turn-taking in conversation (https://www.pnas.org/doi/full/10.1073/pnas.0903616106)\n\n[3] VAPI blog post (https://vapi.ai/blog/speech-latency)"}}, "id": "iCSdsBxu20", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763699775287, "cdate": 1763699775287, "tmdate": 1763737103306, "mdate": 1763737103306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Stream-RAG, a retrieval-augmented generation framework tailored for streaming document settings, where knowledge sources are continuously updated or appended. The authors propose a hybrid architecture that combines segment encoders, recurrent context encoders, and a local-global memory routing mechanism. Their system maintains incremental representations of documents and allows for low-latency RAG in scenarios where documents arrive sequentially and retrieval must be both instantaneous and up-to-date."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a practically important and understudied challenge in retrieval-augmented generation: adapting RAG to streaming data where documents are not static.\n2. The proposed Segment Encoder + Context Encoder structure allows for localized recomputation, minimizing the need to re-embed entire documents on each update.\n3. The design of local and global memory banks is conceptually appealing and appears to offer good trade-offs between recency and long-term context awareness.\n4. Demonstrated latency-speedup (up to 10×) and retrieval improvements over standard RAG models across datasets (HotpotQA, NaturalQuestions, CodeSearchNet).\n5. The authors define and release a streaming QA benchmark based on HotpotQA, which could be useful for future research."}, "weaknesses": {"value": "1. While three models are evaluated, the Streaming RAG method (especially Model-Triggered) is only fully tested on Qwen2.5-7B and OpusLM. Kimi Audio, for example, is excluded from streaming RAG due to tool reference limitations. This raises the question of general applicability across a broader range of E2E SDS architectures.\n\n2. Despite large relative improvements, absolute QA accuracy remains low (e.g., <40%). While the authors mention consistency with CRAG benchmarks, this still limits the practical utility of the system for high-stakes applications. Moreover, no comparison is made with non-E2E baselines (e.g., traditional ASR → LLM → TTS systems).\n\n3. Although latency and accuracy metrics are provided, no user studies or human preference evaluations are presented to validate the subjective impact on conversational flow—especially crucial for spoken dialogue systems where perceived responsiveness is key.\n\n4. The reflector module for the Fixed-Interval variant uses hand-crafted heuristics (top-5 web document overlap, etc.). These design choices are not thoroughly ablated or compared with learned alternatives. How brittle are these heuristics across domains?\n\n5. AudioCRAG, though valuable, consists of synthetic and short utterances. The authors acknowledge only 618 human queries. It remains unclear how well Streaming RAG performs in noisy, multi-turn, or conversational settings. Real-world deployment scenarios are underexplored.\n\n6. The paper compares only with standard RAG and Streaming RAG. There is no baseline using non-streaming anticipatory query prediction, such as speculative decoding or early-termination heuristics. This limits understanding of where the performance gains truly stem from."}, "questions": {"value": "Q1: How robust is the Streaming RAG approach to noisy ASR outputs, especially during partial utterances? Is there any performance degradation reported?\n\nQ2. Can the model recover from tool query hallucinations in real-time? Does the model-triggered variant mitigate cascading failures in cases of early wrong tool queries?\n\nQ3. Is Streaming RAG applicable in multi-turn dialogue settings? If so, how are tool query histories managed across turns?\n\nQ4. Can the authors report any qualitative examples of Streaming RAG responses compared to standard ones (e.g., hallucinated vs. grounded speech output)?\n\nQ5. Have the authors considered latency from audio end-point detection? In real deployments, this often dominates response time. How would that affect the observed latency gains?\n\nQ6. What prevents integration of Streaming RAG into models like Kimi Audio? Is it a limitation of the tool reference length only, or also architectural?\n\nQ7. What is the impact of different chunk sizes or block sizes in fixed-interval RAG? Is there a trade-off between chunk length, responsiveness, and resource usage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h3mHFnkIw7", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Reviewer_vHEX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Reviewer_vHEX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985385265, "cdate": 1761985385265, "tmdate": 1762924797704, "mdate": 1762924797704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the integration of external tool usage (web search, knowledge graphs) into end-to-end speech-in speech-out dialogue systems while minimizing latency through streaming RAG. The main contributions include: (1) a formal framework for tool integration in speech-based systems showing accuracy improvements but 2.3x latency increase; (2) Streaming RAG with two variants (Fixed-Interval and Model-Triggered) that predict tool queries in parallel with user speech, achieving accuracy improvement with 20% latency reduction; and (3) AudioCRAG benchmark with synthetic and human-recorded spoken queries from the CRAG dataset. The paper evaluates three LLM baselines (Qwen-OMNI, OpusLM, Kimi-Audio) across closed-book, open-book, and streaming RAG settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces the systematic method to extend RAG into real-time voice-to-voice LLMs with attention to latency.\n2. Proposes a model-triggered streaming query mechanism that achieves both accuracy improvements and latency reductions.\n3. Releases a valuable new benchmark (AudioCRAG) with synthetic and human speech variants;"}, "weaknesses": {"value": "1. The novelty is incremental in combining known ideas (RAG + streaming inference) rather than introducing fundamentally new architectures.  \n2. Evaluation accuracy levels remain low in absolute terms, raising questions about real-world utility despite relative gains\n3. Heavily dependent on synthetic data for post-training, with limited human data evaluation and possible overfitting to benchmark-specific patterns\n4. Relative improvement in ACC and latency reduction is very confusing;"}, "questions": {"value": "1. How would the proposed Streaming RAG framework perform with more complex multi-turn dialogues or ambiguous queries, where early tool calls could misfire? \n2. What strategies could further raise absolute accuracy for speech output, given the modality gap between text and speech responses highlighted in your results?  \n3. How are the 20.7% and 53.4% latency savings calculated in Streaming RAG + Qwen2.5-7B?   Compared to Open Book, the time to first token (TTFS) is reduced from 5.9 to 5.32, representing a 9.8% decrease. (1 - 5.32/5.9). Many numbers calculated in the manuscript are unclear and messy.\n4. Does the author consider the false positive situation for tool-calling?  What is the ACC for model-triggered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KRhKElvEce", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Reviewer_WeMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Reviewer_WeMu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996649088, "cdate": 1761996649088, "tmdate": 1762924797104, "mdate": 1762924797104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Streaming RAG, a framework designed to integrate parallel Retrieval-Augmented Generation (RAG) processes with speech-to-speech systems. The authors also present a post-training pipeline to instruct models on when to issue tool calls and how to generate spoken summaries. Additionally, they construct a benchmark, AudioCRAG, to evaluate the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- Experimental results convincingly demonstrate the effectiveness of the proposed streaming RAG approach in improving QA accuracy."}, "weaknesses": {"value": "- Insufficient Related Work Discussion: The paper claims to be the first to extend RAG to speech-to-speech systems. However, it overlooks an important related work—WavRAG [1]—which also claims RAG capabilities in speech-to-speech systems. This omission significantly undermines the validity of the paper’s main contribution.\n- Lack of Novelty: As stated by the authors (lines 113–117), RAG for speech-to-text systems and for speech-to-speech systems using multimodal embedding retrieval has already been explored. The current work focuses on applying RAG to speech-to-speech systems in web retrieval and KG API retrieval scenarios. This incremental extension reduces the overall novelty of the contribution. Furthermore, the work appears to be largely engineering-oriented, primarily involving the integration of existing speech-to-speech systems with parallel RAG modules.\n- Marginal Latency Improvement: According to Table 3, the reported latency reduction is only about 6% (from 9.00 to 8.47 seconds). This reviewer considers such a reduction to be trivial and likely imperceptible to end users.\n- Absence of Human Evaluation: The authors claim that Streaming RAG reduces user-perceived latency (as stated in the abstract and Section 2.2). However, no human evaluation is provided to assess whether users actually perceive the latency reduction as significant. For instance, it remains unclear whether the modest latency improvement would be noticeable or meaningful to real users.\n- Writing Inconsistencies: \n    - The title uses \"stream RAG,\" while the main text uses \"Streaming RAG.\" \n    - The term \"speech-out\" is italicized in line 120 (\"speech-in speech-out\") but not elsewhere, resulting in inconsistent formatting.\n\n[1] Yifu Chen, Shengpeng Ji, Haoxiao Wang, Ziqing Wang, Siyu Chen, Jinzheng He, Jin Xu, and Zhou Zhao. 2025. WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12505–12523, Vienna, Austria. Association for Computational Linguistics."}, "questions": {"value": "- Do the authors investigate whether enabling the model to trigger multiple queries in parallel within the model-triggered setting could further enhance performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9TNFrcbQH", "forum": "dLYb6eBxmK", "replyto": "dLYb6eBxmK", "signatures": ["ICLR.cc/2026/Conference/Submission14380/Reviewer_ECqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14380/Reviewer_ECqA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762398637158, "cdate": 1762398637158, "tmdate": 1762924796627, "mdate": 1762924796627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}