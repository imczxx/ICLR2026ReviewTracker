{"id": "afRg6IS1Sc", "number": 17464, "cdate": 1758276353245, "mdate": 1763734840907, "content": {"title": "Rethinking Heavy Models in Multivariate Time Series Anomaly Detection", "abstract": "Multivariate time series anomaly detection (MTS-AD) is widely used, but real-world deployments often face tight computational budgets that limit the practicality of deep learning. We revisit whether heavy deep models (high-FLOPs architectures) are necessary to achieve strong detection performance in such settings. We conduct a systematic, compute-aware comparison of statistical, classical machine learning, and deep learning methods across diverse MTS-AD benchmarks, measuring detection with AUROC (threshold-free, thus application-agnostic) and cost with FLOPs (a hardware-agnostic proxy enabling fair cross-method comparison). We find that traditional approaches often match or surpass deep models, which appear less frequently among the top performers, and that the effectiveness-efficiency trade-off commonly favors non-deep alternatives under limited budgets. These results indicate that deep learning is not uniformly superior for MTS-AD and that heavy architectures can be counterproductive in resource-constrained deployments. These findings offer practical guidance for practitioners designing anomaly monitoring systems under compute constraints, highlighting cases where lightweight models are sufficient and heavy deep models may be worth the cost.", "tldr": "", "keywords": ["Multivariate Time Series Anomaly Detection", "Effectiveness-Efficiency Trade-off", "Resource-Constrained Environment", "AUROC", "FLOPs"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c8c0390e79c77b72c0e1ba4134a34c7e95e30058.pdf", "supplementary_material": "/attachment/2122cf97ccecf3a365731ad241da966f337592ff.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies multi-variate time series anomaly detection in the context of accuracy and compute efficiency.  It carries out a comparison of statistical, classical machine learning, and deep learning based methods through the lens of efficiency and finds that often non-deep learning alternatives perform better under limited compute budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It is a well-written paper that poses an important question and seeks its answer by putting together a systematic study. Section 3.2.1 outlines the principles and reasoning that guided the selection of anomaly detection methods and benchmarks used in this study.  The paper uses FLOPs as a measure of computational resources required by a method and AUROC as the measure of performance.  Table 3 that captures performance vs. accuracy supports the assertion that deep learning model, despite having large computational requirements, rarely achieve performance that is \"far better\" than what is achieved by other techniques."}, "weaknesses": {"value": "I don't quite get Figure 2.  Especially, why the individual dots connected by dashed lines ... aren't these discrete measurements?\n\nAUROC is useful but insufficient for deployment.  We require thre threshold protocol, calibration and robustness to threshold selection, and domain specific accuracy requirements in order to decide which of the given list of methods is most useful in a given setting.  I would hazard a guess while the results are insightfull, these lack actionable information that a practitioned may be able to use to deploy a multi-variate time-series anomaly detection in industrial settings."}, "questions": {"value": "Most anomaly detection methods require some sort of a threshold to make the final estimation whether or not an anomaly as occured.  This work seems to downplay the effect of threshold selection on the overall accuracy.  How AUROC is computed in the absence of threshold, or how thresholds were selected (where needed)?  Is it possible that some methods are less sensitive to threshold selection?  Perhaps this is a factor that should also be taken into account in a study such as the one proposed in this paper?\n\nImagine we are given a method that has very low compute requirements as measured by FLOPs, but also very low accuracy as measured by AUROC.  Where would this method sit within the comparisons presented in Table 3?  Also, when we search for acceptable performance vs. accuracy balance, we often have a some minimum accuracy requirements, which may be different in different domains.  How would that play out in the recommendations outlined in this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hawSMGG4g9", "forum": "afRg6IS1Sc", "replyto": "afRg6IS1Sc", "signatures": ["ICLR.cc/2026/Conference/Submission17464/Reviewer_HTcA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17464/Reviewer_HTcA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751425050, "cdate": 1761751425050, "tmdate": 1762927348429, "mdate": 1762927348429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on the problem of multivariate anomaly detection in limited-resource scenarios, as encountered in many real-world applications. To that end, this experimental work proposes to answer the following two questions:\n\n- What are the most effective options for time series anomaly detection under limited computational resources, and are deep learning methods always the best options? \n- Does a trade-off between detection performance and computational cost truly exist in practice? \n\nTo answer the questions, the paper carries three different experiments to assess performance vs efficiency: 1) trade-off between detection accuracy, measured by AUROC, and the computational cost quantified by training and inference FLOPs; 2) FLOPs (algorithmic operation counts) vs FLOPS (hardware throughput comparison; and 3) scalability as a function of data volume."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is a well-written paper with a clear experimental setup that aims to address a practical, but relevant question.\n- Good coverage of baselines\n- Insightful conclusions"}, "weaknesses": {"value": "Overall, this is a good paper. I see as a weakness that this may not be the typical paper expected in ICLR (which motivates my score), but I do not see major weaknesses for an experimental paper. \n\n- As hardware is central to this work, it would have been good to that the different hardware configuration is reported in the main paper."}, "questions": {"value": "Similar efforts [1], though not focused on resource constraints, have investigated the advantages of traditional vs. deep learning based approaches. Could you position your work with respect to this one and establish similarities and differences?\n\n[1] https://doi.org/10.1016/j.patcog.2022.108945"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RMZIn92bst", "forum": "afRg6IS1Sc", "replyto": "afRg6IS1Sc", "signatures": ["ICLR.cc/2026/Conference/Submission17464/Reviewer_PNpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17464/Reviewer_PNpt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944315318, "cdate": 1761944315318, "tmdate": 1762927348027, "mdate": 1762927348027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the necessity of heavy deep learning architectures for multivariate time series anomaly detection under resource-constrained environments. By introducing a hardware-agnostic measure (FLOPs) as a proxy for computational cost, the study contributes to the discussion on the effectiveness-efficiency trade-off in anomaly detection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The use of a hardware-agnostic FLOPs measure for runtime evaluation is well-motivated and provides a fair framework for cross-model comparison.\n* The derivation and formulation of FLOPs for popular TSAD algorithms are clearly described.\n* The paper presents a comprehensive evaluation of various models with respect to FLOPs, offering insights into the trade-offs between performance and computational efficiency."}, "weaknesses": {"value": "* The study is constrained by limited datasets, algorithms, and evaluation aspects, which may restrict the generalizability of its conclusions.\n* Limited theoretical or diagnostic interpretation of accuracy-runtime trade-off between lightweight and heavy models.\n\nPlease find the detailed comments in the following section."}, "questions": {"value": "* Limited TSAD algorithm coverage. The study includes only 16 anomaly detection algorithms, while recent benchmark efforts (e.g., Schmidl et al., 2022 with 70 methods; Liu & Paparrizos, 2024 with 40 methods) have evaluated much larger and more diverse collections.\n* Limited evaluation measures. Relying solely on AUROC overlooks known limitations of this measure in time series anomaly detection (e.g., its sensitivity to temporal noise, imbalance in anomaly ratio, and inability to capture temporal localization). Recent works have proposed time-seriesâ€“aware measures such as Range-F1 [1] and VUS-PR [2], which should be considered for a more robust evaluation.\n* Dataset selection limitations. The benchmark omits widely used datasets such as Exathlon [3] and TSB-AD (Liu & Paparrizos, 2024), which represent more diverse operational scenarios and anomaly types. \n* The study mainly investigates multivariate time series but does not address the univariate case. Furthermore, it is unclear how model scalability behaves with respect to both sequence length and feature dimensionality, as no explicit analysis of these factors is provided.\n* While adopting FLOPs as a hardware-agnostic proxy is a good practice, complementing it with real runtime measurements and memory footprint analyses across identical hardware would provide stronger evidence for practical applicability in real deployments.\n* The paper would benefit from a deeper analysis of when and why lightweight or heavy models perform better. For instance, cases where LSTM-VAE achieves relatively low FLOPs but competitive performance compared to heavier models like LOF need further investigation. Beyond aggregate AUROC scores across the entire dataset, it would be useful to analyze performance across different anomaly types and anomaly ratios to better contextualize the observed trends.\n\n[1] Tatbul N, Lee TJ, Zdonik S, Alam M, Gottschlich J. Precision and recall for time series. Advances in neural information processing systems. 2018;31.\n\n[2] Paparrizos J, Boniol P, Palpanas T, Tsay RS, Elmore A, Franklin MJ. Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection. Proceedings of the VLDB Endowment. 2022 Jul 1;15(11):2774-87.\n\n[3] Jacob V, Song F, Stiegler A, Rad B, Diao Y, Tatbul N. Exathlon: a benchmark for explainable anomaly detection over time series. Proceedings of the VLDB Endowment. 2021 Jul 1;14(11):2613-26."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DslMsf7TXr", "forum": "afRg6IS1Sc", "replyto": "afRg6IS1Sc", "signatures": ["ICLR.cc/2026/Conference/Submission17464/Reviewer_dAaG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17464/Reviewer_dAaG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966188390, "cdate": 1761966188390, "tmdate": 1762927347499, "mdate": 1762927347499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical study challenging the necessity of \"heavy\" deep learning models for multivariate time series anomaly detection (MTS-AD), particularly in resource-constrained environments . The authors conduct a comparative analysis of statistical, classical machine learning (one-class), and deep learning (reconstruction-based) methods across six benchmarks. The study evaluates models on two axes: detection effectiveness (using the threshold-agnostic AUROC metric) and computational efficiency (using a hardware-agnostic FLOPs metric).\n\nThe study finds that traditional models (e.g., HBOS, Isolation Forest, ABOD) frequently achieve top-tier AUROC performance, often matching or exceeding their deep learning counterparts . Furthermore, the analysis of AUROC vs. FLOPs (Figure 1) suggests that deep learning models present a poor trade-off, incurring high computational costs without delivering superior performance. The authors conclude that deep learning is not uniformly superior and that lightweight traditional models are often the more practical choice for constrained deployments ."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper asks a timely and important question: are heavy deep models worth the cost for MTS-AD in real-world, constrained settings ? This is a critical concern for practitioners in industrial, IoT, and embedded systems.\n2. The joint evaluation of both detection performance (AUROC) and hardware-agnostic computational cost (FLOPs) is a valuable contribution."}, "weaknesses": {"value": "1. The paper's primary flaw lies in its selection of deep learning models. The entire \"Reconstruction\" (i.e., deep learning) category consists almost exclusively of older, simpler autoencoder variants (LSTM-AE, LSTM-VAE, USAD, DeepSVDD). These models are no longer representative of the state-of-the-art. The study omits the entire class of modern, high-performance Transformer-based and CNN-based anomaly detectors. The comparison is therefore not \"Rethinking Heavy Models\" but \"Rethinking Outdated Models\".\n2. The paper incorrectly frames \"deep learning\" as \"heavy\" and \"traditional\" as \"lightweight,\" when its own data often shows the opposite. This invalidates the core narrative.\n3. While AUROC is threshold-agnostic, it is notoriously unreliable for anomaly detection on datasets with high class imbalance (which is characteristic of all TSAD benchmarks) . AUPRC (Area Under Precision-Recall Curve) is the standard, more informative metric in this setting. By optimizing for a potentially misleading metric (AUROC), the performance rankings (Table 3) may not reflect true detection quality. This choice further weakens the paper's conclusions."}, "questions": {"value": "Re: Weakness #1: The paper's conclusions hinge on comparing deep learning to traditional methods. Why did the authors choose to represent the entire deep learning category with only older, reconstruction-based models, while omitting all modern SOTA architectures like Anomaly Transformer, TimesNet, or TranAD, which are the \"heavy models\" the community is actually discussing today?\n\nRe: Weakness #2: The FLOPs data in Table 3 (e.g., SMD, SMAP) shows that statistical models like ABOD and LOF are orders of magnitude more computationally expensive (higher FLOPs) than deep models like OmniAnomaly or LSTM-AE. How do the authors reconcile this fact with the paper's central narrative that deep learning models are the \"heavy\" option and traditional models are the \"lightweight\" alternative?\n\n\nRe: Weakness #3: Why did the authors choose AUROC as the sole metric for detection performance, given that AUPRC is widely accepted as a far more informative and reliable metric for highly imbalanced anomaly detection tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "45DE5qqbQ7", "forum": "afRg6IS1Sc", "replyto": "afRg6IS1Sc", "signatures": ["ICLR.cc/2026/Conference/Submission17464/Reviewer_YBTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17464/Reviewer_YBTB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972523089, "cdate": 1761972523089, "tmdate": 1762927347017, "mdate": 1762927347017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}